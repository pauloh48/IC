{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMedium_10_3_5_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "sW78AaaneEUs",
        "dL0eWrGYhstu",
        "zW6LV4zMhstv",
        "VngEb4vfhstw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6087374-3c9d-4ead-8f05-ba6a868957b6"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 16.91 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "Checking out files: 100% (128/128), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 2.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 48.5 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 47.3 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 7.97 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 2.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.1 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 46.3 MB/s \n",
            "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449925 sha256=915453515f69141dcb9901c62d94585070b15aab1e0b51db24eb600edaf6a06f\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=203f67017add305cd7cb634afb477e68b42dc21e16848ba78853e28c05d2a984\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9445a0d-7449-40d1-9476-959cb8c13ece"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10085, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 10085 (delta 98), reused 130 (delta 58), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10085/10085), 14.95 MiB | 25.43 MiB/s, done.\n",
            "Resolving deltas: 100% (6903/6903), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-8rwq7uf0\n",
            "Created temporary directory: /tmp/pip-req-tracker-qhoyqkj4\n",
            "Initialized build tracking at /tmp/pip-req-tracker-qhoyqkj4\n",
            "Created build tracker: /tmp/pip-req-tracker-qhoyqkj4\n",
            "Entered build tracker: /tmp/pip-req-tracker-qhoyqkj4\n",
            "Created temporary directory: /tmp/pip-install-ou7ne1rg\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-4im6w0t0\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-qhoyqkj4'\n",
            "    Running setup.py (path:/tmp/pip-req-build-4im6w0t0/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-8ac51vf1\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8ac51vf1/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-4im6w0t0 has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-qhoyqkj4'\n",
            "Created temporary directory: /tmp/pip-unpack-qhqreldo\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-jwo8dnkb\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-jwo8dnkb\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-4im6w0t0/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-4im6w0t0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-jwo8dnkb\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-jwo8dnkb/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=74f9761f72b0edf794838a9a21862337102c92f14094d6a917b68020fde0f2aa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8rwq7uf0/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-qhoyqkj4'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06474b47-a325-4c3d-ab24-c4637e1f813e"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.47-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 20.6 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting botocore==1.27.47\n",
            "  Downloading botocore-1.27.47-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 35.3 MB/s \n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.47->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.47->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.47 botocore-1.27.47 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f0a082-d988-4ff2-c510-de9759391c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "id": "4xawOMn6icU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92a1e5f-39c6-48e8-ca15-1374e83f38da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 982, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 982 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (982/982), 251.89 MiB | 19.20 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1273/1273), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a62cc5-7076-47ab-b0a2-39e58bf4b015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/DMedium_10_3_5/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3373b70-bf0c-4261-e084-c6bcdd3295b6"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 422kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.52MB/s]\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 49.3MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5640740990638733\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4782608695652174, f1=0.4285714285714286, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5096163153648376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5263157894736842, f1=0.36363636363636365, best_f1=0.36363636363636365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5595760941505432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.5, f1=0.5925925925925927, best_f1=0.36363636363636365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2257394641637802\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.56, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25573891401290894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7999999999999999, f1=0.7333333333333334, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32583025097846985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7741935483870968, f1=0.8275862068965518, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12048496305942535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.888888888888889, f1=0.8275862068965518, best_f1=0.8275862068965518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08019805699586868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7333333333333334, f1=0.8571428571428571, best_f1=0.8275862068965518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004988952074199915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8571428571428571, f1=0.8461538461538461, best_f1=0.8275862068965518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016349472105503082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9032258064516129, f1=0.8484848484848484, best_f1=0.8484848484848484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006149995140731335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.896551724137931, f1=0.8275862068965518, best_f1=0.8484848484848484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006262524519115686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8750000000000001, f1=0.8, best_f1=0.8484848484848484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008986786007881165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8750000000000001, f1=0.8, best_f1=0.8484848484848484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005184875335544348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8750000000000001, f1=0.8, best_f1=0.8484848484848484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004854745231568813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8750000000000001, f1=0.8, best_f1=0.8484848484848484\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 112723.47it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7777777777777778\n",
            "real_f1 = 0.7777777777777778\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:15, 278.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f5747b-69fd-4c08-ea45-0587af1eb36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6228980422019958\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5863699913024902\n",
            "step: 20, loss: 0.30607908964157104\n",
            "step: 30, loss: 0.1094709187746048\n",
            "step: 40, loss: 0.3555612564086914\n",
            "step: 50, loss: 0.02586987242102623\n",
            "step: 60, loss: 0.04791754111647606\n",
            "step: 70, loss: 0.013000697828829288\n",
            "step: 80, loss: 0.07372020184993744\n",
            "step: 90, loss: 0.16054457426071167\n",
            "step: 100, loss: 0.005453249905258417\n",
            "step: 110, loss: 0.1510896533727646\n",
            "step: 120, loss: 0.015591434203088284\n",
            "step: 130, loss: 0.011086992919445038\n",
            "step: 140, loss: 0.0047528063878417015\n",
            "step: 150, loss: 0.05179345980286598\n",
            "step: 160, loss: 0.02156897820532322\n",
            "step: 170, loss: 0.07825727760791779\n",
            "step: 180, loss: 0.04700244590640068\n",
            "step: 190, loss: 0.021764077246189117\n",
            "step: 200, loss: 0.010646590031683445\n",
            "step: 210, loss: 0.0022854504641145468\n",
            "step: 220, loss: 0.0028285605367273092\n",
            "step: 230, loss: 0.07367614656686783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9808773903262092, f1=0.9761092150170648, best_f1=0.9761092150170648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011377292685210705\n",
            "step: 10, loss: 0.0011840376537293196\n",
            "step: 20, loss: 0.10645114630460739\n",
            "step: 30, loss: 0.17216558754444122\n",
            "step: 40, loss: 0.05012442544102669\n",
            "step: 50, loss: 0.01083909161388874\n",
            "step: 60, loss: 0.004339596722275019\n",
            "step: 70, loss: 0.25144267082214355\n",
            "step: 80, loss: 0.04766112193465233\n",
            "step: 90, loss: 0.015946106985211372\n",
            "step: 100, loss: 0.04882505163550377\n",
            "step: 110, loss: 0.004094211850315332\n",
            "step: 120, loss: 0.002087813103571534\n",
            "step: 130, loss: 0.10734773427248001\n",
            "step: 140, loss: 0.003047225996851921\n",
            "step: 150, loss: 0.03866511583328247\n",
            "step: 160, loss: 0.005615469068288803\n",
            "step: 170, loss: 0.0017206012271344662\n",
            "step: 180, loss: 0.002817912260070443\n",
            "step: 190, loss: 0.002804357325658202\n",
            "step: 200, loss: 0.006099838763475418\n",
            "step: 210, loss: 0.0009519438608549535\n",
            "step: 220, loss: 0.04516240581870079\n",
            "step: 230, loss: 0.0017594879027456045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9887387387387387, f1=0.9809203142536477, best_f1=0.9809203142536477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010605240240693092\n",
            "step: 10, loss: 0.07387708127498627\n",
            "step: 20, loss: 0.00078320357715711\n",
            "step: 30, loss: 0.006043223664164543\n",
            "step: 40, loss: 0.14414824545383453\n",
            "step: 50, loss: 0.005000881850719452\n",
            "step: 60, loss: 0.0025397534482181072\n",
            "step: 70, loss: 0.0048161037266254425\n",
            "step: 80, loss: 0.00821332074701786\n",
            "step: 90, loss: 0.08766929060220718\n",
            "step: 100, loss: 0.002490755869075656\n",
            "step: 110, loss: 0.003835116745904088\n",
            "step: 120, loss: 0.0039279223419725895\n",
            "step: 130, loss: 0.0007320567383430898\n",
            "step: 140, loss: 0.010188527405261993\n",
            "step: 150, loss: 0.022683117538690567\n",
            "step: 160, loss: 0.02370462194085121\n",
            "step: 170, loss: 0.00730894086882472\n",
            "step: 180, loss: 0.03371215984225273\n",
            "step: 190, loss: 0.004540520720183849\n",
            "step: 200, loss: 0.006955225020647049\n",
            "step: 210, loss: 0.0024727811105549335\n",
            "step: 220, loss: 0.0006486361380666494\n",
            "step: 230, loss: 0.0006984625360928476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9898762654668166, f1=0.9819004524886877, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015741082606837153\n",
            "step: 10, loss: 0.0004965653060935438\n",
            "step: 20, loss: 0.0005792625015601516\n",
            "step: 30, loss: 0.0007146396674215794\n",
            "step: 40, loss: 0.11712441593408585\n",
            "step: 50, loss: 0.0009360643452964723\n",
            "step: 60, loss: 0.002142357872799039\n",
            "step: 70, loss: 0.00047619215911254287\n",
            "step: 80, loss: 0.0021774331107735634\n",
            "step: 90, loss: 0.0012047667987644672\n",
            "step: 100, loss: 0.0006502058240585029\n",
            "step: 110, loss: 0.0014660254819318652\n",
            "step: 120, loss: 0.06659480929374695\n",
            "step: 130, loss: 0.0178818441927433\n",
            "step: 140, loss: 0.021686818450689316\n",
            "step: 150, loss: 0.11093389987945557\n",
            "step: 160, loss: 0.05681394785642624\n",
            "step: 170, loss: 0.012401141226291656\n",
            "step: 180, loss: 0.000998098636046052\n",
            "step: 190, loss: 0.005468618590384722\n",
            "step: 200, loss: 0.0018905228935182095\n",
            "step: 210, loss: 0.16360729932785034\n",
            "step: 220, loss: 0.0005826420383527875\n",
            "step: 230, loss: 0.08657993376255035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9853768278965129, f1=0.979591836734694, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007089207065291703\n",
            "step: 10, loss: 0.0005815931945107877\n",
            "step: 20, loss: 0.0019848623778671026\n",
            "step: 30, loss: 0.0003410937206353992\n",
            "step: 40, loss: 0.0007139823865145445\n",
            "step: 50, loss: 0.0013432074338197708\n",
            "step: 60, loss: 0.015767551958560944\n",
            "step: 70, loss: 0.0018171954434365034\n",
            "step: 80, loss: 0.0006604054942727089\n",
            "step: 90, loss: 0.0016027081292122602\n",
            "step: 100, loss: 0.015156090259552002\n",
            "step: 110, loss: 0.00031482859048992395\n",
            "step: 120, loss: 0.00014477409422397614\n",
            "step: 130, loss: 0.0011760396882891655\n",
            "step: 140, loss: 0.003026468912139535\n",
            "step: 150, loss: 0.0019346874905750155\n",
            "step: 160, loss: 0.000435057794675231\n",
            "step: 170, loss: 0.008343076333403587\n",
            "step: 180, loss: 0.0007998895598575473\n",
            "step: 190, loss: 0.13563895225524902\n",
            "step: 200, loss: 0.0036242324858903885\n",
            "step: 210, loss: 0.0011066069127991796\n",
            "step: 220, loss: 0.0031985114328563213\n",
            "step: 230, loss: 0.0005012442707084119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9876543209876544, f1=0.9796839729119639, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005576357361860573\n",
            "step: 10, loss: 0.0005351788713596761\n",
            "step: 20, loss: 0.002163889352232218\n",
            "step: 30, loss: 0.0005643994663842022\n",
            "step: 40, loss: 0.0063793552108109\n",
            "step: 50, loss: 0.0007198374369181693\n",
            "step: 60, loss: 0.00026824252563528717\n",
            "step: 70, loss: 0.0013319378485903144\n",
            "step: 80, loss: 0.0008599652210250497\n",
            "step: 90, loss: 0.0003324495046399534\n",
            "step: 100, loss: 0.040858104825019836\n",
            "step: 110, loss: 0.00034865227644331753\n",
            "step: 120, loss: 0.0002696793235372752\n",
            "step: 130, loss: 0.0020672152750194073\n",
            "step: 140, loss: 0.003127743722870946\n",
            "step: 150, loss: 0.0002121811849065125\n",
            "step: 160, loss: 0.002769381506368518\n",
            "step: 170, loss: 0.0002846438728738576\n",
            "step: 180, loss: 0.10501813143491745\n",
            "step: 190, loss: 0.002540266839787364\n",
            "step: 200, loss: 0.007461454253643751\n",
            "step: 210, loss: 0.0010275250533595681\n",
            "step: 220, loss: 0.0003874943940900266\n",
            "step: 230, loss: 0.05343835800886154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9865168539325843, f1=0.9784335981838819, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004498402122408152\n",
            "step: 10, loss: 0.0015145970974117517\n",
            "step: 20, loss: 0.00022525583335664123\n",
            "step: 30, loss: 0.0002746133250184357\n",
            "step: 40, loss: 0.00016437664453405887\n",
            "step: 50, loss: 0.0002633488620631397\n",
            "step: 60, loss: 0.009661387652158737\n",
            "step: 70, loss: 0.020703161135315895\n",
            "step: 80, loss: 0.00013684276200365275\n",
            "step: 90, loss: 8.224636258091778e-05\n",
            "step: 100, loss: 0.00013390240201260895\n",
            "step: 110, loss: 0.0002596180129330605\n",
            "step: 120, loss: 0.00018831485067494214\n",
            "step: 130, loss: 0.0006743965204805136\n",
            "step: 140, loss: 0.00020107897580601275\n",
            "step: 150, loss: 0.006872196681797504\n",
            "step: 160, loss: 0.07121862471103668\n",
            "step: 170, loss: 0.004555300809442997\n",
            "step: 180, loss: 0.0023133528884500265\n",
            "step: 190, loss: 0.00020077158114872873\n",
            "step: 200, loss: 0.04413405805826187\n",
            "step: 210, loss: 7.450749399140477e-05\n",
            "step: 220, loss: 0.008629337884485722\n",
            "step: 230, loss: 0.005541052203625441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9887387387387387, f1=0.9875706214689265, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015455052198376507\n",
            "step: 10, loss: 0.000242917041759938\n",
            "step: 20, loss: 8.816029730951414e-05\n",
            "step: 30, loss: 0.0009819166734814644\n",
            "step: 40, loss: 0.00025413575349375606\n",
            "step: 50, loss: 0.0023024824913591146\n",
            "step: 60, loss: 0.00011177505075465888\n",
            "step: 70, loss: 0.0001902145304484293\n",
            "step: 80, loss: 0.0003281212120782584\n",
            "step: 90, loss: 4.707573680207133e-05\n",
            "step: 100, loss: 0.0001813489943742752\n",
            "step: 110, loss: 0.005771099589765072\n",
            "step: 120, loss: 0.00015660320059396327\n",
            "step: 130, loss: 0.0006481692544184625\n",
            "step: 140, loss: 7.078477938193828e-05\n",
            "step: 150, loss: 0.00013676160597242415\n",
            "step: 160, loss: 0.0001546582643641159\n",
            "step: 170, loss: 7.767870556563139e-05\n",
            "step: 180, loss: 7.74846485001035e-05\n",
            "step: 190, loss: 8.374649769393727e-05\n",
            "step: 200, loss: 0.00019476505985949188\n",
            "step: 210, loss: 0.0001158034210675396\n",
            "step: 220, loss: 0.00036972720408812165\n",
            "step: 230, loss: 0.0007705431198701262\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9864864864864865, f1=0.9830124575311437, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.337574010714889e-05\n",
            "step: 10, loss: 0.001028841477818787\n",
            "step: 20, loss: 0.0002316754253115505\n",
            "step: 30, loss: 5.8120200264966115e-05\n",
            "step: 40, loss: 0.07008083909749985\n",
            "step: 50, loss: 6.0914462665095925e-05\n",
            "step: 60, loss: 7.242970605148003e-05\n",
            "step: 70, loss: 0.00019373033137526363\n",
            "step: 80, loss: 6.498175935121253e-05\n",
            "step: 90, loss: 7.786645437590778e-05\n",
            "step: 100, loss: 0.00030130412778817117\n",
            "step: 110, loss: 4.996409188606776e-05\n",
            "step: 120, loss: 4.2790699808392674e-05\n",
            "step: 130, loss: 8.721372432773933e-05\n",
            "step: 140, loss: 4.358742808108218e-05\n",
            "step: 150, loss: 0.0008197219576686621\n",
            "step: 160, loss: 0.00010616983490763232\n",
            "step: 170, loss: 0.00021400328841991723\n",
            "step: 180, loss: 0.003443666733801365\n",
            "step: 190, loss: 0.00020360626513138413\n",
            "step: 200, loss: 6.101193866925314e-05\n",
            "step: 210, loss: 0.0014394031604751945\n",
            "step: 220, loss: 5.876241993973963e-05\n",
            "step: 230, loss: 0.0001449597766622901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.987598647125141, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012863671872764826\n",
            "step: 10, loss: 0.0001271922083105892\n",
            "step: 20, loss: 0.00039408382144756615\n",
            "step: 30, loss: 9.804198634810746e-05\n",
            "step: 40, loss: 6.662847590632737e-05\n",
            "step: 50, loss: 0.00019493498257361352\n",
            "step: 60, loss: 0.0005249571986496449\n",
            "step: 70, loss: 0.00023090967442840338\n",
            "step: 80, loss: 6.589612166862935e-05\n",
            "step: 90, loss: 0.00021688378183171153\n",
            "step: 100, loss: 6.994591967668384e-05\n",
            "step: 110, loss: 5.671608232660219e-05\n",
            "step: 120, loss: 0.00013184110866859555\n",
            "step: 130, loss: 6.306399154709652e-05\n",
            "step: 140, loss: 0.026478173211216927\n",
            "step: 150, loss: 0.015274678356945515\n",
            "step: 160, loss: 2.8855340133304708e-05\n",
            "step: 170, loss: 7.933800225146115e-05\n",
            "step: 180, loss: 0.00010248775652144104\n",
            "step: 190, loss: 0.033564191311597824\n",
            "step: 200, loss: 0.00019520451314747334\n",
            "step: 210, loss: 7.315121911233291e-05\n",
            "step: 220, loss: 5.5894724937388673e-05\n",
            "step: 230, loss: 8.545270247850567e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.987598647125141, f1=0.9818181818181818, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.794292079983279e-05\n",
            "step: 10, loss: 6.07870751991868e-05\n",
            "step: 20, loss: 0.0023742879275232553\n",
            "step: 30, loss: 0.020979158580303192\n",
            "step: 40, loss: 6.362090061884373e-05\n",
            "step: 50, loss: 0.00011377176269888878\n",
            "step: 60, loss: 8.013677143026143e-05\n",
            "step: 70, loss: 9.556925215292722e-05\n",
            "step: 80, loss: 4.305018956074491e-05\n",
            "step: 90, loss: 3.9939946873346344e-05\n",
            "step: 100, loss: 4.580301174428314e-05\n",
            "step: 110, loss: 0.012285598553717136\n",
            "step: 120, loss: 2.9156992241041735e-05\n",
            "step: 130, loss: 3.540876059560105e-05\n",
            "step: 140, loss: 5.396997948992066e-05\n",
            "step: 150, loss: 0.03156280145049095\n",
            "step: 160, loss: 3.692729296744801e-05\n",
            "step: 170, loss: 0.024380775168538094\n",
            "step: 180, loss: 4.450655251275748e-05\n",
            "step: 190, loss: 4.729364809463732e-05\n",
            "step: 200, loss: 0.0005974808591417968\n",
            "step: 210, loss: 2.7365293135517277e-05\n",
            "step: 220, loss: 5.19970380992163e-05\n",
            "step: 230, loss: 9.147506352746859e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9887387387387387, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.0905753849074244e-05\n",
            "step: 10, loss: 3.7989066186128184e-05\n",
            "step: 20, loss: 3.6617777368519455e-05\n",
            "step: 30, loss: 0.00017392009613104165\n",
            "step: 40, loss: 0.00015063695900607854\n",
            "step: 50, loss: 0.00010349326475989074\n",
            "step: 60, loss: 0.0001330486120423302\n",
            "step: 70, loss: 5.1678973250091076e-05\n",
            "step: 80, loss: 8.281988993985578e-05\n",
            "step: 90, loss: 3.450248914305121e-05\n",
            "step: 100, loss: 2.598698847577907e-05\n",
            "step: 110, loss: 3.962780465371907e-05\n",
            "step: 120, loss: 3.736672078957781e-05\n",
            "step: 130, loss: 3.91217581636738e-05\n",
            "step: 140, loss: 0.017129182815551758\n",
            "step: 150, loss: 0.00012604934454429895\n",
            "step: 160, loss: 3.472231037449092e-05\n",
            "step: 170, loss: 3.9508871850557625e-05\n",
            "step: 180, loss: 0.021861644461750984\n",
            "step: 190, loss: 0.00010258064139634371\n",
            "step: 200, loss: 2.1751502572442405e-05\n",
            "step: 210, loss: 3.4025673812720925e-05\n",
            "step: 220, loss: 3.743345951079391e-05\n",
            "step: 230, loss: 0.02278188243508339\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9887387387387387, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.53065391513519e-05\n",
            "step: 10, loss: 9.440714347874746e-05\n",
            "step: 20, loss: 7.388195808744058e-05\n",
            "step: 30, loss: 4.8880010581342503e-05\n",
            "step: 40, loss: 5.199880979489535e-05\n",
            "step: 50, loss: 3.8436137401731685e-05\n",
            "step: 60, loss: 4.274185994290747e-05\n",
            "step: 70, loss: 4.235434971633367e-05\n",
            "step: 80, loss: 8.503066055709496e-05\n",
            "step: 90, loss: 4.052539588883519e-05\n",
            "step: 100, loss: 3.7286372389644384e-05\n",
            "step: 110, loss: 0.023902863264083862\n",
            "step: 120, loss: 0.03346480056643486\n",
            "step: 130, loss: 7.683052535867319e-05\n",
            "step: 140, loss: 4.259293564246036e-05\n",
            "step: 150, loss: 5.564148159464821e-05\n",
            "step: 160, loss: 6.737699004588649e-05\n",
            "step: 170, loss: 7.51329425838776e-05\n",
            "step: 180, loss: 0.00010095291509060189\n",
            "step: 190, loss: 0.00015187131066340953\n",
            "step: 200, loss: 8.511371561326087e-05\n",
            "step: 210, loss: 2.5104063752223738e-05\n",
            "step: 220, loss: 6.273540202528238e-05\n",
            "step: 230, loss: 0.0003457744314800948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.987598647125141, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.510568058118224e-05\n",
            "step: 10, loss: 0.015102985315024853\n",
            "step: 20, loss: 7.897005707491189e-05\n",
            "step: 30, loss: 0.00011271794210188091\n",
            "step: 40, loss: 0.0002492136263754219\n",
            "step: 50, loss: 4.639626058633439e-05\n",
            "step: 60, loss: 5.198983126319945e-05\n",
            "step: 70, loss: 3.875985566992313e-05\n",
            "step: 80, loss: 4.036937389173545e-05\n",
            "step: 90, loss: 5.72421595279593e-05\n",
            "step: 100, loss: 3.987680975114927e-05\n",
            "step: 110, loss: 0.00010532675514696166\n",
            "step: 120, loss: 2.420635792077519e-05\n",
            "step: 130, loss: 4.38632741861511e-05\n",
            "step: 140, loss: 0.00016585046250838786\n",
            "step: 150, loss: 4.641794294002466e-05\n",
            "step: 160, loss: 0.004285008180886507\n",
            "step: 170, loss: 2.4955177650554106e-05\n",
            "step: 180, loss: 0.00017087427841033787\n",
            "step: 190, loss: 3.88455628126394e-05\n",
            "step: 200, loss: 3.985387593274936e-05\n",
            "step: 210, loss: 6.056418351363391e-05\n",
            "step: 220, loss: 6.788458995288238e-05\n",
            "step: 230, loss: 9.42218757700175e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.987598647125141, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.871711694751866e-05\n",
            "step: 10, loss: 2.236241562059149e-05\n",
            "step: 20, loss: 5.321148273651488e-05\n",
            "step: 30, loss: 8.611248631495982e-05\n",
            "step: 40, loss: 8.011708268895745e-05\n",
            "step: 50, loss: 0.048464108258485794\n",
            "step: 60, loss: 0.00011985125456703827\n",
            "step: 70, loss: 0.0002250395336886868\n",
            "step: 80, loss: 4.478227856452577e-05\n",
            "step: 90, loss: 5.2095023420406505e-05\n",
            "step: 100, loss: 5.3555799240712076e-05\n",
            "step: 110, loss: 3.7138313928153366e-05\n",
            "step: 120, loss: 6.805254088249058e-05\n",
            "step: 130, loss: 0.024560105055570602\n",
            "step: 140, loss: 2.8251866751816124e-05\n",
            "step: 150, loss: 0.00011864562111441046\n",
            "step: 160, loss: 3.314972491352819e-05\n",
            "step: 170, loss: 3.010279397130944e-05\n",
            "step: 180, loss: 0.00011744382936740294\n",
            "step: 190, loss: 8.522695861756802e-05\n",
            "step: 200, loss: 5.297021198202856e-05\n",
            "step: 210, loss: 7.309284410439432e-05\n",
            "step: 220, loss: 7.567807915620506e-05\n",
            "step: 230, loss: 4.058438935317099e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9886877828054299, f1=0.9807037457434733, best_f1=0.9819004524886877\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 211.82it/s]\n",
            "load_f1 = 0.9876819708846584\n",
            "real_f1 = 0.9887640449438202\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 261.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f692be-609d-4ac3-fbdc-318d382a416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 370kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 256kB/s] \n",
            "Downloading: 100% 440M/440M [00:07<00:00, 60.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6126914024353027\n",
            "step: 10, loss: 0.5402677655220032\n",
            "step: 20, loss: 0.468113511800766\n",
            "step: 30, loss: 0.04900440573692322\n",
            "step: 40, loss: 0.10789372026920319\n",
            "step: 50, loss: 0.18530017137527466\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.03122362308204174\n",
            "step: 70, loss: 0.1570146530866623\n",
            "step: 80, loss: 0.0830511674284935\n",
            "step: 90, loss: 0.23337377607822418\n",
            "step: 100, loss: 0.02696705237030983\n",
            "step: 110, loss: 0.06327388435602188\n",
            "step: 120, loss: 0.05342787131667137\n",
            "step: 130, loss: 0.10768258571624756\n",
            "step: 140, loss: 0.03528009355068207\n",
            "step: 150, loss: 0.031661272048950195\n",
            "step: 160, loss: 0.016096582636237144\n",
            "step: 170, loss: 0.1347726583480835\n",
            "step: 180, loss: 0.1444343626499176\n",
            "step: 190, loss: 0.01397034153342247\n",
            "step: 200, loss: 0.13563118875026703\n",
            "step: 210, loss: 0.18261854350566864\n",
            "step: 220, loss: 0.23281723260879517\n",
            "step: 230, loss: 0.14403918385505676\n",
            "step: 240, loss: 0.061004023998975754\n",
            "step: 250, loss: 0.023994604125618935\n",
            "step: 260, loss: 0.0384732224047184\n",
            "step: 270, loss: 0.030562521889805794\n",
            "step: 280, loss: 0.06900054961442947\n",
            "step: 290, loss: 0.21588750183582306\n",
            "step: 300, loss: 0.05907373130321503\n",
            "step: 310, loss: 0.3186620771884918\n",
            "step: 320, loss: 0.10006927698850632\n",
            "step: 330, loss: 0.026922035962343216\n",
            "step: 340, loss: 0.03725241869688034\n",
            "step: 350, loss: 0.010210404172539711\n",
            "step: 360, loss: 0.0167580246925354\n",
            "step: 370, loss: 0.037335287779569626\n",
            "step: 380, loss: 0.014531533233821392\n",
            "step: 390, loss: 0.0943903774023056\n",
            "step: 400, loss: 0.19441036880016327\n",
            "step: 410, loss: 0.06568671762943268\n",
            "step: 420, loss: 0.018938174471259117\n",
            "step: 430, loss: 0.16228154301643372\n",
            "step: 440, loss: 0.019714433699846268\n",
            "step: 450, loss: 0.0034755882807075977\n",
            "step: 460, loss: 0.004007184877991676\n",
            "step: 470, loss: 0.13278087973594666\n",
            "step: 480, loss: 0.026872573420405388\n",
            "step: 490, loss: 0.04736391082406044\n",
            "step: 500, loss: 0.05719708651304245\n",
            "step: 510, loss: 0.07511918991804123\n",
            "step: 520, loss: 0.03014962747693062\n",
            "step: 530, loss: 0.0027137678116559982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9290681502086231, f1=0.9308755760368663, best_f1=0.9308755760368663\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16470134258270264\n",
            "step: 10, loss: 0.10433731228113174\n",
            "step: 20, loss: 0.00901168305426836\n",
            "step: 30, loss: 0.004355961456894875\n",
            "step: 40, loss: 0.05468469113111496\n",
            "step: 50, loss: 0.12420389801263809\n",
            "step: 60, loss: 0.02467755787074566\n",
            "step: 70, loss: 0.03254658356308937\n",
            "step: 80, loss: 0.06583093106746674\n",
            "step: 90, loss: 0.009970592334866524\n",
            "step: 100, loss: 0.005523313768208027\n",
            "step: 110, loss: 0.0066283163614571095\n",
            "step: 120, loss: 0.1602526158094406\n",
            "step: 130, loss: 0.13755853474140167\n",
            "step: 140, loss: 0.00808892771601677\n",
            "step: 150, loss: 0.05775017663836479\n",
            "step: 160, loss: 0.008292906917631626\n",
            "step: 170, loss: 0.011437583714723587\n",
            "step: 180, loss: 0.030482245609164238\n",
            "step: 190, loss: 0.05791756138205528\n",
            "step: 200, loss: 0.019875988364219666\n",
            "step: 210, loss: 0.06949193775653839\n",
            "step: 220, loss: 0.13286960124969482\n",
            "step: 230, loss: 0.018204577267169952\n",
            "step: 240, loss: 0.11544477194547653\n",
            "step: 250, loss: 0.048200685530900955\n",
            "step: 260, loss: 0.005207365844398737\n",
            "step: 270, loss: 0.20782440900802612\n",
            "step: 280, loss: 0.02910631336271763\n",
            "step: 290, loss: 0.025183653458952904\n",
            "step: 300, loss: 0.1516650915145874\n",
            "step: 310, loss: 0.016077708452939987\n",
            "step: 320, loss: 0.12677335739135742\n",
            "step: 330, loss: 0.08828000724315643\n",
            "step: 340, loss: 0.006435933988541365\n",
            "step: 350, loss: 0.0029837889596819878\n",
            "step: 360, loss: 0.10500706732273102\n",
            "step: 370, loss: 0.14313173294067383\n",
            "step: 380, loss: 0.07010339945554733\n",
            "step: 390, loss: 0.04659273475408554\n",
            "step: 400, loss: 0.04559575766324997\n",
            "step: 410, loss: 0.007899819873273373\n",
            "step: 420, loss: 0.045738041400909424\n",
            "step: 430, loss: 0.010459942743182182\n",
            "step: 440, loss: 0.29136115312576294\n",
            "step: 450, loss: 0.06278116255998611\n",
            "step: 460, loss: 0.008417867124080658\n",
            "step: 470, loss: 0.048007313162088394\n",
            "step: 480, loss: 0.23535068333148956\n",
            "step: 490, loss: 0.022634508088231087\n",
            "step: 500, loss: 0.22416643798351288\n",
            "step: 510, loss: 0.0141258854418993\n",
            "step: 520, loss: 0.044912390410900116\n",
            "step: 530, loss: 0.0026600584387779236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9427516158818098, f1=0.9358974358974358, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19042807817459106\n",
            "step: 10, loss: 0.10025180876255035\n",
            "step: 20, loss: 0.07881336659193039\n",
            "step: 30, loss: 0.11123448610305786\n",
            "step: 40, loss: 0.014192407950758934\n",
            "step: 50, loss: 0.10941998660564423\n",
            "step: 60, loss: 0.05134594812989235\n",
            "step: 70, loss: 0.002770595485344529\n",
            "step: 80, loss: 0.002983263460919261\n",
            "step: 90, loss: 0.013311812654137611\n",
            "step: 100, loss: 0.021001363173127174\n",
            "step: 110, loss: 0.001544963219203055\n",
            "step: 120, loss: 0.002375350333750248\n",
            "step: 130, loss: 0.005271228961646557\n",
            "step: 140, loss: 0.07630261778831482\n",
            "step: 150, loss: 0.01743313856422901\n",
            "step: 160, loss: 0.005410072859376669\n",
            "step: 170, loss: 0.07347887009382248\n",
            "step: 180, loss: 0.007170042954385281\n",
            "step: 190, loss: 0.006059164647012949\n",
            "step: 200, loss: 0.004919389262795448\n",
            "step: 210, loss: 0.0854024812579155\n",
            "step: 220, loss: 0.12235096096992493\n",
            "step: 230, loss: 0.19792912900447845\n",
            "step: 240, loss: 0.009513192810118198\n",
            "step: 250, loss: 0.0349949486553669\n",
            "step: 260, loss: 0.01756730116903782\n",
            "step: 270, loss: 0.009405415505170822\n",
            "step: 280, loss: 0.20107115805149078\n",
            "step: 290, loss: 0.002050023525953293\n",
            "step: 300, loss: 0.0289093516767025\n",
            "step: 310, loss: 0.010493369773030281\n",
            "step: 320, loss: 0.006351838819682598\n",
            "step: 330, loss: 0.0011608557542786002\n",
            "step: 340, loss: 0.00307755870744586\n",
            "step: 350, loss: 0.0020633742678910494\n",
            "step: 360, loss: 0.027407029643654823\n",
            "step: 370, loss: 0.0206987876445055\n",
            "step: 380, loss: 0.046781327575445175\n",
            "step: 390, loss: 0.005782168358564377\n",
            "step: 400, loss: 0.05122540518641472\n",
            "step: 410, loss: 0.001871950109489262\n",
            "step: 420, loss: 0.20396870374679565\n",
            "step: 430, loss: 0.01149191614240408\n",
            "step: 440, loss: 0.004463013727217913\n",
            "step: 450, loss: 0.04572093486785889\n",
            "step: 460, loss: 0.03594893962144852\n",
            "step: 470, loss: 0.12075477093458176\n",
            "step: 480, loss: 0.003158523002639413\n",
            "step: 490, loss: 0.007328635547310114\n",
            "step: 500, loss: 0.004236229229718447\n",
            "step: 510, loss: 0.00239096162840724\n",
            "step: 520, loss: 0.15758922696113586\n",
            "step: 530, loss: 0.022772926837205887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9424964936886395, f1=0.9298653042266605, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00439460389316082\n",
            "step: 10, loss: 0.006696674972772598\n",
            "step: 20, loss: 0.004430629778653383\n",
            "step: 30, loss: 0.0034219662193208933\n",
            "step: 40, loss: 0.008619488216936588\n",
            "step: 50, loss: 0.004716451279819012\n",
            "step: 60, loss: 0.001704710186459124\n",
            "step: 70, loss: 0.0012918857391923666\n",
            "step: 80, loss: 0.026410652324557304\n",
            "step: 90, loss: 0.04263043776154518\n",
            "step: 100, loss: 0.011339865624904633\n",
            "step: 110, loss: 0.035997211933135986\n",
            "step: 120, loss: 0.0003051131498068571\n",
            "step: 130, loss: 0.005666709039360285\n",
            "step: 140, loss: 0.004689618945121765\n",
            "step: 150, loss: 0.0013733998639509082\n",
            "step: 160, loss: 0.0017052431358024478\n",
            "step: 170, loss: 0.0005983520532026887\n",
            "step: 180, loss: 0.028864873573184013\n",
            "step: 190, loss: 0.049972668290138245\n",
            "step: 200, loss: 0.007843556813895702\n",
            "step: 210, loss: 0.02965567074716091\n",
            "step: 220, loss: 0.0022311238572001457\n",
            "step: 230, loss: 0.05489347130060196\n",
            "step: 240, loss: 0.0027021223213523626\n",
            "step: 250, loss: 0.047352537512779236\n",
            "step: 260, loss: 0.0015182873466983438\n",
            "step: 270, loss: 0.01174167636781931\n",
            "step: 280, loss: 0.032747361809015274\n",
            "step: 290, loss: 0.023130401968955994\n",
            "step: 300, loss: 0.00114855554420501\n",
            "step: 310, loss: 0.002995306858792901\n",
            "step: 320, loss: 0.0007468751864507794\n",
            "step: 330, loss: 0.0024861767888069153\n",
            "step: 340, loss: 0.013651836663484573\n",
            "step: 350, loss: 0.08907318860292435\n",
            "step: 360, loss: 0.07902108877897263\n",
            "step: 370, loss: 0.009764429181814194\n",
            "step: 380, loss: 0.003113477723672986\n",
            "step: 390, loss: 0.0003579351759981364\n",
            "step: 400, loss: 0.02502894215285778\n",
            "step: 410, loss: 0.0003441376320552081\n",
            "step: 420, loss: 0.010035932995378971\n",
            "step: 430, loss: 0.04025642201304436\n",
            "step: 440, loss: 0.018507469445466995\n",
            "step: 450, loss: 0.012015493586659431\n",
            "step: 460, loss: 0.06231765076518059\n",
            "step: 470, loss: 0.0630878433585167\n",
            "step: 480, loss: 0.0933866873383522\n",
            "step: 490, loss: 0.04368791729211807\n",
            "step: 500, loss: 0.015693917870521545\n",
            "step: 510, loss: 0.006034651305526495\n",
            "step: 520, loss: 0.022187838330864906\n",
            "step: 530, loss: 0.020674560219049454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9417206290471785, f1=0.9385321100917432, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027133313938975334\n",
            "step: 10, loss: 0.0501541793346405\n",
            "step: 20, loss: 0.014446211978793144\n",
            "step: 30, loss: 0.014249171130359173\n",
            "step: 40, loss: 0.0030805328860878944\n",
            "step: 50, loss: 0.0022533147130161524\n",
            "step: 60, loss: 0.028709422796964645\n",
            "step: 70, loss: 0.0037158646155148745\n",
            "step: 80, loss: 0.0012136220466345549\n",
            "step: 90, loss: 0.001561089651659131\n",
            "step: 100, loss: 0.023567968979477882\n",
            "step: 110, loss: 0.00029149098554626107\n",
            "step: 120, loss: 0.002623210661113262\n",
            "step: 130, loss: 0.0016026379307731986\n",
            "step: 140, loss: 0.0006484212353825569\n",
            "step: 150, loss: 0.004117886070162058\n",
            "step: 160, loss: 0.00048345542745664716\n",
            "step: 170, loss: 0.014032945968210697\n",
            "step: 180, loss: 0.0003970974648837\n",
            "step: 190, loss: 0.007056704722344875\n",
            "step: 200, loss: 0.012542922049760818\n",
            "step: 210, loss: 0.021248135715723038\n",
            "step: 220, loss: 0.0014264601049944758\n",
            "step: 230, loss: 0.007729977369308472\n",
            "step: 240, loss: 0.0030954862013459206\n",
            "step: 250, loss: 0.002454508328810334\n",
            "step: 260, loss: 0.007502687629312277\n",
            "step: 270, loss: 0.0006018176209181547\n",
            "step: 280, loss: 0.0007609904860146344\n",
            "step: 290, loss: 0.031175540760159492\n",
            "step: 300, loss: 0.00616080779582262\n",
            "step: 310, loss: 0.0013811326352879405\n",
            "step: 320, loss: 0.062115151435136795\n",
            "step: 330, loss: 0.019157573580741882\n",
            "step: 340, loss: 0.003741256892681122\n",
            "step: 350, loss: 0.00561300665140152\n",
            "step: 360, loss: 0.010348053649067879\n",
            "step: 370, loss: 0.027382491156458855\n",
            "step: 380, loss: 0.0019198922673240304\n",
            "step: 390, loss: 0.0001955386105692014\n",
            "step: 400, loss: 0.0015488393837586045\n",
            "step: 410, loss: 0.004681967664510012\n",
            "step: 420, loss: 0.008733535185456276\n",
            "step: 430, loss: 0.0008863236871547997\n",
            "step: 440, loss: 0.06002369523048401\n",
            "step: 450, loss: 0.011215785518288612\n",
            "step: 460, loss: 0.0005613012472167611\n",
            "step: 470, loss: 0.02108694612979889\n",
            "step: 480, loss: 0.00033846774022094905\n",
            "step: 490, loss: 0.0002605675545055419\n",
            "step: 500, loss: 0.00107770471367985\n",
            "step: 510, loss: 0.03559482470154762\n",
            "step: 520, loss: 0.03853047266602516\n",
            "step: 530, loss: 0.0005707246600650251\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9339578454332552, f1=0.9286376274328081, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00947534292936325\n",
            "step: 10, loss: 0.004601853899657726\n",
            "step: 20, loss: 0.06074771285057068\n",
            "step: 30, loss: 0.0010695800883695483\n",
            "step: 40, loss: 0.0006468745414167643\n",
            "step: 50, loss: 0.033441949635744095\n",
            "step: 60, loss: 0.0012812333879992366\n",
            "step: 70, loss: 0.0009619094198569655\n",
            "step: 80, loss: 0.004524452146142721\n",
            "step: 90, loss: 0.005964800715446472\n",
            "step: 100, loss: 0.0018746725982055068\n",
            "step: 110, loss: 0.024855200201272964\n",
            "step: 120, loss: 0.000998830539174378\n",
            "step: 130, loss: 0.004614308010786772\n",
            "step: 140, loss: 0.0019042800413444638\n",
            "step: 150, loss: 0.0003633900487329811\n",
            "step: 160, loss: 0.004733152221888304\n",
            "step: 170, loss: 0.0008646278292872012\n",
            "step: 180, loss: 0.005259213037788868\n",
            "step: 190, loss: 0.0003430464712437242\n",
            "step: 200, loss: 0.0050543793477118015\n",
            "step: 210, loss: 0.0044583831913769245\n",
            "step: 220, loss: 0.011813939549028873\n",
            "step: 230, loss: 0.008972346782684326\n",
            "step: 240, loss: 0.02715512178838253\n",
            "step: 250, loss: 0.0003594631270971149\n",
            "step: 260, loss: 0.0002840524248313159\n",
            "step: 270, loss: 0.064564548432827\n",
            "step: 280, loss: 0.0017310590483248234\n",
            "step: 290, loss: 0.0005452170153148472\n",
            "step: 300, loss: 0.021494299173355103\n",
            "step: 310, loss: 0.009627155028283596\n",
            "step: 320, loss: 0.0022081234492361546\n",
            "step: 330, loss: 0.04672866687178612\n",
            "step: 340, loss: 0.0820426344871521\n",
            "step: 350, loss: 0.00475970096886158\n",
            "step: 360, loss: 0.018384601920843124\n",
            "step: 370, loss: 0.0009163650684058666\n",
            "step: 380, loss: 0.00041851852438412607\n",
            "step: 390, loss: 0.015254190191626549\n",
            "step: 400, loss: 5.429358498076908e-05\n",
            "step: 410, loss: 0.0002892209740821272\n",
            "step: 420, loss: 0.0036695031449198723\n",
            "step: 430, loss: 0.00016828252410050482\n",
            "step: 440, loss: 0.00039342796662822366\n",
            "step: 450, loss: 0.0005035824724473059\n",
            "step: 460, loss: 0.00028734520310536027\n",
            "step: 470, loss: 0.002348958281800151\n",
            "step: 480, loss: 0.002258558291941881\n",
            "step: 490, loss: 0.007482689339667559\n",
            "step: 500, loss: 0.0032315445132553577\n",
            "step: 510, loss: 0.0009451280348002911\n",
            "step: 520, loss: 0.0012233753222972155\n",
            "step: 530, loss: 0.09461146593093872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9348837209302325, f1=0.9293954776188279, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007734455284662545\n",
            "step: 10, loss: 0.00023815319582354277\n",
            "step: 20, loss: 0.0001328185317106545\n",
            "step: 30, loss: 0.002177858492359519\n",
            "step: 40, loss: 0.0005235529388301075\n",
            "step: 50, loss: 0.0033789328299462795\n",
            "step: 60, loss: 0.0005265395739115775\n",
            "step: 70, loss: 0.03690299391746521\n",
            "step: 80, loss: 0.10081176459789276\n",
            "step: 90, loss: 0.0006870166980661452\n",
            "step: 100, loss: 0.0004206752637401223\n",
            "step: 110, loss: 0.0016060739289969206\n",
            "step: 120, loss: 0.000213076506042853\n",
            "step: 130, loss: 0.016521550714969635\n",
            "step: 140, loss: 0.027473904192447662\n",
            "step: 150, loss: 0.00021506792108993977\n",
            "step: 160, loss: 5.4203366744332016e-05\n",
            "step: 170, loss: 0.0005073924548923969\n",
            "step: 180, loss: 9.357331873616204e-05\n",
            "step: 190, loss: 0.011743343435227871\n",
            "step: 200, loss: 0.00041495473124086857\n",
            "step: 210, loss: 0.002596869133412838\n",
            "step: 220, loss: 0.02219618484377861\n",
            "step: 230, loss: 0.024682506918907166\n",
            "step: 240, loss: 0.00018663030641619116\n",
            "step: 250, loss: 0.0005920403054915369\n",
            "step: 260, loss: 0.0011854668846353889\n",
            "step: 270, loss: 0.004194135777652264\n",
            "step: 280, loss: 0.0109592629596591\n",
            "step: 290, loss: 0.00020993873476982117\n",
            "step: 300, loss: 0.0016563701210543513\n",
            "step: 310, loss: 0.0005385936819948256\n",
            "step: 320, loss: 0.0001226418826263398\n",
            "step: 330, loss: 0.028524739667773247\n",
            "step: 340, loss: 0.02384384348988533\n",
            "step: 350, loss: 0.0001417706225765869\n",
            "step: 360, loss: 0.00046726653818041086\n",
            "step: 370, loss: 0.0001387690717820078\n",
            "step: 380, loss: 0.006724480073899031\n",
            "step: 390, loss: 0.00037142637302167714\n",
            "step: 400, loss: 0.050647567957639694\n",
            "step: 410, loss: 0.000641263322904706\n",
            "step: 420, loss: 0.0002641274768393487\n",
            "step: 430, loss: 4.6243083488661796e-05\n",
            "step: 440, loss: 0.0026103772688657045\n",
            "step: 450, loss: 0.0001257206458831206\n",
            "step: 460, loss: 0.00048130759387277067\n",
            "step: 470, loss: 0.002466554520651698\n",
            "step: 480, loss: 0.12621493637561798\n",
            "step: 490, loss: 0.0005449372692964971\n",
            "step: 500, loss: 0.00010110207222169265\n",
            "step: 510, loss: 0.13165263831615448\n",
            "step: 520, loss: 0.0019822297617793083\n",
            "step: 530, loss: 0.04207827150821686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9354536950420954, f1=0.9333954354913834, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005759984254837036\n",
            "step: 10, loss: 0.029931064695119858\n",
            "step: 20, loss: 0.0005466046277433634\n",
            "step: 30, loss: 0.0006992521230131388\n",
            "step: 40, loss: 0.000577468890696764\n",
            "step: 50, loss: 0.0009198893676511943\n",
            "step: 60, loss: 0.0005452500772662461\n",
            "step: 70, loss: 0.00018365166033618152\n",
            "step: 80, loss: 0.00026199218700639904\n",
            "step: 90, loss: 0.002723563928157091\n",
            "step: 100, loss: 8.283270290121436e-05\n",
            "step: 110, loss: 5.472078191814944e-05\n",
            "step: 120, loss: 0.16236519813537598\n",
            "step: 130, loss: 0.0003389510966371745\n",
            "step: 140, loss: 0.00038289109943434596\n",
            "step: 150, loss: 7.34229979570955e-05\n",
            "step: 160, loss: 0.0013085752725601196\n",
            "step: 170, loss: 0.00905402097851038\n",
            "step: 180, loss: 0.0005465145222842693\n",
            "step: 190, loss: 0.001322379568591714\n",
            "step: 200, loss: 0.00011883122351719067\n",
            "step: 210, loss: 0.0008667406509630382\n",
            "step: 220, loss: 0.00016144388064276427\n",
            "step: 230, loss: 0.0008764386875554919\n",
            "step: 240, loss: 0.0008360145729966462\n",
            "step: 250, loss: 6.423232844099402e-05\n",
            "step: 260, loss: 0.00012213065929245204\n",
            "step: 270, loss: 0.0006380114355124533\n",
            "step: 280, loss: 0.005680836271494627\n",
            "step: 290, loss: 0.001143038272857666\n",
            "step: 300, loss: 0.03390984982252121\n",
            "step: 310, loss: 0.008233853615820408\n",
            "step: 320, loss: 0.01147383451461792\n",
            "step: 330, loss: 0.0003423707385081798\n",
            "step: 340, loss: 0.00030769180739298463\n",
            "step: 350, loss: 0.004466481506824493\n",
            "step: 360, loss: 0.00022744861780665815\n",
            "step: 370, loss: 0.006224595010280609\n",
            "step: 380, loss: 0.006315531674772501\n",
            "step: 390, loss: 0.12721122801303864\n",
            "step: 400, loss: 0.00014550378546118736\n",
            "step: 410, loss: 5.1009643357247114e-05\n",
            "step: 420, loss: 0.0016301858704537153\n",
            "step: 430, loss: 0.05541117861866951\n",
            "step: 440, loss: 0.011907807551324368\n",
            "step: 450, loss: 0.0011057284427806735\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 460, loss: 0.00023649170179851353\n",
            "step: 470, loss: 4.695031020673923e-05\n",
            "step: 480, loss: 3.146300878142938e-05\n",
            "step: 490, loss: 0.0003483303589746356\n",
            "step: 500, loss: 0.0021673727314919233\n",
            "step: 510, loss: 0.0001238165277754888\n",
            "step: 520, loss: 0.0005160917644388974\n",
            "step: 530, loss: 0.0005198931321501732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9360112097150864, f1=0.9348729792147806, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008355825557373464\n",
            "step: 10, loss: 0.00022499554324895144\n",
            "step: 20, loss: 0.00013223293353803456\n",
            "step: 30, loss: 0.00010986456618411466\n",
            "step: 40, loss: 0.00019821416935883462\n",
            "step: 50, loss: 0.0013377632712945342\n",
            "step: 60, loss: 0.00010886448581004515\n",
            "step: 70, loss: 0.00014847249258309603\n",
            "step: 80, loss: 0.0012717755744233727\n",
            "step: 90, loss: 0.0001654146908549592\n",
            "step: 100, loss: 4.533704850473441e-05\n",
            "step: 110, loss: 2.7670854251482524e-05\n",
            "step: 120, loss: 6.897923594806343e-05\n",
            "step: 130, loss: 0.00015064141189213842\n",
            "step: 140, loss: 0.0001832240232033655\n",
            "step: 150, loss: 5.3567946451948956e-05\n",
            "step: 160, loss: 0.0014471530448645353\n",
            "step: 170, loss: 0.0009369614999741316\n",
            "step: 180, loss: 3.559575998224318e-05\n",
            "step: 190, loss: 7.321361772483215e-05\n",
            "step: 200, loss: 9.173843136522919e-05\n",
            "step: 210, loss: 0.00019836572755593807\n",
            "step: 220, loss: 0.0035806167870759964\n",
            "step: 230, loss: 2.425113598292228e-05\n",
            "step: 240, loss: 0.00015114441339392215\n",
            "step: 250, loss: 0.0003723755944520235\n",
            "step: 260, loss: 0.0009226992842741311\n",
            "step: 270, loss: 2.0146098904660903e-05\n",
            "step: 280, loss: 0.0011049192398786545\n",
            "step: 290, loss: 0.0729079395532608\n",
            "step: 300, loss: 3.68739347322844e-05\n",
            "step: 310, loss: 0.01257374882698059\n",
            "step: 320, loss: 7.502979860873893e-05\n",
            "step: 330, loss: 3.7076395528856665e-05\n",
            "step: 340, loss: 2.837436113622971e-05\n",
            "step: 350, loss: 0.00023923757544253021\n",
            "step: 360, loss: 2.089476038236171e-05\n",
            "step: 370, loss: 0.011564546264708042\n",
            "step: 380, loss: 2.7454727387521416e-05\n",
            "step: 390, loss: 3.0924766178941354e-05\n",
            "step: 400, loss: 0.0003237284254282713\n",
            "step: 410, loss: 2.3695680283708498e-05\n",
            "step: 420, loss: 2.152805245714262e-05\n",
            "step: 430, loss: 0.0001070157450158149\n",
            "step: 440, loss: 0.001521856989711523\n",
            "step: 450, loss: 2.6657427952159196e-05\n",
            "step: 460, loss: 5.615750706056133e-05\n",
            "step: 470, loss: 1.5497043932555243e-05\n",
            "step: 480, loss: 0.0001615441287867725\n",
            "step: 490, loss: 0.000830135599244386\n",
            "step: 500, loss: 0.0005817008204758167\n",
            "step: 510, loss: 0.0021958258002996445\n",
            "step: 520, loss: 0.00010679013939807191\n",
            "step: 530, loss: 0.006799535825848579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9382022471910113, f1=0.9323378441437238, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.309911597985774e-05\n",
            "step: 10, loss: 8.133828669087961e-05\n",
            "step: 20, loss: 2.414305527054239e-05\n",
            "step: 30, loss: 2.7566220524022356e-05\n",
            "step: 40, loss: 6.466188642662019e-05\n",
            "step: 50, loss: 5.734670048695989e-05\n",
            "step: 60, loss: 0.0005265343352220953\n",
            "step: 70, loss: 3.496962381177582e-05\n",
            "step: 80, loss: 2.5510200430289842e-05\n",
            "step: 90, loss: 2.5972211005864665e-05\n",
            "step: 100, loss: 0.01587485335767269\n",
            "step: 110, loss: 3.1441624742001295e-05\n",
            "step: 120, loss: 0.00011856194760184735\n",
            "step: 130, loss: 0.00010291244689142331\n",
            "step: 140, loss: 0.019523916766047478\n",
            "step: 150, loss: 8.720646292204037e-05\n",
            "step: 160, loss: 0.0008008089498616755\n",
            "step: 170, loss: 0.0007165406132116914\n",
            "step: 180, loss: 6.733618647558615e-05\n",
            "step: 190, loss: 0.0014627125347033143\n",
            "step: 200, loss: 0.004393062554299831\n",
            "step: 210, loss: 0.0003648198035079986\n",
            "step: 220, loss: 0.014111100696027279\n",
            "step: 230, loss: 0.017463358119130135\n",
            "step: 240, loss: 8.10562924016267e-05\n",
            "step: 250, loss: 2.7309288270771503e-05\n",
            "step: 260, loss: 0.00010195457434747368\n",
            "step: 270, loss: 4.936775440000929e-05\n",
            "step: 280, loss: 0.00036187394289299846\n",
            "step: 290, loss: 4.7538567741867155e-05\n",
            "step: 300, loss: 0.00017981822020374238\n",
            "step: 310, loss: 0.0001732778619043529\n",
            "step: 320, loss: 0.00017528259195387363\n",
            "step: 330, loss: 0.007129355799406767\n",
            "step: 340, loss: 0.005115174688398838\n",
            "step: 350, loss: 0.0002692163980100304\n",
            "step: 360, loss: 0.00012863404117524624\n",
            "step: 370, loss: 0.0002655237331055105\n",
            "step: 380, loss: 0.005311053246259689\n",
            "step: 390, loss: 8.120511483866721e-05\n",
            "step: 400, loss: 6.660798680968583e-05\n",
            "step: 410, loss: 4.0673512557987124e-05\n",
            "step: 420, loss: 5.350595893105492e-05\n",
            "step: 430, loss: 2.0060344468220137e-05\n",
            "step: 440, loss: 9.360406693303958e-05\n",
            "step: 450, loss: 3.1595085602020845e-05\n",
            "step: 460, loss: 5.2191055146977305e-05\n",
            "step: 470, loss: 0.0017207274213433266\n",
            "step: 480, loss: 7.250547059811652e-05\n",
            "step: 490, loss: 0.0009060962474904954\n",
            "step: 500, loss: 0.0026563284918665886\n",
            "step: 510, loss: 4.86390563310124e-05\n",
            "step: 520, loss: 0.00019348584464751184\n",
            "step: 530, loss: 0.0001533656322862953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9387755102040817, f1=0.9383624655013799, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.262855069711804e-05\n",
            "step: 10, loss: 0.00038091649184934795\n",
            "step: 20, loss: 0.00014379702042788267\n",
            "step: 30, loss: 0.0001652142673265189\n",
            "step: 40, loss: 0.00010924050002358854\n",
            "step: 50, loss: 0.0001415468577761203\n",
            "step: 60, loss: 0.01290160696953535\n",
            "step: 70, loss: 0.00012441458238754421\n",
            "step: 80, loss: 3.810310226981528e-05\n",
            "step: 90, loss: 3.572598143364303e-05\n",
            "step: 100, loss: 0.0022954640444368124\n",
            "step: 110, loss: 2.9764316423097625e-05\n",
            "step: 120, loss: 5.203060572966933e-05\n",
            "step: 130, loss: 0.0005254442221485078\n",
            "step: 140, loss: 0.0001384083880111575\n",
            "step: 150, loss: 2.1561419998761266e-05\n",
            "step: 160, loss: 5.3743973694508895e-05\n",
            "step: 170, loss: 6.727849540766329e-05\n",
            "step: 180, loss: 3.072468098253012e-05\n",
            "step: 190, loss: 0.0011059100506827235\n",
            "step: 200, loss: 0.000933182833250612\n",
            "step: 210, loss: 5.5529610108351335e-05\n",
            "step: 220, loss: 0.0006514700944535434\n",
            "step: 230, loss: 1.9672927010105923e-05\n",
            "step: 240, loss: 2.2142607122077607e-05\n",
            "step: 250, loss: 1.6733791198930703e-05\n",
            "step: 260, loss: 1.920356044138316e-05\n",
            "step: 270, loss: 0.002679127501323819\n",
            "step: 280, loss: 2.2556276235263795e-05\n",
            "step: 290, loss: 4.122330210520886e-05\n",
            "step: 300, loss: 5.577989577432163e-05\n",
            "step: 310, loss: 6.761207623640075e-05\n",
            "step: 320, loss: 3.630356150097214e-05\n",
            "step: 330, loss: 0.00019177155627403408\n",
            "step: 340, loss: 2.1572524929069914e-05\n",
            "step: 350, loss: 3.0090666768955998e-05\n",
            "step: 360, loss: 0.0006781063857488334\n",
            "step: 370, loss: 0.0008142524748109281\n",
            "step: 380, loss: 2.4042408767854795e-05\n",
            "step: 390, loss: 8.033720951061696e-05\n",
            "step: 400, loss: 4.7686411562608555e-05\n",
            "step: 410, loss: 2.9022949092905037e-05\n",
            "step: 420, loss: 4.026004171464592e-05\n",
            "step: 430, loss: 2.2634416382061318e-05\n",
            "step: 440, loss: 0.00011226977949263528\n",
            "step: 450, loss: 5.080740083940327e-05\n",
            "step: 460, loss: 3.840774661512114e-05\n",
            "step: 470, loss: 4.977414573659189e-05\n",
            "step: 480, loss: 7.525023829657584e-05\n",
            "step: 490, loss: 0.0024311181623488665\n",
            "step: 500, loss: 3.270935485488735e-05\n",
            "step: 510, loss: 0.00014729905524291098\n",
            "step: 520, loss: 2.4496841433574446e-05\n",
            "step: 530, loss: 2.3594438971485943e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9347111319868483, f1=0.9298000929800094, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.4559903977205977e-05\n",
            "step: 10, loss: 2.0190571376588196e-05\n",
            "step: 20, loss: 3.382137947482988e-05\n",
            "step: 30, loss: 0.0005193030228838325\n",
            "step: 40, loss: 3.482522879494354e-05\n",
            "step: 50, loss: 0.06438743323087692\n",
            "step: 60, loss: 7.441382331307977e-05\n",
            "step: 70, loss: 0.00010185163409914821\n",
            "step: 80, loss: 2.182946991524659e-05\n",
            "step: 90, loss: 0.00010293370723957196\n",
            "step: 100, loss: 0.0007599650998599827\n",
            "step: 110, loss: 0.00012926376075483859\n",
            "step: 120, loss: 1.728498682496138e-05\n",
            "step: 130, loss: 7.207638554973528e-05\n",
            "step: 140, loss: 1.9453091226750985e-05\n",
            "step: 150, loss: 1.7367021428071894e-05\n",
            "step: 160, loss: 5.388204226619564e-05\n",
            "step: 170, loss: 4.0742248529568315e-05\n",
            "step: 180, loss: 0.0005858806543983519\n",
            "step: 190, loss: 4.6369561459869146e-05\n",
            "step: 200, loss: 0.0007444637594744563\n",
            "step: 210, loss: 0.0002035116485785693\n",
            "step: 220, loss: 0.0018821409903466702\n",
            "step: 230, loss: 1.3392295841185842e-05\n",
            "step: 240, loss: 0.00010437277524033561\n",
            "step: 250, loss: 2.667214175744448e-05\n",
            "step: 260, loss: 2.346871406189166e-05\n",
            "step: 270, loss: 4.285392424208112e-05\n",
            "step: 280, loss: 0.001984104048460722\n",
            "step: 290, loss: 0.0006853451486676931\n",
            "step: 300, loss: 0.03859507292509079\n",
            "step: 310, loss: 8.386038098251447e-05\n",
            "step: 320, loss: 1.5314466509153135e-05\n",
            "step: 330, loss: 6.468353967648e-05\n",
            "step: 340, loss: 0.000882055377587676\n",
            "step: 350, loss: 2.5163668396999128e-05\n",
            "step: 360, loss: 0.0021876762621104717\n",
            "step: 370, loss: 8.394069300265983e-05\n",
            "step: 380, loss: 1.7948264940059744e-05\n",
            "step: 390, loss: 0.18559151887893677\n",
            "step: 400, loss: 4.826001531910151e-05\n",
            "step: 410, loss: 0.00019405802595429122\n",
            "step: 420, loss: 0.0008801458170637488\n",
            "step: 430, loss: 0.015222040005028248\n",
            "step: 440, loss: 0.00026946773868985474\n",
            "step: 450, loss: 4.601033651852049e-05\n",
            "step: 460, loss: 6.518858572235331e-05\n",
            "step: 470, loss: 4.401839760248549e-05\n",
            "step: 480, loss: 0.0015405818121507764\n",
            "step: 490, loss: 9.638497431296855e-05\n",
            "step: 500, loss: 9.04811458894983e-05\n",
            "step: 510, loss: 0.0001501324150012806\n",
            "step: 520, loss: 5.729968688683584e-05\n",
            "step: 530, loss: 0.0003161555214319378\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9393364928909952, f1=0.9340196537201684, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.7720847608870827e-05\n",
            "step: 10, loss: 1.8041100702248514e-05\n",
            "step: 20, loss: 2.588336428743787e-05\n",
            "step: 30, loss: 3.371136335772462e-05\n",
            "step: 40, loss: 2.2205589630175382e-05\n",
            "step: 50, loss: 0.00041822082130238414\n",
            "step: 60, loss: 0.0003134091093670577\n",
            "step: 70, loss: 0.0007419074536301196\n",
            "step: 80, loss: 0.00020964909344911575\n",
            "step: 90, loss: 0.02470560558140278\n",
            "step: 100, loss: 3.734287020051852e-05\n",
            "step: 110, loss: 0.00010821627074619755\n",
            "step: 120, loss: 0.0002478087553754449\n",
            "step: 130, loss: 4.986815474694595e-05\n",
            "step: 140, loss: 0.001130991498939693\n",
            "step: 150, loss: 0.003005096223205328\n",
            "step: 160, loss: 0.0001014044537441805\n",
            "step: 170, loss: 4.490140054258518e-05\n",
            "step: 180, loss: 4.425729639478959e-05\n",
            "step: 190, loss: 1.751232412061654e-05\n",
            "step: 200, loss: 1.7151021893369034e-05\n",
            "step: 210, loss: 8.030104072531685e-05\n",
            "step: 220, loss: 2.5367004127474502e-05\n",
            "step: 230, loss: 1.2423743100953288e-05\n",
            "step: 240, loss: 4.125635314267129e-05\n",
            "step: 250, loss: 4.0970524423755705e-05\n",
            "step: 260, loss: 0.00018653615552466363\n",
            "step: 270, loss: 3.2946347346296534e-05\n",
            "step: 280, loss: 1.7411766748409718e-05\n",
            "step: 290, loss: 0.000323774351272732\n",
            "step: 300, loss: 3.655127147794701e-05\n",
            "step: 310, loss: 2.6160612833336927e-05\n",
            "step: 320, loss: 3.869570355163887e-05\n",
            "step: 330, loss: 2.7579430025070906e-05\n",
            "step: 340, loss: 1.5172874554991722e-05\n",
            "step: 350, loss: 1.3697771464649122e-05\n",
            "step: 360, loss: 1.5444818927790038e-05\n",
            "step: 370, loss: 0.001929566846229136\n",
            "step: 380, loss: 3.5342294722795486e-05\n",
            "step: 390, loss: 0.0008451023022644222\n",
            "step: 400, loss: 1.7653845134191215e-05\n",
            "step: 410, loss: 9.949513332685456e-05\n",
            "step: 420, loss: 1.648785655561369e-05\n",
            "step: 430, loss: 0.0016127958660945296\n",
            "step: 440, loss: 3.969619865529239e-05\n",
            "step: 450, loss: 2.4306302293553017e-05\n",
            "step: 460, loss: 3.2572363124927506e-05\n",
            "step: 470, loss: 1.6286525351461023e-05\n",
            "step: 480, loss: 2.8948414183105342e-05\n",
            "step: 490, loss: 9.311053872806951e-05\n",
            "step: 500, loss: 1.1786699360527564e-05\n",
            "step: 510, loss: 1.7620281141716987e-05\n",
            "step: 520, loss: 7.123462273739278e-05\n",
            "step: 530, loss: 1.2133178643125575e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9403475810239549, f1=0.9335192933519293, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.581360811542254e-05\n",
            "step: 10, loss: 1.1671251741063315e-05\n",
            "step: 20, loss: 1.2494523616624065e-05\n",
            "step: 30, loss: 1.5828507457626984e-05\n",
            "step: 40, loss: 2.6257819627062418e-05\n",
            "step: 50, loss: 3.117509186267853e-05\n",
            "step: 60, loss: 1.5586147128487937e-05\n",
            "step: 70, loss: 1.690869430603925e-05\n",
            "step: 80, loss: 1.591424188518431e-05\n",
            "step: 90, loss: 2.3147778847487643e-05\n",
            "step: 100, loss: 0.004102020990103483\n",
            "step: 110, loss: 2.220085298176855e-05\n",
            "step: 120, loss: 6.634197052335367e-05\n",
            "step: 130, loss: 0.018001476302742958\n",
            "step: 140, loss: 1.4830094187345821e-05\n",
            "step: 150, loss: 1.0974625183735043e-05\n",
            "step: 160, loss: 7.797498255968094e-05\n",
            "step: 170, loss: 2.733737710514106e-05\n",
            "step: 180, loss: 8.751584391575307e-05\n",
            "step: 190, loss: 2.4531764211133122e-05\n",
            "step: 200, loss: 6.179964111652225e-05\n",
            "step: 210, loss: 1.4338497749122325e-05\n",
            "step: 220, loss: 1.4502382327918895e-05\n",
            "step: 230, loss: 1.9959643395850435e-05\n",
            "step: 240, loss: 1.204376167152077e-05\n",
            "step: 250, loss: 1.4282605661719572e-05\n",
            "step: 260, loss: 6.403660518117249e-05\n",
            "step: 270, loss: 2.9768691092613153e-05\n",
            "step: 280, loss: 1.2300803064135835e-05\n",
            "step: 290, loss: 1.645809788897168e-05\n",
            "step: 300, loss: 2.1639620172209106e-05\n",
            "step: 310, loss: 0.00030891262576915324\n",
            "step: 320, loss: 3.9067777834134176e-05\n",
            "step: 330, loss: 2.546489668020513e-05\n",
            "step: 340, loss: 1.79444468813017e-05\n",
            "step: 350, loss: 2.1438259864225984e-05\n",
            "step: 360, loss: 0.00024509686045348644\n",
            "step: 370, loss: 0.000733400578610599\n",
            "step: 380, loss: 1.4819008356425911e-05\n",
            "step: 390, loss: 0.0260605551302433\n",
            "step: 400, loss: 2.871639298973605e-05\n",
            "step: 410, loss: 1.3399715498962905e-05\n",
            "step: 420, loss: 5.55237966182176e-05\n",
            "step: 430, loss: 1.746758607623633e-05\n",
            "step: 440, loss: 8.581849397160113e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 450, loss: 0.008189959451556206\n",
            "step: 460, loss: 2.0357672838144936e-05\n",
            "step: 470, loss: 1.142166365752928e-05\n",
            "step: 480, loss: 1.331030853179982e-05\n",
            "step: 490, loss: 1.2636073734029196e-05\n",
            "step: 500, loss: 1.1064024874940515e-05\n",
            "step: 510, loss: 0.010185821913182735\n",
            "step: 520, loss: 2.5587662094039842e-05\n",
            "step: 530, loss: 0.001273882808163762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9404536862003782, f1=0.9336448598130841, best_f1=0.9358974358974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.1652616194623988e-05\n",
            "step: 10, loss: 1.4923270100553054e-05\n",
            "step: 20, loss: 2.143812525901012e-05\n",
            "step: 30, loss: 8.716288721188903e-05\n",
            "step: 40, loss: 0.03714234009385109\n",
            "step: 50, loss: 0.00013751228107139468\n",
            "step: 60, loss: 1.1317348253214732e-05\n",
            "step: 70, loss: 6.584537914022803e-05\n",
            "step: 80, loss: 1.310916013608221e-05\n",
            "step: 90, loss: 1.4059090972295962e-05\n",
            "step: 100, loss: 6.256495544221252e-05\n",
            "step: 110, loss: 4.521431401371956e-05\n",
            "step: 120, loss: 0.000700093456543982\n",
            "step: 130, loss: 0.04693368077278137\n",
            "step: 140, loss: 2.5971035938709974e-05\n",
            "step: 150, loss: 1.6335126929334365e-05\n",
            "step: 160, loss: 1.5012683434179053e-05\n",
            "step: 170, loss: 1.1175778126926161e-05\n",
            "step: 180, loss: 1.2218815754749812e-05\n",
            "step: 190, loss: 2.9402026484603994e-05\n",
            "step: 200, loss: 0.00014450807066168636\n",
            "step: 210, loss: 0.00019451050320640206\n",
            "step: 220, loss: 1.1350863132975064e-05\n",
            "step: 230, loss: 2.5415811251150444e-05\n",
            "step: 240, loss: 1.7236467101611197e-05\n",
            "step: 250, loss: 4.313247700338252e-05\n",
            "step: 260, loss: 2.5878065571305342e-05\n",
            "step: 270, loss: 1.756801975716371e-05\n",
            "step: 280, loss: 2.5663528504082933e-05\n",
            "step: 290, loss: 1.1332220310578123e-05\n",
            "step: 300, loss: 1.4856087545922492e-05\n",
            "step: 310, loss: 2.923184001701884e-05\n",
            "step: 320, loss: 0.00012242489901836962\n",
            "step: 330, loss: 3.865330290864222e-05\n",
            "step: 340, loss: 1.0814434062922373e-05\n",
            "step: 350, loss: 1.6978936400846578e-05\n",
            "step: 360, loss: 3.233482493669726e-05\n",
            "step: 370, loss: 1.0549953913141508e-05\n",
            "step: 380, loss: 1.5221335161186289e-05\n",
            "step: 390, loss: 2.0040653907926753e-05\n",
            "step: 400, loss: 1.7899519662023522e-05\n",
            "step: 410, loss: 1.3936127288616262e-05\n",
            "step: 420, loss: 6.111847324064001e-05\n",
            "step: 430, loss: 1.7404327081749216e-05\n",
            "step: 440, loss: 0.00012003934534732252\n",
            "step: 450, loss: 1.94859821931459e-05\n",
            "step: 460, loss: 0.0001281368313357234\n",
            "step: 470, loss: 0.008537035435438156\n",
            "step: 480, loss: 1.0341334018448833e-05\n",
            "step: 490, loss: 1.1224219633731991e-05\n",
            "step: 500, loss: 5.999854329274967e-05\n",
            "step: 510, loss: 5.2645817049779e-05\n",
            "step: 520, loss: 1.1719654139596969e-05\n",
            "step: 530, loss: 1.648025499889627e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9388523047977423, f1=0.9320297951582867, best_f1=0.9358974358974358\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:28, 198.11it/s]\n",
            "load_f1 = 0.940795559666975\n",
            "real_f1 = 0.9400369003690037\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10184639-5753-4b1b-a2e3-ad912a127562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5460110306739807\n",
            "step: 10, loss: 0.3640379011631012\n",
            "step: 20, loss: 0.38557374477386475\n",
            "step: 30, loss: 0.3200463652610779\n",
            "step: 40, loss: 0.17705665528774261\n",
            "step: 50, loss: 0.4138333797454834\n",
            "step: 60, loss: 0.3495092988014221\n",
            "step: 70, loss: 0.20973734557628632\n",
            "step: 80, loss: 0.2612484097480774\n",
            "step: 90, loss: 0.3215775787830353\n",
            "step: 100, loss: 0.5610376596450806\n",
            "step: 110, loss: 0.24223178625106812\n",
            "step: 120, loss: 0.31590598821640015\n",
            "step: 130, loss: 0.22225825488567352\n",
            "step: 140, loss: 0.13823223114013672\n",
            "step: 150, loss: 0.14714591205120087\n",
            "step: 160, loss: 0.30325013399124146\n",
            "step: 170, loss: 0.28464218974113464\n",
            "step: 180, loss: 0.06281592696905136\n",
            "step: 190, loss: 0.18835780024528503\n",
            "step: 200, loss: 0.22519442439079285\n",
            "step: 210, loss: 0.1982792317867279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6539923954372623, f1=0.6819047619047619, best_f1=0.6819047619047619\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07427912950515747\n",
            "step: 10, loss: 0.26146015524864197\n",
            "step: 20, loss: 0.20621486008167267\n",
            "step: 30, loss: 0.18790943920612335\n",
            "step: 40, loss: 0.1430908441543579\n",
            "step: 50, loss: 0.13172025978565216\n",
            "step: 60, loss: 0.32401812076568604\n",
            "step: 70, loss: 0.11444240808486938\n",
            "step: 80, loss: 0.11838476359844208\n",
            "step: 90, loss: 0.12966375052928925\n",
            "step: 100, loss: 0.02194887027144432\n",
            "step: 110, loss: 0.1906481236219406\n",
            "step: 120, loss: 0.1337033063173294\n",
            "step: 130, loss: 0.02349781058728695\n",
            "step: 140, loss: 0.1641443818807602\n",
            "step: 150, loss: 0.17309239506721497\n",
            "step: 160, loss: 0.18990522623062134\n",
            "step: 170, loss: 0.10621516406536102\n",
            "step: 180, loss: 0.14439161121845245\n",
            "step: 190, loss: 0.08148373663425446\n",
            "step: 200, loss: 0.05172054097056389\n",
            "step: 210, loss: 0.2188519686460495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6779661016949152, f1=0.7052023121387283, best_f1=0.7052023121387283\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07846951484680176\n",
            "step: 10, loss: 0.1051056832075119\n",
            "step: 20, loss: 0.0718395859003067\n",
            "step: 30, loss: 0.2322656810283661\n",
            "step: 40, loss: 0.12750861048698425\n",
            "step: 50, loss: 0.035229433327913284\n",
            "step: 60, loss: 0.16098646819591522\n",
            "step: 70, loss: 0.09904292225837708\n",
            "step: 80, loss: 0.2345113307237625\n",
            "step: 90, loss: 0.14285115897655487\n",
            "step: 100, loss: 0.09136670082807541\n",
            "step: 110, loss: 0.1823520064353943\n",
            "step: 120, loss: 0.12232840806245804\n",
            "step: 130, loss: 0.12842801213264465\n",
            "step: 140, loss: 0.1435440331697464\n",
            "step: 150, loss: 0.20759905874729156\n",
            "step: 160, loss: 0.014913223683834076\n",
            "step: 170, loss: 0.10252367705106735\n",
            "step: 180, loss: 0.04339880868792534\n",
            "step: 190, loss: 0.25809401273727417\n",
            "step: 200, loss: 0.04074728116393089\n",
            "step: 210, loss: 0.10077787935733795\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6852589641434264, f1=0.6800804828973844, best_f1=0.6800804828973844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2867015302181244\n",
            "step: 10, loss: 0.07729137688875198\n",
            "step: 20, loss: 0.1713157743215561\n",
            "step: 30, loss: 0.07907741516828537\n",
            "step: 40, loss: 0.05741015449166298\n",
            "step: 50, loss: 0.28890261054039\n",
            "step: 60, loss: 0.1830299347639084\n",
            "step: 70, loss: 0.19520939886569977\n",
            "step: 80, loss: 0.18467283248901367\n",
            "step: 90, loss: 0.0421462282538414\n",
            "step: 100, loss: 0.23554624617099762\n",
            "step: 110, loss: 0.16803649067878723\n",
            "step: 120, loss: 0.06807751953601837\n",
            "step: 130, loss: 0.22234056890010834\n",
            "step: 140, loss: 0.21941722929477692\n",
            "step: 150, loss: 0.12331709265708923\n",
            "step: 160, loss: 0.03649255260825157\n",
            "step: 170, loss: 0.12596359848976135\n",
            "step: 180, loss: 0.23122838139533997\n",
            "step: 190, loss: 0.03737068921327591\n",
            "step: 200, loss: 0.45732468366622925\n",
            "step: 210, loss: 0.15000581741333008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7035573122529645, f1=0.6811881188118812, best_f1=0.6811881188118812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10171671956777573\n",
            "step: 10, loss: 0.028143087401986122\n",
            "step: 20, loss: 0.22986458241939545\n",
            "step: 30, loss: 0.1058935672044754\n",
            "step: 40, loss: 0.07174821197986603\n",
            "step: 50, loss: 0.10862641036510468\n",
            "step: 60, loss: 0.203245148062706\n",
            "step: 70, loss: 0.1147703155875206\n",
            "step: 80, loss: 0.08001385629177094\n",
            "step: 90, loss: 0.12532895803451538\n",
            "step: 100, loss: 0.02025780640542507\n",
            "step: 110, loss: 0.27055442333221436\n",
            "step: 120, loss: 0.13001364469528198\n",
            "step: 130, loss: 0.07908331602811813\n",
            "step: 140, loss: 0.05927503481507301\n",
            "step: 150, loss: 0.2591136395931244\n",
            "step: 160, loss: 0.08920928835868835\n",
            "step: 170, loss: 0.07699290663003922\n",
            "step: 180, loss: 0.027257554233074188\n",
            "step: 190, loss: 0.021941982209682465\n",
            "step: 200, loss: 0.08688194304704666\n",
            "step: 210, loss: 0.017693115398287773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7099391480730223, f1=0.674698795180723, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05090261995792389\n",
            "step: 10, loss: 0.050597675144672394\n",
            "step: 20, loss: 0.022069135680794716\n",
            "step: 30, loss: 0.003705403069034219\n",
            "step: 40, loss: 0.007740612607449293\n",
            "step: 50, loss: 0.0803874284029007\n",
            "step: 60, loss: 0.13684354722499847\n",
            "step: 70, loss: 0.0435134582221508\n",
            "step: 80, loss: 0.15811839699745178\n",
            "step: 90, loss: 0.1932021975517273\n",
            "step: 100, loss: 0.00998549722135067\n",
            "step: 110, loss: 0.06163354963064194\n",
            "step: 120, loss: 0.12373057007789612\n",
            "step: 130, loss: 0.09795457869768143\n",
            "step: 140, loss: 0.07236781716346741\n",
            "step: 150, loss: 0.030005119740962982\n",
            "step: 160, loss: 0.02981763519346714\n",
            "step: 170, loss: 0.211735799908638\n",
            "step: 180, loss: 0.10200728476047516\n",
            "step: 190, loss: 0.1428057849407196\n",
            "step: 200, loss: 0.009075342677533627\n",
            "step: 210, loss: 0.08426512777805328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6895161290322581, f1=0.6827309236947792, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04055974632501602\n",
            "step: 10, loss: 0.04148866981267929\n",
            "step: 20, loss: 0.006609252654016018\n",
            "step: 30, loss: 0.02916661463677883\n",
            "step: 40, loss: 0.09240681678056717\n",
            "step: 50, loss: 0.1086752712726593\n",
            "step: 60, loss: 0.026204394176602364\n",
            "step: 70, loss: 0.005446212366223335\n",
            "step: 80, loss: 0.049453362822532654\n",
            "step: 90, loss: 0.03244739770889282\n",
            "step: 100, loss: 0.005402076989412308\n",
            "step: 110, loss: 0.10385506600141525\n",
            "step: 120, loss: 0.08952292054891586\n",
            "step: 130, loss: 0.16519513726234436\n",
            "step: 140, loss: 0.02919781766831875\n",
            "step: 150, loss: 0.011475283652544022\n",
            "step: 160, loss: 0.0687779113650322\n",
            "step: 170, loss: 0.03219043090939522\n",
            "step: 180, loss: 0.10969164967536926\n",
            "step: 190, loss: 0.13864856958389282\n",
            "step: 200, loss: 0.028296858072280884\n",
            "step: 210, loss: 0.05896434187889099\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6992481203007519, f1=0.6765799256505576, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08829942345619202\n",
            "step: 10, loss: 0.07809695601463318\n",
            "step: 20, loss: 0.011327533051371574\n",
            "step: 30, loss: 0.07729370892047882\n",
            "step: 40, loss: 0.059367381036281586\n",
            "step: 50, loss: 0.13742542266845703\n",
            "step: 60, loss: 0.18593141436576843\n",
            "step: 70, loss: 0.024722177535295486\n",
            "step: 80, loss: 0.07872379571199417\n",
            "step: 90, loss: 0.0568871907889843\n",
            "step: 100, loss: 0.09185666590929031\n",
            "step: 110, loss: 0.11116853356361389\n",
            "step: 120, loss: 0.03900628909468651\n",
            "step: 130, loss: 0.006590620614588261\n",
            "step: 140, loss: 0.031745586544275284\n",
            "step: 150, loss: 0.006725140381604433\n",
            "step: 160, loss: 0.025930000469088554\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.028039205819368362\n",
            "step: 180, loss: 0.05684515833854675\n",
            "step: 190, loss: 0.013810480013489723\n",
            "step: 200, loss: 0.015137090347707272\n",
            "step: 210, loss: 0.25603026151657104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.696, f1=0.697029702970297, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033837640658020973\n",
            "step: 10, loss: 0.029602201655507088\n",
            "step: 20, loss: 0.007716941647231579\n",
            "step: 30, loss: 0.02074120193719864\n",
            "step: 40, loss: 0.05881250649690628\n",
            "step: 50, loss: 0.19398312270641327\n",
            "step: 60, loss: 0.1432693898677826\n",
            "step: 70, loss: 0.12687554955482483\n",
            "step: 80, loss: 0.0245358869433403\n",
            "step: 90, loss: 0.0012484057806432247\n",
            "step: 100, loss: 0.020483000203967094\n",
            "step: 110, loss: 0.012955144047737122\n",
            "step: 120, loss: 0.019589394330978394\n",
            "step: 130, loss: 0.09674002230167389\n",
            "step: 140, loss: 0.043878763914108276\n",
            "step: 150, loss: 0.03161392733454704\n",
            "step: 160, loss: 0.0022610013838857412\n",
            "step: 170, loss: 0.01930256001651287\n",
            "step: 180, loss: 0.014774334616959095\n",
            "step: 190, loss: 0.0008339303894899786\n",
            "step: 200, loss: 0.06994052231311798\n",
            "step: 210, loss: 0.018406150862574577\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6720647773279351, f1=0.6680327868852459, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04963570833206177\n",
            "step: 10, loss: 0.13766159117221832\n",
            "step: 20, loss: 0.007140354719012976\n",
            "step: 30, loss: 0.08685290068387985\n",
            "step: 40, loss: 0.0010464085498824716\n",
            "step: 50, loss: 0.014028976671397686\n",
            "step: 60, loss: 0.05703551322221756\n",
            "step: 70, loss: 0.16602793335914612\n",
            "step: 80, loss: 0.058530911803245544\n",
            "step: 90, loss: 0.1051250547170639\n",
            "step: 100, loss: 0.06448351591825485\n",
            "step: 110, loss: 0.0019111970905214548\n",
            "step: 120, loss: 0.01657973602414131\n",
            "step: 130, loss: 0.04611348733305931\n",
            "step: 140, loss: 0.005275181494653225\n",
            "step: 150, loss: 0.05492795258760452\n",
            "step: 160, loss: 0.10450175404548645\n",
            "step: 170, loss: 0.008550608530640602\n",
            "step: 180, loss: 0.043130047619342804\n",
            "step: 190, loss: 0.2928697466850281\n",
            "step: 200, loss: 0.04026957228779793\n",
            "step: 210, loss: 0.04697244614362717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6915113871635611, f1=0.6818181818181819, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04855935275554657\n",
            "step: 10, loss: 0.03713934123516083\n",
            "step: 20, loss: 0.057616036385297775\n",
            "step: 30, loss: 0.0029454207979142666\n",
            "step: 40, loss: 0.028485752642154694\n",
            "step: 50, loss: 0.029838865622878075\n",
            "step: 60, loss: 0.050202395766973495\n",
            "step: 70, loss: 0.003365657525137067\n",
            "step: 80, loss: 0.07429825514554977\n",
            "step: 90, loss: 0.018998514860868454\n",
            "step: 100, loss: 0.20468126237392426\n",
            "step: 110, loss: 0.04424005746841431\n",
            "step: 120, loss: 0.017086008563637733\n",
            "step: 130, loss: 0.015433981083333492\n",
            "step: 140, loss: 0.007049860432744026\n",
            "step: 150, loss: 0.03592013567686081\n",
            "step: 160, loss: 0.016304880380630493\n",
            "step: 170, loss: 0.0023198402486741543\n",
            "step: 180, loss: 0.0017704698257148266\n",
            "step: 190, loss: 0.06504325568675995\n",
            "step: 200, loss: 0.0013662214623764157\n",
            "step: 210, loss: 0.004096821416169405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6604127579737337, f1=0.6783625730994153, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06822383403778076\n",
            "step: 10, loss: 0.047326356172561646\n",
            "step: 20, loss: 0.03380564600229263\n",
            "step: 30, loss: 0.04519183561205864\n",
            "step: 40, loss: 0.01958766020834446\n",
            "step: 50, loss: 0.0023678282741457224\n",
            "step: 60, loss: 0.014476804994046688\n",
            "step: 70, loss: 0.006783766206353903\n",
            "step: 80, loss: 0.0064787482842803\n",
            "step: 90, loss: 0.0730719193816185\n",
            "step: 100, loss: 0.011650768108665943\n",
            "step: 110, loss: 0.00471073854714632\n",
            "step: 120, loss: 0.00239170272834599\n",
            "step: 130, loss: 0.013794654048979282\n",
            "step: 140, loss: 0.018376894295215607\n",
            "step: 150, loss: 0.04575144127011299\n",
            "step: 160, loss: 0.05398211255669594\n",
            "step: 170, loss: 0.01932297833263874\n",
            "step: 180, loss: 0.01130414754152298\n",
            "step: 190, loss: 0.004152459558099508\n",
            "step: 200, loss: 0.004593872930854559\n",
            "step: 210, loss: 0.030747972428798676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6749482401656315, f1=0.6666666666666666, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028550049290060997\n",
            "step: 10, loss: 0.0013405540958046913\n",
            "step: 20, loss: 0.08097144216299057\n",
            "step: 30, loss: 0.25377705693244934\n",
            "step: 40, loss: 0.09989720582962036\n",
            "step: 50, loss: 0.0039873430505394936\n",
            "step: 60, loss: 0.024640027433633804\n",
            "step: 70, loss: 0.10107827186584473\n",
            "step: 80, loss: 0.02193531207740307\n",
            "step: 90, loss: 0.000872860080562532\n",
            "step: 100, loss: 0.09232308715581894\n",
            "step: 110, loss: 0.0038806325756013393\n",
            "step: 120, loss: 0.0318298377096653\n",
            "step: 130, loss: 0.0061177900061011314\n",
            "step: 140, loss: 0.036169059574604034\n",
            "step: 150, loss: 0.005896552931517363\n",
            "step: 160, loss: 0.00803544744849205\n",
            "step: 170, loss: 0.010572453960776329\n",
            "step: 180, loss: 0.18136487901210785\n",
            "step: 190, loss: 0.004217945039272308\n",
            "step: 200, loss: 0.0018376258667558432\n",
            "step: 210, loss: 0.01575513184070587\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6878727634194831, f1=0.6720647773279351, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019961893558502197\n",
            "step: 10, loss: 0.017384210601449013\n",
            "step: 20, loss: 0.0975879430770874\n",
            "step: 30, loss: 0.01453519519418478\n",
            "step: 40, loss: 0.0025937906466424465\n",
            "step: 50, loss: 0.038933444768190384\n",
            "step: 60, loss: 0.009425967931747437\n",
            "step: 70, loss: 0.004380682948976755\n",
            "step: 80, loss: 0.01962720789015293\n",
            "step: 90, loss: 0.28139054775238037\n",
            "step: 100, loss: 0.00554626202210784\n",
            "step: 110, loss: 0.010361350141465664\n",
            "step: 120, loss: 0.042718555778265\n",
            "step: 130, loss: 0.026853490620851517\n",
            "step: 140, loss: 0.02252880111336708\n",
            "step: 150, loss: 0.0029357830062508583\n",
            "step: 160, loss: 0.010521702468395233\n",
            "step: 170, loss: 0.03647460415959358\n",
            "step: 180, loss: 0.0035917069762945175\n",
            "step: 190, loss: 0.004632844123989344\n",
            "step: 200, loss: 0.0030912859365344048\n",
            "step: 210, loss: 0.024338852614164352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6782273603082851, f1=0.671875, best_f1=0.674698795180723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005284283775836229\n",
            "step: 10, loss: 0.0009890757501125336\n",
            "step: 20, loss: 0.006231535691767931\n",
            "step: 30, loss: 0.0389404371380806\n",
            "step: 40, loss: 0.003501209430396557\n",
            "step: 50, loss: 0.011801994405686855\n",
            "step: 60, loss: 0.005087288562208414\n",
            "step: 70, loss: 0.0058805448934435844\n",
            "step: 80, loss: 0.0011002932442352176\n",
            "step: 90, loss: 0.012507778592407703\n",
            "step: 100, loss: 0.00395980104804039\n",
            "step: 110, loss: 0.026293661445379257\n",
            "step: 120, loss: 0.00794249027967453\n",
            "step: 130, loss: 0.07599946111440659\n",
            "step: 140, loss: 0.0035379838664084673\n",
            "step: 150, loss: 0.0013969545252621174\n",
            "step: 160, loss: 0.025451717898249626\n",
            "step: 170, loss: 0.002121566329151392\n",
            "step: 180, loss: 0.0009701864910311997\n",
            "step: 190, loss: 0.0028855896089226007\n",
            "step: 200, loss: 0.009082087315618992\n",
            "step: 210, loss: 0.016096312552690506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.67578125, f1=0.6666666666666666, best_f1=0.674698795180723\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 467.66it/s]\n",
            "load_f1 = 0.6975806451612904\n",
            "real_f1 = 0.6881287726358148\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 250.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d209c456-319a-4249-ec55-cf29211d50b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5863348841667175\n",
            "step: 10, loss: 0.36719444394111633\n",
            "step: 20, loss: 0.2808379828929901\n",
            "step: 30, loss: 0.4149381220340729\n",
            "step: 40, loss: 0.4123986065387726\n",
            "step: 50, loss: 0.283336877822876\n",
            "step: 60, loss: 0.19170859456062317\n",
            "step: 70, loss: 0.2671651542186737\n",
            "step: 80, loss: 0.23234212398529053\n",
            "step: 90, loss: 0.2686989903450012\n",
            "step: 100, loss: 0.2523171305656433\n",
            "step: 110, loss: 0.39307674765586853\n",
            "step: 120, loss: 0.037775252014398575\n",
            "step: 130, loss: 0.09541941434144974\n",
            "step: 140, loss: 0.11081825941801071\n",
            "step: 150, loss: 0.13763533532619476\n",
            "step: 160, loss: 0.05665842071175575\n",
            "step: 170, loss: 0.1655033379793167\n",
            "step: 180, loss: 0.0316956602036953\n",
            "step: 190, loss: 0.12065298110246658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7052896725440806, f1=0.7282051282051282, best_f1=0.7282051282051282\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14337432384490967\n",
            "step: 10, loss: 0.0383516326546669\n",
            "step: 20, loss: 0.1577739417552948\n",
            "step: 30, loss: 0.13050086796283722\n",
            "step: 40, loss: 0.136930450797081\n",
            "step: 50, loss: 0.12288717180490494\n",
            "step: 60, loss: 0.43379244208335876\n",
            "step: 70, loss: 0.10225459188222885\n",
            "step: 80, loss: 0.10163073241710663\n",
            "step: 90, loss: 0.13978393375873566\n",
            "step: 100, loss: 0.009211502969264984\n",
            "step: 110, loss: 0.04561059921979904\n",
            "step: 120, loss: 0.14400553703308105\n",
            "step: 130, loss: 0.05973486974835396\n",
            "step: 140, loss: 0.08390423655509949\n",
            "step: 150, loss: 0.13584911823272705\n",
            "step: 160, loss: 0.025293663144111633\n",
            "step: 170, loss: 0.14221717417240143\n",
            "step: 180, loss: 0.272377610206604\n",
            "step: 190, loss: 0.022600259631872177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7885714285714286, f1=0.7999999999999999, best_f1=0.7999999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06338358670473099\n",
            "step: 10, loss: 0.34959420561790466\n",
            "step: 20, loss: 0.032400909811258316\n",
            "step: 30, loss: 0.11705099791288376\n",
            "step: 40, loss: 0.07433856278657913\n",
            "step: 50, loss: 0.1720314919948578\n",
            "step: 60, loss: 0.11407586187124252\n",
            "step: 70, loss: 0.22069209814071655\n",
            "step: 80, loss: 0.03798311576247215\n",
            "step: 90, loss: 0.12273147702217102\n",
            "step: 100, loss: 0.026897137984633446\n",
            "step: 110, loss: 0.032313939183950424\n",
            "step: 120, loss: 0.02946445718407631\n",
            "step: 130, loss: 0.005897526163607836\n",
            "step: 140, loss: 0.013948960229754448\n",
            "step: 150, loss: 0.109466053545475\n",
            "step: 160, loss: 0.08083123713731766\n",
            "step: 170, loss: 0.127328559756279\n",
            "step: 180, loss: 0.03218990936875343\n",
            "step: 190, loss: 0.06883582472801208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8042895442359249, f1=0.8216216216216217, best_f1=0.8216216216216217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024944698438048363\n",
            "step: 10, loss: 0.11612502485513687\n",
            "step: 20, loss: 0.02769279107451439\n",
            "step: 30, loss: 0.006437323521822691\n",
            "step: 40, loss: 0.006287685129791498\n",
            "step: 50, loss: 0.014398363418877125\n",
            "step: 60, loss: 0.0669625848531723\n",
            "step: 70, loss: 0.02134626731276512\n",
            "step: 80, loss: 0.08039671182632446\n",
            "step: 90, loss: 0.008957872167229652\n",
            "step: 100, loss: 0.07409115880727768\n",
            "step: 110, loss: 0.0014459789963439107\n",
            "step: 120, loss: 0.06643760949373245\n",
            "step: 130, loss: 0.11857735365629196\n",
            "step: 140, loss: 0.027325909584760666\n",
            "step: 150, loss: 0.010603741742670536\n",
            "step: 160, loss: 0.003124062204733491\n",
            "step: 170, loss: 0.1760624796152115\n",
            "step: 180, loss: 0.018527289852499962\n",
            "step: 190, loss: 0.052404824644327164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.8031496062992125, f1=0.7648578811369509, best_f1=0.8216216216216217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12191174924373627\n",
            "step: 10, loss: 0.027622591704130173\n",
            "step: 20, loss: 0.011495121754705906\n",
            "step: 30, loss: 0.0357002392411232\n",
            "step: 40, loss: 0.20701080560684204\n",
            "step: 50, loss: 0.07788628339767456\n",
            "step: 60, loss: 0.12398643046617508\n",
            "step: 70, loss: 0.027227899059653282\n",
            "step: 80, loss: 0.003119734115898609\n",
            "step: 90, loss: 0.006592325400561094\n",
            "step: 100, loss: 0.002146015642210841\n",
            "step: 110, loss: 0.002570433309301734\n",
            "step: 120, loss: 0.0019475817680358887\n",
            "step: 130, loss: 0.038686059415340424\n",
            "step: 140, loss: 0.009533567354083061\n",
            "step: 150, loss: 0.1544882357120514\n",
            "step: 160, loss: 0.20964553952217102\n",
            "step: 170, loss: 0.010844715870916843\n",
            "step: 180, loss: 0.058047857135534286\n",
            "step: 190, loss: 0.12381460517644882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7888040712468193, f1=0.7719298245614035, best_f1=0.8216216216216217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008434156887233257\n",
            "step: 10, loss: 0.1966938078403473\n",
            "step: 20, loss: 0.0040211547166109085\n",
            "step: 30, loss: 0.0010808712104335427\n",
            "step: 40, loss: 0.06296687573194504\n",
            "step: 50, loss: 0.016001274809241295\n",
            "step: 60, loss: 0.012607386335730553\n",
            "step: 70, loss: 0.0145558537915349\n",
            "step: 80, loss: 0.03392740339040756\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.007304161787033081\n",
            "step: 100, loss: 0.0027104979380965233\n",
            "step: 110, loss: 0.010309840552508831\n",
            "step: 120, loss: 0.04039593040943146\n",
            "step: 130, loss: 0.0033841419499367476\n",
            "step: 140, loss: 0.08252058178186417\n",
            "step: 150, loss: 0.00112355244345963\n",
            "step: 160, loss: 0.006836860906332731\n",
            "step: 170, loss: 0.012529334984719753\n",
            "step: 180, loss: 0.002208793070167303\n",
            "step: 190, loss: 0.06509532779455185\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8088642659279778, f1=0.7967914438502673, best_f1=0.7967914438502673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017602334264665842\n",
            "step: 10, loss: 0.0012402830179780722\n",
            "step: 20, loss: 0.16751225292682648\n",
            "step: 30, loss: 0.011175879277288914\n",
            "step: 40, loss: 0.004125806037336588\n",
            "step: 50, loss: 0.03386402875185013\n",
            "step: 60, loss: 0.2181219458580017\n",
            "step: 70, loss: 0.0030959295108914375\n",
            "step: 80, loss: 0.006915769539773464\n",
            "step: 90, loss: 0.020644504576921463\n",
            "step: 100, loss: 0.00037453623372130096\n",
            "step: 110, loss: 0.04211104288697243\n",
            "step: 120, loss: 0.008097061887383461\n",
            "step: 130, loss: 0.0012472844682633877\n",
            "step: 140, loss: 0.0029775933362543583\n",
            "step: 150, loss: 0.01819540373980999\n",
            "step: 160, loss: 0.06871362030506134\n",
            "step: 170, loss: 0.004258759319782257\n",
            "step: 180, loss: 0.0014769192785024643\n",
            "step: 190, loss: 0.0023346843663603067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7967032967032966, f1=0.7880434782608696, best_f1=0.7967914438502673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003444471163675189\n",
            "step: 10, loss: 0.0027870042249560356\n",
            "step: 20, loss: 0.16861699521541595\n",
            "step: 30, loss: 0.01047882903367281\n",
            "step: 40, loss: 0.0030063132289797068\n",
            "step: 50, loss: 0.0014851322630420327\n",
            "step: 60, loss: 0.0009774441132321954\n",
            "step: 70, loss: 0.05389348417520523\n",
            "step: 80, loss: 0.0005209450027905405\n",
            "step: 90, loss: 0.05968094617128372\n",
            "step: 100, loss: 0.00372248119674623\n",
            "step: 110, loss: 0.0015918747521936893\n",
            "step: 120, loss: 0.01903715543448925\n",
            "step: 130, loss: 0.043780550360679626\n",
            "step: 140, loss: 0.0048003895208239555\n",
            "step: 150, loss: 0.014294222928583622\n",
            "step: 160, loss: 0.003174350829795003\n",
            "step: 170, loss: 0.0020541115663945675\n",
            "step: 180, loss: 0.013504471629858017\n",
            "step: 190, loss: 0.00554483849555254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8103896103896104, f1=0.7948051948051948, best_f1=0.7948051948051948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012671143049374223\n",
            "step: 10, loss: 0.04523082077503204\n",
            "step: 20, loss: 0.0017400607466697693\n",
            "step: 30, loss: 0.0044248090125620365\n",
            "step: 40, loss: 0.000816446146927774\n",
            "step: 50, loss: 0.09060851484537125\n",
            "step: 60, loss: 0.0006074737175367773\n",
            "step: 70, loss: 0.0005116761312820017\n",
            "step: 80, loss: 0.0016094736056402326\n",
            "step: 90, loss: 0.034241221845149994\n",
            "step: 100, loss: 0.0019181654788553715\n",
            "step: 110, loss: 0.00936796236783266\n",
            "step: 120, loss: 0.001299688359722495\n",
            "step: 130, loss: 0.00070259312633425\n",
            "step: 140, loss: 0.022694319486618042\n",
            "step: 150, loss: 0.001341380411759019\n",
            "step: 160, loss: 0.025770826265215874\n",
            "step: 170, loss: 0.003401722526177764\n",
            "step: 180, loss: 0.002815653569996357\n",
            "step: 190, loss: 0.001213572919368744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8184281842818427, f1=0.8087431693989071, best_f1=0.8087431693989071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001786579261533916\n",
            "step: 10, loss: 0.07044008374214172\n",
            "step: 20, loss: 0.004325034096837044\n",
            "step: 30, loss: 0.003706501331180334\n",
            "step: 40, loss: 0.019371170550584793\n",
            "step: 50, loss: 0.0009484820766374469\n",
            "step: 60, loss: 0.011258131824433804\n",
            "step: 70, loss: 0.001957722706720233\n",
            "step: 80, loss: 0.002422815188765526\n",
            "step: 90, loss: 0.0012219113996252418\n",
            "step: 100, loss: 0.016109993681311607\n",
            "step: 110, loss: 0.04983460158109665\n",
            "step: 120, loss: 0.10796018689870834\n",
            "step: 130, loss: 0.0013222881825640798\n",
            "step: 140, loss: 0.005334322340786457\n",
            "step: 150, loss: 0.0014415071345865726\n",
            "step: 160, loss: 0.03097892552614212\n",
            "step: 170, loss: 0.001813960145227611\n",
            "step: 180, loss: 0.0017415813636034727\n",
            "step: 190, loss: 0.0015784410061314702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8136482939632546, f1=0.8083989501312335, best_f1=0.8087431693989071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000976820127107203\n",
            "step: 10, loss: 0.0017297740560024977\n",
            "step: 20, loss: 0.015570908784866333\n",
            "step: 30, loss: 0.03148031234741211\n",
            "step: 40, loss: 0.000352498289430514\n",
            "step: 50, loss: 0.0006074673729017377\n",
            "step: 60, loss: 0.0011515399673953652\n",
            "step: 70, loss: 0.0033501535654067993\n",
            "step: 80, loss: 0.0005768518312834203\n",
            "step: 90, loss: 0.00033280940260738134\n",
            "step: 100, loss: 0.0063935332000255585\n",
            "step: 110, loss: 0.0006498200818896294\n",
            "step: 120, loss: 0.000700960517860949\n",
            "step: 130, loss: 0.0008233832777477801\n",
            "step: 140, loss: 0.0016656082589179277\n",
            "step: 150, loss: 0.000462591735413298\n",
            "step: 160, loss: 0.006232441868633032\n",
            "step: 170, loss: 0.0006632785080000758\n",
            "step: 180, loss: 0.003730416763573885\n",
            "step: 190, loss: 0.004310243763029575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8070175438596492, f1=0.8223350253807106, best_f1=0.8087431693989071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005789405549876392\n",
            "step: 10, loss: 0.0022402540780603886\n",
            "step: 20, loss: 0.0007700963760726154\n",
            "step: 30, loss: 0.0003837059484794736\n",
            "step: 40, loss: 0.0022261515259742737\n",
            "step: 50, loss: 0.011634394526481628\n",
            "step: 60, loss: 0.0006709602894261479\n",
            "step: 70, loss: 0.037935446947813034\n",
            "step: 80, loss: 0.0008645522175356746\n",
            "step: 90, loss: 0.0004095335316378623\n",
            "step: 100, loss: 0.00032777138403616846\n",
            "step: 110, loss: 0.00068328419001773\n",
            "step: 120, loss: 0.0015380216063931584\n",
            "step: 130, loss: 0.0008774831658229232\n",
            "step: 140, loss: 0.023356324061751366\n",
            "step: 150, loss: 0.0010922375367954373\n",
            "step: 160, loss: 0.0008180485456250608\n",
            "step: 170, loss: 0.0017802031943574548\n",
            "step: 180, loss: 0.0020912194158881903\n",
            "step: 190, loss: 0.0006197789334692061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8188976377952756, f1=0.8073878627968338, best_f1=0.8073878627968338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002253603423014283\n",
            "step: 10, loss: 0.0037637220229953527\n",
            "step: 20, loss: 0.0015495785046368837\n",
            "step: 30, loss: 0.0011416713241487741\n",
            "step: 40, loss: 0.0009505274356342852\n",
            "step: 50, loss: 0.03564697876572609\n",
            "step: 60, loss: 0.0015775869833305478\n",
            "step: 70, loss: 0.0038602440617978573\n",
            "step: 80, loss: 0.0031725293956696987\n",
            "step: 90, loss: 0.0028974025044590235\n",
            "step: 100, loss: 0.0013501200592145324\n",
            "step: 110, loss: 0.0005852851900272071\n",
            "step: 120, loss: 0.011359594762325287\n",
            "step: 130, loss: 0.00038467784179374576\n",
            "step: 140, loss: 0.0012980567989870906\n",
            "step: 150, loss: 0.0007756119011901319\n",
            "step: 160, loss: 0.0016228166641667485\n",
            "step: 170, loss: 0.0004982429090887308\n",
            "step: 180, loss: 0.0015824951697140932\n",
            "step: 190, loss: 0.005967116914689541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8253968253968254, f1=0.8106666666666666, best_f1=0.8106666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038795163854956627\n",
            "step: 10, loss: 0.0010554904583841562\n",
            "step: 20, loss: 0.0002915211080107838\n",
            "step: 30, loss: 0.001013193977996707\n",
            "step: 40, loss: 0.003027983708307147\n",
            "step: 50, loss: 0.0007099007489159703\n",
            "step: 60, loss: 0.08579505234956741\n",
            "step: 70, loss: 0.0007833486306481063\n",
            "step: 80, loss: 0.0022650519385933876\n",
            "step: 90, loss: 0.004383643623441458\n",
            "step: 100, loss: 0.006480396259576082\n",
            "step: 110, loss: 0.048109084367752075\n",
            "step: 120, loss: 0.03705503046512604\n",
            "step: 130, loss: 0.0037940458860248327\n",
            "step: 140, loss: 0.000668252760078758\n",
            "step: 150, loss: 0.0009469513315707445\n",
            "step: 160, loss: 0.011536970734596252\n",
            "step: 170, loss: 0.10653608292341232\n",
            "step: 180, loss: 0.004057825077325106\n",
            "step: 190, loss: 0.001079733483493328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8320000000000001, f1=0.8128342245989304, best_f1=0.8128342245989304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038182553835213184\n",
            "step: 10, loss: 0.0008416124037466943\n",
            "step: 20, loss: 0.002149177948012948\n",
            "step: 30, loss: 0.0012866926845163107\n",
            "step: 40, loss: 0.0004670748603530228\n",
            "step: 50, loss: 0.003149968571960926\n",
            "step: 60, loss: 0.0009478137362748384\n",
            "step: 70, loss: 0.0011384247336536646\n",
            "step: 80, loss: 0.0019959311466664076\n",
            "step: 90, loss: 0.0009325107675977051\n",
            "step: 100, loss: 0.000826692848931998\n",
            "step: 110, loss: 0.0012162926141172647\n",
            "step: 120, loss: 0.0018391249468550086\n",
            "step: 130, loss: 0.0006646986585110426\n",
            "step: 140, loss: 0.029006563127040863\n",
            "step: 150, loss: 0.0017820593202486634\n",
            "step: 160, loss: 0.010929107666015625\n",
            "step: 170, loss: 0.0008678120793774724\n",
            "step: 180, loss: 0.0008574177627451718\n",
            "step: 190, loss: 0.13083042204380035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8279569892473119, f1=0.8184281842818427, best_f1=0.8128342245989304\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 234.55it/s]\n",
            "load_f1 = 0.7525773195876289\n",
            "real_f1 = 0.6987951807228916\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 259.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cae985b-2c88-4013-f041-990f7902c77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6161351799964905\n",
            "step: 10, loss: 0.3771972060203552\n",
            "step: 20, loss: 0.2929035425186157\n",
            "step: 30, loss: 0.3960883319377899\n",
            "step: 40, loss: 0.2657056152820587\n",
            "step: 50, loss: 0.2588040828704834\n",
            "step: 60, loss: 0.2468966245651245\n",
            "step: 70, loss: 0.35691922903060913\n",
            "step: 80, loss: 0.367149293422699\n",
            "step: 90, loss: 0.24240654706954956\n",
            "step: 100, loss: 0.19538556039333344\n",
            "step: 110, loss: 0.22437922656536102\n",
            "step: 120, loss: 0.17751477658748627\n",
            "step: 130, loss: 0.023072484880685806\n",
            "step: 140, loss: 0.07158875465393066\n",
            "step: 150, loss: 0.2852569818496704\n",
            "step: 160, loss: 0.13901107013225555\n",
            "step: 170, loss: 0.27880918979644775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7574257425742574, f1=0.7303921568627451, best_f1=0.7303921568627451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03112310729920864\n",
            "step: 10, loss: 0.17344270646572113\n",
            "step: 20, loss: 0.04922337085008621\n",
            "step: 30, loss: 0.15818728506565094\n",
            "step: 40, loss: 0.12795612215995789\n",
            "step: 50, loss: 0.06808970868587494\n",
            "step: 60, loss: 0.16136465966701508\n",
            "step: 70, loss: 0.06251000612974167\n",
            "step: 80, loss: 0.022791776806116104\n",
            "step: 90, loss: 0.218578040599823\n",
            "step: 100, loss: 0.21618632972240448\n",
            "step: 110, loss: 0.07609671354293823\n",
            "step: 120, loss: 0.05584296956658363\n",
            "step: 130, loss: 0.05387517064809799\n",
            "step: 140, loss: 0.3202720284461975\n",
            "step: 150, loss: 0.15689127147197723\n",
            "step: 160, loss: 0.12183349579572678\n",
            "step: 170, loss: 0.04670441895723343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.812206572769953, f1=0.7934272300469484, best_f1=0.7934272300469484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1102205365896225\n",
            "step: 10, loss: 0.04202934354543686\n",
            "step: 20, loss: 0.01655176840722561\n",
            "step: 30, loss: 0.11652398109436035\n",
            "step: 40, loss: 0.058949198573827744\n",
            "step: 50, loss: 0.19922301173210144\n",
            "step: 60, loss: 0.07201080769300461\n",
            "step: 70, loss: 0.06148970127105713\n",
            "step: 80, loss: 0.037860725075006485\n",
            "step: 90, loss: 0.0638519749045372\n",
            "step: 100, loss: 0.0014333612052723765\n",
            "step: 110, loss: 0.0440671443939209\n",
            "step: 120, loss: 0.13122093677520752\n",
            "step: 130, loss: 0.13080385327339172\n",
            "step: 140, loss: 0.11479058861732483\n",
            "step: 150, loss: 0.011300114914774895\n",
            "step: 160, loss: 0.03273747116327286\n",
            "step: 170, loss: 0.05341845378279686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8316831683168316, f1=0.8232445520581113, best_f1=0.8232445520581113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03228550776839256\n",
            "step: 10, loss: 0.009719460271298885\n",
            "step: 20, loss: 0.0015901743900030851\n",
            "step: 30, loss: 0.08329065144062042\n",
            "step: 40, loss: 0.002968223299831152\n",
            "step: 50, loss: 0.046448152512311935\n",
            "step: 60, loss: 0.17650921642780304\n",
            "step: 70, loss: 0.010933252051472664\n",
            "step: 80, loss: 0.14375154674053192\n",
            "step: 90, loss: 0.05631589889526367\n",
            "step: 100, loss: 0.20671147108078003\n",
            "step: 110, loss: 0.09203403443098068\n",
            "step: 120, loss: 0.009909788146615028\n",
            "step: 130, loss: 0.012697913683950901\n",
            "step: 140, loss: 0.04340779036283493\n",
            "step: 150, loss: 0.12650710344314575\n",
            "step: 160, loss: 0.04213138669729233\n",
            "step: 170, loss: 0.02165766805410385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8369829683698297, f1=0.8129330254041571, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051570579409599304\n",
            "step: 10, loss: 0.018780067563056946\n",
            "step: 20, loss: 0.034185245633125305\n",
            "step: 30, loss: 0.020762693136930466\n",
            "step: 40, loss: 0.018236752599477768\n",
            "step: 50, loss: 0.012874462641775608\n",
            "step: 60, loss: 0.01376788504421711\n",
            "step: 70, loss: 0.09232843667268753\n",
            "step: 80, loss: 0.05027405172586441\n",
            "step: 90, loss: 0.021878782659769058\n",
            "step: 100, loss: 0.009818745777010918\n",
            "step: 110, loss: 0.013513852842152119\n",
            "step: 120, loss: 0.013012184761464596\n",
            "step: 130, loss: 0.023350976407527924\n",
            "step: 140, loss: 0.043398793786764145\n",
            "step: 150, loss: 0.04778027534484863\n",
            "step: 160, loss: 0.061197079718112946\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.019385050982236862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8363636363636364, f1=0.8181818181818181, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003182914573699236\n",
            "step: 10, loss: 0.11796750873327255\n",
            "step: 20, loss: 0.06253071874380112\n",
            "step: 30, loss: 0.010079636238515377\n",
            "step: 40, loss: 0.03480485454201698\n",
            "step: 50, loss: 0.09468475729227066\n",
            "step: 60, loss: 0.01721484400331974\n",
            "step: 70, loss: 0.0367811918258667\n",
            "step: 80, loss: 0.0035045347176492214\n",
            "step: 90, loss: 0.025548089295625687\n",
            "step: 100, loss: 0.001048882957547903\n",
            "step: 110, loss: 0.002597035840153694\n",
            "step: 120, loss: 0.005008351057767868\n",
            "step: 130, loss: 0.0429978109896183\n",
            "step: 140, loss: 0.003866314422339201\n",
            "step: 150, loss: 0.04989969730377197\n",
            "step: 160, loss: 0.07806938886642456\n",
            "step: 170, loss: 0.005324242170900106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8201058201058201, f1=0.8174807197943444, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014212167588993907\n",
            "step: 10, loss: 0.0017490005120635033\n",
            "step: 20, loss: 0.007514874916523695\n",
            "step: 30, loss: 0.02463478036224842\n",
            "step: 40, loss: 0.004279595334082842\n",
            "step: 50, loss: 0.03873978182673454\n",
            "step: 60, loss: 0.001408133888617158\n",
            "step: 70, loss: 0.0007796110585331917\n",
            "step: 80, loss: 0.010728541761636734\n",
            "step: 90, loss: 0.00027332588797435164\n",
            "step: 100, loss: 0.0015414234949275851\n",
            "step: 110, loss: 0.02113087847828865\n",
            "step: 120, loss: 0.011931680142879486\n",
            "step: 130, loss: 0.13105171918869019\n",
            "step: 140, loss: 0.0010927343973889947\n",
            "step: 150, loss: 0.0015707820421084762\n",
            "step: 160, loss: 0.0005197270074859262\n",
            "step: 170, loss: 0.016324061900377274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8155844155844156, f1=0.8134715025906737, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16222675144672394\n",
            "step: 10, loss: 0.0012776171788573265\n",
            "step: 20, loss: 0.0010246215388178825\n",
            "step: 30, loss: 0.008396302349865437\n",
            "step: 40, loss: 0.0003472567186690867\n",
            "step: 50, loss: 0.0008117278339341283\n",
            "step: 60, loss: 0.0018030498176813126\n",
            "step: 70, loss: 0.0019673246424645185\n",
            "step: 80, loss: 0.009222162887454033\n",
            "step: 90, loss: 0.0023272973485291004\n",
            "step: 100, loss: 0.12085769325494766\n",
            "step: 110, loss: 0.09512405842542648\n",
            "step: 120, loss: 0.0008188595529645681\n",
            "step: 130, loss: 0.0008745656232349575\n",
            "step: 140, loss: 0.0010531700681895018\n",
            "step: 150, loss: 0.008320217952132225\n",
            "step: 160, loss: 0.008464349433779716\n",
            "step: 170, loss: 0.000836511084344238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8072916666666666, f1=0.7969543147208121, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003043690521735698\n",
            "step: 10, loss: 0.009672467596828938\n",
            "step: 20, loss: 0.0011109731858596206\n",
            "step: 30, loss: 0.009986129589378834\n",
            "step: 40, loss: 0.0014839344657957554\n",
            "step: 50, loss: 0.0006263302639126778\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.00838529970496893\n",
            "step: 70, loss: 0.18702952563762665\n",
            "step: 80, loss: 0.03422393277287483\n",
            "step: 90, loss: 0.0822332426905632\n",
            "step: 100, loss: 0.029060132801532745\n",
            "step: 110, loss: 0.009540011174976826\n",
            "step: 120, loss: 0.018633518368005753\n",
            "step: 130, loss: 0.0343262143433094\n",
            "step: 140, loss: 0.0017065084539353848\n",
            "step: 150, loss: 0.0014674450503662229\n",
            "step: 160, loss: 0.006578455679118633\n",
            "step: 170, loss: 0.0380181148648262\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8184019370460048, f1=0.8113207547169812, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04326873645186424\n",
            "step: 10, loss: 0.00015787180745974183\n",
            "step: 20, loss: 0.051748890429735184\n",
            "step: 30, loss: 0.008997778408229351\n",
            "step: 40, loss: 0.00011624390026554465\n",
            "step: 50, loss: 0.17262552678585052\n",
            "step: 60, loss: 0.00018328802252653986\n",
            "step: 70, loss: 0.0017491886392235756\n",
            "step: 80, loss: 0.004454418085515499\n",
            "step: 90, loss: 0.06744886934757233\n",
            "step: 100, loss: 0.001197486068122089\n",
            "step: 110, loss: 0.006633310578763485\n",
            "step: 120, loss: 0.0023265157360583544\n",
            "step: 130, loss: 0.009207779541611671\n",
            "step: 140, loss: 0.0037734273355454206\n",
            "step: 150, loss: 0.021151622757315636\n",
            "step: 160, loss: 0.001689208555035293\n",
            "step: 170, loss: 0.001036317553371191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8203883495145631, f1=0.8, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04515211656689644\n",
            "step: 10, loss: 0.010617751628160477\n",
            "step: 20, loss: 0.00020087591838091612\n",
            "step: 30, loss: 0.00015727621212136\n",
            "step: 40, loss: 0.0005380847142077982\n",
            "step: 50, loss: 0.004784903954714537\n",
            "step: 60, loss: 0.002420746488496661\n",
            "step: 70, loss: 0.00011761269706767052\n",
            "step: 80, loss: 0.00030422789859585464\n",
            "step: 90, loss: 0.004978337325155735\n",
            "step: 100, loss: 0.0009404636221006513\n",
            "step: 110, loss: 0.03592713549733162\n",
            "step: 120, loss: 0.0014014277840033174\n",
            "step: 130, loss: 0.00020036657224409282\n",
            "step: 140, loss: 0.0008130468777380884\n",
            "step: 150, loss: 0.0128707280382514\n",
            "step: 160, loss: 0.0015073306858539581\n",
            "step: 170, loss: 0.00228407746180892\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8211586901763224, f1=0.7971014492753623, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004751875530928373\n",
            "step: 10, loss: 0.0005871678586117923\n",
            "step: 20, loss: 0.0002678988385014236\n",
            "step: 30, loss: 0.00016327921184711158\n",
            "step: 40, loss: 0.0002783870149869472\n",
            "step: 50, loss: 0.00013603994739241898\n",
            "step: 60, loss: 0.00019041210180148482\n",
            "step: 70, loss: 0.04788631945848465\n",
            "step: 80, loss: 9.88426327239722e-05\n",
            "step: 90, loss: 0.005939730908721685\n",
            "step: 100, loss: 0.0002250838588224724\n",
            "step: 110, loss: 0.00023023271933197975\n",
            "step: 120, loss: 0.06071215495467186\n",
            "step: 130, loss: 0.0052580819465219975\n",
            "step: 140, loss: 0.0028118968475610018\n",
            "step: 150, loss: 0.0016077762702479959\n",
            "step: 160, loss: 0.00017984426813200116\n",
            "step: 170, loss: 0.00016920457710511982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8157894736842105, f1=0.7949367088607596, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02444756217300892\n",
            "step: 10, loss: 0.0010728344786912203\n",
            "step: 20, loss: 0.00020279565069358796\n",
            "step: 30, loss: 0.00046663818648085\n",
            "step: 40, loss: 0.00025566943804733455\n",
            "step: 50, loss: 0.0005802965024486184\n",
            "step: 60, loss: 0.0007531403098255396\n",
            "step: 70, loss: 0.0002972372167278081\n",
            "step: 80, loss: 0.00018428079783916473\n",
            "step: 90, loss: 0.0003850380307994783\n",
            "step: 100, loss: 0.0002818226639647037\n",
            "step: 110, loss: 0.0003974397259298712\n",
            "step: 120, loss: 0.042294442653656006\n",
            "step: 130, loss: 0.00012994515418540686\n",
            "step: 140, loss: 0.00012900651199743152\n",
            "step: 150, loss: 0.04766729101538658\n",
            "step: 160, loss: 0.00033674505539238453\n",
            "step: 170, loss: 0.004154559224843979\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8142493638676845, f1=0.7941176470588236, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000335999095113948\n",
            "step: 10, loss: 0.0001297970156883821\n",
            "step: 20, loss: 0.0005137301050126553\n",
            "step: 30, loss: 0.0007824734784662724\n",
            "step: 40, loss: 0.00015044429164845496\n",
            "step: 50, loss: 0.000120554119348526\n",
            "step: 60, loss: 7.899254705989733e-05\n",
            "step: 70, loss: 0.0007737100822851062\n",
            "step: 80, loss: 0.000628872134257108\n",
            "step: 90, loss: 0.0016268916660919785\n",
            "step: 100, loss: 0.007363116834312677\n",
            "step: 110, loss: 0.0019394161645323038\n",
            "step: 120, loss: 0.01666201464831829\n",
            "step: 130, loss: 0.033323001116514206\n",
            "step: 140, loss: 8.721756603335962e-05\n",
            "step: 150, loss: 0.00029534229543060064\n",
            "step: 160, loss: 9.345356374979019e-05\n",
            "step: 170, loss: 0.0013663643039762974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8162162162162162, f1=0.793733681462141, best_f1=0.8129330254041571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001383692870149389\n",
            "step: 10, loss: 0.0010683056898415089\n",
            "step: 20, loss: 0.02758750505745411\n",
            "step: 30, loss: 0.002419780008494854\n",
            "step: 40, loss: 0.0018143674824386835\n",
            "step: 50, loss: 0.0006946674548089504\n",
            "step: 60, loss: 0.006658510304987431\n",
            "step: 70, loss: 0.00010276463581249118\n",
            "step: 80, loss: 0.04137443006038666\n",
            "step: 90, loss: 0.005365755409002304\n",
            "step: 100, loss: 0.0009237434132955968\n",
            "step: 110, loss: 0.0001783553307177499\n",
            "step: 120, loss: 0.0006922970642335713\n",
            "step: 130, loss: 0.0011897677322849631\n",
            "step: 140, loss: 0.0002692330745048821\n",
            "step: 150, loss: 0.00045432886690832675\n",
            "step: 160, loss: 0.00013559585204347968\n",
            "step: 170, loss: 0.00042637038859538734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8164383561643836, f1=0.804289544235925, best_f1=0.8129330254041571\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 275.55it/s]\n",
            "load_f1 = 0.3864915572232645\n",
            "real_f1 = 0.36462093862815886\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 249.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1aecbf-193f-4f16-8f0e-7c23b9df372f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 436kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 912kB/s]\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 51.4MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6328253746032715\n",
            "step: 10, loss: 0.6103754639625549\n",
            "step: 20, loss: 0.3856714963912964\n",
            "step: 30, loss: 0.1393957883119583\n",
            "step: 40, loss: 0.1765221804380417\n",
            "step: 50, loss: 0.10389523208141327\n",
            "step: 60, loss: 0.11553406715393066\n",
            "step: 70, loss: 0.026385825127363205\n",
            "step: 80, loss: 0.051389988511800766\n",
            "step: 90, loss: 0.09927055984735489\n",
            "step: 100, loss: 0.0071890028193593025\n",
            "step: 110, loss: 0.26556140184402466\n",
            "step: 120, loss: 0.023457661271095276\n",
            "step: 130, loss: 0.01379297487437725\n",
            "step: 140, loss: 0.00209015398286283\n",
            "step: 150, loss: 0.02370705083012581\n",
            "step: 160, loss: 0.004031467251479626\n",
            "step: 170, loss: 0.20166727900505066\n",
            "step: 180, loss: 0.03046809509396553\n",
            "step: 190, loss: 0.052427250891923904\n",
            "step: 200, loss: 0.06157727539539337\n",
            "step: 210, loss: 0.007041556295007467\n",
            "step: 220, loss: 0.020971043035387993\n",
            "step: 230, loss: 0.011473480612039566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9668141592920354, f1=0.9597315436241611, best_f1=0.9597315436241611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01792488433420658\n",
            "step: 10, loss: 0.00782647356390953\n",
            "step: 20, loss: 0.16014230251312256\n",
            "step: 30, loss: 0.1593785434961319\n",
            "step: 40, loss: 0.07365930080413818\n",
            "step: 50, loss: 0.005195092409849167\n",
            "step: 60, loss: 0.013420111499726772\n",
            "step: 70, loss: 0.0938880443572998\n",
            "step: 80, loss: 0.008678065612912178\n",
            "step: 90, loss: 0.0027068473864346743\n",
            "step: 100, loss: 0.1329144686460495\n",
            "step: 110, loss: 0.1314181387424469\n",
            "step: 120, loss: 0.035921379923820496\n",
            "step: 130, loss: 0.016445208340883255\n",
            "step: 140, loss: 0.000814572034869343\n",
            "step: 150, loss: 0.014821207150816917\n",
            "step: 160, loss: 0.03506682068109512\n",
            "step: 170, loss: 0.0012440549908205867\n",
            "step: 180, loss: 0.07782574743032455\n",
            "step: 190, loss: 0.0033408936578780413\n",
            "step: 200, loss: 0.003287007100880146\n",
            "step: 210, loss: 0.0009351181797683239\n",
            "step: 220, loss: 0.09925171732902527\n",
            "step: 230, loss: 0.008618604391813278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9767441860465117, f1=0.9787234042553192, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010199536569416523\n",
            "step: 10, loss: 0.007129963953047991\n",
            "step: 20, loss: 0.005501138512045145\n",
            "step: 30, loss: 0.014088592492043972\n",
            "step: 40, loss: 0.047920677810907364\n",
            "step: 50, loss: 0.01934928074479103\n",
            "step: 60, loss: 0.009367890655994415\n",
            "step: 70, loss: 0.0019946449901908636\n",
            "step: 80, loss: 0.0024828696623444557\n",
            "step: 90, loss: 0.021068844944238663\n",
            "step: 100, loss: 0.0029837689362466335\n",
            "step: 110, loss: 0.0005984138115309179\n",
            "step: 120, loss: 0.017453908920288086\n",
            "step: 130, loss: 0.00031821487937122583\n",
            "step: 140, loss: 0.0030783924739807844\n",
            "step: 150, loss: 0.019322263076901436\n",
            "step: 160, loss: 0.007799824699759483\n",
            "step: 170, loss: 0.0035058713983744383\n",
            "step: 180, loss: 0.01648951880633831\n",
            "step: 190, loss: 0.0037169770803302526\n",
            "step: 200, loss: 0.07820051908493042\n",
            "step: 210, loss: 0.009687347337603569\n",
            "step: 220, loss: 0.004310948774218559\n",
            "step: 230, loss: 0.018410498276352882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9788182831661093, f1=0.9696969696969697, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010201437398791313\n",
            "step: 10, loss: 0.005887189880013466\n",
            "step: 20, loss: 0.0056731509976089\n",
            "step: 30, loss: 0.0009044689359143376\n",
            "step: 40, loss: 0.002252852777019143\n",
            "step: 50, loss: 0.0006214429158717394\n",
            "step: 60, loss: 0.0009372188942506909\n",
            "step: 70, loss: 0.0004170264001004398\n",
            "step: 80, loss: 0.000997008290141821\n",
            "step: 90, loss: 0.005551119334995747\n",
            "step: 100, loss: 0.0012163883075118065\n",
            "step: 110, loss: 0.0009063984616659582\n",
            "step: 120, loss: 0.004430186469107866\n",
            "step: 130, loss: 0.001011249958537519\n",
            "step: 140, loss: 0.0004732070374302566\n",
            "step: 150, loss: 0.2270107865333557\n",
            "step: 160, loss: 0.020377403125166893\n",
            "step: 170, loss: 0.0384901687502861\n",
            "step: 180, loss: 0.0007493505254387856\n",
            "step: 190, loss: 0.001524322316981852\n",
            "step: 200, loss: 0.0036994870752096176\n",
            "step: 210, loss: 0.14176134765148163\n",
            "step: 220, loss: 0.0004991292953491211\n",
            "step: 230, loss: 0.004736266098916531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9843749999999999, f1=0.980963045912654, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028591982554644346\n",
            "step: 10, loss: 0.0006475658738054335\n",
            "step: 20, loss: 0.0015521567547693849\n",
            "step: 30, loss: 0.00019148980209138244\n",
            "step: 40, loss: 0.0007148428703658283\n",
            "step: 50, loss: 0.0025109443813562393\n",
            "step: 60, loss: 0.076754130423069\n",
            "step: 70, loss: 0.0007034274167381227\n",
            "step: 80, loss: 0.0007096272311173379\n",
            "step: 90, loss: 0.0023137000389397144\n",
            "step: 100, loss: 0.0005060555413365364\n",
            "step: 110, loss: 0.0003047799982596189\n",
            "step: 120, loss: 0.00022395639098249376\n",
            "step: 130, loss: 0.018499305471777916\n",
            "step: 140, loss: 0.0017306567169725895\n",
            "step: 150, loss: 0.0008509734179824591\n",
            "step: 160, loss: 0.00044459925265982747\n",
            "step: 170, loss: 0.0004343627952039242\n",
            "step: 180, loss: 0.00022754189558327198\n",
            "step: 190, loss: 0.05608052387833595\n",
            "step: 200, loss: 0.009347678162157536\n",
            "step: 210, loss: 0.0018621011404320598\n",
            "step: 220, loss: 0.0009226193651556969\n",
            "step: 230, loss: 0.0021627454552799463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9808773903262092, f1=0.9763779527559054, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009926574304699898\n",
            "step: 10, loss: 0.0037944887299090624\n",
            "step: 20, loss: 0.0008569427882321179\n",
            "step: 30, loss: 0.00018839156837202609\n",
            "step: 40, loss: 0.0002023469569394365\n",
            "step: 50, loss: 0.00014298844325821847\n",
            "step: 60, loss: 0.00019478274043649435\n",
            "step: 70, loss: 0.0005304620135575533\n",
            "step: 80, loss: 0.003683892311528325\n",
            "step: 90, loss: 0.0002481688861735165\n",
            "step: 100, loss: 0.021367738023400307\n",
            "step: 110, loss: 0.0009986136574298143\n",
            "step: 120, loss: 0.014021213166415691\n",
            "step: 130, loss: 0.0032282527536153793\n",
            "step: 140, loss: 0.0006201882497407496\n",
            "step: 150, loss: 0.0676233321428299\n",
            "step: 160, loss: 0.0012558333110064268\n",
            "step: 170, loss: 0.0006288660224527121\n",
            "step: 180, loss: 0.007629347499459982\n",
            "step: 190, loss: 0.00039851327892392874\n",
            "step: 200, loss: 0.0002256120351376012\n",
            "step: 210, loss: 0.0001955215266207233\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 220, loss: 0.00017003185348585248\n",
            "step: 230, loss: 0.06634372472763062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9810055865921787, f1=0.9732142857142857, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003179889463353902\n",
            "step: 10, loss: 0.0035470614675432444\n",
            "step: 20, loss: 0.0010406909277662635\n",
            "step: 30, loss: 0.0015386033337563276\n",
            "step: 40, loss: 0.0004091447626706213\n",
            "step: 50, loss: 0.00016944303933996707\n",
            "step: 60, loss: 0.0001835315051721409\n",
            "step: 70, loss: 0.01965074986219406\n",
            "step: 80, loss: 0.0017972884234040976\n",
            "step: 90, loss: 0.0001796460710465908\n",
            "step: 100, loss: 0.00017409160500392318\n",
            "step: 110, loss: 0.033816803246736526\n",
            "step: 120, loss: 0.00011286497465334833\n",
            "step: 130, loss: 0.00017826406110543758\n",
            "step: 140, loss: 0.0003363180148880929\n",
            "step: 150, loss: 0.006154491566121578\n",
            "step: 160, loss: 0.06445659697055817\n",
            "step: 170, loss: 0.0009090415551327169\n",
            "step: 180, loss: 0.0016777014825493097\n",
            "step: 190, loss: 0.0003110935213044286\n",
            "step: 200, loss: 0.05023506283760071\n",
            "step: 210, loss: 8.921284461393952e-05\n",
            "step: 220, loss: 0.00028760521672666073\n",
            "step: 230, loss: 0.00110783486161381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9797752808988766, f1=0.9808342728297633, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020961380505468696\n",
            "step: 10, loss: 0.0007864487706683576\n",
            "step: 20, loss: 0.00016995955957099795\n",
            "step: 30, loss: 0.000157848815433681\n",
            "step: 40, loss: 0.0009129546815529466\n",
            "step: 50, loss: 0.0005225837812758982\n",
            "step: 60, loss: 0.00013474187289830297\n",
            "step: 70, loss: 0.00031297776149585843\n",
            "step: 80, loss: 0.00016347142809536308\n",
            "step: 90, loss: 0.00018078525317832828\n",
            "step: 100, loss: 0.00039657403249293566\n",
            "step: 110, loss: 0.001632506842724979\n",
            "step: 120, loss: 0.014970698393881321\n",
            "step: 130, loss: 0.000407403422286734\n",
            "step: 140, loss: 0.00011030719906557351\n",
            "step: 150, loss: 0.000184784788871184\n",
            "step: 160, loss: 0.00030136900022625923\n",
            "step: 170, loss: 0.00020341775962151587\n",
            "step: 180, loss: 0.0010131543967872858\n",
            "step: 190, loss: 0.0003966558142565191\n",
            "step: 200, loss: 0.0003735109930858016\n",
            "step: 210, loss: 0.0027247900143265724\n",
            "step: 220, loss: 0.00017812916485127062\n",
            "step: 230, loss: 0.00027660245541483164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9798657718120806, f1=0.9720670391061451, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.050940308137797e-05\n",
            "step: 10, loss: 0.0006292357575148344\n",
            "step: 20, loss: 0.0006247548153623939\n",
            "step: 30, loss: 0.00010151943570235744\n",
            "step: 40, loss: 0.015263205394148827\n",
            "step: 50, loss: 0.00010198281233897433\n",
            "step: 60, loss: 0.0001157305741799064\n",
            "step: 70, loss: 0.0003742183034773916\n",
            "step: 80, loss: 8.476443326799199e-05\n",
            "step: 90, loss: 0.0003033253306057304\n",
            "step: 100, loss: 8.836662163957953e-05\n",
            "step: 110, loss: 4.854464714298956e-05\n",
            "step: 120, loss: 5.6970686273416504e-05\n",
            "step: 130, loss: 0.00012568301463034004\n",
            "step: 140, loss: 6.758780364179984e-05\n",
            "step: 150, loss: 0.0007888160180300474\n",
            "step: 160, loss: 0.00010412138362880796\n",
            "step: 170, loss: 8.265528595075011e-05\n",
            "step: 180, loss: 0.0003145733498968184\n",
            "step: 190, loss: 7.140123489079997e-05\n",
            "step: 200, loss: 0.02908698283135891\n",
            "step: 210, loss: 0.00031279344693757594\n",
            "step: 220, loss: 0.00015698802599217743\n",
            "step: 230, loss: 0.0007589261513203382\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9693530079455165, f1=0.9670079635949943, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004629859700798988\n",
            "step: 10, loss: 0.00010740024299593642\n",
            "step: 20, loss: 0.01873115636408329\n",
            "step: 30, loss: 0.007914287969470024\n",
            "step: 40, loss: 0.00011224202899029478\n",
            "step: 50, loss: 0.00015288361464627087\n",
            "step: 60, loss: 0.00028402928728610277\n",
            "step: 70, loss: 0.00026881915982812643\n",
            "step: 80, loss: 8.747303218115121e-05\n",
            "step: 90, loss: 0.00023022257664706558\n",
            "step: 100, loss: 0.00015536582213826478\n",
            "step: 110, loss: 0.00014835991896688938\n",
            "step: 120, loss: 0.00013886881060898304\n",
            "step: 130, loss: 0.0001272500230697915\n",
            "step: 140, loss: 0.029072681441903114\n",
            "step: 150, loss: 0.0181154552847147\n",
            "step: 160, loss: 0.00013514513557311147\n",
            "step: 170, loss: 0.00012425243039615452\n",
            "step: 180, loss: 0.00023261395108420402\n",
            "step: 190, loss: 0.0036469323094934225\n",
            "step: 200, loss: 7.271322829183191e-05\n",
            "step: 210, loss: 0.0001451301359338686\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 220, loss: 0.00010202536213910207\n",
            "step: 230, loss: 0.00027659436454996467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9777777777777777, f1=0.966740576496674, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.602574674412608e-05\n",
            "step: 10, loss: 0.00018021016148850322\n",
            "step: 20, loss: 0.00035723194014281034\n",
            "step: 30, loss: 0.009828047826886177\n",
            "step: 40, loss: 0.0002799040521495044\n",
            "step: 50, loss: 0.0003114357532467693\n",
            "step: 60, loss: 0.003737029619514942\n",
            "step: 70, loss: 6.803627911722288e-05\n",
            "step: 80, loss: 4.711929432232864e-05\n",
            "step: 90, loss: 6.887591734994203e-05\n",
            "step: 100, loss: 6.47140623186715e-05\n",
            "step: 110, loss: 0.00028062055935151875\n",
            "step: 120, loss: 0.00040006765630096197\n",
            "step: 130, loss: 0.0006111276452429593\n",
            "step: 140, loss: 0.00010312644735677168\n",
            "step: 150, loss: 0.0005037516821175814\n",
            "step: 160, loss: 0.0001942618255270645\n",
            "step: 170, loss: 9.726430289447308e-05\n",
            "step: 180, loss: 0.00014346730313263834\n",
            "step: 190, loss: 7.044243830023333e-05\n",
            "step: 200, loss: 0.001570221851579845\n",
            "step: 210, loss: 6.169363769004121e-05\n",
            "step: 220, loss: 6.233790918486193e-05\n",
            "step: 230, loss: 0.00011906477448064834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9755555555555556, f1=0.972129319955407, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012736314965877682\n",
            "step: 10, loss: 5.9767611674033105e-05\n",
            "step: 20, loss: 0.0003135038714390248\n",
            "step: 30, loss: 0.009522058069705963\n",
            "step: 40, loss: 0.00022044072102289647\n",
            "step: 50, loss: 0.008664265275001526\n",
            "step: 60, loss: 0.00030162956682033837\n",
            "step: 70, loss: 0.05599142983555794\n",
            "step: 80, loss: 0.000851520337164402\n",
            "step: 90, loss: 0.0003932616382371634\n",
            "step: 100, loss: 3.8677975680911914e-05\n",
            "step: 110, loss: 0.0001750697410898283\n",
            "step: 120, loss: 0.00011925496073672548\n",
            "step: 130, loss: 7.831188122509047e-05\n",
            "step: 140, loss: 0.00013540392683353275\n",
            "step: 150, loss: 0.0011816996848210692\n",
            "step: 160, loss: 0.00012989800598006696\n",
            "step: 170, loss: 0.00011658173752948642\n",
            "step: 180, loss: 0.010694917291402817\n",
            "step: 190, loss: 0.0012424761662259698\n",
            "step: 200, loss: 3.991057019447908e-05\n",
            "step: 210, loss: 6.220045906957239e-05\n",
            "step: 220, loss: 0.00018148228991776705\n",
            "step: 230, loss: 0.037205133587121964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9810901001112348, f1=0.9721913236929923, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021827539603691548\n",
            "step: 10, loss: 0.00015335493662860245\n",
            "step: 20, loss: 0.00026702918694354594\n",
            "step: 30, loss: 0.00010478722106199712\n",
            "step: 40, loss: 0.00015158338646870106\n",
            "step: 50, loss: 0.00013600016245618463\n",
            "step: 60, loss: 9.782104461919516e-05\n",
            "step: 70, loss: 7.262768485816196e-05\n",
            "step: 80, loss: 8.315870945807546e-05\n",
            "step: 90, loss: 0.00013365919585339725\n",
            "step: 100, loss: 7.647228630958125e-05\n",
            "step: 110, loss: 0.007331752683967352\n",
            "step: 120, loss: 0.0029432105366140604\n",
            "step: 130, loss: 0.00013222689449321479\n",
            "step: 140, loss: 7.683486910536885e-05\n",
            "step: 150, loss: 7.725120667601004e-05\n",
            "step: 160, loss: 0.0001354548439849168\n",
            "step: 170, loss: 0.00012646609684452415\n",
            "step: 180, loss: 0.00010163323167944327\n",
            "step: 190, loss: 0.00010486526298336685\n",
            "step: 200, loss: 0.0002479152462910861\n",
            "step: 210, loss: 3.591406129999086e-05\n",
            "step: 220, loss: 5.959095506113954e-05\n",
            "step: 230, loss: 0.0004647393070627004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.98, f1=0.9700332963374029, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.924672925379127e-05\n",
            "step: 10, loss: 0.00039316085167229176\n",
            "step: 20, loss: 0.000180906688910909\n",
            "step: 30, loss: 0.00010733016824815422\n",
            "step: 40, loss: 0.036153942346572876\n",
            "step: 50, loss: 0.00012708500435110182\n",
            "step: 60, loss: 6.877809209981933e-05\n",
            "step: 70, loss: 6.0997397667961195e-05\n",
            "step: 80, loss: 6.352281343424693e-05\n",
            "step: 90, loss: 8.134428935591131e-05\n",
            "step: 100, loss: 5.231839168118313e-05\n",
            "step: 110, loss: 0.0004044450761284679\n",
            "step: 120, loss: 0.00013691715139430016\n",
            "step: 130, loss: 7.008054672041908e-05\n",
            "step: 140, loss: 9.600788325769827e-05\n",
            "step: 150, loss: 5.1364120736252517e-05\n",
            "step: 160, loss: 0.0048683746717870235\n",
            "step: 170, loss: 5.644865086651407e-05\n",
            "step: 180, loss: 7.7332035289146e-05\n",
            "step: 190, loss: 0.0005010411259718239\n",
            "step: 200, loss: 6.230977305676788e-05\n",
            "step: 210, loss: 0.0012249964056536555\n",
            "step: 220, loss: 8.422701648669317e-05\n",
            "step: 230, loss: 0.0003696857311297208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.98, f1=0.9732739420935412, best_f1=0.980963045912654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.602186527336016e-05\n",
            "step: 10, loss: 7.605290738865733e-05\n",
            "step: 20, loss: 6.7739216319751e-05\n",
            "step: 30, loss: 9.182745270663872e-05\n",
            "step: 40, loss: 0.0002995871764142066\n",
            "step: 50, loss: 0.02182852104306221\n",
            "step: 60, loss: 9.539919847156852e-05\n",
            "step: 70, loss: 6.849571946077049e-05\n",
            "step: 80, loss: 0.00011994559463346377\n",
            "step: 90, loss: 0.00011178627755725756\n",
            "step: 100, loss: 6.249373109312728e-05\n",
            "step: 110, loss: 0.0001728535135043785\n",
            "step: 120, loss: 6.427001062547788e-05\n",
            "step: 130, loss: 0.00012016308028250933\n",
            "step: 140, loss: 5.535289164981805e-05\n",
            "step: 150, loss: 0.00022514123702421784\n",
            "step: 160, loss: 3.758602906600572e-05\n",
            "step: 170, loss: 3.2021293009165674e-05\n",
            "step: 180, loss: 0.0008234437555074692\n",
            "step: 190, loss: 0.0007194034405983984\n",
            "step: 200, loss: 0.00012264354154467583\n",
            "step: 210, loss: 9.003075683722273e-05\n",
            "step: 220, loss: 0.0002166960621252656\n",
            "step: 230, loss: 6.168849358800799e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9777777777777777, f1=0.9733333333333333, best_f1=0.980963045912654\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 188.91it/s]\n",
            "load_f1 = 0.9821428571428571\n",
            "real_f1 = 0.9821428571428571\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 225.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485f7461-321f-44f4-cae1-9c67c8b21d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 504kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.22MB/s]\n",
            "Downloading: 100% 440M/440M [00:11<00:00, 37.0MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.620211660861969\n",
            "step: 10, loss: 0.5198891162872314\n",
            "step: 20, loss: 0.45027828216552734\n",
            "step: 30, loss: 0.047647327184677124\n",
            "step: 40, loss: 0.1682967096567154\n",
            "step: 50, loss: 0.09712085127830505\n",
            "step: 60, loss: 0.05414538457989693\n",
            "step: 70, loss: 0.09050697833299637\n",
            "step: 80, loss: 0.07514294236898422\n",
            "step: 90, loss: 0.35679978132247925\n",
            "step: 100, loss: 0.013281605206429958\n",
            "step: 110, loss: 0.03360165283083916\n",
            "step: 120, loss: 0.13389691710472107\n",
            "step: 130, loss: 0.03440094739198685\n",
            "step: 140, loss: 0.06585576385259628\n",
            "step: 150, loss: 0.04540632292628288\n",
            "step: 160, loss: 0.10813866555690765\n",
            "step: 170, loss: 0.1983003467321396\n",
            "step: 180, loss: 0.05463750287890434\n",
            "step: 190, loss: 0.01997894048690796\n",
            "step: 200, loss: 0.29899823665618896\n",
            "step: 210, loss: 0.0779263824224472\n",
            "step: 220, loss: 0.15488319098949432\n",
            "step: 230, loss: 0.15293848514556885\n",
            "step: 240, loss: 0.07385851442813873\n",
            "step: 250, loss: 0.01021682471036911\n",
            "step: 260, loss: 0.024034777656197548\n",
            "step: 270, loss: 0.011711493134498596\n",
            "step: 280, loss: 0.043403640389442444\n",
            "step: 290, loss: 0.1181909367442131\n",
            "step: 300, loss: 0.021355193108320236\n",
            "step: 310, loss: 0.2815183401107788\n",
            "step: 320, loss: 0.11297283321619034\n",
            "step: 330, loss: 0.032392971217632294\n",
            "step: 340, loss: 0.034369587898254395\n",
            "step: 350, loss: 0.011839590966701508\n",
            "step: 360, loss: 0.024117687717080116\n",
            "step: 370, loss: 0.09739982336759567\n",
            "step: 380, loss: 0.03245289623737335\n",
            "step: 390, loss: 0.1633678376674652\n",
            "step: 400, loss: 0.22218602895736694\n",
            "step: 410, loss: 0.029796451330184937\n",
            "step: 420, loss: 0.029682034626603127\n",
            "step: 430, loss: 0.15330614149570465\n",
            "step: 440, loss: 0.03440922871232033\n",
            "step: 450, loss: 0.015446494333446026\n",
            "step: 460, loss: 0.004424413666129112\n",
            "step: 470, loss: 0.1311386376619339\n",
            "step: 480, loss: 0.14359542727470398\n",
            "step: 490, loss: 0.16655930876731873\n",
            "step: 500, loss: 0.04612942785024643\n",
            "step: 510, loss: 0.08438608795404434\n",
            "step: 520, loss: 0.14986580610275269\n",
            "step: 530, loss: 0.01488204300403595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9405255878284925, f1=0.933579335793358, best_f1=0.933579335793358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05233093723654747\n",
            "step: 10, loss: 0.03890660032629967\n",
            "step: 20, loss: 0.14259470999240875\n",
            "step: 30, loss: 0.07224797457456589\n",
            "step: 40, loss: 0.15989993512630463\n",
            "step: 50, loss: 0.14552603662014008\n",
            "step: 60, loss: 0.0031328722834587097\n",
            "step: 70, loss: 0.02255094237625599\n",
            "step: 80, loss: 0.13471809029579163\n",
            "step: 90, loss: 0.01982971280813217\n",
            "step: 100, loss: 0.024042408913373947\n",
            "step: 110, loss: 0.06167444586753845\n",
            "step: 120, loss: 0.08233251422643661\n",
            "step: 130, loss: 0.10822433978319168\n",
            "step: 140, loss: 0.050933413207530975\n",
            "step: 150, loss: 0.05634232982993126\n",
            "step: 160, loss: 0.010077916085720062\n",
            "step: 170, loss: 0.015591628849506378\n",
            "step: 180, loss: 0.023225123062729836\n",
            "step: 190, loss: 0.05839761346578598\n",
            "step: 200, loss: 0.002501139650121331\n",
            "step: 210, loss: 0.013156593777239323\n",
            "step: 220, loss: 0.09003039449453354\n",
            "step: 230, loss: 0.005798471625894308\n",
            "step: 240, loss: 0.05563482642173767\n",
            "step: 250, loss: 0.07933655381202698\n",
            "step: 260, loss: 0.003773412201553583\n",
            "step: 270, loss: 0.20738691091537476\n",
            "step: 280, loss: 0.015832656994462013\n",
            "step: 290, loss: 0.11436371505260468\n",
            "step: 300, loss: 0.15776051580905914\n",
            "step: 310, loss: 0.02166842855513096\n",
            "step: 320, loss: 0.09662068635225296\n",
            "step: 330, loss: 0.08461590111255646\n",
            "step: 340, loss: 0.031808797270059586\n",
            "step: 350, loss: 0.0019538283813744783\n",
            "step: 360, loss: 0.08465400338172913\n",
            "step: 370, loss: 0.0708480179309845\n",
            "step: 380, loss: 0.04469585791230202\n",
            "step: 390, loss: 0.07389258593320847\n",
            "step: 400, loss: 0.01096771378070116\n",
            "step: 410, loss: 0.030044084414839745\n",
            "step: 420, loss: 0.010866274125874043\n",
            "step: 430, loss: 0.028429223224520683\n",
            "step: 440, loss: 0.19941022992134094\n",
            "step: 450, loss: 0.046408236026763916\n",
            "step: 460, loss: 0.051631860435009\n",
            "step: 470, loss: 0.09856204688549042\n",
            "step: 480, loss: 0.3055647015571594\n",
            "step: 490, loss: 0.009990720078349113\n",
            "step: 500, loss: 0.23805655539035797\n",
            "step: 510, loss: 0.02696511335670948\n",
            "step: 520, loss: 0.12696263194084167\n",
            "step: 530, loss: 0.009725195355713367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9481000926784059, f1=0.9450853714813106, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11234118044376373\n",
            "step: 10, loss: 0.04636390507221222\n",
            "step: 20, loss: 0.12940388917922974\n",
            "step: 30, loss: 0.0727221816778183\n",
            "step: 40, loss: 0.08530095964670181\n",
            "step: 50, loss: 0.15657086670398712\n",
            "step: 60, loss: 0.08394121378660202\n",
            "step: 70, loss: 0.009101087227463722\n",
            "step: 80, loss: 0.006055274046957493\n",
            "step: 90, loss: 0.00598114775493741\n",
            "step: 100, loss: 0.11693280935287476\n",
            "step: 110, loss: 0.004659219179302454\n",
            "step: 120, loss: 0.0252920500934124\n",
            "step: 130, loss: 0.006570320576429367\n",
            "step: 140, loss: 0.03538621962070465\n",
            "step: 150, loss: 0.01615666225552559\n",
            "step: 160, loss: 0.009945645928382874\n",
            "step: 170, loss: 0.07527970522642136\n",
            "step: 180, loss: 0.006470350082963705\n",
            "step: 190, loss: 0.017423491925001144\n",
            "step: 200, loss: 0.11826492846012115\n",
            "step: 210, loss: 0.06324555724859238\n",
            "step: 220, loss: 0.08644970506429672\n",
            "step: 230, loss: 0.11784248054027557\n",
            "step: 240, loss: 0.010364744812250137\n",
            "step: 250, loss: 0.06098647788167\n",
            "step: 260, loss: 0.012140237726271152\n",
            "step: 270, loss: 0.04539098963141441\n",
            "step: 280, loss: 0.12159612774848938\n",
            "step: 290, loss: 0.005863488651812077\n",
            "step: 300, loss: 0.0076057445257902145\n",
            "step: 310, loss: 0.006593648809939623\n",
            "step: 320, loss: 0.00770585285499692\n",
            "step: 330, loss: 0.006333013065159321\n",
            "step: 340, loss: 0.006145872641354799\n",
            "step: 350, loss: 0.0010060049826279283\n",
            "step: 360, loss: 0.04693809151649475\n",
            "step: 370, loss: 0.04633662477135658\n",
            "step: 380, loss: 0.05419714003801346\n",
            "step: 390, loss: 0.13953787088394165\n",
            "step: 400, loss: 0.022263377904891968\n",
            "step: 410, loss: 0.005117344204336405\n",
            "step: 420, loss: 0.14837293326854706\n",
            "step: 430, loss: 0.03004787489771843\n",
            "step: 440, loss: 0.0876847505569458\n",
            "step: 450, loss: 0.08346634358167648\n",
            "step: 460, loss: 0.06339360028505325\n",
            "step: 470, loss: 0.03136089816689491\n",
            "step: 480, loss: 0.010180804878473282\n",
            "step: 490, loss: 0.016000838950276375\n",
            "step: 500, loss: 0.010124248452484608\n",
            "step: 510, loss: 0.002556161256507039\n",
            "step: 520, loss: 0.2068503201007843\n",
            "step: 530, loss: 0.035690803080797195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9480093676814988, f1=0.9351376574895007, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007651783525943756\n",
            "step: 10, loss: 0.007747984491288662\n",
            "step: 20, loss: 0.003798985853791237\n",
            "step: 30, loss: 0.005484074354171753\n",
            "step: 40, loss: 0.0006829002522863448\n",
            "step: 50, loss: 0.0024147944059222937\n",
            "step: 60, loss: 0.11154317855834961\n",
            "step: 70, loss: 0.010034816339612007\n",
            "step: 80, loss: 0.02163839153945446\n",
            "step: 90, loss: 0.01650879718363285\n",
            "step: 100, loss: 0.01368021685630083\n",
            "step: 110, loss: 0.026085462421178818\n",
            "step: 120, loss: 0.0002282242785440758\n",
            "step: 130, loss: 0.011059500277042389\n",
            "step: 140, loss: 0.0008595511317253113\n",
            "step: 150, loss: 0.010360253043472767\n",
            "step: 160, loss: 0.13086742162704468\n",
            "step: 170, loss: 0.0028291363269090652\n",
            "step: 180, loss: 0.002287530107423663\n",
            "step: 190, loss: 0.00222400831989944\n",
            "step: 200, loss: 0.04207802191376686\n",
            "step: 210, loss: 0.00724708940833807\n",
            "step: 220, loss: 0.0020380914211273193\n",
            "step: 230, loss: 0.13252609968185425\n",
            "step: 240, loss: 0.020757384598255157\n",
            "step: 250, loss: 0.038554392755031586\n",
            "step: 260, loss: 0.0910162478685379\n",
            "step: 270, loss: 0.01984843611717224\n",
            "step: 280, loss: 0.02860099822282791\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 290, loss: 0.15569733083248138\n",
            "step: 300, loss: 0.000856010417919606\n",
            "step: 310, loss: 0.00472077913582325\n",
            "step: 320, loss: 0.04289701581001282\n",
            "step: 330, loss: 0.03074340894818306\n",
            "step: 340, loss: 0.1381680965423584\n",
            "step: 350, loss: 0.024507543072104454\n",
            "step: 360, loss: 0.017858272418379784\n",
            "step: 370, loss: 0.0031684692949056625\n",
            "step: 380, loss: 0.0039327326230704784\n",
            "step: 390, loss: 0.0035052443854510784\n",
            "step: 400, loss: 0.006329922936856747\n",
            "step: 410, loss: 0.017280766740441322\n",
            "step: 420, loss: 0.005191698670387268\n",
            "step: 430, loss: 0.035024404525756836\n",
            "step: 440, loss: 0.0018705211114138365\n",
            "step: 450, loss: 0.002539150882512331\n",
            "step: 460, loss: 0.000718166003935039\n",
            "step: 470, loss: 0.016735989600419998\n",
            "step: 480, loss: 0.18793798983097076\n",
            "step: 490, loss: 0.006689141504466534\n",
            "step: 500, loss: 0.03681725636124611\n",
            "step: 510, loss: 0.040876604616642\n",
            "step: 520, loss: 0.0719016045331955\n",
            "step: 530, loss: 0.012250687927007675\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9475638051044084, f1=0.9313543599257884, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15635819733142853\n",
            "step: 10, loss: 0.06100168824195862\n",
            "step: 20, loss: 0.021887755021452904\n",
            "step: 30, loss: 0.004180803429335356\n",
            "step: 40, loss: 0.10453283041715622\n",
            "step: 50, loss: 0.0015420898562297225\n",
            "step: 60, loss: 0.08831655234098434\n",
            "step: 70, loss: 0.008313958533108234\n",
            "step: 80, loss: 0.0028694516513496637\n",
            "step: 90, loss: 0.007013029418885708\n",
            "step: 100, loss: 0.024081232026219368\n",
            "step: 110, loss: 0.0008686897926963866\n",
            "step: 120, loss: 0.006341390777379274\n",
            "step: 130, loss: 0.0006233368767425418\n",
            "step: 140, loss: 0.002528228797018528\n",
            "step: 150, loss: 0.011857670731842518\n",
            "step: 160, loss: 0.0013863000785931945\n",
            "step: 170, loss: 0.015474239364266396\n",
            "step: 180, loss: 0.0007275070529431105\n",
            "step: 190, loss: 0.004140516743063927\n",
            "step: 200, loss: 0.0013800282031297684\n",
            "step: 210, loss: 0.004275189712643623\n",
            "step: 220, loss: 0.02035343274474144\n",
            "step: 230, loss: 0.03688102215528488\n",
            "step: 240, loss: 0.003670687787234783\n",
            "step: 250, loss: 0.002661708975210786\n",
            "step: 260, loss: 0.006824732292443514\n",
            "step: 270, loss: 0.0006044851616024971\n",
            "step: 280, loss: 0.003905388992279768\n",
            "step: 290, loss: 0.1552412062883377\n",
            "step: 300, loss: 0.030453016981482506\n",
            "step: 310, loss: 0.0012260830262675881\n",
            "step: 320, loss: 0.11858335137367249\n",
            "step: 330, loss: 0.014666816219687462\n",
            "step: 340, loss: 0.004011617973446846\n",
            "step: 350, loss: 0.00295799458399415\n",
            "step: 360, loss: 0.0035473997704684734\n",
            "step: 370, loss: 0.016402091830968857\n",
            "step: 380, loss: 0.0016930276760831475\n",
            "step: 390, loss: 0.0002879010571632534\n",
            "step: 400, loss: 0.006458886433392763\n",
            "step: 410, loss: 0.02234538644552231\n",
            "step: 420, loss: 0.0035137375816702843\n",
            "step: 430, loss: 0.0037326260935515165\n",
            "step: 440, loss: 0.013662921264767647\n",
            "step: 450, loss: 0.01216923538595438\n",
            "step: 460, loss: 0.0008224134799093008\n",
            "step: 470, loss: 0.0009308111621066928\n",
            "step: 480, loss: 0.006741562392562628\n",
            "step: 490, loss: 0.001164212473668158\n",
            "step: 500, loss: 0.013335928320884705\n",
            "step: 510, loss: 0.16832800209522247\n",
            "step: 520, loss: 0.0021908460184931755\n",
            "step: 530, loss: 0.014894774183630943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9397477814105558, f1=0.9335810496980957, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020952897612005472\n",
            "step: 10, loss: 0.0001165009816759266\n",
            "step: 20, loss: 0.1167994812130928\n",
            "step: 30, loss: 0.005653956905007362\n",
            "step: 40, loss: 0.033457208424806595\n",
            "step: 50, loss: 0.025357132777571678\n",
            "step: 60, loss: 0.0008899547392502427\n",
            "step: 70, loss: 0.004138217307627201\n",
            "step: 80, loss: 0.008831402286887169\n",
            "step: 90, loss: 0.010356949642300606\n",
            "step: 100, loss: 0.06344641000032425\n",
            "step: 110, loss: 0.0012862951261922717\n",
            "step: 120, loss: 0.004321553744375706\n",
            "step: 130, loss: 0.031562160700559616\n",
            "step: 140, loss: 0.0075373174622654915\n",
            "step: 150, loss: 0.0009218697668984532\n",
            "step: 160, loss: 0.0014183638850226998\n",
            "step: 170, loss: 0.00933071505278349\n",
            "step: 180, loss: 0.0027152537368237972\n",
            "step: 190, loss: 0.0023836204782128334\n",
            "step: 200, loss: 0.028400126844644547\n",
            "step: 210, loss: 0.029840189963579178\n",
            "step: 220, loss: 0.024125395342707634\n",
            "step: 230, loss: 0.0003351588675286621\n",
            "step: 240, loss: 0.08970962464809418\n",
            "step: 250, loss: 0.029142489656805992\n",
            "step: 260, loss: 0.00037351626087911427\n",
            "step: 270, loss: 0.08172225952148438\n",
            "step: 280, loss: 0.025287680327892303\n",
            "step: 290, loss: 0.001000813441351056\n",
            "step: 300, loss: 0.0009330813190899789\n",
            "step: 310, loss: 0.0010529081337153912\n",
            "step: 320, loss: 0.001612629508599639\n",
            "step: 330, loss: 0.011041772551834583\n",
            "step: 340, loss: 0.02143397368490696\n",
            "step: 350, loss: 0.12683618068695068\n",
            "step: 360, loss: 0.02313319779932499\n",
            "step: 370, loss: 0.003705115057528019\n",
            "step: 380, loss: 0.0011049644090235233\n",
            "step: 390, loss: 0.037256572395563126\n",
            "step: 400, loss: 5.75034755456727e-05\n",
            "step: 410, loss: 0.00041484017856419086\n",
            "step: 420, loss: 0.015536496415734291\n",
            "step: 430, loss: 0.0006996538140811026\n",
            "step: 440, loss: 0.0076814196072518826\n",
            "step: 450, loss: 0.007169710472226143\n",
            "step: 460, loss: 0.00036499349516816437\n",
            "step: 470, loss: 0.007848951034247875\n",
            "step: 480, loss: 0.002770141465589404\n",
            "step: 490, loss: 0.011443265713751316\n",
            "step: 500, loss: 0.009670421481132507\n",
            "step: 510, loss: 0.0033660114277154207\n",
            "step: 520, loss: 0.026807893067598343\n",
            "step: 530, loss: 0.0049149454571306705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9412317818523741, f1=0.9330819981149858, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022482167929410934\n",
            "step: 10, loss: 0.0024709461722522974\n",
            "step: 20, loss: 0.00016023374337237328\n",
            "step: 30, loss: 0.006644922774285078\n",
            "step: 40, loss: 0.03064381517469883\n",
            "step: 50, loss: 0.004434865899384022\n",
            "step: 60, loss: 0.035442955791950226\n",
            "step: 70, loss: 0.00022609240841120481\n",
            "step: 80, loss: 0.001880955183878541\n",
            "step: 90, loss: 0.0072547332383692265\n",
            "step: 100, loss: 0.0003054914704989642\n",
            "step: 110, loss: 0.001045950804837048\n",
            "step: 120, loss: 0.0015737281646579504\n",
            "step: 130, loss: 0.004815349821001291\n",
            "step: 140, loss: 0.0003389579651411623\n",
            "step: 150, loss: 0.0011819397332146764\n",
            "step: 160, loss: 0.0007262652507051826\n",
            "step: 170, loss: 0.0005231171380728483\n",
            "step: 180, loss: 0.0019402956822887063\n",
            "step: 190, loss: 0.001327711739577353\n",
            "step: 200, loss: 0.00391562283039093\n",
            "step: 210, loss: 0.0028987161349505186\n",
            "step: 220, loss: 0.00014260713942348957\n",
            "step: 230, loss: 0.01009590644389391\n",
            "step: 240, loss: 0.00029521732358261943\n",
            "step: 250, loss: 0.004386547952890396\n",
            "step: 260, loss: 0.0013234559446573257\n",
            "step: 270, loss: 0.04652772843837738\n",
            "step: 280, loss: 0.044174496084451675\n",
            "step: 290, loss: 0.0015796617371961474\n",
            "step: 300, loss: 0.005700692068785429\n",
            "step: 310, loss: 0.08769163489341736\n",
            "step: 320, loss: 0.000547171977814287\n",
            "step: 330, loss: 0.00010505871614441276\n",
            "step: 340, loss: 0.01709018275141716\n",
            "step: 350, loss: 0.003822882194072008\n",
            "step: 360, loss: 0.002357610734179616\n",
            "step: 370, loss: 0.003056602319702506\n",
            "step: 380, loss: 0.005993092432618141\n",
            "step: 390, loss: 0.0004042880900669843\n",
            "step: 400, loss: 0.017517801374197006\n",
            "step: 410, loss: 0.001983063528314233\n",
            "step: 420, loss: 0.00020660743757616729\n",
            "step: 430, loss: 0.0037167591508477926\n",
            "step: 440, loss: 0.0003689056320581585\n",
            "step: 450, loss: 0.000530458812136203\n",
            "step: 460, loss: 0.0007567715365439653\n",
            "step: 470, loss: 0.03367133066058159\n",
            "step: 480, loss: 0.0005064456490799785\n",
            "step: 490, loss: 0.000643103092443198\n",
            "step: 500, loss: 0.0014547297032549977\n",
            "step: 510, loss: 0.033362288028001785\n",
            "step: 520, loss: 0.03194228187203407\n",
            "step: 530, loss: 0.03723897784948349\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9294062646096307, f1=0.9346314325452018, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0944775715470314\n",
            "step: 10, loss: 0.04301105812191963\n",
            "step: 20, loss: 0.0002026785077759996\n",
            "step: 30, loss: 0.029201284050941467\n",
            "step: 40, loss: 0.0006061961757950485\n",
            "step: 50, loss: 0.003000528085976839\n",
            "step: 60, loss: 0.0008799658389762044\n",
            "step: 70, loss: 0.0008312500431202352\n",
            "step: 80, loss: 0.059782978147268295\n",
            "step: 90, loss: 0.0007825592765584588\n",
            "step: 100, loss: 0.00021206731616985053\n",
            "step: 110, loss: 0.0003167421091347933\n",
            "step: 120, loss: 0.0024671361315995455\n",
            "step: 130, loss: 0.0002670924295671284\n",
            "step: 140, loss: 0.005584940779954195\n",
            "step: 150, loss: 0.009966854006052017\n",
            "step: 160, loss: 0.010742926970124245\n",
            "step: 170, loss: 0.05819888412952423\n",
            "step: 180, loss: 0.00044966363930143416\n",
            "step: 190, loss: 0.0023438928183168173\n",
            "step: 200, loss: 0.0022510201670229435\n",
            "step: 210, loss: 0.010976525023579597\n",
            "step: 220, loss: 0.0018878328846767545\n",
            "step: 230, loss: 0.002847790950909257\n",
            "step: 240, loss: 0.0020915400236845016\n",
            "step: 250, loss: 0.00011150935461046174\n",
            "step: 260, loss: 0.00010897697211476043\n",
            "step: 270, loss: 0.023271549493074417\n",
            "step: 280, loss: 0.013420699164271355\n",
            "step: 290, loss: 0.005202204920351505\n",
            "step: 300, loss: 0.007139430847018957\n",
            "step: 310, loss: 0.04702724143862724\n",
            "step: 320, loss: 0.000394322705687955\n",
            "step: 330, loss: 0.0031687230803072453\n",
            "step: 340, loss: 0.00015725824050605297\n",
            "step: 350, loss: 9.437783592147753e-05\n",
            "step: 360, loss: 0.0014343311777338386\n",
            "step: 370, loss: 0.001464056666009128\n",
            "step: 380, loss: 0.0017697543371468782\n",
            "step: 390, loss: 0.10871636122465134\n",
            "step: 400, loss: 0.002992029767483473\n",
            "step: 410, loss: 0.0002237840963061899\n",
            "step: 420, loss: 0.0016152530442923307\n",
            "step: 430, loss: 0.04361981898546219\n",
            "step: 440, loss: 0.15147076547145844\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 450, loss: 0.00045853445772081614\n",
            "step: 460, loss: 0.00012239234638400376\n",
            "step: 470, loss: 0.0001554545306134969\n",
            "step: 480, loss: 0.00012199379125377163\n",
            "step: 490, loss: 0.0642336905002594\n",
            "step: 500, loss: 0.007057225797325373\n",
            "step: 510, loss: 0.0009908683132380247\n",
            "step: 520, loss: 0.004507166333496571\n",
            "step: 530, loss: 0.03152082487940788\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9406264609630668, f1=0.931098696461825, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026218879502266645\n",
            "step: 10, loss: 0.012796865776181221\n",
            "step: 20, loss: 0.030078284442424774\n",
            "step: 30, loss: 0.0026625508908182383\n",
            "step: 40, loss: 0.04324067011475563\n",
            "step: 50, loss: 0.0005632779793813825\n",
            "step: 60, loss: 0.000123669917229563\n",
            "step: 70, loss: 0.017914097756147385\n",
            "step: 80, loss: 0.04358018562197685\n",
            "step: 90, loss: 2.513361323508434e-05\n",
            "step: 100, loss: 0.00108260044362396\n",
            "step: 110, loss: 5.672048791893758e-05\n",
            "step: 120, loss: 0.00040986062958836555\n",
            "step: 130, loss: 0.011427802965044975\n",
            "step: 140, loss: 0.0016254113288596272\n",
            "step: 150, loss: 0.0011317436583340168\n",
            "step: 160, loss: 0.003565968247130513\n",
            "step: 170, loss: 0.00959751009941101\n",
            "step: 180, loss: 0.00043484187335707247\n",
            "step: 190, loss: 0.0008132922230288386\n",
            "step: 200, loss: 0.002758611924946308\n",
            "step: 210, loss: 0.0005767885013483465\n",
            "step: 220, loss: 0.0002120180579368025\n",
            "step: 230, loss: 0.00015060383884701878\n",
            "step: 240, loss: 0.00017289008246734738\n",
            "step: 250, loss: 0.0019873876590281725\n",
            "step: 260, loss: 0.00676176929846406\n",
            "step: 270, loss: 7.412239210680127e-05\n",
            "step: 280, loss: 0.008826700039207935\n",
            "step: 290, loss: 0.04201119765639305\n",
            "step: 300, loss: 2.049993236141745e-05\n",
            "step: 310, loss: 0.001967252930626273\n",
            "step: 320, loss: 0.00014866302080918103\n",
            "step: 330, loss: 0.00043477071449160576\n",
            "step: 340, loss: 3.194507007719949e-05\n",
            "step: 350, loss: 0.00012912835518363863\n",
            "step: 360, loss: 2.232896804343909e-05\n",
            "step: 370, loss: 0.00680931843817234\n",
            "step: 380, loss: 0.00030128820799291134\n",
            "step: 390, loss: 0.0009672027663327754\n",
            "step: 400, loss: 0.2944793403148651\n",
            "step: 410, loss: 0.01987561583518982\n",
            "step: 420, loss: 0.00011578621342778206\n",
            "step: 430, loss: 0.00019630344468168914\n",
            "step: 440, loss: 0.0014861708041280508\n",
            "step: 450, loss: 0.14540298283100128\n",
            "step: 460, loss: 0.015920313075184822\n",
            "step: 470, loss: 0.00034766006865538657\n",
            "step: 480, loss: 0.0028751222416758537\n",
            "step: 490, loss: 0.00701560964807868\n",
            "step: 500, loss: 0.0029450496658682823\n",
            "step: 510, loss: 0.002604010282084346\n",
            "step: 520, loss: 0.0066939787939190865\n",
            "step: 530, loss: 0.00013358409341890365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9427107591988821, f1=0.9355586462679647, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006357435486279428\n",
            "step: 10, loss: 0.00016540945216547698\n",
            "step: 20, loss: 0.0002170888619730249\n",
            "step: 30, loss: 0.00017906824359670281\n",
            "step: 40, loss: 0.0001081281661754474\n",
            "step: 50, loss: 0.00027605213108472526\n",
            "step: 60, loss: 0.00019353328389115632\n",
            "step: 70, loss: 6.892043893458322e-05\n",
            "step: 80, loss: 0.00040257067303173244\n",
            "step: 90, loss: 0.0009665217949077487\n",
            "step: 100, loss: 8.670681563671678e-05\n",
            "step: 110, loss: 0.00024255047901533544\n",
            "step: 120, loss: 9.669257997302338e-05\n",
            "step: 130, loss: 0.0003095285501331091\n",
            "step: 140, loss: 0.06731318682432175\n",
            "step: 150, loss: 0.00042964491876773536\n",
            "step: 160, loss: 0.005575270392000675\n",
            "step: 170, loss: 0.00014516613737214357\n",
            "step: 180, loss: 0.0005161995068192482\n",
            "step: 190, loss: 0.0004933088202960789\n",
            "step: 200, loss: 0.014576783403754234\n",
            "step: 210, loss: 0.0003066952631343156\n",
            "step: 220, loss: 0.0020359728951007128\n",
            "step: 230, loss: 0.001323480624705553\n",
            "step: 240, loss: 0.0006386907771229744\n",
            "step: 250, loss: 0.005727978423237801\n",
            "step: 260, loss: 0.0029564956203103065\n",
            "step: 270, loss: 0.0007371801184490323\n",
            "step: 280, loss: 0.1681644469499588\n",
            "step: 290, loss: 0.00024308194406330585\n",
            "step: 300, loss: 0.0004867469542659819\n",
            "step: 310, loss: 0.00028484579524956644\n",
            "step: 320, loss: 0.0012273021275177598\n",
            "step: 330, loss: 0.0008046728326007724\n",
            "step: 340, loss: 0.013100785203278065\n",
            "step: 350, loss: 0.0006162743084132671\n",
            "step: 360, loss: 0.00028617875068448484\n",
            "step: 370, loss: 0.011348847299814224\n",
            "step: 380, loss: 0.15549376606941223\n",
            "step: 390, loss: 0.000759061542339623\n",
            "step: 400, loss: 0.004417884163558483\n",
            "step: 410, loss: 0.002858918160200119\n",
            "step: 420, loss: 0.0019530714489519596\n",
            "step: 430, loss: 0.0018727956339716911\n",
            "step: 440, loss: 0.0037825305480509996\n",
            "step: 450, loss: 0.0003153980360366404\n",
            "step: 460, loss: 0.0004188393068034202\n",
            "step: 470, loss: 0.0030633520800620317\n",
            "step: 480, loss: 0.00015669876302126795\n",
            "step: 490, loss: 0.0072942255064845085\n",
            "step: 500, loss: 0.00624457560479641\n",
            "step: 510, loss: 0.0129269789904356\n",
            "step: 520, loss: 0.0009807520546019077\n",
            "step: 530, loss: 0.0009609129629097879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9401947148817803, f1=0.9380203515263645, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008667180081829429\n",
            "step: 10, loss: 0.0012947161449119449\n",
            "step: 20, loss: 0.0003835085954051465\n",
            "step: 30, loss: 0.0003316118963994086\n",
            "step: 40, loss: 0.00026611299836076796\n",
            "step: 50, loss: 0.0016156903002411127\n",
            "step: 60, loss: 0.022742297500371933\n",
            "step: 70, loss: 7.336202543228865e-05\n",
            "step: 80, loss: 0.00023996559320949018\n",
            "step: 90, loss: 9.9888929980807e-05\n",
            "step: 100, loss: 0.011858879588544369\n",
            "step: 110, loss: 0.0004087313136551529\n",
            "step: 120, loss: 0.00047113242908380926\n",
            "step: 130, loss: 0.00013162096729502082\n",
            "step: 140, loss: 0.024390222504734993\n",
            "step: 150, loss: 4.097199780517258e-05\n",
            "step: 160, loss: 0.016682149842381477\n",
            "step: 170, loss: 7.242238643812016e-05\n",
            "step: 180, loss: 0.0023731058463454247\n",
            "step: 190, loss: 0.005413692444562912\n",
            "step: 200, loss: 0.004933669697493315\n",
            "step: 210, loss: 0.00021267880219966173\n",
            "step: 220, loss: 0.001396775827743113\n",
            "step: 230, loss: 6.545337964780629e-05\n",
            "step: 240, loss: 0.0005139120039530098\n",
            "step: 250, loss: 0.0015297472709789872\n",
            "step: 260, loss: 0.0001171140611404553\n",
            "step: 270, loss: 0.00034439688897691667\n",
            "step: 280, loss: 7.204111170722172e-05\n",
            "step: 290, loss: 0.0044275084510445595\n",
            "step: 300, loss: 7.456340244971216e-05\n",
            "step: 310, loss: 0.0010948857525363564\n",
            "step: 320, loss: 7.038993499008939e-05\n",
            "step: 330, loss: 0.0001403132191626355\n",
            "step: 340, loss: 0.0010208599269390106\n",
            "step: 350, loss: 0.00012692922609858215\n",
            "step: 360, loss: 5.600827716989443e-05\n",
            "step: 370, loss: 0.0005702206981368363\n",
            "step: 380, loss: 0.00013486365787684917\n",
            "step: 390, loss: 0.00014565762830898166\n",
            "step: 400, loss: 0.001218862715177238\n",
            "step: 410, loss: 0.00022284412989392877\n",
            "step: 420, loss: 5.767217226093635e-05\n",
            "step: 430, loss: 0.0001646404853090644\n",
            "step: 440, loss: 0.0003414677339605987\n",
            "step: 450, loss: 0.000393520895158872\n",
            "step: 460, loss: 0.016283096745610237\n",
            "step: 470, loss: 0.0001964321854757145\n",
            "step: 480, loss: 0.0012698742793872952\n",
            "step: 490, loss: 0.0029962186235934496\n",
            "step: 500, loss: 0.00032954997732304037\n",
            "step: 510, loss: 0.00016539696662221104\n",
            "step: 520, loss: 4.236009772284888e-05\n",
            "step: 530, loss: 0.0004060036444570869\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9404651162790698, f1=0.9296296296296297, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.1562372896587476e-05\n",
            "step: 10, loss: 2.1930114598944783e-05\n",
            "step: 20, loss: 0.04181075468659401\n",
            "step: 30, loss: 0.009841651655733585\n",
            "step: 40, loss: 0.0006829541525803506\n",
            "step: 50, loss: 0.023439079523086548\n",
            "step: 60, loss: 0.010908818803727627\n",
            "step: 70, loss: 0.0001693713420536369\n",
            "step: 80, loss: 5.753394361818209e-05\n",
            "step: 90, loss: 3.4358337870799005e-05\n",
            "step: 100, loss: 0.0001638026733417064\n",
            "step: 110, loss: 6.393976946128532e-05\n",
            "step: 120, loss: 2.3152051653596573e-05\n",
            "step: 130, loss: 0.0012353014899417758\n",
            "step: 140, loss: 0.051887284964323044\n",
            "step: 150, loss: 0.00042665351065807045\n",
            "step: 160, loss: 0.00025774064124561846\n",
            "step: 170, loss: 2.342758307349868e-05\n",
            "step: 180, loss: 0.05178562551736832\n",
            "step: 190, loss: 0.003292984329164028\n",
            "step: 200, loss: 0.0002815847401507199\n",
            "step: 210, loss: 6.60197256365791e-05\n",
            "step: 220, loss: 0.00039583133184351027\n",
            "step: 230, loss: 6.885750917717814e-05\n",
            "step: 240, loss: 4.422353231348097e-05\n",
            "step: 250, loss: 4.1251612856285647e-05\n",
            "step: 260, loss: 7.073012966429815e-05\n",
            "step: 270, loss: 6.686306005576625e-05\n",
            "step: 280, loss: 0.0022519463673233986\n",
            "step: 290, loss: 0.000583428714890033\n",
            "step: 300, loss: 0.0003850914363283664\n",
            "step: 310, loss: 2.7633022909867577e-05\n",
            "step: 320, loss: 2.273110112582799e-05\n",
            "step: 330, loss: 0.00011123695730930194\n",
            "step: 340, loss: 0.00016039705951698124\n",
            "step: 350, loss: 0.0031758900731801987\n",
            "step: 360, loss: 0.003205028595402837\n",
            "step: 370, loss: 0.0002861428656615317\n",
            "step: 380, loss: 0.0002762045478448272\n",
            "step: 390, loss: 0.00010604471754049882\n",
            "step: 400, loss: 0.1051715835928917\n",
            "step: 410, loss: 0.0003236234770156443\n",
            "step: 420, loss: 0.006853075698018074\n",
            "step: 430, loss: 0.019093068316578865\n",
            "step: 440, loss: 9.983897325582802e-05\n",
            "step: 450, loss: 0.004374200478196144\n",
            "step: 460, loss: 0.00023194242385216057\n",
            "step: 470, loss: 0.00017421269149053842\n",
            "step: 480, loss: 0.0011635645059868693\n",
            "step: 490, loss: 0.0025915508158504963\n",
            "step: 500, loss: 0.0005736117600463331\n",
            "step: 510, loss: 0.0021265868563205004\n",
            "step: 520, loss: 0.00013039108307566494\n",
            "step: 530, loss: 0.00358349340967834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9428838951310862, f1=0.9343269678621332, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003883660538122058\n",
            "step: 10, loss: 4.0838429413270205e-05\n",
            "step: 20, loss: 0.0003980401379521936\n",
            "step: 30, loss: 0.001544563565403223\n",
            "step: 40, loss: 0.00025353257660754025\n",
            "step: 50, loss: 0.020618535578250885\n",
            "step: 60, loss: 4.8700880142860115e-05\n",
            "step: 70, loss: 0.00016149351722560823\n",
            "step: 80, loss: 0.0008802236989140511\n",
            "step: 90, loss: 0.011593620292842388\n",
            "step: 100, loss: 0.0018849273910745978\n",
            "step: 110, loss: 0.0007159945671446621\n",
            "step: 120, loss: 0.0019149092258885503\n",
            "step: 130, loss: 0.0007572666509076953\n",
            "step: 140, loss: 0.0003434709506109357\n",
            "step: 150, loss: 0.00533656869083643\n",
            "step: 160, loss: 7.203819404821843e-05\n",
            "step: 170, loss: 0.00040937616722658277\n",
            "step: 180, loss: 8.09029588708654e-05\n",
            "step: 190, loss: 0.00010872448910959065\n",
            "step: 200, loss: 6.549867975991219e-05\n",
            "step: 210, loss: 0.002423746045678854\n",
            "step: 220, loss: 8.456877549178898e-05\n",
            "step: 230, loss: 9.619991760700941e-05\n",
            "step: 240, loss: 0.00039743975503370166\n",
            "step: 250, loss: 0.00022435748542193323\n",
            "step: 260, loss: 0.0001204221262014471\n",
            "step: 270, loss: 3.120871406281367e-05\n",
            "step: 280, loss: 5.297297320794314e-05\n",
            "step: 290, loss: 0.00034931523259729147\n",
            "step: 300, loss: 3.804650987149216e-05\n",
            "step: 310, loss: 0.0012479025172069669\n",
            "step: 320, loss: 0.006236674729734659\n",
            "step: 330, loss: 0.0016769074136391282\n",
            "step: 340, loss: 9.069334191735834e-05\n",
            "step: 350, loss: 5.476427759276703e-05\n",
            "step: 360, loss: 0.00031816127011552453\n",
            "step: 370, loss: 0.02207225188612938\n",
            "step: 380, loss: 0.0005380421644076705\n",
            "step: 390, loss: 0.004017170052975416\n",
            "step: 400, loss: 0.0006062027532607317\n",
            "step: 410, loss: 0.001053009764291346\n",
            "step: 420, loss: 0.020877398550510406\n",
            "step: 430, loss: 0.005308268126100302\n",
            "step: 440, loss: 0.0014978524995967746\n",
            "step: 450, loss: 0.001250597182661295\n",
            "step: 460, loss: 0.0015575401484966278\n",
            "step: 470, loss: 0.0003510827955324203\n",
            "step: 480, loss: 0.0003688035940285772\n",
            "step: 490, loss: 0.0025581626687198877\n",
            "step: 500, loss: 0.0003656666085589677\n",
            "step: 510, loss: 0.0005817149067297578\n",
            "step: 520, loss: 0.001928870566189289\n",
            "step: 530, loss: 0.0001385885989293456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9428172942817293, f1=0.9353647276084949, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003116944804787636\n",
            "step: 10, loss: 3.347764868522063e-05\n",
            "step: 20, loss: 0.00013143743854016066\n",
            "step: 30, loss: 0.00010013456630986184\n",
            "step: 40, loss: 0.0001315333356615156\n",
            "step: 50, loss: 0.0023988233879208565\n",
            "step: 60, loss: 1.7475133063271642e-05\n",
            "step: 70, loss: 0.0009431135840713978\n",
            "step: 80, loss: 7.404717325698584e-05\n",
            "step: 90, loss: 0.00017437979113310575\n",
            "step: 100, loss: 0.02803601324558258\n",
            "step: 110, loss: 6.0227230278542265e-05\n",
            "step: 120, loss: 0.0005969246267341077\n",
            "step: 130, loss: 0.007077860180288553\n",
            "step: 140, loss: 0.0002498703252058476\n",
            "step: 150, loss: 3.970452962676063e-05\n",
            "step: 160, loss: 0.0036424214486032724\n",
            "step: 170, loss: 0.0012112708063796163\n",
            "step: 180, loss: 0.00013984291581436992\n",
            "step: 190, loss: 6.810403283452615e-05\n",
            "step: 200, loss: 2.9689297662116587e-05\n",
            "step: 210, loss: 3.712002217071131e-05\n",
            "step: 220, loss: 4.935513061354868e-05\n",
            "step: 230, loss: 6.34959724266082e-05\n",
            "step: 240, loss: 3.20311555697117e-05\n",
            "step: 250, loss: 0.00011431483289925382\n",
            "step: 260, loss: 0.00019743455050047487\n",
            "step: 270, loss: 0.00010628675227053463\n",
            "step: 280, loss: 5.12501756020356e-05\n",
            "step: 290, loss: 5.948275065748021e-05\n",
            "step: 300, loss: 0.0004432309069670737\n",
            "step: 310, loss: 0.0001701531291473657\n",
            "step: 320, loss: 0.00014818414638284594\n",
            "step: 330, loss: 0.0011308752000331879\n",
            "step: 340, loss: 0.00024688965640962124\n",
            "step: 350, loss: 0.0008299996261484921\n",
            "step: 360, loss: 0.022853923961520195\n",
            "step: 370, loss: 0.00011839645594591275\n",
            "step: 380, loss: 0.0002821404195856303\n",
            "step: 390, loss: 0.07555506378412247\n",
            "step: 400, loss: 0.0003448709030635655\n",
            "step: 410, loss: 4.044483648613095e-05\n",
            "step: 420, loss: 8.83085303939879e-05\n",
            "step: 430, loss: 6.985123036429286e-05\n",
            "step: 440, loss: 0.006960684433579445\n",
            "step: 450, loss: 0.0004260904679540545\n",
            "step: 460, loss: 0.004194257780909538\n",
            "step: 470, loss: 0.0013691324274986982\n",
            "step: 480, loss: 4.035875463159755e-05\n",
            "step: 490, loss: 2.765917088254355e-05\n",
            "step: 500, loss: 9.718078217701986e-05\n",
            "step: 510, loss: 0.022004226222634315\n",
            "step: 520, loss: 7.18409355613403e-05\n",
            "step: 530, loss: 3.029476647498086e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9412852519648636, f1=0.9380449747590638, best_f1=0.9450853714813106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3539567337138578e-05\n",
            "step: 10, loss: 3.180038766004145e-05\n",
            "step: 20, loss: 3.093664417974651e-05\n",
            "step: 30, loss: 0.00026773876743391156\n",
            "step: 40, loss: 0.08864908665418625\n",
            "step: 50, loss: 9.162705100607127e-05\n",
            "step: 60, loss: 3.32433519361075e-05\n",
            "step: 70, loss: 0.00011819838255178183\n",
            "step: 80, loss: 7.285436004167423e-05\n",
            "step: 90, loss: 4.461194475879893e-05\n",
            "step: 100, loss: 0.04313957318663597\n",
            "step: 110, loss: 0.000885307730641216\n",
            "step: 120, loss: 0.0028951705899089575\n",
            "step: 130, loss: 0.07402981072664261\n",
            "step: 140, loss: 6.143413338577375e-05\n",
            "step: 150, loss: 2.9543678465415724e-05\n",
            "step: 160, loss: 3.198599733877927e-05\n",
            "step: 170, loss: 6.804140866734087e-05\n",
            "step: 180, loss: 5.216763020143844e-05\n",
            "step: 190, loss: 5.1855753554264084e-05\n",
            "step: 200, loss: 4.838926179218106e-05\n",
            "step: 210, loss: 0.0016104895621538162\n",
            "step: 220, loss: 4.941943916492164e-05\n",
            "step: 230, loss: 0.0015970563981682062\n",
            "step: 240, loss: 0.0007981430971994996\n",
            "step: 250, loss: 8.977087418315932e-05\n",
            "step: 260, loss: 0.005037084221839905\n",
            "step: 270, loss: 3.1504645448876545e-05\n",
            "step: 280, loss: 1.644693656999152e-05\n",
            "step: 290, loss: 2.1371552065829746e-05\n",
            "step: 300, loss: 0.00021376731456257403\n",
            "step: 310, loss: 0.000209154182812199\n",
            "step: 320, loss: 0.0003479969745967537\n",
            "step: 330, loss: 0.00011001239909091964\n",
            "step: 340, loss: 8.194960537366569e-05\n",
            "step: 350, loss: 3.0084087484283373e-05\n",
            "step: 360, loss: 0.003200661391019821\n",
            "step: 370, loss: 0.0004622801789082587\n",
            "step: 380, loss: 7.332273526117206e-05\n",
            "step: 390, loss: 0.0003475230769254267\n",
            "step: 400, loss: 3.980670953751542e-05\n",
            "step: 410, loss: 0.0003453663084656\n",
            "step: 420, loss: 6.828294135630131e-05\n",
            "step: 430, loss: 0.00028090013074688613\n",
            "step: 440, loss: 0.00010203369311057031\n",
            "step: 450, loss: 2.8136193577665836e-05\n",
            "step: 460, loss: 5.409673030953854e-05\n",
            "step: 470, loss: 0.001341169816441834\n",
            "step: 480, loss: 2.2902575437910855e-05\n",
            "step: 490, loss: 2.4586077415733598e-05\n",
            "step: 500, loss: 0.00039677132735960186\n",
            "step: 510, loss: 6.838350964244455e-05\n",
            "step: 520, loss: 3.520882819429971e-05\n",
            "step: 530, loss: 0.0011270437389612198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9419953596287702, f1=0.9349930843706776, best_f1=0.9450853714813106\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:26, 220.33it/s]\n",
            "load_f1 = 0.9446768944676894\n",
            "real_f1 = 0.9446768944676894\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 223.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5af311c-05cd-4de0-d487-7b461ff85bcf"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5951436161994934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4782608695652174, f1=0.45, best_f1=0.45\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5059506297111511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5142857142857143, f1=0.5142857142857143, best_f1=0.5142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4737869203090668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5806451612903226, f1=0.5294117647058824, best_f1=0.5294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2728976607322693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6956521739130435, f1=0.5517241379310344, best_f1=0.5517241379310344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2917260229587555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6875000000000001, f1=0.5, best_f1=0.5517241379310344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19082775712013245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8148148148148148, f1=0.6206896551724138, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10189282894134521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7777777777777778, f1=0.5789473684210527, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04210579767823219\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8387096774193549, f1=0.6470588235294117, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026811426505446434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8387096774193549, f1=0.6470588235294117, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025330014526844025\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8125000000000001, f1=0.6470588235294117, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026916740462183952\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7692307692307692, f1=0.7586206896551724, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005531466566026211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7878787878787878, f1=0.6857142857142857, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036483248695731163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8, f1=0.6470588235294117, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037169780116528273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7857142857142857, f1=0.7333333333333334, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005069883074611425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7857142857142857, f1=0.7333333333333334, best_f1=0.6470588235294117\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 137098.30it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7999999999999999\n",
            "real_f1 = 0.8387096774193549\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 271.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e45a49-7a60-4a16-fa21-48082314080f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6272245049476624\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5736082792282104\n",
            "step: 20, loss: 0.34831702709198\n",
            "step: 30, loss: 0.18740178644657135\n",
            "step: 40, loss: 0.22859103977680206\n",
            "step: 50, loss: 0.04402647167444229\n",
            "step: 60, loss: 0.11197713017463684\n",
            "step: 70, loss: 0.0672559142112732\n",
            "step: 80, loss: 0.06802605837583542\n",
            "step: 90, loss: 0.14019127190113068\n",
            "step: 100, loss: 0.006354705896228552\n",
            "step: 110, loss: 0.09664954245090485\n",
            "step: 120, loss: 0.012116491794586182\n",
            "step: 130, loss: 0.02486845664680004\n",
            "step: 140, loss: 0.004789031110703945\n",
            "step: 150, loss: 0.011197269894182682\n",
            "step: 160, loss: 0.007236007135361433\n",
            "step: 170, loss: 0.05827059596776962\n",
            "step: 180, loss: 0.006281403824687004\n",
            "step: 190, loss: 0.0023051362950354815\n",
            "step: 200, loss: 0.032967709004879\n",
            "step: 210, loss: 0.002137178787961602\n",
            "step: 220, loss: 0.0029133346397429705\n",
            "step: 230, loss: 0.020952044054865837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9809203142536477, f1=0.9749430523917996, best_f1=0.9749430523917996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022934794425964355\n",
            "step: 10, loss: 0.002386761363595724\n",
            "step: 20, loss: 0.05957091227173805\n",
            "step: 30, loss: 0.20308151841163635\n",
            "step: 40, loss: 0.04761108011007309\n",
            "step: 50, loss: 0.007089200895279646\n",
            "step: 60, loss: 0.0013774757971987128\n",
            "step: 70, loss: 0.059876300394535065\n",
            "step: 80, loss: 0.006100693251937628\n",
            "step: 90, loss: 0.00436230655759573\n",
            "step: 100, loss: 0.01658911444246769\n",
            "step: 110, loss: 0.005991771817207336\n",
            "step: 120, loss: 0.16805429756641388\n",
            "step: 130, loss: 0.2539735436439514\n",
            "step: 140, loss: 0.012648270465433598\n",
            "step: 150, loss: 0.03386475145816803\n",
            "step: 160, loss: 0.004350883886218071\n",
            "step: 170, loss: 0.0021131907124072313\n",
            "step: 180, loss: 0.006302378606051207\n",
            "step: 190, loss: 0.043335214257240295\n",
            "step: 200, loss: 0.0010457439348101616\n",
            "step: 210, loss: 0.009859693236649036\n",
            "step: 220, loss: 0.04741985350847244\n",
            "step: 230, loss: 0.002648165449500084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.987598647125141, f1=0.9819819819819819, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007300368510186672\n",
            "step: 10, loss: 0.0015388474566861987\n",
            "step: 20, loss: 0.0006799096590839326\n",
            "step: 30, loss: 0.00906427577137947\n",
            "step: 40, loss: 0.05107806995511055\n",
            "step: 50, loss: 0.003651769831776619\n",
            "step: 60, loss: 0.001797109143808484\n",
            "step: 70, loss: 0.0015384610742330551\n",
            "step: 80, loss: 0.001436686608940363\n",
            "step: 90, loss: 0.011018961668014526\n",
            "step: 100, loss: 0.003989658318459988\n",
            "step: 110, loss: 0.0009442548616789281\n",
            "step: 120, loss: 0.04479938745498657\n",
            "step: 130, loss: 0.00047249949420802295\n",
            "step: 140, loss: 0.008577162399888039\n",
            "step: 150, loss: 0.0016996819758787751\n",
            "step: 160, loss: 0.023966524749994278\n",
            "step: 170, loss: 0.005133269354701042\n",
            "step: 180, loss: 0.0008115096716210246\n",
            "step: 190, loss: 0.0028037538286298513\n",
            "step: 200, loss: 0.0011299968464300036\n",
            "step: 210, loss: 0.0003834753588307649\n",
            "step: 220, loss: 0.0014771951828151941\n",
            "step: 230, loss: 0.00016554261674173176\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9887640449438202, f1=0.9864253393665158, best_f1=0.9864253393665158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012173617142252624\n",
            "step: 10, loss: 0.00017125913291238248\n",
            "step: 20, loss: 0.0003510774113237858\n",
            "step: 30, loss: 0.0026117628440260887\n",
            "step: 40, loss: 0.0006974849384278059\n",
            "step: 50, loss: 0.003439415944740176\n",
            "step: 60, loss: 0.000793388404417783\n",
            "step: 70, loss: 0.00031215333729051054\n",
            "step: 80, loss: 0.01075664907693863\n",
            "step: 90, loss: 0.0006510630482807755\n",
            "step: 100, loss: 0.025065282359719276\n",
            "step: 110, loss: 0.00036390358582139015\n",
            "step: 120, loss: 0.02050042152404785\n",
            "step: 130, loss: 0.0023783191572874784\n",
            "step: 140, loss: 0.0009445723844692111\n",
            "step: 150, loss: 0.061276815831661224\n",
            "step: 160, loss: 0.0005314275622367859\n",
            "step: 170, loss: 0.001378742977976799\n",
            "step: 180, loss: 0.0010271752253174782\n",
            "step: 190, loss: 0.0034784111194312572\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 200, loss: 0.00047951226588338614\n",
            "step: 210, loss: 0.02487030252814293\n",
            "step: 220, loss: 0.004676831420511007\n",
            "step: 230, loss: 0.00216737762093544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9898762654668166, f1=0.9830124575311437, best_f1=0.9830124575311437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036192447878420353\n",
            "step: 10, loss: 0.10303062200546265\n",
            "step: 20, loss: 0.00607782369479537\n",
            "step: 30, loss: 0.00019828815129585564\n",
            "step: 40, loss: 0.0005128983757458627\n",
            "step: 50, loss: 0.000244357215706259\n",
            "step: 60, loss: 0.0001999032247113064\n",
            "step: 70, loss: 0.0003190765273757279\n",
            "step: 80, loss: 0.0006472115055657923\n",
            "step: 90, loss: 0.001629972248338163\n",
            "step: 100, loss: 0.00018194458971265703\n",
            "step: 110, loss: 0.00030124562908895314\n",
            "step: 120, loss: 5.76565507799387e-05\n",
            "step: 130, loss: 0.0004410024848766625\n",
            "step: 140, loss: 0.0008389314753003418\n",
            "step: 150, loss: 0.02168995328247547\n",
            "step: 160, loss: 0.000578473205678165\n",
            "step: 170, loss: 0.0070081851445138454\n",
            "step: 180, loss: 0.0033213121350854635\n",
            "step: 190, loss: 0.0038942359387874603\n",
            "step: 200, loss: 0.0014632671372964978\n",
            "step: 210, loss: 0.00031726292218081653\n",
            "step: 220, loss: 0.000386374827940017\n",
            "step: 230, loss: 0.00011905932478839532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.983050847457627, f1=0.9728506787330317, best_f1=0.9830124575311437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000905118475202471\n",
            "step: 10, loss: 0.0003551135887391865\n",
            "step: 20, loss: 0.003052356420084834\n",
            "step: 30, loss: 0.09444867819547653\n",
            "step: 40, loss: 0.1129281297326088\n",
            "step: 50, loss: 0.0007556056370958686\n",
            "step: 60, loss: 0.00029443236417137086\n",
            "step: 70, loss: 0.003464545588940382\n",
            "step: 80, loss: 0.011766097508370876\n",
            "step: 90, loss: 0.007756147533655167\n",
            "step: 100, loss: 0.038020841777324677\n",
            "step: 110, loss: 0.0004567269061226398\n",
            "step: 120, loss: 0.00028563174419105053\n",
            "step: 130, loss: 0.013392725959420204\n",
            "step: 140, loss: 0.0005155996768735349\n",
            "step: 150, loss: 0.0019310866482555866\n",
            "step: 160, loss: 0.0012806288432329893\n",
            "step: 170, loss: 0.0002965754538308829\n",
            "step: 180, loss: 0.0704735592007637\n",
            "step: 190, loss: 0.00040641098166815937\n",
            "step: 200, loss: 0.0022109285928308964\n",
            "step: 210, loss: 0.0002983398735523224\n",
            "step: 220, loss: 0.00012711016461253166\n",
            "step: 230, loss: 0.07203879207372665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9841269841269841, f1=0.9761092150170648, best_f1=0.9830124575311437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006690435577183962\n",
            "step: 10, loss: 0.00013068558473605663\n",
            "step: 20, loss: 0.00018792421906255186\n",
            "step: 30, loss: 0.0013723153388127685\n",
            "step: 40, loss: 0.0003614618326537311\n",
            "step: 50, loss: 0.00032104685669764876\n",
            "step: 60, loss: 0.040377549827098846\n",
            "step: 70, loss: 0.013956270180642605\n",
            "step: 80, loss: 0.00016588960716035217\n",
            "step: 90, loss: 6.778870738344267e-05\n",
            "step: 100, loss: 0.00012643684749491513\n",
            "step: 110, loss: 0.0005365795805118978\n",
            "step: 120, loss: 0.00012880739814136177\n",
            "step: 130, loss: 0.00040149447158910334\n",
            "step: 140, loss: 0.0001672253420110792\n",
            "step: 150, loss: 0.01582835242152214\n",
            "step: 160, loss: 0.043316859751939774\n",
            "step: 170, loss: 0.0012720456579700112\n",
            "step: 180, loss: 0.0007842901395633817\n",
            "step: 190, loss: 0.00013048452092334628\n",
            "step: 200, loss: 0.040573205798864365\n",
            "step: 210, loss: 9.319237869931385e-05\n",
            "step: 220, loss: 0.00018866221944335848\n",
            "step: 230, loss: 0.01156504638493061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9898762654668166, f1=0.9773755656108598, best_f1=0.9830124575311437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.707301119808108e-05\n",
            "step: 10, loss: 0.00017360415949951857\n",
            "step: 20, loss: 0.004514187108725309\n",
            "step: 30, loss: 0.010436397045850754\n",
            "step: 40, loss: 0.001134455669671297\n",
            "step: 50, loss: 0.0023668084759265184\n",
            "step: 60, loss: 0.0004580229287967086\n",
            "step: 70, loss: 0.00028303847648203373\n",
            "step: 80, loss: 0.00013896847667638212\n",
            "step: 90, loss: 8.205945050576702e-05\n",
            "step: 100, loss: 0.00014291092520579696\n",
            "step: 110, loss: 0.01877664402127266\n",
            "step: 120, loss: 0.006745961960405111\n",
            "step: 130, loss: 0.00030362021061591804\n",
            "step: 140, loss: 8.999575220514089e-05\n",
            "step: 150, loss: 7.180039392551407e-05\n",
            "step: 160, loss: 0.012005187571048737\n",
            "step: 170, loss: 7.175382052082568e-05\n",
            "step: 180, loss: 0.00012391780910547823\n",
            "step: 190, loss: 0.00016058134497143328\n",
            "step: 200, loss: 0.00019854471611324698\n",
            "step: 210, loss: 0.00020545651204884052\n",
            "step: 220, loss: 0.001287960447371006\n",
            "step: 230, loss: 0.00015468250785488635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9909502262443439, f1=0.9794988610478361, best_f1=0.9794988610478361\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017439592920709401\n",
            "step: 10, loss: 0.012102257460355759\n",
            "step: 20, loss: 0.0011170804500579834\n",
            "step: 30, loss: 0.0007559032528661191\n",
            "step: 40, loss: 0.016679760068655014\n",
            "step: 50, loss: 7.448132964782417e-05\n",
            "step: 60, loss: 0.0006399195408448577\n",
            "step: 70, loss: 0.0003628820995800197\n",
            "step: 80, loss: 0.00010898290202021599\n",
            "step: 90, loss: 0.00029861682560294867\n",
            "step: 100, loss: 0.0001971609308384359\n",
            "step: 110, loss: 0.00037497657467611134\n",
            "step: 120, loss: 5.983533992548473e-05\n",
            "step: 130, loss: 0.0004727381747215986\n",
            "step: 140, loss: 0.03343388810753822\n",
            "step: 150, loss: 0.00021377655502874404\n",
            "step: 160, loss: 8.163407619576901e-05\n",
            "step: 170, loss: 0.00012296847125981003\n",
            "step: 180, loss: 0.0029728910885751247\n",
            "step: 190, loss: 0.00029660252039320767\n",
            "step: 200, loss: 0.00021137113799341023\n",
            "step: 210, loss: 0.0005940190167166293\n",
            "step: 220, loss: 8.459872333332896e-05\n",
            "step: 230, loss: 0.0012158039025962353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9920903954802259, f1=0.9819004524886877, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016336645057890564\n",
            "step: 10, loss: 3.423415546421893e-05\n",
            "step: 20, loss: 8.97437785170041e-05\n",
            "step: 30, loss: 0.0002585502224974334\n",
            "step: 40, loss: 6.740404205629602e-05\n",
            "step: 50, loss: 0.00018259583157487214\n",
            "step: 60, loss: 0.001953429076820612\n",
            "step: 70, loss: 0.00012776008225046098\n",
            "step: 80, loss: 7.11213651811704e-05\n",
            "step: 90, loss: 0.00010545110853854567\n",
            "step: 100, loss: 6.786532321712002e-05\n",
            "step: 110, loss: 7.400183676509187e-05\n",
            "step: 120, loss: 0.00017203603056259453\n",
            "step: 130, loss: 0.00010319970169803128\n",
            "step: 140, loss: 0.033141326159238815\n",
            "step: 150, loss: 0.016289198771119118\n",
            "step: 160, loss: 3.132491838186979e-05\n",
            "step: 170, loss: 9.749076707521453e-05\n",
            "step: 180, loss: 7.247319445014e-05\n",
            "step: 190, loss: 0.0253834780305624\n",
            "step: 200, loss: 0.00015127507504075766\n",
            "step: 210, loss: 3.0404697099584155e-05\n",
            "step: 220, loss: 6.496383139165118e-05\n",
            "step: 230, loss: 0.0025178883224725723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9909706546275394, f1=0.9819004524886877, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.474932499462739e-05\n",
            "step: 10, loss: 6.802104326197878e-05\n",
            "step: 20, loss: 0.0027565991040319204\n",
            "step: 30, loss: 0.017240170389413834\n",
            "step: 40, loss: 0.00014015946362633258\n",
            "step: 50, loss: 5.265326035441831e-05\n",
            "step: 60, loss: 9.655233589001e-05\n",
            "step: 70, loss: 0.00029499028460122645\n",
            "step: 80, loss: 5.097561006550677e-05\n",
            "step: 90, loss: 6.24005842837505e-05\n",
            "step: 100, loss: 4.746089689433575e-05\n",
            "step: 110, loss: 0.015073482878506184\n",
            "step: 120, loss: 6.485152698587626e-05\n",
            "step: 130, loss: 2.962142025353387e-05\n",
            "step: 140, loss: 0.00022472890850622207\n",
            "step: 150, loss: 0.030639519914984703\n",
            "step: 160, loss: 0.00020257114374544472\n",
            "step: 170, loss: 0.02406388893723488\n",
            "step: 180, loss: 4.739651194540784e-05\n",
            "step: 190, loss: 4.0383540181210265e-05\n",
            "step: 200, loss: 0.00010029276745626703\n",
            "step: 210, loss: 3.238292993046343e-05\n",
            "step: 220, loss: 7.444697257597e-05\n",
            "step: 230, loss: 4.943187377648428e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9853438556933484, f1=0.9784824462061155, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.627406790154055e-05\n",
            "step: 10, loss: 5.435072671389207e-05\n",
            "step: 20, loss: 4.3956148147117347e-05\n",
            "step: 30, loss: 0.00013628262968268245\n",
            "step: 40, loss: 0.0008214561967179179\n",
            "step: 50, loss: 0.00038283306639641523\n",
            "step: 60, loss: 0.0003497059515211731\n",
            "step: 70, loss: 6.241487426450476e-05\n",
            "step: 80, loss: 5.4375002946471795e-05\n",
            "step: 90, loss: 4.424635699251667e-05\n",
            "step: 100, loss: 0.00011163135059177876\n",
            "step: 110, loss: 6.22851075604558e-05\n",
            "step: 120, loss: 6.0438622313085943e-05\n",
            "step: 130, loss: 4.470538260648027e-05\n",
            "step: 140, loss: 0.02541334554553032\n",
            "step: 150, loss: 4.947617344441824e-05\n",
            "step: 160, loss: 5.535728269023821e-05\n",
            "step: 170, loss: 4.63887226942461e-05\n",
            "step: 180, loss: 0.013739845715463161\n",
            "step: 190, loss: 0.00014948398165870458\n",
            "step: 200, loss: 2.619535916892346e-05\n",
            "step: 210, loss: 3.835045936284587e-05\n",
            "step: 220, loss: 3.9183923945529386e-05\n",
            "step: 230, loss: 0.022650565952062607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9887387387387387, f1=0.9796380090497738, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.150984725332819e-05\n",
            "step: 10, loss: 0.006548650097101927\n",
            "step: 20, loss: 0.00010180337267229334\n",
            "step: 30, loss: 8.953845099313185e-05\n",
            "step: 40, loss: 4.790223465533927e-05\n",
            "step: 50, loss: 0.0004221196286380291\n",
            "step: 60, loss: 4.6038061555009335e-05\n",
            "step: 70, loss: 2.5960711354855448e-05\n",
            "step: 80, loss: 4.626877125701867e-05\n",
            "step: 90, loss: 5.100160342408344e-05\n",
            "step: 100, loss: 2.5301244022557512e-05\n",
            "step: 110, loss: 0.03277277201414108\n",
            "step: 120, loss: 0.02241881750524044\n",
            "step: 130, loss: 8.993552182801068e-05\n",
            "step: 140, loss: 3.58916622644756e-05\n",
            "step: 150, loss: 3.888693026965484e-05\n",
            "step: 160, loss: 4.344801709521562e-05\n",
            "step: 170, loss: 4.154527050559409e-05\n",
            "step: 180, loss: 5.7652501709526405e-05\n",
            "step: 190, loss: 5.777649857918732e-05\n",
            "step: 200, loss: 4.641237683244981e-05\n",
            "step: 210, loss: 1.989631709875539e-05\n",
            "step: 220, loss: 4.4877338950755075e-05\n",
            "step: 230, loss: 3.126889350824058e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9875706214689265, f1=0.9806598407281, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.258006472606212e-05\n",
            "step: 10, loss: 0.015618470497429371\n",
            "step: 20, loss: 8.249521488323808e-05\n",
            "step: 30, loss: 0.00018150685355067253\n",
            "step: 40, loss: 0.0002479999966453761\n",
            "step: 50, loss: 3.3802163670770824e-05\n",
            "step: 60, loss: 4.5127544581191614e-05\n",
            "step: 70, loss: 3.328055390738882e-05\n",
            "step: 80, loss: 3.253192699048668e-05\n",
            "step: 90, loss: 4.202994023216888e-05\n",
            "step: 100, loss: 2.9596276363008656e-05\n",
            "step: 110, loss: 0.00010510202264413238\n",
            "step: 120, loss: 9.202285582432523e-05\n",
            "step: 130, loss: 4.217155583319254e-05\n",
            "step: 140, loss: 7.51671104808338e-05\n",
            "step: 150, loss: 3.484421904431656e-05\n",
            "step: 160, loss: 0.006120601668953896\n",
            "step: 170, loss: 1.6975940525298938e-05\n",
            "step: 180, loss: 0.0002587259514257312\n",
            "step: 190, loss: 6.156806193757802e-05\n",
            "step: 200, loss: 2.881036743929144e-05\n",
            "step: 210, loss: 0.0017898408696055412\n",
            "step: 220, loss: 0.00020805036183446646\n",
            "step: 230, loss: 3.288960215286352e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9853438556933484, f1=0.9762174405436014, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.257083517382853e-05\n",
            "step: 10, loss: 1.8782489860313945e-05\n",
            "step: 20, loss: 3.453216049820185e-05\n",
            "step: 30, loss: 3.4907745430245996e-05\n",
            "step: 40, loss: 4.555695704766549e-05\n",
            "step: 50, loss: 0.046366482973098755\n",
            "step: 60, loss: 7.640897092642263e-05\n",
            "step: 70, loss: 0.0027907495386898518\n",
            "step: 80, loss: 7.208502211142331e-05\n",
            "step: 90, loss: 5.0090042350348085e-05\n",
            "step: 100, loss: 6.695371848763898e-05\n",
            "step: 110, loss: 2.514500738470815e-05\n",
            "step: 120, loss: 0.00010725067113526165\n",
            "step: 130, loss: 0.024307044222950935\n",
            "step: 140, loss: 2.5644174456829205e-05\n",
            "step: 150, loss: 0.00012179525219835341\n",
            "step: 160, loss: 2.8087331884307787e-05\n",
            "step: 170, loss: 2.8016818760079332e-05\n",
            "step: 180, loss: 6.478762225015089e-05\n",
            "step: 190, loss: 0.00029081100365146995\n",
            "step: 200, loss: 5.568227425101213e-05\n",
            "step: 210, loss: 4.245418313075788e-05\n",
            "step: 220, loss: 4.243089279043488e-05\n",
            "step: 230, loss: 5.48144853382837e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9864559819413092, f1=0.9762174405436014, best_f1=0.9819004524886877\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 202.86it/s]\n",
            "load_f1 = 0.992108229988726\n",
            "real_f1 = 0.990990990990991\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 247.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff455af-dfb9-409d-ad77-cd230f3a0b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.623701810836792\n",
            "step: 10, loss: 0.4945133924484253\n",
            "step: 20, loss: 0.4843423068523407\n",
            "step: 30, loss: 0.11872131377458572\n",
            "step: 40, loss: 0.09801803529262543\n",
            "step: 50, loss: 0.26610568165779114\n",
            "step: 60, loss: 0.022304080426692963\n",
            "step: 70, loss: 0.12701059877872467\n",
            "step: 80, loss: 0.10472144186496735\n",
            "step: 90, loss: 0.16097275912761688\n",
            "step: 100, loss: 0.03193754330277443\n",
            "step: 110, loss: 0.11913684010505676\n",
            "step: 120, loss: 0.08952952921390533\n",
            "step: 130, loss: 0.06178748980164528\n",
            "step: 140, loss: 0.03615923970937729\n",
            "step: 150, loss: 0.042633622884750366\n",
            "step: 160, loss: 0.029341338202357292\n",
            "step: 170, loss: 0.16557615995407104\n",
            "step: 180, loss: 0.14337189495563507\n",
            "step: 190, loss: 0.012324427254498005\n",
            "step: 200, loss: 0.12273643165826797\n",
            "step: 210, loss: 0.16687554121017456\n",
            "step: 220, loss: 0.2613532841205597\n",
            "step: 230, loss: 0.1444852650165558\n",
            "step: 240, loss: 0.08140435814857483\n",
            "step: 250, loss: 0.03508973866701126\n",
            "step: 260, loss: 0.046839095652103424\n",
            "step: 270, loss: 0.019742758944630623\n",
            "step: 280, loss: 0.048276014626026154\n",
            "step: 290, loss: 0.29139450192451477\n",
            "step: 300, loss: 0.05600406229496002\n",
            "step: 310, loss: 0.3354531526565552\n",
            "step: 320, loss: 0.08252713084220886\n",
            "step: 330, loss: 0.018460141494870186\n",
            "step: 340, loss: 0.03173963353037834\n",
            "step: 350, loss: 0.06917209923267365\n",
            "step: 360, loss: 0.05163748189806938\n",
            "step: 370, loss: 0.02933325432240963\n",
            "step: 380, loss: 0.022103937342762947\n",
            "step: 390, loss: 0.13489480316638947\n",
            "step: 400, loss: 0.20461997389793396\n",
            "step: 410, loss: 0.08663024753332138\n",
            "step: 420, loss: 0.07537219673395157\n",
            "step: 430, loss: 0.17122811079025269\n",
            "step: 440, loss: 0.01581212319433689\n",
            "step: 450, loss: 0.0031472190748900175\n",
            "step: 460, loss: 0.0033160774037241936\n",
            "step: 470, loss: 0.15659558773040771\n",
            "step: 480, loss: 0.030907146632671356\n",
            "step: 490, loss: 0.05060678347945213\n",
            "step: 500, loss: 0.04918743297457695\n",
            "step: 510, loss: 0.03545137122273445\n",
            "step: 520, loss: 0.023964356631040573\n",
            "step: 530, loss: 0.005591580644249916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9252336448598131, f1=0.9255813953488372, best_f1=0.9255813953488372\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2457345873117447\n",
            "step: 10, loss: 0.09332336485385895\n",
            "step: 20, loss: 0.04714571312069893\n",
            "step: 30, loss: 0.009931259788572788\n",
            "step: 40, loss: 0.07230591773986816\n",
            "step: 50, loss: 0.09544889628887177\n",
            "step: 60, loss: 0.009570027701556683\n",
            "step: 70, loss: 0.02275906875729561\n",
            "step: 80, loss: 0.08836831897497177\n",
            "step: 90, loss: 0.014541668817400932\n",
            "step: 100, loss: 0.007802006788551807\n",
            "step: 110, loss: 0.004815710708498955\n",
            "step: 120, loss: 0.1524265557527542\n",
            "step: 130, loss: 0.1842881143093109\n",
            "step: 140, loss: 0.01924448274075985\n",
            "step: 150, loss: 0.018032241612672806\n",
            "step: 160, loss: 0.006641145329922438\n",
            "step: 170, loss: 0.03409732133150101\n",
            "step: 180, loss: 0.09874418377876282\n",
            "step: 190, loss: 0.14129221439361572\n",
            "step: 200, loss: 0.006501054856926203\n",
            "step: 210, loss: 0.08770856261253357\n",
            "step: 220, loss: 0.11160922050476074\n",
            "step: 230, loss: 0.01371404342353344\n",
            "step: 240, loss: 0.10284619778394699\n",
            "step: 250, loss: 0.1511460840702057\n",
            "step: 260, loss: 0.008983870036900043\n",
            "step: 270, loss: 0.11685728281736374\n",
            "step: 280, loss: 0.10977207869291306\n",
            "step: 290, loss: 0.012493791989982128\n",
            "step: 300, loss: 0.15598392486572266\n",
            "step: 310, loss: 0.013718869537115097\n",
            "step: 320, loss: 0.10214202105998993\n",
            "step: 330, loss: 0.04315662756562233\n",
            "step: 340, loss: 0.012724166736006737\n",
            "step: 350, loss: 0.0023688245564699173\n",
            "step: 360, loss: 0.04496285319328308\n",
            "step: 370, loss: 0.07883835583925247\n",
            "step: 380, loss: 0.04705101251602173\n",
            "step: 390, loss: 0.05528027191758156\n",
            "step: 400, loss: 0.07638463377952576\n",
            "step: 410, loss: 0.014742808416485786\n",
            "step: 420, loss: 0.08849682658910751\n",
            "step: 430, loss: 0.011346842162311077\n",
            "step: 440, loss: 0.20701266825199127\n",
            "step: 450, loss: 0.03397276625037193\n",
            "step: 460, loss: 0.01170344278216362\n",
            "step: 470, loss: 0.09013195335865021\n",
            "step: 480, loss: 0.26276955008506775\n",
            "step: 490, loss: 0.02012096904218197\n",
            "step: 500, loss: 0.18753519654273987\n",
            "step: 510, loss: 0.010199490934610367\n",
            "step: 520, loss: 0.03160173445940018\n",
            "step: 530, loss: 0.007118913810700178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9384404924760602, f1=0.9347329986307622, best_f1=0.9347329986307622\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09993357211351395\n",
            "step: 10, loss: 0.026116138324141502\n",
            "step: 20, loss: 0.044791728258132935\n",
            "step: 30, loss: 0.07612120360136032\n",
            "step: 40, loss: 0.014292768202722073\n",
            "step: 50, loss: 0.03884632885456085\n",
            "step: 60, loss: 0.025354942306876183\n",
            "step: 70, loss: 0.007498187944293022\n",
            "step: 80, loss: 0.003285334212705493\n",
            "step: 90, loss: 0.003967566881328821\n",
            "step: 100, loss: 0.0633479580283165\n",
            "step: 110, loss: 0.0033753076568245888\n",
            "step: 120, loss: 0.0070038288831710815\n",
            "step: 130, loss: 0.003102570306509733\n",
            "step: 140, loss: 0.059780266135931015\n",
            "step: 150, loss: 0.06304875016212463\n",
            "step: 160, loss: 0.00854500848799944\n",
            "step: 170, loss: 0.1195751428604126\n",
            "step: 180, loss: 0.0062765381298959255\n",
            "step: 190, loss: 0.00620250916108489\n",
            "step: 200, loss: 0.005108142737299204\n",
            "step: 210, loss: 0.17528173327445984\n",
            "step: 220, loss: 0.06703665107488632\n",
            "step: 230, loss: 0.16429346799850464\n",
            "step: 240, loss: 0.0013828587252646685\n",
            "step: 250, loss: 0.04333362355828285\n",
            "step: 260, loss: 0.026347726583480835\n",
            "step: 270, loss: 0.005160727072507143\n",
            "step: 280, loss: 0.20104379951953888\n",
            "step: 290, loss: 0.005163144785910845\n",
            "step: 300, loss: 0.053770214319229126\n",
            "step: 310, loss: 0.03344222530722618\n",
            "step: 320, loss: 0.01789964735507965\n",
            "step: 330, loss: 0.0047316597774624825\n",
            "step: 340, loss: 0.007677934598177671\n",
            "step: 350, loss: 0.004512555431574583\n",
            "step: 360, loss: 0.049707964062690735\n",
            "step: 370, loss: 0.005383518058806658\n",
            "step: 380, loss: 0.04153832420706749\n",
            "step: 390, loss: 0.009643038734793663\n",
            "step: 400, loss: 0.09429330378770828\n",
            "step: 410, loss: 0.007349151652306318\n",
            "step: 420, loss: 0.15440435707569122\n",
            "step: 430, loss: 0.00842659454792738\n",
            "step: 440, loss: 0.008188154548406601\n",
            "step: 450, loss: 0.06800371408462524\n",
            "step: 460, loss: 0.013571567833423615\n",
            "step: 470, loss: 0.047313932329416275\n",
            "step: 480, loss: 0.00288943643681705\n",
            "step: 490, loss: 0.010064059868454933\n",
            "step: 500, loss: 0.008483384735882282\n",
            "step: 510, loss: 0.011150562204420567\n",
            "step: 520, loss: 0.05916542559862137\n",
            "step: 530, loss: 0.07660632580518723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.942951438000943, f1=0.9248937175247992, best_f1=0.9248937175247992\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022343031596392393\n",
            "step: 10, loss: 0.004047555383294821\n",
            "step: 20, loss: 0.0005109369521960616\n",
            "step: 30, loss: 0.00030681132921017706\n",
            "step: 40, loss: 0.0009334409842267632\n",
            "step: 50, loss: 0.0015619012992829084\n",
            "step: 60, loss: 0.013950572349131107\n",
            "step: 70, loss: 0.01072100829333067\n",
            "step: 80, loss: 0.0011664509074762464\n",
            "step: 90, loss: 0.04720030725002289\n",
            "step: 100, loss: 0.0023866414558142424\n",
            "step: 110, loss: 0.006206636317074299\n",
            "step: 120, loss: 0.0164936613291502\n",
            "step: 130, loss: 0.0015860182465985417\n",
            "step: 140, loss: 0.008011490106582642\n",
            "step: 150, loss: 0.002635512501001358\n",
            "step: 160, loss: 0.13738608360290527\n",
            "step: 170, loss: 0.0036747646518051624\n",
            "step: 180, loss: 0.001766571425832808\n",
            "step: 190, loss: 0.002567718271166086\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 200, loss: 0.09597384929656982\n",
            "step: 210, loss: 0.08075617998838425\n",
            "step: 220, loss: 0.004137587267905474\n",
            "step: 230, loss: 0.00956612266600132\n",
            "step: 240, loss: 0.005390509497374296\n",
            "step: 250, loss: 0.005603599362075329\n",
            "step: 260, loss: 0.006070077884942293\n",
            "step: 270, loss: 0.005399823654443026\n",
            "step: 280, loss: 0.018240416422486305\n",
            "step: 290, loss: 0.029959436506032944\n",
            "step: 300, loss: 0.0002079273690469563\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 310, loss: 0.0020800030324608088\n",
            "step: 320, loss: 0.035774461925029755\n",
            "step: 330, loss: 0.10830123722553253\n",
            "step: 340, loss: 0.26464971899986267\n",
            "step: 350, loss: 0.02999197319149971\n",
            "step: 360, loss: 0.04763977974653244\n",
            "step: 370, loss: 0.0023350936826318502\n",
            "step: 380, loss: 0.005143157206475735\n",
            "step: 390, loss: 0.002394250128418207\n",
            "step: 400, loss: 0.09177491813898087\n",
            "step: 410, loss: 0.003183964639902115\n",
            "step: 420, loss: 0.01260372344404459\n",
            "step: 430, loss: 0.23093119263648987\n",
            "step: 440, loss: 0.013958356343209743\n",
            "step: 450, loss: 0.012871598824858665\n",
            "step: 460, loss: 0.0012399308616295457\n",
            "step: 470, loss: 0.01827227883040905\n",
            "step: 480, loss: 0.031076300889253616\n",
            "step: 490, loss: 0.000704517588019371\n",
            "step: 500, loss: 0.004578254651278257\n",
            "step: 510, loss: 0.0038408248219639063\n",
            "step: 520, loss: 0.015036883763968945\n",
            "step: 530, loss: 0.01787586882710457\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9435707678075855, f1=0.9355432780847146, best_f1=0.9355432780847146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006591624114662409\n",
            "step: 10, loss: 0.0755748599767685\n",
            "step: 20, loss: 0.10575177520513535\n",
            "step: 30, loss: 0.01595289073884487\n",
            "step: 40, loss: 0.001620826544240117\n",
            "step: 50, loss: 0.011536487378180027\n",
            "step: 60, loss: 0.14158578217029572\n",
            "step: 70, loss: 0.011118919588625431\n",
            "step: 80, loss: 0.018060285598039627\n",
            "step: 90, loss: 0.013694501481950283\n",
            "step: 100, loss: 0.013533495366573334\n",
            "step: 110, loss: 0.0002581815351732075\n",
            "step: 120, loss: 0.0022587471175938845\n",
            "step: 130, loss: 0.016949405893683434\n",
            "step: 140, loss: 0.0004510303260758519\n",
            "step: 150, loss: 0.013489976525306702\n",
            "step: 160, loss: 0.00026604149024933577\n",
            "step: 170, loss: 0.014019312337040901\n",
            "step: 180, loss: 0.0007533621392212808\n",
            "step: 190, loss: 0.0011723223142325878\n",
            "step: 200, loss: 0.0009676067857071757\n",
            "step: 210, loss: 0.06788473576307297\n",
            "step: 220, loss: 0.001299989060498774\n",
            "step: 230, loss: 0.008429566398262978\n",
            "step: 240, loss: 0.00439410237595439\n",
            "step: 250, loss: 0.001619584858417511\n",
            "step: 260, loss: 0.0018543086480349302\n",
            "step: 270, loss: 0.001476938370615244\n",
            "step: 280, loss: 0.008012509904801846\n",
            "step: 290, loss: 0.08182721585035324\n",
            "step: 300, loss: 0.007485808804631233\n",
            "step: 310, loss: 0.009081757627427578\n",
            "step: 320, loss: 0.07404407113790512\n",
            "step: 330, loss: 0.0056714825332164764\n",
            "step: 340, loss: 0.0005204027984291315\n",
            "step: 350, loss: 0.0037911077961325645\n",
            "step: 360, loss: 0.009461716748774052\n",
            "step: 370, loss: 0.08359763771295547\n",
            "step: 380, loss: 0.0008900928660295904\n",
            "step: 390, loss: 0.00015484794857911766\n",
            "step: 400, loss: 0.0008689332753419876\n",
            "step: 410, loss: 0.00043587805703282356\n",
            "step: 420, loss: 0.00477600796148181\n",
            "step: 430, loss: 0.0019182127434760332\n",
            "step: 440, loss: 0.06071166694164276\n",
            "step: 450, loss: 0.01992277055978775\n",
            "step: 460, loss: 0.0011824461398646235\n",
            "step: 470, loss: 0.021472634747624397\n",
            "step: 480, loss: 0.004641628358513117\n",
            "step: 490, loss: 0.0006958594894967973\n",
            "step: 500, loss: 0.002315310761332512\n",
            "step: 510, loss: 0.1160380095243454\n",
            "step: 520, loss: 0.004147810395807028\n",
            "step: 530, loss: 0.11369283497333527\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9319323892188214, f1=0.9262672811059909, best_f1=0.9355432780847146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005046886508353055\n",
            "step: 10, loss: 0.04343898221850395\n",
            "step: 20, loss: 0.0715894028544426\n",
            "step: 30, loss: 0.0006135411094874144\n",
            "step: 40, loss: 0.0006857561529614031\n",
            "step: 50, loss: 0.07358212023973465\n",
            "step: 60, loss: 0.00021978377480991185\n",
            "step: 70, loss: 0.0006023208261467516\n",
            "step: 80, loss: 0.006626402493566275\n",
            "step: 90, loss: 0.0002465713187120855\n",
            "step: 100, loss: 0.009091195650398731\n",
            "step: 110, loss: 0.003815107047557831\n",
            "step: 120, loss: 0.0006481820018962026\n",
            "step: 130, loss: 0.017835542559623718\n",
            "step: 140, loss: 0.007887715473771095\n",
            "step: 150, loss: 0.0005993720842525363\n",
            "step: 160, loss: 0.003993155434727669\n",
            "step: 170, loss: 0.0009693157044239342\n",
            "step: 180, loss: 0.000168906626640819\n",
            "step: 190, loss: 0.0005757362232543528\n",
            "step: 200, loss: 0.023878157138824463\n",
            "step: 210, loss: 0.0017647066852077842\n",
            "step: 220, loss: 0.0019719810225069523\n",
            "step: 230, loss: 0.0006739334785379469\n",
            "step: 240, loss: 0.006507054436951876\n",
            "step: 250, loss: 0.001119795604608953\n",
            "step: 260, loss: 0.0017466249410063028\n",
            "step: 270, loss: 0.08607885986566544\n",
            "step: 280, loss: 0.01240420900285244\n",
            "step: 290, loss: 0.0005156475235708058\n",
            "step: 300, loss: 0.0006546066724695265\n",
            "step: 310, loss: 0.0007575028575956821\n",
            "step: 320, loss: 0.00043696127249859273\n",
            "step: 330, loss: 0.0007488135015591979\n",
            "step: 340, loss: 0.05457005277276039\n",
            "step: 350, loss: 0.004152423702180386\n",
            "step: 360, loss: 0.008092576637864113\n",
            "step: 370, loss: 0.0024012576323002577\n",
            "step: 380, loss: 0.04405370354652405\n",
            "step: 390, loss: 0.014109142124652863\n",
            "step: 400, loss: 7.208426541183144e-05\n",
            "step: 410, loss: 0.0003562263445928693\n",
            "step: 420, loss: 0.004188107326626778\n",
            "step: 430, loss: 0.00012165070074843243\n",
            "step: 440, loss: 0.00015048369823489338\n",
            "step: 450, loss: 0.043448206037282944\n",
            "step: 460, loss: 0.0009914875263348222\n",
            "step: 470, loss: 0.000982682453468442\n",
            "step: 480, loss: 0.008275349624454975\n",
            "step: 490, loss: 0.019931679591536522\n",
            "step: 500, loss: 0.00493704853579402\n",
            "step: 510, loss: 0.00034518196480348706\n",
            "step: 520, loss: 0.020769720897078514\n",
            "step: 530, loss: 0.01788395456969738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.935831381733021, f1=0.9246511627906977, best_f1=0.9355432780847146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000597335456404835\n",
            "step: 10, loss: 0.0005497077945619822\n",
            "step: 20, loss: 0.0008934665820561349\n",
            "step: 30, loss: 0.012620136141777039\n",
            "step: 40, loss: 8.565274038119242e-05\n",
            "step: 50, loss: 0.0013460279442369938\n",
            "step: 60, loss: 0.008171312510967255\n",
            "step: 70, loss: 0.00030869783950038254\n",
            "step: 80, loss: 0.0016263002762570977\n",
            "step: 90, loss: 0.003173267701640725\n",
            "step: 100, loss: 0.008326356299221516\n",
            "step: 110, loss: 0.0033809104934334755\n",
            "step: 120, loss: 0.0005970386555418372\n",
            "step: 130, loss: 0.020606689155101776\n",
            "step: 140, loss: 0.0005651454557664692\n",
            "step: 150, loss: 0.001046106917783618\n",
            "step: 160, loss: 8.863180846674368e-05\n",
            "step: 170, loss: 0.00040038322913460433\n",
            "step: 180, loss: 0.0023693435359746218\n",
            "step: 190, loss: 0.12230940908193588\n",
            "step: 200, loss: 0.0004249429621268064\n",
            "step: 210, loss: 0.12698183953762054\n",
            "step: 220, loss: 0.006071074400097132\n",
            "step: 230, loss: 0.027336986735463142\n",
            "step: 240, loss: 0.0018735831836238503\n",
            "step: 250, loss: 0.003760288469493389\n",
            "step: 260, loss: 0.00010671155177988112\n",
            "step: 270, loss: 0.0009831914212554693\n",
            "step: 280, loss: 0.00011506000009831041\n",
            "step: 290, loss: 0.00013806014612782747\n",
            "step: 300, loss: 0.0013565237168222666\n",
            "step: 310, loss: 7.564183761132881e-05\n",
            "step: 320, loss: 6.596014281967655e-05\n",
            "step: 330, loss: 0.0007624577265232801\n",
            "step: 340, loss: 0.02989668771624565\n",
            "step: 350, loss: 0.0001535210758447647\n",
            "step: 360, loss: 0.007193200755864382\n",
            "step: 370, loss: 0.0001914622262120247\n",
            "step: 380, loss: 0.0001365290954709053\n",
            "step: 390, loss: 0.00043114711297675967\n",
            "step: 400, loss: 0.0007166724535636604\n",
            "step: 410, loss: 0.00014692039985675365\n",
            "step: 420, loss: 0.0273460540920496\n",
            "step: 430, loss: 0.00010106937406817451\n",
            "step: 440, loss: 0.0006157496245577931\n",
            "step: 450, loss: 0.001103741000406444\n",
            "step: 460, loss: 0.00016547339328099042\n",
            "step: 470, loss: 0.00270066992379725\n",
            "step: 480, loss: 0.004776963498443365\n",
            "step: 490, loss: 0.07019282877445221\n",
            "step: 500, loss: 0.00020263949409127235\n",
            "step: 510, loss: 0.00010529880819376558\n",
            "step: 520, loss: 0.0312512032687664\n",
            "step: 530, loss: 0.002824777038767934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9463459759481962, f1=0.936111111111111, best_f1=0.936111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00030552438693121076\n",
            "step: 10, loss: 0.02917538583278656\n",
            "step: 20, loss: 5.01362701470498e-05\n",
            "step: 30, loss: 0.00026371105923317373\n",
            "step: 40, loss: 0.001738260849379003\n",
            "step: 50, loss: 0.005900297313928604\n",
            "step: 60, loss: 0.004279551561921835\n",
            "step: 70, loss: 7.643007847946137e-05\n",
            "step: 80, loss: 0.0001579958334332332\n",
            "step: 90, loss: 0.00014432729221880436\n",
            "step: 100, loss: 8.238048030762002e-05\n",
            "step: 110, loss: 0.000270967953838408\n",
            "step: 120, loss: 0.066722072660923\n",
            "step: 130, loss: 0.00047754429397173226\n",
            "step: 140, loss: 0.0297384113073349\n",
            "step: 150, loss: 8.921051630750299e-05\n",
            "step: 160, loss: 0.01645777001976967\n",
            "step: 170, loss: 0.001375530264340341\n",
            "step: 180, loss: 0.0004301028384361416\n",
            "step: 190, loss: 0.00025352791999466717\n",
            "step: 200, loss: 0.00014421796367969364\n",
            "step: 210, loss: 0.0018637500470504165\n",
            "step: 220, loss: 0.0017514284700155258\n",
            "step: 230, loss: 0.0011533888755366206\n",
            "step: 240, loss: 0.000780132133513689\n",
            "step: 250, loss: 0.00016217563825193793\n",
            "step: 260, loss: 7.197131344582886e-05\n",
            "step: 270, loss: 0.0056600552052259445\n",
            "step: 280, loss: 0.06025013327598572\n",
            "step: 290, loss: 0.002882894594222307\n",
            "step: 300, loss: 0.0014669212978333235\n",
            "step: 310, loss: 0.00032781969639472663\n",
            "step: 320, loss: 0.0015362645499408245\n",
            "step: 330, loss: 0.00014124519657343626\n",
            "step: 340, loss: 0.026928558945655823\n",
            "step: 350, loss: 0.00039271573768928647\n",
            "step: 360, loss: 0.00015756157517898828\n",
            "step: 370, loss: 0.0001609845639904961\n",
            "step: 380, loss: 0.008608348667621613\n",
            "step: 390, loss: 0.11245760321617126\n",
            "step: 400, loss: 0.00025591018493287265\n",
            "step: 410, loss: 6.57343480270356e-05\n",
            "step: 420, loss: 0.00023768868413753808\n",
            "step: 430, loss: 0.054770469665527344\n",
            "step: 440, loss: 0.00205816188827157\n",
            "step: 450, loss: 0.003124341368675232\n",
            "step: 460, loss: 0.0016856420552358031\n",
            "step: 470, loss: 7.800924504408613e-05\n",
            "step: 480, loss: 6.860475696157664e-05\n",
            "step: 490, loss: 0.000833055644761771\n",
            "step: 500, loss: 0.0035217381082475185\n",
            "step: 510, loss: 5.249390596873127e-05\n",
            "step: 520, loss: 6.191492138896137e-05\n",
            "step: 530, loss: 3.340310649946332e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9423347398030943, f1=0.9317004239284032, best_f1=0.936111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022621643438469619\n",
            "step: 10, loss: 0.000896924058906734\n",
            "step: 20, loss: 0.0003844252787530422\n",
            "step: 30, loss: 0.0004838519962504506\n",
            "step: 40, loss: 0.0010180543176829815\n",
            "step: 50, loss: 3.1864317861618474e-05\n",
            "step: 60, loss: 4.1171955672325566e-05\n",
            "step: 70, loss: 0.00015092660032678396\n",
            "step: 80, loss: 0.0181149672716856\n",
            "step: 90, loss: 9.768502786755562e-05\n",
            "step: 100, loss: 9.018395212478936e-05\n",
            "step: 110, loss: 4.7312056267401204e-05\n",
            "step: 120, loss: 6.948080408619717e-05\n",
            "step: 130, loss: 0.00801053736358881\n",
            "step: 140, loss: 0.0001583405683049932\n",
            "step: 150, loss: 4.655354132410139e-05\n",
            "step: 160, loss: 0.0008647011127322912\n",
            "step: 170, loss: 0.003923120442777872\n",
            "step: 180, loss: 0.0009081456228159368\n",
            "step: 190, loss: 0.0002502574643585831\n",
            "step: 200, loss: 0.0001471615832997486\n",
            "step: 210, loss: 0.0001239240518771112\n",
            "step: 220, loss: 0.00032156682573258877\n",
            "step: 230, loss: 5.038714880356565e-05\n",
            "step: 240, loss: 0.0001675678649917245\n",
            "step: 250, loss: 8.496308873873204e-05\n",
            "step: 260, loss: 0.0051398747600615025\n",
            "step: 270, loss: 3.900654337485321e-05\n",
            "step: 280, loss: 0.00044282973976805806\n",
            "step: 290, loss: 0.03713442012667656\n",
            "step: 300, loss: 0.00027403084095567465\n",
            "step: 310, loss: 0.040152616798877716\n",
            "step: 320, loss: 0.0005926844896748662\n",
            "step: 330, loss: 0.0006021006847731769\n",
            "step: 340, loss: 0.0005994337261654437\n",
            "step: 350, loss: 0.001784753636457026\n",
            "step: 360, loss: 0.00010028797260019928\n",
            "step: 370, loss: 0.006189106032252312\n",
            "step: 380, loss: 0.00013336492702364922\n",
            "step: 390, loss: 8.875445928424597e-05\n",
            "step: 400, loss: 0.001001322758384049\n",
            "step: 410, loss: 5.7995297538582236e-05\n",
            "step: 420, loss: 0.0009527879301458597\n",
            "step: 430, loss: 7.119965448509902e-05\n",
            "step: 440, loss: 0.007255688309669495\n",
            "step: 450, loss: 0.0007649409235455096\n",
            "step: 460, loss: 8.944931323640049e-05\n",
            "step: 470, loss: 6.172717257868499e-05\n",
            "step: 480, loss: 0.0001066049953806214\n",
            "step: 490, loss: 0.0012126382207497954\n",
            "step: 500, loss: 0.00014789614942856133\n",
            "step: 510, loss: 0.0005021662800572813\n",
            "step: 520, loss: 0.00012733486073557287\n",
            "step: 530, loss: 0.00037053803680464625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9488372093023255, f1=0.9356183418249191, best_f1=0.9356183418249191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004827382508665323\n",
            "step: 10, loss: 4.816809814656153e-05\n",
            "step: 20, loss: 3.857330375467427e-05\n",
            "step: 30, loss: 3.8244812458287925e-05\n",
            "step: 40, loss: 0.00013440399197861552\n",
            "step: 50, loss: 0.0001894456654554233\n",
            "step: 60, loss: 9.143861097982153e-05\n",
            "step: 70, loss: 4.948638525092974e-05\n",
            "step: 80, loss: 3.005466533068102e-05\n",
            "step: 90, loss: 9.05727210920304e-05\n",
            "step: 100, loss: 4.7986264689825475e-05\n",
            "step: 110, loss: 1.9076860553468578e-05\n",
            "step: 120, loss: 4.6421238948823884e-05\n",
            "step: 130, loss: 0.0003997432650066912\n",
            "step: 140, loss: 0.002562482375651598\n",
            "step: 150, loss: 6.391377246472985e-05\n",
            "step: 160, loss: 0.0001542846584925428\n",
            "step: 170, loss: 4.6134220610838383e-05\n",
            "step: 180, loss: 3.629050843301229e-05\n",
            "step: 190, loss: 5.2209383284207433e-05\n",
            "step: 200, loss: 9.330963803222403e-05\n",
            "step: 210, loss: 7.701084541622549e-05\n",
            "step: 220, loss: 0.003957867156714201\n",
            "step: 230, loss: 0.0020757820457220078\n",
            "step: 240, loss: 4.4707114284392446e-05\n",
            "step: 250, loss: 4.2182735342066735e-05\n",
            "step: 260, loss: 5.192250682739541e-05\n",
            "step: 270, loss: 5.8716119383461773e-05\n",
            "step: 280, loss: 3.418556661927141e-05\n",
            "step: 290, loss: 3.0877654353389516e-05\n",
            "step: 300, loss: 2.4347929866053164e-05\n",
            "step: 310, loss: 8.059187530307099e-05\n",
            "step: 320, loss: 8.276756125269458e-05\n",
            "step: 330, loss: 4.734910180559382e-05\n",
            "step: 340, loss: 0.005160999950021505\n",
            "step: 350, loss: 0.0006954938289709389\n",
            "step: 360, loss: 4.328727300162427e-05\n",
            "step: 370, loss: 0.00043943943455815315\n",
            "step: 380, loss: 0.0031417496502399445\n",
            "step: 390, loss: 9.065320045920089e-05\n",
            "step: 400, loss: 6.016994302626699e-05\n",
            "step: 410, loss: 0.00013748122728429735\n",
            "step: 420, loss: 5.6598280934849754e-05\n",
            "step: 430, loss: 0.007193821482360363\n",
            "step: 440, loss: 0.01556831318885088\n",
            "step: 450, loss: 0.00013936235336586833\n",
            "step: 460, loss: 0.00014894199557602406\n",
            "step: 470, loss: 0.00023645082546863705\n",
            "step: 480, loss: 4.9425652832724154e-05\n",
            "step: 490, loss: 0.011545627377927303\n",
            "step: 500, loss: 0.003590164938941598\n",
            "step: 510, loss: 0.00013562914682552218\n",
            "step: 520, loss: 0.00047739126603119075\n",
            "step: 530, loss: 0.000136528629809618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9494855004677268, f1=0.9364269141531322, best_f1=0.9364269141531322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010647047311067581\n",
            "step: 10, loss: 0.003976549953222275\n",
            "step: 20, loss: 6.573754944838583e-05\n",
            "step: 30, loss: 5.029698149883188e-05\n",
            "step: 40, loss: 0.0014686079230159521\n",
            "step: 50, loss: 9.361447882838547e-05\n",
            "step: 60, loss: 0.016615290194749832\n",
            "step: 70, loss: 7.953529711812735e-05\n",
            "step: 80, loss: 0.0014585808385163546\n",
            "step: 90, loss: 9.052921086549759e-05\n",
            "step: 100, loss: 0.0003699812514241785\n",
            "step: 110, loss: 7.804340566508472e-05\n",
            "step: 120, loss: 7.625245052622631e-05\n",
            "step: 130, loss: 2.558095548010897e-05\n",
            "step: 140, loss: 0.0007713122759014368\n",
            "step: 150, loss: 2.3412816517520696e-05\n",
            "step: 160, loss: 2.735386078711599e-05\n",
            "step: 170, loss: 3.560093682608567e-05\n",
            "step: 180, loss: 3.365305019542575e-05\n",
            "step: 190, loss: 0.00021954927069600672\n",
            "step: 200, loss: 0.0017981103155761957\n",
            "step: 210, loss: 4.4275417167227715e-05\n",
            "step: 220, loss: 9.58348682615906e-05\n",
            "step: 230, loss: 4.8657409934094176e-05\n",
            "step: 240, loss: 2.9957174774608575e-05\n",
            "step: 250, loss: 2.168812534364406e-05\n",
            "step: 260, loss: 3.9917504182085395e-05\n",
            "step: 270, loss: 3.173856021021493e-05\n",
            "step: 280, loss: 0.01785924658179283\n",
            "step: 290, loss: 0.00011738097964553162\n",
            "step: 300, loss: 0.00010066694812849164\n",
            "step: 310, loss: 3.0017052267794497e-05\n",
            "step: 320, loss: 5.0597722292877734e-05\n",
            "step: 330, loss: 5.258203964331187e-05\n",
            "step: 340, loss: 7.179451495176181e-05\n",
            "step: 350, loss: 0.03968669846653938\n",
            "step: 360, loss: 4.344941407907754e-05\n",
            "step: 370, loss: 0.00024267312255688012\n",
            "step: 380, loss: 2.6761714252643287e-05\n",
            "step: 390, loss: 3.9825939893489704e-05\n",
            "step: 400, loss: 0.0042685153894126415\n",
            "step: 410, loss: 3.253190516261384e-05\n",
            "step: 420, loss: 0.00042324053356423974\n",
            "step: 430, loss: 3.7436919228639454e-05\n",
            "step: 440, loss: 0.0054803467355668545\n",
            "step: 450, loss: 3.477007703622803e-05\n",
            "step: 460, loss: 0.0031426637433469296\n",
            "step: 470, loss: 9.93408466456458e-05\n",
            "step: 480, loss: 5.2718249207828194e-05\n",
            "step: 490, loss: 3.8963928091106936e-05\n",
            "step: 500, loss: 4.4796368456445634e-05\n",
            "step: 510, loss: 8.027190051507205e-05\n",
            "step: 520, loss: 6.566941738128662e-05\n",
            "step: 530, loss: 0.00035837641917169094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9428436466698158, f1=0.9317004239284032, best_f1=0.9364269141531322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.459534001303837e-05\n",
            "step: 10, loss: 1.819408316805493e-05\n",
            "step: 20, loss: 2.0887277059955522e-05\n",
            "step: 30, loss: 0.0006088250665925443\n",
            "step: 40, loss: 3.631127401604317e-05\n",
            "step: 50, loss: 0.05328140780329704\n",
            "step: 60, loss: 0.0005099735572002828\n",
            "step: 70, loss: 0.0002659460296854377\n",
            "step: 80, loss: 3.865825419779867e-05\n",
            "step: 90, loss: 0.00010625830327626318\n",
            "step: 100, loss: 2.5122242732322775e-05\n",
            "step: 110, loss: 0.00012783434067387134\n",
            "step: 120, loss: 3.624366581789218e-05\n",
            "step: 130, loss: 3.582212229957804e-05\n",
            "step: 140, loss: 2.4023685909924097e-05\n",
            "step: 150, loss: 2.2574749891646206e-05\n",
            "step: 160, loss: 1.931908991537057e-05\n",
            "step: 170, loss: 3.036698035430163e-05\n",
            "step: 180, loss: 4.6220895455917343e-05\n",
            "step: 190, loss: 2.3662458261242136e-05\n",
            "step: 200, loss: 5.6490876886527985e-05\n",
            "step: 210, loss: 4.383763007353991e-05\n",
            "step: 220, loss: 3.469974035397172e-05\n",
            "step: 230, loss: 5.2473864343483e-05\n",
            "step: 240, loss: 2.2790620278101414e-05\n",
            "step: 250, loss: 1.7683676560409367e-05\n",
            "step: 260, loss: 6.064464832888916e-05\n",
            "step: 270, loss: 2.9134846045053564e-05\n",
            "step: 280, loss: 0.00013019927428103983\n",
            "step: 290, loss: 0.0001418616739101708\n",
            "step: 300, loss: 2.6362966309534386e-05\n",
            "step: 310, loss: 0.0001848804095061496\n",
            "step: 320, loss: 1.5716814232291654e-05\n",
            "step: 330, loss: 2.050735565717332e-05\n",
            "step: 340, loss: 2.0175786630716175e-05\n",
            "step: 350, loss: 7.72670391597785e-05\n",
            "step: 360, loss: 0.0019275328377261758\n",
            "step: 370, loss: 0.00025304959854111075\n",
            "step: 380, loss: 3.852199006360024e-05\n",
            "step: 390, loss: 0.000350680376868695\n",
            "step: 400, loss: 1.8447357433615252e-05\n",
            "step: 410, loss: 0.000121142111311201\n",
            "step: 420, loss: 0.004787764512002468\n",
            "step: 430, loss: 0.012519774958491325\n",
            "step: 440, loss: 0.0026029450818896294\n",
            "step: 450, loss: 0.0007737683481536806\n",
            "step: 460, loss: 7.308842032216489e-05\n",
            "step: 470, loss: 4.295016697142273e-05\n",
            "step: 480, loss: 0.0008513072389177978\n",
            "step: 490, loss: 6.115691940067336e-05\n",
            "step: 500, loss: 0.09146160632371902\n",
            "step: 510, loss: 0.002074414398521185\n",
            "step: 520, loss: 0.18205951154232025\n",
            "step: 530, loss: 2.5640156309236772e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9408450704225352, f1=0.9317757009345794, best_f1=0.9364269141531322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.201825893484056e-05\n",
            "step: 10, loss: 1.5511917808908038e-05\n",
            "step: 20, loss: 1.2446044820535462e-05\n",
            "step: 30, loss: 2.4314425900229253e-05\n",
            "step: 40, loss: 2.5978255507652648e-05\n",
            "step: 50, loss: 8.003312541404739e-05\n",
            "step: 60, loss: 0.0001803119812393561\n",
            "step: 70, loss: 1.7333479263470508e-05\n",
            "step: 80, loss: 1.3921233403380029e-05\n",
            "step: 90, loss: 0.03282007575035095\n",
            "step: 100, loss: 0.0027372322510927916\n",
            "step: 110, loss: 3.6818160879192874e-05\n",
            "step: 120, loss: 7.8066426794976e-05\n",
            "step: 130, loss: 1.5638528566341847e-05\n",
            "step: 140, loss: 1.9762217561947182e-05\n",
            "step: 150, loss: 2.5326993636554107e-05\n",
            "step: 160, loss: 1.7169568309327587e-05\n",
            "step: 170, loss: 0.00019523125956766307\n",
            "step: 180, loss: 2.3598893676535226e-05\n",
            "step: 190, loss: 3.304578422103077e-05\n",
            "step: 200, loss: 2.2279880795395002e-05\n",
            "step: 210, loss: 4.676529351854697e-05\n",
            "step: 220, loss: 2.287170718773268e-05\n",
            "step: 230, loss: 1.3079367818136234e-05\n",
            "step: 240, loss: 3.784167711273767e-05\n",
            "step: 250, loss: 1.7579246559762396e-05\n",
            "step: 260, loss: 7.715277752140537e-05\n",
            "step: 270, loss: 1.3794588085147552e-05\n",
            "step: 280, loss: 3.803240178967826e-05\n",
            "step: 290, loss: 2.0451236196095124e-05\n",
            "step: 300, loss: 3.614879460656084e-05\n",
            "step: 310, loss: 0.00018852738139685243\n",
            "step: 320, loss: 3.271270543336868e-05\n",
            "step: 330, loss: 5.976256215944886e-05\n",
            "step: 340, loss: 2.4057027985691093e-05\n",
            "step: 350, loss: 3.7555426388280466e-05\n",
            "step: 360, loss: 2.8964850571355782e-05\n",
            "step: 370, loss: 6.542215123772621e-05\n",
            "step: 380, loss: 0.000237657455727458\n",
            "step: 390, loss: 0.0038222612347453833\n",
            "step: 400, loss: 1.58991551870713e-05\n",
            "step: 410, loss: 3.452642704360187e-05\n",
            "step: 420, loss: 1.748623253661208e-05\n",
            "step: 430, loss: 0.003484277054667473\n",
            "step: 440, loss: 3.175454185111448e-05\n",
            "step: 450, loss: 2.655286698427517e-05\n",
            "step: 460, loss: 5.479627725435421e-05\n",
            "step: 470, loss: 1.1965511475864332e-05\n",
            "step: 480, loss: 2.8227506845723838e-05\n",
            "step: 490, loss: 2.7814890927402303e-05\n",
            "step: 500, loss: 1.3362364370550495e-05\n",
            "step: 510, loss: 8.30833159852773e-05\n",
            "step: 520, loss: 0.0003628152480814606\n",
            "step: 530, loss: 1.4200631994754076e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9392369288742346, f1=0.9324577861163227, best_f1=0.9364269141531322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.6541600163909607e-05\n",
            "step: 10, loss: 1.2285887351026759e-05\n",
            "step: 20, loss: 1.9710058040800504e-05\n",
            "step: 30, loss: 2.8682692573056556e-05\n",
            "step: 40, loss: 3.0380651878658682e-05\n",
            "step: 50, loss: 0.0005022357800044119\n",
            "step: 60, loss: 1.6368541764677502e-05\n",
            "step: 70, loss: 1.7940576071850955e-05\n",
            "step: 80, loss: 1.9624507331172936e-05\n",
            "step: 90, loss: 2.940807280538138e-05\n",
            "step: 100, loss: 1.923678610182833e-05\n",
            "step: 110, loss: 2.1969872250338085e-05\n",
            "step: 120, loss: 2.147570194210857e-05\n",
            "step: 130, loss: 0.013411998748779297\n",
            "step: 140, loss: 2.3917797079775482e-05\n",
            "step: 150, loss: 1.3813196346745826e-05\n",
            "step: 160, loss: 2.2793918105890043e-05\n",
            "step: 170, loss: 2.4290990040753968e-05\n",
            "step: 180, loss: 6.418691191356629e-05\n",
            "step: 190, loss: 5.3929721616441384e-05\n",
            "step: 200, loss: 2.408320506219752e-05\n",
            "step: 210, loss: 2.1863015717826784e-05\n",
            "step: 220, loss: 1.6223422790062614e-05\n",
            "step: 230, loss: 6.910636147949845e-05\n",
            "step: 240, loss: 1.1339691809553187e-05\n",
            "step: 250, loss: 3.05238354485482e-05\n",
            "step: 260, loss: 2.062636303890031e-05\n",
            "step: 270, loss: 2.853445039363578e-05\n",
            "step: 280, loss: 1.4033002116775606e-05\n",
            "step: 290, loss: 3.1669089366914704e-05\n",
            "step: 300, loss: 2.5211678803316317e-05\n",
            "step: 310, loss: 6.554162973770872e-05\n",
            "step: 320, loss: 2.8787160772481002e-05\n",
            "step: 330, loss: 8.084272121777758e-05\n",
            "step: 340, loss: 2.5181443561450578e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 350, loss: 1.6800819139461964e-05\n",
            "step: 360, loss: 0.00014425716653931886\n",
            "step: 370, loss: 1.5437399270012975e-05\n",
            "step: 380, loss: 2.2905951482243836e-05\n",
            "step: 390, loss: 0.03165927901864052\n",
            "step: 400, loss: 4.278244523447938e-05\n",
            "step: 410, loss: 1.6409489035140723e-05\n",
            "step: 420, loss: 1.5269675714080222e-05\n",
            "step: 430, loss: 0.0002851001627277583\n",
            "step: 440, loss: 3.907041173079051e-05\n",
            "step: 450, loss: 2.8925525839440525e-05\n",
            "step: 460, loss: 1.7545800801599398e-05\n",
            "step: 470, loss: 1.645409793127328e-05\n",
            "step: 480, loss: 1.949770921783056e-05\n",
            "step: 490, loss: 2.669720743142534e-05\n",
            "step: 500, loss: 1.2181561942270491e-05\n",
            "step: 510, loss: 2.2846239517093636e-05\n",
            "step: 520, loss: 1.418564988853177e-05\n",
            "step: 530, loss: 2.631590177770704e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9406819243344232, f1=0.9326521133302369, best_f1=0.9364269141531322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.5813584468560293e-05\n",
            "step: 10, loss: 2.0275943825254217e-05\n",
            "step: 20, loss: 2.111365756718442e-05\n",
            "step: 30, loss: 0.002570496406406164\n",
            "step: 40, loss: 0.04804998263716698\n",
            "step: 50, loss: 2.29470242629759e-05\n",
            "step: 60, loss: 1.0445644875289872e-05\n",
            "step: 70, loss: 4.975925912731327e-05\n",
            "step: 80, loss: 1.7430227671866305e-05\n",
            "step: 90, loss: 2.1609572286251932e-05\n",
            "step: 100, loss: 0.00014972231292631477\n",
            "step: 110, loss: 1.5321913451771252e-05\n",
            "step: 120, loss: 0.006603142246603966\n",
            "step: 130, loss: 0.052741099148988724\n",
            "step: 140, loss: 1.785121094144415e-05\n",
            "step: 150, loss: 1.778784098860342e-05\n",
            "step: 160, loss: 2.249584395030979e-05\n",
            "step: 170, loss: 1.6674099242663942e-05\n",
            "step: 180, loss: 1.2825988960685208e-05\n",
            "step: 190, loss: 3.353107604198158e-05\n",
            "step: 200, loss: 2.2867652660352178e-05\n",
            "step: 210, loss: 3.032918721146416e-05\n",
            "step: 220, loss: 1.426765629730653e-05\n",
            "step: 230, loss: 5.510833580046892e-05\n",
            "step: 240, loss: 1.5910449292277917e-05\n",
            "step: 250, loss: 8.344577508978546e-05\n",
            "step: 260, loss: 1.855158552643843e-05\n",
            "step: 270, loss: 2.9738970624748617e-05\n",
            "step: 280, loss: 1.8662931324797682e-05\n",
            "step: 290, loss: 2.4673145162523724e-05\n",
            "step: 300, loss: 2.9229966457933187e-05\n",
            "step: 310, loss: 4.2316580220358446e-05\n",
            "step: 320, loss: 5.865887214895338e-05\n",
            "step: 330, loss: 5.933361535426229e-05\n",
            "step: 340, loss: 1.1425369848439004e-05\n",
            "step: 350, loss: 1.0944801033474505e-05\n",
            "step: 360, loss: 5.124739254824817e-05\n",
            "step: 370, loss: 1.050523769663414e-05\n",
            "step: 380, loss: 1.849574073276017e-05\n",
            "step: 390, loss: 3.5428147384664044e-05\n",
            "step: 400, loss: 1.5727971913293004e-05\n",
            "step: 410, loss: 1.2945251910423394e-05\n",
            "step: 420, loss: 2.5777664632187225e-05\n",
            "step: 430, loss: 7.73898427723907e-05\n",
            "step: 440, loss: 2.174004475818947e-05\n",
            "step: 450, loss: 1.6740836144890636e-05\n",
            "step: 460, loss: 1.3146413039066829e-05\n",
            "step: 470, loss: 0.010207602754235268\n",
            "step: 480, loss: 1.1403010830690619e-05\n",
            "step: 490, loss: 1.259881173609756e-05\n",
            "step: 500, loss: 6.125640356913209e-05\n",
            "step: 510, loss: 1.6364625480491668e-05\n",
            "step: 520, loss: 1.4446452041738667e-05\n",
            "step: 530, loss: 3.222087980248034e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9404536862003782, f1=0.9317647058823528, best_f1=0.9364269141531322\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:21, 261.19it/s]\n",
            "load_f1 = 0.9496738117427772\n",
            "real_f1 = 0.9473684210526314\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 255.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42edd6db-b6d0-4935-a9bd-e26eba96e5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.553176999092102\n",
            "step: 10, loss: 0.37075871229171753\n",
            "step: 20, loss: 0.3815804421901703\n",
            "step: 30, loss: 0.32474467158317566\n",
            "step: 40, loss: 0.18695521354675293\n",
            "step: 50, loss: 0.47336795926094055\n",
            "step: 60, loss: 0.34243133664131165\n",
            "step: 70, loss: 0.19472730159759521\n",
            "step: 80, loss: 0.1865738481283188\n",
            "step: 90, loss: 0.32823246717453003\n",
            "step: 100, loss: 0.4229927062988281\n",
            "step: 110, loss: 0.28339892625808716\n",
            "step: 120, loss: 0.30333632230758667\n",
            "step: 130, loss: 0.2383698970079422\n",
            "step: 140, loss: 0.209610253572464\n",
            "step: 150, loss: 0.1403825879096985\n",
            "step: 160, loss: 0.3175959885120392\n",
            "step: 170, loss: 0.27820149064064026\n",
            "step: 180, loss: 0.1085294559597969\n",
            "step: 190, loss: 0.1558297872543335\n",
            "step: 200, loss: 0.26177698373794556\n",
            "step: 210, loss: 0.23595941066741943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6309278350515464, f1=0.6778242677824268, best_f1=0.6778242677824268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13386915624141693\n",
            "step: 10, loss: 0.1764293611049652\n",
            "step: 20, loss: 0.1914094090461731\n",
            "step: 30, loss: 0.1546858698129654\n",
            "step: 40, loss: 0.1810772866010666\n",
            "step: 50, loss: 0.17681853473186493\n",
            "step: 60, loss: 0.3216722011566162\n",
            "step: 70, loss: 0.12081484496593475\n",
            "step: 80, loss: 0.14184829592704773\n",
            "step: 90, loss: 0.05664047971367836\n",
            "step: 100, loss: 0.014181986451148987\n",
            "step: 110, loss: 0.15990062057971954\n",
            "step: 120, loss: 0.16830426454544067\n",
            "step: 130, loss: 0.010728895664215088\n",
            "step: 140, loss: 0.26073896884918213\n",
            "step: 150, loss: 0.20040559768676758\n",
            "step: 160, loss: 0.13124924898147583\n",
            "step: 170, loss: 0.10871139913797379\n",
            "step: 180, loss: 0.3088846206665039\n",
            "step: 190, loss: 0.13027262687683105\n",
            "step: 200, loss: 0.045634277164936066\n",
            "step: 210, loss: 0.1846799999475479\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6719056974459725, f1=0.7309236947791166, best_f1=0.7309236947791166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06004958227276802\n",
            "step: 10, loss: 0.10635025799274445\n",
            "step: 20, loss: 0.05147949978709221\n",
            "step: 30, loss: 0.15782535076141357\n",
            "step: 40, loss: 0.116518035531044\n",
            "step: 50, loss: 0.04973814636468887\n",
            "step: 60, loss: 0.1907472163438797\n",
            "step: 70, loss: 0.045914407819509506\n",
            "step: 80, loss: 0.13354326784610748\n",
            "step: 90, loss: 0.023892411962151527\n",
            "step: 100, loss: 0.08620160073041916\n",
            "step: 110, loss: 0.09644295275211334\n",
            "step: 120, loss: 0.08247297257184982\n",
            "step: 130, loss: 0.08084753155708313\n",
            "step: 140, loss: 0.07109463959932327\n",
            "step: 150, loss: 0.19519008696079254\n",
            "step: 160, loss: 0.022435860708355904\n",
            "step: 170, loss: 0.07534468919038773\n",
            "step: 180, loss: 0.05536482855677605\n",
            "step: 190, loss: 0.22243273258209229\n",
            "step: 200, loss: 0.04985963553190231\n",
            "step: 210, loss: 0.08377277851104736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6730038022813688, f1=0.6974951830443159, best_f1=0.6974951830443159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14760351181030273\n",
            "step: 10, loss: 0.04263206198811531\n",
            "step: 20, loss: 0.1812637895345688\n",
            "step: 30, loss: 0.12882211804389954\n",
            "step: 40, loss: 0.007867935113608837\n",
            "step: 50, loss: 0.19991566240787506\n",
            "step: 60, loss: 0.08126000314950943\n",
            "step: 70, loss: 0.12810015678405762\n",
            "step: 80, loss: 0.129172682762146\n",
            "step: 90, loss: 0.008817647583782673\n",
            "step: 100, loss: 0.25365862250328064\n",
            "step: 110, loss: 0.08368035405874252\n",
            "step: 120, loss: 0.0619543120265007\n",
            "step: 130, loss: 0.11860432475805283\n",
            "step: 140, loss: 0.06808052957057953\n",
            "step: 150, loss: 0.07050155848264694\n",
            "step: 160, loss: 0.03716902807354927\n",
            "step: 170, loss: 0.119008868932724\n",
            "step: 180, loss: 0.3855869174003601\n",
            "step: 190, loss: 0.06437060236930847\n",
            "step: 200, loss: 0.24660737812519073\n",
            "step: 210, loss: 0.18397054076194763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6851063829787234, f1=0.673773987206823, best_f1=0.673773987206823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10411219298839569\n",
            "step: 10, loss: 0.12383384257555008\n",
            "step: 20, loss: 0.045382242649793625\n",
            "step: 30, loss: 0.049464307725429535\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.03241768106818199\n",
            "step: 50, loss: 0.007785408291965723\n",
            "step: 60, loss: 0.16804976761341095\n",
            "step: 70, loss: 0.13361811637878418\n",
            "step: 80, loss: 0.0050644297152757645\n",
            "step: 90, loss: 0.16677452623844147\n",
            "step: 100, loss: 0.0031725012231618166\n",
            "step: 110, loss: 0.20864784717559814\n",
            "step: 120, loss: 0.05428912118077278\n",
            "step: 130, loss: 0.08937251567840576\n",
            "step: 140, loss: 0.019596533849835396\n",
            "step: 150, loss: 0.0494365431368351\n",
            "step: 160, loss: 0.03839796036481857\n",
            "step: 170, loss: 0.04224742203950882\n",
            "step: 180, loss: 0.08357441425323486\n",
            "step: 190, loss: 0.008291158825159073\n",
            "step: 200, loss: 0.052284691482782364\n",
            "step: 210, loss: 0.026739904657006264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6926229508196722, f1=0.6903765690376569, best_f1=0.6903765690376569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021278141066432\n",
            "step: 10, loss: 0.00664261681959033\n",
            "step: 20, loss: 0.008158817887306213\n",
            "step: 30, loss: 0.0016472159186378121\n",
            "step: 40, loss: 0.010487882420420647\n",
            "step: 50, loss: 0.03871608152985573\n",
            "step: 60, loss: 0.06270872056484222\n",
            "step: 70, loss: 0.012674887664616108\n",
            "step: 80, loss: 0.25282052159309387\n",
            "step: 90, loss: 0.2391124963760376\n",
            "step: 100, loss: 0.017678843811154366\n",
            "step: 110, loss: 0.00875455979257822\n",
            "step: 120, loss: 0.09099459648132324\n",
            "step: 130, loss: 0.08683932572603226\n",
            "step: 140, loss: 0.028514433652162552\n",
            "step: 150, loss: 0.02107793092727661\n",
            "step: 160, loss: 0.009277050383388996\n",
            "step: 170, loss: 0.040710095316171646\n",
            "step: 180, loss: 0.0080043850466609\n",
            "step: 190, loss: 0.06980721652507782\n",
            "step: 200, loss: 0.002887702314183116\n",
            "step: 210, loss: 0.0030486697796732187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.696, f1=0.6854838709677419, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09157610684633255\n",
            "step: 10, loss: 0.03134679049253464\n",
            "step: 20, loss: 0.0027936608530580997\n",
            "step: 30, loss: 0.016835199669003487\n",
            "step: 40, loss: 0.15010465681552887\n",
            "step: 50, loss: 0.06943988800048828\n",
            "step: 60, loss: 0.006099372170865536\n",
            "step: 70, loss: 0.015037044882774353\n",
            "step: 80, loss: 0.03919978067278862\n",
            "step: 90, loss: 0.0553741455078125\n",
            "step: 100, loss: 0.0008478343952447176\n",
            "step: 110, loss: 0.07308578491210938\n",
            "step: 120, loss: 0.043857499957084656\n",
            "step: 130, loss: 0.005885037127882242\n",
            "step: 140, loss: 0.016769269481301308\n",
            "step: 150, loss: 0.0022808313369750977\n",
            "step: 160, loss: 0.12504923343658447\n",
            "step: 170, loss: 0.004621051251888275\n",
            "step: 180, loss: 0.126916766166687\n",
            "step: 190, loss: 0.05096321552991867\n",
            "step: 200, loss: 0.010616830550134182\n",
            "step: 210, loss: 0.020096996799111366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.680161943319838, f1=0.6893787575150301, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02273685112595558\n",
            "step: 10, loss: 0.050212275236845016\n",
            "step: 20, loss: 0.0015391485067084432\n",
            "step: 30, loss: 0.03419705480337143\n",
            "step: 40, loss: 0.002895776415243745\n",
            "step: 50, loss: 0.0005456604994833469\n",
            "step: 60, loss: 0.11564520746469498\n",
            "step: 70, loss: 0.002704182406887412\n",
            "step: 80, loss: 0.01624414511024952\n",
            "step: 90, loss: 0.016332285478711128\n",
            "step: 100, loss: 0.009256136603653431\n",
            "step: 110, loss: 0.13362886011600494\n",
            "step: 120, loss: 0.0013739484129473567\n",
            "step: 130, loss: 0.0020197227131575346\n",
            "step: 140, loss: 0.09246761351823807\n",
            "step: 150, loss: 0.0029901848174631596\n",
            "step: 160, loss: 0.024425847455859184\n",
            "step: 170, loss: 0.017518620938062668\n",
            "step: 180, loss: 0.0644061267375946\n",
            "step: 190, loss: 0.04004389047622681\n",
            "step: 200, loss: 0.003771659452468157\n",
            "step: 210, loss: 0.08771924674510956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6747967479674798, f1=0.7157464212678937, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032838950864970684\n",
            "step: 10, loss: 0.0015765713760629296\n",
            "step: 20, loss: 0.0008286090451292694\n",
            "step: 30, loss: 0.0008437371579930186\n",
            "step: 40, loss: 0.02185591496527195\n",
            "step: 50, loss: 0.0018368617165833712\n",
            "step: 60, loss: 0.1118161603808403\n",
            "step: 70, loss: 0.0007330940570682287\n",
            "step: 80, loss: 0.0004993480397388339\n",
            "step: 90, loss: 0.0003309838066343218\n",
            "step: 100, loss: 0.001675663166679442\n",
            "step: 110, loss: 0.058996669948101044\n",
            "step: 120, loss: 0.010174836963415146\n",
            "step: 130, loss: 0.04012371972203255\n",
            "step: 140, loss: 0.028211846947669983\n",
            "step: 150, loss: 0.05904652178287506\n",
            "step: 160, loss: 0.00806430820375681\n",
            "step: 170, loss: 0.009545784443616867\n",
            "step: 180, loss: 0.03997355327010155\n",
            "step: 190, loss: 0.0014047850854694843\n",
            "step: 200, loss: 0.13899162411689758\n",
            "step: 210, loss: 0.0011989723425358534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6769230769230768, f1=0.7099567099567099, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00253865122795105\n",
            "step: 10, loss: 0.04497307538986206\n",
            "step: 20, loss: 0.003476504236459732\n",
            "step: 30, loss: 0.03537723049521446\n",
            "step: 40, loss: 0.0008051018812693655\n",
            "step: 50, loss: 0.04756921902298927\n",
            "step: 60, loss: 0.02347647026181221\n",
            "step: 70, loss: 0.05008142814040184\n",
            "step: 80, loss: 0.02365628257393837\n",
            "step: 90, loss: 0.022984851151704788\n",
            "step: 100, loss: 0.01035258173942566\n",
            "step: 110, loss: 0.0001803929771995172\n",
            "step: 120, loss: 0.000629729067441076\n",
            "step: 130, loss: 0.08082003146409988\n",
            "step: 140, loss: 0.006179216783493757\n",
            "step: 150, loss: 0.015862850472331047\n",
            "step: 160, loss: 0.0018803656566888094\n",
            "step: 170, loss: 0.00047567387809976935\n",
            "step: 180, loss: 0.04508793354034424\n",
            "step: 190, loss: 0.050447843968868256\n",
            "step: 200, loss: 0.01525567565113306\n",
            "step: 210, loss: 0.005360471550375223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.672340425531915, f1=0.7031578947368422, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03647060692310333\n",
            "step: 10, loss: 0.03065279871225357\n",
            "step: 20, loss: 0.03142113238573074\n",
            "step: 30, loss: 0.00028267010929994285\n",
            "step: 40, loss: 0.019810069352388382\n",
            "step: 50, loss: 0.006032058969140053\n",
            "step: 60, loss: 0.010684352368116379\n",
            "step: 70, loss: 0.015891581773757935\n",
            "step: 80, loss: 0.0534202940762043\n",
            "step: 90, loss: 0.003176626283675432\n",
            "step: 100, loss: 0.00989631749689579\n",
            "step: 110, loss: 0.07130631804466248\n",
            "step: 120, loss: 0.04566318169236183\n",
            "step: 130, loss: 0.006578630767762661\n",
            "step: 140, loss: 0.0008521853014826775\n",
            "step: 150, loss: 0.0008520475239492953\n",
            "step: 160, loss: 0.020100431516766548\n",
            "step: 170, loss: 0.04763733223080635\n",
            "step: 180, loss: 0.00291487667709589\n",
            "step: 190, loss: 0.013627405278384686\n",
            "step: 200, loss: 0.0017555204685777426\n",
            "step: 210, loss: 0.0004756607813760638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6802218114602587, f1=0.7030075187969925, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.053718965500593185\n",
            "step: 10, loss: 0.000927229761146009\n",
            "step: 20, loss: 0.0023915558122098446\n",
            "step: 30, loss: 0.0036654530558735132\n",
            "step: 40, loss: 0.0038826230447739363\n",
            "step: 50, loss: 0.008736418560147285\n",
            "step: 60, loss: 0.0004439296026248485\n",
            "step: 70, loss: 0.011049496941268444\n",
            "step: 80, loss: 0.0005042661796323955\n",
            "step: 90, loss: 0.018574655055999756\n",
            "step: 100, loss: 0.0015944541664794087\n",
            "step: 110, loss: 0.0005040388787165284\n",
            "step: 120, loss: 0.00016897585010156035\n",
            "step: 130, loss: 0.00011009447189280763\n",
            "step: 140, loss: 0.0019345609471201897\n",
            "step: 150, loss: 0.023536229506134987\n",
            "step: 160, loss: 0.0021291887387633324\n",
            "step: 170, loss: 0.0009395680390298367\n",
            "step: 180, loss: 0.0032101725228130817\n",
            "step: 190, loss: 0.0020732807461172342\n",
            "step: 200, loss: 0.0006171476561576128\n",
            "step: 210, loss: 0.03447621688246727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6561797752808989, f1=0.7015250544662309, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03292252868413925\n",
            "step: 10, loss: 0.00010061921057058498\n",
            "step: 20, loss: 0.015750795602798462\n",
            "step: 30, loss: 0.010494865477085114\n",
            "step: 40, loss: 0.01691197231411934\n",
            "step: 50, loss: 0.0010730003705248237\n",
            "step: 60, loss: 0.0010072838049381971\n",
            "step: 70, loss: 0.18881277740001678\n",
            "step: 80, loss: 0.0376126728951931\n",
            "step: 90, loss: 0.0017010383307933807\n",
            "step: 100, loss: 0.0002014428609982133\n",
            "step: 110, loss: 0.00021740474039688706\n",
            "step: 120, loss: 0.0009590124245733023\n",
            "step: 130, loss: 0.018875740468502045\n",
            "step: 140, loss: 0.06337208300828934\n",
            "step: 150, loss: 0.002301980974152684\n",
            "step: 160, loss: 0.11245302110910416\n",
            "step: 170, loss: 0.00020272481197025627\n",
            "step: 180, loss: 0.024968262761831284\n",
            "step: 190, loss: 0.0003228765563108027\n",
            "step: 200, loss: 0.0012089713709428906\n",
            "step: 210, loss: 0.006855428218841553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6680851063829787, f1=0.7127882599580714, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013189768651500344\n",
            "step: 10, loss: 0.0610395222902298\n",
            "step: 20, loss: 0.0004100239893887192\n",
            "step: 30, loss: 0.0001559251541038975\n",
            "step: 40, loss: 0.0001438520848751068\n",
            "step: 50, loss: 0.01454825047403574\n",
            "step: 60, loss: 0.00013899730402044952\n",
            "step: 70, loss: 9.943875193130225e-05\n",
            "step: 80, loss: 0.0002676605072338134\n",
            "step: 90, loss: 0.0008950581541284919\n",
            "step: 100, loss: 0.011935665272176266\n",
            "step: 110, loss: 0.00036319729406386614\n",
            "step: 120, loss: 0.025531688705086708\n",
            "step: 130, loss: 0.00698267575353384\n",
            "step: 140, loss: 0.02979668788611889\n",
            "step: 150, loss: 0.0019070249982178211\n",
            "step: 160, loss: 0.0009958998998627067\n",
            "step: 170, loss: 0.0001733565586619079\n",
            "step: 180, loss: 0.00036767605342902243\n",
            "step: 190, loss: 0.00047333957627415657\n",
            "step: 200, loss: 0.00015000003622844815\n",
            "step: 210, loss: 0.00048697448801249266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6550218340611355, f1=0.7041036717062635, best_f1=0.6854838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005863960250280797\n",
            "step: 10, loss: 9.534873242955655e-05\n",
            "step: 20, loss: 0.0002934423682745546\n",
            "step: 30, loss: 0.011280297301709652\n",
            "step: 40, loss: 0.00018523797916714102\n",
            "step: 50, loss: 0.0001115706327254884\n",
            "step: 60, loss: 0.020379232242703438\n",
            "step: 70, loss: 0.00029730767710134387\n",
            "step: 80, loss: 7.251750503201038e-05\n",
            "step: 90, loss: 0.013674531131982803\n",
            "step: 100, loss: 0.00012369938485790044\n",
            "step: 110, loss: 0.00016775137919466943\n",
            "step: 120, loss: 0.0001315134868491441\n",
            "step: 130, loss: 0.0001801490579964593\n",
            "step: 140, loss: 8.958158286986873e-05\n",
            "step: 150, loss: 7.582223770441487e-05\n",
            "step: 160, loss: 0.0012507893843576312\n",
            "step: 170, loss: 0.00011196670675417408\n",
            "step: 180, loss: 7.926589751150459e-05\n",
            "step: 190, loss: 0.0003284939157310873\n",
            "step: 200, loss: 0.009300082921981812\n",
            "step: 210, loss: 0.002201198134571314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6738197424892702, f1=0.6993603411513859, best_f1=0.6854838709677419\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 477.74it/s]\n",
            "load_f1 = 0.6975806451612904\n",
            "real_f1 = 0.6958250497017893\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 251.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d7ccb1-e8a0-4186-9ed1-85ccac997e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.578006386756897\n",
            "step: 10, loss: 0.3667844235897064\n",
            "step: 20, loss: 0.26713109016418457\n",
            "step: 30, loss: 0.41870149970054626\n",
            "step: 40, loss: 0.4327484667301178\n",
            "step: 50, loss: 0.3580265939235687\n",
            "step: 60, loss: 0.26098865270614624\n",
            "step: 70, loss: 0.2171774059534073\n",
            "step: 80, loss: 0.2960411012172699\n",
            "step: 90, loss: 0.3011975586414337\n",
            "step: 100, loss: 0.32247868180274963\n",
            "step: 110, loss: 0.29177945852279663\n",
            "step: 120, loss: 0.11316505819559097\n",
            "step: 130, loss: 0.1109917014837265\n",
            "step: 140, loss: 0.04615601524710655\n",
            "step: 150, loss: 0.14227037131786346\n",
            "step: 160, loss: 0.05160421133041382\n",
            "step: 170, loss: 0.18749964237213135\n",
            "step: 180, loss: 0.039927467703819275\n",
            "step: 190, loss: 0.1832856833934784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5925925925925924, f1=0.6257309941520468, best_f1=0.6257309941520468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21678146719932556\n",
            "step: 10, loss: 0.10321661084890366\n",
            "step: 20, loss: 0.12600238621234894\n",
            "step: 30, loss: 0.19641466438770294\n",
            "step: 40, loss: 0.05432778224349022\n",
            "step: 50, loss: 0.0462467335164547\n",
            "step: 60, loss: 0.1916395127773285\n",
            "step: 70, loss: 0.15145288407802582\n",
            "step: 80, loss: 0.21150481700897217\n",
            "step: 90, loss: 0.22393518686294556\n",
            "step: 100, loss: 0.013603896833956242\n",
            "step: 110, loss: 0.09710870683193207\n",
            "step: 120, loss: 0.12689363956451416\n",
            "step: 130, loss: 0.08803948760032654\n",
            "step: 140, loss: 0.09384607523679733\n",
            "step: 150, loss: 0.06114745885133743\n",
            "step: 160, loss: 0.024653751403093338\n",
            "step: 170, loss: 0.2206433266401291\n",
            "step: 180, loss: 0.1956881433725357\n",
            "step: 190, loss: 0.04542078822851181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.774025974025974, f1=0.8031088082901554, best_f1=0.8031088082901554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06391490250825882\n",
            "step: 10, loss: 0.18199267983436584\n",
            "step: 20, loss: 0.020483477041125298\n",
            "step: 30, loss: 0.05779064819216728\n",
            "step: 40, loss: 0.07509326189756393\n",
            "step: 50, loss: 0.11959967762231827\n",
            "step: 60, loss: 0.03777550160884857\n",
            "step: 70, loss: 0.12483709305524826\n",
            "step: 80, loss: 0.059039406478405\n",
            "step: 90, loss: 0.1019350066781044\n",
            "step: 100, loss: 0.01243123784661293\n",
            "step: 110, loss: 0.008483045734465122\n",
            "step: 120, loss: 0.01395280472934246\n",
            "step: 130, loss: 0.009786529466509819\n",
            "step: 140, loss: 0.008119343779981136\n",
            "step: 150, loss: 0.05310855060815811\n",
            "step: 160, loss: 0.13815374672412872\n",
            "step: 170, loss: 0.17282043397426605\n",
            "step: 180, loss: 0.04282096400856972\n",
            "step: 190, loss: 0.12485847622156143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8054054054054054, f1=0.8089887640449438, best_f1=0.8089887640449438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019090067595243454\n",
            "step: 10, loss: 0.04227316752076149\n",
            "step: 20, loss: 0.02888135239481926\n",
            "step: 30, loss: 0.0354495532810688\n",
            "step: 40, loss: 0.05578320473432541\n",
            "step: 50, loss: 0.08758358657360077\n",
            "step: 60, loss: 0.12379251420497894\n",
            "step: 70, loss: 0.0359012670814991\n",
            "step: 80, loss: 0.08871569484472275\n",
            "step: 90, loss: 0.064661405980587\n",
            "step: 100, loss: 0.04364991560578346\n",
            "step: 110, loss: 0.015475600957870483\n",
            "step: 120, loss: 0.10447827726602554\n",
            "step: 130, loss: 0.12453751266002655\n",
            "step: 140, loss: 0.03618118166923523\n",
            "step: 150, loss: 0.005623432341963053\n",
            "step: 160, loss: 0.003703933209180832\n",
            "step: 170, loss: 0.037177279591560364\n",
            "step: 180, loss: 0.0016194452764466405\n",
            "step: 190, loss: 0.10971371829509735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8106666666666666, f1=0.8108108108108106, best_f1=0.8108108108108106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09507670998573303\n",
            "step: 10, loss: 0.01801467128098011\n",
            "step: 20, loss: 0.008178919553756714\n",
            "step: 30, loss: 0.0034816451370716095\n",
            "step: 40, loss: 0.06094171851873398\n",
            "step: 50, loss: 0.042058542370796204\n",
            "step: 60, loss: 0.00842760968953371\n",
            "step: 70, loss: 0.0016833962872624397\n",
            "step: 80, loss: 0.06271612644195557\n",
            "step: 90, loss: 0.008846067823469639\n",
            "step: 100, loss: 0.0007911864668130875\n",
            "step: 110, loss: 0.0033616251312196255\n",
            "step: 120, loss: 0.03545084223151207\n",
            "step: 130, loss: 0.1720031052827835\n",
            "step: 140, loss: 0.05420827865600586\n",
            "step: 150, loss: 0.18296018242835999\n",
            "step: 160, loss: 0.014125276356935501\n",
            "step: 170, loss: 0.004101645201444626\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.14161746203899384\n",
            "step: 190, loss: 0.04186953976750374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8247422680412371, f1=0.8041775456919059, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018299264833331108\n",
            "step: 10, loss: 0.0706033781170845\n",
            "step: 20, loss: 0.0021520708687603474\n",
            "step: 30, loss: 0.0008413285249844193\n",
            "step: 40, loss: 0.16637542843818665\n",
            "step: 50, loss: 0.006495966576039791\n",
            "step: 60, loss: 0.019954491406679153\n",
            "step: 70, loss: 0.006856208201497793\n",
            "step: 80, loss: 0.026293300092220306\n",
            "step: 90, loss: 0.06077868118882179\n",
            "step: 100, loss: 0.0010700454004108906\n",
            "step: 110, loss: 0.09222665429115295\n",
            "step: 120, loss: 0.06189001724123955\n",
            "step: 130, loss: 0.014239431358873844\n",
            "step: 140, loss: 0.0708874985575676\n",
            "step: 150, loss: 0.024753360077738762\n",
            "step: 160, loss: 0.029956139624118805\n",
            "step: 170, loss: 0.014534193091094494\n",
            "step: 180, loss: 0.024664150550961494\n",
            "step: 190, loss: 0.014038165099918842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8011695906432748, f1=0.8, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004283578135073185\n",
            "step: 10, loss: 0.007403087802231312\n",
            "step: 20, loss: 0.01948234625160694\n",
            "step: 30, loss: 0.012381923384964466\n",
            "step: 40, loss: 0.013128619641065598\n",
            "step: 50, loss: 0.017140164971351624\n",
            "step: 60, loss: 0.011041024699807167\n",
            "step: 70, loss: 0.004229220096021891\n",
            "step: 80, loss: 0.010619920678436756\n",
            "step: 90, loss: 0.0006190272397361696\n",
            "step: 100, loss: 0.0005374985048547387\n",
            "step: 110, loss: 0.0009966654470190406\n",
            "step: 120, loss: 0.0006659763748757541\n",
            "step: 130, loss: 0.010254524648189545\n",
            "step: 140, loss: 0.0003672422026284039\n",
            "step: 150, loss: 0.0004902761429548264\n",
            "step: 160, loss: 0.11569881439208984\n",
            "step: 170, loss: 0.17090961337089539\n",
            "step: 180, loss: 0.0011537846876308322\n",
            "step: 190, loss: 0.0005499045946635306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8150134048257373, f1=0.8188976377952756, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007423918694257736\n",
            "step: 10, loss: 0.024822020903229713\n",
            "step: 20, loss: 0.0002931813651230186\n",
            "step: 30, loss: 0.14170129597187042\n",
            "step: 40, loss: 0.0003386471653357148\n",
            "step: 50, loss: 0.0003747375449165702\n",
            "step: 60, loss: 0.0009310602326877415\n",
            "step: 70, loss: 0.002681027865037322\n",
            "step: 80, loss: 0.00027779347146861255\n",
            "step: 90, loss: 0.0015030629001557827\n",
            "step: 100, loss: 0.004172705579549074\n",
            "step: 110, loss: 0.0004177007940597832\n",
            "step: 120, loss: 0.003522885264828801\n",
            "step: 130, loss: 0.003809180110692978\n",
            "step: 140, loss: 0.0014592584921047091\n",
            "step: 150, loss: 0.01022462546825409\n",
            "step: 160, loss: 0.0020405289251357317\n",
            "step: 170, loss: 0.011486314237117767\n",
            "step: 180, loss: 0.0050278021954\n",
            "step: 190, loss: 0.09625375270843506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.772972972972973, f1=0.7913279132791328, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010582326212897897\n",
            "step: 10, loss: 0.007601955905556679\n",
            "step: 20, loss: 0.004789138212800026\n",
            "step: 30, loss: 0.0006382492138072848\n",
            "step: 40, loss: 0.00034983843215741217\n",
            "step: 50, loss: 0.0002501318813301623\n",
            "step: 60, loss: 0.0005159824504517019\n",
            "step: 70, loss: 0.0015989609528332949\n",
            "step: 80, loss: 0.00038343656342476606\n",
            "step: 90, loss: 0.05021959915757179\n",
            "step: 100, loss: 0.03837137669324875\n",
            "step: 110, loss: 0.0008063691784627736\n",
            "step: 120, loss: 0.015553615987300873\n",
            "step: 130, loss: 0.0009173551807180047\n",
            "step: 140, loss: 0.0006379891419783235\n",
            "step: 150, loss: 0.08374587446451187\n",
            "step: 160, loss: 0.0025705844163894653\n",
            "step: 170, loss: 0.001814075279980898\n",
            "step: 180, loss: 0.0016611373284831643\n",
            "step: 190, loss: 0.0007810536189936101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8043478260869565, f1=0.8201058201058201, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011651248205453157\n",
            "step: 10, loss: 0.00033723286469466984\n",
            "step: 20, loss: 0.004775306675583124\n",
            "step: 30, loss: 0.00010994664626196027\n",
            "step: 40, loss: 0.0006387754110619426\n",
            "step: 50, loss: 0.00027531140949577093\n",
            "step: 60, loss: 0.001358936307951808\n",
            "step: 70, loss: 0.00046497874427586794\n",
            "step: 80, loss: 0.012359707616269588\n",
            "step: 90, loss: 0.00015734668704681098\n",
            "step: 100, loss: 0.0006964753265492618\n",
            "step: 110, loss: 0.09315750747919083\n",
            "step: 120, loss: 0.1792207509279251\n",
            "step: 130, loss: 0.000571977871004492\n",
            "step: 140, loss: 0.00037235618219710886\n",
            "step: 150, loss: 0.0006689861183986068\n",
            "step: 160, loss: 0.002861651126295328\n",
            "step: 170, loss: 0.007625051774084568\n",
            "step: 180, loss: 0.000725769205018878\n",
            "step: 190, loss: 0.0005957429530099034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7851002865329514, f1=0.8067226890756303, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017935731739271432\n",
            "step: 10, loss: 0.00045868451707065105\n",
            "step: 20, loss: 0.0030050575733184814\n",
            "step: 30, loss: 0.004264747723937035\n",
            "step: 40, loss: 0.07043381035327911\n",
            "step: 50, loss: 0.020919272676110268\n",
            "step: 60, loss: 0.00020723290799651295\n",
            "step: 70, loss: 0.0003574301954358816\n",
            "step: 80, loss: 0.0017390273278579116\n",
            "step: 90, loss: 0.00016848974337335676\n",
            "step: 100, loss: 0.0004678461409639567\n",
            "step: 110, loss: 0.00017545087030157447\n",
            "step: 120, loss: 0.00034113359288312495\n",
            "step: 130, loss: 0.00014407407434191555\n",
            "step: 140, loss: 0.010381980799138546\n",
            "step: 150, loss: 0.00015106530918274075\n",
            "step: 160, loss: 0.00044080879888497293\n",
            "step: 170, loss: 0.0016314693493768573\n",
            "step: 180, loss: 0.0005908140446990728\n",
            "step: 190, loss: 0.0008336669998243451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8125, f1=0.8315789473684211, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011976848327321932\n",
            "step: 10, loss: 0.0007660356350243092\n",
            "step: 20, loss: 0.00023896862694527954\n",
            "step: 30, loss: 0.000508622732013464\n",
            "step: 40, loss: 0.00101803510915488\n",
            "step: 50, loss: 0.0006523474585264921\n",
            "step: 60, loss: 0.002603694563731551\n",
            "step: 70, loss: 0.0004581788962241262\n",
            "step: 80, loss: 0.00017054707859642804\n",
            "step: 90, loss: 0.00010797684808494523\n",
            "step: 100, loss: 0.0007712286897003651\n",
            "step: 110, loss: 0.00017910920723807067\n",
            "step: 120, loss: 0.0003611682914197445\n",
            "step: 130, loss: 0.00014176232798490673\n",
            "step: 140, loss: 0.0007547543500550091\n",
            "step: 150, loss: 0.00023328509996645153\n",
            "step: 160, loss: 0.00010990414011757821\n",
            "step: 170, loss: 0.0004457206232473254\n",
            "step: 180, loss: 9.608842810848728e-05\n",
            "step: 190, loss: 0.00011515753431012854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7958656330749353, f1=0.8177083333333333, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015532633988186717\n",
            "step: 10, loss: 0.00015539838932454586\n",
            "step: 20, loss: 0.05318048596382141\n",
            "step: 30, loss: 0.0018836443778127432\n",
            "step: 40, loss: 0.0005798023194074631\n",
            "step: 50, loss: 0.000521624053362757\n",
            "step: 60, loss: 0.00011401788651710376\n",
            "step: 70, loss: 0.0003246665291953832\n",
            "step: 80, loss: 0.0002836047497112304\n",
            "step: 90, loss: 0.0012984948698431253\n",
            "step: 100, loss: 0.002807339420542121\n",
            "step: 110, loss: 9.750460594659671e-05\n",
            "step: 120, loss: 0.004061332903802395\n",
            "step: 130, loss: 9.816832607612014e-05\n",
            "step: 140, loss: 0.0005891090258955956\n",
            "step: 150, loss: 8.288770914077759e-05\n",
            "step: 160, loss: 0.0031296603847295046\n",
            "step: 170, loss: 0.0003983031492680311\n",
            "step: 180, loss: 0.0005595030961558223\n",
            "step: 190, loss: 0.0007030838169157505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8099173553719008, f1=0.825136612021858, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019721062562894076\n",
            "step: 10, loss: 0.00016673516074661165\n",
            "step: 20, loss: 0.00010716429096646607\n",
            "step: 30, loss: 0.00014006956189405173\n",
            "step: 40, loss: 0.00020183758169878274\n",
            "step: 50, loss: 0.00014875202032271773\n",
            "step: 60, loss: 0.0002771540603134781\n",
            "step: 70, loss: 0.000484591320855543\n",
            "step: 80, loss: 0.0002025998692261055\n",
            "step: 90, loss: 0.0003695801424328238\n",
            "step: 100, loss: 0.002650762675330043\n",
            "step: 110, loss: 0.00033906748285517097\n",
            "step: 120, loss: 0.0003152707649860531\n",
            "step: 130, loss: 0.00031989518902264535\n",
            "step: 140, loss: 0.0002297960309078917\n",
            "step: 150, loss: 0.00019581365631893277\n",
            "step: 160, loss: 0.00023171697102952749\n",
            "step: 170, loss: 0.00013528557610698044\n",
            "step: 180, loss: 0.00022068011458031833\n",
            "step: 190, loss: 0.00012157580204075202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8216216216216217, f1=0.8355795148247979, best_f1=0.8041775456919059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014226253842934966\n",
            "step: 10, loss: 8.992398215923458e-05\n",
            "step: 20, loss: 0.00024372483312617987\n",
            "step: 30, loss: 0.00011004682164639235\n",
            "step: 40, loss: 0.00011094957153545693\n",
            "step: 50, loss: 0.00010259001282975078\n",
            "step: 60, loss: 9.165731898974627e-05\n",
            "step: 70, loss: 0.0006891295779496431\n",
            "step: 80, loss: 0.0013188401935622096\n",
            "step: 90, loss: 7.921276846900582e-05\n",
            "step: 100, loss: 0.00020252274407539517\n",
            "step: 110, loss: 9.605394734535366e-05\n",
            "step: 120, loss: 0.00013164174742996693\n",
            "step: 130, loss: 0.0001369585224892944\n",
            "step: 140, loss: 0.0015692946035414934\n",
            "step: 150, loss: 0.0001856963208410889\n",
            "step: 160, loss: 0.00033063048613257706\n",
            "step: 170, loss: 0.00022744634770788252\n",
            "step: 180, loss: 0.00023460618103854358\n",
            "step: 190, loss: 0.00011388608982088044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8174386920980926, f1=0.8273972602739726, best_f1=0.8041775456919059\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 232.06it/s]\n",
            "load_f1 = 0.8293963254593176\n",
            "real_f1 = 0.8177083333333333\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643f7b09-e057-4af9-9fd3-b65dae136546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.625508189201355\n",
            "step: 10, loss: 0.34955671429634094\n",
            "step: 20, loss: 0.31388863921165466\n",
            "step: 30, loss: 0.38484176993370056\n",
            "step: 40, loss: 0.2767445743083954\n",
            "step: 50, loss: 0.2614802420139313\n",
            "step: 60, loss: 0.24148686230182648\n",
            "step: 70, loss: 0.36785873770713806\n",
            "step: 80, loss: 0.3441850244998932\n",
            "step: 90, loss: 0.23476211726665497\n",
            "step: 100, loss: 0.22136765718460083\n",
            "step: 110, loss: 0.23919777572155\n",
            "step: 120, loss: 0.1473180204629898\n",
            "step: 130, loss: 0.06386634707450867\n",
            "step: 140, loss: 0.18440231680870056\n",
            "step: 150, loss: 0.17254899442195892\n",
            "step: 160, loss: 0.14628566801548004\n",
            "step: 170, loss: 0.14882813394069672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7178217821782178, f1=0.6964705882352942, best_f1=0.6964705882352942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09744121134281158\n",
            "step: 10, loss: 0.25015920400619507\n",
            "step: 20, loss: 0.038159992545843124\n",
            "step: 30, loss: 0.2876400053501129\n",
            "step: 40, loss: 0.12036231905221939\n",
            "step: 50, loss: 0.07875987887382507\n",
            "step: 60, loss: 0.1438644528388977\n",
            "step: 70, loss: 0.03565936163067818\n",
            "step: 80, loss: 0.06718917191028595\n",
            "step: 90, loss: 0.21696950495243073\n",
            "step: 100, loss: 0.15898795425891876\n",
            "step: 110, loss: 0.07766512036323547\n",
            "step: 120, loss: 0.04942278936505318\n",
            "step: 130, loss: 0.10154952108860016\n",
            "step: 140, loss: 0.3824010193347931\n",
            "step: 150, loss: 0.15958309173583984\n",
            "step: 160, loss: 0.2405923455953598\n",
            "step: 170, loss: 0.04539227485656738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7921225382932167, f1=0.7581699346405228, best_f1=0.7581699346405228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1932191252708435\n",
            "step: 10, loss: 0.05453795939683914\n",
            "step: 20, loss: 0.11608632653951645\n",
            "step: 30, loss: 0.057744283229112625\n",
            "step: 40, loss: 0.06838446110486984\n",
            "step: 50, loss: 0.09533453732728958\n",
            "step: 60, loss: 0.11468178778886795\n",
            "step: 70, loss: 0.06762492656707764\n",
            "step: 80, loss: 0.06971783936023712\n",
            "step: 90, loss: 0.07068713009357452\n",
            "step: 100, loss: 0.004139591474086046\n",
            "step: 110, loss: 0.13476869463920593\n",
            "step: 120, loss: 0.07682481408119202\n",
            "step: 130, loss: 0.14468732476234436\n",
            "step: 140, loss: 0.061672549694776535\n",
            "step: 150, loss: 0.00830790400505066\n",
            "step: 160, loss: 0.01976865902543068\n",
            "step: 170, loss: 0.04742838069796562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7909319899244334, f1=0.8019559902200489, best_f1=0.7581699346405228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005553450435400009\n",
            "step: 10, loss: 0.03825712576508522\n",
            "step: 20, loss: 0.001549420179799199\n",
            "step: 30, loss: 0.008876792155206203\n",
            "step: 40, loss: 0.00074394402327016\n",
            "step: 50, loss: 0.09157276153564453\n",
            "step: 60, loss: 0.12751369178295135\n",
            "step: 70, loss: 0.009679645299911499\n",
            "step: 80, loss: 0.02629970759153366\n",
            "step: 90, loss: 0.02925812639296055\n",
            "step: 100, loss: 0.058948859572410583\n",
            "step: 110, loss: 0.13416925072669983\n",
            "step: 120, loss: 0.009600217454135418\n",
            "step: 130, loss: 0.018833819776773453\n",
            "step: 140, loss: 0.058461710810661316\n",
            "step: 150, loss: 0.1849406361579895\n",
            "step: 160, loss: 0.08577606827020645\n",
            "step: 170, loss: 0.01749419793486595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8087167070217919, f1=0.7904761904761904, best_f1=0.7904761904761904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011569793336093426\n",
            "step: 10, loss: 0.009656297042965889\n",
            "step: 20, loss: 0.016318071633577347\n",
            "step: 30, loss: 0.01812056452035904\n",
            "step: 40, loss: 0.05209031701087952\n",
            "step: 50, loss: 0.01728471741080284\n",
            "step: 60, loss: 0.00829263310879469\n",
            "step: 70, loss: 0.14010784029960632\n",
            "step: 80, loss: 0.0584423765540123\n",
            "step: 90, loss: 0.08920453488826752\n",
            "step: 100, loss: 0.00977635569870472\n",
            "step: 110, loss: 0.0705575942993164\n",
            "step: 120, loss: 0.01946926675736904\n",
            "step: 130, loss: 0.021542934700846672\n",
            "step: 140, loss: 0.0033348542638123035\n",
            "step: 150, loss: 0.10276451706886292\n",
            "step: 160, loss: 0.030677273869514465\n",
            "step: 170, loss: 0.01146057527512312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8229166666666666, f1=0.8088235294117646, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023193038068711758\n",
            "step: 10, loss: 0.07406347244977951\n",
            "step: 20, loss: 0.0026761882472783327\n",
            "step: 30, loss: 0.0011138890404254198\n",
            "step: 40, loss: 0.006983571220189333\n",
            "step: 50, loss: 0.08653867989778519\n",
            "step: 60, loss: 0.019583627581596375\n",
            "step: 70, loss: 0.06431665271520615\n",
            "step: 80, loss: 0.028258871287107468\n",
            "step: 90, loss: 0.05712313577532768\n",
            "step: 100, loss: 0.002050303388386965\n",
            "step: 110, loss: 0.0014015790075063705\n",
            "step: 120, loss: 0.04760381206870079\n",
            "step: 130, loss: 0.015055825933814049\n",
            "step: 140, loss: 0.01420763973146677\n",
            "step: 150, loss: 0.09584487974643707\n",
            "step: 160, loss: 0.041719987988471985\n",
            "step: 170, loss: 0.002318022074177861\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8078817733990147, f1=0.7799043062200957, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030371444299817085\n",
            "step: 10, loss: 0.011282984167337418\n",
            "step: 20, loss: 0.0018917940324172378\n",
            "step: 30, loss: 0.016932141035795212\n",
            "step: 40, loss: 0.0011639980366453528\n",
            "step: 50, loss: 0.01670253649353981\n",
            "step: 60, loss: 0.0003074818232562393\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.12101195752620697\n",
            "step: 80, loss: 0.05288374796509743\n",
            "step: 90, loss: 0.00264176819473505\n",
            "step: 100, loss: 0.012951969169080257\n",
            "step: 110, loss: 0.02074175886809826\n",
            "step: 120, loss: 0.0008957340614870191\n",
            "step: 130, loss: 0.1394144892692566\n",
            "step: 140, loss: 0.000937768432777375\n",
            "step: 150, loss: 0.0026695362757891417\n",
            "step: 160, loss: 0.0004732813686132431\n",
            "step: 170, loss: 0.019067712128162384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8174807197943444, f1=0.7989821882951653, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003477830905467272\n",
            "step: 10, loss: 0.002674720250070095\n",
            "step: 20, loss: 0.006285428535193205\n",
            "step: 30, loss: 0.029276251792907715\n",
            "step: 40, loss: 0.0058335065841674805\n",
            "step: 50, loss: 0.0013440718175843358\n",
            "step: 60, loss: 0.0004563866532407701\n",
            "step: 70, loss: 0.01742713153362274\n",
            "step: 80, loss: 0.009366853162646294\n",
            "step: 90, loss: 0.001375709194689989\n",
            "step: 100, loss: 0.024880848824977875\n",
            "step: 110, loss: 0.06878218799829483\n",
            "step: 120, loss: 0.0036669981200248003\n",
            "step: 130, loss: 0.002973382594063878\n",
            "step: 140, loss: 0.00694191362708807\n",
            "step: 150, loss: 0.01603853888809681\n",
            "step: 160, loss: 0.0018884656019508839\n",
            "step: 170, loss: 0.0017194576794281602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7877237851662404, f1=0.7989821882951653, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00168672448489815\n",
            "step: 10, loss: 0.0075935134664177895\n",
            "step: 20, loss: 0.0007842002087272704\n",
            "step: 30, loss: 0.029799766838550568\n",
            "step: 40, loss: 0.0006573935970664024\n",
            "step: 50, loss: 0.0010624835267663002\n",
            "step: 60, loss: 0.004898776300251484\n",
            "step: 70, loss: 0.0782499834895134\n",
            "step: 80, loss: 0.010694785043597221\n",
            "step: 90, loss: 0.02438315935432911\n",
            "step: 100, loss: 0.021596217527985573\n",
            "step: 110, loss: 0.0007731050718575716\n",
            "step: 120, loss: 0.010645737871527672\n",
            "step: 130, loss: 0.0613638311624527\n",
            "step: 140, loss: 0.0004412233247421682\n",
            "step: 150, loss: 0.010739644058048725\n",
            "step: 160, loss: 0.023575112223625183\n",
            "step: 170, loss: 0.028755178675055504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7921760391198044, f1=0.7761904761904763, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05136848986148834\n",
            "step: 10, loss: 0.00034752002102322876\n",
            "step: 20, loss: 0.04098503664135933\n",
            "step: 30, loss: 0.00913605373352766\n",
            "step: 40, loss: 0.07818183302879333\n",
            "step: 50, loss: 0.04481148719787598\n",
            "step: 60, loss: 0.0010864108335226774\n",
            "step: 70, loss: 0.002016047714278102\n",
            "step: 80, loss: 0.004654434975236654\n",
            "step: 90, loss: 0.007369736209511757\n",
            "step: 100, loss: 0.00041014852467924356\n",
            "step: 110, loss: 0.0021613040007650852\n",
            "step: 120, loss: 0.0005581460427492857\n",
            "step: 130, loss: 0.001263494836166501\n",
            "step: 140, loss: 0.031724654138088226\n",
            "step: 150, loss: 0.0013327470514923334\n",
            "step: 160, loss: 0.0011712285922840238\n",
            "step: 170, loss: 0.0001943429815582931\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7946666666666666, f1=0.8072916666666666, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.042212605476379395\n",
            "step: 10, loss: 0.0008827677229419351\n",
            "step: 20, loss: 0.00034025852801278234\n",
            "step: 30, loss: 0.0004277122498024255\n",
            "step: 40, loss: 0.0035820272751152515\n",
            "step: 50, loss: 0.0020866170525550842\n",
            "step: 60, loss: 0.017613209784030914\n",
            "step: 70, loss: 0.0003097702865488827\n",
            "step: 80, loss: 0.0003527569060679525\n",
            "step: 90, loss: 0.0004112979513593018\n",
            "step: 100, loss: 0.0016677769599482417\n",
            "step: 110, loss: 0.08326825499534607\n",
            "step: 120, loss: 0.0003020609437953681\n",
            "step: 130, loss: 0.00021616708545479923\n",
            "step: 140, loss: 0.0006234032334759831\n",
            "step: 150, loss: 0.00545419380068779\n",
            "step: 160, loss: 0.0011147307232022285\n",
            "step: 170, loss: 0.00084348936798051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7967914438502675, f1=0.8, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02157534658908844\n",
            "step: 10, loss: 0.0004364682245068252\n",
            "step: 20, loss: 0.0009225541725754738\n",
            "step: 30, loss: 0.00022742759028915316\n",
            "step: 40, loss: 0.0018768339650705457\n",
            "step: 50, loss: 0.000201660324819386\n",
            "step: 60, loss: 0.0007964922697283328\n",
            "step: 70, loss: 0.035934142768383026\n",
            "step: 80, loss: 0.0001590300671523437\n",
            "step: 90, loss: 0.00042463047429919243\n",
            "step: 100, loss: 0.0007558751385658979\n",
            "step: 110, loss: 0.000620368286035955\n",
            "step: 120, loss: 0.0053620897233486176\n",
            "step: 130, loss: 0.009580422192811966\n",
            "step: 140, loss: 0.01840750128030777\n",
            "step: 150, loss: 0.001587473787367344\n",
            "step: 160, loss: 0.0006227113190107048\n",
            "step: 170, loss: 0.0004017505270894617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8010335917312662, f1=0.8020050125313284, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03939204290509224\n",
            "step: 10, loss: 0.00022182203247211874\n",
            "step: 20, loss: 0.0010882418137043715\n",
            "step: 30, loss: 0.0005988414050079882\n",
            "step: 40, loss: 0.014990770258009434\n",
            "step: 50, loss: 0.0033236548770219088\n",
            "step: 60, loss: 0.012731666676700115\n",
            "step: 70, loss: 0.001829917193390429\n",
            "step: 80, loss: 0.0009580819751136005\n",
            "step: 90, loss: 0.0010997417848557234\n",
            "step: 100, loss: 0.00032260167063213885\n",
            "step: 110, loss: 0.0007753566605970263\n",
            "step: 120, loss: 0.020781615749001503\n",
            "step: 130, loss: 0.0003042806638404727\n",
            "step: 140, loss: 0.0009651294676586986\n",
            "step: 150, loss: 0.0024414549116045237\n",
            "step: 160, loss: 0.008525168523192406\n",
            "step: 170, loss: 0.002108542248606682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7980535279805353, f1=0.7807228915662651, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004081880615558475\n",
            "step: 10, loss: 0.00022592463938053697\n",
            "step: 20, loss: 0.0005917787202633917\n",
            "step: 30, loss: 0.0004067195695824921\n",
            "step: 40, loss: 0.00018210783309768885\n",
            "step: 50, loss: 0.00019688754400704056\n",
            "step: 60, loss: 0.005386231932789087\n",
            "step: 70, loss: 0.00037097735912539065\n",
            "step: 80, loss: 0.2213425189256668\n",
            "step: 90, loss: 0.0004177512601017952\n",
            "step: 100, loss: 0.0002752718282863498\n",
            "step: 110, loss: 0.0002888008311856538\n",
            "step: 120, loss: 0.04332461208105087\n",
            "step: 130, loss: 0.0008414093754254282\n",
            "step: 140, loss: 0.00026725741918198764\n",
            "step: 150, loss: 0.004742114804685116\n",
            "step: 160, loss: 0.00030819629319012165\n",
            "step: 170, loss: 0.00022990598517935723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7989276139410187, f1=0.796875, best_f1=0.8088235294117646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1149492934346199\n",
            "step: 10, loss: 0.0006335630896501243\n",
            "step: 20, loss: 0.05382486805319786\n",
            "step: 30, loss: 0.0019371388480067253\n",
            "step: 40, loss: 0.0004896469181403518\n",
            "step: 50, loss: 0.00019698671530932188\n",
            "step: 60, loss: 0.015597470104694366\n",
            "step: 70, loss: 0.00024354133347515017\n",
            "step: 80, loss: 0.014773000963032246\n",
            "step: 90, loss: 0.0019365038024261594\n",
            "step: 100, loss: 0.0004276104737073183\n",
            "step: 110, loss: 0.0001682407601037994\n",
            "step: 120, loss: 0.00035932360333390534\n",
            "step: 130, loss: 0.0033603529445827007\n",
            "step: 140, loss: 0.0002440542884869501\n",
            "step: 150, loss: 0.00026147093740291893\n",
            "step: 160, loss: 0.00018204544903710485\n",
            "step: 170, loss: 0.00031304280855692923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8010610079575595, f1=0.7958656330749355, best_f1=0.8088235294117646\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 246.61it/s]\n",
            "load_f1 = 0.8179419525065964\n",
            "real_f1 = 0.8157894736842105\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 216.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d8cc14-6889-4290-be47-7f6dc00c0e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6271450519561768\n",
            "step: 10, loss: 0.6153005957603455\n",
            "step: 20, loss: 0.43128204345703125\n",
            "step: 30, loss: 0.16884230077266693\n",
            "step: 40, loss: 0.1917475163936615\n",
            "step: 50, loss: 0.07869302481412888\n",
            "step: 60, loss: 0.21761369705200195\n",
            "step: 70, loss: 0.04846937954425812\n",
            "step: 80, loss: 0.0776652842760086\n",
            "step: 90, loss: 0.046428173780441284\n",
            "step: 100, loss: 0.02929617650806904\n",
            "step: 110, loss: 0.2844969630241394\n",
            "step: 120, loss: 0.022593745961785316\n",
            "step: 130, loss: 0.02304801158607006\n",
            "step: 140, loss: 0.004168352112174034\n",
            "step: 150, loss: 0.018481934443116188\n",
            "step: 160, loss: 0.06406774371862411\n",
            "step: 170, loss: 0.05344878509640694\n",
            "step: 180, loss: 0.07674388587474823\n",
            "step: 190, loss: 0.04305620864033699\n",
            "step: 200, loss: 0.13743504881858826\n",
            "step: 210, loss: 0.0036993296816945076\n",
            "step: 220, loss: 0.03428885340690613\n",
            "step: 230, loss: 0.01585022732615471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9706546275395034, f1=0.9704545454545453, best_f1=0.9704545454545453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0088838255032897\n",
            "step: 10, loss: 0.027011457830667496\n",
            "step: 20, loss: 0.14829230308532715\n",
            "step: 30, loss: 0.0148554015904665\n",
            "step: 40, loss: 0.0316527858376503\n",
            "step: 50, loss: 0.010610530152916908\n",
            "step: 60, loss: 0.0034286254085600376\n",
            "step: 70, loss: 0.07989975810050964\n",
            "step: 80, loss: 0.04195771738886833\n",
            "step: 90, loss: 0.028412576764822006\n",
            "step: 100, loss: 0.08127564191818237\n",
            "step: 110, loss: 0.09665244817733765\n",
            "step: 120, loss: 0.14297246932983398\n",
            "step: 130, loss: 0.012990279123187065\n",
            "step: 140, loss: 0.002620941959321499\n",
            "step: 150, loss: 0.022611021995544434\n",
            "step: 160, loss: 0.05070710927248001\n",
            "step: 170, loss: 0.02042328380048275\n",
            "step: 180, loss: 0.0051484727300703526\n",
            "step: 190, loss: 0.0017049964517354965\n",
            "step: 200, loss: 0.0018102937610819936\n",
            "step: 210, loss: 0.0011036432115361094\n",
            "step: 220, loss: 0.05046715587377548\n",
            "step: 230, loss: 0.02833561785519123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9719416386083053, f1=0.9720670391061451, best_f1=0.9720670391061451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009377866052091122\n",
            "step: 10, loss: 0.000890303635969758\n",
            "step: 20, loss: 0.0030514367390424013\n",
            "step: 30, loss: 0.0022690966725349426\n",
            "step: 40, loss: 0.028189022094011307\n",
            "step: 50, loss: 0.011848445050418377\n",
            "step: 60, loss: 0.010071715340018272\n",
            "step: 70, loss: 0.004480781964957714\n",
            "step: 80, loss: 0.0012737656943500042\n",
            "step: 90, loss: 0.042407553642988205\n",
            "step: 100, loss: 0.0013612427283078432\n",
            "step: 110, loss: 0.0006357281235978007\n",
            "step: 120, loss: 0.010805091820657253\n",
            "step: 130, loss: 0.0005154961836524308\n",
            "step: 140, loss: 0.00241091032512486\n",
            "step: 150, loss: 0.02431389130651951\n",
            "step: 160, loss: 0.012370679527521133\n",
            "step: 170, loss: 0.012343217618763447\n",
            "step: 180, loss: 0.005123824346810579\n",
            "step: 190, loss: 0.01966150291264057\n",
            "step: 200, loss: 0.0028034034185111523\n",
            "step: 210, loss: 0.0049489825032651424\n",
            "step: 220, loss: 0.0016615958884358406\n",
            "step: 230, loss: 0.04266339913010597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9787709497206705, f1=0.9731543624161074, best_f1=0.9731543624161074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008424604311585426\n",
            "step: 10, loss: 0.0021672130096703768\n",
            "step: 20, loss: 0.00108085956890136\n",
            "step: 30, loss: 0.0012089816154912114\n",
            "step: 40, loss: 0.08310473710298538\n",
            "step: 50, loss: 0.001955757848918438\n",
            "step: 60, loss: 0.0012368496973067522\n",
            "step: 70, loss: 0.001350420294329524\n",
            "step: 80, loss: 0.006833021529018879\n",
            "step: 90, loss: 0.04013385623693466\n",
            "step: 100, loss: 0.031000718474388123\n",
            "step: 110, loss: 0.001808822969906032\n",
            "step: 120, loss: 0.01570291817188263\n",
            "step: 130, loss: 0.0019540726207196712\n",
            "step: 140, loss: 0.0006584858638234437\n",
            "step: 150, loss: 0.2094680368900299\n",
            "step: 160, loss: 0.0031581574585288763\n",
            "step: 170, loss: 0.0018967926735058427\n",
            "step: 180, loss: 0.0006552713457494974\n",
            "step: 190, loss: 0.0014854577602818608\n",
            "step: 200, loss: 0.002490042243152857\n",
            "step: 210, loss: 0.2015429586172104\n",
            "step: 220, loss: 0.0004762661410495639\n",
            "step: 230, loss: 0.0018056719563901424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9787234042553192, f1=0.9807909604519773, best_f1=0.9731543624161074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008257052395492792\n",
            "step: 10, loss: 0.0006347854505293071\n",
            "step: 20, loss: 0.000865981332026422\n",
            "step: 30, loss: 0.0004132275062147528\n",
            "step: 40, loss: 0.001310772611759603\n",
            "step: 50, loss: 0.0006805428420193493\n",
            "step: 60, loss: 0.0028999524656683207\n",
            "step: 70, loss: 0.0003550726978573948\n",
            "step: 80, loss: 0.0004795589193236083\n",
            "step: 90, loss: 0.007233936805278063\n",
            "step: 100, loss: 0.0003679631045088172\n",
            "step: 110, loss: 0.0002012737386394292\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.049383193254470825\n",
            "step: 130, loss: 0.0017914199270308018\n",
            "step: 140, loss: 0.0065753706730902195\n",
            "step: 150, loss: 0.0007626080187037587\n",
            "step: 160, loss: 0.00075200927676633\n",
            "step: 170, loss: 0.005378887988626957\n",
            "step: 180, loss: 0.0009184793452732265\n",
            "step: 190, loss: 0.06941521912813187\n",
            "step: 200, loss: 0.006601470522582531\n",
            "step: 210, loss: 0.0005409940495155752\n",
            "step: 220, loss: 0.0012296239146962762\n",
            "step: 230, loss: 0.005494930315762758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9809203142536477, f1=0.9764837625979844, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004012235440313816\n",
            "step: 10, loss: 0.0003324321296531707\n",
            "step: 20, loss: 0.002303722547367215\n",
            "step: 30, loss: 0.00019296881509944797\n",
            "step: 40, loss: 0.00017057951481547207\n",
            "step: 50, loss: 0.0002506877644918859\n",
            "step: 60, loss: 0.00012973815319128335\n",
            "step: 70, loss: 0.0005652345716953278\n",
            "step: 80, loss: 0.0005483920685946941\n",
            "step: 90, loss: 0.0004498197231441736\n",
            "step: 100, loss: 0.0052136704325675964\n",
            "step: 110, loss: 0.000591420044656843\n",
            "step: 120, loss: 0.011302351951599121\n",
            "step: 130, loss: 0.0008310950943268836\n",
            "step: 140, loss: 0.04339362308382988\n",
            "step: 150, loss: 0.17386634647846222\n",
            "step: 160, loss: 0.002376652555540204\n",
            "step: 170, loss: 0.0007145072449930012\n",
            "step: 180, loss: 0.022567199543118477\n",
            "step: 190, loss: 0.0015926639316603541\n",
            "step: 200, loss: 0.00019354767573531717\n",
            "step: 210, loss: 0.0007546328706666827\n",
            "step: 220, loss: 0.0005504671717062593\n",
            "step: 230, loss: 0.07597699016332626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9732739420935412, f1=0.9688195991091313, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010365251218900084\n",
            "step: 10, loss: 0.00043732079211622477\n",
            "step: 20, loss: 0.0033697725739330053\n",
            "step: 30, loss: 0.005209349095821381\n",
            "step: 40, loss: 0.0004337483842391521\n",
            "step: 50, loss: 0.0005398810608312488\n",
            "step: 60, loss: 0.001485001645050943\n",
            "step: 70, loss: 0.0345061793923378\n",
            "step: 80, loss: 0.0018170162802562118\n",
            "step: 90, loss: 8.739143231650814e-05\n",
            "step: 100, loss: 0.00016150882584042847\n",
            "step: 110, loss: 0.0002713151043280959\n",
            "step: 120, loss: 0.00010861544433282688\n",
            "step: 130, loss: 9.250095899915323e-05\n",
            "step: 140, loss: 0.00029670391813851893\n",
            "step: 150, loss: 0.007512656506150961\n",
            "step: 160, loss: 0.08532062917947769\n",
            "step: 170, loss: 0.0006568029057234526\n",
            "step: 180, loss: 0.006081215105950832\n",
            "step: 190, loss: 0.0007209383766166866\n",
            "step: 200, loss: 0.07312536239624023\n",
            "step: 210, loss: 0.0003052636166103184\n",
            "step: 220, loss: 0.18199506402015686\n",
            "step: 230, loss: 0.004074115306138992\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9777777777777777, f1=0.9732142857142857, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00042994300019927323\n",
            "step: 10, loss: 0.0016563043463975191\n",
            "step: 20, loss: 0.0013335446128621697\n",
            "step: 30, loss: 0.0003195655590388924\n",
            "step: 40, loss: 0.0005175152909941971\n",
            "step: 50, loss: 0.0004717349074780941\n",
            "step: 60, loss: 0.00015820268890820444\n",
            "step: 70, loss: 0.0009589222609065473\n",
            "step: 80, loss: 0.0006130185211077332\n",
            "step: 90, loss: 7.982734678080305e-05\n",
            "step: 100, loss: 0.00045762636000290513\n",
            "step: 110, loss: 0.1347406804561615\n",
            "step: 120, loss: 0.0036429036408662796\n",
            "step: 130, loss: 0.007606690749526024\n",
            "step: 140, loss: 0.00022977097250986844\n",
            "step: 150, loss: 0.00028265881701372564\n",
            "step: 160, loss: 0.00040988618275150657\n",
            "step: 170, loss: 0.002501159207895398\n",
            "step: 180, loss: 0.003135404549539089\n",
            "step: 190, loss: 0.002000238047912717\n",
            "step: 200, loss: 0.0012482242891564965\n",
            "step: 210, loss: 0.002806038362905383\n",
            "step: 220, loss: 0.00037010753294453025\n",
            "step: 230, loss: 0.00032251651282422245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9797297297297298, f1=0.9808342728297633, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013852721895091236\n",
            "step: 10, loss: 0.0005453271442092955\n",
            "step: 20, loss: 0.0028776288963854313\n",
            "step: 30, loss: 0.0003472879179753363\n",
            "step: 40, loss: 0.015499849803745747\n",
            "step: 50, loss: 9.592338028596714e-05\n",
            "step: 60, loss: 0.00020079025125596672\n",
            "step: 70, loss: 0.0008052625344134867\n",
            "step: 80, loss: 9.902416786644608e-05\n",
            "step: 90, loss: 0.0004446821694727987\n",
            "step: 100, loss: 0.00019602762768045068\n",
            "step: 110, loss: 8.360071660717949e-05\n",
            "step: 120, loss: 9.181489440379664e-05\n",
            "step: 130, loss: 0.00010527660197112709\n",
            "step: 140, loss: 0.0008703062776476145\n",
            "step: 150, loss: 0.0002731503627728671\n",
            "step: 160, loss: 0.00029561787960119545\n",
            "step: 170, loss: 0.0007926073740236461\n",
            "step: 180, loss: 0.0012960205785930157\n",
            "step: 190, loss: 0.000516809057444334\n",
            "step: 200, loss: 0.0005083896685391665\n",
            "step: 210, loss: 0.0009832603391259909\n",
            "step: 220, loss: 0.00028504765941761434\n",
            "step: 230, loss: 0.00028536602621898055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9763779527559054, f1=0.9807909604519773, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00036132882814854383\n",
            "step: 10, loss: 0.0001810544345062226\n",
            "step: 20, loss: 0.00031753876828588545\n",
            "step: 30, loss: 0.0007364408229477704\n",
            "step: 40, loss: 0.00015657990297768265\n",
            "step: 50, loss: 0.00019116712792310864\n",
            "step: 60, loss: 0.0003894547699019313\n",
            "step: 70, loss: 0.0004039512714371085\n",
            "step: 80, loss: 0.0002745245583355427\n",
            "step: 90, loss: 0.0005845357081852853\n",
            "step: 100, loss: 0.0002603383327368647\n",
            "step: 110, loss: 0.0003304643032606691\n",
            "step: 120, loss: 0.00027320446679368615\n",
            "step: 130, loss: 0.0008421125239692628\n",
            "step: 140, loss: 0.060741450637578964\n",
            "step: 150, loss: 0.0166164543479681\n",
            "step: 160, loss: 0.0006050083902664483\n",
            "step: 170, loss: 0.00022141887166071683\n",
            "step: 180, loss: 0.00029121656552888453\n",
            "step: 190, loss: 0.003759015817195177\n",
            "step: 200, loss: 0.00022870775137562305\n",
            "step: 210, loss: 5.973963561700657e-05\n",
            "step: 220, loss: 0.00012047526251990348\n",
            "step: 230, loss: 0.0003662552044261247\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9787709497206705, f1=0.9809203142536477, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012300476373638958\n",
            "step: 10, loss: 0.0002754885936155915\n",
            "step: 20, loss: 0.000827212177682668\n",
            "step: 30, loss: 0.001417450257577002\n",
            "step: 40, loss: 0.00039926738827489316\n",
            "step: 50, loss: 0.0002466684381943196\n",
            "step: 60, loss: 0.000761483795940876\n",
            "step: 70, loss: 0.0002011418982874602\n",
            "step: 80, loss: 0.00010115825716638938\n",
            "step: 90, loss: 0.00012350598990451545\n",
            "step: 100, loss: 0.00011933620407944545\n",
            "step: 110, loss: 0.006277669221162796\n",
            "step: 120, loss: 0.00015861308202147484\n",
            "step: 130, loss: 8.263959171017632e-05\n",
            "step: 140, loss: 8.723133214516565e-05\n",
            "step: 150, loss: 0.0002639539889059961\n",
            "step: 160, loss: 0.00015941268065944314\n",
            "step: 170, loss: 0.0004873527796007693\n",
            "step: 180, loss: 0.00017574333469383419\n",
            "step: 190, loss: 0.0001234941155416891\n",
            "step: 200, loss: 9.623437654227018e-05\n",
            "step: 210, loss: 5.299536496750079e-05\n",
            "step: 220, loss: 8.820331277092919e-05\n",
            "step: 230, loss: 0.00017641477461438626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9798657718120806, f1=0.9820224719101124, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.552408300805837e-05\n",
            "step: 10, loss: 6.946772191440687e-05\n",
            "step: 20, loss: 0.00014468468725681305\n",
            "step: 30, loss: 0.00011384129174984992\n",
            "step: 40, loss: 0.00015367643209174275\n",
            "step: 50, loss: 0.0013036163290962577\n",
            "step: 60, loss: 0.00015826641174498945\n",
            "step: 70, loss: 5.079769834992476e-05\n",
            "step: 80, loss: 0.00011777428153436631\n",
            "step: 90, loss: 7.854637078708038e-05\n",
            "step: 100, loss: 6.97678406140767e-05\n",
            "step: 110, loss: 8.465073187835515e-05\n",
            "step: 120, loss: 8.642466855235398e-05\n",
            "step: 130, loss: 7.061679207254201e-05\n",
            "step: 140, loss: 0.0024976967833936214\n",
            "step: 150, loss: 0.00013549112190958112\n",
            "step: 160, loss: 0.00016607521683909\n",
            "step: 170, loss: 0.00010258772817905992\n",
            "step: 180, loss: 0.004322263412177563\n",
            "step: 190, loss: 0.00015578877355437726\n",
            "step: 200, loss: 8.034692291403189e-05\n",
            "step: 210, loss: 0.00015409791376441717\n",
            "step: 220, loss: 0.000274631951469928\n",
            "step: 230, loss: 0.0014812322333455086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.978865406006674, f1=0.9776286353467561, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00048712201532907784\n",
            "step: 10, loss: 0.0003158916952088475\n",
            "step: 20, loss: 0.004023152869194746\n",
            "step: 30, loss: 0.00018499261932447553\n",
            "step: 40, loss: 0.0002518461551517248\n",
            "step: 50, loss: 0.0002333280717721209\n",
            "step: 60, loss: 0.00013949019194114953\n",
            "step: 70, loss: 6.102844781707972e-05\n",
            "step: 80, loss: 0.0001138713996624574\n",
            "step: 90, loss: 0.00011826224363176152\n",
            "step: 100, loss: 6.0127982578705996e-05\n",
            "step: 110, loss: 0.00774032436311245\n",
            "step: 120, loss: 0.0004144276026636362\n",
            "step: 130, loss: 0.00032061644014902413\n",
            "step: 140, loss: 0.00023962634440977126\n",
            "step: 150, loss: 0.00020384743402246386\n",
            "step: 160, loss: 0.0002291215496370569\n",
            "step: 170, loss: 0.00019619902013801038\n",
            "step: 180, loss: 0.00015636863827239722\n",
            "step: 190, loss: 0.00011609386274358258\n",
            "step: 200, loss: 0.000464424432720989\n",
            "step: 210, loss: 6.962892803130671e-05\n",
            "step: 220, loss: 0.00017312593990936875\n",
            "step: 230, loss: 6.083375046728179e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9799554565701558, f1=0.9744160177975528, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015723789692856371\n",
            "step: 10, loss: 9.421409049537033e-05\n",
            "step: 20, loss: 0.000186983568710275\n",
            "step: 30, loss: 0.00013942105579189956\n",
            "step: 40, loss: 0.020689813420176506\n",
            "step: 50, loss: 0.00018326667486689985\n",
            "step: 60, loss: 7.473822188330814e-05\n",
            "step: 70, loss: 0.0008653935510665178\n",
            "step: 80, loss: 0.00013094609312247485\n",
            "step: 90, loss: 0.00010649021714925766\n",
            "step: 100, loss: 0.00016111622971948236\n",
            "step: 110, loss: 0.00011684656055876985\n",
            "step: 120, loss: 4.017371975351125e-05\n",
            "step: 130, loss: 9.705795673653483e-05\n",
            "step: 140, loss: 0.0001027550533763133\n",
            "step: 150, loss: 7.207548333099112e-05\n",
            "step: 160, loss: 0.0009882566519081593\n",
            "step: 170, loss: 4.185995567240752e-05\n",
            "step: 180, loss: 8.647425420349464e-05\n",
            "step: 190, loss: 0.00017233644030056894\n",
            "step: 200, loss: 6.323288107523695e-05\n",
            "step: 210, loss: 8.326645183842629e-05\n",
            "step: 220, loss: 0.0005851500318385661\n",
            "step: 230, loss: 0.0007989166188053787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9810901001112348, f1=0.9722530521642618, best_f1=0.9722530521642618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.5398239055648446e-05\n",
            "step: 10, loss: 4.4971468014409766e-05\n",
            "step: 20, loss: 0.0001931405859068036\n",
            "step: 30, loss: 5.463447087095119e-05\n",
            "step: 40, loss: 7.337704300880432e-05\n",
            "step: 50, loss: 0.022162288427352905\n",
            "step: 60, loss: 8.372803131351247e-05\n",
            "step: 70, loss: 0.00020014468464069068\n",
            "step: 80, loss: 0.0002531872596591711\n",
            "step: 90, loss: 0.00011610258661676198\n",
            "step: 100, loss: 9.698172652861103e-05\n",
            "step: 110, loss: 8.22337024146691e-05\n",
            "step: 120, loss: 0.0001365473581245169\n",
            "step: 130, loss: 0.0027968697249889374\n",
            "step: 140, loss: 7.751585508231074e-05\n",
            "step: 150, loss: 0.00031504558864980936\n",
            "step: 160, loss: 7.459281187038869e-05\n",
            "step: 170, loss: 4.671805800171569e-05\n",
            "step: 180, loss: 0.00015702626842539757\n",
            "step: 190, loss: 0.00018995914433617145\n",
            "step: 200, loss: 0.000308766815578565\n",
            "step: 210, loss: 6.686674169031903e-05\n",
            "step: 220, loss: 7.630258915014565e-05\n",
            "step: 230, loss: 7.715090032434091e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9810901001112348, f1=0.9733333333333333, best_f1=0.9722530521642618\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 178.67it/s]\n",
            "load_f1 = 0.9810901001112348\n",
            "real_f1 = 0.98\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 218.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275fe281-1ae2-40e2-a174-aad33c80006a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6352456212043762\n",
            "step: 10, loss: 0.532656192779541\n",
            "step: 20, loss: 0.4920925796031952\n",
            "step: 30, loss: 0.06079695373773575\n",
            "step: 40, loss: 0.12186449021100998\n",
            "step: 50, loss: 0.17821063101291656\n",
            "step: 60, loss: 0.2380864918231964\n",
            "step: 70, loss: 0.1546720266342163\n",
            "step: 80, loss: 0.13755249977111816\n",
            "step: 90, loss: 0.1255534291267395\n",
            "step: 100, loss: 0.01607920229434967\n",
            "step: 110, loss: 0.10515327751636505\n",
            "step: 120, loss: 0.14257046580314636\n",
            "step: 130, loss: 0.054857488721609116\n",
            "step: 140, loss: 0.08720608055591583\n",
            "step: 150, loss: 0.03432334586977959\n",
            "step: 160, loss: 0.0390322208404541\n",
            "step: 170, loss: 0.19540369510650635\n",
            "step: 180, loss: 0.04003281891345978\n",
            "step: 190, loss: 0.00964298378676176\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 200, loss: 0.24825255572795868\n",
            "step: 210, loss: 0.09779851883649826\n",
            "step: 220, loss: 0.22028222680091858\n",
            "step: 230, loss: 0.15740138292312622\n",
            "step: 240, loss: 0.08587373793125153\n",
            "step: 250, loss: 0.011150641366839409\n",
            "step: 260, loss: 0.030726589262485504\n",
            "step: 270, loss: 0.01858687773346901\n",
            "step: 280, loss: 0.037817247211933136\n",
            "step: 290, loss: 0.07088350504636765\n",
            "step: 300, loss: 0.020986733958125114\n",
            "step: 310, loss: 0.30826064944267273\n",
            "step: 320, loss: 0.13266515731811523\n",
            "step: 330, loss: 0.013411153107881546\n",
            "step: 340, loss: 0.04856182262301445\n",
            "step: 350, loss: 0.032818373292684555\n",
            "step: 360, loss: 0.032238882035017014\n",
            "step: 370, loss: 0.10225626081228256\n",
            "step: 380, loss: 0.023422878235578537\n",
            "step: 390, loss: 0.09636527299880981\n",
            "step: 400, loss: 0.23779603838920593\n",
            "step: 410, loss: 0.058281686156988144\n",
            "step: 420, loss: 0.016752738505601883\n",
            "step: 430, loss: 0.14313413202762604\n",
            "step: 440, loss: 0.045502543449401855\n",
            "step: 450, loss: 0.025120466947555542\n",
            "step: 460, loss: 0.005255893338471651\n",
            "step: 470, loss: 0.1647808849811554\n",
            "step: 480, loss: 0.053660567849874496\n",
            "step: 490, loss: 0.08075788617134094\n",
            "step: 500, loss: 0.07023809105157852\n",
            "step: 510, loss: 0.059497442096471786\n",
            "step: 520, loss: 0.059146683663129807\n",
            "step: 530, loss: 0.008487471379339695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.940959409594096, f1=0.9316081330868762, best_f1=0.9316081330868762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05307995527982712\n",
            "step: 10, loss: 0.027038387954235077\n",
            "step: 20, loss: 0.03937145322561264\n",
            "step: 30, loss: 0.034684669226408005\n",
            "step: 40, loss: 0.10374635457992554\n",
            "step: 50, loss: 0.08791260421276093\n",
            "step: 60, loss: 0.00403040973469615\n",
            "step: 70, loss: 0.01799757033586502\n",
            "step: 80, loss: 0.10266252607107162\n",
            "step: 90, loss: 0.011822097934782505\n",
            "step: 100, loss: 0.015914836898446083\n",
            "step: 110, loss: 0.11195503920316696\n",
            "step: 120, loss: 0.07486983388662338\n",
            "step: 130, loss: 0.08539516478776932\n",
            "step: 140, loss: 0.04493777081370354\n",
            "step: 150, loss: 0.03416001424193382\n",
            "step: 160, loss: 0.0036681590136140585\n",
            "step: 170, loss: 0.01093273889273405\n",
            "step: 180, loss: 0.011515399441123009\n",
            "step: 190, loss: 0.031393103301525116\n",
            "step: 200, loss: 0.0042618680745363235\n",
            "step: 210, loss: 0.04001837596297264\n",
            "step: 220, loss: 0.055941034108400345\n",
            "step: 230, loss: 0.007188259623944759\n",
            "step: 240, loss: 0.023960178717970848\n",
            "step: 250, loss: 0.06588230282068253\n",
            "step: 260, loss: 0.006638604681938887\n",
            "step: 270, loss: 0.24075284600257874\n",
            "step: 280, loss: 0.015767240896821022\n",
            "step: 290, loss: 0.012794791720807552\n",
            "step: 300, loss: 0.17454369366168976\n",
            "step: 310, loss: 0.013573684729635715\n",
            "step: 320, loss: 0.10062653571367264\n",
            "step: 330, loss: 0.03637361153960228\n",
            "step: 340, loss: 0.057780925184488297\n",
            "step: 350, loss: 0.002392341149970889\n",
            "step: 360, loss: 0.0874578133225441\n",
            "step: 370, loss: 0.15053382515907288\n",
            "step: 380, loss: 0.11606507748365402\n",
            "step: 390, loss: 0.04081861302256584\n",
            "step: 400, loss: 0.005448398645967245\n",
            "step: 410, loss: 0.021442698314785957\n",
            "step: 420, loss: 0.01940445974469185\n",
            "step: 430, loss: 0.02780776470899582\n",
            "step: 440, loss: 0.22075439989566803\n",
            "step: 450, loss: 0.031591128557920456\n",
            "step: 460, loss: 0.042020369321107864\n",
            "step: 470, loss: 0.0857573077082634\n",
            "step: 480, loss: 0.1926824152469635\n",
            "step: 490, loss: 0.021002596244215965\n",
            "step: 500, loss: 0.16381320357322693\n",
            "step: 510, loss: 0.014498014003038406\n",
            "step: 520, loss: 0.042679768055677414\n",
            "step: 530, loss: 0.07909803092479706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9438515081206497, f1=0.9354988399071927, best_f1=0.9354988399071927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08881131559610367\n",
            "step: 10, loss: 0.05082077160477638\n",
            "step: 20, loss: 0.14803123474121094\n",
            "step: 30, loss: 0.02156892605125904\n",
            "step: 40, loss: 0.053945232182741165\n",
            "step: 50, loss: 0.03322656452655792\n",
            "step: 60, loss: 0.07750485092401505\n",
            "step: 70, loss: 0.02244856022298336\n",
            "step: 80, loss: 0.001689729979261756\n",
            "step: 90, loss: 0.0052084531635046005\n",
            "step: 100, loss: 0.09629654884338379\n",
            "step: 110, loss: 0.002583834808319807\n",
            "step: 120, loss: 0.013688302598893642\n",
            "step: 130, loss: 0.02073977328836918\n",
            "step: 140, loss: 0.025178194046020508\n",
            "step: 150, loss: 0.010461008176207542\n",
            "step: 160, loss: 0.006455943454056978\n",
            "step: 170, loss: 0.05908512324094772\n",
            "step: 180, loss: 0.0027213275898247957\n",
            "step: 190, loss: 0.0029012414161115885\n",
            "step: 200, loss: 0.06631483137607574\n",
            "step: 210, loss: 0.08709229528903961\n",
            "step: 220, loss: 0.03519751876592636\n",
            "step: 230, loss: 0.08904144912958145\n",
            "step: 240, loss: 0.007115130312740803\n",
            "step: 250, loss: 0.043806906789541245\n",
            "step: 260, loss: 0.035538025200366974\n",
            "step: 270, loss: 0.0021400433033704758\n",
            "step: 280, loss: 0.1290028691291809\n",
            "step: 290, loss: 0.01291780173778534\n",
            "step: 300, loss: 0.04254065454006195\n",
            "step: 310, loss: 0.00626433826982975\n",
            "step: 320, loss: 0.027869101613759995\n",
            "step: 330, loss: 0.008855672553181648\n",
            "step: 340, loss: 0.015904366970062256\n",
            "step: 350, loss: 0.0033845435827970505\n",
            "step: 360, loss: 0.04372011125087738\n",
            "step: 370, loss: 0.1230047345161438\n",
            "step: 380, loss: 0.02883102558553219\n",
            "step: 390, loss: 0.05789574235677719\n",
            "step: 400, loss: 0.0031391477677971125\n",
            "step: 410, loss: 0.004923119675368071\n",
            "step: 420, loss: 0.05904005467891693\n",
            "step: 430, loss: 0.004538670182228088\n",
            "step: 440, loss: 0.03969212621450424\n",
            "step: 450, loss: 0.050042275339365005\n",
            "step: 460, loss: 0.05100805684924126\n",
            "step: 470, loss: 0.1640632003545761\n",
            "step: 480, loss: 0.005017090123146772\n",
            "step: 490, loss: 0.03212356194853783\n",
            "step: 500, loss: 0.03612346202135086\n",
            "step: 510, loss: 0.006496811751276255\n",
            "step: 520, loss: 0.12753599882125854\n",
            "step: 530, loss: 0.023568004369735718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9413936317489617, f1=0.933579335793358, best_f1=0.9354988399071927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022208592854440212\n",
            "step: 10, loss: 0.01092605758458376\n",
            "step: 20, loss: 0.0009163780487142503\n",
            "step: 30, loss: 0.0005403236136771739\n",
            "step: 40, loss: 0.006002435460686684\n",
            "step: 50, loss: 0.006505188532173634\n",
            "step: 60, loss: 0.002018225844949484\n",
            "step: 70, loss: 0.0027375505305826664\n",
            "step: 80, loss: 0.013471471145749092\n",
            "step: 90, loss: 0.019490810111165047\n",
            "step: 100, loss: 0.0006568660028278828\n",
            "step: 110, loss: 0.05766066163778305\n",
            "step: 120, loss: 0.0005804208340123296\n",
            "step: 130, loss: 0.00023348417016677558\n",
            "step: 140, loss: 0.001046252204105258\n",
            "step: 150, loss: 0.021371839568018913\n",
            "step: 160, loss: 0.06006613373756409\n",
            "step: 170, loss: 0.03196197375655174\n",
            "step: 180, loss: 0.001566588063724339\n",
            "step: 190, loss: 0.005957355257123709\n",
            "step: 200, loss: 0.0007454715087078512\n",
            "step: 210, loss: 0.008089900948107243\n",
            "step: 220, loss: 0.0027332683093845844\n",
            "step: 230, loss: 0.26554879546165466\n",
            "step: 240, loss: 0.11600121855735779\n",
            "step: 250, loss: 0.045750461518764496\n",
            "step: 260, loss: 0.0034014026168733835\n",
            "step: 270, loss: 0.008206683211028576\n",
            "step: 280, loss: 0.07357639074325562\n",
            "step: 290, loss: 0.061247654259204865\n",
            "step: 300, loss: 0.00023671610688325018\n",
            "step: 310, loss: 0.003964574076235294\n",
            "step: 320, loss: 0.08019253611564636\n",
            "step: 330, loss: 0.0027857949025928974\n",
            "step: 340, loss: 0.0376526415348053\n",
            "step: 350, loss: 0.0020336767192929983\n",
            "step: 360, loss: 0.04809261113405228\n",
            "step: 370, loss: 0.015599925071001053\n",
            "step: 380, loss: 0.0068705035373568535\n",
            "step: 390, loss: 0.0010084818350151181\n",
            "step: 400, loss: 0.0014838753268122673\n",
            "step: 410, loss: 0.004281720612198114\n",
            "step: 420, loss: 0.0007736764964647591\n",
            "step: 430, loss: 0.07263999432325363\n",
            "step: 440, loss: 0.007088647224009037\n",
            "step: 450, loss: 0.001683628186583519\n",
            "step: 460, loss: 0.00013786710042040795\n",
            "step: 470, loss: 0.01417747512459755\n",
            "step: 480, loss: 0.1459924280643463\n",
            "step: 490, loss: 0.002680265810340643\n",
            "step: 500, loss: 0.007866870611906052\n",
            "step: 510, loss: 0.05457304045557976\n",
            "step: 520, loss: 0.04830816760659218\n",
            "step: 530, loss: 0.021316329017281532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9427640763145649, f1=0.9211267605633803, best_f1=0.9354988399071927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0065442584455013275\n",
            "step: 10, loss: 0.015401566401124\n",
            "step: 20, loss: 0.0005035459762439132\n",
            "step: 30, loss: 0.007809363771229982\n",
            "step: 40, loss: 0.06171258166432381\n",
            "step: 50, loss: 0.00033302936935797334\n",
            "step: 60, loss: 0.021417973563075066\n",
            "step: 70, loss: 0.00597331952303648\n",
            "step: 80, loss: 0.00046365405432879925\n",
            "step: 90, loss: 0.06677165627479553\n",
            "step: 100, loss: 0.03670931234955788\n",
            "step: 110, loss: 0.0003650574362836778\n",
            "step: 120, loss: 0.28127509355545044\n",
            "step: 130, loss: 0.0014671020908281207\n",
            "step: 140, loss: 0.016812866553664207\n",
            "step: 150, loss: 0.0006488834624178708\n",
            "step: 160, loss: 0.0008260426111519337\n",
            "step: 170, loss: 0.04333383962512016\n",
            "step: 180, loss: 0.0008339261985383928\n",
            "step: 190, loss: 0.0011994760716333985\n",
            "step: 200, loss: 0.0005833275499753654\n",
            "step: 210, loss: 0.000534181366674602\n",
            "step: 220, loss: 0.0021475430112332106\n",
            "step: 230, loss: 0.001652037026360631\n",
            "step: 240, loss: 0.0020197622943669558\n",
            "step: 250, loss: 0.0007197251543402672\n",
            "step: 260, loss: 0.0010759307770058513\n",
            "step: 270, loss: 0.00020407364354468882\n",
            "step: 280, loss: 0.009686995297670364\n",
            "step: 290, loss: 0.12256216257810593\n",
            "step: 300, loss: 0.0033532017841935158\n",
            "step: 310, loss: 0.00026903304387815297\n",
            "step: 320, loss: 0.17594820261001587\n",
            "step: 330, loss: 0.028917592018842697\n",
            "step: 340, loss: 0.004591467324644327\n",
            "step: 350, loss: 0.00312393088825047\n",
            "step: 360, loss: 0.012666448950767517\n",
            "step: 370, loss: 0.004586067982017994\n",
            "step: 380, loss: 0.009680964052677155\n",
            "step: 390, loss: 0.0003097923472523689\n",
            "step: 400, loss: 0.018099164590239525\n",
            "step: 410, loss: 0.00317774317227304\n",
            "step: 420, loss: 0.00208106217905879\n",
            "step: 430, loss: 0.0008747575338929892\n",
            "step: 440, loss: 0.02083706296980381\n",
            "step: 450, loss: 0.0005861615645699203\n",
            "step: 460, loss: 0.00038406564272008836\n",
            "step: 470, loss: 0.006254795007407665\n",
            "step: 480, loss: 0.001845062244683504\n",
            "step: 490, loss: 0.0005312507273629308\n",
            "step: 500, loss: 0.008805819787085056\n",
            "step: 510, loss: 0.1228424683213234\n",
            "step: 520, loss: 0.002689980436116457\n",
            "step: 530, loss: 0.003558252239599824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9443665264142123, f1=0.927796130250118, best_f1=0.927796130250118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014722750056535006\n",
            "step: 10, loss: 0.000580055289901793\n",
            "step: 20, loss: 0.03210533410310745\n",
            "step: 30, loss: 0.014781951904296875\n",
            "step: 40, loss: 0.00038857501931488514\n",
            "step: 50, loss: 0.08755011856555939\n",
            "step: 60, loss: 0.00015033849922474474\n",
            "step: 70, loss: 0.0016178057994693518\n",
            "step: 80, loss: 0.013393661007285118\n",
            "step: 90, loss: 0.00021304235269781202\n",
            "step: 100, loss: 0.0016620771493762732\n",
            "step: 110, loss: 0.0009014095412567258\n",
            "step: 120, loss: 0.00016047271492425352\n",
            "step: 130, loss: 0.04183531925082207\n",
            "step: 140, loss: 0.027408452704548836\n",
            "step: 150, loss: 0.00257907179184258\n",
            "step: 160, loss: 0.0017884689150378108\n",
            "step: 170, loss: 0.0019395156996324658\n",
            "step: 180, loss: 0.00017158810805995017\n",
            "step: 190, loss: 0.00037509328103624284\n",
            "step: 200, loss: 0.026874886825680733\n",
            "step: 210, loss: 0.0003220711078029126\n",
            "step: 220, loss: 0.002009822055697441\n",
            "step: 230, loss: 0.0001576813228894025\n",
            "step: 240, loss: 0.07514437288045883\n",
            "step: 250, loss: 0.00031724601285532117\n",
            "step: 260, loss: 0.0002520749403629452\n",
            "step: 270, loss: 0.0355580635368824\n",
            "step: 280, loss: 0.0009859175188466907\n",
            "step: 290, loss: 0.001747244386933744\n",
            "step: 300, loss: 0.00274930358864367\n",
            "step: 310, loss: 0.0012286537094041705\n",
            "step: 320, loss: 0.0005308396648615599\n",
            "step: 330, loss: 0.0003123650676570833\n",
            "step: 340, loss: 0.04458165168762207\n",
            "step: 350, loss: 0.0014054151251912117\n",
            "step: 360, loss: 0.01646062545478344\n",
            "step: 370, loss: 0.0017760739428922534\n",
            "step: 380, loss: 9.918784053297713e-05\n",
            "step: 390, loss: 0.015925323590636253\n",
            "step: 400, loss: 0.0046504465863108635\n",
            "step: 410, loss: 0.0007588238804601133\n",
            "step: 420, loss: 0.02249894104897976\n",
            "step: 430, loss: 0.0032003656961023808\n",
            "step: 440, loss: 0.0005326466052792966\n",
            "step: 450, loss: 0.0014044526033103466\n",
            "step: 460, loss: 0.00026330258697271347\n",
            "step: 470, loss: 0.001081818132661283\n",
            "step: 480, loss: 0.00121210771612823\n",
            "step: 490, loss: 0.0060701146721839905\n",
            "step: 500, loss: 0.002458169823512435\n",
            "step: 510, loss: 0.0035434644669294357\n",
            "step: 520, loss: 0.004271852318197489\n",
            "step: 530, loss: 0.013587447814643383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9445221445221446, f1=0.9327731092436975, best_f1=0.9327731092436975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012078944564564154\n",
            "step: 10, loss: 0.0001429173134965822\n",
            "step: 20, loss: 0.009677836671471596\n",
            "step: 30, loss: 0.00020126551680732518\n",
            "step: 40, loss: 0.00048303065705113113\n",
            "step: 50, loss: 0.00028624440892599523\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.0038012166041880846\n",
            "step: 70, loss: 8.811733277980238e-05\n",
            "step: 80, loss: 0.0014472572365775704\n",
            "step: 90, loss: 0.014678142964839935\n",
            "step: 100, loss: 9.394087828695774e-05\n",
            "step: 110, loss: 0.0009082570904865861\n",
            "step: 120, loss: 0.00029536665533669293\n",
            "step: 130, loss: 0.001845344202592969\n",
            "step: 140, loss: 0.000203657808015123\n",
            "step: 150, loss: 0.00043896649731323123\n",
            "step: 160, loss: 0.0008892358164303005\n",
            "step: 170, loss: 0.0008139497367665172\n",
            "step: 180, loss: 0.004433443769812584\n",
            "step: 190, loss: 0.0030799710657447577\n",
            "step: 200, loss: 0.00030224976944737136\n",
            "step: 210, loss: 0.028197510167956352\n",
            "step: 220, loss: 0.0003109747776761651\n",
            "step: 230, loss: 0.002215280197560787\n",
            "step: 240, loss: 0.0029697760473936796\n",
            "step: 250, loss: 0.0002696537703741342\n",
            "step: 260, loss: 0.0002178713766625151\n",
            "step: 270, loss: 0.0002813331375364214\n",
            "step: 280, loss: 0.017970560118556023\n",
            "step: 290, loss: 9.982487972592935e-05\n",
            "step: 300, loss: 0.002331648487597704\n",
            "step: 310, loss: 0.0002233903214801103\n",
            "step: 320, loss: 0.00033017329405993223\n",
            "step: 330, loss: 0.011236673220992088\n",
            "step: 340, loss: 0.005445412360131741\n",
            "step: 350, loss: 0.0006032777018845081\n",
            "step: 360, loss: 0.0008671144023537636\n",
            "step: 370, loss: 0.009700414724647999\n",
            "step: 380, loss: 0.0002848602889571339\n",
            "step: 390, loss: 0.00037600655923597515\n",
            "step: 400, loss: 0.01208296325057745\n",
            "step: 410, loss: 0.001140745123848319\n",
            "step: 420, loss: 0.0004999063676223159\n",
            "step: 430, loss: 0.00026117145898751915\n",
            "step: 440, loss: 8.459188393317163e-05\n",
            "step: 450, loss: 9.718616638565436e-05\n",
            "step: 460, loss: 0.0007450153934769332\n",
            "step: 470, loss: 0.003453758778050542\n",
            "step: 480, loss: 0.0011901456164196134\n",
            "step: 490, loss: 0.0005360406939871609\n",
            "step: 500, loss: 0.0002722563222050667\n",
            "step: 510, loss: 0.0020879856310784817\n",
            "step: 520, loss: 0.08259718120098114\n",
            "step: 530, loss: 0.016101332381367683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9475638051044084, f1=0.9349442379182156, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033275847090408206\n",
            "step: 10, loss: 0.0888376235961914\n",
            "step: 20, loss: 0.00011313831782899797\n",
            "step: 30, loss: 0.004674664232879877\n",
            "step: 40, loss: 0.00019020926265511662\n",
            "step: 50, loss: 0.004598146304488182\n",
            "step: 60, loss: 0.000991497072391212\n",
            "step: 70, loss: 0.00012934762344229966\n",
            "step: 80, loss: 0.0003294001508038491\n",
            "step: 90, loss: 0.0009120390750467777\n",
            "step: 100, loss: 0.00018762817489914596\n",
            "step: 110, loss: 0.00021395388466771692\n",
            "step: 120, loss: 0.0005554570234380662\n",
            "step: 130, loss: 0.0009164518560282886\n",
            "step: 140, loss: 0.008454961702227592\n",
            "step: 150, loss: 9.441168367629871e-05\n",
            "step: 160, loss: 0.008967995643615723\n",
            "step: 170, loss: 0.0017946702428162098\n",
            "step: 180, loss: 0.0006239706999622285\n",
            "step: 190, loss: 0.002800486981868744\n",
            "step: 200, loss: 0.0005824528052471578\n",
            "step: 210, loss: 0.0001416657614754513\n",
            "step: 220, loss: 0.001734041958115995\n",
            "step: 230, loss: 0.0010843495838344097\n",
            "step: 240, loss: 0.00017280899919569492\n",
            "step: 250, loss: 4.524394898908213e-05\n",
            "step: 260, loss: 2.534641680540517e-05\n",
            "step: 270, loss: 0.0009379719267599285\n",
            "step: 280, loss: 0.05364244431257248\n",
            "step: 290, loss: 3.4081323974533007e-05\n",
            "step: 300, loss: 0.0001253883820027113\n",
            "step: 310, loss: 0.00032816905877552927\n",
            "step: 320, loss: 6.64361723465845e-05\n",
            "step: 330, loss: 0.0007293806411325932\n",
            "step: 340, loss: 6.607022078242153e-05\n",
            "step: 350, loss: 2.8624466722249053e-05\n",
            "step: 360, loss: 0.0001775790733518079\n",
            "step: 370, loss: 0.0003979323082603514\n",
            "step: 380, loss: 0.13780739903450012\n",
            "step: 390, loss: 0.1443852037191391\n",
            "step: 400, loss: 0.0003352757776156068\n",
            "step: 410, loss: 0.00036524931783787906\n",
            "step: 420, loss: 0.0033569294027984142\n",
            "step: 430, loss: 0.025360478088259697\n",
            "step: 440, loss: 0.0004312653618399054\n",
            "step: 450, loss: 0.0003283280530013144\n",
            "step: 460, loss: 0.00024406707962043583\n",
            "step: 470, loss: 0.0003828303306363523\n",
            "step: 480, loss: 0.002544017741456628\n",
            "step: 490, loss: 0.005405545234680176\n",
            "step: 500, loss: 0.005126782227307558\n",
            "step: 510, loss: 0.0024588333908468485\n",
            "step: 520, loss: 0.0014431508025154471\n",
            "step: 530, loss: 9.423073788639158e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9346497414198403, f1=0.9262564584311883, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.035900358110666275\n",
            "step: 10, loss: 0.08569949120283127\n",
            "step: 20, loss: 0.005672179628163576\n",
            "step: 30, loss: 0.000983733800239861\n",
            "step: 40, loss: 0.0005025133723393083\n",
            "step: 50, loss: 0.00017161104187835008\n",
            "step: 60, loss: 0.0003240134392399341\n",
            "step: 70, loss: 0.004549202974885702\n",
            "step: 80, loss: 0.006504269782453775\n",
            "step: 90, loss: 0.0002531386853661388\n",
            "step: 100, loss: 0.00014951755292713642\n",
            "step: 110, loss: 0.0005245048669166863\n",
            "step: 120, loss: 0.0002129328204318881\n",
            "step: 130, loss: 0.008850546553730965\n",
            "step: 140, loss: 0.03750121593475342\n",
            "step: 150, loss: 0.0002702741476241499\n",
            "step: 160, loss: 0.0023221548181027174\n",
            "step: 170, loss: 0.012745882384479046\n",
            "step: 180, loss: 3.3772215829230845e-05\n",
            "step: 190, loss: 4.07075131079182e-05\n",
            "step: 200, loss: 0.00010512029257370159\n",
            "step: 210, loss: 0.00010857820598175749\n",
            "step: 220, loss: 0.000540293229278177\n",
            "step: 230, loss: 7.047659164527431e-05\n",
            "step: 240, loss: 5.20244320796337e-05\n",
            "step: 250, loss: 0.0004027351678814739\n",
            "step: 260, loss: 0.050170186907052994\n",
            "step: 270, loss: 3.324328281451017e-05\n",
            "step: 280, loss: 0.00041969577432610095\n",
            "step: 290, loss: 0.024570681154727936\n",
            "step: 300, loss: 0.00022643906413577497\n",
            "step: 310, loss: 0.027915531769394875\n",
            "step: 320, loss: 0.00014720480248797685\n",
            "step: 330, loss: 0.00037520338082686067\n",
            "step: 340, loss: 2.6504883862799034e-05\n",
            "step: 350, loss: 0.000402268604375422\n",
            "step: 360, loss: 2.590149233583361e-05\n",
            "step: 370, loss: 0.0012039928697049618\n",
            "step: 380, loss: 4.8166883061639965e-05\n",
            "step: 390, loss: 2.422889338049572e-05\n",
            "step: 400, loss: 0.000761119881644845\n",
            "step: 410, loss: 3.1077874155016616e-05\n",
            "step: 420, loss: 3.7254198105074465e-05\n",
            "step: 430, loss: 2.5372439267812297e-05\n",
            "step: 440, loss: 8.324240479851142e-05\n",
            "step: 450, loss: 6.196922186063603e-05\n",
            "step: 460, loss: 3.4971515560755506e-05\n",
            "step: 470, loss: 2.09358458960196e-05\n",
            "step: 480, loss: 3.118707172689028e-05\n",
            "step: 490, loss: 0.0001942959934240207\n",
            "step: 500, loss: 0.0014911815524101257\n",
            "step: 510, loss: 0.008601892739534378\n",
            "step: 520, loss: 3.112759441137314e-05\n",
            "step: 530, loss: 0.003350219689309597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9359742054352833, f1=0.9274563820018366, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019074585288763046\n",
            "step: 10, loss: 0.00030217779567465186\n",
            "step: 20, loss: 0.06172877177596092\n",
            "step: 30, loss: 8.043101843213663e-05\n",
            "step: 40, loss: 0.007077313028275967\n",
            "step: 50, loss: 0.11527369916439056\n",
            "step: 60, loss: 3.7698580854339525e-05\n",
            "step: 70, loss: 0.0003129063989035785\n",
            "step: 80, loss: 0.0003905271296389401\n",
            "step: 90, loss: 0.00038872819277457893\n",
            "step: 100, loss: 4.242406430421397e-05\n",
            "step: 110, loss: 7.542043749708682e-05\n",
            "step: 120, loss: 4.4429092667996883e-05\n",
            "step: 130, loss: 5.744767258875072e-05\n",
            "step: 140, loss: 0.0034593220334500074\n",
            "step: 150, loss: 2.9678678401978686e-05\n",
            "step: 160, loss: 3.157074024784379e-05\n",
            "step: 170, loss: 0.004440210293978453\n",
            "step: 180, loss: 0.014275982044637203\n",
            "step: 190, loss: 0.0009144871146418154\n",
            "step: 200, loss: 0.0005382047384046018\n",
            "step: 210, loss: 0.006974448449909687\n",
            "step: 220, loss: 0.0012798068346455693\n",
            "step: 230, loss: 0.00044984364649280906\n",
            "step: 240, loss: 0.00016055967716965824\n",
            "step: 250, loss: 7.072094740578905e-05\n",
            "step: 260, loss: 8.953754877438769e-05\n",
            "step: 270, loss: 0.00019886373775079846\n",
            "step: 280, loss: 0.00019089740817435086\n",
            "step: 290, loss: 9.361794946016744e-05\n",
            "step: 300, loss: 3.408826523809694e-05\n",
            "step: 310, loss: 0.0005290798144415021\n",
            "step: 320, loss: 0.0012029283680021763\n",
            "step: 330, loss: 0.0004954914911650121\n",
            "step: 340, loss: 0.0001736489066388458\n",
            "step: 350, loss: 0.0006868992932140827\n",
            "step: 360, loss: 5.698019958799705e-05\n",
            "step: 370, loss: 0.0017844465328380466\n",
            "step: 380, loss: 0.011674645356833935\n",
            "step: 390, loss: 0.0005058327806182206\n",
            "step: 400, loss: 0.0001909376005642116\n",
            "step: 410, loss: 0.0016935961320996284\n",
            "step: 420, loss: 0.00010964230023091659\n",
            "step: 430, loss: 0.00011153284140164033\n",
            "step: 440, loss: 0.005069886799901724\n",
            "step: 450, loss: 3.154016667394899e-05\n",
            "step: 460, loss: 6.978342571528628e-05\n",
            "step: 470, loss: 0.06540845334529877\n",
            "step: 480, loss: 0.0018054820830002427\n",
            "step: 490, loss: 0.004261494614183903\n",
            "step: 500, loss: 0.004970652516931295\n",
            "step: 510, loss: 8.936840458773077e-05\n",
            "step: 520, loss: 0.00012036192492814735\n",
            "step: 530, loss: 0.0006637206533923745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9424426766495088, f1=0.9349670122525918, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.4150030337041244e-05\n",
            "step: 10, loss: 0.0005612321547232568\n",
            "step: 20, loss: 0.00010091267176903784\n",
            "step: 30, loss: 0.0008609667420387268\n",
            "step: 40, loss: 9.940540621755645e-05\n",
            "step: 50, loss: 0.002215864835307002\n",
            "step: 60, loss: 0.00028721755370497704\n",
            "step: 70, loss: 6.252850289456546e-05\n",
            "step: 80, loss: 6.537089211633429e-05\n",
            "step: 90, loss: 3.635667235357687e-05\n",
            "step: 100, loss: 0.00020437703642528504\n",
            "step: 110, loss: 0.0012185610830783844\n",
            "step: 120, loss: 9.330708417110145e-05\n",
            "step: 130, loss: 9.665815014159307e-05\n",
            "step: 140, loss: 0.0007849445100873709\n",
            "step: 150, loss: 2.1494533939403482e-05\n",
            "step: 160, loss: 3.738162195077166e-05\n",
            "step: 170, loss: 3.065380224143155e-05\n",
            "step: 180, loss: 0.00012036920816171914\n",
            "step: 190, loss: 0.00032386204111389816\n",
            "step: 200, loss: 0.0025339967105537653\n",
            "step: 210, loss: 0.0014430651208385825\n",
            "step: 220, loss: 0.001346826902590692\n",
            "step: 230, loss: 1.810812136682216e-05\n",
            "step: 240, loss: 0.0007832312257960439\n",
            "step: 250, loss: 1.834304930525832e-05\n",
            "step: 260, loss: 9.615595627110451e-05\n",
            "step: 270, loss: 2.7704281819751486e-05\n",
            "step: 280, loss: 3.538517194101587e-05\n",
            "step: 290, loss: 4.926165274810046e-05\n",
            "step: 300, loss: 3.831532740150578e-05\n",
            "step: 310, loss: 4.379485471872613e-05\n",
            "step: 320, loss: 2.9275293854880147e-05\n",
            "step: 330, loss: 2.8080368792871013e-05\n",
            "step: 340, loss: 3.29681315633934e-05\n",
            "step: 350, loss: 3.991400444647297e-05\n",
            "step: 360, loss: 1.8413851648801938e-05\n",
            "step: 370, loss: 0.000699808239005506\n",
            "step: 380, loss: 2.5174864276777953e-05\n",
            "step: 390, loss: 3.033691245946102e-05\n",
            "step: 400, loss: 5.0011101848213e-05\n",
            "step: 410, loss: 4.078200436197221e-05\n",
            "step: 420, loss: 5.241105463937856e-05\n",
            "step: 430, loss: 2.3740760298096575e-05\n",
            "step: 440, loss: 0.00010826743528014049\n",
            "step: 450, loss: 0.0003316767397336662\n",
            "step: 460, loss: 0.0010340400040149689\n",
            "step: 470, loss: 6.627482798648998e-05\n",
            "step: 480, loss: 0.0021977233700454235\n",
            "step: 490, loss: 0.005473866127431393\n",
            "step: 500, loss: 5.261131809675135e-05\n",
            "step: 510, loss: 7.009629189269617e-05\n",
            "step: 520, loss: 8.390354923903942e-05\n",
            "step: 530, loss: 2.5453733542235568e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9432029795158287, f1=0.9325842696629213, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.684379433048889e-05\n",
            "step: 10, loss: 2.0559560653055087e-05\n",
            "step: 20, loss: 7.10170206730254e-05\n",
            "step: 30, loss: 0.017044879496097565\n",
            "step: 40, loss: 0.0008147166226990521\n",
            "step: 50, loss: 0.03231536224484444\n",
            "step: 60, loss: 0.001815488561987877\n",
            "step: 70, loss: 2.3398082703351974e-05\n",
            "step: 80, loss: 4.620687468559481e-05\n",
            "step: 90, loss: 2.3789079932612367e-05\n",
            "step: 100, loss: 4.731542139779776e-05\n",
            "step: 110, loss: 2.658274934219662e-05\n",
            "step: 120, loss: 5.2597872127080336e-05\n",
            "step: 130, loss: 0.0011934642679989338\n",
            "step: 140, loss: 3.467969145276584e-05\n",
            "step: 150, loss: 5.3532672609435394e-05\n",
            "step: 160, loss: 1.7165915778605267e-05\n",
            "step: 170, loss: 0.0010301629081368446\n",
            "step: 180, loss: 0.0004798089503310621\n",
            "step: 190, loss: 0.0006501968600787222\n",
            "step: 200, loss: 2.911157935159281e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 210, loss: 3.0559491278836504e-05\n",
            "step: 220, loss: 5.469157622428611e-05\n",
            "step: 230, loss: 2.780738941510208e-05\n",
            "step: 240, loss: 1.894240631372668e-05\n",
            "step: 250, loss: 1.7694877897156402e-05\n",
            "step: 260, loss: 1.9944875020883046e-05\n",
            "step: 270, loss: 2.692179987207055e-05\n",
            "step: 280, loss: 0.001557032112032175\n",
            "step: 290, loss: 6.852135265944526e-05\n",
            "step: 300, loss: 0.0006243243115022779\n",
            "step: 310, loss: 0.0001528951688669622\n",
            "step: 320, loss: 1.6066893294919282e-05\n",
            "step: 330, loss: 3.5295375710120425e-05\n",
            "step: 340, loss: 2.102134021697566e-05\n",
            "step: 350, loss: 0.06537846475839615\n",
            "step: 360, loss: 6.644369568675756e-05\n",
            "step: 370, loss: 3.8473601307487115e-05\n",
            "step: 380, loss: 0.0004073140735272318\n",
            "step: 390, loss: 8.273391722468659e-05\n",
            "step: 400, loss: 2.7476717150420882e-05\n",
            "step: 410, loss: 0.00013294164091348648\n",
            "step: 420, loss: 0.004273209720849991\n",
            "step: 430, loss: 0.008469559252262115\n",
            "step: 440, loss: 5.51508528587874e-05\n",
            "step: 450, loss: 0.000255387945799157\n",
            "step: 460, loss: 0.00014757426106370986\n",
            "step: 470, loss: 1.9717597751878202e-05\n",
            "step: 480, loss: 0.00038425609818659723\n",
            "step: 490, loss: 6.563869828823954e-05\n",
            "step: 500, loss: 3.27457491948735e-05\n",
            "step: 510, loss: 0.0004365809727460146\n",
            "step: 520, loss: 2.2623031327384524e-05\n",
            "step: 530, loss: 3.9045375160640106e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9432723863103609, f1=0.9338959212376934, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021187704987823963\n",
            "step: 10, loss: 1.6614578271401115e-05\n",
            "step: 20, loss: 3.8112502807052806e-05\n",
            "step: 30, loss: 2.4049908461165614e-05\n",
            "step: 40, loss: 3.515990829328075e-05\n",
            "step: 50, loss: 2.8497399398474954e-05\n",
            "step: 60, loss: 2.8828082577092573e-05\n",
            "step: 70, loss: 1.8596158042782918e-05\n",
            "step: 80, loss: 1.3135260815033689e-05\n",
            "step: 90, loss: 0.00607477780431509\n",
            "step: 100, loss: 4.562237882055342e-05\n",
            "step: 110, loss: 1.8775113858282566e-05\n",
            "step: 120, loss: 6.44843967165798e-05\n",
            "step: 130, loss: 2.3520051399827935e-05\n",
            "step: 140, loss: 5.7684512285050005e-05\n",
            "step: 150, loss: 0.0002632945543155074\n",
            "step: 160, loss: 4.811092003365047e-05\n",
            "step: 170, loss: 0.00022079811606090516\n",
            "step: 180, loss: 3.191962241544388e-05\n",
            "step: 190, loss: 3.0158334993757308e-05\n",
            "step: 200, loss: 1.8801290934788994e-05\n",
            "step: 210, loss: 3.293979534646496e-05\n",
            "step: 220, loss: 1.8268556232214905e-05\n",
            "step: 230, loss: 1.528839857201092e-05\n",
            "step: 240, loss: 2.9216182156233117e-05\n",
            "step: 250, loss: 1.7024334738380276e-05\n",
            "step: 260, loss: 0.00016152577882166952\n",
            "step: 270, loss: 1.6905110896914266e-05\n",
            "step: 280, loss: 0.0005121264839544892\n",
            "step: 290, loss: 3.820767597062513e-05\n",
            "step: 300, loss: 1.740063089528121e-05\n",
            "step: 310, loss: 0.0005043118144385517\n",
            "step: 320, loss: 0.0024921754375100136\n",
            "step: 330, loss: 0.0013212822377681732\n",
            "step: 340, loss: 2.0548113752738573e-05\n",
            "step: 350, loss: 2.083496656268835e-05\n",
            "step: 360, loss: 7.121159433154389e-05\n",
            "step: 370, loss: 2.127837979060132e-05\n",
            "step: 380, loss: 1.9441917174845003e-05\n",
            "step: 390, loss: 0.0002532093785703182\n",
            "step: 400, loss: 5.2248298743506894e-05\n",
            "step: 410, loss: 0.00107634998857975\n",
            "step: 420, loss: 2.8899290555273183e-05\n",
            "step: 430, loss: 8.142222941387445e-05\n",
            "step: 440, loss: 3.002036282850895e-05\n",
            "step: 450, loss: 6.147746171336621e-05\n",
            "step: 460, loss: 0.0016409845557063818\n",
            "step: 470, loss: 1.2826050806324929e-05\n",
            "step: 480, loss: 1.8302038370165974e-05\n",
            "step: 490, loss: 1.981060631806031e-05\n",
            "step: 500, loss: 0.00020601005235221237\n",
            "step: 510, loss: 3.336286681587808e-05\n",
            "step: 520, loss: 0.0003100034373346716\n",
            "step: 530, loss: 0.00019265408627688885\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9415067852129153, f1=0.930341280972417, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000868661911226809\n",
            "step: 10, loss: 1.227846496476559e-05\n",
            "step: 20, loss: 1.4278882190410513e-05\n",
            "step: 30, loss: 1.5456047549378127e-05\n",
            "step: 40, loss: 5.9080804930999875e-05\n",
            "step: 50, loss: 0.00029316023574210703\n",
            "step: 60, loss: 1.8834507500287145e-05\n",
            "step: 70, loss: 5.071176929050125e-05\n",
            "step: 80, loss: 3.094153726124205e-05\n",
            "step: 90, loss: 1.6428275557700545e-05\n",
            "step: 100, loss: 2.1978346921969205e-05\n",
            "step: 110, loss: 4.7870282287476584e-05\n",
            "step: 120, loss: 3.1450494134332985e-05\n",
            "step: 130, loss: 0.028756888583302498\n",
            "step: 140, loss: 5.5009510106174275e-05\n",
            "step: 150, loss: 1.2233758752699941e-05\n",
            "step: 160, loss: 0.0011182681191712618\n",
            "step: 170, loss: 6.393908552126959e-05\n",
            "step: 180, loss: 1.326564324699575e-05\n",
            "step: 190, loss: 1.3988328646519221e-05\n",
            "step: 200, loss: 2.3740514734527096e-05\n",
            "step: 210, loss: 1.349660669802688e-05\n",
            "step: 220, loss: 2.189666156482417e-05\n",
            "step: 230, loss: 1.9546254407032393e-05\n",
            "step: 240, loss: 1.3265635061543435e-05\n",
            "step: 250, loss: 2.4202247004723176e-05\n",
            "step: 260, loss: 0.0002520121051929891\n",
            "step: 270, loss: 1.4144735359877814e-05\n",
            "step: 280, loss: 2.0417646737769246e-05\n",
            "step: 290, loss: 0.0006317399092949927\n",
            "step: 300, loss: 3.9391739846905693e-05\n",
            "step: 310, loss: 7.702823495492339e-05\n",
            "step: 320, loss: 4.342442116467282e-05\n",
            "step: 330, loss: 3.638501948444173e-05\n",
            "step: 340, loss: 0.0002186959027312696\n",
            "step: 350, loss: 2.982066507684067e-05\n",
            "step: 360, loss: 0.0001576800859766081\n",
            "step: 370, loss: 1.877510476333555e-05\n",
            "step: 380, loss: 5.421416062745266e-05\n",
            "step: 390, loss: 0.00196754839271307\n",
            "step: 400, loss: 0.00014047326112631708\n",
            "step: 410, loss: 1.3004852917219978e-05\n",
            "step: 420, loss: 1.6193351257243194e-05\n",
            "step: 430, loss: 2.2459260435425676e-05\n",
            "step: 440, loss: 5.0391921831760556e-05\n",
            "step: 450, loss: 0.00011274118151050061\n",
            "step: 460, loss: 1.9694754882948473e-05\n",
            "step: 470, loss: 5.077738023828715e-05\n",
            "step: 480, loss: 1.8253618691232987e-05\n",
            "step: 490, loss: 2.209755257354118e-05\n",
            "step: 500, loss: 6.998007302172482e-05\n",
            "step: 510, loss: 2.1717778508900665e-05\n",
            "step: 520, loss: 2.375137592025567e-05\n",
            "step: 530, loss: 2.117362237186171e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9446768944676894, f1=0.9369202226345084, best_f1=0.9349442379182156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.6055788364610635e-05\n",
            "step: 10, loss: 1.56497389980359e-05\n",
            "step: 20, loss: 1.619734575797338e-05\n",
            "step: 30, loss: 2.5267048840760253e-05\n",
            "step: 40, loss: 0.06811527907848358\n",
            "step: 50, loss: 4.610866017173976e-05\n",
            "step: 60, loss: 6.52066373731941e-05\n",
            "step: 70, loss: 5.738526306231506e-05\n",
            "step: 80, loss: 1.4141021893010475e-05\n",
            "step: 90, loss: 1.6636933651170693e-05\n",
            "step: 100, loss: 3.104678398813121e-05\n",
            "step: 110, loss: 0.00035107709118165076\n",
            "step: 120, loss: 1.848823558248114e-05\n",
            "step: 130, loss: 0.09543679654598236\n",
            "step: 140, loss: 1.5020188584458083e-05\n",
            "step: 150, loss: 1.6610865714028478e-05\n",
            "step: 160, loss: 1.4677460058010183e-05\n",
            "step: 170, loss: 1.3247007700556424e-05\n",
            "step: 180, loss: 1.4390592696145177e-05\n",
            "step: 190, loss: 1.7389445929438807e-05\n",
            "step: 200, loss: 1.5571500625810586e-05\n",
            "step: 210, loss: 0.00045110666542313993\n",
            "step: 220, loss: 1.3828144801664166e-05\n",
            "step: 230, loss: 4.031208300148137e-05\n",
            "step: 240, loss: 1.5411324056913145e-05\n",
            "step: 250, loss: 2.2958149202167988e-05\n",
            "step: 260, loss: 2.5434053895878606e-05\n",
            "step: 270, loss: 1.5467214325326495e-05\n",
            "step: 280, loss: 1.4167132576403674e-05\n",
            "step: 290, loss: 1.1891031135746744e-05\n",
            "step: 300, loss: 1.2811158740078099e-05\n",
            "step: 310, loss: 7.318014104384929e-05\n",
            "step: 320, loss: 5.340143616194837e-05\n",
            "step: 330, loss: 2.658828452695161e-05\n",
            "step: 340, loss: 1.1511061529745348e-05\n",
            "step: 350, loss: 1.1350874956406187e-05\n",
            "step: 360, loss: 0.0016202251426875591\n",
            "step: 370, loss: 1.284839163417928e-05\n",
            "step: 380, loss: 2.0801364371436648e-05\n",
            "step: 390, loss: 0.00016117018822114915\n",
            "step: 400, loss: 1.9393430193304084e-05\n",
            "step: 410, loss: 2.340115315746516e-05\n",
            "step: 420, loss: 3.463691973593086e-05\n",
            "step: 430, loss: 2.4768720322754234e-05\n",
            "step: 440, loss: 0.00020703398331534117\n",
            "step: 450, loss: 1.1674964298435953e-05\n",
            "step: 460, loss: 1.3403475350060035e-05\n",
            "step: 470, loss: 0.0012729786103591323\n",
            "step: 480, loss: 3.249774454161525e-05\n",
            "step: 490, loss: 1.2174137737019919e-05\n",
            "step: 500, loss: 1.4610395737690851e-05\n",
            "step: 510, loss: 2.6403866286273114e-05\n",
            "step: 520, loss: 1.4424172150029335e-05\n",
            "step: 530, loss: 0.0002801791997626424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9463955637707948, f1=0.9371534195933457, best_f1=0.9349442379182156\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:26, 215.35it/s]\n",
            "load_f1 = 0.9479553903345725\n",
            "real_f1 = 0.9480519480519481\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 226.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed802a8-a130-4c1b-dc5d-ef616373aa51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=936c07b8b1446469558e86da218c38468de7f68a8438700f3b3ff9128ce1ea9f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5o83u1n/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5deb85f3-d755-4a21-eda5-14070e134d4c"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5824135541915894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.35294117647058826, f1=0.3333333333333333, best_f1=0.3333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5115506649017334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4, f1=0.4, best_f1=0.4\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5545016527175903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6000000000000001, f1=0.5000000000000001, best_f1=0.5000000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2693830728530884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6956521739130435, f1=0.5384615384615384, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3034687638282776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7586206896551724, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19246216118335724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8, f1=0.7200000000000001, best_f1=0.7200000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028797170147299767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8666666666666666, f1=0.7333333333333334, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06198470667004585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8275862068965518, f1=0.689655172413793, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002338794758543372\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8275862068965518, f1=0.689655172413793, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01109262928366661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8750000000000001, f1=0.7741935483870968, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022751064971089363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8666666666666666, f1=0.7333333333333334, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031614163890480995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8666666666666666, f1=0.8125000000000001, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029105134308338165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8666666666666666, f1=0.8125000000000001, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032919321674853563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8666666666666666, f1=0.8125000000000001, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002949628047645092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8666666666666666, f1=0.8125000000000001, best_f1=0.7741935483870968\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 136950.72it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.9032258064516129\n",
            "real_f1 = 0.8666666666666666\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 201.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ca5a08-670c-4b8b-f389-4ba54583547d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5789301991462708\n",
            "step: 10, loss: 0.6137872338294983\n",
            "step: 20, loss: 0.3091577887535095\n",
            "step: 30, loss: 0.1237759217619896\n",
            "step: 40, loss: 0.24463814496994019\n",
            "step: 50, loss: 0.2299698144197464\n",
            "step: 60, loss: 0.11966574192047119\n",
            "step: 70, loss: 0.0129395117983222\n",
            "step: 80, loss: 0.05258003994822502\n",
            "step: 90, loss: 0.17542080581188202\n",
            "step: 100, loss: 0.005857539363205433\n",
            "step: 110, loss: 0.17704127728939056\n",
            "step: 120, loss: 0.009904581122100353\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.004264908377081156\n",
            "step: 140, loss: 0.002968188840895891\n",
            "step: 150, loss: 0.010766617953777313\n",
            "step: 160, loss: 0.006788244470953941\n",
            "step: 170, loss: 0.15663154423236847\n",
            "step: 180, loss: 0.09027890861034393\n",
            "step: 190, loss: 0.004888301249593496\n",
            "step: 200, loss: 0.004113960079848766\n",
            "step: 210, loss: 0.0030322372913360596\n",
            "step: 220, loss: 0.02290383353829384\n",
            "step: 230, loss: 0.04465249553322792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9784335981838819, f1=0.9748283752860413, best_f1=0.9748283752860413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014785799197852612\n",
            "step: 10, loss: 0.002340613631531596\n",
            "step: 20, loss: 0.06454721838235855\n",
            "step: 30, loss: 0.21658356487751007\n",
            "step: 40, loss: 0.05651390179991722\n",
            "step: 50, loss: 0.021752888336777687\n",
            "step: 60, loss: 0.008269977755844593\n",
            "step: 70, loss: 0.16264723241329193\n",
            "step: 80, loss: 0.02326761931180954\n",
            "step: 90, loss: 0.013595158234238625\n",
            "step: 100, loss: 0.1310613751411438\n",
            "step: 110, loss: 0.010860726237297058\n",
            "step: 120, loss: 0.0018513525137677789\n",
            "step: 130, loss: 0.011559084989130497\n",
            "step: 140, loss: 0.008608533069491386\n",
            "step: 150, loss: 0.003684011520817876\n",
            "step: 160, loss: 0.0014799832133576274\n",
            "step: 170, loss: 0.0009657465852797031\n",
            "step: 180, loss: 0.0032463683746755123\n",
            "step: 190, loss: 0.0019865590147674084\n",
            "step: 200, loss: 0.012713221833109856\n",
            "step: 210, loss: 0.0013918100157752633\n",
            "step: 220, loss: 0.028180988505482674\n",
            "step: 230, loss: 0.00275380234234035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9910112359550561, f1=0.9864864864864865, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007534455973654985\n",
            "step: 10, loss: 0.0038136125076562166\n",
            "step: 20, loss: 0.0008156837429851294\n",
            "step: 30, loss: 0.02373586595058441\n",
            "step: 40, loss: 0.011023142375051975\n",
            "step: 50, loss: 0.016957266256213188\n",
            "step: 60, loss: 0.001377667998895049\n",
            "step: 70, loss: 0.0011758541222661734\n",
            "step: 80, loss: 0.0272555872797966\n",
            "step: 90, loss: 0.16801594197750092\n",
            "step: 100, loss: 0.03303420543670654\n",
            "step: 110, loss: 0.006063293199986219\n",
            "step: 120, loss: 0.013111559674143791\n",
            "step: 130, loss: 0.010423421859741211\n",
            "step: 140, loss: 0.008851010352373123\n",
            "step: 150, loss: 0.08775696158409119\n",
            "step: 160, loss: 0.008073955774307251\n",
            "step: 170, loss: 0.00827016867697239\n",
            "step: 180, loss: 0.004778970964252949\n",
            "step: 190, loss: 0.0031471664551645517\n",
            "step: 200, loss: 0.09852371364831924\n",
            "step: 210, loss: 0.017431670799851418\n",
            "step: 220, loss: 0.0011333677684888244\n",
            "step: 230, loss: 0.0012397998943924904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9865168539325843, f1=0.9807474518686297, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0025665683206170797\n",
            "step: 10, loss: 0.00037208598223514855\n",
            "step: 20, loss: 0.0007187819574028254\n",
            "step: 30, loss: 0.0005324904923327267\n",
            "step: 40, loss: 0.007887943647801876\n",
            "step: 50, loss: 0.001898848102428019\n",
            "step: 60, loss: 0.059793103486299515\n",
            "step: 70, loss: 0.0009489909280091524\n",
            "step: 80, loss: 0.0023602142464369535\n",
            "step: 90, loss: 0.002676488598808646\n",
            "step: 100, loss: 0.001125846989452839\n",
            "step: 110, loss: 0.0018777906661853194\n",
            "step: 120, loss: 0.10564666986465454\n",
            "step: 130, loss: 0.03806113451719284\n",
            "step: 140, loss: 0.001880354480817914\n",
            "step: 150, loss: 0.15520089864730835\n",
            "step: 160, loss: 0.07911291718482971\n",
            "step: 170, loss: 0.03405435383319855\n",
            "step: 180, loss: 0.0007204685243777931\n",
            "step: 190, loss: 0.011924228630959988\n",
            "step: 200, loss: 0.004841760266572237\n",
            "step: 210, loss: 0.014650719240307808\n",
            "step: 220, loss: 0.0005343310185708106\n",
            "step: 230, loss: 0.0015393487410619855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9898762654668166, f1=0.9829738933030647, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031950471457093954\n",
            "step: 10, loss: 0.002927830209955573\n",
            "step: 20, loss: 0.029693299904465675\n",
            "step: 30, loss: 0.00016181018145289272\n",
            "step: 40, loss: 0.0026565983425825834\n",
            "step: 50, loss: 0.00035297288559377193\n",
            "step: 60, loss: 0.004111907910555601\n",
            "step: 70, loss: 0.0004934178432449698\n",
            "step: 80, loss: 0.023174215108156204\n",
            "step: 90, loss: 0.0023681907914578915\n",
            "step: 100, loss: 0.00033510272623971105\n",
            "step: 110, loss: 0.00018988960073329508\n",
            "step: 120, loss: 0.00010109437425853685\n",
            "step: 130, loss: 0.005273755639791489\n",
            "step: 140, loss: 0.003439879510551691\n",
            "step: 150, loss: 0.002187910722568631\n",
            "step: 160, loss: 0.0014069804456084967\n",
            "step: 170, loss: 0.009851833805441856\n",
            "step: 180, loss: 0.0012482665479183197\n",
            "step: 190, loss: 0.01845129206776619\n",
            "step: 200, loss: 0.010135342366993427\n",
            "step: 210, loss: 0.000838595733512193\n",
            "step: 220, loss: 0.00139195891097188\n",
            "step: 230, loss: 0.0006822920404374599\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9921259842519685, f1=0.9841269841269841, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009979127207770944\n",
            "step: 10, loss: 0.0004169714520685375\n",
            "step: 20, loss: 0.0013955594040453434\n",
            "step: 30, loss: 0.0005217864527367055\n",
            "step: 40, loss: 0.0002951831847894937\n",
            "step: 50, loss: 0.00048648074152879417\n",
            "step: 60, loss: 0.00031185135594569147\n",
            "step: 70, loss: 0.002967894310131669\n",
            "step: 80, loss: 0.002492028521373868\n",
            "step: 90, loss: 0.00036553386598825455\n",
            "step: 100, loss: 0.004606527741998434\n",
            "step: 110, loss: 0.0006851645885035396\n",
            "step: 120, loss: 0.0008989517809823155\n",
            "step: 130, loss: 0.004955099895596504\n",
            "step: 140, loss: 0.0019232322229072452\n",
            "step: 150, loss: 0.0003414804523345083\n",
            "step: 160, loss: 0.012212550267577171\n",
            "step: 170, loss: 0.000250812154263258\n",
            "step: 180, loss: 0.015562781132757664\n",
            "step: 190, loss: 0.006809313781559467\n",
            "step: 200, loss: 0.0003291608882136643\n",
            "step: 210, loss: 0.0014165174216032028\n",
            "step: 220, loss: 0.0006656166515313089\n",
            "step: 230, loss: 0.09115897864103317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9876819708846584, f1=0.9809203142536477, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007227765396237373\n",
            "step: 10, loss: 0.0007093027816154063\n",
            "step: 20, loss: 0.00019175377383362502\n",
            "step: 30, loss: 0.00035861521610058844\n",
            "step: 40, loss: 0.00015407246246468276\n",
            "step: 50, loss: 0.00029042194364592433\n",
            "step: 60, loss: 0.0002989218628499657\n",
            "step: 70, loss: 0.001264981459826231\n",
            "step: 80, loss: 0.00013484360533766448\n",
            "step: 90, loss: 5.7578912674216554e-05\n",
            "step: 100, loss: 0.00012075181439286098\n",
            "step: 110, loss: 0.00010959240898955613\n",
            "step: 120, loss: 7.558652578154579e-05\n",
            "step: 130, loss: 0.00010028122778749093\n",
            "step: 140, loss: 0.0001182976266136393\n",
            "step: 150, loss: 0.004582305904477835\n",
            "step: 160, loss: 0.07772865891456604\n",
            "step: 170, loss: 0.014116867445409298\n",
            "step: 180, loss: 0.003979107830673456\n",
            "step: 190, loss: 0.0003603527438826859\n",
            "step: 200, loss: 0.07276445627212524\n",
            "step: 210, loss: 9.831254283199087e-05\n",
            "step: 220, loss: 0.0006962300976738334\n",
            "step: 230, loss: 0.0040547773241996765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9843400447427293, f1=0.9798206278026906, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004801071190740913\n",
            "step: 10, loss: 0.0006896106642670929\n",
            "step: 20, loss: 0.007008313201367855\n",
            "step: 30, loss: 0.0028122293297201395\n",
            "step: 40, loss: 0.00168038927949965\n",
            "step: 50, loss: 0.005630503874272108\n",
            "step: 60, loss: 0.006518939975649118\n",
            "step: 70, loss: 0.00041659557609818876\n",
            "step: 80, loss: 0.001426554867066443\n",
            "step: 90, loss: 9.913367830449715e-05\n",
            "step: 100, loss: 0.0024035212118178606\n",
            "step: 110, loss: 0.007738585118204355\n",
            "step: 120, loss: 0.00018537489813752472\n",
            "step: 130, loss: 0.0004743838217109442\n",
            "step: 140, loss: 0.0002847960568033159\n",
            "step: 150, loss: 0.00046130624832585454\n",
            "step: 160, loss: 0.0009200139320455492\n",
            "step: 170, loss: 0.00010645444126566872\n",
            "step: 180, loss: 0.00028737157117575407\n",
            "step: 190, loss: 0.0002194373810198158\n",
            "step: 200, loss: 0.0001372954429825768\n",
            "step: 210, loss: 0.00010712930088629946\n",
            "step: 220, loss: 0.00019100200734101236\n",
            "step: 230, loss: 0.004157502204179764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9842696629213483, f1=0.9797297297297298, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.574008825235069e-05\n",
            "step: 10, loss: 0.0012375657679513097\n",
            "step: 20, loss: 0.002960280515253544\n",
            "step: 30, loss: 8.106107998173684e-05\n",
            "step: 40, loss: 0.2015402913093567\n",
            "step: 50, loss: 0.00012117742153350264\n",
            "step: 60, loss: 0.0001376271975459531\n",
            "step: 70, loss: 0.0004500467621255666\n",
            "step: 80, loss: 0.0001467951515223831\n",
            "step: 90, loss: 0.00021676765754818916\n",
            "step: 100, loss: 0.0006885628099553287\n",
            "step: 110, loss: 6.976993608986959e-05\n",
            "step: 120, loss: 5.2992145356256515e-05\n",
            "step: 130, loss: 0.00019848822557833046\n",
            "step: 140, loss: 0.0002482416166458279\n",
            "step: 150, loss: 0.00017151788051705807\n",
            "step: 160, loss: 7.896271563367918e-05\n",
            "step: 170, loss: 0.000569333671592176\n",
            "step: 180, loss: 0.0034801666624844074\n",
            "step: 190, loss: 0.00012177600001450628\n",
            "step: 200, loss: 8.306517702294514e-05\n",
            "step: 210, loss: 0.00014532654313370585\n",
            "step: 220, loss: 7.319061842281371e-05\n",
            "step: 230, loss: 0.0003772143099922687\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9864864864864865, f1=0.9830890642615557, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.941273361211643e-05\n",
            "step: 10, loss: 5.1097169489366934e-05\n",
            "step: 20, loss: 6.276130443438888e-05\n",
            "step: 30, loss: 0.0002918422396760434\n",
            "step: 40, loss: 0.00011186082701897249\n",
            "step: 50, loss: 0.0002955074014607817\n",
            "step: 60, loss: 0.02004363015294075\n",
            "step: 70, loss: 0.0001857877359725535\n",
            "step: 80, loss: 0.000137742783408612\n",
            "step: 90, loss: 0.0010312896920368075\n",
            "step: 100, loss: 0.00045800747466273606\n",
            "step: 110, loss: 0.0006533077685162425\n",
            "step: 120, loss: 9.450244397157803e-05\n",
            "step: 130, loss: 0.0014871980529278517\n",
            "step: 140, loss: 0.03895723819732666\n",
            "step: 150, loss: 0.011745353229343891\n",
            "step: 160, loss: 4.053274096804671e-05\n",
            "step: 170, loss: 7.526126137236133e-05\n",
            "step: 180, loss: 5.8591289416654035e-05\n",
            "step: 190, loss: 0.0051427362486720085\n",
            "step: 200, loss: 0.0001087260025087744\n",
            "step: 210, loss: 0.00011465224088169634\n",
            "step: 220, loss: 6.990075053181499e-05\n",
            "step: 230, loss: 0.0003258753858972341\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9864253393665158, f1=0.9818181818181818, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.470645166700706e-05\n",
            "step: 10, loss: 0.00023184741439763457\n",
            "step: 20, loss: 0.0022831172682344913\n",
            "step: 30, loss: 0.021799257025122643\n",
            "step: 40, loss: 0.00013607714208774269\n",
            "step: 50, loss: 8.351740689249709e-05\n",
            "step: 60, loss: 0.00011450974852778018\n",
            "step: 70, loss: 0.00035442746593616903\n",
            "step: 80, loss: 6.911229866091162e-05\n",
            "step: 90, loss: 3.972050399170257e-05\n",
            "step: 100, loss: 9.25113563425839e-05\n",
            "step: 110, loss: 0.04089193046092987\n",
            "step: 120, loss: 4.515515320235863e-05\n",
            "step: 130, loss: 4.072500087204389e-05\n",
            "step: 140, loss: 0.00012911784870084375\n",
            "step: 150, loss: 0.0408177375793457\n",
            "step: 160, loss: 0.00011258802987867966\n",
            "step: 170, loss: 0.03427638113498688\n",
            "step: 180, loss: 8.276602602563798e-05\n",
            "step: 190, loss: 8.068985334830359e-05\n",
            "step: 200, loss: 0.0009587613749317825\n",
            "step: 210, loss: 4.3113806896144524e-05\n",
            "step: 220, loss: 0.00010809883679030463\n",
            "step: 230, loss: 6.338296952890232e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9865168539325843, f1=0.9797297297297298, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001590008760103956\n",
            "step: 10, loss: 5.1155711844330654e-05\n",
            "step: 20, loss: 5.8312300097895786e-05\n",
            "step: 30, loss: 0.0001252057554665953\n",
            "step: 40, loss: 7.141969399526715e-05\n",
            "step: 50, loss: 0.0004905465757474303\n",
            "step: 60, loss: 0.0002488421159796417\n",
            "step: 70, loss: 0.00010138455400010571\n",
            "step: 80, loss: 0.00011954170622630045\n",
            "step: 90, loss: 3.684512557811104e-05\n",
            "step: 100, loss: 0.00038532985490746796\n",
            "step: 110, loss: 5.324908488546498e-05\n",
            "step: 120, loss: 0.00012355482613202184\n",
            "step: 130, loss: 6.421072612283751e-05\n",
            "step: 140, loss: 0.0059969741851091385\n",
            "step: 150, loss: 0.0002026259753620252\n",
            "step: 160, loss: 6.509012746391818e-05\n",
            "step: 170, loss: 5.904031058889814e-05\n",
            "step: 180, loss: 0.046905580908060074\n",
            "step: 190, loss: 0.0003023470926564187\n",
            "step: 200, loss: 4.963235551258549e-05\n",
            "step: 210, loss: 0.0002131218498107046\n",
            "step: 220, loss: 0.00020052219042554498\n",
            "step: 230, loss: 0.05082706734538078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9853768278965129, f1=0.9774266365688488, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019610421441029757\n",
            "step: 10, loss: 0.0001391398545820266\n",
            "step: 20, loss: 8.048207382671535e-05\n",
            "step: 30, loss: 0.00023955918732099235\n",
            "step: 40, loss: 6.796608795411885e-05\n",
            "step: 50, loss: 9.560440958011895e-05\n",
            "step: 60, loss: 0.00011887424625456333\n",
            "step: 70, loss: 3.302659388282336e-05\n",
            "step: 80, loss: 6.321974069578573e-05\n",
            "step: 90, loss: 4.187270678812638e-05\n",
            "step: 100, loss: 3.3725620596669614e-05\n",
            "step: 110, loss: 0.03273634985089302\n",
            "step: 120, loss: 0.021573437377810478\n",
            "step: 130, loss: 0.0001704225578578189\n",
            "step: 140, loss: 4.834785431739874e-05\n",
            "step: 150, loss: 8.15453749964945e-05\n",
            "step: 160, loss: 0.00011445068957982585\n",
            "step: 170, loss: 9.845731256064028e-05\n",
            "step: 180, loss: 5.5840861023170874e-05\n",
            "step: 190, loss: 4.7643981815781444e-05\n",
            "step: 200, loss: 0.0001348501245956868\n",
            "step: 210, loss: 6.577472959179431e-05\n",
            "step: 220, loss: 4.259176421328448e-05\n",
            "step: 230, loss: 4.697036638390273e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9865168539325843, f1=0.9797297297297298, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.903513425029814e-05\n",
            "step: 10, loss: 0.009226281195878983\n",
            "step: 20, loss: 8.433168841293082e-05\n",
            "step: 30, loss: 0.0001681896683294326\n",
            "step: 40, loss: 0.0018067860510200262\n",
            "step: 50, loss: 4.870898919762112e-05\n",
            "step: 60, loss: 0.00022253005590755492\n",
            "step: 70, loss: 5.4290092521114275e-05\n",
            "step: 80, loss: 5.032485933043063e-05\n",
            "step: 90, loss: 5.296550443745218e-05\n",
            "step: 100, loss: 8.731109846848994e-05\n",
            "step: 110, loss: 0.00014577059482689947\n",
            "step: 120, loss: 2.6399991838843562e-05\n",
            "step: 130, loss: 3.997820749646053e-05\n",
            "step: 140, loss: 0.00021935621043667197\n",
            "step: 150, loss: 5.154585960553959e-05\n",
            "step: 160, loss: 0.0005315426387824118\n",
            "step: 170, loss: 2.9793553039780818e-05\n",
            "step: 180, loss: 0.00017173841479234397\n",
            "step: 190, loss: 0.00010822412150446326\n",
            "step: 200, loss: 3.91089815821033e-05\n",
            "step: 210, loss: 5.824050822411664e-05\n",
            "step: 220, loss: 0.0007905701640993357\n",
            "step: 230, loss: 0.011172872968018055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9875706214689265, f1=0.9807037457434733, best_f1=0.9841269841269841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.856070442358032e-05\n",
            "step: 10, loss: 2.6642073862603866e-05\n",
            "step: 20, loss: 4.9006674089469016e-05\n",
            "step: 30, loss: 3.500535603961907e-05\n",
            "step: 40, loss: 9.050042717717588e-05\n",
            "step: 50, loss: 0.05239781737327576\n",
            "step: 60, loss: 6.498255243059248e-05\n",
            "step: 70, loss: 4.769804945681244e-05\n",
            "step: 80, loss: 8.847429126035422e-05\n",
            "step: 90, loss: 4.853800783166662e-05\n",
            "step: 100, loss: 3.925145210814662e-05\n",
            "step: 110, loss: 0.00014071198529563844\n",
            "step: 120, loss: 0.00014545275189448148\n",
            "step: 130, loss: 0.013762429356575012\n",
            "step: 140, loss: 3.168613329762593e-05\n",
            "step: 150, loss: 7.199036190286279e-05\n",
            "step: 160, loss: 2.902259075199254e-05\n",
            "step: 170, loss: 3.523204577504657e-05\n",
            "step: 180, loss: 0.00023342552594840527\n",
            "step: 190, loss: 0.0005426799762062728\n",
            "step: 200, loss: 4.94161868118681e-05\n",
            "step: 210, loss: 4.851128323934972e-05\n",
            "step: 220, loss: 0.00012573215644806623\n",
            "step: 230, loss: 4.846827141591348e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9875706214689265, f1=0.9807037457434733, best_f1=0.9841269841269841\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 165.91it/s]\n",
            "load_f1 = 0.9921259842519685\n",
            "real_f1 = 0.9876819708846584\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 191.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9c6d20-7b1d-4acd-e39b-7eb40f59ca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5941812992095947\n",
            "step: 10, loss: 0.5643873810768127\n",
            "step: 20, loss: 0.5053332448005676\n",
            "step: 30, loss: 0.1382715255022049\n",
            "step: 40, loss: 0.06386411190032959\n",
            "step: 50, loss: 0.2568763196468353\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.0614011324942112\n",
            "step: 70, loss: 0.11283409595489502\n",
            "step: 80, loss: 0.13586702942848206\n",
            "step: 90, loss: 0.1993851512670517\n",
            "step: 100, loss: 0.02375837042927742\n",
            "step: 110, loss: 0.043974436819553375\n",
            "step: 120, loss: 0.1050015389919281\n",
            "step: 130, loss: 0.06462683528661728\n",
            "step: 140, loss: 0.06457775086164474\n",
            "step: 150, loss: 0.03822995349764824\n",
            "step: 160, loss: 0.06171903759241104\n",
            "step: 170, loss: 0.1826198697090149\n",
            "step: 180, loss: 0.059611253440380096\n",
            "step: 190, loss: 0.010811558924615383\n",
            "step: 200, loss: 0.1327529102563858\n",
            "step: 210, loss: 0.16594883799552917\n",
            "step: 220, loss: 0.23945407569408417\n",
            "step: 230, loss: 0.16991564631462097\n",
            "step: 240, loss: 0.02302728407084942\n",
            "step: 250, loss: 0.027178358286619186\n",
            "step: 260, loss: 0.0667194202542305\n",
            "step: 270, loss: 0.02007257379591465\n",
            "step: 280, loss: 0.14153800904750824\n",
            "step: 290, loss: 0.19589656591415405\n",
            "step: 300, loss: 0.07285904884338379\n",
            "step: 310, loss: 0.2925444543361664\n",
            "step: 320, loss: 0.09666147828102112\n",
            "step: 330, loss: 0.020689064636826515\n",
            "step: 340, loss: 0.04177229851484299\n",
            "step: 350, loss: 0.07293211668729782\n",
            "step: 360, loss: 0.03356677293777466\n",
            "step: 370, loss: 0.07338785380125046\n",
            "step: 380, loss: 0.05319073796272278\n",
            "step: 390, loss: 0.1228046715259552\n",
            "step: 400, loss: 0.2532435655593872\n",
            "step: 410, loss: 0.06499288976192474\n",
            "step: 420, loss: 0.04461269453167915\n",
            "step: 430, loss: 0.16231374442577362\n",
            "step: 440, loss: 0.01692924089729786\n",
            "step: 450, loss: 0.0049302284605801105\n",
            "step: 460, loss: 0.009584357030689716\n",
            "step: 470, loss: 0.1515865921974182\n",
            "step: 480, loss: 0.05973268672823906\n",
            "step: 490, loss: 0.0907684937119484\n",
            "step: 500, loss: 0.19415059685707092\n",
            "step: 510, loss: 0.060291845351457596\n",
            "step: 520, loss: 0.13726806640625\n",
            "step: 530, loss: 0.0023774015717208385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9315448658649398, f1=0.9326568265682658, best_f1=0.9326568265682658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2780977189540863\n",
            "step: 10, loss: 0.08657890558242798\n",
            "step: 20, loss: 0.015238344669342041\n",
            "step: 30, loss: 0.006529510486871004\n",
            "step: 40, loss: 0.05136450007557869\n",
            "step: 50, loss: 0.10035192966461182\n",
            "step: 60, loss: 0.012172658927738667\n",
            "step: 70, loss: 0.06216040253639221\n",
            "step: 80, loss: 0.06926295906305313\n",
            "step: 90, loss: 0.014123805798590183\n",
            "step: 100, loss: 0.007270304951816797\n",
            "step: 110, loss: 0.047078683972358704\n",
            "step: 120, loss: 0.10443152487277985\n",
            "step: 130, loss: 0.19014109671115875\n",
            "step: 140, loss: 0.014650438912212849\n",
            "step: 150, loss: 0.03840923681855202\n",
            "step: 160, loss: 0.007224952336400747\n",
            "step: 170, loss: 0.04511386528611183\n",
            "step: 180, loss: 0.044283196330070496\n",
            "step: 190, loss: 0.07948414236307144\n",
            "step: 200, loss: 0.006078329402953386\n",
            "step: 210, loss: 0.11212272197008133\n",
            "step: 220, loss: 0.08632946014404297\n",
            "step: 230, loss: 0.011044835671782494\n",
            "step: 240, loss: 0.04679754003882408\n",
            "step: 250, loss: 0.10941175371408463\n",
            "step: 260, loss: 0.008289448916912079\n",
            "step: 270, loss: 0.06584442406892776\n",
            "step: 280, loss: 0.03593801334500313\n",
            "step: 290, loss: 0.015030864626169205\n",
            "step: 300, loss: 0.14843736588954926\n",
            "step: 310, loss: 0.010996242053806782\n",
            "step: 320, loss: 0.05791084095835686\n",
            "step: 330, loss: 0.07475490868091583\n",
            "step: 340, loss: 0.016913369297981262\n",
            "step: 350, loss: 0.0029076675418764353\n",
            "step: 360, loss: 0.05816299840807915\n",
            "step: 370, loss: 0.12896151840686798\n",
            "step: 380, loss: 0.08881200850009918\n",
            "step: 390, loss: 0.17374949157238007\n",
            "step: 400, loss: 0.0611075684428215\n",
            "step: 410, loss: 0.011395051144063473\n",
            "step: 420, loss: 0.009089576080441475\n",
            "step: 430, loss: 0.017355166375637054\n",
            "step: 440, loss: 0.24719834327697754\n",
            "step: 450, loss: 0.025613877922296524\n",
            "step: 460, loss: 0.011727412231266499\n",
            "step: 470, loss: 0.11246947944164276\n",
            "step: 480, loss: 0.2906055152416229\n",
            "step: 490, loss: 0.013998513109982014\n",
            "step: 500, loss: 0.298051655292511\n",
            "step: 510, loss: 0.015595877543091774\n",
            "step: 520, loss: 0.03110715188086033\n",
            "step: 530, loss: 0.007796839345246553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9352245862884161, f1=0.9298578199052132, best_f1=0.9298578199052132\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07157765328884125\n",
            "step: 10, loss: 0.06882112473249435\n",
            "step: 20, loss: 0.06251675635576248\n",
            "step: 30, loss: 0.14206433296203613\n",
            "step: 40, loss: 0.004314890597015619\n",
            "step: 50, loss: 0.07629803568124771\n",
            "step: 60, loss: 0.05760900676250458\n",
            "step: 70, loss: 0.012950544245541096\n",
            "step: 80, loss: 0.0030708713456988335\n",
            "step: 90, loss: 0.005837948061525822\n",
            "step: 100, loss: 0.026986777782440186\n",
            "step: 110, loss: 0.0008679360034875572\n",
            "step: 120, loss: 0.0023735901340842247\n",
            "step: 130, loss: 0.004432807210832834\n",
            "step: 140, loss: 0.02385549433529377\n",
            "step: 150, loss: 0.018410928547382355\n",
            "step: 160, loss: 0.005132345482707024\n",
            "step: 170, loss: 0.04999226704239845\n",
            "step: 180, loss: 0.006798971444368362\n",
            "step: 190, loss: 0.006134665105491877\n",
            "step: 200, loss: 0.005482227075845003\n",
            "step: 210, loss: 0.0429612472653389\n",
            "step: 220, loss: 0.08177008479833603\n",
            "step: 230, loss: 0.025430910289287567\n",
            "step: 240, loss: 0.00464272778481245\n",
            "step: 250, loss: 0.013050743378698826\n",
            "step: 260, loss: 0.051194123923778534\n",
            "step: 270, loss: 0.01048758253455162\n",
            "step: 280, loss: 0.2109597772359848\n",
            "step: 290, loss: 0.0034236120991408825\n",
            "step: 300, loss: 0.11203441768884659\n",
            "step: 310, loss: 0.006900348234921694\n",
            "step: 320, loss: 0.026294007897377014\n",
            "step: 330, loss: 0.003948966506868601\n",
            "step: 340, loss: 0.016912322491407394\n",
            "step: 350, loss: 0.0013569567818194628\n",
            "step: 360, loss: 0.018942272290587425\n",
            "step: 370, loss: 0.008030423894524574\n",
            "step: 380, loss: 0.013602189719676971\n",
            "step: 390, loss: 0.012945551425218582\n",
            "step: 400, loss: 0.043862827122211456\n",
            "step: 410, loss: 0.002217957517132163\n",
            "step: 420, loss: 0.19910794496536255\n",
            "step: 430, loss: 0.011816040612757206\n",
            "step: 440, loss: 0.03656338155269623\n",
            "step: 450, loss: 0.08612114191055298\n",
            "step: 460, loss: 0.03437225520610809\n",
            "step: 470, loss: 0.07602237164974213\n",
            "step: 480, loss: 0.002456828486174345\n",
            "step: 490, loss: 0.0038674494717270136\n",
            "step: 500, loss: 0.017827309668064117\n",
            "step: 510, loss: 0.0050650304183363914\n",
            "step: 520, loss: 0.13993249833583832\n",
            "step: 530, loss: 0.021520711481571198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9351251158480075, f1=0.9252206223873664, best_f1=0.9298578199052132\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0041231997311115265\n",
            "step: 10, loss: 0.0038674913812428713\n",
            "step: 20, loss: 0.005117810796946287\n",
            "step: 30, loss: 0.0015925210900604725\n",
            "step: 40, loss: 0.0006788374157622457\n",
            "step: 50, loss: 0.0027012068312615156\n",
            "step: 60, loss: 0.00232420745305717\n",
            "step: 70, loss: 0.00755506195127964\n",
            "step: 80, loss: 0.009756827726960182\n",
            "step: 90, loss: 0.008976851589977741\n",
            "step: 100, loss: 0.03467829152941704\n",
            "step: 110, loss: 0.061270296573638916\n",
            "step: 120, loss: 0.0003092783153988421\n",
            "step: 130, loss: 0.007448209915310144\n",
            "step: 140, loss: 0.003762062406167388\n",
            "step: 150, loss: 0.002179835457354784\n",
            "step: 160, loss: 0.00325801782310009\n",
            "step: 170, loss: 0.0009943465702235699\n",
            "step: 180, loss: 0.008626949973404408\n",
            "step: 190, loss: 0.017518023028969765\n",
            "step: 200, loss: 0.000902339001186192\n",
            "step: 210, loss: 0.031445298343896866\n",
            "step: 220, loss: 0.007941445335745811\n",
            "step: 230, loss: 0.037310682237148285\n",
            "step: 240, loss: 0.0035359107423573732\n",
            "step: 250, loss: 0.008290288969874382\n",
            "step: 260, loss: 0.0012937899446114898\n",
            "step: 270, loss: 0.008264867588877678\n",
            "step: 280, loss: 0.014006241224706173\n",
            "step: 290, loss: 0.048446204513311386\n",
            "step: 300, loss: 0.0007556918426416814\n",
            "step: 310, loss: 0.03901955857872963\n",
            "step: 320, loss: 0.029797594994306564\n",
            "step: 330, loss: 0.009532667696475983\n",
            "step: 340, loss: 0.0023361516650766134\n",
            "step: 350, loss: 0.073492631316185\n",
            "step: 360, loss: 0.12254122644662857\n",
            "step: 370, loss: 0.01855367049574852\n",
            "step: 380, loss: 0.02060972899198532\n",
            "step: 390, loss: 0.0006884007016196847\n",
            "step: 400, loss: 0.0027361148968338966\n",
            "step: 410, loss: 0.0027228707913309336\n",
            "step: 420, loss: 0.004619908984750509\n",
            "step: 430, loss: 0.1353176087141037\n",
            "step: 440, loss: 0.005338752642273903\n",
            "step: 450, loss: 0.0013638571836054325\n",
            "step: 460, loss: 0.003381518181413412\n",
            "step: 470, loss: 0.015982842072844505\n",
            "step: 480, loss: 0.061667900532484055\n",
            "step: 490, loss: 0.006762935314327478\n",
            "step: 500, loss: 0.0039923666045069695\n",
            "step: 510, loss: 0.005626735743135214\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 520, loss: 0.15956678986549377\n",
            "step: 530, loss: 0.0168771892786026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9393237610004631, f1=0.9346919870310328, best_f1=0.9346919870310328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009611968882381916\n",
            "step: 10, loss: 0.04045988991856575\n",
            "step: 20, loss: 0.0726461261510849\n",
            "step: 30, loss: 0.0010131775634363294\n",
            "step: 40, loss: 0.0030772974714636803\n",
            "step: 50, loss: 0.0024911765940487385\n",
            "step: 60, loss: 0.1097845733165741\n",
            "step: 70, loss: 0.003279578872025013\n",
            "step: 80, loss: 0.005363688338547945\n",
            "step: 90, loss: 0.008700139820575714\n",
            "step: 100, loss: 0.026296114549040794\n",
            "step: 110, loss: 0.00027598056476563215\n",
            "step: 120, loss: 0.02463817410171032\n",
            "step: 130, loss: 0.00042040928383357823\n",
            "step: 140, loss: 0.0007551597664132714\n",
            "step: 150, loss: 0.0003391029022168368\n",
            "step: 160, loss: 0.00028497533639892936\n",
            "step: 170, loss: 0.027159474790096283\n",
            "step: 180, loss: 0.00039516930701211095\n",
            "step: 190, loss: 0.00039028393803164363\n",
            "step: 200, loss: 0.0004607538867276162\n",
            "step: 210, loss: 0.013660683296620846\n",
            "step: 220, loss: 0.001550542307086289\n",
            "step: 230, loss: 0.0021565838251262903\n",
            "step: 240, loss: 0.004057195968925953\n",
            "step: 250, loss: 0.0017805217066779733\n",
            "step: 260, loss: 0.03309642896056175\n",
            "step: 270, loss: 0.0014873897889629006\n",
            "step: 280, loss: 0.01941286399960518\n",
            "step: 290, loss: 0.001888848259113729\n",
            "step: 300, loss: 0.015164827927947044\n",
            "step: 310, loss: 0.0030661076307296753\n",
            "step: 320, loss: 0.12612655758857727\n",
            "step: 330, loss: 0.05286338925361633\n",
            "step: 340, loss: 0.0011349754640832543\n",
            "step: 350, loss: 0.0011508592870086432\n",
            "step: 360, loss: 0.025486718863248825\n",
            "step: 370, loss: 0.006516643334180117\n",
            "step: 380, loss: 0.0014617926208302379\n",
            "step: 390, loss: 0.00020764171495102346\n",
            "step: 400, loss: 0.0038392499554902315\n",
            "step: 410, loss: 0.004825236275792122\n",
            "step: 420, loss: 0.0036449439357966185\n",
            "step: 430, loss: 0.006148270331323147\n",
            "step: 440, loss: 0.004137112759053707\n",
            "step: 450, loss: 0.0018380648689344525\n",
            "step: 460, loss: 0.0003066406352445483\n",
            "step: 470, loss: 0.0066450415179133415\n",
            "step: 480, loss: 0.010101149789988995\n",
            "step: 490, loss: 0.00016616724315099418\n",
            "step: 500, loss: 0.0021800866816192865\n",
            "step: 510, loss: 0.0036731434520334005\n",
            "step: 520, loss: 0.07624439150094986\n",
            "step: 530, loss: 0.00458444794639945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9423697556477639, f1=0.9402573529411765, best_f1=0.9402573529411765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014047208242118359\n",
            "step: 10, loss: 0.003837019670754671\n",
            "step: 20, loss: 0.05430829897522926\n",
            "step: 30, loss: 0.00557180680334568\n",
            "step: 40, loss: 0.0010023800423368812\n",
            "step: 50, loss: 0.1482698768377304\n",
            "step: 60, loss: 0.0001587003789609298\n",
            "step: 70, loss: 0.006814118940383196\n",
            "step: 80, loss: 0.0013915977906435728\n",
            "step: 90, loss: 0.00016132215387187898\n",
            "step: 100, loss: 0.028105566278100014\n",
            "step: 110, loss: 0.011489455588161945\n",
            "step: 120, loss: 0.000732011569198221\n",
            "step: 130, loss: 0.001256960560567677\n",
            "step: 140, loss: 0.005761244334280491\n",
            "step: 150, loss: 0.0023682666942477226\n",
            "step: 160, loss: 0.002510342514142394\n",
            "step: 170, loss: 0.000766777026001364\n",
            "step: 180, loss: 0.0004058222402818501\n",
            "step: 190, loss: 0.00013468334509525448\n",
            "step: 200, loss: 0.003750129835680127\n",
            "step: 210, loss: 0.0017395186005160213\n",
            "step: 220, loss: 0.0034353581722825766\n",
            "step: 230, loss: 0.0002871159522328526\n",
            "step: 240, loss: 0.018229762092232704\n",
            "step: 250, loss: 0.0007393336854875088\n",
            "step: 260, loss: 0.00013179791858419776\n",
            "step: 270, loss: 0.09133229404687881\n",
            "step: 280, loss: 0.0019574405159801245\n",
            "step: 290, loss: 0.00036437882226891816\n",
            "step: 300, loss: 0.0001613629428902641\n",
            "step: 310, loss: 0.0009770046453922987\n",
            "step: 320, loss: 0.0007078393246047199\n",
            "step: 330, loss: 0.000421685486799106\n",
            "step: 340, loss: 0.14815236628055573\n",
            "step: 350, loss: 0.0015005464665591717\n",
            "step: 360, loss: 0.0037887776270508766\n",
            "step: 370, loss: 0.0009495826088823378\n",
            "step: 380, loss: 0.00013805902563035488\n",
            "step: 390, loss: 0.016013724729418755\n",
            "step: 400, loss: 0.00011727940000128001\n",
            "step: 410, loss: 0.0001994982158066705\n",
            "step: 420, loss: 0.0016280082054436207\n",
            "step: 430, loss: 0.000627703033387661\n",
            "step: 440, loss: 0.00020124050206504762\n",
            "step: 450, loss: 0.0003740485990419984\n",
            "step: 460, loss: 0.00014622653543483466\n",
            "step: 470, loss: 0.002947507193312049\n",
            "step: 480, loss: 0.004941871855407953\n",
            "step: 490, loss: 0.007671453524380922\n",
            "step: 500, loss: 0.0011141039431095123\n",
            "step: 510, loss: 0.002371882786974311\n",
            "step: 520, loss: 0.0006616150494664907\n",
            "step: 530, loss: 0.006080096587538719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9434137291280148, f1=0.9382830626450116, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008506195736117661\n",
            "step: 10, loss: 0.0003008809871971607\n",
            "step: 20, loss: 0.0005879531381651759\n",
            "step: 30, loss: 0.0008225663914345205\n",
            "step: 40, loss: 8.762008656049147e-05\n",
            "step: 50, loss: 0.00554334931075573\n",
            "step: 60, loss: 0.0004035922174807638\n",
            "step: 70, loss: 0.02476579137146473\n",
            "step: 80, loss: 0.0007362254546023905\n",
            "step: 90, loss: 0.00021581596229225397\n",
            "step: 100, loss: 0.00010617219231789932\n",
            "step: 110, loss: 0.0008416488999500871\n",
            "step: 120, loss: 0.007578074932098389\n",
            "step: 130, loss: 0.0016807059291750193\n",
            "step: 140, loss: 0.001094402396120131\n",
            "step: 150, loss: 0.0003009513020515442\n",
            "step: 160, loss: 0.0005435283528640866\n",
            "step: 170, loss: 0.00032041672966443\n",
            "step: 180, loss: 0.0001019629489746876\n",
            "step: 190, loss: 0.01004644576460123\n",
            "step: 200, loss: 0.00051596894627437\n",
            "step: 210, loss: 0.002079906640574336\n",
            "step: 220, loss: 0.0001637922105146572\n",
            "step: 230, loss: 0.17489507794380188\n",
            "step: 240, loss: 0.0005520256818272173\n",
            "step: 250, loss: 0.00026371682179160416\n",
            "step: 260, loss: 4.5100474380888045e-05\n",
            "step: 270, loss: 0.00018128432566300035\n",
            "step: 280, loss: 0.0004016971797682345\n",
            "step: 290, loss: 0.0004019980551674962\n",
            "step: 300, loss: 0.003789068665355444\n",
            "step: 310, loss: 0.00013748722267337143\n",
            "step: 320, loss: 0.00013703620061278343\n",
            "step: 330, loss: 0.00032082010875456035\n",
            "step: 340, loss: 0.01677740551531315\n",
            "step: 350, loss: 0.008199213072657585\n",
            "step: 360, loss: 0.011951806955039501\n",
            "step: 370, loss: 0.00011540594277903438\n",
            "step: 380, loss: 0.0009770442266017199\n",
            "step: 390, loss: 0.00016026897355914116\n",
            "step: 400, loss: 0.00115681323222816\n",
            "step: 410, loss: 0.00019969405548181385\n",
            "step: 420, loss: 0.0002456358342897147\n",
            "step: 430, loss: 9.120434697251767e-05\n",
            "step: 440, loss: 0.0007782129687257111\n",
            "step: 450, loss: 0.00036265328526496887\n",
            "step: 460, loss: 0.0005901945987716317\n",
            "step: 470, loss: 0.002805457217618823\n",
            "step: 480, loss: 0.16708363592624664\n",
            "step: 490, loss: 0.00013869033136870712\n",
            "step: 500, loss: 0.0011876319767907262\n",
            "step: 510, loss: 0.0007865465013310313\n",
            "step: 520, loss: 0.0007563892286270857\n",
            "step: 530, loss: 0.02952703647315502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9388707419505367, f1=0.9328984156570364, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019449747924227268\n",
            "step: 10, loss: 0.028702594339847565\n",
            "step: 20, loss: 0.00022207100118976086\n",
            "step: 30, loss: 0.0001581020187586546\n",
            "step: 40, loss: 0.0002351687871851027\n",
            "step: 50, loss: 0.0009815540397539735\n",
            "step: 60, loss: 0.0006588065298274159\n",
            "step: 70, loss: 0.00010843015479622409\n",
            "step: 80, loss: 0.004151594825088978\n",
            "step: 90, loss: 0.00015647959662601352\n",
            "step: 100, loss: 9.023993334267288e-05\n",
            "step: 110, loss: 0.0010543884709477425\n",
            "step: 120, loss: 0.0026573038194328547\n",
            "step: 130, loss: 0.00012743133993353695\n",
            "step: 140, loss: 0.0008106849272735417\n",
            "step: 150, loss: 0.0002230617101304233\n",
            "step: 160, loss: 0.016147928312420845\n",
            "step: 170, loss: 0.016077116131782532\n",
            "step: 180, loss: 0.0012262981617823243\n",
            "step: 190, loss: 0.007712661288678646\n",
            "step: 200, loss: 0.000547017902135849\n",
            "step: 210, loss: 0.00572526641190052\n",
            "step: 220, loss: 0.0003017602430190891\n",
            "step: 230, loss: 5.2952189435018227e-05\n",
            "step: 240, loss: 0.00017343691433779895\n",
            "step: 250, loss: 6.333467172225937e-05\n",
            "step: 260, loss: 6.765213038306683e-05\n",
            "step: 270, loss: 0.0011983970180153847\n",
            "step: 280, loss: 0.029498260468244553\n",
            "step: 290, loss: 9.64468126767315e-05\n",
            "step: 300, loss: 0.025754446163773537\n",
            "step: 310, loss: 0.00047740156878717244\n",
            "step: 320, loss: 0.003470911178737879\n",
            "step: 330, loss: 0.00029126217123121023\n",
            "step: 340, loss: 0.0003149854310322553\n",
            "step: 350, loss: 0.01268818974494934\n",
            "step: 360, loss: 0.00019978363707195967\n",
            "step: 370, loss: 0.0005548756453208625\n",
            "step: 380, loss: 0.002191924722865224\n",
            "step: 390, loss: 0.14698758721351624\n",
            "step: 400, loss: 7.194244972197339e-05\n",
            "step: 410, loss: 0.0001094574254238978\n",
            "step: 420, loss: 0.0016865466022863984\n",
            "step: 430, loss: 0.05716206878423691\n",
            "step: 440, loss: 0.0031389615032821894\n",
            "step: 450, loss: 0.00014494560309685767\n",
            "step: 460, loss: 0.0007080926443450153\n",
            "step: 470, loss: 7.646766607649624e-05\n",
            "step: 480, loss: 4.620238905772567e-05\n",
            "step: 490, loss: 0.0021018204279243946\n",
            "step: 500, loss: 0.0017818756168708205\n",
            "step: 510, loss: 0.00010103568638442084\n",
            "step: 520, loss: 0.0004557289066724479\n",
            "step: 530, loss: 0.00017032174218911678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.941340782122905, f1=0.9360902255639098, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000455373025033623\n",
            "step: 10, loss: 8.95665361895226e-05\n",
            "step: 20, loss: 0.012045766226947308\n",
            "step: 30, loss: 0.001182005275040865\n",
            "step: 40, loss: 0.00015408832405228168\n",
            "step: 50, loss: 0.00011625871411524713\n",
            "step: 60, loss: 4.7956906200852245e-05\n",
            "step: 70, loss: 0.07553111016750336\n",
            "step: 80, loss: 0.0029340507462620735\n",
            "step: 90, loss: 0.00012468238128349185\n",
            "step: 100, loss: 5.7779328926699236e-05\n",
            "step: 110, loss: 6.0101716371718794e-05\n",
            "step: 120, loss: 5.5395765230059624e-05\n",
            "step: 130, loss: 0.013239657506346703\n",
            "step: 140, loss: 0.010860650800168514\n",
            "step: 150, loss: 4.0899922169046476e-05\n",
            "step: 160, loss: 0.003669860539957881\n",
            "step: 170, loss: 0.010667390190064907\n",
            "step: 180, loss: 5.084967051516287e-05\n",
            "step: 190, loss: 6.301762186922133e-05\n",
            "step: 200, loss: 0.001803673687390983\n",
            "step: 210, loss: 4.090848597115837e-05\n",
            "step: 220, loss: 8.812213491182774e-05\n",
            "step: 230, loss: 7.133622420951724e-05\n",
            "step: 240, loss: 0.00010319965076632798\n",
            "step: 250, loss: 7.385495700873435e-05\n",
            "step: 260, loss: 0.0019582565873861313\n",
            "step: 270, loss: 3.355508670210838e-05\n",
            "step: 280, loss: 0.00032043427927419543\n",
            "step: 290, loss: 0.03107600100338459\n",
            "step: 300, loss: 7.081685907905921e-05\n",
            "step: 310, loss: 0.02356523461639881\n",
            "step: 320, loss: 7.716578693361953e-05\n",
            "step: 330, loss: 0.00011655578418867663\n",
            "step: 340, loss: 3.5875003959517926e-05\n",
            "step: 350, loss: 0.007955399341881275\n",
            "step: 360, loss: 2.9685968911508098e-05\n",
            "step: 370, loss: 0.0032036560587584972\n",
            "step: 380, loss: 5.200956002227031e-05\n",
            "step: 390, loss: 3.530990579747595e-05\n",
            "step: 400, loss: 0.00016505367239005864\n",
            "step: 410, loss: 7.908752741059288e-05\n",
            "step: 420, loss: 0.006729843094944954\n",
            "step: 430, loss: 8.44625974423252e-05\n",
            "step: 440, loss: 0.0009811274940147996\n",
            "step: 450, loss: 0.00010374156408943236\n",
            "step: 460, loss: 0.00041291210800409317\n",
            "step: 470, loss: 2.40127610595664e-05\n",
            "step: 480, loss: 4.429062028066255e-05\n",
            "step: 490, loss: 0.0010694322409108281\n",
            "step: 500, loss: 0.0005275965668261051\n",
            "step: 510, loss: 0.0001608380989637226\n",
            "step: 520, loss: 0.0004907906986773014\n",
            "step: 530, loss: 0.0017802906222641468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9384902143522833, f1=0.9311475409836065, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013351424131542444\n",
            "step: 10, loss: 2.6791092750499956e-05\n",
            "step: 20, loss: 0.0007586884312331676\n",
            "step: 30, loss: 9.601030615158379e-05\n",
            "step: 40, loss: 0.0009322646073997021\n",
            "step: 50, loss: 0.0002265843068016693\n",
            "step: 60, loss: 0.0009182404610328376\n",
            "step: 70, loss: 0.009744195267558098\n",
            "step: 80, loss: 0.0011757529573515058\n",
            "step: 90, loss: 0.00035360263427719474\n",
            "step: 100, loss: 4.814880958292633e-05\n",
            "step: 110, loss: 2.0764391592820175e-05\n",
            "step: 120, loss: 0.0002061252889689058\n",
            "step: 130, loss: 0.0016081412322819233\n",
            "step: 140, loss: 0.007163052447140217\n",
            "step: 150, loss: 9.66408260865137e-05\n",
            "step: 160, loss: 2.813590799632948e-05\n",
            "step: 170, loss: 6.731161556672305e-05\n",
            "step: 180, loss: 9.320356912212446e-05\n",
            "step: 190, loss: 0.0391877256333828\n",
            "step: 200, loss: 0.007419873960316181\n",
            "step: 210, loss: 0.0003570143599063158\n",
            "step: 220, loss: 0.00046635387116111815\n",
            "step: 230, loss: 0.00104476825799793\n",
            "step: 240, loss: 0.00035368872340768576\n",
            "step: 250, loss: 2.5916167942341417e-05\n",
            "step: 260, loss: 0.00018000764248427004\n",
            "step: 270, loss: 6.691200542263687e-05\n",
            "step: 280, loss: 0.0003094544226769358\n",
            "step: 290, loss: 3.795038355747238e-05\n",
            "step: 300, loss: 3.0419603717746213e-05\n",
            "step: 310, loss: 0.0022534485906362534\n",
            "step: 320, loss: 0.0007959509966894984\n",
            "step: 330, loss: 0.00043725661817006767\n",
            "step: 340, loss: 0.00047804333735257387\n",
            "step: 350, loss: 0.04786144196987152\n",
            "step: 360, loss: 4.831310798181221e-05\n",
            "step: 370, loss: 0.0016887669917196035\n",
            "step: 380, loss: 0.00021856777311768383\n",
            "step: 390, loss: 0.0005432229372672737\n",
            "step: 400, loss: 0.0006937248399481177\n",
            "step: 410, loss: 0.00042424455750733614\n",
            "step: 420, loss: 0.0001001515265670605\n",
            "step: 430, loss: 0.00029115588404238224\n",
            "step: 440, loss: 0.006133111659437418\n",
            "step: 450, loss: 3.402869697310962e-05\n",
            "step: 460, loss: 3.6413181078387424e-05\n",
            "step: 470, loss: 5.233680712990463e-05\n",
            "step: 480, loss: 0.000781570328399539\n",
            "step: 490, loss: 0.0002310015115654096\n",
            "step: 500, loss: 0.006461269687861204\n",
            "step: 510, loss: 0.0009308846201747656\n",
            "step: 520, loss: 4.441849887371063e-05\n",
            "step: 530, loss: 4.4498301576822996e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9406264609630668, f1=0.9339578454332552, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000314731034450233\n",
            "step: 10, loss: 0.00034799828426912427\n",
            "step: 20, loss: 4.006343078799546e-05\n",
            "step: 30, loss: 2.7983573090750724e-05\n",
            "step: 40, loss: 0.0005905182915739715\n",
            "step: 50, loss: 8.725409134058282e-05\n",
            "step: 60, loss: 0.031125372275710106\n",
            "step: 70, loss: 2.8795728212571703e-05\n",
            "step: 80, loss: 7.234921213239431e-05\n",
            "step: 90, loss: 3.050903251278214e-05\n",
            "step: 100, loss: 0.00038512342143803835\n",
            "step: 110, loss: 2.9857434128643945e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 3.146062954328954e-05\n",
            "step: 130, loss: 1.984409573196899e-05\n",
            "step: 140, loss: 0.17119383811950684\n",
            "step: 150, loss: 1.656610766076483e-05\n",
            "step: 160, loss: 0.09015153348445892\n",
            "step: 170, loss: 0.00020546669838950038\n",
            "step: 180, loss: 0.0006367875612340868\n",
            "step: 190, loss: 0.006387187168002129\n",
            "step: 200, loss: 0.007533607073128223\n",
            "step: 210, loss: 0.00045875422074459493\n",
            "step: 220, loss: 0.003474452067166567\n",
            "step: 230, loss: 0.00010994142212439328\n",
            "step: 240, loss: 5.811066512251273e-05\n",
            "step: 250, loss: 2.0816240066778846e-05\n",
            "step: 260, loss: 8.770735439611599e-05\n",
            "step: 270, loss: 8.974088996183127e-05\n",
            "step: 280, loss: 0.0001896398316603154\n",
            "step: 290, loss: 0.0036021603737026453\n",
            "step: 300, loss: 0.002300660591572523\n",
            "step: 310, loss: 0.00042168295476585627\n",
            "step: 320, loss: 0.003634663298726082\n",
            "step: 330, loss: 0.0017638663994148374\n",
            "step: 340, loss: 0.0003379739064257592\n",
            "step: 350, loss: 0.0006439135177060962\n",
            "step: 360, loss: 0.00019127241102978587\n",
            "step: 370, loss: 0.0001902558869915083\n",
            "step: 380, loss: 5.248470188234933e-05\n",
            "step: 390, loss: 0.0007128320867195725\n",
            "step: 400, loss: 0.0008334564045071602\n",
            "step: 410, loss: 6.16836769040674e-05\n",
            "step: 420, loss: 0.004146751016378403\n",
            "step: 430, loss: 9.314445924246684e-05\n",
            "step: 440, loss: 0.00030462394352070987\n",
            "step: 450, loss: 7.98014661995694e-05\n",
            "step: 460, loss: 0.0009914366528391838\n",
            "step: 470, loss: 0.0009585041552782059\n",
            "step: 480, loss: 0.0001914559688884765\n",
            "step: 490, loss: 0.00019035476725548506\n",
            "step: 500, loss: 8.650852396385744e-05\n",
            "step: 510, loss: 0.00017522698908578604\n",
            "step: 520, loss: 0.00020599788695108145\n",
            "step: 530, loss: 4.190931576886214e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9408424041646947, f1=0.9318723201524535, best_f1=0.9382830626450116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.418035627575591e-05\n",
            "step: 10, loss: 0.0003571631386876106\n",
            "step: 20, loss: 2.812388993334025e-05\n",
            "step: 30, loss: 0.0008414312615059316\n",
            "step: 40, loss: 0.00019293249351903796\n",
            "step: 50, loss: 0.08740647882223129\n",
            "step: 60, loss: 0.0017405145335942507\n",
            "step: 70, loss: 0.0016778951976448298\n",
            "step: 80, loss: 2.2902546334080398e-05\n",
            "step: 90, loss: 6.790741463191807e-05\n",
            "step: 100, loss: 0.0005022193654440343\n",
            "step: 110, loss: 0.003400301793590188\n",
            "step: 120, loss: 2.9934619306004606e-05\n",
            "step: 130, loss: 0.00047900358913466334\n",
            "step: 140, loss: 4.7019755584187806e-05\n",
            "step: 150, loss: 3.318962626508437e-05\n",
            "step: 160, loss: 3.207089321222156e-05\n",
            "step: 170, loss: 6.056451456970535e-05\n",
            "step: 180, loss: 2.495833905413747e-05\n",
            "step: 190, loss: 0.000182046351255849\n",
            "step: 200, loss: 3.0613529816037044e-05\n",
            "step: 210, loss: 7.845721120247617e-05\n",
            "step: 220, loss: 0.003796194214373827\n",
            "step: 230, loss: 3.6402783734956756e-05\n",
            "step: 240, loss: 0.0001221836282638833\n",
            "step: 250, loss: 1.9263088688603602e-05\n",
            "step: 260, loss: 3.63726940122433e-05\n",
            "step: 270, loss: 5.173025783733465e-05\n",
            "step: 280, loss: 0.0018445936730131507\n",
            "step: 290, loss: 9.796566155273467e-05\n",
            "step: 300, loss: 0.00011559268750716001\n",
            "step: 310, loss: 3.870600266964175e-05\n",
            "step: 320, loss: 2.3706876163487323e-05\n",
            "step: 330, loss: 3.431574077694677e-05\n",
            "step: 340, loss: 7.308595377253368e-05\n",
            "step: 350, loss: 4.816855289391242e-05\n",
            "step: 360, loss: 0.000871892727445811\n",
            "step: 370, loss: 0.0003313029301352799\n",
            "step: 380, loss: 2.055953154922463e-05\n",
            "step: 390, loss: 9.237902850145474e-05\n",
            "step: 400, loss: 0.003909861668944359\n",
            "step: 410, loss: 0.00044074462493881583\n",
            "step: 420, loss: 0.000650393427349627\n",
            "step: 430, loss: 0.0055122836492955685\n",
            "step: 440, loss: 2.8984552045585588e-05\n",
            "step: 450, loss: 0.00014830034342594445\n",
            "step: 460, loss: 3.479349106783047e-05\n",
            "step: 470, loss: 6.014843165758066e-05\n",
            "step: 480, loss: 0.0003691283636726439\n",
            "step: 490, loss: 0.00135265092831105\n",
            "step: 500, loss: 2.199720620410517e-05\n",
            "step: 510, loss: 0.00041157047962769866\n",
            "step: 520, loss: 0.00027459609555080533\n",
            "step: 530, loss: 6.166471575852484e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9457943925233645, f1=0.9359513791491351, best_f1=0.9359513791491351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012900344154331833\n",
            "step: 10, loss: 4.346905916463584e-05\n",
            "step: 20, loss: 1.339601385552669e-05\n",
            "step: 30, loss: 2.765560020634439e-05\n",
            "step: 40, loss: 2.1487005142262205e-05\n",
            "step: 50, loss: 4.2102816223632544e-05\n",
            "step: 60, loss: 3.024725810973905e-05\n",
            "step: 70, loss: 4.0628365240991116e-05\n",
            "step: 80, loss: 2.0026707716169767e-05\n",
            "step: 90, loss: 0.023641474545001984\n",
            "step: 100, loss: 0.000649342080578208\n",
            "step: 110, loss: 0.004033796023577452\n",
            "step: 120, loss: 0.000290151423541829\n",
            "step: 130, loss: 5.6896558817243204e-05\n",
            "step: 140, loss: 1.813446215237491e-05\n",
            "step: 150, loss: 5.246836008154787e-05\n",
            "step: 160, loss: 1.848085412348155e-05\n",
            "step: 170, loss: 2.096929711115081e-05\n",
            "step: 180, loss: 3.135798397124745e-05\n",
            "step: 190, loss: 4.355091732577421e-05\n",
            "step: 200, loss: 7.868644024711102e-05\n",
            "step: 210, loss: 1.989634802157525e-05\n",
            "step: 220, loss: 3.7755573430331424e-05\n",
            "step: 230, loss: 1.883842924144119e-05\n",
            "step: 240, loss: 2.3096148652257398e-05\n",
            "step: 250, loss: 2.000812310143374e-05\n",
            "step: 260, loss: 1.4271410691435449e-05\n",
            "step: 270, loss: 1.3664233847521245e-05\n",
            "step: 280, loss: 3.5132568882545456e-05\n",
            "step: 290, loss: 0.00034402761957608163\n",
            "step: 300, loss: 4.5749780838377774e-05\n",
            "step: 310, loss: 0.00018922869639936835\n",
            "step: 320, loss: 4.795884160557762e-05\n",
            "step: 330, loss: 2.933900395873934e-05\n",
            "step: 340, loss: 2.7606500225374475e-05\n",
            "step: 350, loss: 2.0451176169444807e-05\n",
            "step: 360, loss: 1.8614977307152003e-05\n",
            "step: 370, loss: 6.23813975835219e-05\n",
            "step: 380, loss: 0.00020750105613842607\n",
            "step: 390, loss: 0.003016574075445533\n",
            "step: 400, loss: 9.579947800375521e-05\n",
            "step: 410, loss: 0.00030804661219008267\n",
            "step: 420, loss: 2.0306139049353078e-05\n",
            "step: 430, loss: 0.0011428085854277015\n",
            "step: 440, loss: 4.801710383617319e-05\n",
            "step: 450, loss: 6.050852607586421e-05\n",
            "step: 460, loss: 9.797484381124377e-05\n",
            "step: 470, loss: 4.230225022183731e-05\n",
            "step: 480, loss: 2.1468305931193754e-05\n",
            "step: 490, loss: 3.568506144802086e-05\n",
            "step: 500, loss: 2.0685338313342072e-05\n",
            "step: 510, loss: 0.0014836761401966214\n",
            "step: 520, loss: 0.0004534600884653628\n",
            "step: 530, loss: 1.5493298633373342e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9402843601895735, f1=0.9292161520190024, best_f1=0.9359513791491351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.398618567502126e-05\n",
            "step: 10, loss: 1.6491529095219448e-05\n",
            "step: 20, loss: 2.1855716113350354e-05\n",
            "step: 30, loss: 2.8813146855100058e-05\n",
            "step: 40, loss: 2.9692730095121078e-05\n",
            "step: 50, loss: 0.03821853548288345\n",
            "step: 60, loss: 1.4170833310345188e-05\n",
            "step: 70, loss: 0.00010903707152465358\n",
            "step: 80, loss: 3.103692506556399e-05\n",
            "step: 90, loss: 0.0003631806466728449\n",
            "step: 100, loss: 3.721453686011955e-05\n",
            "step: 110, loss: 0.00016572137246839702\n",
            "step: 120, loss: 0.00176671810913831\n",
            "step: 130, loss: 0.026514139026403427\n",
            "step: 140, loss: 4.313373574404977e-05\n",
            "step: 150, loss: 2.620828126964625e-05\n",
            "step: 160, loss: 1.6551224689465016e-05\n",
            "step: 170, loss: 0.0006833224906586111\n",
            "step: 180, loss: 1.7456381101510487e-05\n",
            "step: 190, loss: 2.7448761102277786e-05\n",
            "step: 200, loss: 0.002730715088546276\n",
            "step: 210, loss: 6.107822991907597e-05\n",
            "step: 220, loss: 2.122996374964714e-05\n",
            "step: 230, loss: 2.8661203032243066e-05\n",
            "step: 240, loss: 1.8529202861827798e-05\n",
            "step: 250, loss: 5.1983570301672444e-05\n",
            "step: 260, loss: 5.9004036302212626e-05\n",
            "step: 270, loss: 2.2500020349980332e-05\n",
            "step: 280, loss: 1.85628050530795e-05\n",
            "step: 290, loss: 6.792822387069464e-05\n",
            "step: 300, loss: 2.810204568959307e-05\n",
            "step: 310, loss: 0.0009059629519470036\n",
            "step: 320, loss: 0.00010739979916252196\n",
            "step: 330, loss: 2.8530615963973105e-05\n",
            "step: 340, loss: 2.2474145225714892e-05\n",
            "step: 350, loss: 1.8536777133704163e-05\n",
            "step: 360, loss: 8.554366650059819e-05\n",
            "step: 370, loss: 1.973248799913563e-05\n",
            "step: 380, loss: 3.9248232496902347e-05\n",
            "step: 390, loss: 0.03034222312271595\n",
            "step: 400, loss: 4.1998628148576245e-05\n",
            "step: 410, loss: 1.716953920549713e-05\n",
            "step: 420, loss: 8.574388630222529e-05\n",
            "step: 430, loss: 3.9186328649520874e-05\n",
            "step: 440, loss: 2.8865833883173764e-05\n",
            "step: 450, loss: 0.0024288841523230076\n",
            "step: 460, loss: 2.4053191737039015e-05\n",
            "step: 470, loss: 2.320379644515924e-05\n",
            "step: 480, loss: 3.0450877602561377e-05\n",
            "step: 490, loss: 2.336040597583633e-05\n",
            "step: 500, loss: 1.3057008800387848e-05\n",
            "step: 510, loss: 0.0002679030003491789\n",
            "step: 520, loss: 1.6674135622452013e-05\n",
            "step: 530, loss: 4.5110955397831276e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9355900329102022, f1=0.9318288669487541, best_f1=0.9359513791491351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11432890594005585\n",
            "step: 10, loss: 2.2429350792663172e-05\n",
            "step: 20, loss: 2.6171775971306488e-05\n",
            "step: 30, loss: 0.00012500063166953623\n",
            "step: 40, loss: 0.04502024129033089\n",
            "step: 50, loss: 7.378914597211406e-05\n",
            "step: 60, loss: 0.0014072456397116184\n",
            "step: 70, loss: 8.950468327384442e-05\n",
            "step: 80, loss: 4.435061782714911e-05\n",
            "step: 90, loss: 2.015350037254393e-05\n",
            "step: 100, loss: 3.486867717583664e-05\n",
            "step: 110, loss: 8.684057684149593e-05\n",
            "step: 120, loss: 0.0006673400639556348\n",
            "step: 130, loss: 0.07446606457233429\n",
            "step: 140, loss: 1.8406115486868657e-05\n",
            "step: 150, loss: 2.0421717636054382e-05\n",
            "step: 160, loss: 1.850312401074916e-05\n",
            "step: 170, loss: 2.0388009943417273e-05\n",
            "step: 180, loss: 1.6342604794772342e-05\n",
            "step: 190, loss: 4.424117287271656e-05\n",
            "step: 200, loss: 1.8879318304243498e-05\n",
            "step: 210, loss: 8.077365055214614e-05\n",
            "step: 220, loss: 1.3518964806280565e-05\n",
            "step: 230, loss: 1.3924995073466562e-05\n",
            "step: 240, loss: 2.639192098286003e-05\n",
            "step: 250, loss: 1.668531695031561e-05\n",
            "step: 260, loss: 1.72477539308602e-05\n",
            "step: 270, loss: 1.748253453115467e-05\n",
            "step: 280, loss: 2.0976671294192784e-05\n",
            "step: 290, loss: 1.4580609786207788e-05\n",
            "step: 300, loss: 5.52138990315143e-05\n",
            "step: 310, loss: 0.00016301081632263958\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 320, loss: 9.275582124246284e-05\n",
            "step: 330, loss: 6.919995212228969e-05\n",
            "step: 340, loss: 1.614887332834769e-05\n",
            "step: 350, loss: 1.2270999832253437e-05\n",
            "step: 360, loss: 3.994162398157641e-05\n",
            "step: 370, loss: 1.410006734658964e-05\n",
            "step: 380, loss: 1.6417157894466072e-05\n",
            "step: 390, loss: 3.386054595466703e-05\n",
            "step: 400, loss: 0.00012319588859099895\n",
            "step: 410, loss: 2.0928086087224074e-05\n",
            "step: 420, loss: 3.128315438516438e-05\n",
            "step: 430, loss: 4.725227699964307e-05\n",
            "step: 440, loss: 3.4920954931294546e-05\n",
            "step: 450, loss: 0.0024981950409710407\n",
            "step: 460, loss: 1.4979186744312756e-05\n",
            "step: 470, loss: 0.005371051840484142\n",
            "step: 480, loss: 1.3228371244622394e-05\n",
            "step: 490, loss: 1.6268129911622964e-05\n",
            "step: 500, loss: 2.994173701154068e-05\n",
            "step: 510, loss: 1.5578958482365124e-05\n",
            "step: 520, loss: 2.0290912289055996e-05\n",
            "step: 530, loss: 1.8030066712526605e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9380863039399624, f1=0.9360902255639098, best_f1=0.9359513791491351\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:28, 200.43it/s]\n",
            "load_f1 = 0.9377901578458682\n",
            "real_f1 = 0.935813953488372\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 193.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11dcbdb-e793-4911-eb2c-45409b5ef6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.572301983833313\n",
            "step: 10, loss: 0.3941054940223694\n",
            "step: 20, loss: 0.3654656410217285\n",
            "step: 30, loss: 0.299784779548645\n",
            "step: 40, loss: 0.16337235271930695\n",
            "step: 50, loss: 0.4199814796447754\n",
            "step: 60, loss: 0.24419060349464417\n",
            "step: 70, loss: 0.15371102094650269\n",
            "step: 80, loss: 0.23000980913639069\n",
            "step: 90, loss: 0.2820333242416382\n",
            "step: 100, loss: 0.3874475955963135\n",
            "step: 110, loss: 0.25426873564720154\n",
            "step: 120, loss: 0.28519344329833984\n",
            "step: 130, loss: 0.20220158994197845\n",
            "step: 140, loss: 0.13728071749210358\n",
            "step: 150, loss: 0.13825064897537231\n",
            "step: 160, loss: 0.2566460072994232\n",
            "step: 170, loss: 0.29734113812446594\n",
            "step: 180, loss: 0.07367555797100067\n",
            "step: 190, loss: 0.1728423684835434\n",
            "step: 200, loss: 0.21142742037773132\n",
            "step: 210, loss: 0.2368597835302353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6289308176100629, f1=0.6859504132231405, best_f1=0.6859504132231405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07640405744314194\n",
            "step: 10, loss: 0.1703793704509735\n",
            "step: 20, loss: 0.1745409220457077\n",
            "step: 30, loss: 0.1668172925710678\n",
            "step: 40, loss: 0.22956176102161407\n",
            "step: 50, loss: 0.16090832650661469\n",
            "step: 60, loss: 0.3464376628398895\n",
            "step: 70, loss: 0.09285777807235718\n",
            "step: 80, loss: 0.1406853049993515\n",
            "step: 90, loss: 0.06297405064105988\n",
            "step: 100, loss: 0.011090509593486786\n",
            "step: 110, loss: 0.17843160033226013\n",
            "step: 120, loss: 0.16283495724201202\n",
            "step: 130, loss: 0.013889086432754993\n",
            "step: 140, loss: 0.18979990482330322\n",
            "step: 150, loss: 0.16619355976581573\n",
            "step: 160, loss: 0.14498940110206604\n",
            "step: 170, loss: 0.1104385107755661\n",
            "step: 180, loss: 0.23433561623096466\n",
            "step: 190, loss: 0.2016455978155136\n",
            "step: 200, loss: 0.07181383669376373\n",
            "step: 210, loss: 0.1937071681022644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6561886051080551, f1=0.7054263565891473, best_f1=0.7054263565891473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0904419794678688\n",
            "step: 10, loss: 0.08893804252147675\n",
            "step: 20, loss: 0.04314545914530754\n",
            "step: 30, loss: 0.14380747079849243\n",
            "step: 40, loss: 0.058134738355875015\n",
            "step: 50, loss: 0.04963115230202675\n",
            "step: 60, loss: 0.10345923900604248\n",
            "step: 70, loss: 0.0508284829556942\n",
            "step: 80, loss: 0.1720404028892517\n",
            "step: 90, loss: 0.06239734962582588\n",
            "step: 100, loss: 0.18835270404815674\n",
            "step: 110, loss: 0.11292258650064468\n",
            "step: 120, loss: 0.09297993779182434\n",
            "step: 130, loss: 0.127125546336174\n",
            "step: 140, loss: 0.08184226602315903\n",
            "step: 150, loss: 0.19515156745910645\n",
            "step: 160, loss: 0.017924867570400238\n",
            "step: 170, loss: 0.06026681140065193\n",
            "step: 180, loss: 0.07014056295156479\n",
            "step: 190, loss: 0.23061585426330566\n",
            "step: 200, loss: 0.036090172827243805\n",
            "step: 210, loss: 0.11473441869020462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6639175257731958, f1=0.6805845511482255, best_f1=0.6805845511482255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09613039344549179\n",
            "step: 10, loss: 0.04047086834907532\n",
            "step: 20, loss: 0.264282763004303\n",
            "step: 30, loss: 0.03992411494255066\n",
            "step: 40, loss: 0.02423546090722084\n",
            "step: 50, loss: 0.192145437002182\n",
            "step: 60, loss: 0.366325706243515\n",
            "step: 70, loss: 0.12497251480817795\n",
            "step: 80, loss: 0.07381749153137207\n",
            "step: 90, loss: 0.01807868853211403\n",
            "step: 100, loss: 0.10294680297374725\n",
            "step: 110, loss: 0.1210656389594078\n",
            "step: 120, loss: 0.09058862924575806\n",
            "step: 130, loss: 0.1163761243224144\n",
            "step: 140, loss: 0.09339318424463272\n",
            "step: 150, loss: 0.01910310797393322\n",
            "step: 160, loss: 0.04744334891438484\n",
            "step: 170, loss: 0.16692176461219788\n",
            "step: 180, loss: 0.2286226749420166\n",
            "step: 190, loss: 0.03397594019770622\n",
            "step: 200, loss: 0.21535718441009521\n",
            "step: 210, loss: 0.12660491466522217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6822033898305084, f1=0.6906779661016949, best_f1=0.6906779661016949\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2409161478281021\n",
            "step: 10, loss: 0.09312979876995087\n",
            "step: 20, loss: 0.12772473692893982\n",
            "step: 30, loss: 0.06881605088710785\n",
            "step: 40, loss: 0.03193971514701843\n",
            "step: 50, loss: 0.010953844524919987\n",
            "step: 60, loss: 0.07795514911413193\n",
            "step: 70, loss: 0.029269492253661156\n",
            "step: 80, loss: 0.03813057020306587\n",
            "step: 90, loss: 0.036703743040561676\n",
            "step: 100, loss: 0.00836554728448391\n",
            "step: 110, loss: 0.1549123376607895\n",
            "step: 120, loss: 0.12257692962884903\n",
            "step: 130, loss: 0.06790617853403091\n",
            "step: 140, loss: 0.04927223175764084\n",
            "step: 150, loss: 0.028469009324908257\n",
            "step: 160, loss: 0.05608278885483742\n",
            "step: 170, loss: 0.0556715726852417\n",
            "step: 180, loss: 0.09934604167938232\n",
            "step: 190, loss: 0.002820629859343171\n",
            "step: 200, loss: 0.08378953486680984\n",
            "step: 210, loss: 0.01250764261931181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.673773987206823, f1=0.6960167714884696, best_f1=0.6906779661016949\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028826449066400528\n",
            "step: 10, loss: 0.027839146554470062\n",
            "step: 20, loss: 0.01938534900546074\n",
            "step: 30, loss: 0.0003175801830366254\n",
            "step: 40, loss: 0.05317449942231178\n",
            "step: 50, loss: 0.0036123956087976694\n",
            "step: 60, loss: 0.07237084209918976\n",
            "step: 70, loss: 0.051917415112257004\n",
            "step: 80, loss: 0.2146013081073761\n",
            "step: 90, loss: 0.08238024264574051\n",
            "step: 100, loss: 0.006208932958543301\n",
            "step: 110, loss: 0.023129906505346298\n",
            "step: 120, loss: 0.05846504494547844\n",
            "step: 130, loss: 0.05813491716980934\n",
            "step: 140, loss: 0.047490011900663376\n",
            "step: 150, loss: 0.008781298995018005\n",
            "step: 160, loss: 0.007454437669366598\n",
            "step: 170, loss: 0.043963391333818436\n",
            "step: 180, loss: 0.019166886806488037\n",
            "step: 190, loss: 0.062043558806180954\n",
            "step: 200, loss: 0.0013538019265979528\n",
            "step: 210, loss: 0.03331022337079048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7042253521126761, f1=0.6858237547892722, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004881426692008972\n",
            "step: 10, loss: 0.01857932284474373\n",
            "step: 20, loss: 0.031336553394794464\n",
            "step: 30, loss: 0.023132100701332092\n",
            "step: 40, loss: 0.0029299480374902487\n",
            "step: 50, loss: 0.055837467312812805\n",
            "step: 60, loss: 0.03590191528201103\n",
            "step: 70, loss: 0.017007695510983467\n",
            "step: 80, loss: 0.03929060325026512\n",
            "step: 90, loss: 0.01602630689740181\n",
            "step: 100, loss: 0.00038122956175357103\n",
            "step: 110, loss: 0.09292154759168625\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.06733439117670059\n",
            "step: 130, loss: 0.013500734232366085\n",
            "step: 140, loss: 0.007962401024997234\n",
            "step: 150, loss: 0.04461180046200752\n",
            "step: 160, loss: 0.05218010023236275\n",
            "step: 170, loss: 0.002356053562834859\n",
            "step: 180, loss: 0.07687848061323166\n",
            "step: 190, loss: 0.036777373403310776\n",
            "step: 200, loss: 0.0027891923673450947\n",
            "step: 210, loss: 0.04181869700551033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.683206106870229, f1=0.6880907372400757, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.052142709493637085\n",
            "step: 10, loss: 0.046751800924539566\n",
            "step: 20, loss: 0.04486225172877312\n",
            "step: 30, loss: 0.07829297333955765\n",
            "step: 40, loss: 0.003583156503736973\n",
            "step: 50, loss: 0.0009121280163526535\n",
            "step: 60, loss: 0.04178067296743393\n",
            "step: 70, loss: 0.0030922635924071074\n",
            "step: 80, loss: 0.029476746916770935\n",
            "step: 90, loss: 0.058098189532756805\n",
            "step: 100, loss: 0.0011958916438743472\n",
            "step: 110, loss: 0.08829181641340256\n",
            "step: 120, loss: 0.0005749156698584557\n",
            "step: 130, loss: 0.005619400180876255\n",
            "step: 140, loss: 0.05100309103727341\n",
            "step: 150, loss: 0.006666963454335928\n",
            "step: 160, loss: 0.0465724803507328\n",
            "step: 170, loss: 0.0002868187439162284\n",
            "step: 180, loss: 0.03494931384921074\n",
            "step: 190, loss: 0.002419155091047287\n",
            "step: 200, loss: 0.00030142979812808335\n",
            "step: 210, loss: 0.11991662532091141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6929460580912863, f1=0.6981519507186859, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01671033725142479\n",
            "step: 10, loss: 0.009868151508271694\n",
            "step: 20, loss: 0.0005503348656930029\n",
            "step: 30, loss: 0.004136851988732815\n",
            "step: 40, loss: 0.0018176076700910926\n",
            "step: 50, loss: 0.0012315837666392326\n",
            "step: 60, loss: 0.18269602954387665\n",
            "step: 70, loss: 0.022534994408488274\n",
            "step: 80, loss: 0.0007177735678851604\n",
            "step: 90, loss: 0.003470040624961257\n",
            "step: 100, loss: 0.05619870126247406\n",
            "step: 110, loss: 0.03247244656085968\n",
            "step: 120, loss: 0.0023698643781244755\n",
            "step: 130, loss: 0.014406121335923672\n",
            "step: 140, loss: 0.010081594809889793\n",
            "step: 150, loss: 0.07376833260059357\n",
            "step: 160, loss: 0.0025611247401684523\n",
            "step: 170, loss: 0.017614159733057022\n",
            "step: 180, loss: 0.014488487504422665\n",
            "step: 190, loss: 0.0026204430032521486\n",
            "step: 200, loss: 0.1189228892326355\n",
            "step: 210, loss: 0.019568676128983498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6933867735470941, f1=0.6851485148514852, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019200801849365234\n",
            "step: 10, loss: 0.012185790576040745\n",
            "step: 20, loss: 0.0003892574750352651\n",
            "step: 30, loss: 0.020605523139238358\n",
            "step: 40, loss: 0.0001629416219657287\n",
            "step: 50, loss: 0.0008329750271514058\n",
            "step: 60, loss: 0.004478591959923506\n",
            "step: 70, loss: 0.031497735530138016\n",
            "step: 80, loss: 0.0031908145174384117\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.008657888509333134\n",
            "step: 100, loss: 0.030557088553905487\n",
            "step: 110, loss: 0.00017885548004414886\n",
            "step: 120, loss: 0.01699383743107319\n",
            "step: 130, loss: 0.05571303889155388\n",
            "step: 140, loss: 0.0013341326266527176\n",
            "step: 150, loss: 0.05760533735156059\n",
            "step: 160, loss: 0.015276359394192696\n",
            "step: 170, loss: 0.0013533206656575203\n",
            "step: 180, loss: 0.04480905830860138\n",
            "step: 190, loss: 0.0855480283498764\n",
            "step: 200, loss: 0.0015235567698255181\n",
            "step: 210, loss: 0.01583554968237877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6899383983572897, f1=0.6940451745379876, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023856276646256447\n",
            "step: 10, loss: 0.028520723804831505\n",
            "step: 20, loss: 0.00935782678425312\n",
            "step: 30, loss: 0.0026838176418095827\n",
            "step: 40, loss: 0.021532660350203514\n",
            "step: 50, loss: 0.017657985910773277\n",
            "step: 60, loss: 0.001782726845704019\n",
            "step: 70, loss: 0.0037113421130925417\n",
            "step: 80, loss: 0.05483262240886688\n",
            "step: 90, loss: 0.010741185396909714\n",
            "step: 100, loss: 0.024613086134195328\n",
            "step: 110, loss: 0.09358198940753937\n",
            "step: 120, loss: 0.07742271572351456\n",
            "step: 130, loss: 0.00029983060085214674\n",
            "step: 140, loss: 8.539092232240364e-05\n",
            "step: 150, loss: 0.0014003252144902945\n",
            "step: 160, loss: 0.017459500581026077\n",
            "step: 170, loss: 0.026072517037391663\n",
            "step: 180, loss: 0.00034199556102976203\n",
            "step: 190, loss: 0.013623356819152832\n",
            "step: 200, loss: 0.0004784954944625497\n",
            "step: 210, loss: 0.0001972979516722262\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.698989898989899, f1=0.6944444444444444, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06805618852376938\n",
            "step: 10, loss: 0.006442139856517315\n",
            "step: 20, loss: 0.024676073342561722\n",
            "step: 30, loss: 0.01580211892724037\n",
            "step: 40, loss: 0.00833108089864254\n",
            "step: 50, loss: 0.002629659604281187\n",
            "step: 60, loss: 0.0008792132721282542\n",
            "step: 70, loss: 0.0014075948856770992\n",
            "step: 80, loss: 0.0003743976994883269\n",
            "step: 90, loss: 0.00715081300586462\n",
            "step: 100, loss: 0.007385516073554754\n",
            "step: 110, loss: 0.0008013063343241811\n",
            "step: 120, loss: 0.002309567527845502\n",
            "step: 130, loss: 0.0006861591245979071\n",
            "step: 140, loss: 0.0023346957750618458\n",
            "step: 150, loss: 0.010719646699726582\n",
            "step: 160, loss: 0.012819251045584679\n",
            "step: 170, loss: 0.00550058763474226\n",
            "step: 180, loss: 0.0003755370562430471\n",
            "step: 190, loss: 0.0016693283105269074\n",
            "step: 200, loss: 0.0003042641328647733\n",
            "step: 210, loss: 0.040483709424734116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6954732510288065, f1=0.7087576374745417, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031114855781197548\n",
            "step: 10, loss: 0.0023251017555594444\n",
            "step: 20, loss: 0.025805797427892685\n",
            "step: 30, loss: 0.034925878047943115\n",
            "step: 40, loss: 0.03486964479088783\n",
            "step: 50, loss: 0.0005311189452186227\n",
            "step: 60, loss: 0.00020569877233356237\n",
            "step: 70, loss: 0.030446602031588554\n",
            "step: 80, loss: 0.005530581343919039\n",
            "step: 90, loss: 0.00019766183686442673\n",
            "step: 100, loss: 0.0003711206081788987\n",
            "step: 110, loss: 0.00040917444857768714\n",
            "step: 120, loss: 8.805565448710695e-05\n",
            "step: 130, loss: 0.006470149848610163\n",
            "step: 140, loss: 0.04700175300240517\n",
            "step: 150, loss: 0.00033407853334210813\n",
            "step: 160, loss: 0.0017867237329483032\n",
            "step: 170, loss: 0.0018989361124113202\n",
            "step: 180, loss: 0.03316314518451691\n",
            "step: 190, loss: 0.00022220493701752275\n",
            "step: 200, loss: 0.0005900968099012971\n",
            "step: 210, loss: 0.0006372916977852583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6712018140589568, f1=0.6832579185520362, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001778160221874714\n",
            "step: 10, loss: 0.0014148764312267303\n",
            "step: 20, loss: 0.015374080277979374\n",
            "step: 30, loss: 0.00024111261882353574\n",
            "step: 40, loss: 0.00013531585864257067\n",
            "step: 50, loss: 0.007092924788594246\n",
            "step: 60, loss: 0.017602767795324326\n",
            "step: 70, loss: 0.00014024657139088959\n",
            "step: 80, loss: 0.00015422569413203746\n",
            "step: 90, loss: 0.00021224345255177468\n",
            "step: 100, loss: 0.0007800977327860892\n",
            "step: 110, loss: 0.00014162609295453876\n",
            "step: 120, loss: 0.019378304481506348\n",
            "step: 130, loss: 0.003513060975819826\n",
            "step: 140, loss: 0.015972357243299484\n",
            "step: 150, loss: 0.00010073567682411522\n",
            "step: 160, loss: 9.707530261948705e-05\n",
            "step: 170, loss: 0.000444816512754187\n",
            "step: 180, loss: 0.0018295724876224995\n",
            "step: 190, loss: 0.00011006985732819885\n",
            "step: 200, loss: 0.0007553450996056199\n",
            "step: 210, loss: 0.04123806580901146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6723768736616702, f1=0.7106382978723405, best_f1=0.6858237547892722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016352182137779891\n",
            "step: 10, loss: 0.0002077579847536981\n",
            "step: 20, loss: 0.0009378936956636608\n",
            "step: 30, loss: 0.034532710909843445\n",
            "step: 40, loss: 0.0006323182606138289\n",
            "step: 50, loss: 8.370322757400572e-05\n",
            "step: 60, loss: 0.0003510572714731097\n",
            "step: 70, loss: 0.00016738398699089885\n",
            "step: 80, loss: 9.27329674595967e-05\n",
            "step: 90, loss: 0.01892555132508278\n",
            "step: 100, loss: 0.0001053515516105108\n",
            "step: 110, loss: 8.462019468424842e-05\n",
            "step: 120, loss: 8.915547368815169e-05\n",
            "step: 130, loss: 0.00012842040450777858\n",
            "step: 140, loss: 0.00011005345004377887\n",
            "step: 150, loss: 0.00010367207141825929\n",
            "step: 160, loss: 0.0007853468996472657\n",
            "step: 170, loss: 0.00012120702012907714\n",
            "step: 180, loss: 5.7925572036765516e-05\n",
            "step: 190, loss: 0.0003026595513802022\n",
            "step: 200, loss: 0.0006712895701639354\n",
            "step: 210, loss: 0.0007433033315464854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.676595744680851, f1=0.7130801687763714, best_f1=0.6858237547892722\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 283.24it/s]\n",
            "load_f1 = 0.6836935166994106\n",
            "real_f1 = 0.686046511627907\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ce5b29-439d-49d9-a05b-dfdb08d2bad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5823693871498108\n",
            "step: 10, loss: 0.36139798164367676\n",
            "step: 20, loss: 0.2790636122226715\n",
            "step: 30, loss: 0.4119768440723419\n",
            "step: 40, loss: 0.44671520590782166\n",
            "step: 50, loss: 0.30343276262283325\n",
            "step: 60, loss: 0.26127833127975464\n",
            "step: 70, loss: 0.22446975111961365\n",
            "step: 80, loss: 0.28559038043022156\n",
            "step: 90, loss: 0.2478112429380417\n",
            "step: 100, loss: 0.35517439246177673\n",
            "step: 110, loss: 0.27189111709594727\n",
            "step: 120, loss: 0.1476224660873413\n",
            "step: 130, loss: 0.17033906280994415\n",
            "step: 140, loss: 0.04734409600496292\n",
            "step: 150, loss: 0.12294584512710571\n",
            "step: 160, loss: 0.05829283967614174\n",
            "step: 170, loss: 0.22430208325386047\n",
            "step: 180, loss: 0.05505599454045296\n",
            "step: 190, loss: 0.16613318026065826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6539509536784741, f1=0.6574585635359116, best_f1=0.6574585635359116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17967119812965393\n",
            "step: 10, loss: 0.07112343609333038\n",
            "step: 20, loss: 0.05950772389769554\n",
            "step: 30, loss: 0.16536715626716614\n",
            "step: 40, loss: 0.22316820919513702\n",
            "step: 50, loss: 0.04124399274587631\n",
            "step: 60, loss: 0.17963923513889313\n",
            "step: 70, loss: 0.10944081097841263\n",
            "step: 80, loss: 0.33517390489578247\n",
            "step: 90, loss: 0.2218671590089798\n",
            "step: 100, loss: 0.026005398482084274\n",
            "step: 110, loss: 0.12230682373046875\n",
            "step: 120, loss: 0.19994384050369263\n",
            "step: 130, loss: 0.10001827031373978\n",
            "step: 140, loss: 0.10683367401361465\n",
            "step: 150, loss: 0.05890798941254616\n",
            "step: 160, loss: 0.029864901676774025\n",
            "step: 170, loss: 0.11519993096590042\n",
            "step: 180, loss: 0.33510681986808777\n",
            "step: 190, loss: 0.02481904998421669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7867036011080332, f1=0.7616438356164383, best_f1=0.7616438356164383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022343872115015984\n",
            "step: 10, loss: 0.02482236921787262\n",
            "step: 20, loss: 0.011309939436614513\n",
            "step: 30, loss: 0.0161169171333313\n",
            "step: 40, loss: 0.05391566827893257\n",
            "step: 50, loss: 0.12146729975938797\n",
            "step: 60, loss: 0.01932242326438427\n",
            "step: 70, loss: 0.0649748295545578\n",
            "step: 80, loss: 0.05909239500761032\n",
            "step: 90, loss: 0.08042450994253159\n",
            "step: 100, loss: 0.061029501259326935\n",
            "step: 110, loss: 0.007285099010914564\n",
            "step: 120, loss: 0.009488922543823719\n",
            "step: 130, loss: 0.02574612945318222\n",
            "step: 140, loss: 0.016337376087903976\n",
            "step: 150, loss: 0.09487242251634598\n",
            "step: 160, loss: 0.08640403300523758\n",
            "step: 170, loss: 0.06463710218667984\n",
            "step: 180, loss: 0.03787438943982124\n",
            "step: 190, loss: 0.09734451025724411\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8021108179419525, f1=0.802030456852792, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02283041551709175\n",
            "step: 10, loss: 0.16990026831626892\n",
            "step: 20, loss: 0.03812193498015404\n",
            "step: 30, loss: 0.009410946629941463\n",
            "step: 40, loss: 0.016425572335720062\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.07011391967535019\n",
            "step: 60, loss: 0.08538215607404709\n",
            "step: 70, loss: 0.05389038845896721\n",
            "step: 80, loss: 0.12212590873241425\n",
            "step: 90, loss: 0.02496788464486599\n",
            "step: 100, loss: 0.017213841900229454\n",
            "step: 110, loss: 0.0017357668839395046\n",
            "step: 120, loss: 0.04424126818776131\n",
            "step: 130, loss: 0.14886124432086945\n",
            "step: 140, loss: 0.016344722360372543\n",
            "step: 150, loss: 0.008121415972709656\n",
            "step: 160, loss: 0.008815024979412556\n",
            "step: 170, loss: 0.12436394393444061\n",
            "step: 180, loss: 0.012891613878309727\n",
            "step: 190, loss: 0.04867830127477646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7671957671957672, f1=0.7777777777777779, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0721939206123352\n",
            "step: 10, loss: 0.015240266919136047\n",
            "step: 20, loss: 0.10138542950153351\n",
            "step: 30, loss: 0.010217140428721905\n",
            "step: 40, loss: 0.10061194002628326\n",
            "step: 50, loss: 0.15603618323802948\n",
            "step: 60, loss: 0.08403822034597397\n",
            "step: 70, loss: 0.03257272019982338\n",
            "step: 80, loss: 0.024161657318472862\n",
            "step: 90, loss: 0.011556667275726795\n",
            "step: 100, loss: 0.0017372473375871778\n",
            "step: 110, loss: 0.037494976073503494\n",
            "step: 120, loss: 0.0059323860332369804\n",
            "step: 130, loss: 0.10684336721897125\n",
            "step: 140, loss: 0.004927216097712517\n",
            "step: 150, loss: 0.16192373633384705\n",
            "step: 160, loss: 0.00948312971740961\n",
            "step: 170, loss: 0.0022482769563794136\n",
            "step: 180, loss: 0.003131476230919361\n",
            "step: 190, loss: 0.038464222103357315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7616438356164383, f1=0.7548209366391184, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004416508600115776\n",
            "step: 10, loss: 0.18004998564720154\n",
            "step: 20, loss: 0.003540550824254751\n",
            "step: 30, loss: 0.0013530862051993608\n",
            "step: 40, loss: 0.004347852431237698\n",
            "step: 50, loss: 0.0028399445582181215\n",
            "step: 60, loss: 0.16819961369037628\n",
            "step: 70, loss: 0.17688293755054474\n",
            "step: 80, loss: 0.026408646255731583\n",
            "step: 90, loss: 0.016260787844657898\n",
            "step: 100, loss: 0.0005952690844424069\n",
            "step: 110, loss: 0.04186815395951271\n",
            "step: 120, loss: 0.0017931152833625674\n",
            "step: 130, loss: 0.004089527763426304\n",
            "step: 140, loss: 0.003595352405682206\n",
            "step: 150, loss: 0.010654152370989323\n",
            "step: 160, loss: 0.020657211542129517\n",
            "step: 170, loss: 0.04332399740815163\n",
            "step: 180, loss: 0.003158275969326496\n",
            "step: 190, loss: 0.004267342854291201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7819148936170213, f1=0.7798408488063661, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014026794582605362\n",
            "step: 10, loss: 0.0012075980193912983\n",
            "step: 20, loss: 0.012435481883585453\n",
            "step: 30, loss: 0.005342384334653616\n",
            "step: 40, loss: 0.005264537874609232\n",
            "step: 50, loss: 0.013309570960700512\n",
            "step: 60, loss: 0.028523048385977745\n",
            "step: 70, loss: 0.02391928620636463\n",
            "step: 80, loss: 0.006843126844614744\n",
            "step: 90, loss: 0.0054137324914336205\n",
            "step: 100, loss: 0.0013278024271130562\n",
            "step: 110, loss: 0.003363928757607937\n",
            "step: 120, loss: 0.00267079402692616\n",
            "step: 130, loss: 0.0032707606442272663\n",
            "step: 140, loss: 0.0007370196981355548\n",
            "step: 150, loss: 0.022953882813453674\n",
            "step: 160, loss: 0.009629325941205025\n",
            "step: 170, loss: 0.0013098041526973248\n",
            "step: 180, loss: 0.01389120239764452\n",
            "step: 190, loss: 0.0026498616207391024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7880434782608696, f1=0.7989417989417988, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014815190806984901\n",
            "step: 10, loss: 0.013223609887063503\n",
            "step: 20, loss: 0.0004260240530129522\n",
            "step: 30, loss: 0.0063969907350838184\n",
            "step: 40, loss: 0.0004663862055167556\n",
            "step: 50, loss: 0.15459348261356354\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.0005536035168915987\n",
            "step: 70, loss: 0.001675630221143365\n",
            "step: 80, loss: 0.00018038696725852787\n",
            "step: 90, loss: 0.0002726721577346325\n",
            "step: 100, loss: 0.0008367647533304989\n",
            "step: 110, loss: 0.0004922044463455677\n",
            "step: 120, loss: 0.0010283977026119828\n",
            "step: 130, loss: 0.0005596933187916875\n",
            "step: 140, loss: 0.0005626978818327188\n",
            "step: 150, loss: 0.0027743473183363676\n",
            "step: 160, loss: 0.0008936345693655312\n",
            "step: 170, loss: 0.002606310648843646\n",
            "step: 180, loss: 0.004496345296502113\n",
            "step: 190, loss: 0.4852410852909088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7688311688311688, f1=0.7735368956743003, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003125400049611926\n",
            "step: 10, loss: 0.04939427971839905\n",
            "step: 20, loss: 0.0015230823773890734\n",
            "step: 30, loss: 0.0028543751686811447\n",
            "step: 40, loss: 0.001208114204928279\n",
            "step: 50, loss: 0.004229413811117411\n",
            "step: 60, loss: 0.018002083525061607\n",
            "step: 70, loss: 0.0006583057111129165\n",
            "step: 80, loss: 0.0005042277043685317\n",
            "step: 90, loss: 0.0009581436170265079\n",
            "step: 100, loss: 0.0814618170261383\n",
            "step: 110, loss: 0.0035992045886814594\n",
            "step: 120, loss: 0.0026816665194928646\n",
            "step: 130, loss: 0.0025244627613574266\n",
            "step: 140, loss: 0.00228467071428895\n",
            "step: 150, loss: 0.004849170334637165\n",
            "step: 160, loss: 0.00027753645554184914\n",
            "step: 170, loss: 0.0009322082041762769\n",
            "step: 180, loss: 0.00434296578168869\n",
            "step: 190, loss: 0.0002506980672478676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7873563218390804, f1=0.776536312849162, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007533561438322067\n",
            "step: 10, loss: 0.0005206839996390045\n",
            "step: 20, loss: 0.20725691318511963\n",
            "step: 30, loss: 0.0037379942368716\n",
            "step: 40, loss: 0.04035133123397827\n",
            "step: 50, loss: 0.0006451589870266616\n",
            "step: 60, loss: 0.0006848065531812608\n",
            "step: 70, loss: 0.0007278256816789508\n",
            "step: 80, loss: 0.000760201015509665\n",
            "step: 90, loss: 0.0006546287913806736\n",
            "step: 100, loss: 0.0023698064032942057\n",
            "step: 110, loss: 0.0017040452221408486\n",
            "step: 120, loss: 0.15135179460048676\n",
            "step: 130, loss: 0.00043905971688218415\n",
            "step: 140, loss: 0.001573885208927095\n",
            "step: 150, loss: 0.0006396325770765543\n",
            "step: 160, loss: 0.018822096288204193\n",
            "step: 170, loss: 0.0013020007172599435\n",
            "step: 180, loss: 0.00029432872543111444\n",
            "step: 190, loss: 0.0007162682595662773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7903225806451613, f1=0.8115183246073298, best_f1=0.802030456852792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005538600962609053\n",
            "step: 10, loss: 0.0005234460113570094\n",
            "step: 20, loss: 0.004193445667624474\n",
            "step: 30, loss: 0.006658586207777262\n",
            "step: 40, loss: 0.0003849398344755173\n",
            "step: 50, loss: 0.0003051572712138295\n",
            "step: 60, loss: 0.0004162489785812795\n",
            "step: 70, loss: 0.0004327136557549238\n",
            "step: 80, loss: 0.00046319596003741026\n",
            "step: 90, loss: 0.0005331168649718165\n",
            "step: 100, loss: 0.0016615181230008602\n",
            "step: 110, loss: 0.0005565047031268477\n",
            "step: 120, loss: 0.0006035658880136907\n",
            "step: 130, loss: 0.00512717617675662\n",
            "step: 140, loss: 0.0005544417072087526\n",
            "step: 150, loss: 0.00025543637457303703\n",
            "step: 160, loss: 0.0011810294818133116\n",
            "step: 170, loss: 0.0028845653869211674\n",
            "step: 180, loss: 0.00040378450648859143\n",
            "step: 190, loss: 0.0007576559437438846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8042895442359249, f1=0.7814207650273224, best_f1=0.7814207650273224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006039765663444996\n",
            "step: 10, loss: 0.0006932242540642619\n",
            "step: 20, loss: 0.000359586498234421\n",
            "step: 30, loss: 0.14162983000278473\n",
            "step: 40, loss: 0.0006049525691196322\n",
            "step: 50, loss: 0.0010551774175837636\n",
            "step: 60, loss: 0.020154988393187523\n",
            "step: 70, loss: 0.00045878274249844253\n",
            "step: 80, loss: 0.0019173604669049382\n",
            "step: 90, loss: 0.0003233580500818789\n",
            "step: 100, loss: 0.0006293276092037559\n",
            "step: 110, loss: 0.0006966934888623655\n",
            "step: 120, loss: 0.00017628143541514874\n",
            "step: 130, loss: 0.0006320445681922138\n",
            "step: 140, loss: 0.001473252777941525\n",
            "step: 150, loss: 0.00038687142659910023\n",
            "step: 160, loss: 0.00021077644487377256\n",
            "step: 170, loss: 0.0015882629668340087\n",
            "step: 180, loss: 8.76850972417742e-05\n",
            "step: 190, loss: 0.00017582850705366582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7966573816155988, f1=0.7912087912087912, best_f1=0.7814207650273224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005166057962924242\n",
            "step: 10, loss: 0.0002861553803086281\n",
            "step: 20, loss: 0.001296639209613204\n",
            "step: 30, loss: 0.0018618948524817824\n",
            "step: 40, loss: 0.0009737786022014916\n",
            "step: 50, loss: 0.0002940944686997682\n",
            "step: 60, loss: 0.0001525125262560323\n",
            "step: 70, loss: 0.0018098036525771022\n",
            "step: 80, loss: 0.0007169791497290134\n",
            "step: 90, loss: 0.0003508404770400375\n",
            "step: 100, loss: 0.0011442331597208977\n",
            "step: 110, loss: 0.0001864479563664645\n",
            "step: 120, loss: 0.004817552398890257\n",
            "step: 130, loss: 0.00012880380381830037\n",
            "step: 140, loss: 0.00022456690203398466\n",
            "step: 150, loss: 0.0001697426341706887\n",
            "step: 160, loss: 0.0001360053865937516\n",
            "step: 170, loss: 0.00018407318566460162\n",
            "step: 180, loss: 0.00013748220226261765\n",
            "step: 190, loss: 0.0010834136046469212\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7947368421052631, f1=0.7908163265306122, best_f1=0.7814207650273224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013998591748531908\n",
            "step: 10, loss: 0.000955234922003001\n",
            "step: 20, loss: 0.00019947507826145738\n",
            "step: 30, loss: 8.278261520899832e-05\n",
            "step: 40, loss: 0.0024064930621534586\n",
            "step: 50, loss: 0.00022571507724933326\n",
            "step: 60, loss: 0.01990429498255253\n",
            "step: 70, loss: 0.00011534907389432192\n",
            "step: 80, loss: 9.766790753928944e-05\n",
            "step: 90, loss: 0.0027706639375537634\n",
            "step: 100, loss: 0.002492742147296667\n",
            "step: 110, loss: 0.0005418646614998579\n",
            "step: 120, loss: 0.0002979286073241383\n",
            "step: 130, loss: 0.0001766862696968019\n",
            "step: 140, loss: 0.00020590041822288185\n",
            "step: 150, loss: 0.00017978948017116636\n",
            "step: 160, loss: 0.0006636455655097961\n",
            "step: 170, loss: 0.00021460893913172185\n",
            "step: 180, loss: 0.0002210819802712649\n",
            "step: 190, loss: 0.00019875913858413696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7978723404255318, f1=0.7866323907455013, best_f1=0.7814207650273224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005377138499170542\n",
            "step: 10, loss: 0.0002377331693423912\n",
            "step: 20, loss: 0.00019460574549157172\n",
            "step: 30, loss: 0.00010719015699578449\n",
            "step: 40, loss: 0.13149785995483398\n",
            "step: 50, loss: 0.00013558450154960155\n",
            "step: 60, loss: 0.00025498951436020434\n",
            "step: 70, loss: 0.0005971029750071466\n",
            "step: 80, loss: 0.00028105787350796163\n",
            "step: 90, loss: 0.0001561603567097336\n",
            "step: 100, loss: 0.0002676699950825423\n",
            "step: 110, loss: 0.00016833478002808988\n",
            "step: 120, loss: 0.0002828467986546457\n",
            "step: 130, loss: 0.00016365162446163595\n",
            "step: 140, loss: 0.00019201147370040417\n",
            "step: 150, loss: 0.00025680565158836544\n",
            "step: 160, loss: 0.0001658838737057522\n",
            "step: 170, loss: 0.04905526340007782\n",
            "step: 180, loss: 0.002641662722453475\n",
            "step: 190, loss: 0.00022922085190657526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7958115183246073, f1=0.7869674185463658, best_f1=0.7814207650273224\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:11, 174.22it/s]\n",
            "load_f1 = 0.5156794425087108\n",
            "real_f1 = 0.5094664371772805\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f80270b-bca6-4b16-a185-c61f65e8013a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6247397661209106\n",
            "step: 10, loss: 0.3557359278202057\n",
            "step: 20, loss: 0.2834051549434662\n",
            "step: 30, loss: 0.40132585167884827\n",
            "step: 40, loss: 0.2753695845603943\n",
            "step: 50, loss: 0.26979121565818787\n",
            "step: 60, loss: 0.23669739067554474\n",
            "step: 70, loss: 0.3674873411655426\n",
            "step: 80, loss: 0.33642902970314026\n",
            "step: 90, loss: 0.2037467509508133\n",
            "step: 100, loss: 0.09567026793956757\n",
            "step: 110, loss: 0.131685271859169\n",
            "step: 120, loss: 0.12959757447242737\n",
            "step: 130, loss: 0.04608611389994621\n",
            "step: 140, loss: 0.13999848067760468\n",
            "step: 150, loss: 0.39430058002471924\n",
            "step: 160, loss: 0.14999598264694214\n",
            "step: 170, loss: 0.23324286937713623\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7563451776649747, f1=0.727710843373494, best_f1=0.727710843373494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09803439676761627\n",
            "step: 10, loss: 0.20545488595962524\n",
            "step: 20, loss: 0.09366738796234131\n",
            "step: 30, loss: 0.27811795473098755\n",
            "step: 40, loss: 0.05070352181792259\n",
            "step: 50, loss: 0.06934833526611328\n",
            "step: 60, loss: 0.14549550414085388\n",
            "step: 70, loss: 0.12068112939596176\n",
            "step: 80, loss: 0.09786354005336761\n",
            "step: 90, loss: 0.16819195449352264\n",
            "step: 100, loss: 0.07156727463006973\n",
            "step: 110, loss: 0.08533837646245956\n",
            "step: 120, loss: 0.07879835367202759\n",
            "step: 130, loss: 0.053931478410959244\n",
            "step: 140, loss: 0.35383936762809753\n",
            "step: 150, loss: 0.14788289368152618\n",
            "step: 160, loss: 0.18028166890144348\n",
            "step: 170, loss: 0.03680713102221489\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8037383177570093, f1=0.7667436489607391, best_f1=0.7667436489607391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1639876812696457\n",
            "step: 10, loss: 0.06437782198190689\n",
            "step: 20, loss: 0.09415853023529053\n",
            "step: 30, loss: 0.1554536670446396\n",
            "step: 40, loss: 0.0693693608045578\n",
            "step: 50, loss: 0.022579507902264595\n",
            "step: 60, loss: 0.023029319941997528\n",
            "step: 70, loss: 0.07122821360826492\n",
            "step: 80, loss: 0.05866663530468941\n",
            "step: 90, loss: 0.08580131083726883\n",
            "step: 100, loss: 0.03139529749751091\n",
            "step: 110, loss: 0.05998484417796135\n",
            "step: 120, loss: 0.07870571315288544\n",
            "step: 130, loss: 0.17779803276062012\n",
            "step: 140, loss: 0.024851318448781967\n",
            "step: 150, loss: 0.019277743995189667\n",
            "step: 160, loss: 0.017877349629998207\n",
            "step: 170, loss: 0.06171262264251709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.8019559902200489, f1=0.7865707434052758, best_f1=0.7667436489607391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006530554033815861\n",
            "step: 10, loss: 0.019609889015555382\n",
            "step: 20, loss: 0.04399288445711136\n",
            "step: 30, loss: 0.025226695463061333\n",
            "step: 40, loss: 0.0030955791007727385\n",
            "step: 50, loss: 0.10359357297420502\n",
            "step: 60, loss: 0.11500443518161774\n",
            "step: 70, loss: 0.006238303147256374\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.02726191282272339\n",
            "step: 90, loss: 0.08451920002698898\n",
            "step: 100, loss: 0.17299172282218933\n",
            "step: 110, loss: 0.08107675611972809\n",
            "step: 120, loss: 0.018166884779930115\n",
            "step: 130, loss: 0.03380856662988663\n",
            "step: 140, loss: 0.026949871331453323\n",
            "step: 150, loss: 0.13063260912895203\n",
            "step: 160, loss: 0.08970829099416733\n",
            "step: 170, loss: 0.006755107548087835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8421052631578948, f1=0.8105263157894737, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007052374072372913\n",
            "step: 10, loss: 0.08909985423088074\n",
            "step: 20, loss: 0.06588388234376907\n",
            "step: 30, loss: 0.013061020523309708\n",
            "step: 40, loss: 0.013039574958384037\n",
            "step: 50, loss: 0.004659783095121384\n",
            "step: 60, loss: 0.03800704702734947\n",
            "step: 70, loss: 0.23741455376148224\n",
            "step: 80, loss: 0.0489974170923233\n",
            "step: 90, loss: 0.06120522692799568\n",
            "step: 100, loss: 0.01729172095656395\n",
            "step: 110, loss: 0.03956054151058197\n",
            "step: 120, loss: 0.025052864104509354\n",
            "step: 130, loss: 0.030158866196870804\n",
            "step: 140, loss: 0.03643541783094406\n",
            "step: 150, loss: 0.02451668307185173\n",
            "step: 160, loss: 0.09319374710321426\n",
            "step: 170, loss: 0.0175810307264328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8407960199004973, f1=0.836272040302267, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007393453735858202\n",
            "step: 10, loss: 0.011073608882725239\n",
            "step: 20, loss: 0.01889253780245781\n",
            "step: 30, loss: 0.04147773236036301\n",
            "step: 40, loss: 0.05325925350189209\n",
            "step: 50, loss: 0.07707115262746811\n",
            "step: 60, loss: 0.03636804223060608\n",
            "step: 70, loss: 0.09096931666135788\n",
            "step: 80, loss: 0.024120764806866646\n",
            "step: 90, loss: 0.010934561491012573\n",
            "step: 100, loss: 0.007033841218799353\n",
            "step: 110, loss: 0.0029913310427218676\n",
            "step: 120, loss: 0.05412686988711357\n",
            "step: 130, loss: 0.009501706808805466\n",
            "step: 140, loss: 0.03084452450275421\n",
            "step: 150, loss: 0.015896888449788094\n",
            "step: 160, loss: 0.016999203711748123\n",
            "step: 170, loss: 0.003082261187955737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8393285371702638, f1=0.8137254901960784, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014485983410850167\n",
            "step: 10, loss: 0.056258365511894226\n",
            "step: 20, loss: 0.03008987382054329\n",
            "step: 30, loss: 0.029297756031155586\n",
            "step: 40, loss: 0.008847227320075035\n",
            "step: 50, loss: 0.04356733337044716\n",
            "step: 60, loss: 0.0013196842046454549\n",
            "step: 70, loss: 0.00048074114602059126\n",
            "step: 80, loss: 0.04309306666254997\n",
            "step: 90, loss: 0.00025988678680732846\n",
            "step: 100, loss: 0.0004626772424671799\n",
            "step: 110, loss: 0.008421923033893108\n",
            "step: 120, loss: 0.001648954814299941\n",
            "step: 130, loss: 0.12773381173610687\n",
            "step: 140, loss: 0.011524415574967861\n",
            "step: 150, loss: 0.0017628296045586467\n",
            "step: 160, loss: 0.00038232735823839903\n",
            "step: 170, loss: 0.013440708629786968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8277511961722487, f1=0.8173076923076923, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01979573257267475\n",
            "step: 10, loss: 0.0020578971598297358\n",
            "step: 20, loss: 0.025443060323596\n",
            "step: 30, loss: 0.04129326343536377\n",
            "step: 40, loss: 0.0033373581245541573\n",
            "step: 50, loss: 0.001474421238526702\n",
            "step: 60, loss: 0.014353714883327484\n",
            "step: 70, loss: 0.0007475012680515647\n",
            "step: 80, loss: 0.015224956907331944\n",
            "step: 90, loss: 0.001989985816180706\n",
            "step: 100, loss: 0.018865926191210747\n",
            "step: 110, loss: 0.05880993977189064\n",
            "step: 120, loss: 0.0350002646446228\n",
            "step: 130, loss: 0.0037435346748679876\n",
            "step: 140, loss: 0.01149837113916874\n",
            "step: 150, loss: 0.021116366609930992\n",
            "step: 160, loss: 0.006164606660604477\n",
            "step: 170, loss: 0.0017917925724759698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8383838383838383, f1=0.8223350253807107, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005018077208660543\n",
            "step: 10, loss: 0.0005653458065353334\n",
            "step: 20, loss: 0.0005109384073875844\n",
            "step: 30, loss: 0.006676857825368643\n",
            "step: 40, loss: 0.0002979809360112995\n",
            "step: 50, loss: 0.00048322626389563084\n",
            "step: 60, loss: 0.00779092637822032\n",
            "step: 70, loss: 0.057401143014431\n",
            "step: 80, loss: 0.0016724836314097047\n",
            "step: 90, loss: 0.04991661384701729\n",
            "step: 100, loss: 0.007119432091712952\n",
            "step: 110, loss: 0.002348655601963401\n",
            "step: 120, loss: 0.0056169223971664906\n",
            "step: 130, loss: 0.07143401354551315\n",
            "step: 140, loss: 0.0005462634726427495\n",
            "step: 150, loss: 0.0005910326144658029\n",
            "step: 160, loss: 0.021150846034288406\n",
            "step: 170, loss: 0.002123346086591482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8368421052631579, f1=0.8358974358974359, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.052415672689676285\n",
            "step: 10, loss: 0.0013870454858988523\n",
            "step: 20, loss: 0.0022158552892506123\n",
            "step: 30, loss: 0.011646276339888573\n",
            "step: 40, loss: 0.0002445212157908827\n",
            "step: 50, loss: 0.05683981999754906\n",
            "step: 60, loss: 0.00022050221741665155\n",
            "step: 70, loss: 0.001052939798682928\n",
            "step: 80, loss: 0.0032348872628062963\n",
            "step: 90, loss: 0.015621309168636799\n",
            "step: 100, loss: 0.00034587091067805886\n",
            "step: 110, loss: 0.006925664842128754\n",
            "step: 120, loss: 0.0009932677494361997\n",
            "step: 130, loss: 0.006491872947663069\n",
            "step: 140, loss: 0.002214438281953335\n",
            "step: 150, loss: 0.0012581741902977228\n",
            "step: 160, loss: 0.001544465310871601\n",
            "step: 170, loss: 0.0002776343608275056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8324607329842932, f1=0.8372093023255814, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022469041869044304\n",
            "step: 10, loss: 0.004128649830818176\n",
            "step: 20, loss: 0.23532213270664215\n",
            "step: 30, loss: 0.00014429680595640093\n",
            "step: 40, loss: 0.00020066062279511243\n",
            "step: 50, loss: 0.004929929506033659\n",
            "step: 60, loss: 0.0035269649233669043\n",
            "step: 70, loss: 0.0007842086488381028\n",
            "step: 80, loss: 0.0007843874045647681\n",
            "step: 90, loss: 0.0002897685917560011\n",
            "step: 100, loss: 0.004590378142893314\n",
            "step: 110, loss: 0.04852055385708809\n",
            "step: 120, loss: 0.0002118435368174687\n",
            "step: 130, loss: 0.00012230032007209957\n",
            "step: 140, loss: 0.013733474537730217\n",
            "step: 150, loss: 0.0041802567429840565\n",
            "step: 160, loss: 0.004010302014648914\n",
            "step: 170, loss: 0.0049280025996267796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8238482384823848, f1=0.8052631578947368, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028586555272340775\n",
            "step: 10, loss: 0.000138975286972709\n",
            "step: 20, loss: 0.0001560179516673088\n",
            "step: 30, loss: 0.00022802987950854003\n",
            "step: 40, loss: 0.00024837470846250653\n",
            "step: 50, loss: 9.569436952006072e-05\n",
            "step: 60, loss: 0.006319834850728512\n",
            "step: 70, loss: 0.05224290490150452\n",
            "step: 80, loss: 7.18730443622917e-05\n",
            "step: 90, loss: 0.00018588286184240133\n",
            "step: 100, loss: 0.0004863426147494465\n",
            "step: 110, loss: 0.00022006934159435332\n",
            "step: 120, loss: 0.04662118852138519\n",
            "step: 130, loss: 0.002932666800916195\n",
            "step: 140, loss: 0.005481495056301355\n",
            "step: 150, loss: 0.00038962418329901993\n",
            "step: 160, loss: 0.00018373466446064413\n",
            "step: 170, loss: 0.00015723310934845358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8342245989304814, f1=0.806282722513089, best_f1=0.8105263157894737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027101965621113777\n",
            "step: 10, loss: 0.0010031445417553186\n",
            "step: 20, loss: 8.422793325735256e-05\n",
            "step: 30, loss: 0.00010993202886311337\n",
            "step: 40, loss: 0.00016059441259130836\n",
            "step: 50, loss: 0.0003915915731340647\n",
            "step: 60, loss: 0.016320014372467995\n",
            "step: 70, loss: 0.000235377621720545\n",
            "step: 80, loss: 0.0001642543647903949\n",
            "step: 90, loss: 7.128328434191644e-05\n",
            "step: 100, loss: 0.00018172601994592696\n",
            "step: 110, loss: 7.810909301042557e-05\n",
            "step: 120, loss: 0.04238767549395561\n",
            "step: 130, loss: 0.00066809740383178\n",
            "step: 140, loss: 9.989050886360928e-05\n",
            "step: 150, loss: 0.01725485548377037\n",
            "step: 160, loss: 0.00041188867180608213\n",
            "step: 170, loss: 0.0001936785993166268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8503937007874015, f1=0.8286445012787724, best_f1=0.8286445012787724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000240803012275137\n",
            "step: 10, loss: 0.00010501353244762868\n",
            "step: 20, loss: 0.0009393573272973299\n",
            "step: 30, loss: 0.00010471000132383779\n",
            "step: 40, loss: 0.0001952246093424037\n",
            "step: 50, loss: 0.0002549690834712237\n",
            "step: 60, loss: 0.00010861168266274035\n",
            "step: 70, loss: 0.00011427969002397731\n",
            "step: 80, loss: 0.0024676385801285505\n",
            "step: 90, loss: 0.05531714856624603\n",
            "step: 100, loss: 0.0001428145042154938\n",
            "step: 110, loss: 0.00032266779453493655\n",
            "step: 120, loss: 0.00610985467210412\n",
            "step: 130, loss: 0.00019488527323119342\n",
            "step: 140, loss: 0.00011650851229205728\n",
            "step: 150, loss: 0.00023297264124266803\n",
            "step: 160, loss: 4.595372956828214e-05\n",
            "step: 170, loss: 0.00019231399346608669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8488063660477452, f1=0.824742268041237, best_f1=0.8286445012787724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039390684105455875\n",
            "step: 10, loss: 0.0009647891856729984\n",
            "step: 20, loss: 0.010717416182160378\n",
            "step: 30, loss: 0.00012746674474328756\n",
            "step: 40, loss: 0.00014511636982206255\n",
            "step: 50, loss: 0.0013555031036958098\n",
            "step: 60, loss: 0.0044702948071062565\n",
            "step: 70, loss: 0.00013254614896140993\n",
            "step: 80, loss: 0.016949187964200974\n",
            "step: 90, loss: 0.00013278382539283484\n",
            "step: 100, loss: 0.000492600433062762\n",
            "step: 110, loss: 0.00018438750703353435\n",
            "step: 120, loss: 0.00017108075553551316\n",
            "step: 130, loss: 0.0027756975032389164\n",
            "step: 140, loss: 0.0003277519135735929\n",
            "step: 150, loss: 0.010654158890247345\n",
            "step: 160, loss: 0.00018583291966933757\n",
            "step: 170, loss: 0.003023434430360794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8488063660477452, f1=0.8385416666666666, best_f1=0.8286445012787724\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 231.98it/s]\n",
            "load_f1 = 0.5216\n",
            "real_f1 = 0.5220125786163522\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9062b7-70b4-4233-8fd9-425bd1b0f9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6194750070571899\n",
            "step: 10, loss: 0.6161248683929443\n",
            "step: 20, loss: 0.3646714389324188\n",
            "step: 30, loss: 0.07974057644605637\n",
            "step: 40, loss: 0.23349368572235107\n",
            "step: 50, loss: 0.058283086866140366\n",
            "step: 60, loss: 0.10044883191585541\n",
            "step: 70, loss: 0.07128609716892242\n",
            "step: 80, loss: 0.06452182680368423\n",
            "step: 90, loss: 0.02671315148472786\n",
            "step: 100, loss: 0.012483140453696251\n",
            "step: 110, loss: 0.308052122592926\n",
            "step: 120, loss: 0.017459839582443237\n",
            "step: 130, loss: 0.011220098473131657\n",
            "step: 140, loss: 0.0038542302791029215\n",
            "step: 150, loss: 0.008862766437232494\n",
            "step: 160, loss: 0.0030408089514821768\n",
            "step: 170, loss: 0.14826180040836334\n",
            "step: 180, loss: 0.02860269695520401\n",
            "step: 190, loss: 0.08088603615760803\n",
            "step: 200, loss: 0.21614088118076324\n",
            "step: 210, loss: 0.015486326068639755\n",
            "step: 220, loss: 0.031737830489873886\n",
            "step: 230, loss: 0.020072808489203453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9705215419501134, f1=0.9704545454545453, best_f1=0.9704545454545453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0073662251234054565\n",
            "step: 10, loss: 0.01342717744410038\n",
            "step: 20, loss: 0.07732295989990234\n",
            "step: 30, loss: 0.017877647653222084\n",
            "step: 40, loss: 0.020342664793133736\n",
            "step: 50, loss: 0.002430680440738797\n",
            "step: 60, loss: 0.0030196174047887325\n",
            "step: 70, loss: 0.09509935975074768\n",
            "step: 80, loss: 0.008904285728931427\n",
            "step: 90, loss: 0.0032832338474690914\n",
            "step: 100, loss: 0.10952574014663696\n",
            "step: 110, loss: 0.20117107033729553\n",
            "step: 120, loss: 0.15603390336036682\n",
            "step: 130, loss: 0.014829511754214764\n",
            "step: 140, loss: 0.0017981319688260555\n",
            "step: 150, loss: 0.01569884642958641\n",
            "step: 160, loss: 0.16152271628379822\n",
            "step: 170, loss: 0.0017253595869988203\n",
            "step: 180, loss: 0.008099624887108803\n",
            "step: 190, loss: 0.004323799163103104\n",
            "step: 200, loss: 0.0020966229494661093\n",
            "step: 210, loss: 0.005608984734863043\n",
            "step: 220, loss: 0.15206070244312286\n",
            "step: 230, loss: 0.022555917501449585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.972129319955407, f1=0.9718785151856018, best_f1=0.9718785151856018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012753073126077652\n",
            "step: 10, loss: 0.003391615580767393\n",
            "step: 20, loss: 0.0026529557071626186\n",
            "step: 30, loss: 0.011045488528907299\n",
            "step: 40, loss: 0.11922239512205124\n",
            "step: 50, loss: 0.0251388531178236\n",
            "step: 60, loss: 0.004208309110254049\n",
            "step: 70, loss: 0.025801509618759155\n",
            "step: 80, loss: 0.0012001912109553814\n",
            "step: 90, loss: 0.009886585175991058\n",
            "step: 100, loss: 0.0008118034456856549\n",
            "step: 110, loss: 0.0005542384460568428\n",
            "step: 120, loss: 0.007345037069171667\n",
            "step: 130, loss: 0.0005014548078179359\n",
            "step: 140, loss: 0.007338046096265316\n",
            "step: 150, loss: 0.015074782073497772\n",
            "step: 160, loss: 0.004116553347557783\n",
            "step: 170, loss: 0.003682695794850588\n",
            "step: 180, loss: 0.02389516495168209\n",
            "step: 190, loss: 0.0019012369448319077\n",
            "step: 200, loss: 0.02804599329829216\n",
            "step: 210, loss: 0.01989644579589367\n",
            "step: 220, loss: 0.0004446805687621236\n",
            "step: 230, loss: 0.0037094682920724154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9752252252252253, f1=0.9762174405436014, best_f1=0.9762174405436014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012402908760122955\n",
            "step: 10, loss: 0.00025415653362870216\n",
            "step: 20, loss: 0.00025439599994570017\n",
            "step: 30, loss: 0.0021591896656900644\n",
            "step: 40, loss: 0.028383836150169373\n",
            "step: 50, loss: 0.00036227976670488715\n",
            "step: 60, loss: 0.0003771544143091887\n",
            "step: 70, loss: 0.0009666305268183351\n",
            "step: 80, loss: 0.00193393649533391\n",
            "step: 90, loss: 0.001081919064745307\n",
            "step: 100, loss: 0.00050795276183635\n",
            "step: 110, loss: 0.001288932398892939\n",
            "step: 120, loss: 0.02604277990758419\n",
            "step: 130, loss: 0.0006667233537882566\n",
            "step: 140, loss: 0.0012615375453606248\n",
            "step: 150, loss: 0.10786391794681549\n",
            "step: 160, loss: 0.10004390776157379\n",
            "step: 170, loss: 0.010935881175100803\n",
            "step: 180, loss: 0.001379733206704259\n",
            "step: 190, loss: 0.0032453418243676424\n",
            "step: 200, loss: 0.006291971076279879\n",
            "step: 210, loss: 0.17183321714401245\n",
            "step: 220, loss: 0.0001818019663915038\n",
            "step: 230, loss: 0.009873474948108196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9797752808988766, f1=0.9730337078651685, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005451628239825368\n",
            "step: 10, loss: 0.00020262147882021964\n",
            "step: 20, loss: 0.015504357405006886\n",
            "step: 30, loss: 0.0001868314138846472\n",
            "step: 40, loss: 0.0012964140623807907\n",
            "step: 50, loss: 0.00020481737738009542\n",
            "step: 60, loss: 0.09952779859304428\n",
            "step: 70, loss: 0.0004382619808893651\n",
            "step: 80, loss: 0.00022264209110289812\n",
            "step: 90, loss: 0.003093302482739091\n",
            "step: 100, loss: 0.0009967224905267358\n",
            "step: 110, loss: 0.0024319260846823454\n",
            "step: 120, loss: 0.001637789187952876\n",
            "step: 130, loss: 0.005023275502026081\n",
            "step: 140, loss: 0.004977117292582989\n",
            "step: 150, loss: 0.004216309636831284\n",
            "step: 160, loss: 0.011044611223042011\n",
            "step: 170, loss: 0.0036645757500082254\n",
            "step: 180, loss: 0.0006603632355108857\n",
            "step: 190, loss: 0.09698843210935593\n",
            "step: 200, loss: 0.0003144099027849734\n",
            "step: 210, loss: 0.001642611576244235\n",
            "step: 220, loss: 0.0010985886910930276\n",
            "step: 230, loss: 0.00027295740437693894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9798657718120806, f1=0.9753914988814317, best_f1=0.9753914988814317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023281658068299294\n",
            "step: 10, loss: 0.000890693801920861\n",
            "step: 20, loss: 0.0033239468466490507\n",
            "step: 30, loss: 0.00010917381587205455\n",
            "step: 40, loss: 0.0006090556853450835\n",
            "step: 50, loss: 0.00021360802929848433\n",
            "step: 60, loss: 0.00020411642617546022\n",
            "step: 70, loss: 0.0007053544977679849\n",
            "step: 80, loss: 0.0052948566153645515\n",
            "step: 90, loss: 0.002000825246796012\n",
            "step: 100, loss: 0.03423747792840004\n",
            "step: 110, loss: 0.0006507838843390346\n",
            "step: 120, loss: 0.002986431121826172\n",
            "step: 130, loss: 0.0056368205696344376\n",
            "step: 140, loss: 0.0003512481926009059\n",
            "step: 150, loss: 0.22213885188102722\n",
            "step: 160, loss: 0.019485656172037125\n",
            "step: 170, loss: 0.0007494613528251648\n",
            "step: 180, loss: 0.037206877022981644\n",
            "step: 190, loss: 0.004200577735900879\n",
            "step: 200, loss: 0.000856558617670089\n",
            "step: 210, loss: 0.0013216867810115218\n",
            "step: 220, loss: 0.006780377589166164\n",
            "step: 230, loss: 0.09361729770898819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9700996677740864, f1=0.9720044792833147, best_f1=0.9753914988814317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010997564531862736\n",
            "step: 10, loss: 0.0009401583229191601\n",
            "step: 20, loss: 0.001669325865805149\n",
            "step: 30, loss: 0.002643708372488618\n",
            "step: 40, loss: 0.002806133823469281\n",
            "step: 50, loss: 0.001995963742956519\n",
            "step: 60, loss: 0.00023166494793258607\n",
            "step: 70, loss: 0.01763947494328022\n",
            "step: 80, loss: 0.0055902814492583275\n",
            "step: 90, loss: 0.00018868160259444267\n",
            "step: 100, loss: 0.0007501806830987334\n",
            "step: 110, loss: 0.00036276219179853797\n",
            "step: 120, loss: 0.0009431431535631418\n",
            "step: 130, loss: 0.0200492013245821\n",
            "step: 140, loss: 0.00032895541517063975\n",
            "step: 150, loss: 0.007622320204973221\n",
            "step: 160, loss: 0.060898054391145706\n",
            "step: 170, loss: 0.002153279259800911\n",
            "step: 180, loss: 0.0006986769731156528\n",
            "step: 190, loss: 0.00029442779486998916\n",
            "step: 200, loss: 0.05338705703616142\n",
            "step: 210, loss: 0.00020963030692655593\n",
            "step: 220, loss: 0.0023365411907434464\n",
            "step: 230, loss: 0.007831570692360401\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9819819819819819, f1=0.9718785151856018, best_f1=0.9718785151856018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004193398344796151\n",
            "step: 10, loss: 0.00025966137764044106\n",
            "step: 20, loss: 0.0012117901351302862\n",
            "step: 30, loss: 0.00033176448778249323\n",
            "step: 40, loss: 0.006811528000980616\n",
            "step: 50, loss: 0.0035609796177595854\n",
            "step: 60, loss: 0.00013345239858608693\n",
            "step: 70, loss: 0.00017868535360321403\n",
            "step: 80, loss: 0.00015544844791293144\n",
            "step: 90, loss: 0.0001099434302886948\n",
            "step: 100, loss: 0.0002762439544312656\n",
            "step: 110, loss: 0.013667821884155273\n",
            "step: 120, loss: 0.00029984707362018526\n",
            "step: 130, loss: 0.0002536377578508109\n",
            "step: 140, loss: 6.970459071453661e-05\n",
            "step: 150, loss: 0.00029449930298142135\n",
            "step: 160, loss: 0.00028067186940461397\n",
            "step: 170, loss: 0.0003326224396005273\n",
            "step: 180, loss: 0.0004560007946565747\n",
            "step: 190, loss: 0.0003020634758286178\n",
            "step: 200, loss: 0.0004379774909466505\n",
            "step: 210, loss: 0.0011177943088114262\n",
            "step: 220, loss: 0.00012392719509080052\n",
            "step: 230, loss: 0.0003272832836955786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9799107142857142, f1=0.9710467706013363, best_f1=0.9718785151856018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.2986877815565094e-05\n",
            "step: 10, loss: 0.000912858871743083\n",
            "step: 20, loss: 0.0023990122135728598\n",
            "step: 30, loss: 8.318775508087128e-05\n",
            "step: 40, loss: 0.011642088182270527\n",
            "step: 50, loss: 4.95616186526604e-05\n",
            "step: 60, loss: 0.00011786506365751848\n",
            "step: 70, loss: 0.00024410116020590067\n",
            "step: 80, loss: 9.167271491605788e-05\n",
            "step: 90, loss: 0.00011274839926045388\n",
            "step: 100, loss: 0.0006965339416638017\n",
            "step: 110, loss: 3.833464870695025e-05\n",
            "step: 120, loss: 4.1561037505744025e-05\n",
            "step: 130, loss: 0.0013020883779972792\n",
            "step: 140, loss: 4.627997986972332e-05\n",
            "step: 150, loss: 0.0001229079207405448\n",
            "step: 160, loss: 0.00032445212127640843\n",
            "step: 170, loss: 8.313944999827072e-05\n",
            "step: 180, loss: 0.007619715295732021\n",
            "step: 190, loss: 9.758550731930882e-05\n",
            "step: 200, loss: 9.196051541948691e-05\n",
            "step: 210, loss: 0.00010489217675058171\n",
            "step: 220, loss: 7.222348358482122e-05\n",
            "step: 230, loss: 0.00023090248578228056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.980963045912654, f1=0.968609865470852, best_f1=0.9718785151856018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002449035528115928\n",
            "step: 10, loss: 0.05585525184869766\n",
            "step: 20, loss: 0.085670106112957\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 30, loss: 0.012515585869550705\n",
            "step: 40, loss: 0.00039065987220965326\n",
            "step: 50, loss: 0.0009422289440408349\n",
            "step: 60, loss: 0.0012978101149201393\n",
            "step: 70, loss: 0.0004530657606665045\n",
            "step: 80, loss: 0.00020306259102653712\n",
            "step: 90, loss: 0.00172189821023494\n",
            "step: 100, loss: 0.0001300701405853033\n",
            "step: 110, loss: 0.00025678728707134724\n",
            "step: 120, loss: 8.373724267585203e-05\n",
            "step: 130, loss: 9.662059164838865e-05\n",
            "step: 140, loss: 0.032384295016527176\n",
            "step: 150, loss: 0.017053600400686264\n",
            "step: 160, loss: 9.638887422624975e-05\n",
            "step: 170, loss: 0.00011405273107811809\n",
            "step: 180, loss: 0.0010326550109311938\n",
            "step: 190, loss: 0.0035033582244068384\n",
            "step: 200, loss: 5.582606900134124e-05\n",
            "step: 210, loss: 0.000733471882995218\n",
            "step: 220, loss: 6.501511234091595e-05\n",
            "step: 230, loss: 0.0004392405680846423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9820224719101124, f1=0.9774266365688488, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.710442539770156e-05\n",
            "step: 10, loss: 0.00010335882689105347\n",
            "step: 20, loss: 0.00020921658142469823\n",
            "step: 30, loss: 0.0014432409079745412\n",
            "step: 40, loss: 9.91829001577571e-05\n",
            "step: 50, loss: 0.00024843381834216416\n",
            "step: 60, loss: 0.000856187951285392\n",
            "step: 70, loss: 0.00017394873430021107\n",
            "step: 80, loss: 4.728895873995498e-05\n",
            "step: 90, loss: 6.311019387794659e-05\n",
            "step: 100, loss: 6.189555278979242e-05\n",
            "step: 110, loss: 0.00020288307860028\n",
            "step: 120, loss: 4.9591446440899745e-05\n",
            "step: 130, loss: 3.548484164639376e-05\n",
            "step: 140, loss: 6.027493145666085e-05\n",
            "step: 150, loss: 0.0009968304075300694\n",
            "step: 160, loss: 5.1803737733280286e-05\n",
            "step: 170, loss: 8.135528332786635e-05\n",
            "step: 180, loss: 0.000173702763277106\n",
            "step: 190, loss: 5.61961714993231e-05\n",
            "step: 200, loss: 0.00028400789597071707\n",
            "step: 210, loss: 9.431868966203183e-05\n",
            "step: 220, loss: 8.000041270861402e-05\n",
            "step: 230, loss: 0.0014716187724843621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9763779527559054, f1=0.9685393258426966, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.561485497513786e-05\n",
            "step: 10, loss: 5.3364648920251057e-05\n",
            "step: 20, loss: 9.353779751108959e-05\n",
            "step: 30, loss: 0.0001190494149341248\n",
            "step: 40, loss: 0.00033301967778243124\n",
            "step: 50, loss: 0.0013871947303414345\n",
            "step: 60, loss: 0.0006152118439786136\n",
            "step: 70, loss: 7.034646114334464e-05\n",
            "step: 80, loss: 0.00020031616440974176\n",
            "step: 90, loss: 5.2619139751186594e-05\n",
            "step: 100, loss: 0.0004880890191998333\n",
            "step: 110, loss: 0.0002807346754707396\n",
            "step: 120, loss: 5.970513302600011e-05\n",
            "step: 130, loss: 5.640030576614663e-05\n",
            "step: 140, loss: 0.00011293105490040034\n",
            "step: 150, loss: 0.00015013596566859633\n",
            "step: 160, loss: 5.908196544623934e-05\n",
            "step: 170, loss: 7.243028085213155e-05\n",
            "step: 180, loss: 0.005496774334460497\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.02734309248626232\n",
            "step: 200, loss: 4.094756513950415e-05\n",
            "step: 210, loss: 5.9830908867297694e-05\n",
            "step: 220, loss: 0.00014583885786123574\n",
            "step: 230, loss: 0.006487508304417133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9787709497206705, f1=0.96875, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.479346095351502e-05\n",
            "step: 10, loss: 7.391707913484424e-05\n",
            "step: 20, loss: 0.00023488550505135208\n",
            "step: 30, loss: 6.527345249196514e-05\n",
            "step: 40, loss: 6.080164894228801e-05\n",
            "step: 50, loss: 5.9858302847715095e-05\n",
            "step: 60, loss: 4.8172259994316846e-05\n",
            "step: 70, loss: 3.0784740374656394e-05\n",
            "step: 80, loss: 4.771642852574587e-05\n",
            "step: 90, loss: 0.0005872056935913861\n",
            "step: 100, loss: 3.336993540870026e-05\n",
            "step: 110, loss: 8.669902308611199e-05\n",
            "step: 120, loss: 0.0011662017786875367\n",
            "step: 130, loss: 5.668696030625142e-05\n",
            "step: 140, loss: 5.71615673834458e-05\n",
            "step: 150, loss: 8.604550384916365e-05\n",
            "step: 160, loss: 0.0002100130368489772\n",
            "step: 170, loss: 0.00039263913640752435\n",
            "step: 180, loss: 4.31853550253436e-05\n",
            "step: 190, loss: 5.835989577462897e-05\n",
            "step: 200, loss: 7.238052785396576e-05\n",
            "step: 210, loss: 2.431424945825711e-05\n",
            "step: 220, loss: 4.868466567131691e-05\n",
            "step: 230, loss: 6.1016948166070506e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.977728285077951, f1=0.9698996655518396, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.488747774506919e-05\n",
            "step: 10, loss: 5.10647842020262e-05\n",
            "step: 20, loss: 3.8439353374997154e-05\n",
            "step: 30, loss: 5.974433588562533e-05\n",
            "step: 40, loss: 0.014307687059044838\n",
            "step: 50, loss: 5.3744963224744424e-05\n",
            "step: 60, loss: 4.833558341488242e-05\n",
            "step: 70, loss: 5.3662326536141336e-05\n",
            "step: 80, loss: 5.420409070211463e-05\n",
            "step: 90, loss: 0.0009028479689732194\n",
            "step: 100, loss: 4.5553362724604085e-05\n",
            "step: 110, loss: 5.358768248697743e-05\n",
            "step: 120, loss: 3.0273571610450745e-05\n",
            "step: 130, loss: 4.140469536650926e-05\n",
            "step: 140, loss: 4.4578162487596273e-05\n",
            "step: 150, loss: 3.555994408088736e-05\n",
            "step: 160, loss: 0.01109416875988245\n",
            "step: 170, loss: 3.980353721999563e-05\n",
            "step: 180, loss: 3.8297614082694054e-05\n",
            "step: 190, loss: 0.00010308334458386526\n",
            "step: 200, loss: 3.2799776818137616e-05\n",
            "step: 210, loss: 0.00013119280629325658\n",
            "step: 220, loss: 3.902861863025464e-05\n",
            "step: 230, loss: 0.001969914184883237\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9799107142857142, f1=0.9665178571428571, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.156655657221563e-05\n",
            "step: 10, loss: 2.0995266822865233e-05\n",
            "step: 20, loss: 3.3466734748799354e-05\n",
            "step: 30, loss: 3.526595173752867e-05\n",
            "step: 40, loss: 0.0003110423276666552\n",
            "step: 50, loss: 0.03520059213042259\n",
            "step: 60, loss: 4.955410258844495e-05\n",
            "step: 70, loss: 3.478911821730435e-05\n",
            "step: 80, loss: 0.0002521382411941886\n",
            "step: 90, loss: 6.2996317865327e-05\n",
            "step: 100, loss: 4.366923894849606e-05\n",
            "step: 110, loss: 3.701599416672252e-05\n",
            "step: 120, loss: 5.376990884542465e-05\n",
            "step: 130, loss: 6.336024671327323e-05\n",
            "step: 140, loss: 3.3448057365603745e-05\n",
            "step: 150, loss: 6.808878242736682e-05\n",
            "step: 160, loss: 2.925343687820714e-05\n",
            "step: 170, loss: 2.7256901375949383e-05\n",
            "step: 180, loss: 4.8743211664259434e-05\n",
            "step: 190, loss: 0.0003428220807109028\n",
            "step: 200, loss: 5.140840221429244e-05\n",
            "step: 210, loss: 7.39823590265587e-05\n",
            "step: 220, loss: 0.0002078630350297317\n",
            "step: 230, loss: 7.048348925309256e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9788182831661093, f1=0.9675977653631285, best_f1=0.9774266365688488\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 166.45it/s]\n",
            "load_f1 = 0.9810055865921787\n",
            "real_f1 = 0.9821826280623607\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a98956-4aa3-40aa-9e44-1410e0eb4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6272004246711731\n",
            "step: 10, loss: 0.5235728025436401\n",
            "step: 20, loss: 0.5073021054267883\n",
            "step: 30, loss: 0.10616632550954819\n",
            "step: 40, loss: 0.21608470380306244\n",
            "step: 50, loss: 0.1492369920015335\n",
            "step: 60, loss: 0.07054004818201065\n",
            "step: 70, loss: 0.08430299907922745\n",
            "step: 80, loss: 0.02450070157647133\n",
            "step: 90, loss: 0.2002764642238617\n",
            "step: 100, loss: 0.028222206979990005\n",
            "step: 110, loss: 0.04467798396945\n",
            "step: 120, loss: 0.17538297176361084\n",
            "step: 130, loss: 0.06589365005493164\n",
            "step: 140, loss: 0.07256913930177689\n",
            "step: 150, loss: 0.05100129544734955\n",
            "step: 160, loss: 0.060991380363702774\n",
            "step: 170, loss: 0.27634960412979126\n",
            "step: 180, loss: 0.03698642551898956\n",
            "step: 190, loss: 0.016247019171714783\n",
            "step: 200, loss: 0.2553931176662445\n",
            "step: 210, loss: 0.05721835047006607\n",
            "step: 220, loss: 0.19579879939556122\n",
            "step: 230, loss: 0.15862980484962463\n",
            "step: 240, loss: 0.06677629053592682\n",
            "step: 250, loss: 0.011067197658121586\n",
            "step: 260, loss: 0.022216342389583588\n",
            "step: 270, loss: 0.011600811965763569\n",
            "step: 280, loss: 0.06013108417391777\n",
            "step: 290, loss: 0.09856702387332916\n",
            "step: 300, loss: 0.016628216952085495\n",
            "step: 310, loss: 0.3863380551338196\n",
            "step: 320, loss: 0.12281493842601776\n",
            "step: 330, loss: 0.04300514981150627\n",
            "step: 340, loss: 0.0727325826883316\n",
            "step: 350, loss: 0.028976909816265106\n",
            "step: 360, loss: 0.019710782915353775\n",
            "step: 370, loss: 0.1554291844367981\n",
            "step: 380, loss: 0.050133660435676575\n",
            "step: 390, loss: 0.12444085627794266\n",
            "step: 400, loss: 0.19818562269210815\n",
            "step: 410, loss: 0.06177016720175743\n",
            "step: 420, loss: 0.037731193006038666\n",
            "step: 430, loss: 0.20412544906139374\n",
            "step: 440, loss: 0.09866957366466522\n",
            "step: 450, loss: 0.01208152249455452\n",
            "step: 460, loss: 0.005931690335273743\n",
            "step: 470, loss: 0.1063336506485939\n",
            "step: 480, loss: 0.0716637447476387\n",
            "step: 490, loss: 0.10688390582799911\n",
            "step: 500, loss: 0.12221809476613998\n",
            "step: 510, loss: 0.06144881993532181\n",
            "step: 520, loss: 0.0702042207121849\n",
            "step: 530, loss: 0.009836861863732338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9405255878284925, f1=0.9377593360995851, best_f1=0.9377593360995851\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14288875460624695\n",
            "step: 10, loss: 0.06131424754858017\n",
            "step: 20, loss: 0.008292056620121002\n",
            "step: 30, loss: 0.058203067630529404\n",
            "step: 40, loss: 0.08619215339422226\n",
            "step: 50, loss: 0.19534224271774292\n",
            "step: 60, loss: 0.005707398988306522\n",
            "step: 70, loss: 0.023439066484570503\n",
            "step: 80, loss: 0.06768332421779633\n",
            "step: 90, loss: 0.009037629701197147\n",
            "step: 100, loss: 0.013542176224291325\n",
            "step: 110, loss: 0.07860809564590454\n",
            "step: 120, loss: 0.04427005723118782\n",
            "step: 130, loss: 0.0987568348646164\n",
            "step: 140, loss: 0.041056860238313675\n",
            "step: 150, loss: 0.05708111450076103\n",
            "step: 160, loss: 0.026686975732445717\n",
            "step: 170, loss: 0.007395267952233553\n",
            "step: 180, loss: 0.04555478319525719\n",
            "step: 190, loss: 0.057669468224048615\n",
            "step: 200, loss: 0.02169552631676197\n",
            "step: 210, loss: 0.023508861660957336\n",
            "step: 220, loss: 0.049335725605487823\n",
            "step: 230, loss: 0.005089908372610807\n",
            "step: 240, loss: 0.03694797307252884\n",
            "step: 250, loss: 0.023923128843307495\n",
            "step: 260, loss: 0.0044083306565880775\n",
            "step: 270, loss: 0.1293390691280365\n",
            "step: 280, loss: 0.033159613609313965\n",
            "step: 290, loss: 0.014812936075031757\n",
            "step: 300, loss: 0.16617673635482788\n",
            "step: 310, loss: 0.016358425840735435\n",
            "step: 320, loss: 0.053163930773735046\n",
            "step: 330, loss: 0.042985063046216965\n",
            "step: 340, loss: 0.01803339645266533\n",
            "step: 350, loss: 0.004884444177150726\n",
            "step: 360, loss: 0.01846652664244175\n",
            "step: 370, loss: 0.16563540697097778\n",
            "step: 380, loss: 0.07807080447673798\n",
            "step: 390, loss: 0.054316818714141846\n",
            "step: 400, loss: 0.04873327538371086\n",
            "step: 410, loss: 0.004623436369001865\n",
            "step: 420, loss: 0.017648285254836082\n",
            "step: 430, loss: 0.007643613498657942\n",
            "step: 440, loss: 0.20685136318206787\n",
            "step: 450, loss: 0.032134462147951126\n",
            "step: 460, loss: 0.02397949807345867\n",
            "step: 470, loss: 0.12230385839939117\n",
            "step: 480, loss: 0.19799016416072845\n",
            "step: 490, loss: 0.01499460730701685\n",
            "step: 500, loss: 0.10351144522428513\n",
            "step: 510, loss: 0.010308869183063507\n",
            "step: 520, loss: 0.05392074212431908\n",
            "step: 530, loss: 0.005216307006776333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9431345353675451, f1=0.9425287356321841, best_f1=0.9425287356321841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11098097264766693\n",
            "step: 10, loss: 0.07666566222906113\n",
            "step: 20, loss: 0.043073903769254684\n",
            "step: 30, loss: 0.089785635471344\n",
            "step: 40, loss: 0.11462970823049545\n",
            "step: 50, loss: 0.08958758413791656\n",
            "step: 60, loss: 0.014724367298185825\n",
            "step: 70, loss: 0.0014668825315311551\n",
            "step: 80, loss: 0.0051310257986187935\n",
            "step: 90, loss: 0.006068656221032143\n",
            "step: 100, loss: 0.04995546489953995\n",
            "step: 110, loss: 0.0013560012448579073\n",
            "step: 120, loss: 0.014665033668279648\n",
            "step: 130, loss: 0.0026652689557522535\n",
            "step: 140, loss: 0.015101131983101368\n",
            "step: 150, loss: 0.010269733145833015\n",
            "step: 160, loss: 0.0058285389095544815\n",
            "step: 170, loss: 0.0763477310538292\n",
            "step: 180, loss: 0.007921409793198109\n",
            "step: 190, loss: 0.0028929964173585176\n",
            "step: 200, loss: 0.03562583029270172\n",
            "step: 210, loss: 0.06871092319488525\n",
            "step: 220, loss: 0.01750458963215351\n",
            "step: 230, loss: 0.18137693405151367\n",
            "step: 240, loss: 0.002310781739652157\n",
            "step: 250, loss: 0.018983112648129463\n",
            "step: 260, loss: 0.012777854688465595\n",
            "step: 270, loss: 0.004828695207834244\n",
            "step: 280, loss: 0.09714604914188385\n",
            "step: 290, loss: 0.00921417772769928\n",
            "step: 300, loss: 0.033964600414037704\n",
            "step: 310, loss: 0.004470459651201963\n",
            "step: 320, loss: 0.04122941568493843\n",
            "step: 330, loss: 0.0018927904311567545\n",
            "step: 340, loss: 0.019195031374692917\n",
            "step: 350, loss: 0.0034629369620233774\n",
            "step: 360, loss: 0.02764681726694107\n",
            "step: 370, loss: 0.022948380559682846\n",
            "step: 380, loss: 0.006561726331710815\n",
            "step: 390, loss: 0.021461213007569313\n",
            "step: 400, loss: 0.006984437815845013\n",
            "step: 410, loss: 0.004919407423585653\n",
            "step: 420, loss: 0.04898110032081604\n",
            "step: 430, loss: 0.005915164016187191\n",
            "step: 440, loss: 0.036522697657346725\n",
            "step: 450, loss: 0.09842580556869507\n",
            "step: 460, loss: 0.04361281543970108\n",
            "step: 470, loss: 0.1199057474732399\n",
            "step: 480, loss: 0.026036398485302925\n",
            "step: 490, loss: 0.02168889343738556\n",
            "step: 500, loss: 0.042767394334077835\n",
            "step: 510, loss: 0.003363149706274271\n",
            "step: 520, loss: 0.1067817285656929\n",
            "step: 530, loss: 0.002842986024916172\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9439338235294118, f1=0.9352914180816887, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.033740390092134476\n",
            "step: 10, loss: 0.010532183572649956\n",
            "step: 20, loss: 0.0018326519057154655\n",
            "step: 30, loss: 0.001579827512614429\n",
            "step: 40, loss: 0.0012204106897115707\n",
            "step: 50, loss: 0.0036830792669206858\n",
            "step: 60, loss: 0.0046413978561758995\n",
            "step: 70, loss: 0.02352041006088257\n",
            "step: 80, loss: 0.014415772631764412\n",
            "step: 90, loss: 0.015117165632545948\n",
            "step: 100, loss: 0.006948092486709356\n",
            "step: 110, loss: 0.07793375104665756\n",
            "step: 120, loss: 0.00047046825056895614\n",
            "step: 130, loss: 0.00033412702032364905\n",
            "step: 140, loss: 0.0011833348544314504\n",
            "step: 150, loss: 0.01163561549037695\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.00919048860669136\n",
            "step: 170, loss: 0.05164431780576706\n",
            "step: 180, loss: 0.008004611358046532\n",
            "step: 190, loss: 0.005251975264400244\n",
            "step: 200, loss: 0.030644405633211136\n",
            "step: 210, loss: 0.009546671994030476\n",
            "step: 220, loss: 0.01827685907483101\n",
            "step: 230, loss: 0.0385027751326561\n",
            "step: 240, loss: 0.01522266399115324\n",
            "step: 250, loss: 0.04569340869784355\n",
            "step: 260, loss: 0.0013314869720488787\n",
            "step: 270, loss: 0.006375585217028856\n",
            "step: 280, loss: 0.01774822548031807\n",
            "step: 290, loss: 0.037680529057979584\n",
            "step: 300, loss: 0.00040790141792967916\n",
            "step: 310, loss: 0.0021345065906643867\n",
            "step: 320, loss: 0.09997234493494034\n",
            "step: 330, loss: 0.002084638923406601\n",
            "step: 340, loss: 0.0009889323264360428\n",
            "step: 350, loss: 0.0074003166519105434\n",
            "step: 360, loss: 0.009110280312597752\n",
            "step: 370, loss: 0.0018353033810853958\n",
            "step: 380, loss: 0.003679964691400528\n",
            "step: 390, loss: 0.0013888422399759293\n",
            "step: 400, loss: 0.01073186844587326\n",
            "step: 410, loss: 0.0009988319361582398\n",
            "step: 420, loss: 0.0009056529379449785\n",
            "step: 430, loss: 0.2086169719696045\n",
            "step: 440, loss: 0.0013713842490687966\n",
            "step: 450, loss: 0.0027041276916861534\n",
            "step: 460, loss: 0.0009840400889515877\n",
            "step: 470, loss: 0.014690746553242207\n",
            "step: 480, loss: 0.11242925375699997\n",
            "step: 490, loss: 0.005162014625966549\n",
            "step: 500, loss: 0.0017483358969911933\n",
            "step: 510, loss: 0.02563529834151268\n",
            "step: 520, loss: 0.06061875820159912\n",
            "step: 530, loss: 0.016510339453816414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9418819188191883, f1=0.9310027598896043, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019639020785689354\n",
            "step: 10, loss: 0.0419631190598011\n",
            "step: 20, loss: 0.007081306539475918\n",
            "step: 30, loss: 0.017749568447470665\n",
            "step: 40, loss: 0.004715407267212868\n",
            "step: 50, loss: 0.0008197634015232325\n",
            "step: 60, loss: 0.1334879994392395\n",
            "step: 70, loss: 0.003988267853856087\n",
            "step: 80, loss: 0.0013969797873869538\n",
            "step: 90, loss: 0.01665639691054821\n",
            "step: 100, loss: 0.014468623325228691\n",
            "step: 110, loss: 0.0006038848659954965\n",
            "step: 120, loss: 0.025274403393268585\n",
            "step: 130, loss: 0.0009008751367218792\n",
            "step: 140, loss: 0.004323720466345549\n",
            "step: 150, loss: 0.004058232996612787\n",
            "step: 160, loss: 0.0005972103681415319\n",
            "step: 170, loss: 0.02184934727847576\n",
            "step: 180, loss: 0.001340566435828805\n",
            "step: 190, loss: 0.0015326832653954625\n",
            "step: 200, loss: 0.0005040615797042847\n",
            "step: 210, loss: 0.0010265057208016515\n",
            "step: 220, loss: 0.0012519779847934842\n",
            "step: 230, loss: 0.003255135379731655\n",
            "step: 240, loss: 0.006182704586535692\n",
            "step: 250, loss: 0.0022237251978367567\n",
            "step: 260, loss: 0.07742716372013092\n",
            "step: 270, loss: 0.004707083571702242\n",
            "step: 280, loss: 0.0012679058127105236\n",
            "step: 290, loss: 0.1083468422293663\n",
            "step: 300, loss: 0.001889124047011137\n",
            "step: 310, loss: 0.001265066210180521\n",
            "step: 320, loss: 0.20797783136367798\n",
            "step: 330, loss: 0.007567860186100006\n",
            "step: 340, loss: 0.0028523497749119997\n",
            "step: 350, loss: 0.002007841132581234\n",
            "step: 360, loss: 0.009524885565042496\n",
            "step: 370, loss: 0.0038357821758836508\n",
            "step: 380, loss: 0.002425567479804158\n",
            "step: 390, loss: 0.00030943553429096937\n",
            "step: 400, loss: 0.02158728614449501\n",
            "step: 410, loss: 0.0005181105807423592\n",
            "step: 420, loss: 0.0006942061008885503\n",
            "step: 430, loss: 0.003348582424223423\n",
            "step: 440, loss: 0.01251804269850254\n",
            "step: 450, loss: 0.35068389773368835\n",
            "step: 460, loss: 0.011592944152653217\n",
            "step: 470, loss: 0.01923641934990883\n",
            "step: 480, loss: 0.01267493236809969\n",
            "step: 490, loss: 0.0031214230693876743\n",
            "step: 500, loss: 0.005353225395083427\n",
            "step: 510, loss: 0.05186744034290314\n",
            "step: 520, loss: 0.002062127459794283\n",
            "step: 530, loss: 0.00859035737812519\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9429234338747099, f1=0.9294336118848654, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022042966447770596\n",
            "step: 10, loss: 0.005314744543284178\n",
            "step: 20, loss: 0.11751555651426315\n",
            "step: 30, loss: 0.03947502747178078\n",
            "step: 40, loss: 0.0048834169283509254\n",
            "step: 50, loss: 0.03212522342801094\n",
            "step: 60, loss: 0.00044238465488888323\n",
            "step: 70, loss: 0.0010357487481087446\n",
            "step: 80, loss: 0.033158741891384125\n",
            "step: 90, loss: 0.021143700927495956\n",
            "step: 100, loss: 0.0771137923002243\n",
            "step: 110, loss: 0.0018515175906941295\n",
            "step: 120, loss: 0.0003179635386914015\n",
            "step: 130, loss: 0.0027306987904012203\n",
            "step: 140, loss: 0.0017729797400534153\n",
            "step: 150, loss: 0.0002591146621853113\n",
            "step: 160, loss: 0.001160445623099804\n",
            "step: 170, loss: 0.0008183013997040689\n",
            "step: 180, loss: 0.00022569352586288005\n",
            "step: 190, loss: 0.0006258194916881621\n",
            "step: 200, loss: 0.00014513432688545436\n",
            "step: 210, loss: 0.001925033750012517\n",
            "step: 220, loss: 0.0025275058578699827\n",
            "step: 230, loss: 0.00033298644120804965\n",
            "step: 240, loss: 0.15827083587646484\n",
            "step: 250, loss: 0.0037474865093827248\n",
            "step: 260, loss: 0.00758483586832881\n",
            "step: 270, loss: 0.10261627286672592\n",
            "step: 280, loss: 0.0036953389644622803\n",
            "step: 290, loss: 0.00047219530097208917\n",
            "step: 300, loss: 0.0006791278719902039\n",
            "step: 310, loss: 0.0010142945684492588\n",
            "step: 320, loss: 0.001575126894749701\n",
            "step: 330, loss: 0.002969358116388321\n",
            "step: 340, loss: 0.053577642887830734\n",
            "step: 350, loss: 0.0007964418618939817\n",
            "step: 360, loss: 0.010601851157844067\n",
            "step: 370, loss: 0.0018512862734496593\n",
            "step: 380, loss: 0.000511490274220705\n",
            "step: 390, loss: 0.01776484027504921\n",
            "step: 400, loss: 0.0051491702906787395\n",
            "step: 410, loss: 0.0008601297740824521\n",
            "step: 420, loss: 0.001998472260311246\n",
            "step: 430, loss: 0.002798121189698577\n",
            "step: 440, loss: 0.0008934882353059947\n",
            "step: 450, loss: 0.0008485745056532323\n",
            "step: 460, loss: 0.0008721243357285857\n",
            "step: 470, loss: 0.0072496505454182625\n",
            "step: 480, loss: 0.0023898605722934008\n",
            "step: 490, loss: 0.006004024296998978\n",
            "step: 500, loss: 0.0028429124504327774\n",
            "step: 510, loss: 0.012903902679681778\n",
            "step: 520, loss: 0.0966711938381195\n",
            "step: 530, loss: 0.009109114296734333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9337717238139972, f1=0.9251764705882354, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004848989192396402\n",
            "step: 10, loss: 0.0025244993157684803\n",
            "step: 20, loss: 0.0010460257763043046\n",
            "step: 30, loss: 0.0009797875536605716\n",
            "step: 40, loss: 0.07600057125091553\n",
            "step: 50, loss: 0.011286946013569832\n",
            "step: 60, loss: 0.0007554487092420459\n",
            "step: 70, loss: 0.0005808662390336394\n",
            "step: 80, loss: 0.005165865644812584\n",
            "step: 90, loss: 9.056152339326218e-05\n",
            "step: 100, loss: 0.0001916752225952223\n",
            "step: 110, loss: 0.004827518947422504\n",
            "step: 120, loss: 0.00037873885594308376\n",
            "step: 130, loss: 0.0013969405554234982\n",
            "step: 140, loss: 0.0006865523755550385\n",
            "step: 150, loss: 0.005982842296361923\n",
            "step: 160, loss: 0.000419168034568429\n",
            "step: 170, loss: 0.0032943100668489933\n",
            "step: 180, loss: 0.0024655915331095457\n",
            "step: 190, loss: 0.0025333696976304054\n",
            "step: 200, loss: 0.000658104894682765\n",
            "step: 210, loss: 0.005331553053110838\n",
            "step: 220, loss: 0.0007956284680403769\n",
            "step: 230, loss: 0.04417821019887924\n",
            "step: 240, loss: 0.0013141511008143425\n",
            "step: 250, loss: 0.020853858441114426\n",
            "step: 260, loss: 0.0001737868442432955\n",
            "step: 270, loss: 0.001854151370935142\n",
            "step: 280, loss: 0.0015514246188104153\n",
            "step: 290, loss: 0.0005422562826424837\n",
            "step: 300, loss: 0.0024811476469039917\n",
            "step: 310, loss: 0.0007841947954148054\n",
            "step: 320, loss: 0.00015179529145825654\n",
            "step: 330, loss: 0.0007812652620486915\n",
            "step: 340, loss: 0.09908942878246307\n",
            "step: 350, loss: 0.0024240389466285706\n",
            "step: 360, loss: 0.0012987595982849598\n",
            "step: 370, loss: 0.0009199728956446052\n",
            "step: 380, loss: 0.0002993193920701742\n",
            "step: 390, loss: 0.004757903516292572\n",
            "step: 400, loss: 0.007238794583827257\n",
            "step: 410, loss: 0.002082107588648796\n",
            "step: 420, loss: 0.0006316208746284246\n",
            "step: 430, loss: 0.00011138594709336758\n",
            "step: 440, loss: 0.00037582352524623275\n",
            "step: 450, loss: 0.0010071111610159278\n",
            "step: 460, loss: 0.0011608779896050692\n",
            "step: 470, loss: 0.006261701695621014\n",
            "step: 480, loss: 0.001074352883733809\n",
            "step: 490, loss: 0.0005348783452063799\n",
            "step: 500, loss: 0.0014875531196594238\n",
            "step: 510, loss: 0.0014282800257205963\n",
            "step: 520, loss: 0.013070868328213692\n",
            "step: 530, loss: 0.001356185064651072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9436422915696321, f1=0.9327102803738317, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002564545429777354\n",
            "step: 10, loss: 0.018846753984689713\n",
            "step: 20, loss: 8.835546032059938e-05\n",
            "step: 30, loss: 7.925854879431427e-05\n",
            "step: 40, loss: 0.00013519621279556304\n",
            "step: 50, loss: 0.0007700116257183254\n",
            "step: 60, loss: 0.0017727125668898225\n",
            "step: 70, loss: 0.0014928141608834267\n",
            "step: 80, loss: 0.006341694854199886\n",
            "step: 90, loss: 0.0008242556941695511\n",
            "step: 100, loss: 0.00015310866001527756\n",
            "step: 110, loss: 0.0001792669208953157\n",
            "step: 120, loss: 0.0007706396281719208\n",
            "step: 130, loss: 0.00020649044017773122\n",
            "step: 140, loss: 0.0013928741682320833\n",
            "step: 150, loss: 0.00042227667290717363\n",
            "step: 160, loss: 0.004527080338448286\n",
            "step: 170, loss: 0.12328045070171356\n",
            "step: 180, loss: 0.00013499501801561564\n",
            "step: 190, loss: 0.002457579830661416\n",
            "step: 200, loss: 0.001471907366067171\n",
            "step: 210, loss: 0.00032357353484258056\n",
            "step: 220, loss: 0.0021448128391057253\n",
            "step: 230, loss: 0.0014170485083013773\n",
            "step: 240, loss: 0.00030919871642254293\n",
            "step: 250, loss: 0.00016973938909359276\n",
            "step: 260, loss: 6.286644929787144e-05\n",
            "step: 270, loss: 0.0012946794740855694\n",
            "step: 280, loss: 0.007322737947106361\n",
            "step: 290, loss: 5.273654096527025e-05\n",
            "step: 300, loss: 0.0005341256037354469\n",
            "step: 310, loss: 0.0001607511076144874\n",
            "step: 320, loss: 0.0012367385206744075\n",
            "step: 330, loss: 0.000487250741571188\n",
            "step: 340, loss: 0.00021796207875013351\n",
            "step: 350, loss: 0.000287471164483577\n",
            "step: 360, loss: 0.00018070425721816719\n",
            "step: 370, loss: 4.584066118695773e-05\n",
            "step: 380, loss: 0.0003336460213176906\n",
            "step: 390, loss: 0.1210169792175293\n",
            "step: 400, loss: 0.0006906163180246949\n",
            "step: 410, loss: 9.41435937420465e-05\n",
            "step: 420, loss: 0.0024384858552366495\n",
            "step: 430, loss: 0.030811244621872902\n",
            "step: 440, loss: 0.0009525472414679825\n",
            "step: 450, loss: 0.018312877044081688\n",
            "step: 460, loss: 5.6238794059026986e-05\n",
            "step: 470, loss: 0.00017352370196022093\n",
            "step: 480, loss: 0.00012679815699812025\n",
            "step: 490, loss: 0.004097582306712866\n",
            "step: 500, loss: 5.770352800027467e-05\n",
            "step: 510, loss: 0.00010760006989585236\n",
            "step: 520, loss: 0.002110548783093691\n",
            "step: 530, loss: 0.001251010224223137\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9407925407925408, f1=0.9304713019132057, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013861593324691057\n",
            "step: 10, loss: 6.625542300753295e-05\n",
            "step: 20, loss: 0.014251741580665112\n",
            "step: 30, loss: 0.00014252049732021987\n",
            "step: 40, loss: 8.392035670112818e-05\n",
            "step: 50, loss: 3.6125635233474895e-05\n",
            "step: 60, loss: 8.440203964710236e-05\n",
            "step: 70, loss: 0.000940535101108253\n",
            "step: 80, loss: 0.004113515838980675\n",
            "step: 90, loss: 7.088040729286149e-05\n",
            "step: 100, loss: 9.840691927820444e-05\n",
            "step: 110, loss: 3.3392330806236714e-05\n",
            "step: 120, loss: 0.00031365518225356936\n",
            "step: 130, loss: 9.281449456466362e-05\n",
            "step: 140, loss: 0.00021231210848782212\n",
            "step: 150, loss: 0.0004812789265997708\n",
            "step: 160, loss: 0.0010333404643461108\n",
            "step: 170, loss: 0.006356037221848965\n",
            "step: 180, loss: 0.00010095862671732903\n",
            "step: 190, loss: 7.901651406427845e-05\n",
            "step: 200, loss: 0.00030223626527003944\n",
            "step: 210, loss: 6.974046846153215e-05\n",
            "step: 220, loss: 0.00014513617497868836\n",
            "step: 230, loss: 3.119832763331942e-05\n",
            "step: 240, loss: 9.089826926356182e-05\n",
            "step: 250, loss: 6.941175524843857e-05\n",
            "step: 260, loss: 0.013012897223234177\n",
            "step: 270, loss: 7.326139893848449e-05\n",
            "step: 280, loss: 0.00038217316614463925\n",
            "step: 290, loss: 0.025785524398088455\n",
            "step: 300, loss: 6.000600478728302e-05\n",
            "step: 310, loss: 0.01875491440296173\n",
            "step: 320, loss: 0.022554395720362663\n",
            "step: 330, loss: 0.0014916479121893644\n",
            "step: 340, loss: 4.726499173557386e-05\n",
            "step: 350, loss: 0.003247916465625167\n",
            "step: 360, loss: 2.6545683795120567e-05\n",
            "step: 370, loss: 7.541397644672543e-05\n",
            "step: 380, loss: 9.575439617037773e-05\n",
            "step: 390, loss: 3.7586938560707495e-05\n",
            "step: 400, loss: 0.002654924290254712\n",
            "step: 410, loss: 4.081217048224062e-05\n",
            "step: 420, loss: 6.105812644818798e-05\n",
            "step: 430, loss: 8.823150710668415e-05\n",
            "step: 440, loss: 0.0006943597691133618\n",
            "step: 450, loss: 0.0002344265376450494\n",
            "step: 460, loss: 0.00012481538578867912\n",
            "step: 470, loss: 2.367005799897015e-05\n",
            "step: 480, loss: 6.403630686691031e-05\n",
            "step: 490, loss: 0.0851978287100792\n",
            "step: 500, loss: 0.0019950876012444496\n",
            "step: 510, loss: 0.0004910128191113472\n",
            "step: 520, loss: 4.268127304385416e-05\n",
            "step: 530, loss: 0.002121128374710679\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9415067852129153, f1=0.9288389513108615, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00032221493893302977\n",
            "step: 10, loss: 5.262840204522945e-05\n",
            "step: 20, loss: 0.00010356791608501226\n",
            "step: 30, loss: 5.479238461703062e-05\n",
            "step: 40, loss: 0.0018577948212623596\n",
            "step: 50, loss: 2.7901602152269334e-05\n",
            "step: 60, loss: 3.7571589928120375e-05\n",
            "step: 70, loss: 9.830026829149574e-05\n",
            "step: 80, loss: 3.307903534732759e-05\n",
            "step: 90, loss: 0.0009805632289499044\n",
            "step: 100, loss: 2.3025637347018346e-05\n",
            "step: 110, loss: 2.2150092263473198e-05\n",
            "step: 120, loss: 0.000659878016449511\n",
            "step: 130, loss: 4.459768388187513e-05\n",
            "step: 140, loss: 0.004928731359541416\n",
            "step: 150, loss: 2.87691545963753e-05\n",
            "step: 160, loss: 2.2414451450458728e-05\n",
            "step: 170, loss: 0.00028872984694316983\n",
            "step: 180, loss: 0.0001674913364695385\n",
            "step: 190, loss: 0.00241421558894217\n",
            "step: 200, loss: 0.00017656019190326333\n",
            "step: 210, loss: 0.0030649900436401367\n",
            "step: 220, loss: 0.00039186934009194374\n",
            "step: 230, loss: 0.0004703010490629822\n",
            "step: 240, loss: 9.56835356191732e-05\n",
            "step: 250, loss: 6.925882189534605e-05\n",
            "step: 260, loss: 0.0004418156750034541\n",
            "step: 270, loss: 8.594502287451178e-05\n",
            "step: 280, loss: 0.0001311350060859695\n",
            "step: 290, loss: 0.00035311165265738964\n",
            "step: 300, loss: 4.051040014019236e-05\n",
            "step: 310, loss: 0.00017587428737897426\n",
            "step: 320, loss: 0.001345415716059506\n",
            "step: 330, loss: 0.0001090957157430239\n",
            "step: 340, loss: 6.347798625938594e-05\n",
            "step: 350, loss: 0.00041243768646381795\n",
            "step: 360, loss: 9.8641132353805e-05\n",
            "step: 370, loss: 0.00488103786483407\n",
            "step: 380, loss: 0.0028964588418602943\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 390, loss: 0.022983413189649582\n",
            "step: 400, loss: 0.00023434274771716446\n",
            "step: 410, loss: 0.0029637408442795277\n",
            "step: 420, loss: 9.168749238597229e-05\n",
            "step: 430, loss: 0.00018935193656943738\n",
            "step: 440, loss: 0.002612876007333398\n",
            "step: 450, loss: 0.07264342159032822\n",
            "step: 460, loss: 0.0001127942741732113\n",
            "step: 470, loss: 0.00044436260941438377\n",
            "step: 480, loss: 0.0001321997115155682\n",
            "step: 490, loss: 0.01568804867565632\n",
            "step: 500, loss: 0.0227727759629488\n",
            "step: 510, loss: 0.0003702256944961846\n",
            "step: 520, loss: 0.001465339446440339\n",
            "step: 530, loss: 0.0015578370075672865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9396471680594244, f1=0.9327808471454879, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026948435697704554\n",
            "step: 10, loss: 0.002280389191582799\n",
            "step: 20, loss: 0.00015770200116094202\n",
            "step: 30, loss: 7.079356873873621e-05\n",
            "step: 40, loss: 0.0003314973146189004\n",
            "step: 50, loss: 0.011087297461926937\n",
            "step: 60, loss: 0.010978983715176582\n",
            "step: 70, loss: 5.544964369619265e-05\n",
            "step: 80, loss: 0.00013492839934770018\n",
            "step: 90, loss: 0.0038221480790525675\n",
            "step: 100, loss: 0.001370733487419784\n",
            "step: 110, loss: 0.0006006488692946732\n",
            "step: 120, loss: 7.2822644142434e-05\n",
            "step: 130, loss: 0.0006634301389567554\n",
            "step: 140, loss: 9.347878949483857e-05\n",
            "step: 150, loss: 2.5942043066606857e-05\n",
            "step: 160, loss: 3.4169839636888355e-05\n",
            "step: 170, loss: 0.00012138004240114242\n",
            "step: 180, loss: 0.0003504337801132351\n",
            "step: 190, loss: 0.0016508070984855294\n",
            "step: 200, loss: 0.0033031560014933348\n",
            "step: 210, loss: 0.031755756586790085\n",
            "step: 220, loss: 0.0029163160361349583\n",
            "step: 230, loss: 3.0138888178044e-05\n",
            "step: 240, loss: 0.00018724090477917343\n",
            "step: 250, loss: 2.2347552658175118e-05\n",
            "step: 260, loss: 0.0006202272488735616\n",
            "step: 270, loss: 0.00010863073111977428\n",
            "step: 280, loss: 0.001996374223381281\n",
            "step: 290, loss: 0.0002615425328258425\n",
            "step: 300, loss: 0.00010597814252832904\n",
            "step: 310, loss: 0.00012911845988128334\n",
            "step: 320, loss: 4.8217367293545976e-05\n",
            "step: 330, loss: 0.0001250309287570417\n",
            "step: 340, loss: 0.00015654378512408584\n",
            "step: 350, loss: 9.481603774474934e-05\n",
            "step: 360, loss: 0.0010961823863908648\n",
            "step: 370, loss: 0.0005616115522570908\n",
            "step: 380, loss: 5.23304843227379e-05\n",
            "step: 390, loss: 4.084609463461675e-05\n",
            "step: 400, loss: 4.684424857259728e-05\n",
            "step: 410, loss: 0.00027635821606963873\n",
            "step: 420, loss: 0.006102574989199638\n",
            "step: 430, loss: 8.540256385458633e-05\n",
            "step: 440, loss: 0.00016888520622160286\n",
            "step: 450, loss: 0.00048502441495656967\n",
            "step: 460, loss: 0.009083203040063381\n",
            "step: 470, loss: 0.0003103544295299798\n",
            "step: 480, loss: 0.0012636955361813307\n",
            "step: 490, loss: 0.0014311660779640079\n",
            "step: 500, loss: 0.0003022932796739042\n",
            "step: 510, loss: 5.759291525464505e-05\n",
            "step: 520, loss: 9.10982271307148e-05\n",
            "step: 530, loss: 0.0009946102509275079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9432723863103609, f1=0.9316360207449317, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.7746805648785084e-05\n",
            "step: 10, loss: 2.827019852702506e-05\n",
            "step: 20, loss: 0.00023561855778098106\n",
            "step: 30, loss: 0.0003527898225001991\n",
            "step: 40, loss: 0.0013562690000981092\n",
            "step: 50, loss: 0.10336959362030029\n",
            "step: 60, loss: 0.0014103177236393094\n",
            "step: 70, loss: 0.00016773765673860908\n",
            "step: 80, loss: 6.282466347329319e-05\n",
            "step: 90, loss: 0.0001398472231812775\n",
            "step: 100, loss: 4.7430352424271405e-05\n",
            "step: 110, loss: 0.00015282671665772796\n",
            "step: 120, loss: 2.0268833395675756e-05\n",
            "step: 130, loss: 0.0017410427099093795\n",
            "step: 140, loss: 0.00016864579811226577\n",
            "step: 150, loss: 4.030410127597861e-05\n",
            "step: 160, loss: 3.791056224144995e-05\n",
            "step: 170, loss: 9.162104834103957e-05\n",
            "step: 180, loss: 6.773199856979772e-05\n",
            "step: 190, loss: 6.793341162847355e-05\n",
            "step: 200, loss: 4.649524998967536e-05\n",
            "step: 210, loss: 8.008073200471699e-05\n",
            "step: 220, loss: 0.00010946614929707721\n",
            "step: 230, loss: 5.689289537258446e-05\n",
            "step: 240, loss: 0.000514787738211453\n",
            "step: 250, loss: 3.068306250497699e-05\n",
            "step: 260, loss: 5.726050221710466e-05\n",
            "step: 270, loss: 4.116209311177954e-05\n",
            "step: 280, loss: 0.0015138343442231417\n",
            "step: 290, loss: 0.00020065391436219215\n",
            "step: 300, loss: 0.00017940135148819536\n",
            "step: 310, loss: 4.554582483251579e-05\n",
            "step: 320, loss: 2.3152026187744923e-05\n",
            "step: 330, loss: 5.184067049412988e-05\n",
            "step: 340, loss: 2.8974225642741658e-05\n",
            "step: 350, loss: 0.00011766857642214745\n",
            "step: 360, loss: 7.684656156925485e-05\n",
            "step: 370, loss: 6.869983189972118e-05\n",
            "step: 380, loss: 0.0004006443778052926\n",
            "step: 390, loss: 6.742229743395001e-05\n",
            "step: 400, loss: 0.00011187895142938942\n",
            "step: 410, loss: 0.001212661387398839\n",
            "step: 420, loss: 0.005529988091439009\n",
            "step: 430, loss: 0.01370376069098711\n",
            "step: 440, loss: 1.8819901015376672e-05\n",
            "step: 450, loss: 0.00042594512342475355\n",
            "step: 460, loss: 2.264150680275634e-05\n",
            "step: 470, loss: 3.22773921652697e-05\n",
            "step: 480, loss: 0.008234171196818352\n",
            "step: 490, loss: 0.002870891708880663\n",
            "step: 500, loss: 8.609989890828729e-05\n",
            "step: 510, loss: 0.0003095369029324502\n",
            "step: 520, loss: 5.061526098870672e-05\n",
            "step: 530, loss: 2.970814603031613e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9425393883225208, f1=0.9363086936308694, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004718996584415436\n",
            "step: 10, loss: 2.0600469724740833e-05\n",
            "step: 20, loss: 2.004092493734788e-05\n",
            "step: 30, loss: 5.5333068303298205e-05\n",
            "step: 40, loss: 2.1636058590956964e-05\n",
            "step: 50, loss: 0.0004411092959344387\n",
            "step: 60, loss: 2.4258342818939127e-05\n",
            "step: 70, loss: 1.9806861018878408e-05\n",
            "step: 80, loss: 4.252983126207255e-05\n",
            "step: 90, loss: 0.015363655984401703\n",
            "step: 100, loss: 0.00192410743329674\n",
            "step: 110, loss: 5.475041689351201e-05\n",
            "step: 120, loss: 2.7245663659414276e-05\n",
            "step: 130, loss: 3.6243909562472254e-05\n",
            "step: 140, loss: 3.812789873336442e-05\n",
            "step: 150, loss: 0.0001527641579741612\n",
            "step: 160, loss: 2.077175122394692e-05\n",
            "step: 170, loss: 5.228129157330841e-05\n",
            "step: 180, loss: 7.374207052635029e-05\n",
            "step: 190, loss: 1.8592618289403617e-05\n",
            "step: 200, loss: 2.9312290280358866e-05\n",
            "step: 210, loss: 5.901730401092209e-05\n",
            "step: 220, loss: 2.501413291611243e-05\n",
            "step: 230, loss: 1.9494089428917505e-05\n",
            "step: 240, loss: 0.0001279749267268926\n",
            "step: 250, loss: 1.7925816791830584e-05\n",
            "step: 260, loss: 2.5915647711372003e-05\n",
            "step: 270, loss: 1.563483601785265e-05\n",
            "step: 280, loss: 0.0006416037795133889\n",
            "step: 290, loss: 3.272401590947993e-05\n",
            "step: 300, loss: 0.00017067475710064173\n",
            "step: 310, loss: 0.0003993926220573485\n",
            "step: 320, loss: 0.0005518191028386354\n",
            "step: 330, loss: 0.001543352147564292\n",
            "step: 340, loss: 0.0001006650272756815\n",
            "step: 350, loss: 2.572618905105628e-05\n",
            "step: 360, loss: 2.4459368432871997e-05\n",
            "step: 370, loss: 2.9518208975787275e-05\n",
            "step: 380, loss: 3.7503272324102e-05\n",
            "step: 390, loss: 0.0022898055613040924\n",
            "step: 400, loss: 4.9019734433386475e-05\n",
            "step: 410, loss: 0.0004343913751654327\n",
            "step: 420, loss: 0.00012882117880508304\n",
            "step: 430, loss: 2.3032802346278913e-05\n",
            "step: 440, loss: 4.364532287581824e-05\n",
            "step: 450, loss: 0.00037938301102258265\n",
            "step: 460, loss: 0.001111416146159172\n",
            "step: 470, loss: 1.4800353710597847e-05\n",
            "step: 480, loss: 2.963112638099119e-05\n",
            "step: 490, loss: 2.216134816990234e-05\n",
            "step: 500, loss: 1.7638880308368243e-05\n",
            "step: 510, loss: 3.482849933789112e-05\n",
            "step: 520, loss: 0.0003348281024955213\n",
            "step: 530, loss: 1.642085589992348e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9407337723424272, f1=0.926944971537002, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003245527623221278\n",
            "step: 10, loss: 1.3459350157063454e-05\n",
            "step: 20, loss: 1.813065682654269e-05\n",
            "step: 30, loss: 2.2626742065767758e-05\n",
            "step: 40, loss: 8.19938286440447e-05\n",
            "step: 50, loss: 3.270694287493825e-05\n",
            "step: 60, loss: 1.3708910955756437e-05\n",
            "step: 70, loss: 2.033194868999999e-05\n",
            "step: 80, loss: 1.8879358322010376e-05\n",
            "step: 90, loss: 1.8682052541407757e-05\n",
            "step: 100, loss: 9.438523557037115e-05\n",
            "step: 110, loss: 3.261708843638189e-05\n",
            "step: 120, loss: 0.00022101987269707024\n",
            "step: 130, loss: 0.03098961152136326\n",
            "step: 140, loss: 1.676686770224478e-05\n",
            "step: 150, loss: 4.372306648292579e-05\n",
            "step: 160, loss: 0.00012707366840913892\n",
            "step: 170, loss: 0.0008588081691414118\n",
            "step: 180, loss: 3.328493403387256e-05\n",
            "step: 190, loss: 2.1255698811728507e-05\n",
            "step: 200, loss: 7.798159640515223e-05\n",
            "step: 210, loss: 1.7717236914904788e-05\n",
            "step: 220, loss: 1.7624019164941274e-05\n",
            "step: 230, loss: 2.7063346351496875e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 240, loss: 1.5754036212456413e-05\n",
            "step: 250, loss: 2.9231061489554122e-05\n",
            "step: 260, loss: 2.6947122023557313e-05\n",
            "step: 270, loss: 1.936742410180159e-05\n",
            "step: 280, loss: 1.425280515832128e-05\n",
            "step: 290, loss: 3.129451215500012e-05\n",
            "step: 300, loss: 3.915305933333002e-05\n",
            "step: 310, loss: 3.333609492983669e-05\n",
            "step: 320, loss: 7.241106504807249e-05\n",
            "step: 330, loss: 0.00022635261120740324\n",
            "step: 340, loss: 6.170944834593683e-05\n",
            "step: 350, loss: 3.978548920713365e-05\n",
            "step: 360, loss: 6.817837856942788e-05\n",
            "step: 370, loss: 2.0853569367318414e-05\n",
            "step: 380, loss: 8.418483776040375e-05\n",
            "step: 390, loss: 0.0028893568087369204\n",
            "step: 400, loss: 0.00017294203280471265\n",
            "step: 410, loss: 5.3389518143376336e-05\n",
            "step: 420, loss: 5.0564183766255155e-05\n",
            "step: 430, loss: 0.00034710083855316043\n",
            "step: 440, loss: 6.116409349488094e-05\n",
            "step: 450, loss: 3.33464631694369e-05\n",
            "step: 460, loss: 1.8194003132521175e-05\n",
            "step: 470, loss: 2.4444336304441094e-05\n",
            "step: 480, loss: 2.2217109290068038e-05\n",
            "step: 490, loss: 6.085087079554796e-05\n",
            "step: 500, loss: 1.3176226275390945e-05\n",
            "step: 510, loss: 4.5487813622457907e-05\n",
            "step: 520, loss: 1.965053343155887e-05\n",
            "step: 530, loss: 5.42418347322382e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9414542020774315, f1=0.9248334919124643, best_f1=0.9352914180816887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.861511479248293e-05\n",
            "step: 10, loss: 3.9954418753040954e-05\n",
            "step: 20, loss: 4.09401509386953e-05\n",
            "step: 30, loss: 0.0002514992083888501\n",
            "step: 40, loss: 0.07536045461893082\n",
            "step: 50, loss: 3.357307650730945e-05\n",
            "step: 60, loss: 3.0232182325562462e-05\n",
            "step: 70, loss: 8.880647510522977e-05\n",
            "step: 80, loss: 1.8436052414472215e-05\n",
            "step: 90, loss: 1.8696955521591008e-05\n",
            "step: 100, loss: 0.00021628969989251345\n",
            "step: 110, loss: 4.466665268409997e-05\n",
            "step: 120, loss: 1.8167795133194886e-05\n",
            "step: 130, loss: 0.06735361367464066\n",
            "step: 140, loss: 0.00012068869546055794\n",
            "step: 150, loss: 1.769490154401865e-05\n",
            "step: 160, loss: 2.3743290512356907e-05\n",
            "step: 170, loss: 1.9810686353594065e-05\n",
            "step: 180, loss: 1.4636456398875453e-05\n",
            "step: 190, loss: 2.2340080249705352e-05\n",
            "step: 200, loss: 1.641707967792172e-05\n",
            "step: 210, loss: 0.00019976745534222573\n",
            "step: 220, loss: 9.434369712835178e-05\n",
            "step: 230, loss: 3.167265822412446e-05\n",
            "step: 240, loss: 2.1270867364364676e-05\n",
            "step: 250, loss: 2.9555230867117643e-05\n",
            "step: 260, loss: 7.939182978589088e-05\n",
            "step: 270, loss: 3.222905797883868e-05\n",
            "step: 280, loss: 1.706885996100027e-05\n",
            "step: 290, loss: 1.3340111763682216e-05\n",
            "step: 300, loss: 2.5623350666137412e-05\n",
            "step: 310, loss: 2.659353776834905e-05\n",
            "step: 320, loss: 0.00014113535871729255\n",
            "step: 330, loss: 0.00014589977217838168\n",
            "step: 340, loss: 2.698358366615139e-05\n",
            "step: 350, loss: 1.3850398318027146e-05\n",
            "step: 360, loss: 0.0014912665355950594\n",
            "step: 370, loss: 1.4692363038193434e-05\n",
            "step: 380, loss: 3.150598058709875e-05\n",
            "step: 390, loss: 3.900527735822834e-05\n",
            "step: 400, loss: 7.302843005163595e-05\n",
            "step: 410, loss: 0.03693424165248871\n",
            "step: 420, loss: 2.1661940991180018e-05\n",
            "step: 430, loss: 0.000324700289638713\n",
            "step: 440, loss: 3.181197098456323e-05\n",
            "step: 450, loss: 1.6081798094091937e-05\n",
            "step: 460, loss: 1.7389338609063998e-05\n",
            "step: 470, loss: 0.004711776506155729\n",
            "step: 480, loss: 1.2393936231092084e-05\n",
            "step: 490, loss: 1.527723179606255e-05\n",
            "step: 500, loss: 2.23696024477249e-05\n",
            "step: 510, loss: 2.1043670130893588e-05\n",
            "step: 520, loss: 3.156663660774939e-05\n",
            "step: 530, loss: 0.000603840162511915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9411206077872746, f1=0.9241050119331742, best_f1=0.9352914180816887\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 196.05it/s]\n",
            "load_f1 = 0.943344081068632\n",
            "real_f1 = 0.9415554532903819\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2671a08-3c9a-4aab-e5fa-eabb482ecded"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5723162293434143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4761904761904762, f1=0.3829787234042553, best_f1=0.3829787234042553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5089961290359497\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.4761904761904762, f1=0.4761904761904762, best_f1=0.3829787234042553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.600621223449707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5714285714285714, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2770836651325226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6250000000000001, f1=0.5454545454545454, best_f1=0.5454545454545454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2899373471736908\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6666666666666665, f1=0.5641025641025641, best_f1=0.5641025641025641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22987358272075653\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8387096774193549, f1=0.7058823529411764, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05309892073273659\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8484848484848484, f1=0.7058823529411764, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00888088345527649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8666666666666666, f1=0.7857142857142857, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0068386271595954895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8666666666666666, f1=0.7586206896551724, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02832161635160446\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8275862068965518, f1=0.7857142857142857, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015190683770924807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8750000000000001, f1=0.7058823529411764, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14676958322525024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9032258064516129, f1=0.7741935483870968, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029221305157989264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9032258064516129, f1=0.7741935483870968, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018053180538117886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9032258064516129, f1=0.7741935483870968, best_f1=0.7741935483870968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0043836915865540504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9032258064516129, f1=0.7741935483870968, best_f1=0.7741935483870968\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 129427.49it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8235294117647058\n",
            "real_f1 = 0.8\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 196.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0583bb75-f8dc-4c00-b551-2cd9164aec8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5879495143890381\n",
            "step: 10, loss: 0.6351197361946106\n",
            "step: 20, loss: 0.34574073553085327\n",
            "step: 30, loss: 0.11681168526411057\n",
            "step: 40, loss: 0.31944727897644043\n",
            "step: 50, loss: 0.05297078937292099\n",
            "step: 60, loss: 0.029154961928725243\n",
            "step: 70, loss: 0.036405228078365326\n",
            "step: 80, loss: 0.13111251592636108\n",
            "step: 90, loss: 0.1730131357908249\n",
            "step: 100, loss: 0.018699346110224724\n",
            "step: 110, loss: 0.15918830037117004\n",
            "step: 120, loss: 0.013745211064815521\n",
            "step: 130, loss: 0.006614878308027983\n",
            "step: 140, loss: 0.0025431180838495493\n",
            "step: 150, loss: 0.035667259246110916\n",
            "step: 160, loss: 0.011252583004534245\n",
            "step: 170, loss: 0.060955967754125595\n",
            "step: 180, loss: 0.03254802152514458\n",
            "step: 190, loss: 0.002762076910585165\n",
            "step: 200, loss: 0.014281907118856907\n",
            "step: 210, loss: 0.008263975381851196\n",
            "step: 220, loss: 0.00136531179305166\n",
            "step: 230, loss: 0.003931193146854639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9819413092550789, f1=0.9783352337514253, best_f1=0.9783352337514253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05390576273202896\n",
            "step: 10, loss: 0.0026842777151614428\n",
            "step: 20, loss: 0.12898890674114227\n",
            "step: 30, loss: 0.2168799340724945\n",
            "step: 40, loss: 0.0233453456312418\n",
            "step: 50, loss: 0.0036158678121864796\n",
            "step: 60, loss: 0.0028858589939773083\n",
            "step: 70, loss: 0.2202688455581665\n",
            "step: 80, loss: 0.029705142602324486\n",
            "step: 90, loss: 0.0036493621300905943\n",
            "step: 100, loss: 0.014117994345724583\n",
            "step: 110, loss: 0.08072357624769211\n",
            "step: 120, loss: 0.002212034771218896\n",
            "step: 130, loss: 0.009624618105590343\n",
            "step: 140, loss: 0.0023933937773108482\n",
            "step: 150, loss: 0.0066144089214503765\n",
            "step: 160, loss: 0.024550406262278557\n",
            "step: 170, loss: 0.00129384093452245\n",
            "step: 180, loss: 0.01312487293034792\n",
            "step: 190, loss: 0.12223276495933533\n",
            "step: 200, loss: 0.00226397393271327\n",
            "step: 210, loss: 0.0018056257395073771\n",
            "step: 220, loss: 0.020421171560883522\n",
            "step: 230, loss: 0.004712779074907303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9898762654668166, f1=0.9864559819413092, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003034637775272131\n",
            "step: 10, loss: 0.02469855360686779\n",
            "step: 20, loss: 0.0007814903510734439\n",
            "step: 30, loss: 0.022559912875294685\n",
            "step: 40, loss: 0.10054778307676315\n",
            "step: 50, loss: 0.006515864748507738\n",
            "step: 60, loss: 0.04003532603383064\n",
            "step: 70, loss: 0.015405433252453804\n",
            "step: 80, loss: 0.002343378961086273\n",
            "step: 90, loss: 0.06467949599027634\n",
            "step: 100, loss: 0.0008441125391982496\n",
            "step: 110, loss: 0.0012339416425675154\n",
            "step: 120, loss: 0.012915899977087975\n",
            "step: 130, loss: 0.0009190545533783734\n",
            "step: 140, loss: 0.028543950989842415\n",
            "step: 150, loss: 0.0015570688992738724\n",
            "step: 160, loss: 0.003814501455053687\n",
            "step: 170, loss: 0.0052234698086977005\n",
            "step: 180, loss: 0.009063372388482094\n",
            "step: 190, loss: 0.008730482310056686\n",
            "step: 200, loss: 0.003764956258237362\n",
            "step: 210, loss: 0.0011029099114239216\n",
            "step: 220, loss: 0.0010992359602823853\n",
            "step: 230, loss: 0.0012637045001611114\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9921436588103255, f1=0.984304932735426, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007234452059492469\n",
            "step: 10, loss: 0.00039289050619117916\n",
            "step: 20, loss: 0.0006007771007716656\n",
            "step: 30, loss: 0.0013302351580932736\n",
            "step: 40, loss: 0.008734439499676228\n",
            "step: 50, loss: 0.0007849635439924896\n",
            "step: 60, loss: 0.002537741092965007\n",
            "step: 70, loss: 0.00034267493174411356\n",
            "step: 80, loss: 0.0022585338447242975\n",
            "step: 90, loss: 0.007754125166684389\n",
            "step: 100, loss: 0.0007282645092345774\n",
            "step: 110, loss: 0.0028620774392038584\n",
            "step: 120, loss: 0.007108010351657867\n",
            "step: 130, loss: 0.0053014010190963745\n",
            "step: 140, loss: 0.0005013084155507386\n",
            "step: 150, loss: 0.2019900530576706\n",
            "step: 160, loss: 0.0009371594060212374\n",
            "step: 170, loss: 0.14957131445407867\n",
            "step: 180, loss: 0.0006661242805421352\n",
            "step: 190, loss: 0.00564616871997714\n",
            "step: 200, loss: 0.004561245441436768\n",
            "step: 210, loss: 0.09485898166894913\n",
            "step: 220, loss: 0.0006562020862475038\n",
            "step: 230, loss: 0.007256180047988892\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9853768278965129, f1=0.9783352337514253, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014444597763940692\n",
            "step: 10, loss: 0.001003818935714662\n",
            "step: 20, loss: 0.0020126334857195616\n",
            "step: 30, loss: 0.00045191129902377725\n",
            "step: 40, loss: 0.00021836013183929026\n",
            "step: 50, loss: 0.0001528878347016871\n",
            "step: 60, loss: 0.0004475772730074823\n",
            "step: 70, loss: 0.0001936507469508797\n",
            "step: 80, loss: 0.009728369303047657\n",
            "step: 90, loss: 0.0005792819429188967\n",
            "step: 100, loss: 0.0002345589455217123\n",
            "step: 110, loss: 0.0014998021069914103\n",
            "step: 120, loss: 5.9447294916026294e-05\n",
            "step: 130, loss: 0.0008990878704935312\n",
            "step: 140, loss: 0.0015950131928548217\n",
            "step: 150, loss: 0.009235890582203865\n",
            "step: 160, loss: 0.0030216870363801718\n",
            "step: 170, loss: 0.010244395583868027\n",
            "step: 180, loss: 0.004695449955761433\n",
            "step: 190, loss: 0.004693671129643917\n",
            "step: 200, loss: 0.023004302754998207\n",
            "step: 210, loss: 0.00028646443388424814\n",
            "step: 220, loss: 0.0028451785910874605\n",
            "step: 230, loss: 0.0004607686714734882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9876265466816648, f1=0.9852774631936579, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00478289183229208\n",
            "step: 10, loss: 0.00041232671355828643\n",
            "step: 20, loss: 0.000691188732162118\n",
            "step: 30, loss: 0.025773653760552406\n",
            "step: 40, loss: 0.0001502961531514302\n",
            "step: 50, loss: 0.00015251834702212363\n",
            "step: 60, loss: 9.188438707496971e-05\n",
            "step: 70, loss: 0.0036190368700772524\n",
            "step: 80, loss: 0.005182295106351376\n",
            "step: 90, loss: 0.0009801110718399286\n",
            "step: 100, loss: 0.03518830984830856\n",
            "step: 110, loss: 0.0018849615007638931\n",
            "step: 120, loss: 0.00016500003403052688\n",
            "step: 130, loss: 0.003767314599826932\n",
            "step: 140, loss: 0.0001761463936418295\n",
            "step: 150, loss: 0.0008286773227155209\n",
            "step: 160, loss: 0.0078040361404418945\n",
            "step: 170, loss: 0.0012574568390846252\n",
            "step: 180, loss: 0.01594812609255314\n",
            "step: 190, loss: 0.02565000392496586\n",
            "step: 200, loss: 0.0012633062433451414\n",
            "step: 210, loss: 0.003537634154781699\n",
            "step: 220, loss: 0.00016266273451037705\n",
            "step: 230, loss: 0.052543267607688904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9875706214689265, f1=0.9818181818181818, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001972736092284322\n",
            "step: 10, loss: 0.0001184842549264431\n",
            "step: 20, loss: 9.681722440291196e-05\n",
            "step: 30, loss: 0.0023350210394710302\n",
            "step: 40, loss: 0.00013099003990646452\n",
            "step: 50, loss: 0.00026476336643099785\n",
            "step: 60, loss: 0.0008130475180223584\n",
            "step: 70, loss: 0.004751952830702066\n",
            "step: 80, loss: 8.734393486520275e-05\n",
            "step: 90, loss: 7.204952999018133e-05\n",
            "step: 100, loss: 7.263312727445737e-05\n",
            "step: 110, loss: 0.00011355804599588737\n",
            "step: 120, loss: 4.915283716400154e-05\n",
            "step: 130, loss: 0.00017375661991536617\n",
            "step: 140, loss: 0.00010109956201631576\n",
            "step: 150, loss: 0.005143456626683474\n",
            "step: 160, loss: 0.06225467100739479\n",
            "step: 170, loss: 0.0008251820108853281\n",
            "step: 180, loss: 0.0003327679296489805\n",
            "step: 190, loss: 0.00019587329006753862\n",
            "step: 200, loss: 0.02306114323437214\n",
            "step: 210, loss: 0.00010224224388366565\n",
            "step: 220, loss: 0.005968289915472269\n",
            "step: 230, loss: 0.00015880742284934968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9852774631936579, f1=0.9829738933030647, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011551351053640246\n",
            "step: 10, loss: 0.0001525240804767236\n",
            "step: 20, loss: 0.00044083886314183474\n",
            "step: 30, loss: 0.00013391162792686373\n",
            "step: 40, loss: 0.0006051035597920418\n",
            "step: 50, loss: 0.00017655329429544508\n",
            "step: 60, loss: 0.0579773411154747\n",
            "step: 70, loss: 0.0004746206686832011\n",
            "step: 80, loss: 0.00010590430611046031\n",
            "step: 90, loss: 6.993782881181687e-05\n",
            "step: 100, loss: 0.0003514644631650299\n",
            "step: 110, loss: 0.13076075911521912\n",
            "step: 120, loss: 0.025742443278431892\n",
            "step: 130, loss: 0.0056189014576375484\n",
            "step: 140, loss: 0.00031807288178242743\n",
            "step: 150, loss: 0.0006666792905889452\n",
            "step: 160, loss: 0.00019269382755737752\n",
            "step: 170, loss: 0.00024589995155110955\n",
            "step: 180, loss: 0.005230267066508532\n",
            "step: 190, loss: 0.00017291698895860463\n",
            "step: 200, loss: 8.719740435481071e-05\n",
            "step: 210, loss: 0.0001421451597707346\n",
            "step: 220, loss: 0.00011356857430655509\n",
            "step: 230, loss: 0.00014789898705203086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.983277591973244, f1=0.9832402234636871, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011781267676269636\n",
            "step: 10, loss: 0.0002889663737732917\n",
            "step: 20, loss: 0.001804152736440301\n",
            "step: 30, loss: 0.0002576246333774179\n",
            "step: 40, loss: 0.0044439793564379215\n",
            "step: 50, loss: 5.5939293815754354e-05\n",
            "step: 60, loss: 6.605085945921019e-05\n",
            "step: 70, loss: 0.00073156354483217\n",
            "step: 80, loss: 7.002842176007107e-05\n",
            "step: 90, loss: 6.305424903985113e-05\n",
            "step: 100, loss: 0.00011311419075354934\n",
            "step: 110, loss: 7.899488264229149e-05\n",
            "step: 120, loss: 3.400357672944665e-05\n",
            "step: 130, loss: 4.1892326407833025e-05\n",
            "step: 140, loss: 3.7955465813865885e-05\n",
            "step: 150, loss: 0.0005810832954011858\n",
            "step: 160, loss: 8.273313869722188e-05\n",
            "step: 170, loss: 7.419603207381442e-05\n",
            "step: 180, loss: 0.005658951587975025\n",
            "step: 190, loss: 0.00022005170467309654\n",
            "step: 200, loss: 0.04656320437788963\n",
            "step: 210, loss: 0.002171424450352788\n",
            "step: 220, loss: 0.00017101748380810022\n",
            "step: 230, loss: 0.00017254478007089347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9854748603351955, f1=0.9799107142857142, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018628520774655044\n",
            "step: 10, loss: 0.0002628426591400057\n",
            "step: 20, loss: 0.00017298245802521706\n",
            "step: 30, loss: 5.6910317653091624e-05\n",
            "step: 40, loss: 0.00011106068996014073\n",
            "step: 50, loss: 0.00013041192141827196\n",
            "step: 60, loss: 0.00026726644136942923\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 0.00019825121853500605\n",
            "step: 80, loss: 3.824257873930037e-05\n",
            "step: 90, loss: 0.0005536496755667031\n",
            "step: 100, loss: 6.830380880273879e-05\n",
            "step: 110, loss: 0.0012217385228723288\n",
            "step: 120, loss: 0.00433831661939621\n",
            "step: 130, loss: 5.6154221965698525e-05\n",
            "step: 140, loss: 0.04454032704234123\n",
            "step: 150, loss: 0.011202300898730755\n",
            "step: 160, loss: 3.275924609624781e-05\n",
            "step: 170, loss: 9.294466144638136e-05\n",
            "step: 180, loss: 0.0005862741963937879\n",
            "step: 190, loss: 0.014473901130259037\n",
            "step: 200, loss: 0.00017339135229121894\n",
            "step: 210, loss: 3.3626936783548445e-05\n",
            "step: 220, loss: 4.263439404894598e-05\n",
            "step: 230, loss: 0.0005508980248123407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9866071428571428, f1=0.9821826280623607, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.778023165068589e-05\n",
            "step: 10, loss: 0.00039337313501164317\n",
            "step: 20, loss: 0.0035738544538617134\n",
            "step: 30, loss: 0.020904377102851868\n",
            "step: 40, loss: 0.00037730945041403174\n",
            "step: 50, loss: 4.256975807948038e-05\n",
            "step: 60, loss: 0.0009115960565395653\n",
            "step: 70, loss: 0.0002498266112525016\n",
            "step: 80, loss: 0.0001226866152137518\n",
            "step: 90, loss: 0.0009565692162141204\n",
            "step: 100, loss: 0.0006287145661190152\n",
            "step: 110, loss: 0.04843176528811455\n",
            "step: 120, loss: 6.134423892945051e-05\n",
            "step: 130, loss: 0.0002642635372467339\n",
            "step: 140, loss: 0.006916780024766922\n",
            "step: 150, loss: 0.013471723534166813\n",
            "step: 160, loss: 0.00012671838339883834\n",
            "step: 170, loss: 0.019948724657297134\n",
            "step: 180, loss: 0.0005352067528292537\n",
            "step: 190, loss: 4.224723670631647e-05\n",
            "step: 200, loss: 0.0004216574889142066\n",
            "step: 210, loss: 4.9603931984165683e-05\n",
            "step: 220, loss: 4.8471538320882246e-05\n",
            "step: 230, loss: 0.0021268706768751144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9855072463768116, f1=0.978865406006674, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020585661695804447\n",
            "step: 10, loss: 0.00023934507044032216\n",
            "step: 20, loss: 5.889394014957361e-05\n",
            "step: 30, loss: 5.8766003348864615e-05\n",
            "step: 40, loss: 0.00037561325007118285\n",
            "step: 50, loss: 0.0003129302931483835\n",
            "step: 60, loss: 0.0002905502915382385\n",
            "step: 70, loss: 7.248880865518004e-05\n",
            "step: 80, loss: 0.00014954968355596066\n",
            "step: 90, loss: 3.10198993247468e-05\n",
            "step: 100, loss: 7.298479613382369e-05\n",
            "step: 110, loss: 0.00013770622899755836\n",
            "step: 120, loss: 5.515721932169981e-05\n",
            "step: 130, loss: 4.6750035835430026e-05\n",
            "step: 140, loss: 0.011258103884756565\n",
            "step: 150, loss: 0.000302895117783919\n",
            "step: 160, loss: 3.574703077902086e-05\n",
            "step: 170, loss: 0.0001980039814952761\n",
            "step: 180, loss: 0.0209537111222744\n",
            "step: 190, loss: 4.795128552359529e-05\n",
            "step: 200, loss: 4.379410529509187e-05\n",
            "step: 210, loss: 0.00010554441541898996\n",
            "step: 220, loss: 0.006266613956540823\n",
            "step: 230, loss: 0.03709025681018829\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9865771812080537, f1=0.9831649831649831, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.772046567173675e-05\n",
            "step: 10, loss: 0.001109635573811829\n",
            "step: 20, loss: 0.0020912804175168276\n",
            "step: 30, loss: 6.580150511581451e-05\n",
            "step: 40, loss: 9.25034619285725e-05\n",
            "step: 50, loss: 0.00011046246800106019\n",
            "step: 60, loss: 8.630973024992272e-05\n",
            "step: 70, loss: 3.544343780959025e-05\n",
            "step: 80, loss: 5.596923438133672e-05\n",
            "step: 90, loss: 5.9567744756350294e-05\n",
            "step: 100, loss: 2.638949081301689e-05\n",
            "step: 110, loss: 0.002330099930986762\n",
            "step: 120, loss: 0.050241775810718536\n",
            "step: 130, loss: 7.576151983812451e-05\n",
            "step: 140, loss: 3.4167438570875674e-05\n",
            "step: 150, loss: 4.0007842471823096e-05\n",
            "step: 160, loss: 0.00012075588892912492\n",
            "step: 170, loss: 0.0010935167083516717\n",
            "step: 180, loss: 0.00011847605492221192\n",
            "step: 190, loss: 2.9589289624709636e-05\n",
            "step: 200, loss: 0.0009296130738221109\n",
            "step: 210, loss: 2.5212273612851277e-05\n",
            "step: 220, loss: 0.00015661866927985102\n",
            "step: 230, loss: 7.428721437463537e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.987709497206704, f1=0.9820627802690582, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.351912528160028e-05\n",
            "step: 10, loss: 0.005523880943655968\n",
            "step: 20, loss: 0.0001470683782827109\n",
            "step: 30, loss: 0.00935161393135786\n",
            "step: 40, loss: 0.007014417555183172\n",
            "step: 50, loss: 6.116246368037537e-05\n",
            "step: 60, loss: 5.0615195505088195e-05\n",
            "step: 70, loss: 3.358611138537526e-05\n",
            "step: 80, loss: 8.215112757170573e-05\n",
            "step: 90, loss: 4.36465852544643e-05\n",
            "step: 100, loss: 4.032059950986877e-05\n",
            "step: 110, loss: 6.38954879832454e-05\n",
            "step: 120, loss: 2.0511153707047924e-05\n",
            "step: 130, loss: 3.280014789197594e-05\n",
            "step: 140, loss: 0.00027953696553595364\n",
            "step: 150, loss: 3.978308814112097e-05\n",
            "step: 160, loss: 0.0064516691491007805\n",
            "step: 170, loss: 8.293409337056801e-05\n",
            "step: 180, loss: 0.00010312608355889097\n",
            "step: 190, loss: 3.660708898678422e-05\n",
            "step: 200, loss: 0.00011502976121846586\n",
            "step: 210, loss: 5.502076237462461e-05\n",
            "step: 220, loss: 0.0008735916344448924\n",
            "step: 230, loss: 0.002164522185921669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.987709497206704, f1=0.9832026875699889, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.86369026475586e-05\n",
            "step: 10, loss: 2.5402232495252974e-05\n",
            "step: 20, loss: 3.88124244636856e-05\n",
            "step: 30, loss: 3.7026722566224635e-05\n",
            "step: 40, loss: 0.00014920382818672806\n",
            "step: 50, loss: 0.03518601879477501\n",
            "step: 60, loss: 0.00013637264783028513\n",
            "step: 70, loss: 0.0002053125062957406\n",
            "step: 80, loss: 7.307058695005253e-05\n",
            "step: 90, loss: 0.00011112997162854299\n",
            "step: 100, loss: 0.00026511639589443803\n",
            "step: 110, loss: 4.054691089550033e-05\n",
            "step: 120, loss: 0.0002516602980904281\n",
            "step: 130, loss: 0.02910393476486206\n",
            "step: 140, loss: 2.2686699594487436e-05\n",
            "step: 150, loss: 0.00025340908905491233\n",
            "step: 160, loss: 6.941657193237916e-05\n",
            "step: 170, loss: 8.181881275959313e-05\n",
            "step: 180, loss: 0.0002829797158483416\n",
            "step: 190, loss: 0.0002808517892844975\n",
            "step: 200, loss: 0.00013480654160957783\n",
            "step: 210, loss: 7.520983490394428e-05\n",
            "step: 220, loss: 0.0002244346251245588\n",
            "step: 230, loss: 3.4207954740850255e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9866071428571428, f1=0.9820627802690582, best_f1=0.984304932735426\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 168.08it/s]\n",
            "load_f1 = 0.9932432432432432\n",
            "real_f1 = 0.9921259842519685\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbdeb3b-6ebb-4ea6-9282-ba9010b58ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6056830286979675\n",
            "step: 10, loss: 0.48671266436576843\n",
            "step: 20, loss: 0.4952046573162079\n",
            "step: 30, loss: 0.1341300904750824\n",
            "step: 40, loss: 0.05080261826515198\n",
            "step: 50, loss: 0.30849355459213257\n",
            "step: 60, loss: 0.06354303658008575\n",
            "step: 70, loss: 0.2153526246547699\n",
            "step: 80, loss: 0.1531357616186142\n",
            "step: 90, loss: 0.2993803322315216\n",
            "step: 100, loss: 0.029736565425992012\n",
            "step: 110, loss: 0.07863210141658783\n",
            "step: 120, loss: 0.08997026085853577\n",
            "step: 130, loss: 0.024150926619768143\n",
            "step: 140, loss: 0.033066023141145706\n",
            "step: 150, loss: 0.04951201751828194\n",
            "step: 160, loss: 0.030431905761361122\n",
            "step: 170, loss: 0.17827516794204712\n",
            "step: 180, loss: 0.08608818799257278\n",
            "step: 190, loss: 0.012509978376328945\n",
            "step: 200, loss: 0.136866495013237\n",
            "step: 210, loss: 0.08008138090372086\n",
            "step: 220, loss: 0.22274905443191528\n",
            "step: 230, loss: 0.14563053846359253\n",
            "step: 240, loss: 0.01800447888672352\n",
            "step: 250, loss: 0.0491865836083889\n",
            "step: 260, loss: 0.08452793210744858\n",
            "step: 270, loss: 0.10682818293571472\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 280, loss: 0.08151289820671082\n",
            "step: 290, loss: 0.20138494670391083\n",
            "step: 300, loss: 0.035791803151369095\n",
            "step: 310, loss: 0.24357402324676514\n",
            "step: 320, loss: 0.04698223993182182\n",
            "step: 330, loss: 0.016436053439974785\n",
            "step: 340, loss: 0.0616389624774456\n",
            "step: 350, loss: 0.1640530526638031\n",
            "step: 360, loss: 0.0801292136311531\n",
            "step: 370, loss: 0.06537435948848724\n",
            "step: 380, loss: 0.0780545324087143\n",
            "step: 390, loss: 0.19008587300777435\n",
            "step: 400, loss: 0.1979454755783081\n",
            "step: 410, loss: 0.04874361306428909\n",
            "step: 420, loss: 0.03949306532740593\n",
            "step: 430, loss: 0.21747484803199768\n",
            "step: 440, loss: 0.04117783159017563\n",
            "step: 450, loss: 0.006181924603879452\n",
            "step: 460, loss: 0.02059437893331051\n",
            "step: 470, loss: 0.10012863576412201\n",
            "step: 480, loss: 0.059720829129219055\n",
            "step: 490, loss: 0.16258442401885986\n",
            "step: 500, loss: 0.11362351477146149\n",
            "step: 510, loss: 0.04616708308458328\n",
            "step: 520, loss: 0.06633046269416809\n",
            "step: 530, loss: 0.0022200376261025667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9311475409836065, f1=0.9335192933519293, best_f1=0.9335192933519293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32736778259277344\n",
            "step: 10, loss: 0.09538831561803818\n",
            "step: 20, loss: 0.011863930150866508\n",
            "step: 30, loss: 0.018558889627456665\n",
            "step: 40, loss: 0.057668011635541916\n",
            "step: 50, loss: 0.06914232671260834\n",
            "step: 60, loss: 0.08681558817625046\n",
            "step: 70, loss: 0.1029915139079094\n",
            "step: 80, loss: 0.04197295382618904\n",
            "step: 90, loss: 0.010253914631903172\n",
            "step: 100, loss: 0.020336942747235298\n",
            "step: 110, loss: 0.015646900981664658\n",
            "step: 120, loss: 0.15157704055309296\n",
            "step: 130, loss: 0.11721634864807129\n",
            "step: 140, loss: 0.019785122945904732\n",
            "step: 150, loss: 0.1392667442560196\n",
            "step: 160, loss: 0.007501333951950073\n",
            "step: 170, loss: 0.00579499639570713\n",
            "step: 180, loss: 0.025112604722380638\n",
            "step: 190, loss: 0.07412412017583847\n",
            "step: 200, loss: 0.009859699755907059\n",
            "step: 210, loss: 0.07333612442016602\n",
            "step: 220, loss: 0.14755812287330627\n",
            "step: 230, loss: 0.09406350553035736\n",
            "step: 240, loss: 0.1404649317264557\n",
            "step: 250, loss: 0.14598813652992249\n",
            "step: 260, loss: 0.0071792397648096085\n",
            "step: 270, loss: 0.11122774332761765\n",
            "step: 280, loss: 0.015152291394770145\n",
            "step: 290, loss: 0.02878561057150364\n",
            "step: 300, loss: 0.19770193099975586\n",
            "step: 310, loss: 0.0129060298204422\n",
            "step: 320, loss: 0.11531262844800949\n",
            "step: 330, loss: 0.08798030763864517\n",
            "step: 340, loss: 0.052827879786491394\n",
            "step: 350, loss: 0.0013317351695150137\n",
            "step: 360, loss: 0.011479396373033524\n",
            "step: 370, loss: 0.15175282955169678\n",
            "step: 380, loss: 0.0427955687046051\n",
            "step: 390, loss: 0.055422160774469376\n",
            "step: 400, loss: 0.06953930854797363\n",
            "step: 410, loss: 0.007388134021311998\n",
            "step: 420, loss: 0.03590783104300499\n",
            "step: 430, loss: 0.01305961050093174\n",
            "step: 440, loss: 0.15576881170272827\n",
            "step: 450, loss: 0.058814775198698044\n",
            "step: 460, loss: 0.016009507700800896\n",
            "step: 470, loss: 0.131790891289711\n",
            "step: 480, loss: 0.276548832654953\n",
            "step: 490, loss: 0.01302926056087017\n",
            "step: 500, loss: 0.2997358441352844\n",
            "step: 510, loss: 0.016481023281812668\n",
            "step: 520, loss: 0.05668371170759201\n",
            "step: 530, loss: 0.014469383284449577\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9354243542435424, f1=0.9327188940092166, best_f1=0.9327188940092166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24939970672130585\n",
            "step: 10, loss: 0.04198877885937691\n",
            "step: 20, loss: 0.03865250572562218\n",
            "step: 30, loss: 0.09467437863349915\n",
            "step: 40, loss: 0.06603863835334778\n",
            "step: 50, loss: 0.051603931933641434\n",
            "step: 60, loss: 0.0285358689725399\n",
            "step: 70, loss: 0.00424532312899828\n",
            "step: 80, loss: 0.00333752972073853\n",
            "step: 90, loss: 0.020661812275648117\n",
            "step: 100, loss: 0.03500435873866081\n",
            "step: 110, loss: 0.0013908512191846967\n",
            "step: 120, loss: 0.017931632697582245\n",
            "step: 130, loss: 0.006471480708569288\n",
            "step: 140, loss: 0.009537650272250175\n",
            "step: 150, loss: 0.02164044976234436\n",
            "step: 160, loss: 0.011805471032857895\n",
            "step: 170, loss: 0.07213246077299118\n",
            "step: 180, loss: 0.007356804795563221\n",
            "step: 190, loss: 0.011235922574996948\n",
            "step: 200, loss: 0.038233090192079544\n",
            "step: 210, loss: 0.08814413100481033\n",
            "step: 220, loss: 0.09466797858476639\n",
            "step: 230, loss: 0.0759652853012085\n",
            "step: 240, loss: 0.023069854825735092\n",
            "step: 250, loss: 0.029205821454524994\n",
            "step: 260, loss: 0.017059680074453354\n",
            "step: 270, loss: 0.0043665338307619095\n",
            "step: 280, loss: 0.15108883380889893\n",
            "step: 290, loss: 0.0032393834553658962\n",
            "step: 300, loss: 0.020099008455872536\n",
            "step: 310, loss: 0.00988102052360773\n",
            "step: 320, loss: 0.013665435835719109\n",
            "step: 330, loss: 0.003982939291745424\n",
            "step: 340, loss: 0.0034012198448181152\n",
            "step: 350, loss: 0.006805736105889082\n",
            "step: 360, loss: 0.045689214020967484\n",
            "step: 370, loss: 0.003684328170493245\n",
            "step: 380, loss: 0.012462570331990719\n",
            "step: 390, loss: 0.025184009224176407\n",
            "step: 400, loss: 0.008088530972599983\n",
            "step: 410, loss: 0.004237615503370762\n",
            "step: 420, loss: 0.1432313621044159\n",
            "step: 430, loss: 0.03685395419597626\n",
            "step: 440, loss: 0.005988053046166897\n",
            "step: 450, loss: 0.06118442490696907\n",
            "step: 460, loss: 0.028526952490210533\n",
            "step: 470, loss: 0.04922507330775261\n",
            "step: 480, loss: 0.02219744771718979\n",
            "step: 490, loss: 0.02312634326517582\n",
            "step: 500, loss: 0.00931494403630495\n",
            "step: 510, loss: 0.016796208918094635\n",
            "step: 520, loss: 0.12684382498264313\n",
            "step: 530, loss: 0.024367153644561768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9391955098222639, f1=0.926829268292683, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009339220821857452\n",
            "step: 10, loss: 0.0044335490092635155\n",
            "step: 20, loss: 0.026616381481289864\n",
            "step: 30, loss: 0.0007198153180070221\n",
            "step: 40, loss: 0.038019414991140366\n",
            "step: 50, loss: 0.004984982777386904\n",
            "step: 60, loss: 0.018056239932775497\n",
            "step: 70, loss: 0.0869821086525917\n",
            "step: 80, loss: 0.0007868855609558523\n",
            "step: 90, loss: 0.020868385210633278\n",
            "step: 100, loss: 0.02394157275557518\n",
            "step: 110, loss: 0.013768717646598816\n",
            "step: 120, loss: 0.033447034657001495\n",
            "step: 130, loss: 0.0031491422560065985\n",
            "step: 140, loss: 0.0010262442519888282\n",
            "step: 150, loss: 0.017861701548099518\n",
            "step: 160, loss: 0.051754847168922424\n",
            "step: 170, loss: 0.0031844917684793472\n",
            "step: 180, loss: 0.004009251948446035\n",
            "step: 190, loss: 0.006798724178224802\n",
            "step: 200, loss: 0.014395911246538162\n",
            "step: 210, loss: 0.0033675245940685272\n",
            "step: 220, loss: 0.003952420316636562\n",
            "step: 230, loss: 0.04662406072020531\n",
            "step: 240, loss: 0.001200301107019186\n",
            "step: 250, loss: 0.003226139582693577\n",
            "step: 260, loss: 0.0007537920610047877\n",
            "step: 270, loss: 0.005812426097691059\n",
            "step: 280, loss: 0.09957852959632874\n",
            "step: 290, loss: 0.035617027431726456\n",
            "step: 300, loss: 0.0020054348278790712\n",
            "step: 310, loss: 0.007574854884296656\n",
            "step: 320, loss: 0.02399243600666523\n",
            "step: 330, loss: 0.06971930712461472\n",
            "step: 340, loss: 0.00099464925006032\n",
            "step: 350, loss: 0.1418742686510086\n",
            "step: 360, loss: 0.05192306637763977\n",
            "step: 370, loss: 0.01131319161504507\n",
            "step: 380, loss: 0.037814319133758545\n",
            "step: 390, loss: 0.0632757917046547\n",
            "step: 400, loss: 0.011770457029342651\n",
            "step: 410, loss: 0.003290473949164152\n",
            "step: 420, loss: 0.010031045414507389\n",
            "step: 430, loss: 0.0875239148736\n",
            "step: 440, loss: 0.02174782194197178\n",
            "step: 450, loss: 0.050848253071308136\n",
            "step: 460, loss: 0.0018623630749061704\n",
            "step: 470, loss: 0.0013745836913585663\n",
            "step: 480, loss: 0.09186716377735138\n",
            "step: 490, loss: 0.010472786612808704\n",
            "step: 500, loss: 0.017956597730517387\n",
            "step: 510, loss: 0.005510191898792982\n",
            "step: 520, loss: 0.02494591660797596\n",
            "step: 530, loss: 0.047882165759801865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.932666060054595, f1=0.9307201458523247, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03794964775443077\n",
            "step: 10, loss: 0.027283180505037308\n",
            "step: 20, loss: 0.01942852884531021\n",
            "step: 30, loss: 0.02383052371442318\n",
            "step: 40, loss: 0.08990051597356796\n",
            "step: 50, loss: 0.010679371654987335\n",
            "step: 60, loss: 0.13901618123054504\n",
            "step: 70, loss: 0.014845027588307858\n",
            "step: 80, loss: 0.02747073769569397\n",
            "step: 90, loss: 0.05422923341393471\n",
            "step: 100, loss: 0.011852667666971684\n",
            "step: 110, loss: 0.0013614208437502384\n",
            "step: 120, loss: 0.006379642523825169\n",
            "step: 130, loss: 0.0005686875665560365\n",
            "step: 140, loss: 0.004598767962306738\n",
            "step: 150, loss: 0.0019551070872694254\n",
            "step: 160, loss: 0.0003935680433642119\n",
            "step: 170, loss: 0.017736496403813362\n",
            "step: 180, loss: 0.0007363775512203574\n",
            "step: 190, loss: 0.000932585506234318\n",
            "step: 200, loss: 0.0009043471654877067\n",
            "step: 210, loss: 0.018693163990974426\n",
            "step: 220, loss: 0.0018205762607976794\n",
            "step: 230, loss: 0.005784440785646439\n",
            "step: 240, loss: 0.0012880938593298197\n",
            "step: 250, loss: 0.0044695232063531876\n",
            "step: 260, loss: 0.002189900726079941\n",
            "step: 270, loss: 0.00024354978813789785\n",
            "step: 280, loss: 0.004171317908912897\n",
            "step: 290, loss: 0.07628636062145233\n",
            "step: 300, loss: 0.003184887347742915\n",
            "step: 310, loss: 0.00403132289648056\n",
            "step: 320, loss: 0.023923086002469063\n",
            "step: 330, loss: 0.10124138742685318\n",
            "step: 340, loss: 0.009599590674042702\n",
            "step: 350, loss: 0.0027733941096812487\n",
            "step: 360, loss: 0.01418119017034769\n",
            "step: 370, loss: 0.028549090027809143\n",
            "step: 380, loss: 0.0012097564758732915\n",
            "step: 390, loss: 0.000647296488750726\n",
            "step: 400, loss: 0.010681627318263054\n",
            "step: 410, loss: 0.008631756529211998\n",
            "step: 420, loss: 0.005814466159790754\n",
            "step: 430, loss: 0.00045634451089426875\n",
            "step: 440, loss: 0.024229271337389946\n",
            "step: 450, loss: 0.0020521690603345633\n",
            "step: 460, loss: 0.09543853998184204\n",
            "step: 470, loss: 0.00687940651550889\n",
            "step: 480, loss: 0.019907433539628983\n",
            "step: 490, loss: 0.0019678995013237\n",
            "step: 500, loss: 0.0037279590032994747\n",
            "step: 510, loss: 0.06164151430130005\n",
            "step: 520, loss: 0.03935135155916214\n",
            "step: 530, loss: 0.0009094922570511699\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9367441860465117, f1=0.9299303944315545, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005554516799747944\n",
            "step: 10, loss: 0.0022785442415624857\n",
            "step: 20, loss: 0.12328599393367767\n",
            "step: 30, loss: 0.06185663864016533\n",
            "step: 40, loss: 0.0005983130540698767\n",
            "step: 50, loss: 0.02599221281707287\n",
            "step: 60, loss: 0.00013047936954535544\n",
            "step: 70, loss: 0.001372210681438446\n",
            "step: 80, loss: 0.03025827184319496\n",
            "step: 90, loss: 0.006505743134766817\n",
            "step: 100, loss: 0.005143646150827408\n",
            "step: 110, loss: 0.04206559807062149\n",
            "step: 120, loss: 0.002096751006320119\n",
            "step: 130, loss: 0.007199229672551155\n",
            "step: 140, loss: 0.06778635829687119\n",
            "step: 150, loss: 0.0011361652286723256\n",
            "step: 160, loss: 0.012381902895867825\n",
            "step: 170, loss: 0.0019018049351871014\n",
            "step: 180, loss: 0.0011807094560936093\n",
            "step: 190, loss: 0.02156616933643818\n",
            "step: 200, loss: 0.04314490780234337\n",
            "step: 210, loss: 0.02092619799077511\n",
            "step: 220, loss: 0.018607251346111298\n",
            "step: 230, loss: 0.1001490131020546\n",
            "step: 240, loss: 0.07113561034202576\n",
            "step: 250, loss: 0.0034308135509490967\n",
            "step: 260, loss: 0.0023680238518863916\n",
            "step: 270, loss: 0.0781107023358345\n",
            "step: 280, loss: 0.0012781384866684675\n",
            "step: 290, loss: 0.00021697735064662993\n",
            "step: 300, loss: 0.010161800310015678\n",
            "step: 310, loss: 0.00044439564226195216\n",
            "step: 320, loss: 0.00048000813694670796\n",
            "step: 330, loss: 0.00014029645535629243\n",
            "step: 340, loss: 0.0956144705414772\n",
            "step: 350, loss: 0.0009134876308962703\n",
            "step: 360, loss: 0.022119618952274323\n",
            "step: 370, loss: 0.02253863774240017\n",
            "step: 380, loss: 0.00020596652757376432\n",
            "step: 390, loss: 0.04438285157084465\n",
            "step: 400, loss: 0.0026242416352033615\n",
            "step: 410, loss: 0.0006402177968993783\n",
            "step: 420, loss: 0.0017397957853972912\n",
            "step: 430, loss: 0.0012692735763266683\n",
            "step: 440, loss: 0.0015166024677455425\n",
            "step: 450, loss: 0.0010082758963108063\n",
            "step: 460, loss: 0.000631962320767343\n",
            "step: 470, loss: 0.002874253084883094\n",
            "step: 480, loss: 0.0030456294771283865\n",
            "step: 490, loss: 0.009398948401212692\n",
            "step: 500, loss: 0.0007880909834057093\n",
            "step: 510, loss: 0.0016222366830334067\n",
            "step: 520, loss: 0.006323203910142183\n",
            "step: 530, loss: 0.02933003380894661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9352112676056337, f1=0.9321561338289963, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006959354504942894\n",
            "step: 10, loss: 0.007610516157001257\n",
            "step: 20, loss: 0.001968431519344449\n",
            "step: 30, loss: 0.0029490762390196323\n",
            "step: 40, loss: 0.00034719990799203515\n",
            "step: 50, loss: 0.012215007096529007\n",
            "step: 60, loss: 0.002982890233397484\n",
            "step: 70, loss: 0.00021638930775225163\n",
            "step: 80, loss: 0.0019849399104714394\n",
            "step: 90, loss: 0.0052072107791900635\n",
            "step: 100, loss: 0.006848773919045925\n",
            "step: 110, loss: 0.0016440856270492077\n",
            "step: 120, loss: 0.0078095123171806335\n",
            "step: 130, loss: 0.003304634476080537\n",
            "step: 140, loss: 0.001816337462514639\n",
            "step: 150, loss: 0.0005451479810290039\n",
            "step: 160, loss: 0.00012470530054997653\n",
            "step: 170, loss: 0.00034823984606191516\n",
            "step: 180, loss: 0.0001621696283109486\n",
            "step: 190, loss: 0.023840848356485367\n",
            "step: 200, loss: 0.00045185245107859373\n",
            "step: 210, loss: 0.0006272773025557399\n",
            "step: 220, loss: 0.005370244849473238\n",
            "step: 230, loss: 0.035916637629270554\n",
            "step: 240, loss: 0.0009105698554776609\n",
            "step: 250, loss: 0.006196146830916405\n",
            "step: 260, loss: 0.00014676939463242888\n",
            "step: 270, loss: 0.0008699442842043936\n",
            "step: 280, loss: 0.0001353724510408938\n",
            "step: 290, loss: 0.001302648219279945\n",
            "step: 300, loss: 0.003075928892940283\n",
            "step: 310, loss: 0.0003206967667210847\n",
            "step: 320, loss: 0.0010070214048027992\n",
            "step: 330, loss: 0.004830007441341877\n",
            "step: 340, loss: 0.03550759702920914\n",
            "step: 350, loss: 0.0009281316306442022\n",
            "step: 360, loss: 0.0014218565775081515\n",
            "step: 370, loss: 0.004361363127827644\n",
            "step: 380, loss: 0.0003060217131860554\n",
            "step: 390, loss: 0.0021163499914109707\n",
            "step: 400, loss: 0.00869528204202652\n",
            "step: 410, loss: 0.0006794519140385091\n",
            "step: 420, loss: 0.0009551741532050073\n",
            "step: 430, loss: 0.0004713804228231311\n",
            "step: 440, loss: 0.019640401005744934\n",
            "step: 450, loss: 0.001385033130645752\n",
            "step: 460, loss: 0.000963413855060935\n",
            "step: 470, loss: 0.006307045929133892\n",
            "step: 480, loss: 0.0025783188175410032\n",
            "step: 490, loss: 0.00039238433237187564\n",
            "step: 500, loss: 0.00029780069598928094\n",
            "step: 510, loss: 0.0021114563569426537\n",
            "step: 520, loss: 0.04169868305325508\n",
            "step: 530, loss: 0.13045839965343475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9324137931034482, f1=0.930909090909091, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041670061647892\n",
            "step: 10, loss: 0.024370282888412476\n",
            "step: 20, loss: 0.0015955112176015973\n",
            "step: 30, loss: 0.00016007987142074853\n",
            "step: 40, loss: 0.005397573113441467\n",
            "step: 50, loss: 0.006050364580005407\n",
            "step: 60, loss: 0.002183083910495043\n",
            "step: 70, loss: 0.00016071554273366928\n",
            "step: 80, loss: 0.004260585177689791\n",
            "step: 90, loss: 0.0012740340316668153\n",
            "step: 100, loss: 0.00014489333261735737\n",
            "step: 110, loss: 0.0063695297576487064\n",
            "step: 120, loss: 0.03587467968463898\n",
            "step: 130, loss: 0.003174409968778491\n",
            "step: 140, loss: 0.011510745622217655\n",
            "step: 150, loss: 0.00045720700290985405\n",
            "step: 160, loss: 0.04115695506334305\n",
            "step: 170, loss: 0.005289482418447733\n",
            "step: 180, loss: 0.0014852641616016626\n",
            "step: 190, loss: 0.0005209124065004289\n",
            "step: 200, loss: 0.0004472793370950967\n",
            "step: 210, loss: 0.009370808489620686\n",
            "step: 220, loss: 0.0007408653036691248\n",
            "step: 230, loss: 0.0006232518353499472\n",
            "step: 240, loss: 0.004827433731406927\n",
            "step: 250, loss: 0.002376715885475278\n",
            "step: 260, loss: 6.332679186016321e-05\n",
            "step: 270, loss: 0.004792178049683571\n",
            "step: 280, loss: 0.02085241861641407\n",
            "step: 290, loss: 0.0012903622118756175\n",
            "step: 300, loss: 0.008436402305960655\n",
            "step: 310, loss: 0.037063054740428925\n",
            "step: 320, loss: 0.01869329810142517\n",
            "step: 330, loss: 0.18807344138622284\n",
            "step: 340, loss: 0.188223734498024\n",
            "step: 350, loss: 0.0010119216749444604\n",
            "step: 360, loss: 0.018057463690638542\n",
            "step: 370, loss: 0.00042739370837807655\n",
            "step: 380, loss: 0.0022502674255520105\n",
            "step: 390, loss: 0.10012997686862946\n",
            "step: 400, loss: 0.0004541263042483479\n",
            "step: 410, loss: 0.021019993349909782\n",
            "step: 420, loss: 0.0032410058192908764\n",
            "step: 430, loss: 0.07804747670888901\n",
            "step: 440, loss: 0.014534824527800083\n",
            "step: 450, loss: 0.00013980931544210762\n",
            "step: 460, loss: 0.0004035850870423019\n",
            "step: 470, loss: 0.0011897203512489796\n",
            "step: 480, loss: 0.017784271389245987\n",
            "step: 490, loss: 0.005054904147982597\n",
            "step: 500, loss: 0.021266059949994087\n",
            "step: 510, loss: 0.011286921799182892\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 520, loss: 0.006900493521243334\n",
            "step: 530, loss: 0.0007926359539851546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.929550827423168, f1=0.9286726499763817, best_f1=0.926829268292683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007915862835943699\n",
            "step: 10, loss: 0.0009961959440261126\n",
            "step: 20, loss: 0.0026381940115243196\n",
            "step: 30, loss: 0.0006350145558826625\n",
            "step: 40, loss: 0.003929448779672384\n",
            "step: 50, loss: 0.0002756406320258975\n",
            "step: 60, loss: 0.0004148985317442566\n",
            "step: 70, loss: 0.006808914244174957\n",
            "step: 80, loss: 0.012327347882091999\n",
            "step: 90, loss: 5.5249282013392076e-05\n",
            "step: 100, loss: 4.616181831806898e-05\n",
            "step: 110, loss: 0.0003137844614684582\n",
            "step: 120, loss: 0.0006865108734928071\n",
            "step: 130, loss: 0.010897754691541195\n",
            "step: 140, loss: 0.009136009961366653\n",
            "step: 150, loss: 0.00023914290068205446\n",
            "step: 160, loss: 0.001908357604406774\n",
            "step: 170, loss: 0.006786716636270285\n",
            "step: 180, loss: 0.000269969052169472\n",
            "step: 190, loss: 0.022734306752681732\n",
            "step: 200, loss: 0.0034798167180269957\n",
            "step: 210, loss: 0.0006051189266145229\n",
            "step: 220, loss: 0.0018625186057761312\n",
            "step: 230, loss: 7.19817981007509e-05\n",
            "step: 240, loss: 0.007040295749902725\n",
            "step: 250, loss: 0.0004813862906303257\n",
            "step: 260, loss: 0.0405578576028347\n",
            "step: 270, loss: 3.0326687920023687e-05\n",
            "step: 280, loss: 0.037007737904787064\n",
            "step: 290, loss: 0.03841273486614227\n",
            "step: 300, loss: 0.00011258501763222739\n",
            "step: 310, loss: 0.024701951071619987\n",
            "step: 320, loss: 0.020824836567044258\n",
            "step: 330, loss: 0.0003915833367500454\n",
            "step: 340, loss: 0.005898700095713139\n",
            "step: 350, loss: 0.003520638681948185\n",
            "step: 360, loss: 0.004448834806680679\n",
            "step: 370, loss: 0.012839782051742077\n",
            "step: 380, loss: 0.0001455462770536542\n",
            "step: 390, loss: 0.00020196553668938577\n",
            "step: 400, loss: 0.0005841788370162249\n",
            "step: 410, loss: 7.58079913794063e-05\n",
            "step: 420, loss: 4.7822271881159395e-05\n",
            "step: 430, loss: 0.00011907513544429094\n",
            "step: 440, loss: 0.003221531631425023\n",
            "step: 450, loss: 0.00017780107737053186\n",
            "step: 460, loss: 0.0003522511979099363\n",
            "step: 470, loss: 4.801083923666738e-05\n",
            "step: 480, loss: 0.000520434114150703\n",
            "step: 490, loss: 0.003524093423038721\n",
            "step: 500, loss: 0.00648772157728672\n",
            "step: 510, loss: 0.00024727245909161866\n",
            "step: 520, loss: 0.010837799869477749\n",
            "step: 530, loss: 0.00036568002542480826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9394785847299814, f1=0.9340710004610421, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.454058580449782e-05\n",
            "step: 10, loss: 4.421252015163191e-05\n",
            "step: 20, loss: 6.364652654156089e-05\n",
            "step: 30, loss: 0.00013531475269701332\n",
            "step: 40, loss: 0.0024958932772278786\n",
            "step: 50, loss: 0.0005191202508285642\n",
            "step: 60, loss: 0.0009859109995886683\n",
            "step: 70, loss: 2.6601521312841214e-05\n",
            "step: 80, loss: 3.400949208298698e-05\n",
            "step: 90, loss: 0.0023721216712146997\n",
            "step: 100, loss: 9.588438842911273e-05\n",
            "step: 110, loss: 0.00010919304622802883\n",
            "step: 120, loss: 0.00024070648942142725\n",
            "step: 130, loss: 0.000521952984854579\n",
            "step: 140, loss: 0.01036748941987753\n",
            "step: 150, loss: 0.0015949145890772343\n",
            "step: 160, loss: 2.8817916245316155e-05\n",
            "step: 170, loss: 5.8981720940209925e-05\n",
            "step: 180, loss: 0.00023081515973899513\n",
            "step: 190, loss: 0.000544873415492475\n",
            "step: 200, loss: 0.00017304430366493762\n",
            "step: 210, loss: 0.0011860972736030817\n",
            "step: 220, loss: 0.009278766810894012\n",
            "step: 230, loss: 0.0001313849788857624\n",
            "step: 240, loss: 0.01182801928371191\n",
            "step: 250, loss: 3.005020334967412e-05\n",
            "step: 260, loss: 0.0010305749019607902\n",
            "step: 270, loss: 0.0002448589075356722\n",
            "step: 280, loss: 0.00032830191776156425\n",
            "step: 290, loss: 2.936202145065181e-05\n",
            "step: 300, loss: 4.784147677128203e-05\n",
            "step: 310, loss: 0.00010681013372959569\n",
            "step: 320, loss: 0.0017713407287374139\n",
            "step: 330, loss: 0.00010952635784633458\n",
            "step: 340, loss: 0.17442962527275085\n",
            "step: 350, loss: 0.0010495488531887531\n",
            "step: 360, loss: 0.0002038375532720238\n",
            "step: 370, loss: 0.007093470543622971\n",
            "step: 380, loss: 0.0027806952130049467\n",
            "step: 390, loss: 0.0009079980663955212\n",
            "step: 400, loss: 0.0013860510662198067\n",
            "step: 410, loss: 0.0011048997985199094\n",
            "step: 420, loss: 0.008816234767436981\n",
            "step: 430, loss: 0.0006162190111353993\n",
            "step: 440, loss: 0.012708866968750954\n",
            "step: 450, loss: 0.00011291760165477172\n",
            "step: 460, loss: 0.0005043551209382713\n",
            "step: 470, loss: 0.001075942418538034\n",
            "step: 480, loss: 0.00021091099188197404\n",
            "step: 490, loss: 0.0009229364222846925\n",
            "step: 500, loss: 0.09813597798347473\n",
            "step: 510, loss: 0.0002661156468093395\n",
            "step: 520, loss: 0.004409634508192539\n",
            "step: 530, loss: 0.0007620699470862746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9348335677449601, f1=0.9333333333333333, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007697161752730608\n",
            "step: 10, loss: 0.06860776990652084\n",
            "step: 20, loss: 0.000189512618817389\n",
            "step: 30, loss: 0.0013449442340061069\n",
            "step: 40, loss: 0.0013458606554195285\n",
            "step: 50, loss: 0.0522882379591465\n",
            "step: 60, loss: 0.027614498510956764\n",
            "step: 70, loss: 0.00012275883636903018\n",
            "step: 80, loss: 0.00011720779002644122\n",
            "step: 90, loss: 0.0004550693556666374\n",
            "step: 100, loss: 0.0006398643599823117\n",
            "step: 110, loss: 0.002220471389591694\n",
            "step: 120, loss: 7.472754805348814e-05\n",
            "step: 130, loss: 7.898345938883722e-05\n",
            "step: 140, loss: 0.07773122936487198\n",
            "step: 150, loss: 0.0002799921203404665\n",
            "step: 160, loss: 0.0040560816414654255\n",
            "step: 170, loss: 0.0005378416972234845\n",
            "step: 180, loss: 0.0004696075338870287\n",
            "step: 190, loss: 0.0013175783678889275\n",
            "step: 200, loss: 0.005740666761994362\n",
            "step: 210, loss: 0.004590925760567188\n",
            "step: 220, loss: 0.004136829171329737\n",
            "step: 230, loss: 6.024260073900223e-05\n",
            "step: 240, loss: 0.0001245315943378955\n",
            "step: 250, loss: 3.889131039613858e-05\n",
            "step: 260, loss: 0.0007925268146209419\n",
            "step: 270, loss: 0.0002533411607146263\n",
            "step: 280, loss: 0.0001901772920973599\n",
            "step: 290, loss: 0.00018152742995880544\n",
            "step: 300, loss: 0.0345175601541996\n",
            "step: 310, loss: 0.00026636122493073344\n",
            "step: 320, loss: 0.00020057686197105795\n",
            "step: 330, loss: 0.0013436791487038136\n",
            "step: 340, loss: 0.00012542912736535072\n",
            "step: 350, loss: 0.0021855824161320925\n",
            "step: 360, loss: 0.0004995348863303661\n",
            "step: 370, loss: 0.009601721540093422\n",
            "step: 380, loss: 4.183123746770434e-05\n",
            "step: 390, loss: 0.00010121092782355845\n",
            "step: 400, loss: 6.525596108986065e-05\n",
            "step: 410, loss: 0.0003382716968189925\n",
            "step: 420, loss: 0.0005807711859233677\n",
            "step: 430, loss: 4.433613139553927e-05\n",
            "step: 440, loss: 0.00013685862359125167\n",
            "step: 450, loss: 6.406997999874875e-05\n",
            "step: 460, loss: 0.0037528902757912874\n",
            "step: 470, loss: 3.6857913073617965e-05\n",
            "step: 480, loss: 0.0006023449823260307\n",
            "step: 490, loss: 0.0005421192036010325\n",
            "step: 500, loss: 5.544985833694227e-05\n",
            "step: 510, loss: 0.00024409320030827075\n",
            "step: 520, loss: 0.0002741527569014579\n",
            "step: 530, loss: 4.3056763388449326e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9350163627863488, f1=0.926829268292683, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.435472798533738e-05\n",
            "step: 10, loss: 5.26384828845039e-05\n",
            "step: 20, loss: 7.65760923968628e-05\n",
            "step: 30, loss: 0.004512839950621128\n",
            "step: 40, loss: 0.0002439456438878551\n",
            "step: 50, loss: 0.021371006965637207\n",
            "step: 60, loss: 0.011648290790617466\n",
            "step: 70, loss: 0.0009974446147680283\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 5.675432475982234e-05\n",
            "step: 90, loss: 0.000160598021466285\n",
            "step: 100, loss: 5.742390567320399e-05\n",
            "step: 110, loss: 0.00020124389266129583\n",
            "step: 120, loss: 2.4679438865859993e-05\n",
            "step: 130, loss: 5.195089761400595e-05\n",
            "step: 140, loss: 2.0514620700851083e-05\n",
            "step: 150, loss: 1.8868282495532185e-05\n",
            "step: 160, loss: 3.989645483670756e-05\n",
            "step: 170, loss: 0.00012409345072228462\n",
            "step: 180, loss: 2.848939402610995e-05\n",
            "step: 190, loss: 7.310553337447345e-05\n",
            "step: 200, loss: 4.031500793644227e-05\n",
            "step: 210, loss: 6.766356091247872e-05\n",
            "step: 220, loss: 0.00010378483420936391\n",
            "step: 230, loss: 2.5189863663399592e-05\n",
            "step: 240, loss: 0.06353951245546341\n",
            "step: 250, loss: 8.698959572939202e-05\n",
            "step: 260, loss: 0.0002729384577833116\n",
            "step: 270, loss: 0.05188968777656555\n",
            "step: 280, loss: 0.0003495760611258447\n",
            "step: 290, loss: 0.017027784138917923\n",
            "step: 300, loss: 0.009394156746566296\n",
            "step: 310, loss: 0.0002947361790575087\n",
            "step: 320, loss: 0.0007806906360201538\n",
            "step: 330, loss: 0.0006880760192871094\n",
            "step: 340, loss: 5.739837069995701e-05\n",
            "step: 350, loss: 0.005129473749548197\n",
            "step: 360, loss: 0.004941971506923437\n",
            "step: 370, loss: 0.0025847756769508123\n",
            "step: 380, loss: 0.00041380972834303975\n",
            "step: 390, loss: 0.00028513651341199875\n",
            "step: 400, loss: 0.12568283081054688\n",
            "step: 410, loss: 0.004356415942311287\n",
            "step: 420, loss: 0.014074563048779964\n",
            "step: 430, loss: 0.01524368766695261\n",
            "step: 440, loss: 0.0013096253387629986\n",
            "step: 450, loss: 0.0007511820294894278\n",
            "step: 460, loss: 0.0031034129206091166\n",
            "step: 470, loss: 0.0006226947880350053\n",
            "step: 480, loss: 0.002003444591537118\n",
            "step: 490, loss: 0.0007466258830390871\n",
            "step: 500, loss: 0.000148570048622787\n",
            "step: 510, loss: 0.017333460971713066\n",
            "step: 520, loss: 0.0010805519996210933\n",
            "step: 530, loss: 0.0004632060881704092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9330855018587362, f1=0.9276830953477659, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00035129438037984073\n",
            "step: 10, loss: 0.006358546670526266\n",
            "step: 20, loss: 0.01981649361550808\n",
            "step: 30, loss: 0.00016375964332837611\n",
            "step: 40, loss: 0.0001306662888964638\n",
            "step: 50, loss: 0.0016697155078873038\n",
            "step: 60, loss: 0.0008342606597580016\n",
            "step: 70, loss: 9.239177597919479e-05\n",
            "step: 80, loss: 0.0004056835896335542\n",
            "step: 90, loss: 0.03287338837981224\n",
            "step: 100, loss: 0.008627488277852535\n",
            "step: 110, loss: 0.0029804708901792765\n",
            "step: 120, loss: 0.0006515515269711614\n",
            "step: 130, loss: 0.00017377604672219604\n",
            "step: 140, loss: 0.0002780131471809\n",
            "step: 150, loss: 0.0005648695514537394\n",
            "step: 160, loss: 0.004382043145596981\n",
            "step: 170, loss: 0.00026393510052002966\n",
            "step: 180, loss: 0.0003787569294217974\n",
            "step: 190, loss: 0.00012455953401513398\n",
            "step: 200, loss: 3.3467047614976764e-05\n",
            "step: 210, loss: 5.910408799536526e-05\n",
            "step: 220, loss: 0.00015867926413193345\n",
            "step: 230, loss: 0.0004118185315746814\n",
            "step: 240, loss: 0.03134601190686226\n",
            "step: 250, loss: 3.0139999580569565e-05\n",
            "step: 260, loss: 4.963453102391213e-05\n",
            "step: 270, loss: 2.194517946918495e-05\n",
            "step: 280, loss: 0.000356975884642452\n",
            "step: 290, loss: 0.0003779747348744422\n",
            "step: 300, loss: 2.9551845727837645e-05\n",
            "step: 310, loss: 0.003247002372518182\n",
            "step: 320, loss: 0.0016312624793499708\n",
            "step: 330, loss: 0.0020500710234045982\n",
            "step: 340, loss: 0.004532813560217619\n",
            "step: 350, loss: 0.01653623767197132\n",
            "step: 360, loss: 0.0009276469936594367\n",
            "step: 370, loss: 0.00036204984644427896\n",
            "step: 380, loss: 0.0013401854084804654\n",
            "step: 390, loss: 0.041708238422870636\n",
            "step: 400, loss: 0.00029365590307861567\n",
            "step: 410, loss: 0.000869989744387567\n",
            "step: 420, loss: 6.568984099430963e-05\n",
            "step: 430, loss: 0.00879593100398779\n",
            "step: 440, loss: 0.00021004687005188316\n",
            "step: 450, loss: 0.0013452075654640794\n",
            "step: 460, loss: 0.0026928421575576067\n",
            "step: 470, loss: 5.6705743190832436e-05\n",
            "step: 480, loss: 0.00018218497280031443\n",
            "step: 490, loss: 0.001124494126997888\n",
            "step: 500, loss: 0.0010373125551268458\n",
            "step: 510, loss: 0.0004455716407392174\n",
            "step: 520, loss: 0.005759123247116804\n",
            "step: 530, loss: 5.523742220248096e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9345794392523366, f1=0.9312267657992565, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017421635857317597\n",
            "step: 10, loss: 3.2352756534237415e-05\n",
            "step: 20, loss: 0.0002985569299198687\n",
            "step: 30, loss: 0.00012598819739650935\n",
            "step: 40, loss: 0.00017096014926210046\n",
            "step: 50, loss: 0.00023264587798621505\n",
            "step: 60, loss: 6.622139335377142e-05\n",
            "step: 70, loss: 0.00024054590903688222\n",
            "step: 80, loss: 5.024009442422539e-05\n",
            "step: 90, loss: 2.428444531688001e-05\n",
            "step: 100, loss: 0.00037381291622295976\n",
            "step: 110, loss: 5.6779084843583405e-05\n",
            "step: 120, loss: 0.000296086713206023\n",
            "step: 130, loss: 0.0404512956738472\n",
            "step: 140, loss: 0.0009816192323341966\n",
            "step: 150, loss: 0.0007836492732167244\n",
            "step: 160, loss: 0.0005533905932679772\n",
            "step: 170, loss: 0.0003464118344709277\n",
            "step: 180, loss: 5.7293891586596146e-05\n",
            "step: 190, loss: 0.000621766725089401\n",
            "step: 200, loss: 0.000789186917245388\n",
            "step: 210, loss: 3.5249879147158936e-05\n",
            "step: 220, loss: 0.002676336793228984\n",
            "step: 230, loss: 0.002437853952869773\n",
            "step: 240, loss: 0.00012750955647788942\n",
            "step: 250, loss: 8.299916225951165e-05\n",
            "step: 260, loss: 0.0003866908955387771\n",
            "step: 270, loss: 0.0001239673438249156\n",
            "step: 280, loss: 4.5017288357485086e-05\n",
            "step: 290, loss: 0.000180298593477346\n",
            "step: 300, loss: 0.0007676705718040466\n",
            "step: 310, loss: 0.054982613772153854\n",
            "step: 320, loss: 0.024825716391205788\n",
            "step: 330, loss: 0.000978928990662098\n",
            "step: 340, loss: 0.0008342408109456301\n",
            "step: 350, loss: 0.00021806989389006048\n",
            "step: 360, loss: 0.08189328014850616\n",
            "step: 370, loss: 2.3226642952067778e-05\n",
            "step: 380, loss: 4.980304220225662e-05\n",
            "step: 390, loss: 0.026337746530771255\n",
            "step: 400, loss: 0.0007920251809991896\n",
            "step: 410, loss: 4.129300941713154e-05\n",
            "step: 420, loss: 0.00012182573118479922\n",
            "step: 430, loss: 8.701248589204624e-05\n",
            "step: 440, loss: 0.0026046892162412405\n",
            "step: 450, loss: 0.0007194186910055578\n",
            "step: 460, loss: 0.001530562760308385\n",
            "step: 470, loss: 6.401701830327511e-05\n",
            "step: 480, loss: 0.0003941963950637728\n",
            "step: 490, loss: 0.0008849675068631768\n",
            "step: 500, loss: 5.48542921023909e-05\n",
            "step: 510, loss: 0.0010149864247068763\n",
            "step: 520, loss: 0.0005220783059485257\n",
            "step: 530, loss: 7.683315925532952e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.937528921795465, f1=0.9306384933394579, best_f1=0.9340710004610421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.359236507094465e-05\n",
            "step: 10, loss: 3.200231367372908e-05\n",
            "step: 20, loss: 0.0010074661113321781\n",
            "step: 30, loss: 0.002411931985989213\n",
            "step: 40, loss: 0.05208873003721237\n",
            "step: 50, loss: 0.0689588338136673\n",
            "step: 60, loss: 5.935559966019355e-05\n",
            "step: 70, loss: 0.0019506418611854315\n",
            "step: 80, loss: 0.00037348348996602\n",
            "step: 90, loss: 2.8102529540774412e-05\n",
            "step: 100, loss: 4.994125265511684e-05\n",
            "step: 110, loss: 0.012945085763931274\n",
            "step: 120, loss: 0.006390544585883617\n",
            "step: 130, loss: 0.11271315813064575\n",
            "step: 140, loss: 0.0002422169636702165\n",
            "step: 150, loss: 4.550907760858536e-05\n",
            "step: 160, loss: 0.00010635854414431378\n",
            "step: 170, loss: 7.659800758119673e-05\n",
            "step: 180, loss: 4.4533953769132495e-05\n",
            "step: 190, loss: 0.00031806787592358887\n",
            "step: 200, loss: 7.309420470846817e-05\n",
            "step: 210, loss: 0.000795798609033227\n",
            "step: 220, loss: 0.037366122007369995\n",
            "step: 230, loss: 0.001104669296182692\n",
            "step: 240, loss: 0.0034633981995284557\n",
            "step: 250, loss: 0.00032319052843376994\n",
            "step: 260, loss: 0.0002137532428605482\n",
            "step: 270, loss: 2.9966895453981124e-05\n",
            "step: 280, loss: 0.007704245392233133\n",
            "step: 290, loss: 2.0037889044033363e-05\n",
            "step: 300, loss: 2.000797030632384e-05\n",
            "step: 310, loss: 0.00025842711329460144\n",
            "step: 320, loss: 0.0008973596268333495\n",
            "step: 330, loss: 0.00016521039651706815\n",
            "step: 340, loss: 7.065061072353274e-05\n",
            "step: 350, loss: 5.497105303220451e-05\n",
            "step: 360, loss: 0.000584669120144099\n",
            "step: 370, loss: 9.089898958336562e-05\n",
            "step: 380, loss: 5.349602724891156e-05\n",
            "step: 390, loss: 0.00011833666212623939\n",
            "step: 400, loss: 4.6865508920745924e-05\n",
            "step: 410, loss: 0.00013759502326138318\n",
            "step: 420, loss: 0.0009303619153797626\n",
            "step: 430, loss: 0.00025126218679361045\n",
            "step: 440, loss: 0.0005013871123082936\n",
            "step: 450, loss: 5.844051338499412e-05\n",
            "step: 460, loss: 6.379274418577552e-05\n",
            "step: 470, loss: 0.003371267346665263\n",
            "step: 480, loss: 0.0002341497311135754\n",
            "step: 490, loss: 2.3472270186175592e-05\n",
            "step: 500, loss: 0.00038282445166260004\n",
            "step: 510, loss: 0.001078105764463544\n",
            "step: 520, loss: 4.2990741349058226e-05\n",
            "step: 530, loss: 0.00031127355759963393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.938134810710988, f1=0.9327231121281464, best_f1=0.9340710004610421\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:31, 182.09it/s]\n",
            "load_f1 = 0.9342723004694835\n",
            "real_f1 = 0.9341486359360301\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 182.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022c5c6c-e28a-4807-fba2-b17efc8fb573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5652856826782227\n",
            "step: 10, loss: 0.3907673954963684\n",
            "step: 20, loss: 0.3769378960132599\n",
            "step: 30, loss: 0.2921670079231262\n",
            "step: 40, loss: 0.15990251302719116\n",
            "step: 50, loss: 0.34373199939727783\n",
            "step: 60, loss: 0.13302618265151978\n",
            "step: 70, loss: 0.19179973006248474\n",
            "step: 80, loss: 0.22902096807956696\n",
            "step: 90, loss: 0.3585101366043091\n",
            "step: 100, loss: 0.5478050708770752\n",
            "step: 110, loss: 0.265102744102478\n",
            "step: 120, loss: 0.38448017835617065\n",
            "step: 130, loss: 0.20309247076511383\n",
            "step: 140, loss: 0.1883535534143448\n",
            "step: 150, loss: 0.16657309234142303\n",
            "step: 160, loss: 0.25122445821762085\n",
            "step: 170, loss: 0.2537182569503784\n",
            "step: 180, loss: 0.11301770806312561\n",
            "step: 190, loss: 0.17340192198753357\n",
            "step: 200, loss: 0.19866417348384857\n",
            "step: 210, loss: 0.2054128795862198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6484375, f1=0.6769825918762089, best_f1=0.6769825918762089\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09086306393146515\n",
            "step: 10, loss: 0.26500314474105835\n",
            "step: 20, loss: 0.21525613963603973\n",
            "step: 30, loss: 0.17670148611068726\n",
            "step: 40, loss: 0.15084326267242432\n",
            "step: 50, loss: 0.12640506029129028\n",
            "step: 60, loss: 0.4048025906085968\n",
            "step: 70, loss: 0.14677786827087402\n",
            "step: 80, loss: 0.14890938997268677\n",
            "step: 90, loss: 0.07325001806020737\n",
            "step: 100, loss: 0.012609010562300682\n",
            "step: 110, loss: 0.1326940804719925\n",
            "step: 120, loss: 0.194444939494133\n",
            "step: 130, loss: 0.04046957567334175\n",
            "step: 140, loss: 0.1825299859046936\n",
            "step: 150, loss: 0.24761606752872467\n",
            "step: 160, loss: 0.16477146744728088\n",
            "step: 170, loss: 0.11513444781303406\n",
            "step: 180, loss: 0.3088395297527313\n",
            "step: 190, loss: 0.17683729529380798\n",
            "step: 200, loss: 0.05713687092065811\n",
            "step: 210, loss: 0.198047935962677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6875, f1=0.6918238993710691, best_f1=0.6918238993710691\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05261028930544853\n",
            "step: 10, loss: 0.1586970090866089\n",
            "step: 20, loss: 0.1735595166683197\n",
            "step: 30, loss: 0.15817150473594666\n",
            "step: 40, loss: 0.058478400111198425\n",
            "step: 50, loss: 0.05877547711133957\n",
            "step: 60, loss: 0.24301321804523468\n",
            "step: 70, loss: 0.030456680804491043\n",
            "step: 80, loss: 0.12822431325912476\n",
            "step: 90, loss: 0.04621618986129761\n",
            "step: 100, loss: 0.1479964554309845\n",
            "step: 110, loss: 0.08152537047863007\n",
            "step: 120, loss: 0.18420270085334778\n",
            "step: 130, loss: 0.21009789407253265\n",
            "step: 140, loss: 0.1012144461274147\n",
            "step: 150, loss: 0.2573927342891693\n",
            "step: 160, loss: 0.01280668843537569\n",
            "step: 170, loss: 0.10003358125686646\n",
            "step: 180, loss: 0.054406650364398956\n",
            "step: 190, loss: 0.19407247006893158\n",
            "step: 200, loss: 0.0963604524731636\n",
            "step: 210, loss: 0.14001429080963135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6963249516441006, f1=0.7137254901960784, best_f1=0.7137254901960784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09790296852588654\n",
            "step: 10, loss: 0.08643648773431778\n",
            "step: 20, loss: 0.21335488557815552\n",
            "step: 30, loss: 0.10321828722953796\n",
            "step: 40, loss: 0.026657624170184135\n",
            "step: 50, loss: 0.14033056795597076\n",
            "step: 60, loss: 0.28737935423851013\n",
            "step: 70, loss: 0.17332793772220612\n",
            "step: 80, loss: 0.045533012598752975\n",
            "step: 90, loss: 0.03182852268218994\n",
            "step: 100, loss: 0.19412310421466827\n",
            "step: 110, loss: 0.09091690182685852\n",
            "step: 120, loss: 0.14129887521266937\n",
            "step: 130, loss: 0.25271955132484436\n",
            "step: 140, loss: 0.09889674186706543\n",
            "step: 150, loss: 0.06235896050930023\n",
            "step: 160, loss: 0.01631062850356102\n",
            "step: 170, loss: 0.22615444660186768\n",
            "step: 180, loss: 0.5887908935546875\n",
            "step: 190, loss: 0.11085732281208038\n",
            "step: 200, loss: 0.1552671641111374\n",
            "step: 210, loss: 0.299483060836792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6694915254237288, f1=0.6877637130801688, best_f1=0.7137254901960784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13726630806922913\n",
            "step: 10, loss: 0.09424791485071182\n",
            "step: 20, loss: 0.15198804438114166\n",
            "step: 30, loss: 0.022287892177700996\n",
            "step: 40, loss: 0.2959010601043701\n",
            "step: 50, loss: 0.0672507956624031\n",
            "step: 60, loss: 0.08596711605787277\n",
            "step: 70, loss: 0.09874825179576874\n",
            "step: 80, loss: 0.017470940947532654\n",
            "step: 90, loss: 0.20592668652534485\n",
            "step: 100, loss: 0.003996573388576508\n",
            "step: 110, loss: 0.11664550006389618\n",
            "step: 120, loss: 0.09889265894889832\n",
            "step: 130, loss: 0.07428630441427231\n",
            "step: 140, loss: 0.04242611303925514\n",
            "step: 150, loss: 0.03543508052825928\n",
            "step: 160, loss: 0.22040468454360962\n",
            "step: 170, loss: 0.055949967354536057\n",
            "step: 180, loss: 0.11412177234888077\n",
            "step: 190, loss: 0.06814837455749512\n",
            "step: 200, loss: 0.083650141954422\n",
            "step: 210, loss: 0.02408229373395443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.696798493408663, f1=0.701492537313433, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03935646638274193\n",
            "step: 10, loss: 0.04559030756354332\n",
            "step: 20, loss: 0.058024339377880096\n",
            "step: 30, loss: 0.002080114558339119\n",
            "step: 40, loss: 0.023844629526138306\n",
            "step: 50, loss: 0.006660402752459049\n",
            "step: 60, loss: 0.30838829278945923\n",
            "step: 70, loss: 0.0500839538872242\n",
            "step: 80, loss: 0.2286207377910614\n",
            "step: 90, loss: 0.08929130434989929\n",
            "step: 100, loss: 0.03042975254356861\n",
            "step: 110, loss: 0.1036205068230629\n",
            "step: 120, loss: 0.1324075311422348\n",
            "step: 130, loss: 0.11004579812288284\n",
            "step: 140, loss: 0.13713814318180084\n",
            "step: 150, loss: 0.007469921838492155\n",
            "step: 160, loss: 0.0035563965793699026\n",
            "step: 170, loss: 0.14765632152557373\n",
            "step: 180, loss: 0.02587031200528145\n",
            "step: 190, loss: 0.19166824221611023\n",
            "step: 200, loss: 0.004672127775847912\n",
            "step: 210, loss: 0.042625393718481064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.674698795180723, f1=0.6707818930041152, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005826172884553671\n",
            "step: 10, loss: 0.05333897843956947\n",
            "step: 20, loss: 0.026358995586633682\n",
            "step: 30, loss: 0.03513123095035553\n",
            "step: 40, loss: 0.07039960473775864\n",
            "step: 50, loss: 0.052981700748205185\n",
            "step: 60, loss: 0.0398930124938488\n",
            "step: 70, loss: 0.023088403046131134\n",
            "step: 80, loss: 0.09701993316411972\n",
            "step: 90, loss: 0.0462910495698452\n",
            "step: 100, loss: 0.0062019783072173595\n",
            "step: 110, loss: 0.15396836400032043\n",
            "step: 120, loss: 0.26065900921821594\n",
            "step: 130, loss: 0.03413809463381767\n",
            "step: 140, loss: 0.004312231671065092\n",
            "step: 150, loss: 0.010979575105011463\n",
            "step: 160, loss: 0.03449005261063576\n",
            "step: 170, loss: 0.1275259256362915\n",
            "step: 180, loss: 0.030738260596990585\n",
            "step: 190, loss: 0.13738718628883362\n",
            "step: 200, loss: 0.010047446936368942\n",
            "step: 210, loss: 0.06365258246660233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6613545816733067, f1=0.6653306613226453, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.044730622321367264\n",
            "step: 10, loss: 0.08082811534404755\n",
            "step: 20, loss: 0.1056201383471489\n",
            "step: 30, loss: 0.11674336344003677\n",
            "step: 40, loss: 0.008841478265821934\n",
            "step: 50, loss: 0.01088839489966631\n",
            "step: 60, loss: 0.1199108436703682\n",
            "step: 70, loss: 0.011371787637472153\n",
            "step: 80, loss: 0.026220355182886124\n",
            "step: 90, loss: 0.003658856963738799\n",
            "step: 100, loss: 0.08172465115785599\n",
            "step: 110, loss: 0.055743258446455\n",
            "step: 120, loss: 0.006054713856428862\n",
            "step: 130, loss: 0.012135591357946396\n",
            "step: 140, loss: 0.06058010086417198\n",
            "step: 150, loss: 0.023784618824720383\n",
            "step: 160, loss: 0.038437824696302414\n",
            "step: 170, loss: 0.029849810525774956\n",
            "step: 180, loss: 0.05508897826075554\n",
            "step: 190, loss: 0.010166436433792114\n",
            "step: 200, loss: 0.002368057845160365\n",
            "step: 210, loss: 0.22949719429016113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6680000000000001, f1=0.6787878787878788, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012740237638354301\n",
            "step: 10, loss: 0.09815485030412674\n",
            "step: 20, loss: 0.006413740571588278\n",
            "step: 30, loss: 0.023754987865686417\n",
            "step: 40, loss: 0.06885318458080292\n",
            "step: 50, loss: 0.009688016027212143\n",
            "step: 60, loss: 0.26880231499671936\n",
            "step: 70, loss: 0.010935651138424873\n",
            "step: 80, loss: 0.006376605946570635\n",
            "step: 90, loss: 0.004723094869405031\n",
            "step: 100, loss: 0.02867664210498333\n",
            "step: 110, loss: 0.01682164892554283\n",
            "step: 120, loss: 0.010623996146023273\n",
            "step: 130, loss: 0.007983023300766945\n",
            "step: 140, loss: 0.07327984273433685\n",
            "step: 150, loss: 0.0581275038421154\n",
            "step: 160, loss: 0.016786864027380943\n",
            "step: 170, loss: 0.011582969687879086\n",
            "step: 180, loss: 0.043652232736349106\n",
            "step: 190, loss: 0.00483017647638917\n",
            "step: 200, loss: 0.08683383464813232\n",
            "step: 210, loss: 0.03628423810005188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6651982378854626, f1=0.6546275395033859, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03515547513961792\n",
            "step: 10, loss: 0.07486793398857117\n",
            "step: 20, loss: 0.0010720898862928152\n",
            "step: 30, loss: 0.09961601346731186\n",
            "step: 40, loss: 0.00861481111496687\n",
            "step: 50, loss: 0.016121286898851395\n",
            "step: 60, loss: 0.08905156701803207\n",
            "step: 70, loss: 0.208916574716568\n",
            "step: 80, loss: 0.026269666850566864\n",
            "step: 90, loss: 0.20811666548252106\n",
            "step: 100, loss: 0.12106245011091232\n",
            "step: 110, loss: 0.01711735688149929\n",
            "step: 120, loss: 0.009282526560127735\n",
            "step: 130, loss: 0.026314876973628998\n",
            "step: 140, loss: 0.03674235939979553\n",
            "step: 150, loss: 0.04782969877123833\n",
            "step: 160, loss: 0.010045405477285385\n",
            "step: 170, loss: 0.006759746000170708\n",
            "step: 180, loss: 0.13227058947086334\n",
            "step: 190, loss: 0.07479646056890488\n",
            "step: 200, loss: 0.01117662899196148\n",
            "step: 210, loss: 0.06805715709924698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6826568265682657, f1=0.6593001841620626, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018836595118045807\n",
            "step: 10, loss: 0.14545203745365143\n",
            "step: 20, loss: 0.1078914999961853\n",
            "step: 30, loss: 0.001704772817902267\n",
            "step: 40, loss: 0.028851358219981194\n",
            "step: 50, loss: 0.00787155982106924\n",
            "step: 60, loss: 0.0031019444577395916\n",
            "step: 70, loss: 0.004058328922837973\n",
            "step: 80, loss: 0.05911487713456154\n",
            "step: 90, loss: 0.05999954417347908\n",
            "step: 100, loss: 0.018253155052661896\n",
            "step: 110, loss: 0.2092432975769043\n",
            "step: 120, loss: 0.19366838037967682\n",
            "step: 130, loss: 0.00787840411067009\n",
            "step: 140, loss: 0.002275738399475813\n",
            "step: 150, loss: 0.0026144967414438725\n",
            "step: 160, loss: 0.01036666426807642\n",
            "step: 170, loss: 0.004344361834228039\n",
            "step: 180, loss: 0.0031032655388116837\n",
            "step: 190, loss: 0.019318653270602226\n",
            "step: 200, loss: 0.0010882215574383736\n",
            "step: 210, loss: 0.007829640060663223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6710526315789473, f1=0.662280701754386, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15441584587097168\n",
            "step: 10, loss: 0.00041464975220151246\n",
            "step: 20, loss: 0.042608827352523804\n",
            "step: 30, loss: 0.003674035659059882\n",
            "step: 40, loss: 0.06078360974788666\n",
            "step: 50, loss: 0.0013467412209138274\n",
            "step: 60, loss: 0.06792419403791428\n",
            "step: 70, loss: 0.0007953917374834418\n",
            "step: 80, loss: 0.02109525352716446\n",
            "step: 90, loss: 0.044288598001003265\n",
            "step: 100, loss: 0.019009245559573174\n",
            "step: 110, loss: 0.00826225895434618\n",
            "step: 120, loss: 0.006801344454288483\n",
            "step: 130, loss: 0.013258401304483414\n",
            "step: 140, loss: 0.04606947675347328\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 150, loss: 0.020366529002785683\n",
            "step: 160, loss: 0.012261942960321903\n",
            "step: 170, loss: 0.005278879776597023\n",
            "step: 180, loss: 0.015064878389239311\n",
            "step: 190, loss: 0.12608259916305542\n",
            "step: 200, loss: 0.008177515119314194\n",
            "step: 210, loss: 0.028264304623007774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6653992395437263, f1=0.6577437858508605, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05513427406549454\n",
            "step: 10, loss: 0.007641611620783806\n",
            "step: 20, loss: 0.01275221910327673\n",
            "step: 30, loss: 0.08130696415901184\n",
            "step: 40, loss: 0.09095196425914764\n",
            "step: 50, loss: 0.0017986692255362868\n",
            "step: 60, loss: 0.047360360622406006\n",
            "step: 70, loss: 0.04620056971907616\n",
            "step: 80, loss: 0.02392730675637722\n",
            "step: 90, loss: 0.002611400792375207\n",
            "step: 100, loss: 0.011087877675890923\n",
            "step: 110, loss: 0.014775658957660198\n",
            "step: 120, loss: 0.003154212376102805\n",
            "step: 130, loss: 0.0010525044053792953\n",
            "step: 140, loss: 0.051223449409008026\n",
            "step: 150, loss: 0.021846050396561623\n",
            "step: 160, loss: 0.003623134922236204\n",
            "step: 170, loss: 0.011365135200321674\n",
            "step: 180, loss: 0.028846630826592445\n",
            "step: 190, loss: 0.004128532949835062\n",
            "step: 200, loss: 0.012836672365665436\n",
            "step: 210, loss: 0.03666891157627106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.679324894514768, f1=0.6767895878524947, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008016815991140902\n",
            "step: 10, loss: 0.02651875466108322\n",
            "step: 20, loss: 0.001996008912101388\n",
            "step: 30, loss: 0.0029966384172439575\n",
            "step: 40, loss: 0.08377201110124588\n",
            "step: 50, loss: 0.05492750555276871\n",
            "step: 60, loss: 0.009741355665028095\n",
            "step: 70, loss: 0.0030334703624248505\n",
            "step: 80, loss: 0.0012430183123797178\n",
            "step: 90, loss: 0.03278113529086113\n",
            "step: 100, loss: 0.0035315747372806072\n",
            "step: 110, loss: 0.011633730493485928\n",
            "step: 120, loss: 0.04734884202480316\n",
            "step: 130, loss: 0.052552852779626846\n",
            "step: 140, loss: 0.01889384165406227\n",
            "step: 150, loss: 0.00045076131937094033\n",
            "step: 160, loss: 0.008613495156168938\n",
            "step: 170, loss: 0.0011528534814715385\n",
            "step: 180, loss: 0.020728429779410362\n",
            "step: 190, loss: 0.003079934511333704\n",
            "step: 200, loss: 0.020796209573745728\n",
            "step: 210, loss: 0.027434833347797394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6779661016949152, f1=0.6724890829694323, best_f1=0.701492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018847546307370067\n",
            "step: 10, loss: 0.0004362166509963572\n",
            "step: 20, loss: 0.0022657299414277077\n",
            "step: 30, loss: 0.16841654479503632\n",
            "step: 40, loss: 0.0017538656247779727\n",
            "step: 50, loss: 0.0014327747048810124\n",
            "step: 60, loss: 0.03601112589240074\n",
            "step: 70, loss: 0.0027955954428762197\n",
            "step: 80, loss: 0.0022851969115436077\n",
            "step: 90, loss: 0.0191498976200819\n",
            "step: 100, loss: 0.004589024931192398\n",
            "step: 110, loss: 0.00031852288520894945\n",
            "step: 120, loss: 0.008587832562625408\n",
            "step: 130, loss: 0.06624113023281097\n",
            "step: 140, loss: 0.00039658925379626453\n",
            "step: 150, loss: 0.0022045923396945\n",
            "step: 160, loss: 0.008546956814825535\n",
            "step: 170, loss: 0.0006160775083117187\n",
            "step: 180, loss: 0.0007602993282489479\n",
            "step: 190, loss: 0.051203321665525436\n",
            "step: 200, loss: 0.005925463046878576\n",
            "step: 210, loss: 0.010023328475654125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6720647773279351, f1=0.673469387755102, best_f1=0.701492537313433\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 278.96it/s]\n",
            "load_f1 = 0.6946107784431138\n",
            "real_f1 = 0.6956521739130436\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 178.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff143d4-503f-4bbf-9a73-3d989ce1b730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5782621502876282\n",
            "step: 10, loss: 0.354979544878006\n",
            "step: 20, loss: 0.2731013000011444\n",
            "step: 30, loss: 0.43319621682167053\n",
            "step: 40, loss: 0.43185487389564514\n",
            "step: 50, loss: 0.29107028245925903\n",
            "step: 60, loss: 0.25781431794166565\n",
            "step: 70, loss: 0.29815489053726196\n",
            "step: 80, loss: 0.26085326075553894\n",
            "step: 90, loss: 0.2669897973537445\n",
            "step: 100, loss: 0.30330130457878113\n",
            "step: 110, loss: 0.34197261929512024\n",
            "step: 120, loss: 0.09649531543254852\n",
            "step: 130, loss: 0.09927815943956375\n",
            "step: 140, loss: 0.06911297142505646\n",
            "step: 150, loss: 0.24752597510814667\n",
            "step: 160, loss: 0.1067902147769928\n",
            "step: 170, loss: 0.2119292914867401\n",
            "step: 180, loss: 0.03482069820165634\n",
            "step: 190, loss: 0.1881684958934784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6733668341708543, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2998875379562378\n",
            "step: 10, loss: 0.044975169003009796\n",
            "step: 20, loss: 0.12249886989593506\n",
            "step: 30, loss: 0.2880551815032959\n",
            "step: 40, loss: 0.14970912039279938\n",
            "step: 50, loss: 0.07896122336387634\n",
            "step: 60, loss: 0.24194645881652832\n",
            "step: 70, loss: 0.1769617795944214\n",
            "step: 80, loss: 0.18135465681552887\n",
            "step: 90, loss: 0.3552413582801819\n",
            "step: 100, loss: 0.012356016784906387\n",
            "step: 110, loss: 0.060385413467884064\n",
            "step: 120, loss: 0.27190616726875305\n",
            "step: 130, loss: 0.06648913025856018\n",
            "step: 140, loss: 0.04197143763303757\n",
            "step: 150, loss: 0.09708297252655029\n",
            "step: 160, loss: 0.029514294117689133\n",
            "step: 170, loss: 0.12260895222425461\n",
            "step: 180, loss: 0.22709667682647705\n",
            "step: 190, loss: 0.02003883197903633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8010610079575596, f1=0.7896103896103895, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03526846319437027\n",
            "step: 10, loss: 0.0501880906522274\n",
            "step: 20, loss: 0.09451814740896225\n",
            "step: 30, loss: 0.017887573689222336\n",
            "step: 40, loss: 0.07958859950304031\n",
            "step: 50, loss: 0.24643279612064362\n",
            "step: 60, loss: 0.003871069522574544\n",
            "step: 70, loss: 0.06910131871700287\n",
            "step: 80, loss: 0.015190882608294487\n",
            "step: 90, loss: 0.10757546871900558\n",
            "step: 100, loss: 0.05823846906423569\n",
            "step: 110, loss: 0.1395512968301773\n",
            "step: 120, loss: 0.07640766352415085\n",
            "step: 130, loss: 0.008134079165756702\n",
            "step: 140, loss: 0.013965423218905926\n",
            "step: 150, loss: 0.10463521629571915\n",
            "step: 160, loss: 0.03618304431438446\n",
            "step: 170, loss: 0.07111161947250366\n",
            "step: 180, loss: 0.009630517102777958\n",
            "step: 190, loss: 0.12567846477031708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7934782608695652, f1=0.8169761273209548, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03068314865231514\n",
            "step: 10, loss: 0.13601000607013702\n",
            "step: 20, loss: 0.04649369791150093\n",
            "step: 30, loss: 0.011709353886544704\n",
            "step: 40, loss: 0.034868620336055756\n",
            "step: 50, loss: 0.023539040237665176\n",
            "step: 60, loss: 0.10473117977380753\n",
            "step: 70, loss: 0.3529830574989319\n",
            "step: 80, loss: 0.20885398983955383\n",
            "step: 90, loss: 0.010671332478523254\n",
            "step: 100, loss: 0.11645162105560303\n",
            "step: 110, loss: 0.04253696650266647\n",
            "step: 120, loss: 0.09452421963214874\n",
            "step: 130, loss: 0.07309078425168991\n",
            "step: 140, loss: 0.0878283903002739\n",
            "step: 150, loss: 0.12723170220851898\n",
            "step: 160, loss: 0.0066381958313286304\n",
            "step: 170, loss: 0.0693470910191536\n",
            "step: 180, loss: 0.01544719934463501\n",
            "step: 190, loss: 0.17501147091388702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7704918032786885, f1=0.7786666666666667, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1806756854057312\n",
            "step: 10, loss: 0.013895973563194275\n",
            "step: 20, loss: 0.04695356264710426\n",
            "step: 30, loss: 0.00511049572378397\n",
            "step: 40, loss: 0.06158332899212837\n",
            "step: 50, loss: 0.1047038659453392\n",
            "step: 60, loss: 0.11234328150749207\n",
            "step: 70, loss: 0.019676096737384796\n",
            "step: 80, loss: 0.013144111260771751\n",
            "step: 90, loss: 0.019158923998475075\n",
            "step: 100, loss: 0.005917536094784737\n",
            "step: 110, loss: 0.009909010492265224\n",
            "step: 120, loss: 0.0037628142163157463\n",
            "step: 130, loss: 0.11420723795890808\n",
            "step: 140, loss: 0.05832366645336151\n",
            "step: 150, loss: 0.04800385981798172\n",
            "step: 160, loss: 0.011142592877149582\n",
            "step: 170, loss: 0.0041717663407325745\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.12532973289489746\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 190, loss: 0.03594290092587471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7913279132791328, f1=0.8191489361702128, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026769591495394707\n",
            "step: 10, loss: 0.15794119238853455\n",
            "step: 20, loss: 0.03943765535950661\n",
            "step: 30, loss: 0.0019378849538043141\n",
            "step: 40, loss: 0.056069422513246536\n",
            "step: 50, loss: 0.004031339660286903\n",
            "step: 60, loss: 0.03272628411650658\n",
            "step: 70, loss: 0.004784054588526487\n",
            "step: 80, loss: 0.020683182403445244\n",
            "step: 90, loss: 0.018508248031139374\n",
            "step: 100, loss: 0.0014023496769368649\n",
            "step: 110, loss: 0.1263478547334671\n",
            "step: 120, loss: 0.014199376106262207\n",
            "step: 130, loss: 0.008415773510932922\n",
            "step: 140, loss: 0.0455123595893383\n",
            "step: 150, loss: 0.002087977947667241\n",
            "step: 160, loss: 0.035590920597314835\n",
            "step: 170, loss: 0.0270572267472744\n",
            "step: 180, loss: 0.02049342915415764\n",
            "step: 190, loss: 0.013333415612578392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7780821917808218, f1=0.7862796833773087, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032642255537211895\n",
            "step: 10, loss: 0.01004622969776392\n",
            "step: 20, loss: 0.17812985181808472\n",
            "step: 30, loss: 0.0057601542212069035\n",
            "step: 40, loss: 0.006415836047381163\n",
            "step: 50, loss: 0.011516694910824299\n",
            "step: 60, loss: 0.0058692279271781445\n",
            "step: 70, loss: 0.0029511938337236643\n",
            "step: 80, loss: 0.02931848168373108\n",
            "step: 90, loss: 0.008123956620693207\n",
            "step: 100, loss: 0.0010464643128216267\n",
            "step: 110, loss: 0.050682276487350464\n",
            "step: 120, loss: 0.0023597029503434896\n",
            "step: 130, loss: 0.001045609824359417\n",
            "step: 140, loss: 0.0009542639018036425\n",
            "step: 150, loss: 0.011891862377524376\n",
            "step: 160, loss: 0.036210767924785614\n",
            "step: 170, loss: 0.14013753831386566\n",
            "step: 180, loss: 0.015934107825160027\n",
            "step: 190, loss: 0.002941161161288619\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7828418230563003, f1=0.7795698924731183, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026064130943268538\n",
            "step: 10, loss: 0.049032747745513916\n",
            "step: 20, loss: 0.004423834849148989\n",
            "step: 30, loss: 0.09288257360458374\n",
            "step: 40, loss: 0.01563139632344246\n",
            "step: 50, loss: 0.10040686279535294\n",
            "step: 60, loss: 0.06735201925039291\n",
            "step: 70, loss: 0.005646057426929474\n",
            "step: 80, loss: 0.0007263813749887049\n",
            "step: 90, loss: 0.0006727800937369466\n",
            "step: 100, loss: 0.027194317430257797\n",
            "step: 110, loss: 0.008846556767821312\n",
            "step: 120, loss: 0.004384610336273909\n",
            "step: 130, loss: 0.0005325052770785987\n",
            "step: 140, loss: 0.004664693959057331\n",
            "step: 150, loss: 0.007151535712182522\n",
            "step: 160, loss: 0.0005596785922534764\n",
            "step: 170, loss: 0.0008721629856154323\n",
            "step: 180, loss: 0.004303382709622383\n",
            "step: 190, loss: 0.22234420478343964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7830687830687831, f1=0.7780678851174935, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010084420209750533\n",
            "step: 10, loss: 0.07388052344322205\n",
            "step: 20, loss: 0.0024663687217980623\n",
            "step: 30, loss: 0.0007919908966869116\n",
            "step: 40, loss: 0.005351308733224869\n",
            "step: 50, loss: 0.006020638160407543\n",
            "step: 60, loss: 0.004077460616827011\n",
            "step: 70, loss: 0.008761354722082615\n",
            "step: 80, loss: 0.010358858853578568\n",
            "step: 90, loss: 0.06329087913036346\n",
            "step: 100, loss: 0.029028726741671562\n",
            "step: 110, loss: 0.006985506974160671\n",
            "step: 120, loss: 0.008045039139688015\n",
            "step: 130, loss: 0.1513843834400177\n",
            "step: 140, loss: 0.0037268144078552723\n",
            "step: 150, loss: 0.0020575327798724174\n",
            "step: 160, loss: 0.004554847255349159\n",
            "step: 170, loss: 0.011134592816233635\n",
            "step: 180, loss: 0.09521184861660004\n",
            "step: 190, loss: 0.05053999274969101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7712082262210797, f1=0.7830423940149626, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006185355596244335\n",
            "step: 10, loss: 0.0038254272658377886\n",
            "step: 20, loss: 0.0020371987484395504\n",
            "step: 30, loss: 0.0005602336023002863\n",
            "step: 40, loss: 0.08008576184511185\n",
            "step: 50, loss: 0.026266030967235565\n",
            "step: 60, loss: 0.003838102100417018\n",
            "step: 70, loss: 0.003943517804145813\n",
            "step: 80, loss: 0.007081673946231604\n",
            "step: 90, loss: 0.0015132918488234282\n",
            "step: 100, loss: 0.0012529162922874093\n",
            "step: 110, loss: 0.0010903098154813051\n",
            "step: 120, loss: 0.10992400348186493\n",
            "step: 130, loss: 0.0035385009832680225\n",
            "step: 140, loss: 0.002005018526688218\n",
            "step: 150, loss: 0.003107621567323804\n",
            "step: 160, loss: 0.0014022403629496694\n",
            "step: 170, loss: 0.001118380925618112\n",
            "step: 180, loss: 0.0010177608346566558\n",
            "step: 190, loss: 0.00199604919180274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7851458885941645, f1=0.8134715025906736, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007011779234744608\n",
            "step: 10, loss: 0.0006871689110994339\n",
            "step: 20, loss: 0.0010631741024553776\n",
            "step: 30, loss: 0.011987222358584404\n",
            "step: 40, loss: 0.00036650634137913585\n",
            "step: 50, loss: 0.05588288977742195\n",
            "step: 60, loss: 0.0006934336852282286\n",
            "step: 70, loss: 0.0006820374983362854\n",
            "step: 80, loss: 0.000933525909204036\n",
            "step: 90, loss: 0.0002784958924166858\n",
            "step: 100, loss: 0.0007164230337366462\n",
            "step: 110, loss: 0.001062346389517188\n",
            "step: 120, loss: 0.05647636577486992\n",
            "step: 130, loss: 0.0015150789404287934\n",
            "step: 140, loss: 0.0036270858254283667\n",
            "step: 150, loss: 0.0011234792182222009\n",
            "step: 160, loss: 0.0016489307163283229\n",
            "step: 170, loss: 0.0014870526501908898\n",
            "step: 180, loss: 0.01267226506024599\n",
            "step: 190, loss: 0.09879404306411743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7769028871391076, f1=0.8, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020980052649974823\n",
            "step: 10, loss: 0.020248042419552803\n",
            "step: 20, loss: 0.0009089100058190525\n",
            "step: 30, loss: 0.001798369805328548\n",
            "step: 40, loss: 0.005536659620702267\n",
            "step: 50, loss: 0.03810668736696243\n",
            "step: 60, loss: 0.006041122134774923\n",
            "step: 70, loss: 0.0006406959146261215\n",
            "step: 80, loss: 0.0007848537061363459\n",
            "step: 90, loss: 0.0016175297787413\n",
            "step: 100, loss: 0.001275237649679184\n",
            "step: 110, loss: 0.004957225173711777\n",
            "step: 120, loss: 0.0008066837908700109\n",
            "step: 130, loss: 0.006963049061596394\n",
            "step: 140, loss: 0.002390265231952071\n",
            "step: 150, loss: 0.0020627903286367655\n",
            "step: 160, loss: 0.0007126420387066901\n",
            "step: 170, loss: 0.0014040714595466852\n",
            "step: 180, loss: 0.0010640650289133191\n",
            "step: 190, loss: 0.04478176310658455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7819148936170213, f1=0.8095238095238094, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021149124950170517\n",
            "step: 10, loss: 0.0052154515869915485\n",
            "step: 20, loss: 0.005207792390137911\n",
            "step: 30, loss: 0.0029847491532564163\n",
            "step: 40, loss: 0.0031132949516177177\n",
            "step: 50, loss: 0.20890916883945465\n",
            "step: 60, loss: 0.0012935335980728269\n",
            "step: 70, loss: 0.008577670902013779\n",
            "step: 80, loss: 0.0019142113160341978\n",
            "step: 90, loss: 0.005318997427821159\n",
            "step: 100, loss: 0.017637088894844055\n",
            "step: 110, loss: 0.0006631235592067242\n",
            "step: 120, loss: 0.0018084516050294042\n",
            "step: 130, loss: 0.0005016233189962804\n",
            "step: 140, loss: 0.0003719865344464779\n",
            "step: 150, loss: 0.0006975685246288776\n",
            "step: 160, loss: 0.0005469952593557537\n",
            "step: 170, loss: 0.0004239374538883567\n",
            "step: 180, loss: 0.000593235541600734\n",
            "step: 190, loss: 0.0023057921789586544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7882037533512065, f1=0.8167539267015705, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008144961902871728\n",
            "step: 10, loss: 0.0005201524472795427\n",
            "step: 20, loss: 0.00045117794070392847\n",
            "step: 30, loss: 0.0009808873292058706\n",
            "step: 40, loss: 0.0038458104245364666\n",
            "step: 50, loss: 0.0024207544047385454\n",
            "step: 60, loss: 0.0007692394428886473\n",
            "step: 70, loss: 0.0015639691846445203\n",
            "step: 80, loss: 0.003589852713048458\n",
            "step: 90, loss: 0.04106758162379265\n",
            "step: 100, loss: 0.00646961759775877\n",
            "step: 110, loss: 0.0014846139820292592\n",
            "step: 120, loss: 0.10969969630241394\n",
            "step: 130, loss: 0.0012404578737914562\n",
            "step: 140, loss: 0.1679559350013733\n",
            "step: 150, loss: 0.0016741689760237932\n",
            "step: 160, loss: 0.00477292574942112\n",
            "step: 170, loss: 0.0010086086113005877\n",
            "step: 180, loss: 0.0030754278413951397\n",
            "step: 190, loss: 0.0017442272510379553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7924528301886792, f1=0.8073878627968338, best_f1=0.7896103896103895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004835131578147411\n",
            "step: 10, loss: 0.001766012399457395\n",
            "step: 20, loss: 0.0032281007152050734\n",
            "step: 30, loss: 0.004968248773366213\n",
            "step: 40, loss: 0.002064278582111001\n",
            "step: 50, loss: 0.0013932292349636555\n",
            "step: 60, loss: 0.001874046167358756\n",
            "step: 70, loss: 0.010964402928948402\n",
            "step: 80, loss: 0.03393801674246788\n",
            "step: 90, loss: 0.00083813356468454\n",
            "step: 100, loss: 0.0005751755088567734\n",
            "step: 110, loss: 0.00040334323421120644\n",
            "step: 120, loss: 0.0012721145758405328\n",
            "step: 130, loss: 0.013888990506529808\n",
            "step: 140, loss: 0.0038966492284089327\n",
            "step: 150, loss: 0.0016545800026506186\n",
            "step: 160, loss: 0.00044614425860345364\n",
            "step: 170, loss: 0.01798618771135807\n",
            "step: 180, loss: 0.0007217250531539321\n",
            "step: 190, loss: 0.0005781762301921844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7936507936507936, f1=0.8041237113402062, best_f1=0.7896103896103895\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 166.12it/s]\n",
            "load_f1 = 0.6074766355140186\n",
            "real_f1 = 0.6037735849056604\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 176.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61134257-ed39-448b-c39c-2aa34443640e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 388kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.27MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 72.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6373497247695923\n",
            "step: 10, loss: 0.3721559941768646\n",
            "step: 20, loss: 0.3066442310810089\n",
            "step: 30, loss: 0.38014018535614014\n",
            "step: 40, loss: 0.26081645488739014\n",
            "step: 50, loss: 0.24878303706645966\n",
            "step: 60, loss: 0.24997681379318237\n",
            "step: 70, loss: 0.37006959319114685\n",
            "step: 80, loss: 0.3759680688381195\n",
            "step: 90, loss: 0.2548896074295044\n",
            "step: 100, loss: 0.222920760512352\n",
            "step: 110, loss: 0.1788758486509323\n",
            "step: 120, loss: 0.12111561000347137\n",
            "step: 130, loss: 0.06048804521560669\n",
            "step: 140, loss: 0.22171001136302948\n",
            "step: 150, loss: 0.15624015033245087\n",
            "step: 160, loss: 0.11266078054904938\n",
            "step: 170, loss: 0.2367572784423828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7008547008547009, f1=0.6709677419354838, best_f1=0.6709677419354838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.058229707181453705\n",
            "step: 10, loss: 0.11479490995407104\n",
            "step: 20, loss: 0.0347757488489151\n",
            "step: 30, loss: 0.16419066488742828\n",
            "step: 40, loss: 0.15731805562973022\n",
            "step: 50, loss: 0.13692045211791992\n",
            "step: 60, loss: 0.15995903313159943\n",
            "step: 70, loss: 0.08281766623258591\n",
            "step: 80, loss: 0.050005748867988586\n",
            "step: 90, loss: 0.16822251677513123\n",
            "step: 100, loss: 0.14461375772953033\n",
            "step: 110, loss: 0.09603899717330933\n",
            "step: 120, loss: 0.08204219490289688\n",
            "step: 130, loss: 0.07510780543088913\n",
            "step: 140, loss: 0.15176604688167572\n",
            "step: 150, loss: 0.17673994600772858\n",
            "step: 160, loss: 0.13191883265972137\n",
            "step: 170, loss: 0.042249441146850586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8082901554404144, f1=0.7939698492462312, best_f1=0.7939698492462312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09520711749792099\n",
            "step: 10, loss: 0.16664718091487885\n",
            "step: 20, loss: 0.03895142301917076\n",
            "step: 30, loss: 0.0874994620680809\n",
            "step: 40, loss: 0.13495080173015594\n",
            "step: 50, loss: 0.08862970024347305\n",
            "step: 60, loss: 0.1089923158288002\n",
            "step: 70, loss: 0.0931791439652443\n",
            "step: 80, loss: 0.0460478775203228\n",
            "step: 90, loss: 0.02488229237496853\n",
            "step: 100, loss: 0.10942574590444565\n",
            "step: 110, loss: 0.08596231788396835\n",
            "step: 120, loss: 0.12161177396774292\n",
            "step: 130, loss: 0.031130878254771233\n",
            "step: 140, loss: 0.07587211579084396\n",
            "step: 150, loss: 0.026434509083628654\n",
            "step: 160, loss: 0.04291718080639839\n",
            "step: 170, loss: 0.08319764584302902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.8040201005025125, f1=0.8019559902200489, best_f1=0.7939698492462312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009364948607981205\n",
            "step: 10, loss: 0.032714031636714935\n",
            "step: 20, loss: 0.0060034762136638165\n",
            "step: 30, loss: 0.2902781665325165\n",
            "step: 40, loss: 0.004769536200910807\n",
            "step: 50, loss: 0.15188340842723846\n",
            "step: 60, loss: 0.22879542410373688\n",
            "step: 70, loss: 0.012635506689548492\n",
            "step: 80, loss: 0.12463007867336273\n",
            "step: 90, loss: 0.061099834740161896\n",
            "step: 100, loss: 0.20702725648880005\n",
            "step: 110, loss: 0.09409885853528976\n",
            "step: 120, loss: 0.006486514117568731\n",
            "step: 130, loss: 0.0315246656537056\n",
            "step: 140, loss: 0.025133166462183\n",
            "step: 150, loss: 0.14704224467277527\n",
            "step: 160, loss: 0.03316393122076988\n",
            "step: 170, loss: 0.034432508051395416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.786046511627907, f1=0.7963800904977376, best_f1=0.7939698492462312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01592177525162697\n",
            "step: 10, loss: 0.00814928486943245\n",
            "step: 20, loss: 0.038551487028598785\n",
            "step: 30, loss: 0.17474855482578278\n",
            "step: 40, loss: 0.07688043266534805\n",
            "step: 50, loss: 0.1355120837688446\n",
            "step: 60, loss: 0.07339449226856232\n",
            "step: 70, loss: 0.26180416345596313\n",
            "step: 80, loss: 0.10814085602760315\n",
            "step: 90, loss: 0.1241757720708847\n",
            "step: 100, loss: 0.04321916401386261\n",
            "step: 110, loss: 0.030607588589191437\n",
            "step: 120, loss: 0.02683088928461075\n",
            "step: 130, loss: 0.1979800909757614\n",
            "step: 140, loss: 0.01187109388411045\n",
            "step: 150, loss: 0.11060590296983719\n",
            "step: 160, loss: 0.11030419170856476\n",
            "step: 170, loss: 0.010309805162250996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8205128205128205, f1=0.8193384223918575, best_f1=0.8193384223918575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00797942467033863\n",
            "step: 10, loss: 0.10712599754333496\n",
            "step: 20, loss: 0.05964013189077377\n",
            "step: 30, loss: 0.0737161785364151\n",
            "step: 40, loss: 0.0497276671230793\n",
            "step: 50, loss: 0.1344417780637741\n",
            "step: 60, loss: 0.016885120421648026\n",
            "step: 70, loss: 0.06302080303430557\n",
            "step: 80, loss: 0.07030148059129715\n",
            "step: 90, loss: 0.026351571083068848\n",
            "step: 100, loss: 0.017087187618017197\n",
            "step: 110, loss: 0.0018224731320515275\n",
            "step: 120, loss: 0.05716472864151001\n",
            "step: 130, loss: 0.016140643507242203\n",
            "step: 140, loss: 0.1832413226366043\n",
            "step: 150, loss: 0.15498459339141846\n",
            "step: 160, loss: 0.0640069916844368\n",
            "step: 170, loss: 0.0032473853789269924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8317307692307692, f1=0.794188861985472, best_f1=0.794188861985472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010743483435362577\n",
            "step: 10, loss: 0.011459735222160816\n",
            "step: 20, loss: 0.002115854760631919\n",
            "step: 30, loss: 0.04583398997783661\n",
            "step: 40, loss: 0.0372355692088604\n",
            "step: 50, loss: 0.022918419912457466\n",
            "step: 60, loss: 0.0027004012372344732\n",
            "step: 70, loss: 0.0003218232304789126\n",
            "step: 80, loss: 0.006406849250197411\n",
            "step: 90, loss: 0.00021757037029601634\n",
            "step: 100, loss: 0.0002621258608996868\n",
            "step: 110, loss: 0.04535377770662308\n",
            "step: 120, loss: 0.005366533529013395\n",
            "step: 130, loss: 0.14402399957180023\n",
            "step: 140, loss: 0.0008195352857001126\n",
            "step: 150, loss: 0.005334786139428616\n",
            "step: 160, loss: 0.000272390287136659\n",
            "step: 170, loss: 0.023099452257156372\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.836272040302267, f1=0.8109452736318408, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002890246454626322\n",
            "step: 10, loss: 0.008053073659539223\n",
            "step: 20, loss: 0.0042982823215425014\n",
            "step: 30, loss: 0.013092819601297379\n",
            "step: 40, loss: 0.0007725739269517362\n",
            "step: 50, loss: 0.006032106000930071\n",
            "step: 60, loss: 0.0029320931062102318\n",
            "step: 70, loss: 0.0028405161574482918\n",
            "step: 80, loss: 0.002388622146099806\n",
            "step: 90, loss: 0.003929572645574808\n",
            "step: 100, loss: 0.09033811837434769\n",
            "step: 110, loss: 0.07008867710828781\n",
            "step: 120, loss: 0.01919819973409176\n",
            "step: 130, loss: 0.004686539992690086\n",
            "step: 140, loss: 0.0329296737909317\n",
            "step: 150, loss: 0.025357071310281754\n",
            "step: 160, loss: 0.03611864522099495\n",
            "step: 170, loss: 0.002767491154372692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8295165394402035, f1=0.8009950248756218, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012744634877890348\n",
            "step: 10, loss: 0.006024669390171766\n",
            "step: 20, loss: 0.006608565337955952\n",
            "step: 30, loss: 0.05532778427004814\n",
            "step: 40, loss: 0.18642717599868774\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.004359578248113394\n",
            "step: 60, loss: 0.01309962384402752\n",
            "step: 70, loss: 0.0007179719978012145\n",
            "step: 80, loss: 0.0011101439595222473\n",
            "step: 90, loss: 0.04283272475004196\n",
            "step: 100, loss: 0.03368737921118736\n",
            "step: 110, loss: 0.006751608569175005\n",
            "step: 120, loss: 0.0012727768626064062\n",
            "step: 130, loss: 0.052285924553871155\n",
            "step: 140, loss: 0.010852070525288582\n",
            "step: 150, loss: 0.0027690825518220663\n",
            "step: 160, loss: 0.004226600285619497\n",
            "step: 170, loss: 0.0014808328123763204\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8312958435207823, f1=0.8019323671497586, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041142914444208145\n",
            "step: 10, loss: 0.00019603797409217805\n",
            "step: 20, loss: 0.06731490045785904\n",
            "step: 30, loss: 0.012644757516682148\n",
            "step: 40, loss: 0.025757968425750732\n",
            "step: 50, loss: 0.12032090127468109\n",
            "step: 60, loss: 0.0007471602293662727\n",
            "step: 70, loss: 0.027741000056266785\n",
            "step: 80, loss: 0.0051085264421999454\n",
            "step: 90, loss: 0.0011001203674823046\n",
            "step: 100, loss: 0.0005252755945548415\n",
            "step: 110, loss: 0.003878816729411483\n",
            "step: 120, loss: 0.01179398875683546\n",
            "step: 130, loss: 0.024044658988714218\n",
            "step: 140, loss: 0.009656500071287155\n",
            "step: 150, loss: 0.005922697950154543\n",
            "step: 160, loss: 0.06285074353218079\n",
            "step: 170, loss: 0.006392533425241709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8316831683168316, f1=0.7780429594272077, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04881914332509041\n",
            "step: 10, loss: 0.002229547593742609\n",
            "step: 20, loss: 0.004674346186220646\n",
            "step: 30, loss: 0.001004080637358129\n",
            "step: 40, loss: 0.0014298309106379747\n",
            "step: 50, loss: 0.0005381070077419281\n",
            "step: 60, loss: 0.0034808439668267965\n",
            "step: 70, loss: 0.025492584332823753\n",
            "step: 80, loss: 0.0020868254359811544\n",
            "step: 90, loss: 0.0023756222799420357\n",
            "step: 100, loss: 0.0008373733726330101\n",
            "step: 110, loss: 0.08927112817764282\n",
            "step: 120, loss: 0.003371786791831255\n",
            "step: 130, loss: 0.00020710799435619265\n",
            "step: 140, loss: 0.015019599348306656\n",
            "step: 150, loss: 0.005332986358553171\n",
            "step: 160, loss: 0.00599583238363266\n",
            "step: 170, loss: 0.0029512643814086914\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8296296296296296, f1=0.8173076923076923, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014721553772687912\n",
            "step: 10, loss: 0.0004920699284411967\n",
            "step: 20, loss: 0.00036168392398394644\n",
            "step: 30, loss: 0.0025096735917031765\n",
            "step: 40, loss: 0.001244635903276503\n",
            "step: 50, loss: 0.0004718563868664205\n",
            "step: 60, loss: 0.005545745603740215\n",
            "step: 70, loss: 0.020982157438993454\n",
            "step: 80, loss: 0.0003536739095579833\n",
            "step: 90, loss: 0.0013542763190343976\n",
            "step: 100, loss: 0.0006190590211190283\n",
            "step: 110, loss: 0.0007947086123749614\n",
            "step: 120, loss: 0.04340243712067604\n",
            "step: 130, loss: 0.04165467992424965\n",
            "step: 140, loss: 0.02741178683936596\n",
            "step: 150, loss: 0.21530768275260925\n",
            "step: 160, loss: 0.0008311295532621443\n",
            "step: 170, loss: 0.0003515627176966518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8329297820823245, f1=0.8133971291866029, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018315888941287994\n",
            "step: 10, loss: 0.005507337860763073\n",
            "step: 20, loss: 0.0003225411637686193\n",
            "step: 30, loss: 0.000924637948628515\n",
            "step: 40, loss: 0.019669005647301674\n",
            "step: 50, loss: 0.003247430780902505\n",
            "step: 60, loss: 0.021877655759453773\n",
            "step: 70, loss: 0.0004380558093544096\n",
            "step: 80, loss: 0.0010010653641074896\n",
            "step: 90, loss: 0.00022744774469174445\n",
            "step: 100, loss: 0.0037309606559574604\n",
            "step: 110, loss: 0.00020571811182890087\n",
            "step: 120, loss: 0.03522762656211853\n",
            "step: 130, loss: 0.011388426646590233\n",
            "step: 140, loss: 0.001194002223201096\n",
            "step: 150, loss: 0.001674190629273653\n",
            "step: 160, loss: 0.010333982296288013\n",
            "step: 170, loss: 0.003582798643037677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8226221079691517, f1=0.8120300751879699, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004305231384932995\n",
            "step: 10, loss: 0.0001658465771470219\n",
            "step: 20, loss: 0.013088444247841835\n",
            "step: 30, loss: 0.0017250578384846449\n",
            "step: 40, loss: 0.001377870561555028\n",
            "step: 50, loss: 0.00040238883229903877\n",
            "step: 60, loss: 0.00551664037629962\n",
            "step: 70, loss: 0.0006848580087535083\n",
            "step: 80, loss: 0.005330360494554043\n",
            "step: 90, loss: 0.00042493094224482775\n",
            "step: 100, loss: 0.012090425938367844\n",
            "step: 110, loss: 0.0017552459612488747\n",
            "step: 120, loss: 0.041745588183403015\n",
            "step: 130, loss: 0.001330366125330329\n",
            "step: 140, loss: 0.0012845058226957917\n",
            "step: 150, loss: 0.0003129805554635823\n",
            "step: 160, loss: 0.0007059563649818301\n",
            "step: 170, loss: 0.0007666859892196953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8211586901763224, f1=0.8088235294117646, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037938072346150875\n",
            "step: 10, loss: 0.000609388982411474\n",
            "step: 20, loss: 0.053263988345861435\n",
            "step: 30, loss: 0.00017525660223327577\n",
            "step: 40, loss: 0.0011958586983382702\n",
            "step: 50, loss: 0.000658674631267786\n",
            "step: 60, loss: 0.0026275671552866697\n",
            "step: 70, loss: 0.005295877810567617\n",
            "step: 80, loss: 0.016722390428185463\n",
            "step: 90, loss: 0.0013595662312582135\n",
            "step: 100, loss: 0.011196565814316273\n",
            "step: 110, loss: 0.0002148147177649662\n",
            "step: 120, loss: 0.0003521730250213295\n",
            "step: 130, loss: 0.0011884482810273767\n",
            "step: 140, loss: 0.001838593976572156\n",
            "step: 150, loss: 0.013169157318770885\n",
            "step: 160, loss: 0.0003221704682800919\n",
            "step: 170, loss: 0.03447093814611435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8188585607940446, f1=0.8115942028985508, best_f1=0.8109452736318408\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 277.32it/s]\n",
            "load_f1 = 0.4214876033057851\n",
            "real_f1 = 0.41269841269841273\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 236.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c91c86-0b22-4b41-b14a-8ce02836d139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5984289646148682\n",
            "step: 10, loss: 0.6309069991111755\n",
            "step: 20, loss: 0.3546966016292572\n",
            "step: 30, loss: 0.09655976295471191\n",
            "step: 40, loss: 0.17539358139038086\n",
            "step: 50, loss: 0.06712396442890167\n",
            "step: 60, loss: 0.17435792088508606\n",
            "step: 70, loss: 0.201813206076622\n",
            "step: 80, loss: 0.028476113453507423\n",
            "step: 90, loss: 0.12350612133741379\n",
            "step: 100, loss: 0.0067200809717178345\n",
            "step: 110, loss: 0.23084577918052673\n",
            "step: 120, loss: 0.016604648903012276\n",
            "step: 130, loss: 0.03743276745080948\n",
            "step: 140, loss: 0.004087226931005716\n",
            "step: 150, loss: 0.01537527609616518\n",
            "step: 160, loss: 0.15068882703781128\n",
            "step: 170, loss: 0.15937568247318268\n",
            "step: 180, loss: 0.04287632927298546\n",
            "step: 190, loss: 0.05039301514625549\n",
            "step: 200, loss: 0.11427188664674759\n",
            "step: 210, loss: 0.009329305961728096\n",
            "step: 220, loss: 0.0061249504797160625\n",
            "step: 230, loss: 0.02340780571103096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9701657458563535, f1=0.9686800894854586, best_f1=0.9686800894854586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013851052150130272\n",
            "step: 10, loss: 0.00896868109703064\n",
            "step: 20, loss: 0.15081098675727844\n",
            "step: 30, loss: 0.017128974199295044\n",
            "step: 40, loss: 0.09457256644964218\n",
            "step: 50, loss: 0.005010006483644247\n",
            "step: 60, loss: 0.021296672523021698\n",
            "step: 70, loss: 0.10069683194160461\n",
            "step: 80, loss: 0.009261484257876873\n",
            "step: 90, loss: 0.013975290581583977\n",
            "step: 100, loss: 0.13498049974441528\n",
            "step: 110, loss: 0.09977183490991592\n",
            "step: 120, loss: 0.09378498792648315\n",
            "step: 130, loss: 0.014841698110103607\n",
            "step: 140, loss: 0.004239553119987249\n",
            "step: 150, loss: 0.011544683948159218\n",
            "step: 160, loss: 0.024938123300671577\n",
            "step: 170, loss: 0.0007957394700497389\n",
            "step: 180, loss: 0.0014437763020396233\n",
            "step: 190, loss: 0.005042870994657278\n",
            "step: 200, loss: 0.0028269989416003227\n",
            "step: 210, loss: 0.002628088230267167\n",
            "step: 220, loss: 0.2800047993659973\n",
            "step: 230, loss: 0.02612587995827198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9787234042553192, f1=0.9731543624161074, best_f1=0.9731543624161074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045350052416324615\n",
            "step: 10, loss: 0.0036739949136972427\n",
            "step: 20, loss: 0.04836370050907135\n",
            "step: 30, loss: 0.02876836247742176\n",
            "step: 40, loss: 0.09180104732513428\n",
            "step: 50, loss: 0.0054868971928954124\n",
            "step: 60, loss: 0.00502582686021924\n",
            "step: 70, loss: 0.06869909912347794\n",
            "step: 80, loss: 0.0013224952854216099\n",
            "step: 90, loss: 0.06123630702495575\n",
            "step: 100, loss: 0.0033728666603565216\n",
            "step: 110, loss: 0.0010065010283142328\n",
            "step: 120, loss: 0.009662720374763012\n",
            "step: 130, loss: 0.00044324889313429594\n",
            "step: 140, loss: 0.008170687593519688\n",
            "step: 150, loss: 0.04428361728787422\n",
            "step: 160, loss: 0.01705487072467804\n",
            "step: 170, loss: 0.011886299587786198\n",
            "step: 180, loss: 0.027774561196565628\n",
            "step: 190, loss: 0.002250595949590206\n",
            "step: 200, loss: 0.055200014263391495\n",
            "step: 210, loss: 0.0037380452267825603\n",
            "step: 220, loss: 0.0008323651854880154\n",
            "step: 230, loss: 0.020257391035556793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9732739420935412, f1=0.9689578713968958, best_f1=0.9731543624161074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000995245180092752\n",
            "step: 10, loss: 0.009164686314761639\n",
            "step: 20, loss: 0.0013332006055861712\n",
            "step: 30, loss: 0.02582743391394615\n",
            "step: 40, loss: 0.003326427424326539\n",
            "step: 50, loss: 0.0028233216144144535\n",
            "step: 60, loss: 0.0006686746492050588\n",
            "step: 70, loss: 0.004465918987989426\n",
            "step: 80, loss: 0.005886731669306755\n",
            "step: 90, loss: 0.0035246300976723433\n",
            "step: 100, loss: 0.0006640676874667406\n",
            "step: 110, loss: 0.006522298324853182\n",
            "step: 120, loss: 0.010660193860530853\n",
            "step: 130, loss: 0.0008751689456403255\n",
            "step: 140, loss: 0.0004271193465683609\n",
            "step: 150, loss: 0.2630937099456787\n",
            "step: 160, loss: 0.1391354203224182\n",
            "step: 170, loss: 0.008372090756893158\n",
            "step: 180, loss: 0.0041752588003873825\n",
            "step: 190, loss: 0.0073751420713961124\n",
            "step: 200, loss: 0.0024316590279340744\n",
            "step: 210, loss: 0.14490751922130585\n",
            "step: 220, loss: 0.002840770408511162\n",
            "step: 230, loss: 0.00581250200048089\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9832402234636871, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026955727953463793\n",
            "step: 10, loss: 0.0007221636478789151\n",
            "step: 20, loss: 0.0014410251751542091\n",
            "step: 30, loss: 0.0004732509842142463\n",
            "step: 40, loss: 0.0015311157330870628\n",
            "step: 50, loss: 0.0007383530028164387\n",
            "step: 60, loss: 0.07170861959457397\n",
            "step: 70, loss: 0.000781480222940445\n",
            "step: 80, loss: 0.0008264842326752841\n",
            "step: 90, loss: 0.0030201307963579893\n",
            "step: 100, loss: 0.0003705160634126514\n",
            "step: 110, loss: 0.0008788537816144526\n",
            "step: 120, loss: 0.00027132374816574156\n",
            "step: 130, loss: 0.002779579721391201\n",
            "step: 140, loss: 0.008617883548140526\n",
            "step: 150, loss: 0.014534891583025455\n",
            "step: 160, loss: 0.008775421418249607\n",
            "step: 170, loss: 0.013560797087848186\n",
            "step: 180, loss: 0.0006051842938177288\n",
            "step: 190, loss: 0.09453287720680237\n",
            "step: 200, loss: 0.0008401330560445786\n",
            "step: 210, loss: 0.0011362485820427537\n",
            "step: 220, loss: 0.0009760076645761728\n",
            "step: 230, loss: 0.0007772105163894594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9798657718120806, f1=0.9732142857142857, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014684272464364767\n",
            "step: 10, loss: 0.0015549955423921347\n",
            "step: 20, loss: 0.00632155267521739\n",
            "step: 30, loss: 0.001153071760199964\n",
            "step: 40, loss: 0.0024412176571786404\n",
            "step: 50, loss: 0.0007600426324643195\n",
            "step: 60, loss: 0.0004525560070760548\n",
            "step: 70, loss: 0.0021152214612811804\n",
            "step: 80, loss: 0.0009417766705155373\n",
            "step: 90, loss: 0.00022821960737928748\n",
            "step: 100, loss: 0.09424160420894623\n",
            "step: 110, loss: 0.0015072317328304052\n",
            "step: 120, loss: 0.00397541793063283\n",
            "step: 130, loss: 0.0006031978409737349\n",
            "step: 140, loss: 0.0004666615277528763\n",
            "step: 150, loss: 0.00925297848880291\n",
            "step: 160, loss: 0.00024032860528677702\n",
            "step: 170, loss: 0.00023012679594103247\n",
            "step: 180, loss: 0.0006472321110777557\n",
            "step: 190, loss: 0.09762938320636749\n",
            "step: 200, loss: 0.00036260270280763507\n",
            "step: 210, loss: 0.004557713866233826\n",
            "step: 220, loss: 0.01838451810181141\n",
            "step: 230, loss: 0.05696210265159607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9787234042553192, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027445268351584673\n",
            "step: 10, loss: 0.0006363533902913332\n",
            "step: 20, loss: 0.01566896215081215\n",
            "step: 30, loss: 0.0013519260101020336\n",
            "step: 40, loss: 0.005884020123630762\n",
            "step: 50, loss: 0.007424355484545231\n",
            "step: 60, loss: 0.057914379984140396\n",
            "step: 70, loss: 0.023513825610280037\n",
            "step: 80, loss: 0.0005288023967295885\n",
            "step: 90, loss: 0.0004240469424985349\n",
            "step: 100, loss: 0.00023700515157543123\n",
            "step: 110, loss: 0.00030378237715922296\n",
            "step: 120, loss: 0.00018424075096845627\n",
            "step: 130, loss: 0.00014873997133690864\n",
            "step: 140, loss: 0.0002971444628201425\n",
            "step: 150, loss: 0.003336466383188963\n",
            "step: 160, loss: 0.06314016878604889\n",
            "step: 170, loss: 0.06022866070270538\n",
            "step: 180, loss: 0.0006608616677112877\n",
            "step: 190, loss: 0.0011101956479251385\n",
            "step: 200, loss: 0.052366990596055984\n",
            "step: 210, loss: 0.0008043296402320266\n",
            "step: 220, loss: 0.0100239934399724\n",
            "step: 230, loss: 0.0020781387574970722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9809203142536477, f1=0.9786276715410572, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003972681879531592\n",
            "step: 10, loss: 0.004967528395354748\n",
            "step: 20, loss: 0.00042072052019648254\n",
            "step: 30, loss: 0.00012007301847916096\n",
            "step: 40, loss: 0.0018298751674592495\n",
            "step: 50, loss: 0.0015085435006767511\n",
            "step: 60, loss: 0.00019506212265696377\n",
            "step: 70, loss: 0.0005417457432486117\n",
            "step: 80, loss: 0.00015424579032696784\n",
            "step: 90, loss: 0.00023310972028411925\n",
            "step: 100, loss: 0.005309737287461758\n",
            "step: 110, loss: 0.018334027379751205\n",
            "step: 120, loss: 0.01320928055793047\n",
            "step: 130, loss: 0.0010769636137410998\n",
            "step: 140, loss: 0.0004878875915892422\n",
            "step: 150, loss: 0.00021901819854974747\n",
            "step: 160, loss: 0.0001888091501314193\n",
            "step: 170, loss: 0.0024599346797913313\n",
            "step: 180, loss: 0.0017938674427568913\n",
            "step: 190, loss: 0.0006340989493764937\n",
            "step: 200, loss: 0.0006916237180121243\n",
            "step: 210, loss: 0.0015551565447822213\n",
            "step: 220, loss: 0.0004981131642125547\n",
            "step: 230, loss: 0.0006799310795031488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9776286353467561, f1=0.9743589743589743, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.698908979771659e-05\n",
            "step: 10, loss: 0.0002878982340916991\n",
            "step: 20, loss: 0.001501289545558393\n",
            "step: 30, loss: 0.00034001137828454375\n",
            "step: 40, loss: 0.10902591049671173\n",
            "step: 50, loss: 5.7779103372013196e-05\n",
            "step: 60, loss: 0.00013286435569170862\n",
            "step: 70, loss: 0.0017425973201170564\n",
            "step: 80, loss: 0.00020849840075243264\n",
            "step: 90, loss: 0.0024319372605532408\n",
            "step: 100, loss: 0.00013228855095803738\n",
            "step: 110, loss: 5.6173372286139056e-05\n",
            "step: 120, loss: 6.935495184734464e-05\n",
            "step: 130, loss: 5.115633030072786e-05\n",
            "step: 140, loss: 4.8008241719799116e-05\n",
            "step: 150, loss: 0.00030313999741338193\n",
            "step: 160, loss: 5.7621440646471456e-05\n",
            "step: 170, loss: 8.109051850624382e-05\n",
            "step: 180, loss: 0.0018014473607763648\n",
            "step: 190, loss: 0.0006952836993150413\n",
            "step: 200, loss: 0.0008425044361501932\n",
            "step: 210, loss: 0.0038159836549311876\n",
            "step: 220, loss: 0.00010541867959545925\n",
            "step: 230, loss: 0.00017670862143859267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.983050847457627, f1=0.9773242630385486, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.797450962243602e-05\n",
            "step: 10, loss: 0.00022072743740864098\n",
            "step: 20, loss: 0.00011302793427603319\n",
            "step: 30, loss: 0.00028150880825705826\n",
            "step: 40, loss: 0.000963641214184463\n",
            "step: 50, loss: 0.0001192636409541592\n",
            "step: 60, loss: 0.0003893494722433388\n",
            "step: 70, loss: 0.0017613967647776008\n",
            "step: 80, loss: 0.00016425202193204314\n",
            "step: 90, loss: 0.098353311419487\n",
            "step: 100, loss: 0.00014315196312963963\n",
            "step: 110, loss: 0.00017966885934583843\n",
            "step: 120, loss: 0.0011437444481998682\n",
            "step: 130, loss: 0.0638740286231041\n",
            "step: 140, loss: 0.04443597048521042\n",
            "step: 150, loss: 0.014533407986164093\n",
            "step: 160, loss: 0.0006833617226220667\n",
            "step: 170, loss: 0.00014301619376055896\n",
            "step: 180, loss: 0.008383904583752155\n",
            "step: 190, loss: 0.0014926594449207187\n",
            "step: 200, loss: 3.90694840461947e-05\n",
            "step: 210, loss: 0.00011540992272784933\n",
            "step: 220, loss: 6.72367968945764e-05\n",
            "step: 230, loss: 0.0008465422433800995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9831271091113611, f1=0.9752252252252253, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.288101237965748e-05\n",
            "step: 10, loss: 0.00011028294102288783\n",
            "step: 20, loss: 0.0009812969947233796\n",
            "step: 30, loss: 0.0029399532359093428\n",
            "step: 40, loss: 0.0004341907042544335\n",
            "step: 50, loss: 0.000178035581484437\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 60, loss: 0.0028273717034608126\n",
            "step: 70, loss: 0.00044263325980864465\n",
            "step: 80, loss: 0.00011667661601677537\n",
            "step: 90, loss: 0.00029668663046322763\n",
            "step: 100, loss: 0.000277891056612134\n",
            "step: 110, loss: 0.009352811612188816\n",
            "step: 120, loss: 0.0001223678991664201\n",
            "step: 130, loss: 0.00018085465126205236\n",
            "step: 140, loss: 3.724443740793504e-05\n",
            "step: 150, loss: 0.0032621752470731735\n",
            "step: 160, loss: 0.0001697027764748782\n",
            "step: 170, loss: 0.00076038867700845\n",
            "step: 180, loss: 0.029632655903697014\n",
            "step: 190, loss: 0.0001228434412041679\n",
            "step: 200, loss: 0.0007520526414737105\n",
            "step: 210, loss: 0.00014797880430705845\n",
            "step: 220, loss: 8.925537986215204e-05\n",
            "step: 230, loss: 0.00015483886818401515\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9854423292273236, f1=0.9710467706013363, best_f1=0.9710467706013363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013890400296077132\n",
            "step: 10, loss: 6.536470755236223e-05\n",
            "step: 20, loss: 0.0006118747987784445\n",
            "step: 30, loss: 0.00024306404520757496\n",
            "step: 40, loss: 0.00045152968959882855\n",
            "step: 50, loss: 0.005125865805894136\n",
            "step: 60, loss: 0.0034589862916618586\n",
            "step: 70, loss: 0.00014863707474432886\n",
            "step: 80, loss: 0.00040368115878663957\n",
            "step: 90, loss: 8.860675006872043e-05\n",
            "step: 100, loss: 3.16455407300964e-05\n",
            "step: 110, loss: 4.4369797251420096e-05\n",
            "step: 120, loss: 3.753053169930354e-05\n",
            "step: 130, loss: 7.370120874838904e-05\n",
            "step: 140, loss: 0.0008733628201298416\n",
            "step: 150, loss: 0.0011622001184150577\n",
            "step: 160, loss: 0.00010064908565254882\n",
            "step: 170, loss: 0.0005908144521526992\n",
            "step: 180, loss: 0.006835748441517353\n",
            "step: 190, loss: 9.005245374282822e-05\n",
            "step: 200, loss: 3.5366738302400336e-05\n",
            "step: 210, loss: 0.00010808841761900112\n",
            "step: 220, loss: 0.0003660086658783257\n",
            "step: 230, loss: 0.0018957251450046897\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.983277591973244, f1=0.9721913236929923, best_f1=0.9710467706013363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001933191088028252\n",
            "step: 10, loss: 0.0001370374084217474\n",
            "step: 20, loss: 0.0001475857279729098\n",
            "step: 30, loss: 0.00011251357500441372\n",
            "step: 40, loss: 0.00017905501590576023\n",
            "step: 50, loss: 0.0007865853840485215\n",
            "step: 60, loss: 0.0001444533991161734\n",
            "step: 70, loss: 5.123562004882842e-05\n",
            "step: 80, loss: 4.695808456744999e-05\n",
            "step: 90, loss: 0.0004777562862727791\n",
            "step: 100, loss: 6.804862641729414e-05\n",
            "step: 110, loss: 0.0038912754971534014\n",
            "step: 120, loss: 7.550996087957174e-05\n",
            "step: 130, loss: 4.6783385187154636e-05\n",
            "step: 140, loss: 5.379128924687393e-05\n",
            "step: 150, loss: 6.633213342865929e-05\n",
            "step: 160, loss: 0.00017348950495943427\n",
            "step: 170, loss: 4.5572665840154514e-05\n",
            "step: 180, loss: 5.316307215252891e-05\n",
            "step: 190, loss: 4.1903007513610646e-05\n",
            "step: 200, loss: 0.0001752923271851614\n",
            "step: 210, loss: 2.5700224796310067e-05\n",
            "step: 220, loss: 0.00012698580394499004\n",
            "step: 230, loss: 4.800660826731473e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.984304932735426, f1=0.9808773903262092, best_f1=0.9710467706013363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010330440272809938\n",
            "step: 10, loss: 0.00026847634580917656\n",
            "step: 20, loss: 0.0015346587169915438\n",
            "step: 30, loss: 5.288760075927712e-05\n",
            "step: 40, loss: 0.013415229506790638\n",
            "step: 50, loss: 0.0001073743260349147\n",
            "step: 60, loss: 8.750831329962239e-05\n",
            "step: 70, loss: 4.074518074048683e-05\n",
            "step: 80, loss: 3.356762317707762e-05\n",
            "step: 90, loss: 8.64499161252752e-05\n",
            "step: 100, loss: 3.6986319173593074e-05\n",
            "step: 110, loss: 0.00044901767978444695\n",
            "step: 120, loss: 0.00019247279851697385\n",
            "step: 130, loss: 5.062769923824817e-05\n",
            "step: 140, loss: 0.0013309925561770797\n",
            "step: 150, loss: 3.888610808644444e-05\n",
            "step: 160, loss: 0.0021860802080482244\n",
            "step: 170, loss: 0.0002268755342811346\n",
            "step: 180, loss: 7.126080890884623e-05\n",
            "step: 190, loss: 2.8810638468712568e-05\n",
            "step: 200, loss: 2.932835377578158e-05\n",
            "step: 210, loss: 0.00010381893662270159\n",
            "step: 220, loss: 0.000230448815273121\n",
            "step: 230, loss: 0.0025051224511116743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.984304932735426, f1=0.978675645342312, best_f1=0.9710467706013363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.096735599683598e-05\n",
            "step: 10, loss: 2.1341853425838053e-05\n",
            "step: 20, loss: 4.1330196836497635e-05\n",
            "step: 30, loss: 0.00010872911661863327\n",
            "step: 40, loss: 0.00010881891648750752\n",
            "step: 50, loss: 0.030773457139730453\n",
            "step: 60, loss: 3.4554854210000485e-05\n",
            "step: 70, loss: 0.0001477933255955577\n",
            "step: 80, loss: 0.0005969498888589442\n",
            "step: 90, loss: 0.00010435389413032681\n",
            "step: 100, loss: 4.963890023645945e-05\n",
            "step: 110, loss: 5.408029755926691e-05\n",
            "step: 120, loss: 4.1735638660611585e-05\n",
            "step: 130, loss: 0.0035261265002191067\n",
            "step: 140, loss: 0.000649604422505945\n",
            "step: 150, loss: 0.00019504937517922372\n",
            "step: 160, loss: 2.6583047656458803e-05\n",
            "step: 170, loss: 2.6556910597719252e-05\n",
            "step: 180, loss: 0.0004945112159475684\n",
            "step: 190, loss: 0.00020583099103532732\n",
            "step: 200, loss: 4.4454882299760357e-05\n",
            "step: 210, loss: 9.21455430216156e-05\n",
            "step: 220, loss: 0.001287955092266202\n",
            "step: 230, loss: 3.140323678962886e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9842696629213483, f1=0.9797752808988766, best_f1=0.9710467706013363\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 201.62it/s]\n",
            "load_f1 = 0.9831271091113611\n",
            "real_f1 = 0.9808773903262092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 243.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a936b403-f3ca-470f-8ee0-c94ef8dec117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6151407361030579\n",
            "step: 10, loss: 0.498611718416214\n",
            "step: 20, loss: 0.4994441568851471\n",
            "step: 30, loss: 0.08650235086679459\n",
            "step: 40, loss: 0.19029004871845245\n",
            "step: 50, loss: 0.059788137674331665\n",
            "step: 60, loss: 0.37446287274360657\n",
            "step: 70, loss: 0.16117902100086212\n",
            "step: 80, loss: 0.1706964671611786\n",
            "step: 90, loss: 0.0519549697637558\n",
            "step: 100, loss: 0.04800776019692421\n",
            "step: 110, loss: 0.061222366988658905\n",
            "step: 120, loss: 0.14908680319786072\n",
            "step: 130, loss: 0.05207423120737076\n",
            "step: 140, loss: 0.05296257138252258\n",
            "step: 150, loss: 0.08730778843164444\n",
            "step: 160, loss: 0.029156316071748734\n",
            "step: 170, loss: 0.158375084400177\n",
            "step: 180, loss: 0.07641295343637466\n",
            "step: 190, loss: 0.025662528350949287\n",
            "step: 200, loss: 0.2834603786468506\n",
            "step: 210, loss: 0.12235049158334732\n",
            "step: 220, loss: 0.18399566411972046\n",
            "step: 230, loss: 0.16160009801387787\n",
            "step: 240, loss: 0.047593459486961365\n",
            "step: 250, loss: 0.009007933549582958\n",
            "step: 260, loss: 0.07953186333179474\n",
            "step: 270, loss: 0.009064409881830215\n",
            "step: 280, loss: 0.05208146199584007\n",
            "step: 290, loss: 0.05063496530056\n",
            "step: 300, loss: 0.014199203811585903\n",
            "step: 310, loss: 0.2815638780593872\n",
            "step: 320, loss: 0.07134076207876205\n",
            "step: 330, loss: 0.04084891453385353\n",
            "step: 340, loss: 0.053085315972566605\n",
            "step: 350, loss: 0.05964807793498039\n",
            "step: 360, loss: 0.05695812404155731\n",
            "step: 370, loss: 0.16613955795764923\n",
            "step: 380, loss: 0.04278506711125374\n",
            "step: 390, loss: 0.09927941113710403\n",
            "step: 400, loss: 0.19911642372608185\n",
            "step: 410, loss: 0.040803212672472\n",
            "step: 420, loss: 0.02254304476082325\n",
            "step: 430, loss: 0.13155284523963928\n",
            "step: 440, loss: 0.07054674625396729\n",
            "step: 450, loss: 0.004385902546346188\n",
            "step: 460, loss: 0.012911925092339516\n",
            "step: 470, loss: 0.12330543249845505\n",
            "step: 480, loss: 0.0586797259747982\n",
            "step: 490, loss: 0.1763785183429718\n",
            "step: 500, loss: 0.07169415801763535\n",
            "step: 510, loss: 0.09550493210554123\n",
            "step: 520, loss: 0.04052546247839928\n",
            "step: 530, loss: 0.007444813381880522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9375879868606289, f1=0.9285042333019755, best_f1=0.9285042333019755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10558290779590607\n",
            "step: 10, loss: 0.0455656535923481\n",
            "step: 20, loss: 0.09935462474822998\n",
            "step: 30, loss: 0.03832734003663063\n",
            "step: 40, loss: 0.13085800409317017\n",
            "step: 50, loss: 0.11892498284578323\n",
            "step: 60, loss: 0.005241325590759516\n",
            "step: 70, loss: 0.01885008066892624\n",
            "step: 80, loss: 0.08842672407627106\n",
            "step: 90, loss: 0.0075035616755485535\n",
            "step: 100, loss: 0.03782433643937111\n",
            "step: 110, loss: 0.04113822802901268\n",
            "step: 120, loss: 0.14198914170265198\n",
            "step: 130, loss: 0.10312672704458237\n",
            "step: 140, loss: 0.04322122037410736\n",
            "step: 150, loss: 0.08799736201763153\n",
            "step: 160, loss: 0.011876348406076431\n",
            "step: 170, loss: 0.038938459008932114\n",
            "step: 180, loss: 0.017257677391171455\n",
            "step: 190, loss: 0.12573271989822388\n",
            "step: 200, loss: 0.01891661062836647\n",
            "step: 210, loss: 0.03230121731758118\n",
            "step: 220, loss: 0.07334835827350616\n",
            "step: 230, loss: 0.01949165388941765\n",
            "step: 240, loss: 0.03472708910703659\n",
            "step: 250, loss: 0.032593291252851486\n",
            "step: 260, loss: 0.002616389887407422\n",
            "step: 270, loss: 0.23470713198184967\n",
            "step: 280, loss: 0.017758939415216446\n",
            "step: 290, loss: 0.022838864475488663\n",
            "step: 300, loss: 0.06698677688837051\n",
            "step: 310, loss: 0.011688657104969025\n",
            "step: 320, loss: 0.18604399263858795\n",
            "step: 330, loss: 0.0903654620051384\n",
            "step: 340, loss: 0.020375076681375504\n",
            "step: 350, loss: 0.001711913151666522\n",
            "step: 360, loss: 0.094424307346344\n",
            "step: 370, loss: 0.1508675217628479\n",
            "step: 380, loss: 0.08141909539699554\n",
            "step: 390, loss: 0.044105883687734604\n",
            "step: 400, loss: 0.004266228526830673\n",
            "step: 410, loss: 0.03803836181759834\n",
            "step: 420, loss: 0.019923485815525055\n",
            "step: 430, loss: 0.011435496620833874\n",
            "step: 440, loss: 0.1251361221075058\n",
            "step: 450, loss: 0.028346970677375793\n",
            "step: 460, loss: 0.02177179977297783\n",
            "step: 470, loss: 0.07596619427204132\n",
            "step: 480, loss: 0.19151940941810608\n",
            "step: 490, loss: 0.01228451356291771\n",
            "step: 500, loss: 0.16835075616836548\n",
            "step: 510, loss: 0.009236367419362068\n",
            "step: 520, loss: 0.04895889014005661\n",
            "step: 530, loss: 0.014841062016785145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9428962996802193, f1=0.9340009103322713, best_f1=0.9340009103322713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06524061411619186\n",
            "step: 10, loss: 0.029602600261569023\n",
            "step: 20, loss: 0.03448404744267464\n",
            "step: 30, loss: 0.016370108351111412\n",
            "step: 40, loss: 0.18149897456169128\n",
            "step: 50, loss: 0.08365107327699661\n",
            "step: 60, loss: 0.011132261715829372\n",
            "step: 70, loss: 0.01861380785703659\n",
            "step: 80, loss: 0.0038391663692891598\n",
            "step: 90, loss: 0.0045702229253947735\n",
            "step: 100, loss: 0.07371866703033447\n",
            "step: 110, loss: 0.005107162520289421\n",
            "step: 120, loss: 0.01789812371134758\n",
            "step: 130, loss: 0.010905050672590733\n",
            "step: 140, loss: 0.017450129613280296\n",
            "step: 150, loss: 0.025260936468839645\n",
            "step: 160, loss: 0.014916162006556988\n",
            "step: 170, loss: 0.06078106909990311\n",
            "step: 180, loss: 0.005396023392677307\n",
            "step: 190, loss: 0.017885128036141396\n",
            "step: 200, loss: 0.06370701640844345\n",
            "step: 210, loss: 0.1559295803308487\n",
            "step: 220, loss: 0.11713948100805283\n",
            "step: 230, loss: 0.12331704795360565\n",
            "step: 240, loss: 0.002143921097740531\n",
            "step: 250, loss: 0.033940695226192474\n",
            "step: 260, loss: 0.0183029156178236\n",
            "step: 270, loss: 0.008891777135431767\n",
            "step: 280, loss: 0.13353604078292847\n",
            "step: 290, loss: 0.0018567681545391679\n",
            "step: 300, loss: 0.02916271612048149\n",
            "step: 310, loss: 0.011511236429214478\n",
            "step: 320, loss: 0.037351094186306\n",
            "step: 330, loss: 0.06715411692857742\n",
            "step: 340, loss: 0.0033518995624035597\n",
            "step: 350, loss: 0.015988094732165337\n",
            "step: 360, loss: 0.039315998554229736\n",
            "step: 370, loss: 0.04222545400261879\n",
            "step: 380, loss: 0.03060954436659813\n",
            "step: 390, loss: 0.02878705784678459\n",
            "step: 400, loss: 0.02330838516354561\n",
            "step: 410, loss: 0.015322616323828697\n",
            "step: 420, loss: 0.19424153864383698\n",
            "step: 430, loss: 0.003091546706855297\n",
            "step: 440, loss: 0.0032645552419126034\n",
            "step: 450, loss: 0.054036471992731094\n",
            "step: 460, loss: 0.06515383720397949\n",
            "step: 470, loss: 0.11395596712827682\n",
            "step: 480, loss: 0.004905366338789463\n",
            "step: 490, loss: 0.006538409274071455\n",
            "step: 500, loss: 0.014749010093510151\n",
            "step: 510, loss: 0.010048478841781616\n",
            "step: 520, loss: 0.14688417315483093\n",
            "step: 530, loss: 0.06721779704093933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9432753888380604, f1=0.9370437956204379, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008904380723834038\n",
            "step: 10, loss: 0.003949452191591263\n",
            "step: 20, loss: 0.004837804473936558\n",
            "step: 30, loss: 0.0017339453333988786\n",
            "step: 40, loss: 0.0029417197220027447\n",
            "step: 50, loss: 0.00964549370110035\n",
            "step: 60, loss: 0.009431998245418072\n",
            "step: 70, loss: 0.0030820576939731836\n",
            "step: 80, loss: 0.003360083093866706\n",
            "step: 90, loss: 0.09238351136445999\n",
            "step: 100, loss: 0.004862761590629816\n",
            "step: 110, loss: 0.008742421865463257\n",
            "step: 120, loss: 0.0011908684391528368\n",
            "step: 130, loss: 0.003645041724666953\n",
            "step: 140, loss: 0.004497702233493328\n",
            "step: 150, loss: 0.004230888094753027\n",
            "step: 160, loss: 0.004655010532587767\n",
            "step: 170, loss: 0.0016838803421705961\n",
            "step: 180, loss: 0.005443362053483725\n",
            "step: 190, loss: 0.013484693132340908\n",
            "step: 200, loss: 0.008702441118657589\n",
            "step: 210, loss: 0.027105871587991714\n",
            "step: 220, loss: 0.0038557497318834066\n",
            "step: 230, loss: 0.10772474110126495\n",
            "step: 240, loss: 0.0042862980626523495\n",
            "step: 250, loss: 0.03561095893383026\n",
            "step: 260, loss: 0.0019520529313012958\n",
            "step: 270, loss: 0.012215596623718739\n",
            "step: 280, loss: 0.018566159531474113\n",
            "step: 290, loss: 0.02200276404619217\n",
            "step: 300, loss: 0.00029906368581578135\n",
            "step: 310, loss: 0.002358893398195505\n",
            "step: 320, loss: 0.05648963898420334\n",
            "step: 330, loss: 0.001366603304632008\n",
            "step: 340, loss: 0.060478899627923965\n",
            "step: 350, loss: 0.009563934989273548\n",
            "step: 360, loss: 0.03953031823039055\n",
            "step: 370, loss: 0.006730073131620884\n",
            "step: 380, loss: 0.019876591861248016\n",
            "step: 390, loss: 0.0046530296094715595\n",
            "step: 400, loss: 0.011304252780973911\n",
            "step: 410, loss: 0.00978096667677164\n",
            "step: 420, loss: 0.008133253082633018\n",
            "step: 430, loss: 0.11054114997386932\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 440, loss: 0.025807706639170647\n",
            "step: 450, loss: 0.01099482737481594\n",
            "step: 460, loss: 0.01945132575929165\n",
            "step: 470, loss: 0.05960230156779289\n",
            "step: 480, loss: 0.10183247178792953\n",
            "step: 490, loss: 0.004796828608959913\n",
            "step: 500, loss: 0.007065026555210352\n",
            "step: 510, loss: 0.06541437655687332\n",
            "step: 520, loss: 0.03248481824994087\n",
            "step: 530, loss: 0.022880196571350098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9361305361305362, f1=0.9333333333333333, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08912589401006699\n",
            "step: 10, loss: 0.03862324729561806\n",
            "step: 20, loss: 0.006208847276866436\n",
            "step: 30, loss: 0.003753799246624112\n",
            "step: 40, loss: 0.02384381927549839\n",
            "step: 50, loss: 0.004383810795843601\n",
            "step: 60, loss: 0.12086358666419983\n",
            "step: 70, loss: 0.19680537283420563\n",
            "step: 80, loss: 0.005156306549906731\n",
            "step: 90, loss: 0.006354981102049351\n",
            "step: 100, loss: 0.06642155349254608\n",
            "step: 110, loss: 0.0005409918958321214\n",
            "step: 120, loss: 0.005366073921322823\n",
            "step: 130, loss: 0.0004383940249681473\n",
            "step: 140, loss: 0.002132181078195572\n",
            "step: 150, loss: 0.020698845386505127\n",
            "step: 160, loss: 0.00026473868638277054\n",
            "step: 170, loss: 0.0624498575925827\n",
            "step: 180, loss: 0.001005880651064217\n",
            "step: 190, loss: 0.0011539848055690527\n",
            "step: 200, loss: 0.0028791711665689945\n",
            "step: 210, loss: 0.0006793620996177197\n",
            "step: 220, loss: 0.010668179020285606\n",
            "step: 230, loss: 0.008349776268005371\n",
            "step: 240, loss: 0.0032138654496520758\n",
            "step: 250, loss: 0.0017709607491269708\n",
            "step: 260, loss: 0.010667594149708748\n",
            "step: 270, loss: 0.002001209184527397\n",
            "step: 280, loss: 0.002103407634422183\n",
            "step: 290, loss: 0.12426038831472397\n",
            "step: 300, loss: 0.000418245792388916\n",
            "step: 310, loss: 0.000402203033445403\n",
            "step: 320, loss: 0.11806940287351608\n",
            "step: 330, loss: 0.11429297178983688\n",
            "step: 340, loss: 0.002484875964000821\n",
            "step: 350, loss: 0.0026825706008821726\n",
            "step: 360, loss: 0.005277905613183975\n",
            "step: 370, loss: 0.055437564849853516\n",
            "step: 380, loss: 0.0015057047130540013\n",
            "step: 390, loss: 0.000498096109367907\n",
            "step: 400, loss: 0.005715100094676018\n",
            "step: 410, loss: 0.0009538005106151104\n",
            "step: 420, loss: 0.007912706583738327\n",
            "step: 430, loss: 0.006325682625174522\n",
            "step: 440, loss: 0.02003752999007702\n",
            "step: 450, loss: 0.019340885803103447\n",
            "step: 460, loss: 0.00104944605845958\n",
            "step: 470, loss: 0.005369126331061125\n",
            "step: 480, loss: 0.10024020820856094\n",
            "step: 490, loss: 0.0012233038432896137\n",
            "step: 500, loss: 0.018038801848888397\n",
            "step: 510, loss: 0.02371320128440857\n",
            "step: 520, loss: 0.020172329619526863\n",
            "step: 530, loss: 0.005414656363427639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.940566037735849, f1=0.9307116104868914, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009600095800124109\n",
            "step: 10, loss: 0.0005235603312030435\n",
            "step: 20, loss: 0.10374331474304199\n",
            "step: 30, loss: 0.0021930246148258448\n",
            "step: 40, loss: 0.00031540298368781805\n",
            "step: 50, loss: 0.049377523362636566\n",
            "step: 60, loss: 0.00014161382569000125\n",
            "step: 70, loss: 0.00023862929083406925\n",
            "step: 80, loss: 0.00018618536705616862\n",
            "step: 90, loss: 0.00017717316222842783\n",
            "step: 100, loss: 0.061445996165275574\n",
            "step: 110, loss: 0.0005773320444859564\n",
            "step: 120, loss: 0.0004289918579161167\n",
            "step: 130, loss: 0.005656825844198465\n",
            "step: 140, loss: 0.0027461564168334007\n",
            "step: 150, loss: 0.002017123159021139\n",
            "step: 160, loss: 0.000824645918328315\n",
            "step: 170, loss: 0.0033330018632113934\n",
            "step: 180, loss: 0.0015987918013706803\n",
            "step: 190, loss: 0.0014492650516331196\n",
            "step: 200, loss: 0.005687995348125696\n",
            "step: 210, loss: 0.01502626109868288\n",
            "step: 220, loss: 0.010803074575960636\n",
            "step: 230, loss: 0.002216694178059697\n",
            "step: 240, loss: 0.03789214789867401\n",
            "step: 250, loss: 0.06622784584760666\n",
            "step: 260, loss: 0.0009032702655531466\n",
            "step: 270, loss: 0.08040814846754074\n",
            "step: 280, loss: 0.002779910573735833\n",
            "step: 290, loss: 0.0005645607598125935\n",
            "step: 300, loss: 0.0009104465716518462\n",
            "step: 310, loss: 0.0005275861476548016\n",
            "step: 320, loss: 0.0004045942041557282\n",
            "step: 330, loss: 0.0008874892955645919\n",
            "step: 340, loss: 0.12026768177747726\n",
            "step: 350, loss: 0.0036504080053418875\n",
            "step: 360, loss: 0.01670900732278824\n",
            "step: 370, loss: 0.006203827448189259\n",
            "step: 380, loss: 0.0010106973350048065\n",
            "step: 390, loss: 0.008107256144285202\n",
            "step: 400, loss: 0.00018949908553622663\n",
            "step: 410, loss: 0.00024157826555892825\n",
            "step: 420, loss: 0.05080390349030495\n",
            "step: 430, loss: 0.0006488944054581225\n",
            "step: 440, loss: 0.00013730583305004984\n",
            "step: 450, loss: 0.0002755539317149669\n",
            "step: 460, loss: 0.000630476395599544\n",
            "step: 470, loss: 0.003791116876527667\n",
            "step: 480, loss: 0.05642838403582573\n",
            "step: 490, loss: 0.007940880954265594\n",
            "step: 500, loss: 0.000495652318932116\n",
            "step: 510, loss: 0.011270133778452873\n",
            "step: 520, loss: 0.007844940759241581\n",
            "step: 530, loss: 0.015047477558255196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.937037037037037, f1=0.9345622119815669, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001762198400683701\n",
            "step: 10, loss: 0.04614872485399246\n",
            "step: 20, loss: 0.08495283126831055\n",
            "step: 30, loss: 0.003821341088041663\n",
            "step: 40, loss: 0.017215527594089508\n",
            "step: 50, loss: 0.0032850028946995735\n",
            "step: 60, loss: 0.0031086658127605915\n",
            "step: 70, loss: 0.0012383689172565937\n",
            "step: 80, loss: 0.0016527457628399134\n",
            "step: 90, loss: 0.0002535604580771178\n",
            "step: 100, loss: 0.00026045070262625813\n",
            "step: 110, loss: 0.029041487723588943\n",
            "step: 120, loss: 0.0006995477015152574\n",
            "step: 130, loss: 0.012135009281337261\n",
            "step: 140, loss: 0.0007879044278524816\n",
            "step: 150, loss: 0.00040070584509521723\n",
            "step: 160, loss: 0.00011132559302495793\n",
            "step: 170, loss: 0.0007918429910205305\n",
            "step: 180, loss: 0.0012603342765942216\n",
            "step: 190, loss: 0.003316395916044712\n",
            "step: 200, loss: 0.00012022574810544029\n",
            "step: 210, loss: 0.04373618960380554\n",
            "step: 220, loss: 0.0003124377108179033\n",
            "step: 230, loss: 0.014501760713756084\n",
            "step: 240, loss: 0.0021739862859249115\n",
            "step: 250, loss: 0.0036295459140092134\n",
            "step: 260, loss: 7.824500789865851e-05\n",
            "step: 270, loss: 0.003978237509727478\n",
            "step: 280, loss: 0.00014809585991315544\n",
            "step: 290, loss: 0.00016840647731442004\n",
            "step: 300, loss: 0.0012778755044564605\n",
            "step: 310, loss: 4.060022911289707e-05\n",
            "step: 320, loss: 0.0002453448250889778\n",
            "step: 330, loss: 0.062389880418777466\n",
            "step: 340, loss: 0.029132938012480736\n",
            "step: 350, loss: 0.0010760592995211482\n",
            "step: 360, loss: 0.0026682044845074415\n",
            "step: 370, loss: 0.0006179802003316581\n",
            "step: 380, loss: 0.0006062220199964941\n",
            "step: 390, loss: 0.011426093056797981\n",
            "step: 400, loss: 0.015713118016719818\n",
            "step: 410, loss: 0.001998194260522723\n",
            "step: 420, loss: 0.0014009373262524605\n",
            "step: 430, loss: 0.010004639625549316\n",
            "step: 440, loss: 0.0022888407111167908\n",
            "step: 450, loss: 0.0020676949061453342\n",
            "step: 460, loss: 0.001231490634381771\n",
            "step: 470, loss: 0.007058532442897558\n",
            "step: 480, loss: 0.001145065645687282\n",
            "step: 490, loss: 0.0003064429620280862\n",
            "step: 500, loss: 0.0005392829771153629\n",
            "step: 510, loss: 0.005440752487629652\n",
            "step: 520, loss: 0.05500676855444908\n",
            "step: 530, loss: 0.013700399547815323\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9424964936886395, f1=0.9350770667912189, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032278711441904306\n",
            "step: 10, loss: 0.011908264830708504\n",
            "step: 20, loss: 0.0007992186001501977\n",
            "step: 30, loss: 0.11784708499908447\n",
            "step: 40, loss: 0.005061198491603136\n",
            "step: 50, loss: 0.0024486894253641367\n",
            "step: 60, loss: 0.005445966962724924\n",
            "step: 70, loss: 0.0007303571328520775\n",
            "step: 80, loss: 0.0013811865355819464\n",
            "step: 90, loss: 0.0018923073075711727\n",
            "step: 100, loss: 0.0003188431728631258\n",
            "step: 110, loss: 0.0014963001012802124\n",
            "step: 120, loss: 0.0007198599632829428\n",
            "step: 130, loss: 0.003525699721649289\n",
            "step: 140, loss: 0.001964612863957882\n",
            "step: 150, loss: 0.0008087062160484493\n",
            "step: 160, loss: 0.003417676780372858\n",
            "step: 170, loss: 0.011292004957795143\n",
            "step: 180, loss: 0.0005249793757684529\n",
            "step: 190, loss: 0.00306318630464375\n",
            "step: 200, loss: 0.0014358183834701777\n",
            "step: 210, loss: 0.010200487449765205\n",
            "step: 220, loss: 0.0021623969078063965\n",
            "step: 230, loss: 0.0011309412075206637\n",
            "step: 240, loss: 0.020101821050047874\n",
            "step: 250, loss: 0.002484625671058893\n",
            "step: 260, loss: 0.00016601852257736027\n",
            "step: 270, loss: 0.013582170009613037\n",
            "step: 280, loss: 0.07201102375984192\n",
            "step: 290, loss: 0.0003168561961501837\n",
            "step: 300, loss: 0.0041598966345191\n",
            "step: 310, loss: 0.0031282398849725723\n",
            "step: 320, loss: 0.0008234693668782711\n",
            "step: 330, loss: 0.0006481494638137519\n",
            "step: 340, loss: 0.0006094048731029034\n",
            "step: 350, loss: 0.0008307248936034739\n",
            "step: 360, loss: 0.012036353349685669\n",
            "step: 370, loss: 0.0004903123481199145\n",
            "step: 380, loss: 0.004625431727617979\n",
            "step: 390, loss: 0.1348719298839569\n",
            "step: 400, loss: 0.0003047392820008099\n",
            "step: 410, loss: 0.0009299339144490659\n",
            "step: 420, loss: 0.004049584269523621\n",
            "step: 430, loss: 0.025896424427628517\n",
            "step: 440, loss: 0.01230963971465826\n",
            "step: 450, loss: 0.00011028658627765253\n",
            "step: 460, loss: 0.0005138297565281391\n",
            "step: 470, loss: 0.00035739768645726144\n",
            "step: 480, loss: 0.0004346403875388205\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 490, loss: 0.003082094481214881\n",
            "step: 500, loss: 0.002112076384946704\n",
            "step: 510, loss: 0.0006430052453652024\n",
            "step: 520, loss: 0.002551455283537507\n",
            "step: 530, loss: 0.0003768884635064751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9304267161410019, f1=0.9330889092575618, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005385462660342455\n",
            "step: 10, loss: 0.015247256495058537\n",
            "step: 20, loss: 0.01507467683404684\n",
            "step: 30, loss: 0.0005995542160235345\n",
            "step: 40, loss: 0.0003961297916248441\n",
            "step: 50, loss: 0.0019224494462832808\n",
            "step: 60, loss: 0.00015256600454449654\n",
            "step: 70, loss: 0.0008623882313258946\n",
            "step: 80, loss: 0.0035148952156305313\n",
            "step: 90, loss: 7.394226850010455e-05\n",
            "step: 100, loss: 0.00016357410640921444\n",
            "step: 110, loss: 0.0001564693229738623\n",
            "step: 120, loss: 0.00041850589332170784\n",
            "step: 130, loss: 0.0029187400359660387\n",
            "step: 140, loss: 0.02672170288860798\n",
            "step: 150, loss: 0.005857937969267368\n",
            "step: 160, loss: 0.000993768684566021\n",
            "step: 170, loss: 0.014497078955173492\n",
            "step: 180, loss: 0.00019159112707711756\n",
            "step: 190, loss: 0.00041046395199373364\n",
            "step: 200, loss: 0.0009450875222682953\n",
            "step: 210, loss: 0.00024366623256355524\n",
            "step: 220, loss: 0.03561023250222206\n",
            "step: 230, loss: 0.00019716887618415058\n",
            "step: 240, loss: 0.0002027001464739442\n",
            "step: 250, loss: 0.019835907965898514\n",
            "step: 260, loss: 0.029962463304400444\n",
            "step: 270, loss: 0.00010657548409653828\n",
            "step: 280, loss: 0.0025710423942655325\n",
            "step: 290, loss: 0.03187975659966469\n",
            "step: 300, loss: 0.00030262547079473734\n",
            "step: 310, loss: 0.017620563507080078\n",
            "step: 320, loss: 0.0002608778013382107\n",
            "step: 330, loss: 0.0008856051717884839\n",
            "step: 340, loss: 0.0008817656198516488\n",
            "step: 350, loss: 0.04030049964785576\n",
            "step: 360, loss: 0.00010748279601102695\n",
            "step: 370, loss: 0.004427964333444834\n",
            "step: 380, loss: 6.406501779565588e-05\n",
            "step: 390, loss: 6.844118615845218e-05\n",
            "step: 400, loss: 0.013918484561145306\n",
            "step: 410, loss: 0.0003684677940327674\n",
            "step: 420, loss: 0.021275997161865234\n",
            "step: 430, loss: 7.153901970013976e-05\n",
            "step: 440, loss: 0.0012883482268080115\n",
            "step: 450, loss: 7.067326077958569e-05\n",
            "step: 460, loss: 6.823390140198171e-05\n",
            "step: 470, loss: 3.6274032026994973e-05\n",
            "step: 480, loss: 5.947234603809193e-05\n",
            "step: 490, loss: 0.0018937716959044337\n",
            "step: 500, loss: 0.0008308116812258959\n",
            "step: 510, loss: 0.00022044249635655433\n",
            "step: 520, loss: 7.815063872840255e-05\n",
            "step: 530, loss: 6.146530358819291e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9364107395195479, f1=0.9243065350258581, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006051960866898298\n",
            "step: 10, loss: 0.00013817936996929348\n",
            "step: 20, loss: 3.358930553076789e-05\n",
            "step: 30, loss: 0.00018263564561493695\n",
            "step: 40, loss: 0.0018219614867120981\n",
            "step: 50, loss: 0.00010461773490533233\n",
            "step: 60, loss: 0.00010282814037054777\n",
            "step: 70, loss: 0.0001090989971999079\n",
            "step: 80, loss: 4.778481161338277e-05\n",
            "step: 90, loss: 0.01678111031651497\n",
            "step: 100, loss: 0.00013860267063137144\n",
            "step: 110, loss: 3.404427116038278e-05\n",
            "step: 120, loss: 0.0014134857337921858\n",
            "step: 130, loss: 0.00012495573901105672\n",
            "step: 140, loss: 0.014825576916337013\n",
            "step: 150, loss: 0.0007151653990149498\n",
            "step: 160, loss: 8.083043212536722e-05\n",
            "step: 170, loss: 0.0005307603860273957\n",
            "step: 180, loss: 0.0008418934303335845\n",
            "step: 190, loss: 0.0004950478905811906\n",
            "step: 200, loss: 0.0008719688630662858\n",
            "step: 210, loss: 9.26822394831106e-05\n",
            "step: 220, loss: 0.006014689803123474\n",
            "step: 230, loss: 0.0037410634104162455\n",
            "step: 240, loss: 0.007587210275232792\n",
            "step: 250, loss: 0.0004492154985200614\n",
            "step: 260, loss: 0.0012634817976504564\n",
            "step: 270, loss: 0.0005070297047495842\n",
            "step: 280, loss: 0.0005063664866611362\n",
            "step: 290, loss: 0.00020957400556653738\n",
            "step: 300, loss: 0.0037484089843928814\n",
            "step: 310, loss: 0.0003292392357252538\n",
            "step: 320, loss: 0.0012908661738038063\n",
            "step: 330, loss: 0.0017994447844102979\n",
            "step: 340, loss: 0.009158669970929623\n",
            "step: 350, loss: 0.00013048553955741227\n",
            "step: 360, loss: 0.0007071148720569909\n",
            "step: 370, loss: 0.003760724328458309\n",
            "step: 380, loss: 0.3022940754890442\n",
            "step: 390, loss: 0.0005947727477177978\n",
            "step: 400, loss: 0.013501091860234737\n",
            "step: 410, loss: 0.016940820962190628\n",
            "step: 420, loss: 0.0007826121291145682\n",
            "step: 430, loss: 0.00018365599680691957\n",
            "step: 440, loss: 0.0018448163755238056\n",
            "step: 450, loss: 5.6601118558319286e-05\n",
            "step: 460, loss: 0.0007708513294346631\n",
            "step: 470, loss: 0.004652512725442648\n",
            "step: 480, loss: 0.00027325566043145955\n",
            "step: 490, loss: 0.0001858882897067815\n",
            "step: 500, loss: 0.005451507866382599\n",
            "step: 510, loss: 0.013291459530591965\n",
            "step: 520, loss: 0.0001456875033909455\n",
            "step: 530, loss: 0.0016932319849729538\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.941230911614993, f1=0.9344413665743306, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001160383180831559\n",
            "step: 10, loss: 0.0007091814768500626\n",
            "step: 20, loss: 0.0004310276999603957\n",
            "step: 30, loss: 0.0006009223288856447\n",
            "step: 40, loss: 0.002942407503724098\n",
            "step: 50, loss: 0.00040150509448722005\n",
            "step: 60, loss: 0.050766997039318085\n",
            "step: 70, loss: 6.283899710979313e-05\n",
            "step: 80, loss: 0.001326612662523985\n",
            "step: 90, loss: 3.403505979804322e-05\n",
            "step: 100, loss: 0.00148426101077348\n",
            "step: 110, loss: 0.0026008840650320053\n",
            "step: 120, loss: 0.00030794565100222826\n",
            "step: 130, loss: 0.00015328630979638547\n",
            "step: 140, loss: 0.00035586333251558244\n",
            "step: 150, loss: 3.177203325321898e-05\n",
            "step: 160, loss: 4.626892041414976e-05\n",
            "step: 170, loss: 0.00010454103903612122\n",
            "step: 180, loss: 0.0031863427720963955\n",
            "step: 190, loss: 0.0004815188003703952\n",
            "step: 200, loss: 0.005612696520984173\n",
            "step: 210, loss: 0.0003301371762063354\n",
            "step: 220, loss: 0.0021217993926256895\n",
            "step: 230, loss: 0.000613864918705076\n",
            "step: 240, loss: 0.0005558126140385866\n",
            "step: 250, loss: 2.5334826204925776e-05\n",
            "step: 260, loss: 0.0005404465482570231\n",
            "step: 270, loss: 0.0013835816644132137\n",
            "step: 280, loss: 0.0002080425329040736\n",
            "step: 290, loss: 0.000659829645883292\n",
            "step: 300, loss: 0.002165385289117694\n",
            "step: 310, loss: 0.0015665048267692327\n",
            "step: 320, loss: 0.0003279412048868835\n",
            "step: 330, loss: 0.000719727948307991\n",
            "step: 340, loss: 0.013687783852219582\n",
            "step: 350, loss: 0.00015227605763357133\n",
            "step: 360, loss: 0.00015920065925456583\n",
            "step: 370, loss: 0.010535934008657932\n",
            "step: 380, loss: 0.0006844506715424359\n",
            "step: 390, loss: 3.6636778531828895e-05\n",
            "step: 400, loss: 0.00010961174848489463\n",
            "step: 410, loss: 0.0010880357585847378\n",
            "step: 420, loss: 0.0020641740411520004\n",
            "step: 430, loss: 0.00010380493040429428\n",
            "step: 440, loss: 0.0012729560257866979\n",
            "step: 450, loss: 0.0007002807105891407\n",
            "step: 460, loss: 0.04299754649400711\n",
            "step: 470, loss: 0.0010000338079407811\n",
            "step: 480, loss: 0.0014090609038248658\n",
            "step: 490, loss: 0.0020889148581773043\n",
            "step: 500, loss: 0.0005430825985968113\n",
            "step: 510, loss: 0.01497254054993391\n",
            "step: 520, loss: 3.415363607928157e-05\n",
            "step: 530, loss: 0.0013677716488018632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9362511893434825, f1=0.9275225011842728, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.152195368893445e-05\n",
            "step: 10, loss: 0.000594529090449214\n",
            "step: 20, loss: 0.00014212523819878697\n",
            "step: 30, loss: 0.0029545719735324383\n",
            "step: 40, loss: 0.0012513544643297791\n",
            "step: 50, loss: 0.014616010710597038\n",
            "step: 60, loss: 0.002761795883998275\n",
            "step: 70, loss: 0.0004371034156065434\n",
            "step: 80, loss: 0.0008627185598015785\n",
            "step: 90, loss: 0.001926093245856464\n",
            "step: 100, loss: 0.004140346776694059\n",
            "step: 110, loss: 0.000623348809313029\n",
            "step: 120, loss: 7.005884981481358e-05\n",
            "step: 130, loss: 0.0012913875980302691\n",
            "step: 140, loss: 2.618433973111678e-05\n",
            "step: 150, loss: 8.234747656388208e-05\n",
            "step: 160, loss: 0.0002777794434223324\n",
            "step: 170, loss: 0.00013949688582215458\n",
            "step: 180, loss: 0.00013304651656653732\n",
            "step: 190, loss: 7.831682887626812e-05\n",
            "step: 200, loss: 0.0009099471499212086\n",
            "step: 210, loss: 0.10664243251085281\n",
            "step: 220, loss: 0.06698650866746902\n",
            "step: 230, loss: 2.748431325017009e-05\n",
            "step: 240, loss: 0.029580306261777878\n",
            "step: 250, loss: 0.00020138385298196226\n",
            "step: 260, loss: 0.000402259174734354\n",
            "step: 270, loss: 0.04797749221324921\n",
            "step: 280, loss: 0.0020235555712133646\n",
            "step: 290, loss: 0.0010640803957358003\n",
            "step: 300, loss: 0.008988097310066223\n",
            "step: 310, loss: 0.0017925251740962267\n",
            "step: 320, loss: 0.0008576316758990288\n",
            "step: 330, loss: 0.0006665520486421883\n",
            "step: 340, loss: 0.0005333898006938398\n",
            "step: 350, loss: 0.005688670556992292\n",
            "step: 360, loss: 0.0008250526152551174\n",
            "step: 370, loss: 0.007181132212281227\n",
            "step: 380, loss: 0.00036131354863755405\n",
            "step: 390, loss: 0.0002527305914554745\n",
            "step: 400, loss: 9.957607835531235e-05\n",
            "step: 410, loss: 0.0008033594349399209\n",
            "step: 420, loss: 0.00275238836184144\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 430, loss: 0.016737356781959534\n",
            "step: 440, loss: 0.0005517117097042501\n",
            "step: 450, loss: 0.054982807487249374\n",
            "step: 460, loss: 3.206576730008237e-05\n",
            "step: 470, loss: 5.954884545644745e-05\n",
            "step: 480, loss: 0.0006215705070644617\n",
            "step: 490, loss: 0.0004638737300410867\n",
            "step: 500, loss: 9.271187445847318e-05\n",
            "step: 510, loss: 0.001555637107230723\n",
            "step: 520, loss: 3.0359529773704708e-05\n",
            "step: 530, loss: 0.0006913365214131773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9358490566037737, f1=0.9256820319849483, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00042412642505951226\n",
            "step: 10, loss: 6.520071474369615e-05\n",
            "step: 20, loss: 0.00055231514852494\n",
            "step: 30, loss: 0.0001893620501505211\n",
            "step: 40, loss: 3.247551649110392e-05\n",
            "step: 50, loss: 0.00194718805141747\n",
            "step: 60, loss: 0.0004938740748912096\n",
            "step: 70, loss: 6.836315151304007e-05\n",
            "step: 80, loss: 0.0002690812689252198\n",
            "step: 90, loss: 0.024553533643484116\n",
            "step: 100, loss: 0.0005218240548856556\n",
            "step: 110, loss: 0.0002920805709436536\n",
            "step: 120, loss: 0.00018999757594428957\n",
            "step: 130, loss: 0.00015150191029533744\n",
            "step: 140, loss: 0.004965293221175671\n",
            "step: 150, loss: 0.0013258750550448895\n",
            "step: 160, loss: 5.111169957672246e-05\n",
            "step: 170, loss: 5.337366383173503e-05\n",
            "step: 180, loss: 5.363774471334182e-05\n",
            "step: 190, loss: 0.00015978625742718577\n",
            "step: 200, loss: 6.482560274889693e-05\n",
            "step: 210, loss: 0.00025843645562417805\n",
            "step: 220, loss: 5.9895464801229537e-05\n",
            "step: 230, loss: 6.072139149182476e-05\n",
            "step: 240, loss: 0.00031323067378252745\n",
            "step: 250, loss: 0.00014778853801544756\n",
            "step: 260, loss: 0.007190557196736336\n",
            "step: 270, loss: 4.2638937884476036e-05\n",
            "step: 280, loss: 2.437768125673756e-05\n",
            "step: 290, loss: 9.527287329547107e-05\n",
            "step: 300, loss: 7.53647182136774e-05\n",
            "step: 310, loss: 0.005774525459855795\n",
            "step: 320, loss: 0.00256898021325469\n",
            "step: 330, loss: 0.0014097450766712427\n",
            "step: 340, loss: 2.826956369972322e-05\n",
            "step: 350, loss: 3.246363849029876e-05\n",
            "step: 360, loss: 0.000107012310763821\n",
            "step: 370, loss: 0.0004137996002100408\n",
            "step: 380, loss: 0.00011010811431333423\n",
            "step: 390, loss: 0.0024486996699124575\n",
            "step: 400, loss: 0.0002931435010395944\n",
            "step: 410, loss: 0.006553260609507561\n",
            "step: 420, loss: 0.03586304932832718\n",
            "step: 430, loss: 0.0006816595559939742\n",
            "step: 440, loss: 0.0001949752913787961\n",
            "step: 450, loss: 0.00514135230332613\n",
            "step: 460, loss: 0.013235070742666721\n",
            "step: 470, loss: 0.00020710896933451295\n",
            "step: 480, loss: 0.00017352664144709706\n",
            "step: 490, loss: 3.4614022297319025e-05\n",
            "step: 500, loss: 0.0006529099773615599\n",
            "step: 510, loss: 0.05326385796070099\n",
            "step: 520, loss: 0.0008923950372263789\n",
            "step: 530, loss: 0.00018046863260678947\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9345266132830901, f1=0.9275634995296332, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019172033062204719\n",
            "step: 10, loss: 3.61174825229682e-05\n",
            "step: 20, loss: 9.232215961674228e-05\n",
            "step: 30, loss: 4.0086139051709324e-05\n",
            "step: 40, loss: 0.00232674740254879\n",
            "step: 50, loss: 0.00021519461006391793\n",
            "step: 60, loss: 2.7644400688586757e-05\n",
            "step: 70, loss: 0.0004288137424737215\n",
            "step: 80, loss: 3.101552647422068e-05\n",
            "step: 90, loss: 6.129900430096313e-05\n",
            "step: 100, loss: 0.02506747469305992\n",
            "step: 110, loss: 0.00010501797805773094\n",
            "step: 120, loss: 0.0005138287669979036\n",
            "step: 130, loss: 0.008153699338436127\n",
            "step: 140, loss: 0.004576202016323805\n",
            "step: 150, loss: 0.029725363478064537\n",
            "step: 160, loss: 0.0009786138543859124\n",
            "step: 170, loss: 0.0013019187608733773\n",
            "step: 180, loss: 4.111107045901008e-05\n",
            "step: 190, loss: 0.0003756332735065371\n",
            "step: 200, loss: 0.005776016507297754\n",
            "step: 210, loss: 3.960657340940088e-05\n",
            "step: 220, loss: 0.00011576130782486871\n",
            "step: 230, loss: 0.001357392524369061\n",
            "step: 240, loss: 0.00438730837777257\n",
            "step: 250, loss: 9.847761248238385e-05\n",
            "step: 260, loss: 0.0008594162063673139\n",
            "step: 270, loss: 0.00038569417665712535\n",
            "step: 280, loss: 0.00042038559331558645\n",
            "step: 290, loss: 0.00027351052267476916\n",
            "step: 300, loss: 0.0010623699054121971\n",
            "step: 310, loss: 0.00015276740305125713\n",
            "step: 320, loss: 0.001136264530941844\n",
            "step: 330, loss: 0.0028000620659440756\n",
            "step: 340, loss: 0.0007712118094787002\n",
            "step: 350, loss: 0.002578641753643751\n",
            "step: 360, loss: 0.0010320640867576003\n",
            "step: 370, loss: 6.289142766036093e-05\n",
            "step: 380, loss: 0.0003593284636735916\n",
            "step: 390, loss: 0.05029962211847305\n",
            "step: 400, loss: 9.075069101527333e-05\n",
            "step: 410, loss: 4.59542716271244e-05\n",
            "step: 420, loss: 0.0006217111949808896\n",
            "step: 430, loss: 0.000752398103941232\n",
            "step: 440, loss: 0.00033534131944179535\n",
            "step: 450, loss: 0.00016270054038614035\n",
            "step: 460, loss: 0.00035480171209201217\n",
            "step: 470, loss: 0.0002093836519634351\n",
            "step: 480, loss: 5.6690321798669174e-05\n",
            "step: 490, loss: 7.974875188665465e-05\n",
            "step: 500, loss: 4.58138165413402e-05\n",
            "step: 510, loss: 0.04684567078948021\n",
            "step: 520, loss: 0.0002846584829967469\n",
            "step: 530, loss: 0.0010326525662094355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9368372521899492, f1=0.9364426154549611, best_f1=0.9370437956204379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.440081465872936e-05\n",
            "step: 10, loss: 5.664639320457354e-05\n",
            "step: 20, loss: 8.526291640009731e-05\n",
            "step: 30, loss: 0.00013054319424554706\n",
            "step: 40, loss: 0.08032801002264023\n",
            "step: 50, loss: 5.145139220985584e-05\n",
            "step: 60, loss: 4.88908844999969e-05\n",
            "step: 70, loss: 0.0008211205713450909\n",
            "step: 80, loss: 4.557389911497012e-05\n",
            "step: 90, loss: 4.3948082748102024e-05\n",
            "step: 100, loss: 0.00045650440733879805\n",
            "step: 110, loss: 0.0012246210826560855\n",
            "step: 120, loss: 0.00019105669343844056\n",
            "step: 130, loss: 0.10403910279273987\n",
            "step: 140, loss: 4.557523425319232e-05\n",
            "step: 150, loss: 8.243363117799163e-05\n",
            "step: 160, loss: 5.547218461288139e-05\n",
            "step: 170, loss: 0.00016803100879769772\n",
            "step: 180, loss: 9.58741147769615e-05\n",
            "step: 190, loss: 0.0001658374967519194\n",
            "step: 200, loss: 0.00011551322677405551\n",
            "step: 210, loss: 0.0015526128700003028\n",
            "step: 220, loss: 6.634386227233335e-05\n",
            "step: 230, loss: 0.0010198801755905151\n",
            "step: 240, loss: 4.60703122371342e-05\n",
            "step: 250, loss: 0.0001810524263419211\n",
            "step: 260, loss: 0.0055924407206475735\n",
            "step: 270, loss: 1.7426591512048617e-05\n",
            "step: 280, loss: 3.0436527595156804e-05\n",
            "step: 290, loss: 2.416899769741576e-05\n",
            "step: 300, loss: 4.34864305134397e-05\n",
            "step: 310, loss: 0.0005451447796076536\n",
            "step: 320, loss: 0.0007119920337572694\n",
            "step: 330, loss: 0.00034998764749616385\n",
            "step: 340, loss: 0.00019947152759414166\n",
            "step: 350, loss: 0.002161626936867833\n",
            "step: 360, loss: 0.0018804160645231605\n",
            "step: 370, loss: 0.00016275219968520105\n",
            "step: 380, loss: 0.0006496002897620201\n",
            "step: 390, loss: 0.005795896053314209\n",
            "step: 400, loss: 0.0011329872068017721\n",
            "step: 410, loss: 0.0007592998445034027\n",
            "step: 420, loss: 7.051281863823533e-05\n",
            "step: 430, loss: 0.0003246846026740968\n",
            "step: 440, loss: 8.699990576133132e-05\n",
            "step: 450, loss: 5.291198976919986e-05\n",
            "step: 460, loss: 2.8795404432457872e-05\n",
            "step: 470, loss: 0.0014030815800651908\n",
            "step: 480, loss: 3.7615784094668925e-05\n",
            "step: 490, loss: 1.537407297291793e-05\n",
            "step: 500, loss: 0.0002840383385773748\n",
            "step: 510, loss: 7.055851892801002e-05\n",
            "step: 520, loss: 0.00020830114954151213\n",
            "step: 530, loss: 0.0005928856553509831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9370892018779343, f1=0.9314685314685315, best_f1=0.9370437956204379\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 252.71it/s]\n",
            "load_f1 = 0.9381584974805315\n",
            "real_f1 = 0.9388686131386861\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 255.71it/s]\n"
          ]
        }
      ]
    }
  ]
}