{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADirty_90_3_5_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "zW6LV4zMhstv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4d96c56e-9d2e-4de4-d350-20d1c4efdbd2"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 23.72 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 56.5 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 31.3 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 22.70 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 3.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.9 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 28.0 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 50.0 MB/s \n",
            "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=9e2251f61747fa5b7535a92c60a3d3dd2f1853ee35cdb5502e28a99d2b5f8c46\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=8017edeac256484cdfc23c98624971d72a9a2b92ae8badcf0271cbb11dbfb505\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7d7e86-d261-49b5-b306-cc89c818dbbf"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10117, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 10117 (delta 118), reused 152 (delta 62), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10117/10117), 14.96 MiB | 27.51 MiB/s, done.\n",
            "Resolving deltas: 100% (6922/6922), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-5wffcd1b\n",
            "Created temporary directory: /tmp/pip-req-tracker-8hkpjas0\n",
            "Initialized build tracking at /tmp/pip-req-tracker-8hkpjas0\n",
            "Created build tracker: /tmp/pip-req-tracker-8hkpjas0\n",
            "Entered build tracker: /tmp/pip-req-tracker-8hkpjas0\n",
            "Created temporary directory: /tmp/pip-install-spd1krn_\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-_hs43qmk\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-8hkpjas0'\n",
            "    Running setup.py (path:/tmp/pip-req-build-_hs43qmk/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-205g9gqt\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-205g9gqt/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-205g9gqt/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-205g9gqt/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-205g9gqt/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-205g9gqt/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-205g9gqt/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-_hs43qmk has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-8hkpjas0'\n",
            "Created temporary directory: /tmp/pip-unpack-ede66ry_\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-jv3tllm0\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-jv3tllm0\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-_hs43qmk/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-_hs43qmk/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-jv3tllm0\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-jv3tllm0/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298244 sha256=8d0fcea2835ce2f8e38770d0fa4113ddf166fff96c1b2a281ffeac9e656be717\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5wffcd1b/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-8hkpjas0'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70260ca9-5fc1-49ee-d50b-0e04509d0f21"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.48-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 24.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.48\n",
            "  Downloading botocore-1.27.48-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.48->awscli) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.48->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.48 botocore-1.27.48 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82b0dc1-9c85-4705-c469-13baa6dd2a8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "ed5f7953-456b-4d37-c7c9-2a8e73a9f407"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 1014, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 1014 (delta 27), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (1014/1014), 254.12 MiB | 31.04 MiB/s, done.\n",
            "Resolving deltas: 100% (611/611), done.\n",
            "Checking out files: 100% (1284/1284), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1856b4-24b0-4830-fba3-6fcbd59284de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/ADirty_90_3_5/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d6bc59-9fc9-4216-f57e-953588c1e9bc"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rDownloading:   0% 0.00/433 [00:00<?, ?B/s]\rDownloading: 100% 433/433 [00:00<00:00, 477kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 21.7MB/s]\n",
            "Downloading: 100% 440M/440M [00:10<00:00, 41.5MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5720033049583435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.34146341463414637, f1=0.32558139534883723, best_f1=0.32558139534883723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5069850087165833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2666666666666667, f1=0.2666666666666667, best_f1=0.32558139534883723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5798655152320862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5, f1=0.4761904761904762, best_f1=0.4761904761904762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3368094861507416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5384615384615384, f1=0.39285714285714285, best_f1=0.39285714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3068941533565521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.689655172413793, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24506069719791412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6666666666666666, f1=0.588235294117647, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30417242646217346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7741935483870968, f1=0.6285714285714286, best_f1=0.6285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09796712547540665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7999999999999999, f1=0.6470588235294117, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0074877869337797165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6666666666666667, f1=0.6111111111111112, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07175976037979126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6086956521739131, f1=0.6086956521739131, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024537667632102966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6875000000000001, f1=0.689655172413793, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012160641141235828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7647058823529412, f1=0.689655172413793, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03215481713414192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7647058823529412, f1=0.6451612903225806, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005913224071264267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7647058823529412, f1=0.6451612903225806, best_f1=0.6470588235294117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021892879158258438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7647058823529412, f1=0.6451612903225806, best_f1=0.6470588235294117\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 137394.41it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7222222222222223\n",
            "real_f1 = 0.7222222222222223\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:14, 297.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2238fbb-3205-4513-ea3d-4140bcff71ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5949347019195557\n",
            "step: 10, loss: 0.6474952101707458\n",
            "step: 20, loss: 0.47262078523635864\n",
            "step: 30, loss: 0.2920142412185669\n",
            "step: 40, loss: 0.24742825329303741\n",
            "step: 50, loss: 0.18259882926940918\n",
            "step: 60, loss: 0.03269226849079132\n",
            "step: 70, loss: 0.03941121697425842\n",
            "step: 80, loss: 0.12630808353424072\n",
            "step: 90, loss: 0.13624577224254608\n",
            "step: 100, loss: 0.00811282079666853\n",
            "step: 110, loss: 0.255650132894516\n",
            "step: 120, loss: 0.010322793386876583\n",
            "step: 130, loss: 0.02224976383149624\n",
            "step: 140, loss: 0.002848349278792739\n",
            "step: 150, loss: 0.05224922299385071\n",
            "step: 160, loss: 0.03262464329600334\n",
            "step: 170, loss: 0.03923500329256058\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.027710745111107826\n",
            "step: 190, loss: 0.10529842972755432\n",
            "step: 200, loss: 0.12410972267389297\n",
            "step: 210, loss: 0.0037677818909287453\n",
            "step: 220, loss: 0.023919792845845222\n",
            "step: 230, loss: 0.08862630277872086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9731543624161074, f1=0.9694915254237287, best_f1=0.9694915254237287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005354568362236023\n",
            "step: 10, loss: 0.003106033429503441\n",
            "step: 20, loss: 0.0682389885187149\n",
            "step: 30, loss: 0.19083403050899506\n",
            "step: 40, loss: 0.04039183631539345\n",
            "step: 50, loss: 0.00621689623221755\n",
            "step: 60, loss: 0.00247971061617136\n",
            "step: 70, loss: 0.061013251543045044\n",
            "step: 80, loss: 0.0073066093027591705\n",
            "step: 90, loss: 0.003449326381087303\n",
            "step: 100, loss: 0.0019724618177860975\n",
            "step: 110, loss: 0.02771984227001667\n",
            "step: 120, loss: 0.0011020053643733263\n",
            "step: 130, loss: 0.008377890102565289\n",
            "step: 140, loss: 0.052343934774398804\n",
            "step: 150, loss: 0.006693798117339611\n",
            "step: 160, loss: 0.003414308186620474\n",
            "step: 170, loss: 0.029470132663846016\n",
            "step: 180, loss: 0.014826801605522633\n",
            "step: 190, loss: 0.034943535923957825\n",
            "step: 200, loss: 0.002897003898397088\n",
            "step: 210, loss: 0.0014401667285710573\n",
            "step: 220, loss: 0.04498720541596413\n",
            "step: 230, loss: 0.05995551869273186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9808773903262092, f1=0.9761634506242906, best_f1=0.9761634506242906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016241326928138733\n",
            "step: 10, loss: 0.003695590188726783\n",
            "step: 20, loss: 0.004647485911846161\n",
            "step: 30, loss: 0.010324399918317795\n",
            "step: 40, loss: 0.12134087830781937\n",
            "step: 50, loss: 0.05007064342498779\n",
            "step: 60, loss: 0.0016660374822095037\n",
            "step: 70, loss: 0.007403544150292873\n",
            "step: 80, loss: 0.00044265229371376336\n",
            "step: 90, loss: 0.18829013407230377\n",
            "step: 100, loss: 0.012795482762157917\n",
            "step: 110, loss: 0.0014464114792644978\n",
            "step: 120, loss: 0.04524993151426315\n",
            "step: 130, loss: 0.02004918083548546\n",
            "step: 140, loss: 0.11190776526927948\n",
            "step: 150, loss: 0.00989333726465702\n",
            "step: 160, loss: 0.007005855906754732\n",
            "step: 170, loss: 0.005971415434032679\n",
            "step: 180, loss: 0.006063604727387428\n",
            "step: 190, loss: 0.021255914121866226\n",
            "step: 200, loss: 0.006006855517625809\n",
            "step: 210, loss: 0.0010170917958021164\n",
            "step: 220, loss: 0.0004989688168279827\n",
            "step: 230, loss: 0.00031097972532734275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.983050847457627, f1=0.979591836734694, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038714613765478134\n",
            "step: 10, loss: 0.00041800676262937486\n",
            "step: 20, loss: 0.0011831915471702814\n",
            "step: 30, loss: 0.00018742481188382953\n",
            "step: 40, loss: 0.0011302692582830787\n",
            "step: 50, loss: 0.0012500013690441847\n",
            "step: 60, loss: 0.03746064007282257\n",
            "step: 70, loss: 0.0013669467298313975\n",
            "step: 80, loss: 0.006473298650234938\n",
            "step: 90, loss: 0.001825847546570003\n",
            "step: 100, loss: 0.0010409692768007517\n",
            "step: 110, loss: 0.0002771923318505287\n",
            "step: 120, loss: 0.005929472390562296\n",
            "step: 130, loss: 0.001017891801893711\n",
            "step: 140, loss: 0.0024423429276794195\n",
            "step: 150, loss: 0.19095559418201447\n",
            "step: 160, loss: 0.024743184447288513\n",
            "step: 170, loss: 0.014330173842608929\n",
            "step: 180, loss: 0.0009175604791380465\n",
            "step: 190, loss: 0.0021322693210095167\n",
            "step: 200, loss: 0.0012822240823879838\n",
            "step: 210, loss: 0.01689569093286991\n",
            "step: 220, loss: 0.0004453261790331453\n",
            "step: 230, loss: 0.002721893833950162\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9748858447488584, f1=0.9725400457665903, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007229694747366011\n",
            "step: 10, loss: 0.0005583651945926249\n",
            "step: 20, loss: 0.006039253436028957\n",
            "step: 30, loss: 0.00152622873429209\n",
            "step: 40, loss: 0.001277233357541263\n",
            "step: 50, loss: 0.0003048629150725901\n",
            "step: 60, loss: 0.0016674257349222898\n",
            "step: 70, loss: 0.0007267402834258974\n",
            "step: 80, loss: 0.02390642650425434\n",
            "step: 90, loss: 0.0009264365071430802\n",
            "step: 100, loss: 0.00044907984556630254\n",
            "step: 110, loss: 0.00017362575454171747\n",
            "step: 120, loss: 0.00010432703129481524\n",
            "step: 130, loss: 0.0011732025304809213\n",
            "step: 140, loss: 0.005150360055267811\n",
            "step: 150, loss: 0.011823872104287148\n",
            "step: 160, loss: 0.04138091951608658\n",
            "step: 170, loss: 0.014091567136347294\n",
            "step: 180, loss: 0.1237744614481926\n",
            "step: 190, loss: 0.0018536016577854753\n",
            "step: 200, loss: 0.002846787218004465\n",
            "step: 210, loss: 0.003951105289161205\n",
            "step: 220, loss: 0.01734408363699913\n",
            "step: 230, loss: 0.001153103425167501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9753363228699552, f1=0.972972972972973, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013344367034733295\n",
            "step: 10, loss: 0.0010986811248585582\n",
            "step: 20, loss: 0.0067760637030005455\n",
            "step: 30, loss: 0.0015524406917393208\n",
            "step: 40, loss: 0.030252806842327118\n",
            "step: 50, loss: 0.005761909764260054\n",
            "step: 60, loss: 0.0014886637218296528\n",
            "step: 70, loss: 0.0003865601902361959\n",
            "step: 80, loss: 0.0012946303468197584\n",
            "step: 90, loss: 0.003052087966352701\n",
            "step: 100, loss: 0.1077704131603241\n",
            "step: 110, loss: 0.0013454598374664783\n",
            "step: 120, loss: 0.0031285129953175783\n",
            "step: 130, loss: 0.009500396437942982\n",
            "step: 140, loss: 0.001330572529695928\n",
            "step: 150, loss: 0.0017119050025939941\n",
            "step: 160, loss: 0.002214224310591817\n",
            "step: 170, loss: 0.0007530316361226141\n",
            "step: 180, loss: 0.03469209000468254\n",
            "step: 190, loss: 0.04997289553284645\n",
            "step: 200, loss: 0.0043792566284537315\n",
            "step: 210, loss: 0.006627856288105249\n",
            "step: 220, loss: 0.021487068384885788\n",
            "step: 230, loss: 0.09391648322343826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9709821428571428, f1=0.972972972972973, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007179269101470709\n",
            "step: 10, loss: 0.00022568897111341357\n",
            "step: 20, loss: 0.0014499187236651778\n",
            "step: 30, loss: 0.0004356866411399096\n",
            "step: 40, loss: 0.001181766507215798\n",
            "step: 50, loss: 0.0004927815753035247\n",
            "step: 60, loss: 0.024932876229286194\n",
            "step: 70, loss: 0.01567830517888069\n",
            "step: 80, loss: 0.0012657769257202744\n",
            "step: 90, loss: 0.00016317566041834652\n",
            "step: 100, loss: 0.00025059605832211673\n",
            "step: 110, loss: 0.0006649639108218253\n",
            "step: 120, loss: 0.0002695767907425761\n",
            "step: 130, loss: 0.00026473807520233095\n",
            "step: 140, loss: 0.00044195743976160884\n",
            "step: 150, loss: 0.0003579641634132713\n",
            "step: 160, loss: 0.1112314835190773\n",
            "step: 170, loss: 0.015542935580015182\n",
            "step: 180, loss: 0.0009067466016858816\n",
            "step: 190, loss: 0.00097687728703022\n",
            "step: 200, loss: 0.027908772230148315\n",
            "step: 210, loss: 0.00016536891052965075\n",
            "step: 220, loss: 0.003762422129511833\n",
            "step: 230, loss: 0.0002570563810877502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9744160177975528, f1=0.9655937846836848, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001488400885136798\n",
            "step: 10, loss: 0.00040165966493077576\n",
            "step: 20, loss: 0.00010290925274603069\n",
            "step: 30, loss: 0.00015587943198624998\n",
            "step: 40, loss: 0.0008212769753299654\n",
            "step: 50, loss: 0.00498042069375515\n",
            "step: 60, loss: 0.00032505931449122727\n",
            "step: 70, loss: 0.0007154599297791719\n",
            "step: 80, loss: 0.004198855720460415\n",
            "step: 90, loss: 0.00010747288615675643\n",
            "step: 100, loss: 0.0005486739100888371\n",
            "step: 110, loss: 0.00039421196561306715\n",
            "step: 120, loss: 0.00024886461324058473\n",
            "step: 130, loss: 0.04313312843441963\n",
            "step: 140, loss: 0.00019223896379116923\n",
            "step: 150, loss: 0.0005424689734354615\n",
            "step: 160, loss: 0.00016021894407458603\n",
            "step: 170, loss: 0.00017904816195368767\n",
            "step: 180, loss: 0.00028651917818933725\n",
            "step: 190, loss: 0.0001977930951397866\n",
            "step: 200, loss: 0.0037150857970118523\n",
            "step: 210, loss: 0.00025709872716106474\n",
            "step: 220, loss: 0.001629595528356731\n",
            "step: 230, loss: 0.0001702653826214373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9775784753363228, f1=0.9731543624161074, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.78300886345096e-05\n",
            "step: 10, loss: 0.0003629827988334\n",
            "step: 20, loss: 0.0006425310275517404\n",
            "step: 30, loss: 9.526211943011731e-05\n",
            "step: 40, loss: 0.037043143063783646\n",
            "step: 50, loss: 0.00030971551313996315\n",
            "step: 60, loss: 0.00027338560903444886\n",
            "step: 70, loss: 0.00017517055675853044\n",
            "step: 80, loss: 0.0010211883345618844\n",
            "step: 90, loss: 0.0001617055240785703\n",
            "step: 100, loss: 0.0073409490287303925\n",
            "step: 110, loss: 8.042235276661813e-05\n",
            "step: 120, loss: 6.363471038639545e-05\n",
            "step: 130, loss: 9.473100362811238e-05\n",
            "step: 140, loss: 9.506390051683411e-05\n",
            "step: 150, loss: 0.00044064680696465075\n",
            "step: 160, loss: 7.906159589765593e-05\n",
            "step: 170, loss: 0.00019091971626039594\n",
            "step: 180, loss: 0.0012545585632324219\n",
            "step: 190, loss: 0.00015008858463261276\n",
            "step: 200, loss: 0.00018672032456379384\n",
            "step: 210, loss: 0.0012927788775414228\n",
            "step: 220, loss: 0.000591087038628757\n",
            "step: 230, loss: 0.0017139182891696692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9705215419501134, f1=0.9727272727272728, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005258644232526422\n",
            "step: 10, loss: 5.5477008572779596e-05\n",
            "step: 20, loss: 0.00036613992415368557\n",
            "step: 30, loss: 0.00019674631766974926\n",
            "step: 40, loss: 0.00010898730397457257\n",
            "step: 50, loss: 0.00013211106124799699\n",
            "step: 60, loss: 0.000933364499360323\n",
            "step: 70, loss: 0.0005846096319146454\n",
            "step: 80, loss: 0.00018347262812312692\n",
            "step: 90, loss: 0.00040247265133075416\n",
            "step: 100, loss: 0.00011314541916362941\n",
            "step: 110, loss: 0.00013017418677918613\n",
            "step: 120, loss: 0.0010237437672913074\n",
            "step: 130, loss: 6.929630762897432e-05\n",
            "step: 140, loss: 0.053057827055454254\n",
            "step: 150, loss: 0.004924169275909662\n",
            "step: 160, loss: 4.4042975787306204e-05\n",
            "step: 170, loss: 8.740316843613982e-05\n",
            "step: 180, loss: 0.00025300614652223885\n",
            "step: 190, loss: 0.022930286824703217\n",
            "step: 200, loss: 7.266853208420798e-05\n",
            "step: 210, loss: 7.417523011099547e-05\n",
            "step: 220, loss: 0.0002572553639765829\n",
            "step: 230, loss: 0.00012190519919386134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9766925638179801, f1=0.9787234042553192, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027129726368002594\n",
            "step: 10, loss: 7.247338362503797e-05\n",
            "step: 20, loss: 0.0032187344040721655\n",
            "step: 30, loss: 0.02081144042313099\n",
            "step: 40, loss: 0.0001835258590290323\n",
            "step: 50, loss: 5.1741029892582446e-05\n",
            "step: 60, loss: 0.006765812169760466\n",
            "step: 70, loss: 7.11529646650888e-05\n",
            "step: 80, loss: 6.103933264967054e-05\n",
            "step: 90, loss: 9.855032840278e-05\n",
            "step: 100, loss: 6.309230229817331e-05\n",
            "step: 110, loss: 6.532757834065706e-05\n",
            "step: 120, loss: 8.323500514961779e-05\n",
            "step: 130, loss: 7.443549111485481e-05\n",
            "step: 140, loss: 0.0005015701754018664\n",
            "step: 150, loss: 0.058118946850299835\n",
            "step: 160, loss: 5.690529360435903e-05\n",
            "step: 170, loss: 0.0014442243846133351\n",
            "step: 180, loss: 0.00014189672947395593\n",
            "step: 190, loss: 5.183899702387862e-05\n",
            "step: 200, loss: 6.651139847235754e-05\n",
            "step: 210, loss: 5.377221168600954e-05\n",
            "step: 220, loss: 8.497143426211551e-05\n",
            "step: 230, loss: 9.284912084694952e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9797297297297298, f1=0.9763779527559054, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.028716936474666e-05\n",
            "step: 10, loss: 7.577845826745033e-05\n",
            "step: 20, loss: 0.00014419904618989676\n",
            "step: 30, loss: 0.0038363554049283266\n",
            "step: 40, loss: 7.433644350385293e-05\n",
            "step: 50, loss: 0.006244224030524492\n",
            "step: 60, loss: 0.0010457466123625636\n",
            "step: 70, loss: 5.104025694890879e-05\n",
            "step: 80, loss: 5.831171802128665e-05\n",
            "step: 90, loss: 5.573937596636824e-05\n",
            "step: 100, loss: 0.0004397466836962849\n",
            "step: 110, loss: 8.027235890040174e-05\n",
            "step: 120, loss: 0.00011214467667741701\n",
            "step: 130, loss: 0.0001484107633586973\n",
            "step: 140, loss: 9.388761827722192e-05\n",
            "step: 150, loss: 0.00010268465848639607\n",
            "step: 160, loss: 0.0001870602573035285\n",
            "step: 170, loss: 8.131224603857845e-05\n",
            "step: 180, loss: 9.530560055281967e-05\n",
            "step: 190, loss: 0.0002210782840847969\n",
            "step: 200, loss: 5.495607547345571e-05\n",
            "step: 210, loss: 6.488856161013246e-05\n",
            "step: 220, loss: 0.0001257963594980538\n",
            "step: 230, loss: 0.00020813783339690417\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9831271091113611, f1=0.9786276715410572, best_f1=0.9786276715410572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.314345177495852e-05\n",
            "step: 10, loss: 7.501692743971944e-05\n",
            "step: 20, loss: 0.00024913757806643844\n",
            "step: 30, loss: 0.0002564121095929295\n",
            "step: 40, loss: 0.00010590069723548368\n",
            "step: 50, loss: 0.0005255905562080443\n",
            "step: 60, loss: 0.00011051712499465793\n",
            "step: 70, loss: 4.164519123150967e-05\n",
            "step: 80, loss: 7.719955465290695e-05\n",
            "step: 90, loss: 0.0003762313863262534\n",
            "step: 100, loss: 2.7260606657364406e-05\n",
            "step: 110, loss: 0.011450082994997501\n",
            "step: 120, loss: 0.0007605403661727905\n",
            "step: 130, loss: 9.741722169565037e-05\n",
            "step: 140, loss: 5.929883263888769e-05\n",
            "step: 150, loss: 5.380453512771055e-05\n",
            "step: 160, loss: 0.005363497883081436\n",
            "step: 170, loss: 8.756366150919348e-05\n",
            "step: 180, loss: 6.672771996818483e-05\n",
            "step: 190, loss: 4.916414036415517e-05\n",
            "step: 200, loss: 0.00038068380672484636\n",
            "step: 210, loss: 3.482116881059483e-05\n",
            "step: 220, loss: 0.000483106094179675\n",
            "step: 230, loss: 5.917581802350469e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9830124575311437, f1=0.9751131221719457, best_f1=0.9786276715410572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.6116943596862257e-05\n",
            "step: 10, loss: 0.006677971687167883\n",
            "step: 20, loss: 4.971312591806054e-05\n",
            "step: 30, loss: 8.037830411922187e-05\n",
            "step: 40, loss: 0.0007441797060891986\n",
            "step: 50, loss: 9.350258915219456e-05\n",
            "step: 60, loss: 0.00014386010298039764\n",
            "step: 70, loss: 0.00025639741215854883\n",
            "step: 80, loss: 5.723380672861822e-05\n",
            "step: 90, loss: 8.812348096398637e-05\n",
            "step: 100, loss: 4.8284015065291896e-05\n",
            "step: 110, loss: 0.00016986938135232776\n",
            "step: 120, loss: 2.7357580620446242e-05\n",
            "step: 130, loss: 5.902600969420746e-05\n",
            "step: 140, loss: 7.611369801452383e-05\n",
            "step: 150, loss: 4.938183701597154e-05\n",
            "step: 160, loss: 0.008292661048471928\n",
            "step: 170, loss: 2.3275202693184838e-05\n",
            "step: 180, loss: 6.114703137427568e-05\n",
            "step: 190, loss: 6.4228726841975e-05\n",
            "step: 200, loss: 6.343552377074957e-05\n",
            "step: 210, loss: 0.0002187188365496695\n",
            "step: 220, loss: 7.336428097914904e-05\n",
            "step: 230, loss: 6.618392944801599e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9830890642615557, f1=0.9741282339707535, best_f1=0.9786276715410572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.6960725740063936e-05\n",
            "step: 10, loss: 3.345114237163216e-05\n",
            "step: 20, loss: 7.172395999077708e-05\n",
            "step: 30, loss: 8.460569370072335e-05\n",
            "step: 40, loss: 0.00012680955114774406\n",
            "step: 50, loss: 0.04565681144595146\n",
            "step: 60, loss: 5.245486318017356e-05\n",
            "step: 70, loss: 0.0001290431246161461\n",
            "step: 80, loss: 0.0001006711318041198\n",
            "step: 90, loss: 8.393786265514791e-05\n",
            "step: 100, loss: 0.00012114617129554972\n",
            "step: 110, loss: 6.350325566018e-05\n",
            "step: 120, loss: 7.22353724995628e-05\n",
            "step: 130, loss: 6.498800939880311e-05\n",
            "step: 140, loss: 4.927697227685712e-05\n",
            "step: 150, loss: 0.00018738643848337233\n",
            "step: 160, loss: 4.94109008286614e-05\n",
            "step: 170, loss: 5.0510319852037355e-05\n",
            "step: 180, loss: 0.0059735774993896484\n",
            "step: 190, loss: 6.138319440651685e-05\n",
            "step: 200, loss: 6.554508581757545e-05\n",
            "step: 210, loss: 6.371762719936669e-05\n",
            "step: 220, loss: 5.48948701180052e-05\n",
            "step: 230, loss: 6.633638258790597e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.983050847457627, f1=0.972972972972973, best_f1=0.9786276715410572\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 211.46it/s]\n",
            "load_f1 = 0.9785310734463276\n",
            "real_f1 = 0.9740112994350283\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 269.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ade1f3-2efb-48bb-861d-a8dbfdb08bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 393kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
            "Downloading: 100% 440M/440M [00:09<00:00, 48.6MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.647811233997345\n",
            "step: 10, loss: 0.5494239926338196\n",
            "step: 20, loss: 0.5842128396034241\n",
            "step: 30, loss: 0.36220595240592957\n",
            "step: 40, loss: 0.1723824143409729\n",
            "step: 50, loss: 0.28127768635749817\n",
            "step: 60, loss: 0.09187918901443481\n",
            "step: 70, loss: 0.11512069404125214\n",
            "step: 80, loss: 0.04243184253573418\n",
            "step: 90, loss: 0.5786586999893188\n",
            "step: 100, loss: 0.14049172401428223\n",
            "step: 110, loss: 0.11035636812448502\n",
            "step: 120, loss: 0.1745450645685196\n",
            "step: 130, loss: 0.09030871093273163\n",
            "step: 140, loss: 0.10694514214992523\n",
            "step: 150, loss: 0.11331816017627716\n",
            "step: 160, loss: 0.051772214472293854\n",
            "step: 170, loss: 0.25438928604125977\n",
            "step: 180, loss: 0.1362900286912918\n",
            "step: 190, loss: 0.06452261656522751\n",
            "step: 200, loss: 0.1307314932346344\n",
            "step: 210, loss: 0.09265559166669846\n",
            "step: 220, loss: 0.26909446716308594\n",
            "step: 230, loss: 0.16347548365592957\n",
            "step: 240, loss: 0.08283238112926483\n",
            "step: 250, loss: 0.1285710632801056\n",
            "step: 260, loss: 0.3250022232532501\n",
            "step: 270, loss: 0.031790196895599365\n",
            "step: 280, loss: 0.100617416203022\n",
            "step: 290, loss: 0.10284170508384705\n",
            "step: 300, loss: 0.14861708879470825\n",
            "step: 310, loss: 0.14149491488933563\n",
            "step: 320, loss: 0.08952653408050537\n",
            "step: 330, loss: 0.05216789245605469\n",
            "step: 340, loss: 0.1644037365913391\n",
            "step: 350, loss: 0.08751174807548523\n",
            "step: 360, loss: 0.03602233901619911\n",
            "step: 370, loss: 0.19372649490833282\n",
            "step: 380, loss: 0.02706661820411682\n",
            "step: 390, loss: 0.34010806679725647\n",
            "step: 400, loss: 0.2150147557258606\n",
            "step: 410, loss: 0.1426236480474472\n",
            "step: 420, loss: 0.14524458348751068\n",
            "step: 430, loss: 0.15586505830287933\n",
            "step: 440, loss: 0.03584467992186546\n",
            "step: 450, loss: 0.0261545330286026\n",
            "step: 460, loss: 0.02530554123222828\n",
            "step: 470, loss: 0.09829290956258774\n",
            "step: 480, loss: 0.1003790944814682\n",
            "step: 490, loss: 0.24213235080242157\n",
            "step: 500, loss: 0.10277579724788666\n",
            "step: 510, loss: 0.14047175645828247\n",
            "step: 520, loss: 0.028476187959313393\n",
            "step: 530, loss: 0.015119875781238079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8903406439570696, f1=0.889814814814815, best_f1=0.889814814814815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16397078335285187\n",
            "step: 10, loss: 0.10100184381008148\n",
            "step: 20, loss: 0.010834301821887493\n",
            "step: 30, loss: 0.010852339677512646\n",
            "step: 40, loss: 0.10155065357685089\n",
            "step: 50, loss: 0.1616738736629486\n",
            "step: 60, loss: 0.04857989773154259\n",
            "step: 70, loss: 0.13431353867053986\n",
            "step: 80, loss: 0.20593476295471191\n",
            "step: 90, loss: 0.055593740195035934\n",
            "step: 100, loss: 0.1266070455312729\n",
            "step: 110, loss: 0.04535534977912903\n",
            "step: 120, loss: 0.15490931272506714\n",
            "step: 130, loss: 0.08612284064292908\n",
            "step: 140, loss: 0.05870300531387329\n",
            "step: 150, loss: 0.0986516997218132\n",
            "step: 160, loss: 0.10018251836299896\n",
            "step: 170, loss: 0.06850933283567429\n",
            "step: 180, loss: 0.049243755638599396\n",
            "step: 190, loss: 0.04854971170425415\n",
            "step: 200, loss: 0.04228592664003372\n",
            "step: 210, loss: 0.11171203851699829\n",
            "step: 220, loss: 0.044274769723415375\n",
            "step: 230, loss: 0.018724270164966583\n",
            "step: 240, loss: 0.037176717072725296\n",
            "step: 250, loss: 0.09846056997776031\n",
            "step: 260, loss: 0.03338748589158058\n",
            "step: 270, loss: 0.2726437449455261\n",
            "step: 280, loss: 0.08380959928035736\n",
            "step: 290, loss: 0.0708615854382515\n",
            "step: 300, loss: 0.0588870607316494\n",
            "step: 310, loss: 0.02816847711801529\n",
            "step: 320, loss: 0.24990598857402802\n",
            "step: 330, loss: 0.05557199567556381\n",
            "step: 340, loss: 0.014593265950679779\n",
            "step: 350, loss: 0.01424410566687584\n",
            "step: 360, loss: 0.11928031593561172\n",
            "step: 370, loss: 0.12069493532180786\n",
            "step: 380, loss: 0.2223578691482544\n",
            "step: 390, loss: 0.12777690589427948\n",
            "step: 400, loss: 0.02957611158490181\n",
            "step: 410, loss: 0.024338027462363243\n",
            "step: 420, loss: 0.08180277049541473\n",
            "step: 430, loss: 0.04348449781537056\n",
            "step: 440, loss: 0.052223753184080124\n",
            "step: 450, loss: 0.038351885974407196\n",
            "step: 460, loss: 0.056531406939029694\n",
            "step: 470, loss: 0.017820097506046295\n",
            "step: 480, loss: 0.1492721289396286\n",
            "step: 490, loss: 0.04218794032931328\n",
            "step: 500, loss: 0.37766870856285095\n",
            "step: 510, loss: 0.035944439470767975\n",
            "step: 520, loss: 0.12610788643360138\n",
            "step: 530, loss: 0.05543491244316101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9072261072261073, f1=0.9066293183940244, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07293684035539627\n",
            "step: 10, loss: 0.028563128784298897\n",
            "step: 20, loss: 0.2432786375284195\n",
            "step: 30, loss: 0.03760824352502823\n",
            "step: 40, loss: 0.012613344937562943\n",
            "step: 50, loss: 0.014415017329156399\n",
            "step: 60, loss: 0.005697699263691902\n",
            "step: 70, loss: 0.04613369703292847\n",
            "step: 80, loss: 0.003329269355162978\n",
            "step: 90, loss: 0.017527220770716667\n",
            "step: 100, loss: 0.06298162043094635\n",
            "step: 110, loss: 0.0048758178018033504\n",
            "step: 120, loss: 0.051425911486148834\n",
            "step: 130, loss: 0.005377823952585459\n",
            "step: 140, loss: 0.027138348668813705\n",
            "step: 150, loss: 0.06164112687110901\n",
            "step: 160, loss: 0.15166126191616058\n",
            "step: 170, loss: 0.1408804953098297\n",
            "step: 180, loss: 0.01747901737689972\n",
            "step: 190, loss: 0.059946998953819275\n",
            "step: 200, loss: 0.04112037271261215\n",
            "step: 210, loss: 0.02553158439695835\n",
            "step: 220, loss: 0.17295090854167938\n",
            "step: 230, loss: 0.07649321109056473\n",
            "step: 240, loss: 0.030881347134709358\n",
            "step: 250, loss: 0.13979646563529968\n",
            "step: 260, loss: 0.03484697639942169\n",
            "step: 270, loss: 0.011363185942173004\n",
            "step: 280, loss: 0.15933075547218323\n",
            "step: 290, loss: 0.006579921115189791\n",
            "step: 300, loss: 0.07477002590894699\n",
            "step: 310, loss: 0.08952976018190384\n",
            "step: 320, loss: 0.025979988276958466\n",
            "step: 330, loss: 0.022036032751202583\n",
            "step: 340, loss: 0.024346599355340004\n",
            "step: 350, loss: 0.004460613708943129\n",
            "step: 360, loss: 0.02542649768292904\n",
            "step: 370, loss: 0.004731003660708666\n",
            "step: 380, loss: 0.01043845433741808\n",
            "step: 390, loss: 0.02449987642467022\n",
            "step: 400, loss: 0.028205275535583496\n",
            "step: 410, loss: 0.022565225139260292\n",
            "step: 420, loss: 0.05211041867733002\n",
            "step: 430, loss: 0.05825033038854599\n",
            "step: 440, loss: 0.015297717414796352\n",
            "step: 450, loss: 0.079509437084198\n",
            "step: 460, loss: 0.012896551750600338\n",
            "step: 470, loss: 0.12256109714508057\n",
            "step: 480, loss: 0.11611513048410416\n",
            "step: 490, loss: 0.02863420732319355\n",
            "step: 500, loss: 0.04084272310137749\n",
            "step: 510, loss: 0.04952598363161087\n",
            "step: 520, loss: 0.037877295166254044\n",
            "step: 530, loss: 0.1645965278148651\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9021636876763877, f1=0.9036827195467423, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01569865643978119\n",
            "step: 10, loss: 0.02407826855778694\n",
            "step: 20, loss: 0.0020978860557079315\n",
            "step: 30, loss: 0.018241021782159805\n",
            "step: 40, loss: 0.007308726664632559\n",
            "step: 50, loss: 0.01698964089155197\n",
            "step: 60, loss: 0.02164490334689617\n",
            "step: 70, loss: 0.037648364901542664\n",
            "step: 80, loss: 0.07483972609043121\n",
            "step: 90, loss: 0.17527659237384796\n",
            "step: 100, loss: 0.017547691240906715\n",
            "step: 110, loss: 0.08341483771800995\n",
            "step: 120, loss: 0.002551440615206957\n",
            "step: 130, loss: 0.04945339262485504\n",
            "step: 140, loss: 0.0205307025462389\n",
            "step: 150, loss: 0.029245935380458832\n",
            "step: 160, loss: 0.1780402660369873\n",
            "step: 170, loss: 0.030457429587841034\n",
            "step: 180, loss: 0.012798111885786057\n",
            "step: 190, loss: 0.07360578328371048\n",
            "step: 200, loss: 0.019467752426862717\n",
            "step: 210, loss: 0.05747215077280998\n",
            "step: 220, loss: 0.013759045861661434\n",
            "step: 230, loss: 0.01996162347495556\n",
            "step: 240, loss: 0.030906252562999725\n",
            "step: 250, loss: 0.09365851432085037\n",
            "step: 260, loss: 0.02675936557352543\n",
            "step: 270, loss: 0.009040475822985172\n",
            "step: 280, loss: 0.17107969522476196\n",
            "step: 290, loss: 0.019335594028234482\n",
            "step: 300, loss: 0.004291221033781767\n",
            "step: 310, loss: 0.006876432802528143\n",
            "step: 320, loss: 0.05005520209670067\n",
            "step: 330, loss: 0.1308414787054062\n",
            "step: 340, loss: 0.013417341746389866\n",
            "step: 350, loss: 0.005071138497442007\n",
            "step: 360, loss: 0.08563249558210373\n",
            "step: 370, loss: 0.0031824209727346897\n",
            "step: 380, loss: 0.005355031695216894\n",
            "step: 390, loss: 0.0004121899255551398\n",
            "step: 400, loss: 0.005068385042250156\n",
            "step: 410, loss: 0.04307830333709717\n",
            "step: 420, loss: 0.02023025043308735\n",
            "step: 430, loss: 0.1264687329530716\n",
            "step: 440, loss: 0.011859320104122162\n",
            "step: 450, loss: 0.006153311114758253\n",
            "step: 460, loss: 0.11877915263175964\n",
            "step: 470, loss: 0.005614117253571749\n",
            "step: 480, loss: 0.014267893508076668\n",
            "step: 490, loss: 0.00385110080242157\n",
            "step: 500, loss: 0.004019572399556637\n",
            "step: 510, loss: 0.006806913297623396\n",
            "step: 520, loss: 0.12990786135196686\n",
            "step: 530, loss: 0.05764206498861313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.8993916705662143, f1=0.8992974238875878, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011798330582678318\n",
            "step: 10, loss: 0.07073735445737839\n",
            "step: 20, loss: 0.007519317790865898\n",
            "step: 30, loss: 0.0010110235307365656\n",
            "step: 40, loss: 0.01348496600985527\n",
            "step: 50, loss: 0.006399962119758129\n",
            "step: 60, loss: 0.021868186071515083\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 0.0022733728401362896\n",
            "step: 80, loss: 0.00034528967808000743\n",
            "step: 90, loss: 0.001747853122651577\n",
            "step: 100, loss: 0.008312135003507137\n",
            "step: 110, loss: 0.0006643941160291433\n",
            "step: 120, loss: 0.0030046002939343452\n",
            "step: 130, loss: 0.0002484039287082851\n",
            "step: 140, loss: 0.05401915684342384\n",
            "step: 150, loss: 0.0063459607772529125\n",
            "step: 160, loss: 0.006590717006474733\n",
            "step: 170, loss: 0.023315558210015297\n",
            "step: 180, loss: 0.005407615564763546\n",
            "step: 190, loss: 0.002349128946661949\n",
            "step: 200, loss: 0.0034223925322294235\n",
            "step: 210, loss: 0.03231794014573097\n",
            "step: 220, loss: 0.012757787480950356\n",
            "step: 230, loss: 0.09759166836738586\n",
            "step: 240, loss: 0.0031984946690499783\n",
            "step: 250, loss: 0.00709828594699502\n",
            "step: 260, loss: 0.03274538740515709\n",
            "step: 270, loss: 0.008488517254590988\n",
            "step: 280, loss: 0.015876678749918938\n",
            "step: 290, loss: 0.11906970292329788\n",
            "step: 300, loss: 0.015320667065680027\n",
            "step: 310, loss: 0.019250456243753433\n",
            "step: 320, loss: 0.04634205996990204\n",
            "step: 330, loss: 0.0045089442282915115\n",
            "step: 340, loss: 0.005897450726479292\n",
            "step: 350, loss: 0.00059078837512061\n",
            "step: 360, loss: 0.04916940629482269\n",
            "step: 370, loss: 0.04331698268651962\n",
            "step: 380, loss: 0.0011502584675326943\n",
            "step: 390, loss: 0.007951371371746063\n",
            "step: 400, loss: 0.015595278702676296\n",
            "step: 410, loss: 0.054428793489933014\n",
            "step: 420, loss: 0.003203712170943618\n",
            "step: 430, loss: 0.005780986975878477\n",
            "step: 440, loss: 0.15745189785957336\n",
            "step: 450, loss: 0.08995192497968674\n",
            "step: 460, loss: 0.0013687562895938754\n",
            "step: 470, loss: 0.027100345119833946\n",
            "step: 480, loss: 0.0016496196622028947\n",
            "step: 490, loss: 0.002561695873737335\n",
            "step: 500, loss: 0.012499107979238033\n",
            "step: 510, loss: 0.0161886066198349\n",
            "step: 520, loss: 0.0450095497071743\n",
            "step: 530, loss: 0.0016278453404083848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.898876404494382, f1=0.896195396899953, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001959907589480281\n",
            "step: 10, loss: 0.007643344812095165\n",
            "step: 20, loss: 0.07074648141860962\n",
            "step: 30, loss: 0.001595405163243413\n",
            "step: 40, loss: 0.007966279983520508\n",
            "step: 50, loss: 0.001784533727914095\n",
            "step: 60, loss: 0.0005145350005477667\n",
            "step: 70, loss: 0.00038838316686451435\n",
            "step: 80, loss: 0.0022881021723151207\n",
            "step: 90, loss: 0.00016264714940916747\n",
            "step: 100, loss: 0.0011520495172590017\n",
            "step: 110, loss: 0.0030010573100298643\n",
            "step: 120, loss: 0.0018916113767772913\n",
            "step: 130, loss: 0.00023864551621954888\n",
            "step: 140, loss: 0.0029470480512827635\n",
            "step: 150, loss: 0.002463166369125247\n",
            "step: 160, loss: 0.00032915148767642677\n",
            "step: 170, loss: 0.00032755767460912466\n",
            "step: 180, loss: 0.0002130036591552198\n",
            "step: 190, loss: 0.001425933325663209\n",
            "step: 200, loss: 0.0009207528783008456\n",
            "step: 210, loss: 0.012919827364385128\n",
            "step: 220, loss: 0.04706556722521782\n",
            "step: 230, loss: 0.013461830094456673\n",
            "step: 240, loss: 0.0007222371059469879\n",
            "step: 250, loss: 0.004101188387721777\n",
            "step: 260, loss: 0.001528649590909481\n",
            "step: 270, loss: 0.03106693923473358\n",
            "step: 280, loss: 0.005521080456674099\n",
            "step: 290, loss: 0.000517316279001534\n",
            "step: 300, loss: 0.022365592420101166\n",
            "step: 310, loss: 0.002969134598970413\n",
            "step: 320, loss: 0.0012278460199013352\n",
            "step: 330, loss: 0.0010945081012323499\n",
            "step: 340, loss: 0.047897640615701675\n",
            "step: 350, loss: 0.026404578238725662\n",
            "step: 360, loss: 0.003624339820817113\n",
            "step: 370, loss: 0.0024126458447426558\n",
            "step: 380, loss: 0.0005728491814807057\n",
            "step: 390, loss: 0.017456619068980217\n",
            "step: 400, loss: 0.012547628954052925\n",
            "step: 410, loss: 0.00431144330650568\n",
            "step: 420, loss: 0.0025217782240360975\n",
            "step: 430, loss: 0.000702431658282876\n",
            "step: 440, loss: 0.00018297068891115487\n",
            "step: 450, loss: 0.000679230666719377\n",
            "step: 460, loss: 0.0017552358331158757\n",
            "step: 470, loss: 0.0033835703507065773\n",
            "step: 480, loss: 0.0009452891536056995\n",
            "step: 490, loss: 0.02232344262301922\n",
            "step: 500, loss: 0.012191607616841793\n",
            "step: 510, loss: 0.0022906665690243244\n",
            "step: 520, loss: 0.003912837710231543\n",
            "step: 530, loss: 0.009601489640772343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8541882109617372, f1=0.8520770010131712, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09145091474056244\n",
            "step: 10, loss: 0.08475705981254578\n",
            "step: 20, loss: 0.004835125058889389\n",
            "step: 30, loss: 0.0020312578417360783\n",
            "step: 40, loss: 0.05934610962867737\n",
            "step: 50, loss: 0.06025087088346481\n",
            "step: 60, loss: 0.0013893176801502705\n",
            "step: 70, loss: 0.005867209285497665\n",
            "step: 80, loss: 0.059454295784235\n",
            "step: 90, loss: 0.05280892923474312\n",
            "step: 100, loss: 0.01409897394478321\n",
            "step: 110, loss: 0.0011890458408743143\n",
            "step: 120, loss: 0.0021469821222126484\n",
            "step: 130, loss: 0.0042966436594724655\n",
            "step: 140, loss: 0.00562291219830513\n",
            "step: 150, loss: 0.003542455844581127\n",
            "step: 160, loss: 0.0005789091810584068\n",
            "step: 170, loss: 0.0010493779554963112\n",
            "step: 180, loss: 0.0008266806835308671\n",
            "step: 190, loss: 0.0010997112840414047\n",
            "step: 200, loss: 0.004117715638130903\n",
            "step: 210, loss: 0.005196452606469393\n",
            "step: 220, loss: 0.0020161669235676527\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 230, loss: 0.0032784908544272184\n",
            "step: 240, loss: 0.024753080680966377\n",
            "step: 250, loss: 0.0006220964132808149\n",
            "step: 260, loss: 0.0013264778535813093\n",
            "step: 270, loss: 0.001580013777129352\n",
            "step: 280, loss: 0.0014601188013330102\n",
            "step: 290, loss: 0.00017031475726980716\n",
            "step: 300, loss: 0.032140281051397324\n",
            "step: 310, loss: 0.005021471995860338\n",
            "step: 320, loss: 0.05685483291745186\n",
            "step: 330, loss: 0.0013547746930271387\n",
            "step: 340, loss: 0.009721129201352596\n",
            "step: 350, loss: 0.007458092179149389\n",
            "step: 360, loss: 0.0020222170278429985\n",
            "step: 370, loss: 0.00040162098594009876\n",
            "step: 380, loss: 0.0005313162109814584\n",
            "step: 390, loss: 0.0003322933043818921\n",
            "step: 400, loss: 0.0006048395880497992\n",
            "step: 410, loss: 0.006998786702752113\n",
            "step: 420, loss: 0.0013360281009227037\n",
            "step: 430, loss: 0.00010918063344433904\n",
            "step: 440, loss: 0.00037576662725768983\n",
            "step: 450, loss: 0.00651740375906229\n",
            "step: 460, loss: 0.00029964192071929574\n",
            "step: 470, loss: 0.0010573933832347393\n",
            "step: 480, loss: 0.23866714537143707\n",
            "step: 490, loss: 0.006987466476857662\n",
            "step: 500, loss: 0.00026994399377144873\n",
            "step: 510, loss: 0.002025479916483164\n",
            "step: 520, loss: 0.0005013575428165495\n",
            "step: 530, loss: 0.0016456806333735585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8989186647860837, f1=0.894095595126523, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003175064630340785\n",
            "step: 10, loss: 0.07656294852495193\n",
            "step: 20, loss: 0.0018700479995459318\n",
            "step: 30, loss: 0.0002659682941157371\n",
            "step: 40, loss: 0.0005497136735357344\n",
            "step: 50, loss: 0.059704627841711044\n",
            "step: 60, loss: 0.0012866089818999171\n",
            "step: 70, loss: 0.0003877556009683758\n",
            "step: 80, loss: 0.0009031263180077076\n",
            "step: 90, loss: 0.0006365045555867255\n",
            "step: 100, loss: 0.004810127429664135\n",
            "step: 110, loss: 0.01987762749195099\n",
            "step: 120, loss: 0.004281779285520315\n",
            "step: 130, loss: 0.007865027524530888\n",
            "step: 140, loss: 0.003303566947579384\n",
            "step: 150, loss: 0.07572116702795029\n",
            "step: 160, loss: 0.15529878437519073\n",
            "step: 170, loss: 0.0037799112033098936\n",
            "step: 180, loss: 0.00039712764555588365\n",
            "step: 190, loss: 0.01301513146609068\n",
            "step: 200, loss: 0.007266638800501823\n",
            "step: 210, loss: 0.013688507489860058\n",
            "step: 220, loss: 0.0033951064106076956\n",
            "step: 230, loss: 0.0025226627476513386\n",
            "step: 240, loss: 0.000872583594173193\n",
            "step: 250, loss: 0.0005851843161508441\n",
            "step: 260, loss: 9.503518958808854e-05\n",
            "step: 270, loss: 0.002507178345695138\n",
            "step: 280, loss: 0.02312338724732399\n",
            "step: 290, loss: 0.001435974147170782\n",
            "step: 300, loss: 0.0021455527748912573\n",
            "step: 310, loss: 0.0070272828452289104\n",
            "step: 320, loss: 0.009729431010782719\n",
            "step: 330, loss: 0.0004598561208695173\n",
            "step: 340, loss: 0.00011634513066383079\n",
            "step: 350, loss: 0.15739281475543976\n",
            "step: 360, loss: 0.001043965108692646\n",
            "step: 370, loss: 0.00019938507466576993\n",
            "step: 380, loss: 0.001097705913707614\n",
            "step: 390, loss: 0.05767018720507622\n",
            "step: 400, loss: 0.08273495733737946\n",
            "step: 410, loss: 0.0004439139738678932\n",
            "step: 420, loss: 0.0010127901332452893\n",
            "step: 430, loss: 0.045647647231817245\n",
            "step: 440, loss: 0.008268558420240879\n",
            "step: 450, loss: 9.90795815596357e-05\n",
            "step: 460, loss: 0.00043708982411772013\n",
            "step: 470, loss: 0.0013584191910922527\n",
            "step: 480, loss: 0.0007485741516575217\n",
            "step: 490, loss: 0.0530405268073082\n",
            "step: 500, loss: 0.0005017299554310739\n",
            "step: 510, loss: 0.014974415302276611\n",
            "step: 520, loss: 0.0006510508246719837\n",
            "step: 530, loss: 0.0030609623063355684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9012915129151292, f1=0.9052924791086351, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00501625519245863\n",
            "step: 10, loss: 0.00036775748594664037\n",
            "step: 20, loss: 0.039056286215782166\n",
            "step: 30, loss: 0.00038978736847639084\n",
            "step: 40, loss: 8.226965292124078e-05\n",
            "step: 50, loss: 0.004490026738494635\n",
            "step: 60, loss: 6.833951192675158e-05\n",
            "step: 70, loss: 0.0029701213352382183\n",
            "step: 80, loss: 0.028736256062984467\n",
            "step: 90, loss: 0.001168648130260408\n",
            "step: 100, loss: 0.0002962926519103348\n",
            "step: 110, loss: 0.00022495249868370593\n",
            "step: 120, loss: 0.00023061968386173248\n",
            "step: 130, loss: 0.0003655939071904868\n",
            "step: 140, loss: 0.00039549346547573805\n",
            "step: 150, loss: 0.0038824095390737057\n",
            "step: 160, loss: 0.0004581470275297761\n",
            "step: 170, loss: 0.08100370317697525\n",
            "step: 180, loss: 0.006456985138356686\n",
            "step: 190, loss: 8.606123446952552e-05\n",
            "step: 200, loss: 0.0005797009216621518\n",
            "step: 210, loss: 0.0003431506047490984\n",
            "step: 220, loss: 0.0019869026727974415\n",
            "step: 230, loss: 0.00021235542953945696\n",
            "step: 240, loss: 0.0022145239636301994\n",
            "step: 250, loss: 7.638331589987502e-05\n",
            "step: 260, loss: 0.000409450352890417\n",
            "step: 270, loss: 4.488354898057878e-05\n",
            "step: 280, loss: 8.870924648363143e-05\n",
            "step: 290, loss: 0.061984747648239136\n",
            "step: 300, loss: 0.00012371756020002067\n",
            "step: 310, loss: 0.007231553550809622\n",
            "step: 320, loss: 5.434102058643475e-05\n",
            "step: 330, loss: 0.025845970958471298\n",
            "step: 340, loss: 0.004931510426104069\n",
            "step: 350, loss: 0.00032660941360518336\n",
            "step: 360, loss: 0.002523049945011735\n",
            "step: 370, loss: 0.0008061649859882891\n",
            "step: 380, loss: 0.001140698790550232\n",
            "step: 390, loss: 8.86790658114478e-05\n",
            "step: 400, loss: 0.023591380566358566\n",
            "step: 410, loss: 0.0006939383456483483\n",
            "step: 420, loss: 0.0003659942594822496\n",
            "step: 430, loss: 0.0005784990498796105\n",
            "step: 440, loss: 0.00023689029330853373\n",
            "step: 450, loss: 9.755208884598687e-05\n",
            "step: 460, loss: 0.0009722936665639281\n",
            "step: 470, loss: 3.1578372727381065e-05\n",
            "step: 480, loss: 6.819577538408339e-05\n",
            "step: 490, loss: 0.012271403335034847\n",
            "step: 500, loss: 0.02020960859954357\n",
            "step: 510, loss: 0.00012393403449095786\n",
            "step: 520, loss: 0.00024652061983942986\n",
            "step: 530, loss: 0.0005308522959239781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8915662650602411, f1=0.8802714493456132, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015636080934200436\n",
            "step: 10, loss: 7.949389691930264e-05\n",
            "step: 20, loss: 0.0009490328375250101\n",
            "step: 30, loss: 0.017878249287605286\n",
            "step: 40, loss: 0.004088717978447676\n",
            "step: 50, loss: 0.000215407126233913\n",
            "step: 60, loss: 0.000986688886769116\n",
            "step: 70, loss: 8.00763227744028e-05\n",
            "step: 80, loss: 4.712292866315693e-05\n",
            "step: 90, loss: 0.0016217863885685802\n",
            "step: 100, loss: 0.0001920322683872655\n",
            "step: 110, loss: 0.00010787496285047382\n",
            "step: 120, loss: 0.00010749555804068223\n",
            "step: 130, loss: 0.00017089811444748193\n",
            "step: 140, loss: 0.006379060447216034\n",
            "step: 150, loss: 7.895252201706171e-05\n",
            "step: 160, loss: 0.00016345951007679105\n",
            "step: 170, loss: 0.00033791293390095234\n",
            "step: 180, loss: 0.0001945232506841421\n",
            "step: 190, loss: 0.00010388327791588381\n",
            "step: 200, loss: 0.0003817190299741924\n",
            "step: 210, loss: 7.514363824157044e-05\n",
            "step: 220, loss: 0.030531981959939003\n",
            "step: 230, loss: 0.0005212530377320945\n",
            "step: 240, loss: 5.4923759307712317e-05\n",
            "step: 250, loss: 0.00012249792052898556\n",
            "step: 260, loss: 0.0010863076895475388\n",
            "step: 270, loss: 6.416703399736434e-05\n",
            "step: 280, loss: 0.00011160477879457176\n",
            "step: 290, loss: 0.0018878518603742123\n",
            "step: 300, loss: 4.669942063628696e-05\n",
            "step: 310, loss: 0.0015663005178794265\n",
            "step: 320, loss: 0.006416867021471262\n",
            "step: 330, loss: 0.00029212544905021787\n",
            "step: 340, loss: 0.04264157637953758\n",
            "step: 350, loss: 0.009246254339814186\n",
            "step: 360, loss: 5.931714258622378e-05\n",
            "step: 370, loss: 0.0014182627201080322\n",
            "step: 380, loss: 0.002640998689457774\n",
            "step: 390, loss: 7.824190106475726e-05\n",
            "step: 400, loss: 0.00014200800796970725\n",
            "step: 410, loss: 0.0003399798006284982\n",
            "step: 420, loss: 0.001327341073192656\n",
            "step: 430, loss: 5.199392398935743e-05\n",
            "step: 440, loss: 0.01709640398621559\n",
            "step: 450, loss: 0.0022299133706837893\n",
            "step: 460, loss: 8.691888797329739e-05\n",
            "step: 470, loss: 0.003747442504391074\n",
            "step: 480, loss: 0.0015775990905240178\n",
            "step: 490, loss: 0.010049731470644474\n",
            "step: 500, loss: 0.04678747430443764\n",
            "step: 510, loss: 4.715202157967724e-05\n",
            "step: 520, loss: 0.004061440005898476\n",
            "step: 530, loss: 9.490254160482436e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9005186232909006, f1=0.8988235294117647, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01327164564281702\n",
            "step: 10, loss: 0.030335238203406334\n",
            "step: 20, loss: 0.0007852581329643726\n",
            "step: 30, loss: 0.0016033223364502192\n",
            "step: 40, loss: 0.000430736574344337\n",
            "step: 50, loss: 0.00016986274567898363\n",
            "step: 60, loss: 0.0025930367410182953\n",
            "step: 70, loss: 0.0015325904823839664\n",
            "step: 80, loss: 0.00011878336226800457\n",
            "step: 90, loss: 0.00038309700903482735\n",
            "step: 100, loss: 0.002825892996042967\n",
            "step: 110, loss: 0.000887469737790525\n",
            "step: 120, loss: 0.005827693734318018\n",
            "step: 130, loss: 4.938096026307903e-05\n",
            "step: 140, loss: 0.0014163721352815628\n",
            "step: 150, loss: 3.573416324798018e-05\n",
            "step: 160, loss: 4.329586954554543e-05\n",
            "step: 170, loss: 4.523218376561999e-05\n",
            "step: 180, loss: 5.6970009609358385e-05\n",
            "step: 190, loss: 0.0019171107560396194\n",
            "step: 200, loss: 0.006851543672382832\n",
            "step: 210, loss: 0.0007474057492800057\n",
            "step: 220, loss: 3.219233258278109e-05\n",
            "step: 230, loss: 1.6510261048097163e-05\n",
            "step: 240, loss: 0.0004046458634547889\n",
            "step: 250, loss: 1.9237102605984546e-05\n",
            "step: 260, loss: 2.712288187467493e-05\n",
            "step: 270, loss: 6.470482185250148e-05\n",
            "step: 280, loss: 0.00024020556884352118\n",
            "step: 290, loss: 0.0017045480199158192\n",
            "step: 300, loss: 0.00043674150947481394\n",
            "step: 310, loss: 0.014366794377565384\n",
            "step: 320, loss: 4.613984128809534e-05\n",
            "step: 330, loss: 0.0007627158774994314\n",
            "step: 340, loss: 0.00024522122112102807\n",
            "step: 350, loss: 0.0007912563160061836\n",
            "step: 360, loss: 0.0001851747219916433\n",
            "step: 370, loss: 8.397374767810106e-05\n",
            "step: 380, loss: 9.053134999703616e-05\n",
            "step: 390, loss: 4.76355453429278e-05\n",
            "step: 400, loss: 2.8948164981557056e-05\n",
            "step: 410, loss: 8.90292867552489e-05\n",
            "step: 420, loss: 9.662473166827112e-05\n",
            "step: 430, loss: 3.338263195473701e-05\n",
            "step: 440, loss: 4.789338345290162e-05\n",
            "step: 450, loss: 0.0005343133234418929\n",
            "step: 460, loss: 0.031209733337163925\n",
            "step: 470, loss: 0.0026642591692507267\n",
            "step: 480, loss: 0.00028405693592503667\n",
            "step: 490, loss: 0.012600107118487358\n",
            "step: 500, loss: 0.004703808110207319\n",
            "step: 510, loss: 0.00011982820433331653\n",
            "step: 520, loss: 3.6715133319376037e-05\n",
            "step: 530, loss: 0.0025844057090580463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8953817153628651, f1=0.890151515151515, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005056214286014438\n",
            "step: 10, loss: 4.287867704988457e-05\n",
            "step: 20, loss: 0.00038193902582861483\n",
            "step: 30, loss: 9.381523705087602e-05\n",
            "step: 40, loss: 4.236413587932475e-05\n",
            "step: 50, loss: 0.3565235435962677\n",
            "step: 60, loss: 0.007412869017571211\n",
            "step: 70, loss: 0.0012337646912783384\n",
            "step: 80, loss: 9.463007882004604e-05\n",
            "step: 90, loss: 0.0012592726852744818\n",
            "step: 100, loss: 0.002615768928080797\n",
            "step: 110, loss: 0.0042558168061077595\n",
            "step: 120, loss: 4.101686499780044e-05\n",
            "step: 130, loss: 0.00024924284662120044\n",
            "step: 140, loss: 0.00044293838436715305\n",
            "step: 150, loss: 0.00024821035913191736\n",
            "step: 160, loss: 3.822695725830272e-05\n",
            "step: 170, loss: 5.641002644551918e-05\n",
            "step: 180, loss: 6.192300497787073e-05\n",
            "step: 190, loss: 8.888058073353022e-05\n",
            "step: 200, loss: 0.00021884430316276848\n",
            "step: 210, loss: 9.066741040442139e-05\n",
            "step: 220, loss: 0.030441412702202797\n",
            "step: 230, loss: 0.0010508791310712695\n",
            "step: 240, loss: 0.0001673347142059356\n",
            "step: 250, loss: 8.694224379723892e-05\n",
            "step: 260, loss: 0.00016205149586312473\n",
            "step: 270, loss: 0.0005325032980181277\n",
            "step: 280, loss: 0.00012702587991952896\n",
            "step: 290, loss: 0.0006102841580286622\n",
            "step: 300, loss: 0.00010946739348582923\n",
            "step: 310, loss: 0.0010556766064837575\n",
            "step: 320, loss: 5.34609098394867e-05\n",
            "step: 330, loss: 0.0001773963449522853\n",
            "step: 340, loss: 9.471151133766398e-05\n",
            "step: 350, loss: 0.003797483630478382\n",
            "step: 360, loss: 0.0030575653072446585\n",
            "step: 370, loss: 0.012453199364244938\n",
            "step: 380, loss: 9.818193211685866e-05\n",
            "step: 390, loss: 0.00013218987442087382\n",
            "step: 400, loss: 0.011393913067877293\n",
            "step: 410, loss: 0.000166424666531384\n",
            "step: 420, loss: 0.0011122371070086956\n",
            "step: 430, loss: 0.01801755651831627\n",
            "step: 440, loss: 0.008350236341357231\n",
            "step: 450, loss: 5.235791832092218e-05\n",
            "step: 460, loss: 0.00019813820836134255\n",
            "step: 470, loss: 6.778456008760259e-05\n",
            "step: 480, loss: 0.00019752349180635065\n",
            "step: 490, loss: 0.000591915100812912\n",
            "step: 500, loss: 4.00980279664509e-05\n",
            "step: 510, loss: 0.00030786808929406106\n",
            "step: 520, loss: 3.480043960735202e-05\n",
            "step: 530, loss: 4.2797426431206986e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8964539007092198, f1=0.8903846153846154, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002432781271636486\n",
            "step: 10, loss: 3.447984272497706e-05\n",
            "step: 20, loss: 0.0002870759926736355\n",
            "step: 30, loss: 4.1455561586190015e-05\n",
            "step: 40, loss: 0.00012193768634460866\n",
            "step: 50, loss: 0.0014636816922575235\n",
            "step: 60, loss: 0.0002434065390843898\n",
            "step: 70, loss: 0.00011582157458178699\n",
            "step: 80, loss: 0.010705910623073578\n",
            "step: 90, loss: 0.0008913776255212724\n",
            "step: 100, loss: 0.0007116153719834983\n",
            "step: 110, loss: 2.0481349565670826e-05\n",
            "step: 120, loss: 3.6275319871492684e-05\n",
            "step: 130, loss: 3.253156683058478e-05\n",
            "step: 140, loss: 7.201225525932387e-05\n",
            "step: 150, loss: 0.0004496315377764404\n",
            "step: 160, loss: 0.00018179393373429775\n",
            "step: 170, loss: 4.1224069718737155e-05\n",
            "step: 180, loss: 0.00025910887052305043\n",
            "step: 190, loss: 0.0001338042493443936\n",
            "step: 200, loss: 0.00023882219102233648\n",
            "step: 210, loss: 2.4810027753119357e-05\n",
            "step: 220, loss: 7.270154310390353e-05\n",
            "step: 230, loss: 3.137303428957239e-05\n",
            "step: 240, loss: 0.00045767752453684807\n",
            "step: 250, loss: 1.7590640709386207e-05\n",
            "step: 260, loss: 5.6267152103828266e-05\n",
            "step: 270, loss: 0.00013277748075779527\n",
            "step: 280, loss: 3.143260255455971e-05\n",
            "step: 290, loss: 0.0008612123201601207\n",
            "step: 300, loss: 0.00013304836465977132\n",
            "step: 310, loss: 2.4325630874955095e-05\n",
            "step: 320, loss: 0.005312405992299318\n",
            "step: 330, loss: 3.352021667524241e-05\n",
            "step: 340, loss: 0.00017954486247617751\n",
            "step: 350, loss: 0.0007530570146627724\n",
            "step: 360, loss: 1.7817848856793717e-05\n",
            "step: 370, loss: 2.904489156208001e-05\n",
            "step: 380, loss: 2.9417495170491748e-05\n",
            "step: 390, loss: 0.0004358927544672042\n",
            "step: 400, loss: 6.358593964250758e-05\n",
            "step: 410, loss: 9.33703122427687e-05\n",
            "step: 420, loss: 2.023165870923549e-05\n",
            "step: 430, loss: 0.001196324359625578\n",
            "step: 440, loss: 0.00015252636512741446\n",
            "step: 450, loss: 3.2380899938289076e-05\n",
            "step: 460, loss: 7.21164105925709e-05\n",
            "step: 470, loss: 5.290356057230383e-05\n",
            "step: 480, loss: 0.014952571131289005\n",
            "step: 490, loss: 3.580906195566058e-05\n",
            "step: 500, loss: 0.00016491211135871708\n",
            "step: 510, loss: 0.006014125421643257\n",
            "step: 520, loss: 0.0025488724932074547\n",
            "step: 530, loss: 0.00012445647735148668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9052924791086351, f1=0.8992537313432837, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003875821130350232\n",
            "step: 10, loss: 1.8838287360267714e-05\n",
            "step: 20, loss: 2.0198169295326807e-05\n",
            "step: 30, loss: 8.832291496219113e-05\n",
            "step: 40, loss: 0.0007861738558858633\n",
            "step: 50, loss: 0.0007583448314107955\n",
            "step: 60, loss: 7.046671089483425e-05\n",
            "step: 70, loss: 2.4109009245876223e-05\n",
            "step: 80, loss: 0.000244210590608418\n",
            "step: 90, loss: 2.7554682674235664e-05\n",
            "step: 100, loss: 4.408716267789714e-05\n",
            "step: 110, loss: 3.048144390049856e-05\n",
            "step: 120, loss: 3.0284914828371257e-05\n",
            "step: 130, loss: 0.0006550800753757358\n",
            "step: 140, loss: 4.152144902036525e-05\n",
            "step: 150, loss: 1.4327320059237536e-05\n",
            "step: 160, loss: 1.792212606233079e-05\n",
            "step: 170, loss: 2.39602813962847e-05\n",
            "step: 180, loss: 5.9642046835506335e-05\n",
            "step: 190, loss: 0.00014303527132142335\n",
            "step: 200, loss: 3.7958780012559146e-05\n",
            "step: 210, loss: 2.8984628443140537e-05\n",
            "step: 220, loss: 2.0585253878380172e-05\n",
            "step: 230, loss: 3.150283373543061e-05\n",
            "step: 240, loss: 1.5519399312324822e-05\n",
            "step: 250, loss: 5.618557770503685e-05\n",
            "step: 260, loss: 2.603129905764945e-05\n",
            "step: 270, loss: 2.263429996673949e-05\n",
            "step: 280, loss: 2.010503703786526e-05\n",
            "step: 290, loss: 0.00472570676356554\n",
            "step: 300, loss: 5.717786916648038e-05\n",
            "step: 310, loss: 0.00014723441563546658\n",
            "step: 320, loss: 3.374897642061114e-05\n",
            "step: 330, loss: 0.0026414317544549704\n",
            "step: 340, loss: 2.5729535991558805e-05\n",
            "step: 350, loss: 0.00012428409536369145\n",
            "step: 360, loss: 0.015233295038342476\n",
            "step: 370, loss: 8.200009324355051e-05\n",
            "step: 380, loss: 2.3696067728451453e-05\n",
            "step: 390, loss: 0.01980794221162796\n",
            "step: 400, loss: 2.312234755663667e-05\n",
            "step: 410, loss: 1.814558527257759e-05\n",
            "step: 420, loss: 0.000753088213969022\n",
            "step: 430, loss: 0.00031011548708193004\n",
            "step: 440, loss: 0.0003940566093660891\n",
            "step: 450, loss: 5.906872684136033e-05\n",
            "step: 460, loss: 4.560175511869602e-05\n",
            "step: 470, loss: 0.00014114684017840773\n",
            "step: 480, loss: 2.6507465008762665e-05\n",
            "step: 490, loss: 2.4388447855017148e-05\n",
            "step: 500, loss: 2.0078994566574693e-05\n",
            "step: 510, loss: 0.00022869862732477486\n",
            "step: 520, loss: 1.9676421288750134e-05\n",
            "step: 530, loss: 3.945023854612373e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.904363974001857, f1=0.8987400839944004, best_f1=0.9066293183940244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.177022906835191e-05\n",
            "step: 10, loss: 2.593050703580957e-05\n",
            "step: 20, loss: 2.659763595147524e-05\n",
            "step: 30, loss: 0.0009011070942506194\n",
            "step: 40, loss: 0.014456008560955524\n",
            "step: 50, loss: 2.7219904950470664e-05\n",
            "step: 60, loss: 3.4080410841852427e-05\n",
            "step: 70, loss: 7.014318543951958e-05\n",
            "step: 80, loss: 3.5510554880602285e-05\n",
            "step: 90, loss: 6.584092625416815e-05\n",
            "step: 100, loss: 6.442212907131761e-05\n",
            "step: 110, loss: 0.0008479120442643762\n",
            "step: 120, loss: 0.00033927123877219856\n",
            "step: 130, loss: 0.024679917842149734\n",
            "step: 140, loss: 2.682857302716002e-05\n",
            "step: 150, loss: 3.575652954168618e-05\n",
            "step: 160, loss: 3.217667472199537e-05\n",
            "step: 170, loss: 4.902291402686387e-05\n",
            "step: 180, loss: 1.6592146494076587e-05\n",
            "step: 190, loss: 0.0005764120724052191\n",
            "step: 200, loss: 2.634759403008502e-05\n",
            "step: 210, loss: 0.0005347754340618849\n",
            "step: 220, loss: 1.8778859157464467e-05\n",
            "step: 230, loss: 2.4578597731306218e-05\n",
            "step: 240, loss: 0.002159703290089965\n",
            "step: 250, loss: 6.8148277932778e-05\n",
            "step: 260, loss: 1.8406333765597083e-05\n",
            "step: 270, loss: 2.9726155844400637e-05\n",
            "step: 280, loss: 4.023977089673281e-05\n",
            "step: 290, loss: 2.5486209779046476e-05\n",
            "step: 300, loss: 2.01794937311206e-05\n",
            "step: 310, loss: 6.372393545461819e-05\n",
            "step: 320, loss: 0.00035158140235580504\n",
            "step: 330, loss: 3.788737740251236e-05\n",
            "step: 340, loss: 1.7478821973782033e-05\n",
            "step: 350, loss: 2.9471399102476425e-05\n",
            "step: 360, loss: 0.0007416097796522081\n",
            "step: 370, loss: 1.4412992641155142e-05\n",
            "step: 380, loss: 4.364753476693295e-05\n",
            "step: 390, loss: 0.0016785531770437956\n",
            "step: 400, loss: 3.6642679333454e-05\n",
            "step: 410, loss: 2.182223579438869e-05\n",
            "step: 420, loss: 3.006690167239867e-05\n",
            "step: 430, loss: 2.5808327336562797e-05\n",
            "step: 440, loss: 2.3896282073110342e-05\n",
            "step: 450, loss: 1.5757776054670103e-05\n",
            "step: 460, loss: 1.849207910709083e-05\n",
            "step: 470, loss: 7.281827129190788e-05\n",
            "step: 480, loss: 1.5157985217228997e-05\n",
            "step: 490, loss: 2.6366371457697824e-05\n",
            "step: 500, loss: 0.0010703125735744834\n",
            "step: 510, loss: 2.494742329872679e-05\n",
            "step: 520, loss: 2.4813545678625815e-05\n",
            "step: 530, loss: 2.035455690929666e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9020159399906235, f1=0.8962264150943396, best_f1=0.9066293183940244\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:25, 227.25it/s]\n",
            "load_f1 = 0.9065550906555091\n",
            "real_f1 = 0.901109057301294\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 243.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48a4777-e0da-4ed1-829f-e446f5bc8fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5516156554222107\n",
            "step: 10, loss: 0.38477006554603577\n",
            "step: 20, loss: 0.38197559118270874\n",
            "step: 30, loss: 0.30619603395462036\n",
            "step: 40, loss: 0.20554213225841522\n",
            "step: 50, loss: 0.4644105136394501\n",
            "step: 60, loss: 0.3182171881198883\n",
            "step: 70, loss: 0.2343844473361969\n",
            "step: 80, loss: 0.17086917161941528\n",
            "step: 90, loss: 0.4773711860179901\n",
            "step: 100, loss: 0.49917611479759216\n",
            "step: 110, loss: 0.3038064241409302\n",
            "step: 120, loss: 0.2667301297187805\n",
            "step: 130, loss: 0.3832992911338806\n",
            "step: 140, loss: 0.22242212295532227\n",
            "step: 150, loss: 0.35010725259780884\n",
            "step: 160, loss: 0.2899315655231476\n",
            "step: 170, loss: 0.32188138365745544\n",
            "step: 180, loss: 0.15375567972660065\n",
            "step: 190, loss: 0.22756078839302063\n",
            "step: 200, loss: 0.34526899456977844\n",
            "step: 210, loss: 0.1913747936487198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3719745222929936, f1=0.39800995024875624, best_f1=0.39800995024875624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1533277928829193\n",
            "step: 10, loss: 0.3083803057670593\n",
            "step: 20, loss: 0.23382870852947235\n",
            "step: 30, loss: 0.28334102034568787\n",
            "step: 40, loss: 0.18506555259227753\n",
            "step: 50, loss: 0.11894641071557999\n",
            "step: 60, loss: 0.4237271845340729\n",
            "step: 70, loss: 0.1399688720703125\n",
            "step: 80, loss: 0.3485444486141205\n",
            "step: 90, loss: 0.10670174658298492\n",
            "step: 100, loss: 0.03337734937667847\n",
            "step: 110, loss: 0.1745401918888092\n",
            "step: 120, loss: 0.12911127507686615\n",
            "step: 130, loss: 0.05666111037135124\n",
            "step: 140, loss: 0.21454600989818573\n",
            "step: 150, loss: 0.27788597345352173\n",
            "step: 160, loss: 0.24057255685329437\n",
            "step: 170, loss: 0.18898767232894897\n",
            "step: 180, loss: 0.2926322817802429\n",
            "step: 190, loss: 0.2771397829055786\n",
            "step: 200, loss: 0.08542478084564209\n",
            "step: 210, loss: 0.2429238259792328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.48394004282655245, f1=0.4948024948024948, best_f1=0.4948024948024948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11109201610088348\n",
            "step: 10, loss: 0.2594306170940399\n",
            "step: 20, loss: 0.19941440224647522\n",
            "step: 30, loss: 0.22817020118236542\n",
            "step: 40, loss: 0.10872134566307068\n",
            "step: 50, loss: 0.12735715508460999\n",
            "step: 60, loss: 0.18853704631328583\n",
            "step: 70, loss: 0.3448071777820587\n",
            "step: 80, loss: 0.09647605568170547\n",
            "step: 90, loss: 0.13641822338104248\n",
            "step: 100, loss: 0.19696944952011108\n",
            "step: 110, loss: 0.22750389575958252\n",
            "step: 120, loss: 0.17160575091838837\n",
            "step: 130, loss: 0.17155785858631134\n",
            "step: 140, loss: 0.10488978028297424\n",
            "step: 150, loss: 0.27466318011283875\n",
            "step: 160, loss: 0.05423179641366005\n",
            "step: 170, loss: 0.19240456819534302\n",
            "step: 180, loss: 0.17687398195266724\n",
            "step: 190, loss: 0.22847431898117065\n",
            "step: 200, loss: 0.13052307069301605\n",
            "step: 210, loss: 0.07549970597028732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.499089253187614, f1=0.5261194029850746, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2525823414325714\n",
            "step: 10, loss: 0.24870768189430237\n",
            "step: 20, loss: 0.12024005502462387\n",
            "step: 30, loss: 0.12344493716955185\n",
            "step: 40, loss: 0.061270032078027725\n",
            "step: 50, loss: 0.15474426746368408\n",
            "step: 60, loss: 0.3138546049594879\n",
            "step: 70, loss: 0.23443648219108582\n",
            "step: 80, loss: 0.11089975386857986\n",
            "step: 90, loss: 0.07817861437797546\n",
            "step: 100, loss: 0.32061752676963806\n",
            "step: 110, loss: 0.33689695596694946\n",
            "step: 120, loss: 0.17507876455783844\n",
            "step: 130, loss: 0.0988142117857933\n",
            "step: 140, loss: 0.2725219428539276\n",
            "step: 150, loss: 0.17009393870830536\n",
            "step: 160, loss: 0.06594906002283096\n",
            "step: 170, loss: 0.3469376862049103\n",
            "step: 180, loss: 0.4904560446739197\n",
            "step: 190, loss: 0.19473664462566376\n",
            "step: 200, loss: 0.22540441155433655\n",
            "step: 210, loss: 0.2776365280151367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.4827586206896552, f1=0.4910394265232975, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2569030523300171\n",
            "step: 10, loss: 0.18254971504211426\n",
            "step: 20, loss: 0.24017348885536194\n",
            "step: 30, loss: 0.134999617934227\n",
            "step: 40, loss: 0.0593431331217289\n",
            "step: 50, loss: 0.3756142854690552\n",
            "step: 60, loss: 0.0885249450802803\n",
            "step: 70, loss: 0.08782108128070831\n",
            "step: 80, loss: 0.0756634920835495\n",
            "step: 90, loss: 0.13494233787059784\n",
            "step: 100, loss: 0.007733840495347977\n",
            "step: 110, loss: 0.2757951021194458\n",
            "step: 120, loss: 0.19189248979091644\n",
            "step: 130, loss: 0.15138082206249237\n",
            "step: 140, loss: 0.22149546444416046\n",
            "step: 150, loss: 0.10888627171516418\n",
            "step: 160, loss: 0.09647073596715927\n",
            "step: 170, loss: 0.14804446697235107\n",
            "step: 180, loss: 0.04784199595451355\n",
            "step: 190, loss: 0.04498615860939026\n",
            "step: 200, loss: 0.23579128086566925\n",
            "step: 210, loss: 0.025125056505203247\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4868421052631579, f1=0.47511312217194573, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06880190223455429\n",
            "step: 10, loss: 0.12586544454097748\n",
            "step: 20, loss: 0.05925082415342331\n",
            "step: 30, loss: 0.006538929417729378\n",
            "step: 40, loss: 0.06669461727142334\n",
            "step: 50, loss: 0.04280814155936241\n",
            "step: 60, loss: 0.14741846919059753\n",
            "step: 70, loss: 0.009596450254321098\n",
            "step: 80, loss: 0.06302373856306076\n",
            "step: 90, loss: 0.4166621267795563\n",
            "step: 100, loss: 0.026417803019285202\n",
            "step: 110, loss: 0.10127130150794983\n",
            "step: 120, loss: 0.024881117045879364\n",
            "step: 130, loss: 0.19177110493183136\n",
            "step: 140, loss: 0.08708412945270538\n",
            "step: 150, loss: 0.056750085204839706\n",
            "step: 160, loss: 0.024912571534514427\n",
            "step: 170, loss: 0.1674778312444687\n",
            "step: 180, loss: 0.047472018748521805\n",
            "step: 190, loss: 0.07525449991226196\n",
            "step: 200, loss: 0.05617377907037735\n",
            "step: 210, loss: 0.05711009353399277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.46393762183235865, f1=0.4675834970530452, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007847190834581852\n",
            "step: 10, loss: 0.08267977833747864\n",
            "step: 20, loss: 0.02713206224143505\n",
            "step: 30, loss: 0.02019021101295948\n",
            "step: 40, loss: 0.07252655178308487\n",
            "step: 50, loss: 0.15607735514640808\n",
            "step: 60, loss: 0.06168413907289505\n",
            "step: 70, loss: 0.027273310348391533\n",
            "step: 80, loss: 0.08853266388177872\n",
            "step: 90, loss: 0.05655086040496826\n",
            "step: 100, loss: 0.029626768082380295\n",
            "step: 110, loss: 0.16982360184192657\n",
            "step: 120, loss: 0.03361399844288826\n",
            "step: 130, loss: 0.1209927499294281\n",
            "step: 140, loss: 0.04044698178768158\n",
            "step: 150, loss: 0.027437124401330948\n",
            "step: 160, loss: 0.042170606553554535\n",
            "step: 170, loss: 0.04317723214626312\n",
            "step: 180, loss: 0.02494535781443119\n",
            "step: 190, loss: 0.09734640270471573\n",
            "step: 200, loss: 0.052050478756427765\n",
            "step: 210, loss: 0.11092385649681091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4496124031007752, f1=0.4591439688715953, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003918835427612066\n",
            "step: 10, loss: 0.1395975798368454\n",
            "step: 20, loss: 0.017973007634282112\n",
            "step: 30, loss: 0.029602685943245888\n",
            "step: 40, loss: 0.05412667989730835\n",
            "step: 50, loss: 0.10044778138399124\n",
            "step: 60, loss: 0.14893142879009247\n",
            "step: 70, loss: 0.018060797825455666\n",
            "step: 80, loss: 0.10123546421527863\n",
            "step: 90, loss: 0.008479571901261806\n",
            "step: 100, loss: 0.04757295548915863\n",
            "step: 110, loss: 0.14958034455776215\n",
            "step: 120, loss: 0.015084651298820972\n",
            "step: 130, loss: 0.004053195007145405\n",
            "step: 140, loss: 0.010944307781755924\n",
            "step: 150, loss: 0.11694692820310593\n",
            "step: 160, loss: 0.016283133998513222\n",
            "step: 170, loss: 0.015041464939713478\n",
            "step: 180, loss: 0.040019307285547256\n",
            "step: 190, loss: 0.00621245289221406\n",
            "step: 200, loss: 0.0023442988749593496\n",
            "step: 210, loss: 0.11992128193378448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.47418738049713194, f1=0.46062992125984253, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04081298038363457\n",
            "step: 10, loss: 0.04796475172042847\n",
            "step: 20, loss: 0.010475976392626762\n",
            "step: 30, loss: 0.01630445383489132\n",
            "step: 40, loss: 0.2170509546995163\n",
            "step: 50, loss: 0.17539246380329132\n",
            "step: 60, loss: 0.13735075294971466\n",
            "step: 70, loss: 0.04289231821894646\n",
            "step: 80, loss: 0.017688343301415443\n",
            "step: 90, loss: 0.042578890919685364\n",
            "step: 100, loss: 0.02252950333058834\n",
            "step: 110, loss: 0.10936256498098373\n",
            "step: 120, loss: 0.00451258011162281\n",
            "step: 130, loss: 0.059755075722932816\n",
            "step: 140, loss: 0.02665117383003235\n",
            "step: 150, loss: 0.26198381185531616\n",
            "step: 160, loss: 0.01186465099453926\n",
            "step: 170, loss: 0.04049847647547722\n",
            "step: 180, loss: 0.03276890143752098\n",
            "step: 190, loss: 0.044459857046604156\n",
            "step: 200, loss: 0.038454800844192505\n",
            "step: 210, loss: 0.01773499697446823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.44181459566074954, f1=0.4572564612326043, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13565416634082794\n",
            "step: 10, loss: 0.0369541272521019\n",
            "step: 20, loss: 0.001898337621241808\n",
            "step: 30, loss: 0.1308075487613678\n",
            "step: 40, loss: 0.006459391675889492\n",
            "step: 50, loss: 0.07951418310403824\n",
            "step: 60, loss: 0.009913794696331024\n",
            "step: 70, loss: 0.03296234831213951\n",
            "step: 80, loss: 0.02335839346051216\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 90, loss: 0.07653330266475677\n",
            "step: 100, loss: 0.0036351759918034077\n",
            "step: 110, loss: 0.06344561278820038\n",
            "step: 120, loss: 0.012345103546977043\n",
            "step: 130, loss: 0.0573861338198185\n",
            "step: 140, loss: 0.0023903357796370983\n",
            "step: 150, loss: 0.02234419248998165\n",
            "step: 160, loss: 0.11647326499223709\n",
            "step: 170, loss: 0.01529106218367815\n",
            "step: 180, loss: 0.09432677924633026\n",
            "step: 190, loss: 0.2618560492992401\n",
            "step: 200, loss: 0.20962022244930267\n",
            "step: 210, loss: 0.09123227000236511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.444043321299639, f1=0.45472061657032753, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02646823786199093\n",
            "step: 10, loss: 0.005863533820956945\n",
            "step: 20, loss: 0.01855272240936756\n",
            "step: 30, loss: 0.013707274571061134\n",
            "step: 40, loss: 0.025727737694978714\n",
            "step: 50, loss: 0.004527258221060038\n",
            "step: 60, loss: 0.18236221373081207\n",
            "step: 70, loss: 0.007959089241921902\n",
            "step: 80, loss: 0.2354428768157959\n",
            "step: 90, loss: 0.045560263097286224\n",
            "step: 100, loss: 0.02423195354640484\n",
            "step: 110, loss: 0.13451717793941498\n",
            "step: 120, loss: 0.00803031399846077\n",
            "step: 130, loss: 0.012447762303054333\n",
            "step: 140, loss: 0.015983447432518005\n",
            "step: 150, loss: 0.0039151571691036224\n",
            "step: 160, loss: 0.004867636598646641\n",
            "step: 170, loss: 0.006516020279377699\n",
            "step: 180, loss: 0.003499542595818639\n",
            "step: 190, loss: 0.02299739606678486\n",
            "step: 200, loss: 0.014023983851075172\n",
            "step: 210, loss: 0.004483662545681\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4618181818181818, f1=0.4635514018691589, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008174590766429901\n",
            "step: 10, loss: 0.0017432703170925379\n",
            "step: 20, loss: 0.03356987237930298\n",
            "step: 30, loss: 0.07889938354492188\n",
            "step: 40, loss: 0.10811200737953186\n",
            "step: 50, loss: 0.0015757700894027948\n",
            "step: 60, loss: 0.10353279858827591\n",
            "step: 70, loss: 0.00498080113902688\n",
            "step: 80, loss: 0.0043104724027216434\n",
            "step: 90, loss: 0.009373034350574017\n",
            "step: 100, loss: 0.022868363186717033\n",
            "step: 110, loss: 0.009655038826167583\n",
            "step: 120, loss: 0.027858102694153786\n",
            "step: 130, loss: 0.004167599603533745\n",
            "step: 140, loss: 0.003110239515081048\n",
            "step: 150, loss: 0.0438871793448925\n",
            "step: 160, loss: 0.004876322578638792\n",
            "step: 170, loss: 0.0039971135556697845\n",
            "step: 180, loss: 0.033792782574892044\n",
            "step: 190, loss: 0.024890802800655365\n",
            "step: 200, loss: 0.012060887180268764\n",
            "step: 210, loss: 0.0051330639980733395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.4410646387832699, f1=0.45999999999999996, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022479195147752762\n",
            "step: 10, loss: 0.004918100778013468\n",
            "step: 20, loss: 0.03411475196480751\n",
            "step: 30, loss: 0.05569113790988922\n",
            "step: 40, loss: 0.0038465680554509163\n",
            "step: 50, loss: 0.025729266926646233\n",
            "step: 60, loss: 0.004916914738714695\n",
            "step: 70, loss: 0.06199216470122337\n",
            "step: 80, loss: 0.006312616169452667\n",
            "step: 90, loss: 0.0005969438934698701\n",
            "step: 100, loss: 0.0027393994387239218\n",
            "step: 110, loss: 0.0024386662989854813\n",
            "step: 120, loss: 0.17738182842731476\n",
            "step: 130, loss: 0.00345748127438128\n",
            "step: 140, loss: 0.001231921254657209\n",
            "step: 150, loss: 0.010775333270430565\n",
            "step: 160, loss: 0.002273445250466466\n",
            "step: 170, loss: 0.012862518429756165\n",
            "step: 180, loss: 0.22868947684764862\n",
            "step: 190, loss: 0.0006387735484167933\n",
            "step: 200, loss: 0.019944507628679276\n",
            "step: 210, loss: 0.016583522781729698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.4555984555984556, f1=0.4476386036960986, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016045838128775358\n",
            "step: 10, loss: 0.037996575236320496\n",
            "step: 20, loss: 0.007284677121788263\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.011354937218129635\n",
            "step: 40, loss: 0.002710650209337473\n",
            "step: 50, loss: 0.004760422743856907\n",
            "step: 60, loss: 0.025784680619835854\n",
            "step: 70, loss: 0.0064803543500602245\n",
            "step: 80, loss: 0.003685828996822238\n",
            "step: 90, loss: 0.10225604474544525\n",
            "step: 100, loss: 0.13768118619918823\n",
            "step: 110, loss: 0.004294858779758215\n",
            "step: 120, loss: 0.0029857198242098093\n",
            "step: 130, loss: 0.013033991679549217\n",
            "step: 140, loss: 0.07320744544267654\n",
            "step: 150, loss: 0.014955899678170681\n",
            "step: 160, loss: 0.004324646200984716\n",
            "step: 170, loss: 0.13887271285057068\n",
            "step: 180, loss: 0.00494109932333231\n",
            "step: 190, loss: 0.02359004318714142\n",
            "step: 200, loss: 0.007121098227798939\n",
            "step: 210, loss: 0.004970721900463104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.44799999999999995, f1=0.45473684210526316, best_f1=0.5261194029850746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024806340225040913\n",
            "step: 10, loss: 0.005002561490982771\n",
            "step: 20, loss: 0.06693369895219803\n",
            "step: 30, loss: 0.014626901596784592\n",
            "step: 40, loss: 0.0014346996322274208\n",
            "step: 50, loss: 0.012044484727084637\n",
            "step: 60, loss: 0.08696925640106201\n",
            "step: 70, loss: 0.0020415170583873987\n",
            "step: 80, loss: 0.0015838347608223557\n",
            "step: 90, loss: 0.002966947853565216\n",
            "step: 100, loss: 0.000835337326861918\n",
            "step: 110, loss: 0.03868332505226135\n",
            "step: 120, loss: 0.0031581635121256113\n",
            "step: 130, loss: 0.0014434022596105933\n",
            "step: 140, loss: 0.0004986025742255151\n",
            "step: 150, loss: 0.0009603235521353781\n",
            "step: 160, loss: 0.015811007469892502\n",
            "step: 170, loss: 0.0006271664751693606\n",
            "step: 180, loss: 0.0017444450641050935\n",
            "step: 190, loss: 0.006951163988560438\n",
            "step: 200, loss: 0.0022421462927013636\n",
            "step: 210, loss: 0.1635756641626358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.4488977955911824, f1=0.45702306079664573, best_f1=0.5261194029850746\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 463.20it/s]\n",
            "load_f1 = 0.5062388591800357\n",
            "real_f1 = 0.5\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 269.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b594fe2-680b-4ac2-94e4-4f4b67754ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5669991374015808\n",
            "step: 10, loss: 0.3563770055770874\n",
            "step: 20, loss: 0.28573769330978394\n",
            "step: 30, loss: 0.4394654333591461\n",
            "step: 40, loss: 0.41891783475875854\n",
            "step: 50, loss: 0.3046737015247345\n",
            "step: 60, loss: 0.2950672507286072\n",
            "step: 70, loss: 0.2921471893787384\n",
            "step: 80, loss: 0.23104652762413025\n",
            "step: 90, loss: 0.31017622351646423\n",
            "step: 100, loss: 0.31596043705940247\n",
            "step: 110, loss: 0.35721296072006226\n",
            "step: 120, loss: 0.11731131374835968\n",
            "step: 130, loss: 0.14145933091640472\n",
            "step: 140, loss: 0.08052177727222443\n",
            "step: 150, loss: 0.12646172940731049\n",
            "step: 160, loss: 0.10208643972873688\n",
            "step: 170, loss: 0.2104695439338684\n",
            "step: 180, loss: 0.03449787572026253\n",
            "step: 190, loss: 0.2655513286590576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6849315068493149, f1=0.6770833333333335, best_f1=0.6770833333333335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18866702914237976\n",
            "step: 10, loss: 0.0690009593963623\n",
            "step: 20, loss: 0.19513562321662903\n",
            "step: 30, loss: 0.1526561826467514\n",
            "step: 40, loss: 0.15869739651679993\n",
            "step: 50, loss: 0.07093410938978195\n",
            "step: 60, loss: 0.32272598147392273\n",
            "step: 70, loss: 0.1362772285938263\n",
            "step: 80, loss: 0.11236918717622757\n",
            "step: 90, loss: 0.16504065692424774\n",
            "step: 100, loss: 0.03168398141860962\n",
            "step: 110, loss: 0.15045712888240814\n",
            "step: 120, loss: 0.3363989293575287\n",
            "step: 130, loss: 0.08532950282096863\n",
            "step: 140, loss: 0.05877063795924187\n",
            "step: 150, loss: 0.19898805022239685\n",
            "step: 160, loss: 0.10544648766517639\n",
            "step: 170, loss: 0.18033096194267273\n",
            "step: 180, loss: 0.11083055287599564\n",
            "step: 190, loss: 0.026221629232168198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7324675324675326, f1=0.7593052109181142, best_f1=0.7593052109181142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.049716413021087646\n",
            "step: 10, loss: 0.2050304412841797\n",
            "step: 20, loss: 0.15851512551307678\n",
            "step: 30, loss: 0.2790665626525879\n",
            "step: 40, loss: 0.20573388040065765\n",
            "step: 50, loss: 0.340950608253479\n",
            "step: 60, loss: 0.08331672102212906\n",
            "step: 70, loss: 0.09520358592271805\n",
            "step: 80, loss: 0.11196332424879074\n",
            "step: 90, loss: 0.05900635942816734\n",
            "step: 100, loss: 0.045510560274124146\n",
            "step: 110, loss: 0.046051446348428726\n",
            "step: 120, loss: 0.02999313361942768\n",
            "step: 130, loss: 0.04230271652340889\n",
            "step: 140, loss: 0.07068148255348206\n",
            "step: 150, loss: 0.15951426327228546\n",
            "step: 160, loss: 0.04452424496412277\n",
            "step: 170, loss: 0.08258774876594543\n",
            "step: 180, loss: 0.08265149593353271\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.1311793476343155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7712082262210797, f1=0.7719298245614035, best_f1=0.7719298245614035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02290402166545391\n",
            "step: 10, loss: 0.22268663346767426\n",
            "step: 20, loss: 0.01593191549181938\n",
            "step: 30, loss: 0.05950024351477623\n",
            "step: 40, loss: 0.15862715244293213\n",
            "step: 50, loss: 0.09621763229370117\n",
            "step: 60, loss: 0.11116162687540054\n",
            "step: 70, loss: 0.13005569577217102\n",
            "step: 80, loss: 0.17257997393608093\n",
            "step: 90, loss: 0.026484280824661255\n",
            "step: 100, loss: 0.10829020291566849\n",
            "step: 110, loss: 0.006486358121037483\n",
            "step: 120, loss: 0.10996086150407791\n",
            "step: 130, loss: 0.15278463065624237\n",
            "step: 140, loss: 0.11043108999729156\n",
            "step: 150, loss: 0.08920034766197205\n",
            "step: 160, loss: 0.05569033324718475\n",
            "step: 170, loss: 0.07047149538993835\n",
            "step: 180, loss: 0.03861548751592636\n",
            "step: 190, loss: 0.13160118460655212\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.76, f1=0.7648725212464589, best_f1=0.7719298245614035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05891585350036621\n",
            "step: 10, loss: 0.005465284921228886\n",
            "step: 20, loss: 0.05822551250457764\n",
            "step: 30, loss: 0.011787984520196915\n",
            "step: 40, loss: 0.05598985403776169\n",
            "step: 50, loss: 0.17650356888771057\n",
            "step: 60, loss: 0.033980827778577805\n",
            "step: 70, loss: 0.0244512390345335\n",
            "step: 80, loss: 0.0930861085653305\n",
            "step: 90, loss: 0.01298443228006363\n",
            "step: 100, loss: 0.002018332714214921\n",
            "step: 110, loss: 0.01387566328048706\n",
            "step: 120, loss: 0.09833719581365585\n",
            "step: 130, loss: 0.012527018785476685\n",
            "step: 140, loss: 0.16744209825992584\n",
            "step: 150, loss: 0.0675189271569252\n",
            "step: 160, loss: 0.024159317836165428\n",
            "step: 170, loss: 0.009513855911791325\n",
            "step: 180, loss: 0.13006770610809326\n",
            "step: 190, loss: 0.10511761903762817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7653061224489796, f1=0.7193877551020408, best_f1=0.7719298245614035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011553010903298855\n",
            "step: 10, loss: 0.09094428271055222\n",
            "step: 20, loss: 0.0046874480322003365\n",
            "step: 30, loss: 0.03724236786365509\n",
            "step: 40, loss: 0.14281520247459412\n",
            "step: 50, loss: 0.00857336912304163\n",
            "step: 60, loss: 0.0712134838104248\n",
            "step: 70, loss: 0.0340469554066658\n",
            "step: 80, loss: 0.0516098290681839\n",
            "step: 90, loss: 0.03014385513961315\n",
            "step: 100, loss: 0.0016339208232238889\n",
            "step: 110, loss: 0.019108258187770844\n",
            "step: 120, loss: 0.2703437805175781\n",
            "step: 130, loss: 0.09979597479104996\n",
            "step: 140, loss: 0.02070734277367592\n",
            "step: 150, loss: 0.011403489857912064\n",
            "step: 160, loss: 0.05194355547428131\n",
            "step: 170, loss: 0.037370339035987854\n",
            "step: 180, loss: 0.04656781628727913\n",
            "step: 190, loss: 0.06716009229421616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7734806629834254, f1=0.7826086956521738, best_f1=0.7826086956521738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013160320930182934\n",
            "step: 10, loss: 0.005679173860698938\n",
            "step: 20, loss: 0.07729094475507736\n",
            "step: 30, loss: 0.02996532991528511\n",
            "step: 40, loss: 0.004561900161206722\n",
            "step: 50, loss: 0.18137557804584503\n",
            "step: 60, loss: 0.08934424072504044\n",
            "step: 70, loss: 0.0022073527798056602\n",
            "step: 80, loss: 0.014499600976705551\n",
            "step: 90, loss: 0.00247846357524395\n",
            "step: 100, loss: 0.0028896601870656013\n",
            "step: 110, loss: 0.012071935459971428\n",
            "step: 120, loss: 0.0011146850883960724\n",
            "step: 130, loss: 0.04472050443291664\n",
            "step: 140, loss: 0.028603294864296913\n",
            "step: 150, loss: 0.22888098657131195\n",
            "step: 160, loss: 0.13699595630168915\n",
            "step: 170, loss: 0.021238449960947037\n",
            "step: 180, loss: 0.004311729222536087\n",
            "step: 190, loss: 0.004826403688639402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7811634349030471, f1=0.7845303867403315, best_f1=0.7845303867403315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004266939125955105\n",
            "step: 10, loss: 0.006062089465558529\n",
            "step: 20, loss: 0.2152368277311325\n",
            "step: 30, loss: 0.0035333908163011074\n",
            "step: 40, loss: 0.021324975416064262\n",
            "step: 50, loss: 0.0011088327737525105\n",
            "step: 60, loss: 0.003471772186458111\n",
            "step: 70, loss: 0.02043156325817108\n",
            "step: 80, loss: 0.0014933552592992783\n",
            "step: 90, loss: 0.008824458345770836\n",
            "step: 100, loss: 0.01855318807065487\n",
            "step: 110, loss: 0.14381645619869232\n",
            "step: 120, loss: 0.02849585935473442\n",
            "step: 130, loss: 0.0023078974336385727\n",
            "step: 140, loss: 0.0972885936498642\n",
            "step: 150, loss: 0.08836150914430618\n",
            "step: 160, loss: 0.06466471403837204\n",
            "step: 170, loss: 0.004194925539195538\n",
            "step: 180, loss: 0.05116704851388931\n",
            "step: 190, loss: 0.02970467321574688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.79155672823219, f1=0.772845953002611, best_f1=0.772845953002611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01914818212389946\n",
            "step: 10, loss: 0.002476002089679241\n",
            "step: 20, loss: 0.0023341828491538763\n",
            "step: 30, loss: 0.026942085474729538\n",
            "step: 40, loss: 0.0026212024968117476\n",
            "step: 50, loss: 0.009508755058050156\n",
            "step: 60, loss: 0.0010891694109886885\n",
            "step: 70, loss: 0.003264941740781069\n",
            "step: 80, loss: 0.0012104682391509414\n",
            "step: 90, loss: 0.004059562459588051\n",
            "step: 100, loss: 0.0019012034172192216\n",
            "step: 110, loss: 0.004923532251268625\n",
            "step: 120, loss: 0.0023412536829710007\n",
            "step: 130, loss: 0.004196696914732456\n",
            "step: 140, loss: 0.031893059611320496\n",
            "step: 150, loss: 0.0015740209491923451\n",
            "step: 160, loss: 0.0005385020049288869\n",
            "step: 170, loss: 0.0026676184497773647\n",
            "step: 180, loss: 0.001990197692066431\n",
            "step: 190, loss: 0.0008229318773373961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7838616714697405, f1=0.7392550143266476, best_f1=0.772845953002611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004064297303557396\n",
            "step: 10, loss: 0.0007253009243868291\n",
            "step: 20, loss: 0.010397692210972309\n",
            "step: 30, loss: 0.00033048729528672993\n",
            "step: 40, loss: 0.006299017462879419\n",
            "step: 50, loss: 0.016630878672003746\n",
            "step: 60, loss: 0.0020211581140756607\n",
            "step: 70, loss: 0.07199663668870926\n",
            "step: 80, loss: 0.0009346367442049086\n",
            "step: 90, loss: 0.003279935335740447\n",
            "step: 100, loss: 0.008070547133684158\n",
            "step: 110, loss: 0.05352494493126869\n",
            "step: 120, loss: 0.08130523562431335\n",
            "step: 130, loss: 0.0029940688982605934\n",
            "step: 140, loss: 0.006980027537792921\n",
            "step: 150, loss: 0.017591720446944237\n",
            "step: 160, loss: 0.020572267472743988\n",
            "step: 170, loss: 0.005191023461520672\n",
            "step: 180, loss: 0.15574753284454346\n",
            "step: 190, loss: 0.002033549128100276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7880434782608696, f1=0.7870619946091645, best_f1=0.772845953002611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006669610156677663\n",
            "step: 10, loss: 0.007736229337751865\n",
            "step: 20, loss: 0.004706737585365772\n",
            "step: 30, loss: 0.0038105370476841927\n",
            "step: 40, loss: 0.003517244243994355\n",
            "step: 50, loss: 0.005147071089595556\n",
            "step: 60, loss: 0.026927510276436806\n",
            "step: 70, loss: 0.0012742863036692142\n",
            "step: 80, loss: 0.03159359470009804\n",
            "step: 90, loss: 0.0015011432114988565\n",
            "step: 100, loss: 0.00052467524074018\n",
            "step: 110, loss: 0.0014479487435892224\n",
            "step: 120, loss: 0.0009463653550483286\n",
            "step: 130, loss: 0.0027082376182079315\n",
            "step: 140, loss: 0.006891028489917517\n",
            "step: 150, loss: 0.0011933700880035758\n",
            "step: 160, loss: 0.0041369847021996975\n",
            "step: 170, loss: 0.0006030541262589395\n",
            "step: 180, loss: 0.013475948013365269\n",
            "step: 190, loss: 0.001628492260351777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.793201133144476, f1=0.7422096317280454, best_f1=0.7422096317280454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008152599330060184\n",
            "step: 10, loss: 0.000783736992161721\n",
            "step: 20, loss: 0.11330869048833847\n",
            "step: 30, loss: 0.0018019923008978367\n",
            "step: 40, loss: 0.0030975118279457092\n",
            "step: 50, loss: 0.025294341146945953\n",
            "step: 60, loss: 0.000797385408077389\n",
            "step: 70, loss: 0.000722641940228641\n",
            "step: 80, loss: 0.0005850344314239919\n",
            "step: 90, loss: 0.01746349222958088\n",
            "step: 100, loss: 0.00044816863373853266\n",
            "step: 110, loss: 0.0003317016235087067\n",
            "step: 120, loss: 0.02899681217968464\n",
            "step: 130, loss: 0.0016329885693266988\n",
            "step: 140, loss: 0.0019457279704511166\n",
            "step: 150, loss: 0.00069981295382604\n",
            "step: 160, loss: 0.0002679827739484608\n",
            "step: 170, loss: 0.0015019080601632595\n",
            "step: 180, loss: 0.0005528354668058455\n",
            "step: 190, loss: 0.0018998049199581146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7786666666666667, f1=0.765625, best_f1=0.7422096317280454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21870063245296478\n",
            "step: 10, loss: 0.051601167768239975\n",
            "step: 20, loss: 0.006463070400059223\n",
            "step: 30, loss: 0.0019322825828567147\n",
            "step: 40, loss: 0.0005759296473115683\n",
            "step: 50, loss: 0.005157663021236658\n",
            "step: 60, loss: 0.0008377806516364217\n",
            "step: 70, loss: 0.0012029180070385337\n",
            "step: 80, loss: 0.0010068066185340285\n",
            "step: 90, loss: 0.0011775654274970293\n",
            "step: 100, loss: 0.0003628105332609266\n",
            "step: 110, loss: 0.001372704515233636\n",
            "step: 120, loss: 0.0008184186881408095\n",
            "step: 130, loss: 0.0021516787819564342\n",
            "step: 140, loss: 0.0003866353363264352\n",
            "step: 150, loss: 0.001415325328707695\n",
            "step: 160, loss: 0.004119126591831446\n",
            "step: 170, loss: 0.0004073833115398884\n",
            "step: 180, loss: 0.0009091104147955775\n",
            "step: 190, loss: 0.008930196985602379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7735368956743003, f1=0.7551020408163265, best_f1=0.7422096317280454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022467102389782667\n",
            "step: 10, loss: 0.0008750377455726266\n",
            "step: 20, loss: 0.0007370767998509109\n",
            "step: 30, loss: 0.0002827641146723181\n",
            "step: 40, loss: 0.004076520446687937\n",
            "step: 50, loss: 0.0007670930353924632\n",
            "step: 60, loss: 0.0719413161277771\n",
            "step: 70, loss: 0.0019373701652511954\n",
            "step: 80, loss: 0.0009199320338666439\n",
            "step: 90, loss: 0.0036152570974081755\n",
            "step: 100, loss: 0.12206050753593445\n",
            "step: 110, loss: 0.005820742342621088\n",
            "step: 120, loss: 0.18409976363182068\n",
            "step: 130, loss: 0.0003632090229075402\n",
            "step: 140, loss: 0.0026273152325302362\n",
            "step: 150, loss: 0.0004595224454533309\n",
            "step: 160, loss: 0.0005326498067006469\n",
            "step: 170, loss: 0.0013971652369946241\n",
            "step: 180, loss: 0.027370641008019447\n",
            "step: 190, loss: 0.004341017920523882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7777777777777779, f1=0.7506702412868632, best_f1=0.7422096317280454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033171274699270725\n",
            "step: 10, loss: 0.0010006467346102\n",
            "step: 20, loss: 0.0007933027809485793\n",
            "step: 30, loss: 0.0008430028101429343\n",
            "step: 40, loss: 0.0005395353073254228\n",
            "step: 50, loss: 0.01456774678081274\n",
            "step: 60, loss: 0.0009720121161080897\n",
            "step: 70, loss: 0.026932979002594948\n",
            "step: 80, loss: 0.00047225228627212346\n",
            "step: 90, loss: 0.00037791821523569524\n",
            "step: 100, loss: 0.0005337080219760537\n",
            "step: 110, loss: 0.0009265955886803567\n",
            "step: 120, loss: 0.00040430252556689084\n",
            "step: 130, loss: 0.0005447946605272591\n",
            "step: 140, loss: 0.0016626815777271986\n",
            "step: 150, loss: 0.0009456851985305548\n",
            "step: 160, loss: 0.0017015996854752302\n",
            "step: 170, loss: 0.0005457581137306988\n",
            "step: 180, loss: 0.014649055898189545\n",
            "step: 190, loss: 0.004592378623783588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7835616438356164, f1=0.7452054794520547, best_f1=0.7422096317280454\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 239.62it/s]\n",
            "load_f1 = 0.7365728900255755\n",
            "real_f1 = 0.7104622871046229\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 271.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5ad580-15f9-437a-9374-13229a18107f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6306028366088867\n",
            "step: 10, loss: 0.375547856092453\n",
            "step: 20, loss: 0.2945812940597534\n",
            "step: 30, loss: 0.3802810609340668\n",
            "step: 40, loss: 0.2842734158039093\n",
            "step: 50, loss: 0.2595020830631256\n",
            "step: 60, loss: 0.24579459428787231\n",
            "step: 70, loss: 0.3723761737346649\n",
            "step: 80, loss: 0.3605038523674011\n",
            "step: 90, loss: 0.2873445451259613\n",
            "step: 100, loss: 0.17969846725463867\n",
            "step: 110, loss: 0.2084912806749344\n",
            "step: 120, loss: 0.2148619294166565\n",
            "step: 130, loss: 0.06457886099815369\n",
            "step: 140, loss: 0.3348064124584198\n",
            "step: 150, loss: 0.39388591051101685\n",
            "step: 160, loss: 0.1294320821762085\n",
            "step: 170, loss: 0.20263883471488953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.684863523573201, f1=0.6634382566585957, best_f1=0.6634382566585957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.079014852643013\n",
            "step: 10, loss: 0.19095943868160248\n",
            "step: 20, loss: 0.04997483268380165\n",
            "step: 30, loss: 0.17313706874847412\n",
            "step: 40, loss: 0.0754399448633194\n",
            "step: 50, loss: 0.18916182219982147\n",
            "step: 60, loss: 0.3007543683052063\n",
            "step: 70, loss: 0.08682786673307419\n",
            "step: 80, loss: 0.08088172227144241\n",
            "step: 90, loss: 0.1440044343471527\n",
            "step: 100, loss: 0.14541016519069672\n",
            "step: 110, loss: 0.07066642493009567\n",
            "step: 120, loss: 0.20166926085948944\n",
            "step: 130, loss: 0.02898235246539116\n",
            "step: 140, loss: 0.24286778271198273\n",
            "step: 150, loss: 0.1733795702457428\n",
            "step: 160, loss: 0.19295963644981384\n",
            "step: 170, loss: 0.09779797494411469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7427055702917773, f1=0.6961038961038961, best_f1=0.6961038961038961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10328592360019684\n",
            "step: 10, loss: 0.06677012890577316\n",
            "step: 20, loss: 0.0222457367926836\n",
            "step: 30, loss: 0.07060609757900238\n",
            "step: 40, loss: 0.07010503113269806\n",
            "step: 50, loss: 0.17225901782512665\n",
            "step: 60, loss: 0.27802643179893494\n",
            "step: 70, loss: 0.16293105483055115\n",
            "step: 80, loss: 0.0669272318482399\n",
            "step: 90, loss: 0.07312189042568207\n",
            "step: 100, loss: 0.02546011656522751\n",
            "step: 110, loss: 0.14184501767158508\n",
            "step: 120, loss: 0.07851701229810715\n",
            "step: 130, loss: 0.0701800137758255\n",
            "step: 140, loss: 0.07976000756025314\n",
            "step: 150, loss: 0.02307366020977497\n",
            "step: 160, loss: 0.08416341245174408\n",
            "step: 170, loss: 0.0746067464351654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7303370786516854, f1=0.6933333333333334, best_f1=0.6961038961038961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026310259476304054\n",
            "step: 10, loss: 0.04835205897688866\n",
            "step: 20, loss: 0.014103780500590801\n",
            "step: 30, loss: 0.12760713696479797\n",
            "step: 40, loss: 0.002269486663863063\n",
            "step: 50, loss: 0.16953463852405548\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.12996380031108856\n",
            "step: 70, loss: 0.034537266939878464\n",
            "step: 80, loss: 0.08200138807296753\n",
            "step: 90, loss: 0.044092100113630295\n",
            "step: 100, loss: 0.13309752941131592\n",
            "step: 110, loss: 0.07564502954483032\n",
            "step: 120, loss: 0.031142832711338997\n",
            "step: 130, loss: 0.19486136734485626\n",
            "step: 140, loss: 0.041177138686180115\n",
            "step: 150, loss: 0.04464639350771904\n",
            "step: 160, loss: 0.03293468803167343\n",
            "step: 170, loss: 0.05767257511615753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7287234042553191, f1=0.6788511749347259, best_f1=0.6961038961038961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07500722259283066\n",
            "step: 10, loss: 0.12597286701202393\n",
            "step: 20, loss: 0.011574970558285713\n",
            "step: 30, loss: 0.03641238063573837\n",
            "step: 40, loss: 0.004987575579434633\n",
            "step: 50, loss: 0.11135996133089066\n",
            "step: 60, loss: 0.02249016985297203\n",
            "step: 70, loss: 0.21048504114151\n",
            "step: 80, loss: 0.10747247189283371\n",
            "step: 90, loss: 0.08123087137937546\n",
            "step: 100, loss: 0.12578809261322021\n",
            "step: 110, loss: 0.18662677705287933\n",
            "step: 120, loss: 0.021726766601204872\n",
            "step: 130, loss: 0.0921863317489624\n",
            "step: 140, loss: 0.10668592900037766\n",
            "step: 150, loss: 0.1381109058856964\n",
            "step: 160, loss: 0.207439124584198\n",
            "step: 170, loss: 0.21252337098121643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7487179487179487, f1=0.6891566265060242, best_f1=0.6891566265060242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007019991520792246\n",
            "step: 10, loss: 0.09705978631973267\n",
            "step: 20, loss: 0.00596743356436491\n",
            "step: 30, loss: 0.04155189171433449\n",
            "step: 40, loss: 0.014034731313586235\n",
            "step: 50, loss: 0.11586020886898041\n",
            "step: 60, loss: 0.2002231627702713\n",
            "step: 70, loss: 0.06904945522546768\n",
            "step: 80, loss: 0.029152492061257362\n",
            "step: 90, loss: 0.0991290882229805\n",
            "step: 100, loss: 0.036694541573524475\n",
            "step: 110, loss: 0.02458631433546543\n",
            "step: 120, loss: 0.06695941835641861\n",
            "step: 130, loss: 0.041255079209804535\n",
            "step: 140, loss: 0.0160845797508955\n",
            "step: 150, loss: 0.022930379956960678\n",
            "step: 160, loss: 0.06982962787151337\n",
            "step: 170, loss: 0.0239556971937418\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7425474254742549, f1=0.6596306068601583, best_f1=0.6891566265060242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028772868681699038\n",
            "step: 10, loss: 0.03168449550867081\n",
            "step: 20, loss: 0.003129341173917055\n",
            "step: 30, loss: 0.004621278028935194\n",
            "step: 40, loss: 0.029182258993387222\n",
            "step: 50, loss: 0.032347094267606735\n",
            "step: 60, loss: 0.012271659448742867\n",
            "step: 70, loss: 0.13849014043807983\n",
            "step: 80, loss: 0.04809127002954483\n",
            "step: 90, loss: 0.0009752122568897903\n",
            "step: 100, loss: 0.14490115642547607\n",
            "step: 110, loss: 0.0024730449076741934\n",
            "step: 120, loss: 0.028240490704774857\n",
            "step: 130, loss: 0.1767711043357849\n",
            "step: 140, loss: 0.004547589924186468\n",
            "step: 150, loss: 0.004451517015695572\n",
            "step: 160, loss: 0.009483936242759228\n",
            "step: 170, loss: 0.07319007068872452\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7544303797468355, f1=0.6875, best_f1=0.6875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020692383870482445\n",
            "step: 10, loss: 0.002484292956069112\n",
            "step: 20, loss: 0.012767273001372814\n",
            "step: 30, loss: 0.008640802465379238\n",
            "step: 40, loss: 0.00026189247728325427\n",
            "step: 50, loss: 0.0010154732735827565\n",
            "step: 60, loss: 0.027266407385468483\n",
            "step: 70, loss: 0.01956201158463955\n",
            "step: 80, loss: 0.007243017200380564\n",
            "step: 90, loss: 0.03141449764370918\n",
            "step: 100, loss: 0.037373386323451996\n",
            "step: 110, loss: 0.03237709775567055\n",
            "step: 120, loss: 0.0017965127481147647\n",
            "step: 130, loss: 0.005122466012835503\n",
            "step: 140, loss: 0.04166979715228081\n",
            "step: 150, loss: 0.011879796162247658\n",
            "step: 160, loss: 0.013646155595779419\n",
            "step: 170, loss: 0.0012797435047104955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7611548556430446, f1=0.6765432098765432, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015076100826263428\n",
            "step: 10, loss: 0.00968840904533863\n",
            "step: 20, loss: 0.0004940062644891441\n",
            "step: 30, loss: 0.003560279496014118\n",
            "step: 40, loss: 0.007098449394106865\n",
            "step: 50, loss: 0.001211208407767117\n",
            "step: 60, loss: 0.07017780095338821\n",
            "step: 70, loss: 0.042450617998838425\n",
            "step: 80, loss: 0.001767513225786388\n",
            "step: 90, loss: 0.03181811794638634\n",
            "step: 100, loss: 0.0032179132103919983\n",
            "step: 110, loss: 0.001822007354348898\n",
            "step: 120, loss: 0.12371838092803955\n",
            "step: 130, loss: 0.03654395043849945\n",
            "step: 140, loss: 0.0056238723918795586\n",
            "step: 150, loss: 0.004871978424489498\n",
            "step: 160, loss: 0.017549004405736923\n",
            "step: 170, loss: 0.01583797298371792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7559055118110237, f1=0.6855670103092784, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03354783356189728\n",
            "step: 10, loss: 0.0012065083719789982\n",
            "step: 20, loss: 0.023009544238448143\n",
            "step: 30, loss: 0.13776260614395142\n",
            "step: 40, loss: 0.00031032165861688554\n",
            "step: 50, loss: 0.001271998044103384\n",
            "step: 60, loss: 0.001305223093368113\n",
            "step: 70, loss: 0.028370968997478485\n",
            "step: 80, loss: 0.00042955190292559564\n",
            "step: 90, loss: 0.013544498011469841\n",
            "step: 100, loss: 0.00028725009178742766\n",
            "step: 110, loss: 0.028255203738808632\n",
            "step: 120, loss: 0.001065578544512391\n",
            "step: 130, loss: 0.002793444087728858\n",
            "step: 140, loss: 0.07746372371912003\n",
            "step: 150, loss: 0.024699252098798752\n",
            "step: 160, loss: 0.036234524101018906\n",
            "step: 170, loss: 0.006069280207157135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7349081364829397, f1=0.6865671641791046, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10612209886312485\n",
            "step: 10, loss: 0.002861126558855176\n",
            "step: 20, loss: 0.002123218961060047\n",
            "step: 30, loss: 0.0006749752792529762\n",
            "step: 40, loss: 0.0006674777250736952\n",
            "step: 50, loss: 0.009155414067208767\n",
            "step: 60, loss: 0.0965646505355835\n",
            "step: 70, loss: 0.005555363371968269\n",
            "step: 80, loss: 0.0004404261999297887\n",
            "step: 90, loss: 0.0017991416389122605\n",
            "step: 100, loss: 0.0005787370027974248\n",
            "step: 110, loss: 0.15905094146728516\n",
            "step: 120, loss: 0.0012548785889521241\n",
            "step: 130, loss: 0.0023813049774616957\n",
            "step: 140, loss: 0.003494460368528962\n",
            "step: 150, loss: 0.006112012080848217\n",
            "step: 160, loss: 0.02200733870267868\n",
            "step: 170, loss: 0.0049991304986178875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7329842931937172, f1=0.6812652068126521, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004689149500336498\n",
            "step: 10, loss: 0.0015856027603149414\n",
            "step: 20, loss: 0.006759136449545622\n",
            "step: 30, loss: 0.0010082870721817017\n",
            "step: 40, loss: 0.0025459073949605227\n",
            "step: 50, loss: 0.0003938714216928929\n",
            "step: 60, loss: 0.019197987392544746\n",
            "step: 70, loss: 0.022358929738402367\n",
            "step: 80, loss: 0.00015368068125098944\n",
            "step: 90, loss: 0.0006388541660271585\n",
            "step: 100, loss: 0.0009108234080486\n",
            "step: 110, loss: 0.006590315140783787\n",
            "step: 120, loss: 0.04183662682771683\n",
            "step: 130, loss: 0.0014758300967514515\n",
            "step: 140, loss: 0.00262852874584496\n",
            "step: 150, loss: 0.023599622771143913\n",
            "step: 160, loss: 0.007709610275924206\n",
            "step: 170, loss: 0.0006120817852206528\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7238605898123324, f1=0.7022900763358779, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000818712986074388\n",
            "step: 10, loss: 0.001346004195511341\n",
            "step: 20, loss: 0.0029931417666375637\n",
            "step: 30, loss: 0.00048106032772921026\n",
            "step: 40, loss: 0.0007539608050137758\n",
            "step: 50, loss: 0.008941726759076118\n",
            "step: 60, loss: 0.0015178648754954338\n",
            "step: 70, loss: 0.0014067908050492406\n",
            "step: 80, loss: 0.001850320608355105\n",
            "step: 90, loss: 0.00038418322219513357\n",
            "step: 100, loss: 0.02925891801714897\n",
            "step: 110, loss: 8.75144251040183e-05\n",
            "step: 120, loss: 0.009767325595021248\n",
            "step: 130, loss: 7.199410174507648e-05\n",
            "step: 140, loss: 0.00038413709262385964\n",
            "step: 150, loss: 0.0005043752025812864\n",
            "step: 160, loss: 0.00028735995874740183\n",
            "step: 170, loss: 0.002277128864079714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7282608695652175, f1=0.7049608355091384, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003902257012668997\n",
            "step: 10, loss: 0.00010722235310822725\n",
            "step: 20, loss: 0.0008086463203653693\n",
            "step: 30, loss: 0.0009229448041878641\n",
            "step: 40, loss: 0.0001419565815012902\n",
            "step: 50, loss: 0.0001473233860451728\n",
            "step: 60, loss: 0.001504651503637433\n",
            "step: 70, loss: 0.0008912413031794131\n",
            "step: 80, loss: 0.001386629999615252\n",
            "step: 90, loss: 0.0003998947504442185\n",
            "step: 100, loss: 0.008808568120002747\n",
            "step: 110, loss: 0.0002708864922169596\n",
            "step: 120, loss: 0.006375140510499477\n",
            "step: 130, loss: 0.019248586148023605\n",
            "step: 140, loss: 0.0006029929500073195\n",
            "step: 150, loss: 7.492581789847463e-05\n",
            "step: 160, loss: 4.481776340981014e-05\n",
            "step: 170, loss: 8.832142339088023e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7467362924281984, f1=0.7032418952618454, best_f1=0.6765432098765432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00024650455452501774\n",
            "step: 10, loss: 0.010770321823656559\n",
            "step: 20, loss: 0.0035095438361167908\n",
            "step: 30, loss: 0.0006704897969029844\n",
            "step: 40, loss: 0.0008904136484488845\n",
            "step: 50, loss: 0.0058739641681313515\n",
            "step: 60, loss: 0.010514365509152412\n",
            "step: 70, loss: 6.555533036589622e-05\n",
            "step: 80, loss: 0.001033502398058772\n",
            "step: 90, loss: 0.0013039883924648166\n",
            "step: 100, loss: 0.00043748709140345454\n",
            "step: 110, loss: 0.00013066285464446992\n",
            "step: 120, loss: 0.004694841336458921\n",
            "step: 130, loss: 0.00041687514749355614\n",
            "step: 140, loss: 0.00018318775983061641\n",
            "step: 150, loss: 0.0006024149479344487\n",
            "step: 160, loss: 0.00024240183120127767\n",
            "step: 170, loss: 0.0004016159218735993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7454068241469816, f1=0.7043701799485862, best_f1=0.6765432098765432\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 291.17it/s]\n",
            "load_f1 = 0.5204918032786885\n",
            "real_f1 = 0.4932562620423892\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 274.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b933737a-7fd1-4bb4-872f-b9c36705ff08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 376kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 253kB/s] \n",
            "Downloading: 100% 440M/440M [00:06<00:00, 64.0MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6056886315345764\n",
            "step: 10, loss: 0.6477164030075073\n",
            "step: 20, loss: 0.5119900703430176\n",
            "step: 30, loss: 0.5053315162658691\n",
            "step: 40, loss: 0.34141796827316284\n",
            "step: 50, loss: 0.11920282244682312\n",
            "step: 60, loss: 0.06928717344999313\n",
            "step: 70, loss: 0.05863545462489128\n",
            "step: 80, loss: 0.1128811240196228\n",
            "step: 90, loss: 0.08870967477560043\n",
            "step: 100, loss: 0.00643910700455308\n",
            "step: 110, loss: 0.12502220273017883\n",
            "step: 120, loss: 0.03325754404067993\n",
            "step: 130, loss: 0.010545316152274609\n",
            "step: 140, loss: 0.028176141902804375\n",
            "step: 150, loss: 0.03826598450541496\n",
            "step: 160, loss: 0.007408057805150747\n",
            "step: 170, loss: 0.25152692198753357\n",
            "step: 180, loss: 0.0796140804886818\n",
            "step: 190, loss: 0.06281603872776031\n",
            "step: 200, loss: 0.06306150555610657\n",
            "step: 210, loss: 0.019388064742088318\n",
            "step: 220, loss: 0.014405859634280205\n",
            "step: 230, loss: 0.06326166540384293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9508928571428572, f1=0.9592760180995475, best_f1=0.9592760180995475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008791577070951462\n",
            "step: 10, loss: 0.03250126913189888\n",
            "step: 20, loss: 0.2226814329624176\n",
            "step: 30, loss: 0.3092679977416992\n",
            "step: 40, loss: 0.08200091123580933\n",
            "step: 50, loss: 0.005605530459433794\n",
            "step: 60, loss: 0.07906699180603027\n",
            "step: 70, loss: 0.11032112687826157\n",
            "step: 80, loss: 0.09134995192289352\n",
            "step: 90, loss: 0.012800583615899086\n",
            "step: 100, loss: 0.0578479990363121\n",
            "step: 110, loss: 0.1459243893623352\n",
            "step: 120, loss: 0.04309351369738579\n",
            "step: 130, loss: 0.11459571123123169\n",
            "step: 140, loss: 0.002224658615887165\n",
            "step: 150, loss: 0.02547730877995491\n",
            "step: 160, loss: 0.031063206493854523\n",
            "step: 170, loss: 0.044052738696336746\n",
            "step: 180, loss: 0.11072483658790588\n",
            "step: 190, loss: 0.021165112033486366\n",
            "step: 200, loss: 0.006745608989149332\n",
            "step: 210, loss: 0.002472186926752329\n",
            "step: 220, loss: 0.08070580661296844\n",
            "step: 230, loss: 0.13104377686977386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9426048565121413, f1=0.931537598204265, best_f1=0.9592760180995475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01691759191453457\n",
            "step: 10, loss: 0.014123152941465378\n",
            "step: 20, loss: 0.050486184656620026\n",
            "step: 30, loss: 0.016101285815238953\n",
            "step: 40, loss: 0.12799471616744995\n",
            "step: 50, loss: 0.06502983719110489\n",
            "step: 60, loss: 0.01078851893544197\n",
            "step: 70, loss: 0.06711624562740326\n",
            "step: 80, loss: 0.001951704267412424\n",
            "step: 90, loss: 0.07047472149133682\n",
            "step: 100, loss: 0.1631428599357605\n",
            "step: 110, loss: 0.0017884352710098028\n",
            "step: 120, loss: 0.008695611730217934\n",
            "step: 130, loss: 0.001301920972764492\n",
            "step: 140, loss: 0.030527396127581596\n",
            "step: 150, loss: 0.046671267598867416\n",
            "step: 160, loss: 0.024337556213140488\n",
            "step: 170, loss: 0.03313437104225159\n",
            "step: 180, loss: 0.061952855437994\n",
            "step: 190, loss: 0.009832366369664669\n",
            "step: 200, loss: 0.03783845156431198\n",
            "step: 210, loss: 0.005356995388865471\n",
            "step: 220, loss: 0.006451861932873726\n",
            "step: 230, loss: 0.0007962350500747561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9581920903954803, f1=0.9476082004555808, best_f1=0.9476082004555808\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012707623653113842\n",
            "step: 10, loss: 0.0009926398051902652\n",
            "step: 20, loss: 0.0005195985431782901\n",
            "step: 30, loss: 0.001970821525901556\n",
            "step: 40, loss: 0.0028502659406512976\n",
            "step: 50, loss: 0.0008518394315615296\n",
            "step: 60, loss: 0.001441402593627572\n",
            "step: 70, loss: 0.004948113113641739\n",
            "step: 80, loss: 0.0014881190145388246\n",
            "step: 90, loss: 0.014757232740521431\n",
            "step: 100, loss: 0.0709032341837883\n",
            "step: 110, loss: 0.0022736345417797565\n",
            "step: 120, loss: 0.05791407451033592\n",
            "step: 130, loss: 0.048039648681879044\n",
            "step: 140, loss: 0.0032400607597082853\n",
            "step: 150, loss: 0.281240314245224\n",
            "step: 160, loss: 0.013970690779387951\n",
            "step: 170, loss: 0.010973690077662468\n",
            "step: 180, loss: 0.0013722728472203016\n",
            "step: 190, loss: 0.012073108926415443\n",
            "step: 200, loss: 0.0015954344999045134\n",
            "step: 210, loss: 0.07271133363246918\n",
            "step: 220, loss: 0.00218446203507483\n",
            "step: 230, loss: 0.006318948231637478\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9651293588301463, f1=0.9573033707865168, best_f1=0.9573033707865168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012565661454573274\n",
            "step: 10, loss: 0.0031498614698648453\n",
            "step: 20, loss: 0.006162989418953657\n",
            "step: 30, loss: 0.0010558712529018521\n",
            "step: 40, loss: 0.005690914113074541\n",
            "step: 50, loss: 0.006712621543556452\n",
            "step: 60, loss: 0.15412576496601105\n",
            "step: 70, loss: 0.0009461325826123357\n",
            "step: 80, loss: 0.003429712727665901\n",
            "step: 90, loss: 0.03385109454393387\n",
            "step: 100, loss: 0.0005808679852634668\n",
            "step: 110, loss: 0.0006628865958191454\n",
            "step: 120, loss: 0.0009869420900940895\n",
            "step: 130, loss: 0.009230067953467369\n",
            "step: 140, loss: 0.007560462225228548\n",
            "step: 150, loss: 0.0018889947095885873\n",
            "step: 160, loss: 0.0003822194703388959\n",
            "step: 170, loss: 0.014951176010072231\n",
            "step: 180, loss: 0.0019887047819793224\n",
            "step: 190, loss: 0.1096482053399086\n",
            "step: 200, loss: 0.010726679116487503\n",
            "step: 210, loss: 0.005224545951932669\n",
            "step: 220, loss: 0.042011458426713943\n",
            "step: 230, loss: 0.0016551786102354527\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9642857142857144, f1=0.9497206703910615, best_f1=0.9573033707865168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013059388846158981\n",
            "step: 10, loss: 0.003180459374561906\n",
            "step: 20, loss: 0.07019439339637756\n",
            "step: 30, loss: 0.0003605192177928984\n",
            "step: 40, loss: 0.0024054355453699827\n",
            "step: 50, loss: 0.000447474216343835\n",
            "step: 60, loss: 0.0004714185488410294\n",
            "step: 70, loss: 0.0013633582275360823\n",
            "step: 80, loss: 0.0011405693367123604\n",
            "step: 90, loss: 0.0051711080595850945\n",
            "step: 100, loss: 0.04996149614453316\n",
            "step: 110, loss: 0.0008809567079879344\n",
            "step: 120, loss: 0.001086438074707985\n",
            "step: 130, loss: 0.0008517105598002672\n",
            "step: 140, loss: 0.00120898790191859\n",
            "step: 150, loss: 0.0052543580532073975\n",
            "step: 160, loss: 0.016695650294423103\n",
            "step: 170, loss: 0.00026127960882149637\n",
            "step: 180, loss: 0.008161196485161781\n",
            "step: 190, loss: 0.012583872303366661\n",
            "step: 200, loss: 0.0008764928788878024\n",
            "step: 210, loss: 0.00236326246522367\n",
            "step: 220, loss: 0.0003632647276390344\n",
            "step: 230, loss: 0.09339550882577896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9655937846836848, f1=0.9507829977628636, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043829042464494705\n",
            "step: 10, loss: 0.00019102550868410617\n",
            "step: 20, loss: 0.00039476921665482223\n",
            "step: 30, loss: 0.00031123351072892547\n",
            "step: 40, loss: 0.0002912667114287615\n",
            "step: 50, loss: 0.0004146495775785297\n",
            "step: 60, loss: 0.004121480043977499\n",
            "step: 70, loss: 0.01682838797569275\n",
            "step: 80, loss: 0.013075508177280426\n",
            "step: 90, loss: 0.00030519935535266995\n",
            "step: 100, loss: 0.0007524718530476093\n",
            "step: 110, loss: 0.013149912469089031\n",
            "step: 120, loss: 0.00017930469766724855\n",
            "step: 130, loss: 0.00018011065549217165\n",
            "step: 140, loss: 0.0011173433158546686\n",
            "step: 150, loss: 0.00034890740062110126\n",
            "step: 160, loss: 0.09747380018234253\n",
            "step: 170, loss: 0.0010547207202762365\n",
            "step: 180, loss: 0.005928706377744675\n",
            "step: 190, loss: 0.0002915715449489653\n",
            "step: 200, loss: 0.07945563644170761\n",
            "step: 210, loss: 0.00015720498049631715\n",
            "step: 220, loss: 0.026302343234419823\n",
            "step: 230, loss: 0.0010751902591437101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9608938547486034, f1=0.9516310461192351, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005383676616474986\n",
            "step: 10, loss: 0.0016022762283682823\n",
            "step: 20, loss: 0.008338849060237408\n",
            "step: 30, loss: 0.0007017123280093074\n",
            "step: 40, loss: 0.0037285704165697098\n",
            "step: 50, loss: 0.000808928394690156\n",
            "step: 60, loss: 0.000543877249583602\n",
            "step: 70, loss: 0.0009467563359066844\n",
            "step: 80, loss: 0.005893042776733637\n",
            "step: 90, loss: 0.00330904102884233\n",
            "step: 100, loss: 0.0030787871219217777\n",
            "step: 110, loss: 0.0021122093312442303\n",
            "step: 120, loss: 0.0006872161757200956\n",
            "step: 130, loss: 0.0014307152014225721\n",
            "step: 140, loss: 0.00032852301956154406\n",
            "step: 150, loss: 0.0008712007547728717\n",
            "step: 160, loss: 0.0011596172116696835\n",
            "step: 170, loss: 0.0008585843606851995\n",
            "step: 180, loss: 0.0015370770124718547\n",
            "step: 190, loss: 0.005286881234496832\n",
            "step: 200, loss: 0.002016456564888358\n",
            "step: 210, loss: 0.005433120299130678\n",
            "step: 220, loss: 0.001388011034578085\n",
            "step: 230, loss: 0.001915643224492669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9579067121729239, f1=0.9465301478953355, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013125576078891754\n",
            "step: 10, loss: 0.0022570763248950243\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.12611795961856842\n",
            "step: 30, loss: 0.0001017262038658373\n",
            "step: 40, loss: 0.09906216710805893\n",
            "step: 50, loss: 0.0001905754761537537\n",
            "step: 60, loss: 0.0009637639159336686\n",
            "step: 70, loss: 0.00012030071957269683\n",
            "step: 80, loss: 0.0009435968822799623\n",
            "step: 90, loss: 0.0004346877394709736\n",
            "step: 100, loss: 0.0009612567955628037\n",
            "step: 110, loss: 0.00018495690892450511\n",
            "step: 120, loss: 0.0005984918680042028\n",
            "step: 130, loss: 0.00018528325017541647\n",
            "step: 140, loss: 0.002005132846534252\n",
            "step: 150, loss: 0.00045001041144132614\n",
            "step: 160, loss: 0.0003233767638448626\n",
            "step: 170, loss: 0.004722686484456062\n",
            "step: 180, loss: 0.010366939939558506\n",
            "step: 190, loss: 0.00022880153846926987\n",
            "step: 200, loss: 0.00041190857882611454\n",
            "step: 210, loss: 0.0001778937003109604\n",
            "step: 220, loss: 0.00017195515101775527\n",
            "step: 230, loss: 0.0011826584814116359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9592760180995475, f1=0.9462857142857144, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016895937733352184\n",
            "step: 10, loss: 6.774830399081111e-05\n",
            "step: 20, loss: 0.0002072489442070946\n",
            "step: 30, loss: 0.00018164831271860749\n",
            "step: 40, loss: 0.000221898706513457\n",
            "step: 50, loss: 0.001403805916197598\n",
            "step: 60, loss: 0.0003839356650132686\n",
            "step: 70, loss: 0.0001507286069681868\n",
            "step: 80, loss: 0.00013572101306635886\n",
            "step: 90, loss: 0.0004344775516074151\n",
            "step: 100, loss: 0.0012167464010417461\n",
            "step: 110, loss: 0.0001861476484918967\n",
            "step: 120, loss: 0.0021654332522302866\n",
            "step: 130, loss: 0.00038520805537700653\n",
            "step: 140, loss: 0.02220771089196205\n",
            "step: 150, loss: 0.03626595437526703\n",
            "step: 160, loss: 0.00011996504326816648\n",
            "step: 170, loss: 9.906773630063981e-05\n",
            "step: 180, loss: 0.0003515086427796632\n",
            "step: 190, loss: 0.0013048367109149694\n",
            "step: 200, loss: 0.0002109608321916312\n",
            "step: 210, loss: 8.885315037332475e-05\n",
            "step: 220, loss: 0.00010373729310231283\n",
            "step: 230, loss: 0.0001487514964537695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9551569506726457, f1=0.9470124013528749, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008997551631182432\n",
            "step: 10, loss: 0.00035926737473346293\n",
            "step: 20, loss: 0.0005076415254734457\n",
            "step: 30, loss: 0.057978205382823944\n",
            "step: 40, loss: 0.011564713902771473\n",
            "step: 50, loss: 0.00022409451776184142\n",
            "step: 60, loss: 0.0005134175298735499\n",
            "step: 70, loss: 0.0010936959879472852\n",
            "step: 80, loss: 0.00015954130503814667\n",
            "step: 90, loss: 0.007041020784527063\n",
            "step: 100, loss: 0.0027490719221532345\n",
            "step: 110, loss: 0.0004403524217195809\n",
            "step: 120, loss: 0.0003657430352177471\n",
            "step: 130, loss: 0.0013385799247771502\n",
            "step: 140, loss: 0.0015335968928411603\n",
            "step: 150, loss: 0.009333007968962193\n",
            "step: 160, loss: 0.00010031298006651923\n",
            "step: 170, loss: 0.040801092982292175\n",
            "step: 180, loss: 0.0005783412489108741\n",
            "step: 190, loss: 0.00010318938439013436\n",
            "step: 200, loss: 0.000633508141618222\n",
            "step: 210, loss: 0.00022574169270228595\n",
            "step: 220, loss: 0.00017848171410150826\n",
            "step: 230, loss: 0.0001868109975475818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9569060773480662, f1=0.9494505494505495, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012360165128484368\n",
            "step: 10, loss: 0.0008704523788765073\n",
            "step: 20, loss: 0.000509253703057766\n",
            "step: 30, loss: 0.03255153074860573\n",
            "step: 40, loss: 0.0014362073270604014\n",
            "step: 50, loss: 0.0016826100181788206\n",
            "step: 60, loss: 0.008240296505391598\n",
            "step: 70, loss: 6.524587661260739e-05\n",
            "step: 80, loss: 0.0004216005327180028\n",
            "step: 90, loss: 0.00017924417625181377\n",
            "step: 100, loss: 9.493820834904909e-05\n",
            "step: 110, loss: 0.0001432339195162058\n",
            "step: 120, loss: 0.0001899939961731434\n",
            "step: 130, loss: 0.0028572892770171165\n",
            "step: 140, loss: 0.00013773306272923946\n",
            "step: 150, loss: 7.927157275844365e-05\n",
            "step: 160, loss: 0.0003539232711773366\n",
            "step: 170, loss: 0.0006881842855364084\n",
            "step: 180, loss: 0.0012693193275481462\n",
            "step: 190, loss: 0.00041675209649838507\n",
            "step: 200, loss: 0.00010213711357209831\n",
            "step: 210, loss: 0.00024406146258115768\n",
            "step: 220, loss: 0.0002876035578083247\n",
            "step: 230, loss: 0.006777593400329351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9581920903954803, f1=0.9479638009049774, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0052519566379487514\n",
            "step: 10, loss: 9.420516289537773e-05\n",
            "step: 20, loss: 0.0006558746099472046\n",
            "step: 30, loss: 0.00038855941966176033\n",
            "step: 40, loss: 0.00010164744162466377\n",
            "step: 50, loss: 0.00014517392264679074\n",
            "step: 60, loss: 0.00012402029824443161\n",
            "step: 70, loss: 6.030278018442914e-05\n",
            "step: 80, loss: 6.819751433795318e-05\n",
            "step: 90, loss: 0.005664260592311621\n",
            "step: 100, loss: 7.242141145979986e-05\n",
            "step: 110, loss: 0.0034507049713283777\n",
            "step: 120, loss: 0.09027349948883057\n",
            "step: 130, loss: 0.05925688147544861\n",
            "step: 140, loss: 0.00010787863720906898\n",
            "step: 150, loss: 0.00011929919855901971\n",
            "step: 160, loss: 0.00024531225790269673\n",
            "step: 170, loss: 7.670898048672825e-05\n",
            "step: 180, loss: 0.00013916095485910773\n",
            "step: 190, loss: 5.112447252031416e-05\n",
            "step: 200, loss: 7.823151827324182e-05\n",
            "step: 210, loss: 3.820009442279115e-05\n",
            "step: 220, loss: 6.165131344459951e-05\n",
            "step: 230, loss: 5.8166471717413515e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9574944071588367, f1=0.9503386004514672, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.444190072827041e-05\n",
            "step: 10, loss: 0.00014962385466787964\n",
            "step: 20, loss: 9.725490963319317e-05\n",
            "step: 30, loss: 8.537832763977349e-05\n",
            "step: 40, loss: 0.009719189256429672\n",
            "step: 50, loss: 0.00029854566673748195\n",
            "step: 60, loss: 0.00011826503032352775\n",
            "step: 70, loss: 0.00019748690829146653\n",
            "step: 80, loss: 0.00013454060535877943\n",
            "step: 90, loss: 0.0010749166831374168\n",
            "step: 100, loss: 0.00020366060198284686\n",
            "step: 110, loss: 0.00039010224281810224\n",
            "step: 120, loss: 2.5275150619563647e-05\n",
            "step: 130, loss: 0.00016836417489685118\n",
            "step: 140, loss: 0.00010208610910922289\n",
            "step: 150, loss: 5.426335701486096e-05\n",
            "step: 160, loss: 0.001442533452063799\n",
            "step: 170, loss: 2.34388280659914e-05\n",
            "step: 180, loss: 7.650200132047758e-05\n",
            "step: 190, loss: 0.00010371936514275149\n",
            "step: 200, loss: 0.00010249627666780725\n",
            "step: 210, loss: 0.000263432040810585\n",
            "step: 220, loss: 0.00015921200974844396\n",
            "step: 230, loss: 0.00013084286183584481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9597315436241611, f1=0.9496080627099663, best_f1=0.9507829977628636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.941076647606678e-05\n",
            "step: 10, loss: 4.596187500283122e-05\n",
            "step: 20, loss: 0.00020746963855344802\n",
            "step: 30, loss: 5.0708524213405326e-05\n",
            "step: 40, loss: 9.537526784697548e-05\n",
            "step: 50, loss: 0.026563363149762154\n",
            "step: 60, loss: 0.0003934327978640795\n",
            "step: 70, loss: 0.0001571676111780107\n",
            "step: 80, loss: 0.0002408359869150445\n",
            "step: 90, loss: 0.0001638206304050982\n",
            "step: 100, loss: 7.132275641197339e-05\n",
            "step: 110, loss: 6.078464866732247e-05\n",
            "step: 120, loss: 0.00011523339344421402\n",
            "step: 130, loss: 0.0003770582261495292\n",
            "step: 140, loss: 5.2371597121236846e-05\n",
            "step: 150, loss: 0.00012159966718172655\n",
            "step: 160, loss: 4.8025529395090416e-05\n",
            "step: 170, loss: 7.009552064118907e-05\n",
            "step: 180, loss: 0.0005350380088202655\n",
            "step: 190, loss: 0.0009472128585912287\n",
            "step: 200, loss: 0.0004242589930072427\n",
            "step: 210, loss: 6.420948193408549e-05\n",
            "step: 220, loss: 0.0006134888390079141\n",
            "step: 230, loss: 4.2257113818777725e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9599109131403119, f1=0.9511111111111112, best_f1=0.9507829977628636\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 158.53it/s]\n",
            "load_f1 = 0.9610678531701891\n",
            "real_f1 = 0.9599109131403119\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 179.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e21690-2836-4dc0-fd72-32f082fca768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6458385586738586\n",
            "step: 10, loss: 0.5338491797447205\n",
            "step: 20, loss: 0.5805912017822266\n",
            "step: 30, loss: 0.32320523262023926\n",
            "step: 40, loss: 0.2911936938762665\n",
            "step: 50, loss: 0.1420029103755951\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.23036623001098633\n",
            "step: 70, loss: 0.13080285489559174\n",
            "step: 80, loss: 0.20285218954086304\n",
            "step: 90, loss: 0.3509712219238281\n",
            "step: 100, loss: 0.24363499879837036\n",
            "step: 110, loss: 0.08518707752227783\n",
            "step: 120, loss: 0.13717660307884216\n",
            "step: 130, loss: 0.2524554133415222\n",
            "step: 140, loss: 0.11530061811208725\n",
            "step: 150, loss: 0.12420317530632019\n",
            "step: 160, loss: 0.11213447898626328\n",
            "step: 170, loss: 0.2630195617675781\n",
            "step: 180, loss: 0.03360070288181305\n",
            "step: 190, loss: 0.012237400747835636\n",
            "step: 200, loss: 0.15835241973400116\n",
            "step: 210, loss: 0.0640980526804924\n",
            "step: 220, loss: 0.11617438495159149\n",
            "step: 230, loss: 0.15422998368740082\n",
            "step: 240, loss: 0.13924555480480194\n",
            "step: 250, loss: 0.0680382028222084\n",
            "step: 260, loss: 0.030272820964455605\n",
            "step: 270, loss: 0.11121238768100739\n",
            "step: 280, loss: 0.13781870901584625\n",
            "step: 290, loss: 0.17780816555023193\n",
            "step: 300, loss: 0.06127922236919403\n",
            "step: 310, loss: 0.14937879145145416\n",
            "step: 320, loss: 0.09536803513765335\n",
            "step: 330, loss: 0.108514204621315\n",
            "step: 340, loss: 0.13143853843212128\n",
            "step: 350, loss: 0.06371571868658066\n",
            "step: 360, loss: 0.1413014531135559\n",
            "step: 370, loss: 0.13004370033740997\n",
            "step: 380, loss: 0.02654033713042736\n",
            "step: 390, loss: 0.22187069058418274\n",
            "step: 400, loss: 0.26112645864486694\n",
            "step: 410, loss: 0.07726050168275833\n",
            "step: 420, loss: 0.10355448722839355\n",
            "step: 430, loss: 0.1541513055562973\n",
            "step: 440, loss: 0.09551849216222763\n",
            "step: 450, loss: 0.014758805744349957\n",
            "step: 460, loss: 0.04987778887152672\n",
            "step: 470, loss: 0.0916495993733406\n",
            "step: 480, loss: 0.10287825018167496\n",
            "step: 490, loss: 0.13377881050109863\n",
            "step: 500, loss: 0.07660434395074844\n",
            "step: 510, loss: 0.09184859693050385\n",
            "step: 520, loss: 0.24766786396503448\n",
            "step: 530, loss: 0.013968085870146751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.906791569086651, f1=0.8881578947368421, best_f1=0.8881578947368421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12449858337640762\n",
            "step: 10, loss: 0.1057661920785904\n",
            "step: 20, loss: 0.07745780795812607\n",
            "step: 30, loss: 0.08267044275999069\n",
            "step: 40, loss: 0.09026886522769928\n",
            "step: 50, loss: 0.14782637357711792\n",
            "step: 60, loss: 0.022903235629200935\n",
            "step: 70, loss: 0.035274017602205276\n",
            "step: 80, loss: 0.10413920134305954\n",
            "step: 90, loss: 0.09359388053417206\n",
            "step: 100, loss: 0.10704532265663147\n",
            "step: 110, loss: 0.07330134510993958\n",
            "step: 120, loss: 0.04367583245038986\n",
            "step: 130, loss: 0.13500146567821503\n",
            "step: 140, loss: 0.08358009159564972\n",
            "step: 150, loss: 0.15675979852676392\n",
            "step: 160, loss: 0.029556291177868843\n",
            "step: 170, loss: 0.19172883033752441\n",
            "step: 180, loss: 0.02368447557091713\n",
            "step: 190, loss: 0.10886456072330475\n",
            "step: 200, loss: 0.03720122575759888\n",
            "step: 210, loss: 0.11750950664281845\n",
            "step: 220, loss: 0.057887837290763855\n",
            "step: 230, loss: 0.07031211256980896\n",
            "step: 240, loss: 0.09981245547533035\n",
            "step: 250, loss: 0.08514942973852158\n",
            "step: 260, loss: 0.029104411602020264\n",
            "step: 270, loss: 0.2700631618499756\n",
            "step: 280, loss: 0.16671490669250488\n",
            "step: 290, loss: 0.02721807174384594\n",
            "step: 300, loss: 0.18244080245494843\n",
            "step: 310, loss: 0.022900035604834557\n",
            "step: 320, loss: 0.17586617171764374\n",
            "step: 330, loss: 0.09788604080677032\n",
            "step: 340, loss: 0.050743404775857925\n",
            "step: 350, loss: 0.005403914954513311\n",
            "step: 360, loss: 0.19457566738128662\n",
            "step: 370, loss: 0.27179044485092163\n",
            "step: 380, loss: 0.1398356705904007\n",
            "step: 390, loss: 0.15574456751346588\n",
            "step: 400, loss: 0.028127169236540794\n",
            "step: 410, loss: 0.09113152325153351\n",
            "step: 420, loss: 0.04497024416923523\n",
            "step: 430, loss: 0.20488138496875763\n",
            "step: 440, loss: 0.08994040638208389\n",
            "step: 450, loss: 0.034059759229421616\n",
            "step: 460, loss: 0.13068531453609467\n",
            "step: 470, loss: 0.07804366201162338\n",
            "step: 480, loss: 0.16524578630924225\n",
            "step: 490, loss: 0.047712989151477814\n",
            "step: 500, loss: 0.2867357134819031\n",
            "step: 510, loss: 0.028550703078508377\n",
            "step: 520, loss: 0.13490645587444305\n",
            "step: 530, loss: 0.04069199413061142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9133211678832116, f1=0.9132629646626893, best_f1=0.9132629646626893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09132565557956696\n",
            "step: 10, loss: 0.07833033055067062\n",
            "step: 20, loss: 0.16270138323307037\n",
            "step: 30, loss: 0.30612432956695557\n",
            "step: 40, loss: 0.03282702714204788\n",
            "step: 50, loss: 0.12963137030601501\n",
            "step: 60, loss: 0.02937251515686512\n",
            "step: 70, loss: 0.05234447494149208\n",
            "step: 80, loss: 0.020953986793756485\n",
            "step: 90, loss: 0.03627201169729233\n",
            "step: 100, loss: 0.03244626149535179\n",
            "step: 110, loss: 0.0056075481697916985\n",
            "step: 120, loss: 0.044847991317510605\n",
            "step: 130, loss: 0.005671561695635319\n",
            "step: 140, loss: 0.008839139714837074\n",
            "step: 150, loss: 0.1020573154091835\n",
            "step: 160, loss: 0.04377405345439911\n",
            "step: 170, loss: 0.09695538878440857\n",
            "step: 180, loss: 0.06667955219745636\n",
            "step: 190, loss: 0.1380840390920639\n",
            "step: 200, loss: 0.02047593891620636\n",
            "step: 210, loss: 0.051554542034864426\n",
            "step: 220, loss: 0.09937074035406113\n",
            "step: 230, loss: 0.21525363624095917\n",
            "step: 240, loss: 0.1022290512919426\n",
            "step: 250, loss: 0.007971487008035183\n",
            "step: 260, loss: 0.047472160309553146\n",
            "step: 270, loss: 0.07286716252565384\n",
            "step: 280, loss: 0.16142994165420532\n",
            "step: 290, loss: 0.0134092066437006\n",
            "step: 300, loss: 0.07934559881687164\n",
            "step: 310, loss: 0.1464604288339615\n",
            "step: 320, loss: 0.05847702920436859\n",
            "step: 330, loss: 0.012587637640535831\n",
            "step: 340, loss: 0.014835579320788383\n",
            "step: 350, loss: 0.02242213673889637\n",
            "step: 360, loss: 0.06284986436367035\n",
            "step: 370, loss: 0.029063493013381958\n",
            "step: 380, loss: 0.008305469527840614\n",
            "step: 390, loss: 0.06261922419071198\n",
            "step: 400, loss: 0.043435513973236084\n",
            "step: 410, loss: 0.009080016054213047\n",
            "step: 420, loss: 0.12573584914207458\n",
            "step: 430, loss: 0.008249152451753616\n",
            "step: 440, loss: 0.10962120443582535\n",
            "step: 450, loss: 0.04847179353237152\n",
            "step: 460, loss: 0.10932448506355286\n",
            "step: 470, loss: 0.04171104356646538\n",
            "step: 480, loss: 0.15129530429840088\n",
            "step: 490, loss: 0.013435701839625835\n",
            "step: 500, loss: 0.14163537323474884\n",
            "step: 510, loss: 0.016634037718176842\n",
            "step: 520, loss: 0.14002768695354462\n",
            "step: 530, loss: 0.20981276035308838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.914577530176416, f1=0.9064409967089797, best_f1=0.9064409967089797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04798051714897156\n",
            "step: 10, loss: 0.014451922848820686\n",
            "step: 20, loss: 0.0037016139831393957\n",
            "step: 30, loss: 0.059354811906814575\n",
            "step: 40, loss: 0.05320942774415016\n",
            "step: 50, loss: 0.07061005383729935\n",
            "step: 60, loss: 0.10496469587087631\n",
            "step: 70, loss: 0.050061389803886414\n",
            "step: 80, loss: 0.006152356509119272\n",
            "step: 90, loss: 0.14257945120334625\n",
            "step: 100, loss: 0.0018942009191960096\n",
            "step: 110, loss: 0.2046266496181488\n",
            "step: 120, loss: 0.0017642317106947303\n",
            "step: 130, loss: 0.0016201342223212123\n",
            "step: 140, loss: 0.03022405132651329\n",
            "step: 150, loss: 0.06149066239595413\n",
            "step: 160, loss: 0.03461673855781555\n",
            "step: 170, loss: 0.013833894394338131\n",
            "step: 180, loss: 0.028697310015559196\n",
            "step: 190, loss: 0.04491313919425011\n",
            "step: 200, loss: 0.028800198808312416\n",
            "step: 210, loss: 0.04692227765917778\n",
            "step: 220, loss: 0.0065596760250627995\n",
            "step: 230, loss: 0.044121332466602325\n",
            "step: 240, loss: 0.007828169502317905\n",
            "step: 250, loss: 0.11717624962329865\n",
            "step: 260, loss: 0.006144512910395861\n",
            "step: 270, loss: 0.04896538704633713\n",
            "step: 280, loss: 0.16461993753910065\n",
            "step: 290, loss: 0.02226891927421093\n",
            "step: 300, loss: 0.005601199809461832\n",
            "step: 310, loss: 0.007879525423049927\n",
            "step: 320, loss: 0.004625466652214527\n",
            "step: 330, loss: 0.027457047253847122\n",
            "step: 340, loss: 0.13294605910778046\n",
            "step: 350, loss: 0.039502374827861786\n",
            "step: 360, loss: 0.08144433796405792\n",
            "step: 370, loss: 0.008571477606892586\n",
            "step: 380, loss: 0.003808780573308468\n",
            "step: 390, loss: 0.006867063231766224\n",
            "step: 400, loss: 0.018401991575956345\n",
            "step: 410, loss: 0.00791771337389946\n",
            "step: 420, loss: 0.010524297133088112\n",
            "step: 430, loss: 0.05550113692879677\n",
            "step: 440, loss: 0.006567536387592554\n",
            "step: 450, loss: 0.032040953636169434\n",
            "step: 460, loss: 0.011032812297344208\n",
            "step: 470, loss: 0.034533146768808365\n",
            "step: 480, loss: 0.08920101821422577\n",
            "step: 490, loss: 0.0013748033670708537\n",
            "step: 500, loss: 0.019132595509290695\n",
            "step: 510, loss: 0.05825566500425339\n",
            "step: 520, loss: 0.03849392756819725\n",
            "step: 530, loss: 0.03156854957342148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9124767225325885, f1=0.9059429106223678, best_f1=0.9064409967089797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029038092121481895\n",
            "step: 10, loss: 0.06101314350962639\n",
            "step: 20, loss: 0.0025150151923298836\n",
            "step: 30, loss: 0.002837030915543437\n",
            "step: 40, loss: 0.02065243199467659\n",
            "step: 50, loss: 0.058233052492141724\n",
            "step: 60, loss: 0.09751803427934647\n",
            "step: 70, loss: 0.045642051845788956\n",
            "step: 80, loss: 0.002021005842834711\n",
            "step: 90, loss: 0.005949602462351322\n",
            "step: 100, loss: 0.007328774780035019\n",
            "step: 110, loss: 0.0011364702368155122\n",
            "step: 120, loss: 0.0012851146748289466\n",
            "step: 130, loss: 0.002462235512211919\n",
            "step: 140, loss: 0.002770782681182027\n",
            "step: 150, loss: 0.01205619890242815\n",
            "step: 160, loss: 0.0008937412058003247\n",
            "step: 170, loss: 0.009866036474704742\n",
            "step: 180, loss: 0.002718702657148242\n",
            "step: 190, loss: 0.002024558838456869\n",
            "step: 200, loss: 0.008248649537563324\n",
            "step: 210, loss: 0.19636496901512146\n",
            "step: 220, loss: 0.0849122628569603\n",
            "step: 230, loss: 0.05562504380941391\n",
            "step: 240, loss: 0.0086777089163661\n",
            "step: 250, loss: 0.07553191483020782\n",
            "step: 260, loss: 0.0011847586138173938\n",
            "step: 270, loss: 0.0011515025980770588\n",
            "step: 280, loss: 0.010508504696190357\n",
            "step: 290, loss: 0.17371374368667603\n",
            "step: 300, loss: 0.008258906193077564\n",
            "step: 310, loss: 0.004787803627550602\n",
            "step: 320, loss: 0.16721034049987793\n",
            "step: 330, loss: 0.03288843482732773\n",
            "step: 340, loss: 0.006091021001338959\n",
            "step: 350, loss: 0.005629411898553371\n",
            "step: 360, loss: 0.01726541481912136\n",
            "step: 370, loss: 0.014443854801356792\n",
            "step: 380, loss: 0.0044230083003640175\n",
            "step: 390, loss: 0.0014424691908061504\n",
            "step: 400, loss: 0.1290895938873291\n",
            "step: 410, loss: 0.020754659548401833\n",
            "step: 420, loss: 0.0023669307120144367\n",
            "step: 430, loss: 0.08661311864852905\n",
            "step: 440, loss: 0.22665369510650635\n",
            "step: 450, loss: 0.026165412738919258\n",
            "step: 460, loss: 0.06595437228679657\n",
            "step: 470, loss: 0.002959584351629019\n",
            "step: 480, loss: 0.007976804859936237\n",
            "step: 490, loss: 0.0017417063936591148\n",
            "step: 500, loss: 0.024164168164134026\n",
            "step: 510, loss: 0.0472676120698452\n",
            "step: 520, loss: 0.0012829243205487728\n",
            "step: 530, loss: 0.1616525501012802\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9080622347949081, f1=0.9013685700802266, best_f1=0.9064409967089797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006041083950549364\n",
            "step: 10, loss: 0.004495135974138975\n",
            "step: 20, loss: 0.12814758718013763\n",
            "step: 30, loss: 0.016843143850564957\n",
            "step: 40, loss: 0.057601917535066605\n",
            "step: 50, loss: 0.0017118655377998948\n",
            "step: 60, loss: 0.014007806777954102\n",
            "step: 70, loss: 0.005854525603353977\n",
            "step: 80, loss: 0.08436269313097\n",
            "step: 90, loss: 0.004862921312451363\n",
            "step: 100, loss: 0.0011310018599033356\n",
            "step: 110, loss: 0.012077059596776962\n",
            "step: 120, loss: 0.05601564794778824\n",
            "step: 130, loss: 0.006117447279393673\n",
            "step: 140, loss: 0.0029290567617863417\n",
            "step: 150, loss: 0.0038010869175195694\n",
            "step: 160, loss: 0.004460807424038649\n",
            "step: 170, loss: 0.011853020638227463\n",
            "step: 180, loss: 0.014020736329257488\n",
            "step: 190, loss: 0.05828911438584328\n",
            "step: 200, loss: 0.0022132531739771366\n",
            "step: 210, loss: 0.012520567514002323\n",
            "step: 220, loss: 0.0010864256182685494\n",
            "step: 230, loss: 0.00374305690638721\n",
            "step: 240, loss: 0.009433148428797722\n",
            "step: 250, loss: 0.01307608000934124\n",
            "step: 260, loss: 0.02349051646888256\n",
            "step: 270, loss: 0.025757353752851486\n",
            "step: 280, loss: 0.002936879638582468\n",
            "step: 290, loss: 0.0005743193905800581\n",
            "step: 300, loss: 0.0006558529566973448\n",
            "step: 310, loss: 0.015501422807574272\n",
            "step: 320, loss: 0.002446679165586829\n",
            "step: 330, loss: 0.06813430786132812\n",
            "step: 340, loss: 0.09142213314771652\n",
            "step: 350, loss: 0.04491430148482323\n",
            "step: 360, loss: 0.011356320232152939\n",
            "step: 370, loss: 0.006584004499018192\n",
            "step: 380, loss: 0.0006219608476385474\n",
            "step: 390, loss: 0.02113204076886177\n",
            "step: 400, loss: 0.029227830469608307\n",
            "step: 410, loss: 0.004209303297102451\n",
            "step: 420, loss: 0.08887719362974167\n",
            "step: 430, loss: 0.0024699026253074408\n",
            "step: 440, loss: 0.00033088645432144403\n",
            "step: 450, loss: 0.0018950414378196\n",
            "step: 460, loss: 0.002837776206433773\n",
            "step: 470, loss: 0.004716121591627598\n",
            "step: 480, loss: 0.015370012260973454\n",
            "step: 490, loss: 0.009916224516928196\n",
            "step: 500, loss: 0.015606631524860859\n",
            "step: 510, loss: 0.07715202122926712\n",
            "step: 520, loss: 0.05319838970899582\n",
            "step: 530, loss: 0.004160596057772636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9006993006993006, f1=0.8928901200369345, best_f1=0.9064409967089797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004805175121873617\n",
            "step: 10, loss: 0.0013121768133714795\n",
            "step: 20, loss: 0.008428615517914295\n",
            "step: 30, loss: 0.04876039922237396\n",
            "step: 40, loss: 0.002440560609102249\n",
            "step: 50, loss: 0.1952071189880371\n",
            "step: 60, loss: 0.023958489298820496\n",
            "step: 70, loss: 0.009345454163849354\n",
            "step: 80, loss: 0.007519339676946402\n",
            "step: 90, loss: 0.020557915791869164\n",
            "step: 100, loss: 0.0004443157813511789\n",
            "step: 110, loss: 0.003402275964617729\n",
            "step: 120, loss: 0.049967944622039795\n",
            "step: 130, loss: 0.08997242152690887\n",
            "step: 140, loss: 0.005978003144264221\n",
            "step: 150, loss: 0.0009402782307006419\n",
            "step: 160, loss: 0.0005894734640605748\n",
            "step: 170, loss: 0.0033343906980007887\n",
            "step: 180, loss: 0.00031587283592671156\n",
            "step: 190, loss: 0.0017458497313782573\n",
            "step: 200, loss: 0.0031008499208837748\n",
            "step: 210, loss: 0.008258079178631306\n",
            "step: 220, loss: 0.0010296928230673075\n",
            "step: 230, loss: 0.052745599299669266\n",
            "step: 240, loss: 0.028642408549785614\n",
            "step: 250, loss: 0.00344344531185925\n",
            "step: 260, loss: 0.0008169428328983486\n",
            "step: 270, loss: 0.00033440583501942456\n",
            "step: 280, loss: 0.0030733332969248295\n",
            "step: 290, loss: 0.004029001109302044\n",
            "step: 300, loss: 0.00473259761929512\n",
            "step: 310, loss: 0.003790108487010002\n",
            "step: 320, loss: 0.0016910959966480732\n",
            "step: 330, loss: 0.017002491280436516\n",
            "step: 340, loss: 0.04070466756820679\n",
            "step: 350, loss: 0.01998104713857174\n",
            "step: 360, loss: 0.0015512867830693722\n",
            "step: 370, loss: 0.0014048166340216994\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 380, loss: 0.0031630361918359995\n",
            "step: 390, loss: 0.00032040252699516714\n",
            "step: 400, loss: 0.017226388677954674\n",
            "step: 410, loss: 0.019377872347831726\n",
            "step: 420, loss: 0.0051532406359910965\n",
            "step: 430, loss: 0.005028167739510536\n",
            "step: 440, loss: 0.035996437072753906\n",
            "step: 450, loss: 0.0015243447851389647\n",
            "step: 460, loss: 0.00039459875551983714\n",
            "step: 470, loss: 0.016120711341500282\n",
            "step: 480, loss: 0.06319751590490341\n",
            "step: 490, loss: 0.017772266641259193\n",
            "step: 500, loss: 0.0024981407914310694\n",
            "step: 510, loss: 0.04945256561040878\n",
            "step: 520, loss: 0.02072226069867611\n",
            "step: 530, loss: 0.04202684387564659\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9138090824837812, f1=0.9085027726432533, best_f1=0.9064409967089797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023090558126568794\n",
            "step: 10, loss: 0.006934137549251318\n",
            "step: 20, loss: 0.00414546113461256\n",
            "step: 30, loss: 0.013105612248182297\n",
            "step: 40, loss: 0.0019846856594085693\n",
            "step: 50, loss: 0.01465801801532507\n",
            "step: 60, loss: 0.0026403784286230803\n",
            "step: 70, loss: 0.005605196580290794\n",
            "step: 80, loss: 0.003953332081437111\n",
            "step: 90, loss: 0.0011110022896900773\n",
            "step: 100, loss: 0.009832869283854961\n",
            "step: 110, loss: 0.003015324240550399\n",
            "step: 120, loss: 0.0052505554631352425\n",
            "step: 130, loss: 0.0003380505077075213\n",
            "step: 140, loss: 0.08826810121536255\n",
            "step: 150, loss: 0.0033203568309545517\n",
            "step: 160, loss: 0.012080434709787369\n",
            "step: 170, loss: 0.11149918287992477\n",
            "step: 180, loss: 0.00497865118086338\n",
            "step: 190, loss: 0.0037183475214987993\n",
            "step: 200, loss: 0.0024744493421167135\n",
            "step: 210, loss: 0.018024714663624763\n",
            "step: 220, loss: 0.0027993100229650736\n",
            "step: 230, loss: 0.006868044380098581\n",
            "step: 240, loss: 0.0016383016481995583\n",
            "step: 250, loss: 0.009339312091469765\n",
            "step: 260, loss: 0.0007106643752194941\n",
            "step: 270, loss: 0.0015827217139303684\n",
            "step: 280, loss: 0.009982790797948837\n",
            "step: 290, loss: 0.0003836791729554534\n",
            "step: 300, loss: 0.040052466094493866\n",
            "step: 310, loss: 0.023279013112187386\n",
            "step: 320, loss: 0.0013187223812565207\n",
            "step: 330, loss: 0.0027119156438857317\n",
            "step: 340, loss: 0.028465943410992622\n",
            "step: 350, loss: 0.0022646707948297262\n",
            "step: 360, loss: 0.009953036904335022\n",
            "step: 370, loss: 0.0007646558806300163\n",
            "step: 380, loss: 0.0025919212494045496\n",
            "step: 390, loss: 0.0012604141375049949\n",
            "step: 400, loss: 0.0012521882308647037\n",
            "step: 410, loss: 0.0002636425488162786\n",
            "step: 420, loss: 0.016754431650042534\n",
            "step: 430, loss: 0.0317801758646965\n",
            "step: 440, loss: 0.02518703043460846\n",
            "step: 450, loss: 0.0029770899564027786\n",
            "step: 460, loss: 0.0006530483369715512\n",
            "step: 470, loss: 0.0005849734297953546\n",
            "step: 480, loss: 0.0001486189430579543\n",
            "step: 490, loss: 0.04242865741252899\n",
            "step: 500, loss: 0.06720563769340515\n",
            "step: 510, loss: 0.007988535799086094\n",
            "step: 520, loss: 0.010032502934336662\n",
            "step: 530, loss: 0.0006304431008175015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9155389640690621, f1=0.9034965034965035, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019205452408641577\n",
            "step: 10, loss: 0.0020321847405284643\n",
            "step: 20, loss: 0.010260287672281265\n",
            "step: 30, loss: 0.01562872715294361\n",
            "step: 40, loss: 0.0015924533363431692\n",
            "step: 50, loss: 0.0016583132091909647\n",
            "step: 60, loss: 0.0005927662132307887\n",
            "step: 70, loss: 0.004344901069998741\n",
            "step: 80, loss: 0.06516478955745697\n",
            "step: 90, loss: 5.847239663125947e-05\n",
            "step: 100, loss: 0.000995595008134842\n",
            "step: 110, loss: 0.00038328184746205807\n",
            "step: 120, loss: 0.018082158640027046\n",
            "step: 130, loss: 0.007334609981626272\n",
            "step: 140, loss: 0.0005316672031767666\n",
            "step: 150, loss: 0.005117754451930523\n",
            "step: 160, loss: 0.00045882147969678044\n",
            "step: 170, loss: 0.05364629998803139\n",
            "step: 180, loss: 0.0024617535527795553\n",
            "step: 190, loss: 0.007009307388216257\n",
            "step: 200, loss: 0.0017369546694681048\n",
            "step: 210, loss: 0.001283105812035501\n",
            "step: 220, loss: 0.005963183008134365\n",
            "step: 230, loss: 0.0025895913131535053\n",
            "step: 240, loss: 0.00023526183213107288\n",
            "step: 250, loss: 0.0007805557106621563\n",
            "step: 260, loss: 0.022066479548811913\n",
            "step: 270, loss: 0.0002646348439157009\n",
            "step: 280, loss: 0.004439723677933216\n",
            "step: 290, loss: 0.054826125502586365\n",
            "step: 300, loss: 0.0006471697124652565\n",
            "step: 310, loss: 0.014956002123653889\n",
            "step: 320, loss: 0.004209577571600676\n",
            "step: 330, loss: 0.0009889568900689483\n",
            "step: 340, loss: 0.0018904133467003703\n",
            "step: 350, loss: 0.0009344038553535938\n",
            "step: 360, loss: 8.798010821919888e-05\n",
            "step: 370, loss: 0.001963237766176462\n",
            "step: 380, loss: 0.0005121475551277399\n",
            "step: 390, loss: 0.00040684948908165097\n",
            "step: 400, loss: 0.17187553644180298\n",
            "step: 410, loss: 0.034193072468042374\n",
            "step: 420, loss: 0.013735871762037277\n",
            "step: 430, loss: 0.006574103608727455\n",
            "step: 440, loss: 0.0039932397194206715\n",
            "step: 450, loss: 0.0071909986436367035\n",
            "step: 460, loss: 0.0005043564597144723\n",
            "step: 470, loss: 0.00022647310106549412\n",
            "step: 480, loss: 0.00013925525126978755\n",
            "step: 490, loss: 0.010424632579088211\n",
            "step: 500, loss: 0.26576241850852966\n",
            "step: 510, loss: 0.1657240390777588\n",
            "step: 520, loss: 0.11350519955158234\n",
            "step: 530, loss: 0.012460428290069103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9147647880763857, f1=0.9067164179104478, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008332473807968199\n",
            "step: 10, loss: 0.000949582492467016\n",
            "step: 20, loss: 0.00017324801592621952\n",
            "step: 30, loss: 0.00023791154671926051\n",
            "step: 40, loss: 0.00027849647449329495\n",
            "step: 50, loss: 0.006483228411525488\n",
            "step: 60, loss: 0.00035917063360102475\n",
            "step: 70, loss: 0.0002398401265963912\n",
            "step: 80, loss: 9.695608605397865e-05\n",
            "step: 90, loss: 0.00027495561516843736\n",
            "step: 100, loss: 6.662191299255937e-05\n",
            "step: 110, loss: 7.084740354912356e-05\n",
            "step: 120, loss: 0.0024430672638118267\n",
            "step: 130, loss: 0.0026684908661991358\n",
            "step: 140, loss: 0.007404182571917772\n",
            "step: 150, loss: 0.0003897899005096406\n",
            "step: 160, loss: 0.0013074291637167335\n",
            "step: 170, loss: 0.0007537979981862009\n",
            "step: 180, loss: 0.03394049033522606\n",
            "step: 190, loss: 0.01076553575694561\n",
            "step: 200, loss: 0.0017264883499592543\n",
            "step: 210, loss: 0.0021107876673340797\n",
            "step: 220, loss: 0.0028354593086987734\n",
            "step: 230, loss: 0.006908917799592018\n",
            "step: 240, loss: 0.0002766929392237216\n",
            "step: 250, loss: 6.590421980945393e-05\n",
            "step: 260, loss: 0.00579454330727458\n",
            "step: 270, loss: 0.017694950103759766\n",
            "step: 280, loss: 0.0010285940952599049\n",
            "step: 290, loss: 0.0006640144856646657\n",
            "step: 300, loss: 3.3794967748690397e-05\n",
            "step: 310, loss: 9.178930486086756e-05\n",
            "step: 320, loss: 0.00032341352198272943\n",
            "step: 330, loss: 0.0007276355754584074\n",
            "step: 340, loss: 0.055937591940164566\n",
            "step: 350, loss: 0.016179069876670837\n",
            "step: 360, loss: 0.05812479183077812\n",
            "step: 370, loss: 0.0009474153048358858\n",
            "step: 380, loss: 0.02332283742725849\n",
            "step: 390, loss: 0.0018668259726837277\n",
            "step: 400, loss: 0.002711877692490816\n",
            "step: 410, loss: 0.004941301885992289\n",
            "step: 420, loss: 0.00019592768512666225\n",
            "step: 430, loss: 9.603925718693063e-05\n",
            "step: 440, loss: 0.011901958845555782\n",
            "step: 450, loss: 0.0013095731846988201\n",
            "step: 460, loss: 0.005328099709004164\n",
            "step: 470, loss: 0.00022757667466066778\n",
            "step: 480, loss: 0.031927645206451416\n",
            "step: 490, loss: 0.003922203090041876\n",
            "step: 500, loss: 0.04657768830657005\n",
            "step: 510, loss: 0.0007117468630895019\n",
            "step: 520, loss: 0.05608082562685013\n",
            "step: 530, loss: 0.0002200812887167558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9102623101702715, f1=0.9018518518518519, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00035832656431011856\n",
            "step: 10, loss: 0.0008682691841386259\n",
            "step: 20, loss: 0.023672016337513924\n",
            "step: 30, loss: 0.0003403203736525029\n",
            "step: 40, loss: 0.0007406921358779073\n",
            "step: 50, loss: 0.024525988847017288\n",
            "step: 60, loss: 0.00022608730068895966\n",
            "step: 70, loss: 0.0010124711552634835\n",
            "step: 80, loss: 0.0004696745309047401\n",
            "step: 90, loss: 0.00010468398249940947\n",
            "step: 100, loss: 0.000695700931828469\n",
            "step: 110, loss: 0.002654790412634611\n",
            "step: 120, loss: 0.00032251118682324886\n",
            "step: 130, loss: 0.02314835600554943\n",
            "step: 140, loss: 0.008625196292996407\n",
            "step: 150, loss: 0.00038234301609918475\n",
            "step: 160, loss: 0.002233789535239339\n",
            "step: 170, loss: 0.0021081336308270693\n",
            "step: 180, loss: 0.0007377789006568491\n",
            "step: 190, loss: 0.0005823103711009026\n",
            "step: 200, loss: 0.053749363869428635\n",
            "step: 210, loss: 0.03662165254354477\n",
            "step: 220, loss: 0.00037990909186191857\n",
            "step: 230, loss: 0.0004265836614649743\n",
            "step: 240, loss: 0.0004072236188221723\n",
            "step: 250, loss: 0.00013269938062876463\n",
            "step: 260, loss: 0.0004313043027650565\n",
            "step: 270, loss: 0.0007613276829943061\n",
            "step: 280, loss: 0.00036897964309901\n",
            "step: 290, loss: 0.00017694069538265467\n",
            "step: 300, loss: 0.00042575254337862134\n",
            "step: 310, loss: 0.0006582794594578445\n",
            "step: 320, loss: 0.002688029548153281\n",
            "step: 330, loss: 0.0015739199006929994\n",
            "step: 340, loss: 0.0030872002243995667\n",
            "step: 350, loss: 0.0001273232337553054\n",
            "step: 360, loss: 0.00032748345984146\n",
            "step: 370, loss: 0.0039435564540326595\n",
            "step: 380, loss: 0.00011361181532265618\n",
            "step: 390, loss: 0.0006013691890984774\n",
            "step: 400, loss: 0.00017383597150910646\n",
            "step: 410, loss: 0.000131098015117459\n",
            "step: 420, loss: 0.0005274260765872896\n",
            "step: 430, loss: 0.00013351930829230696\n",
            "step: 440, loss: 0.003929690923541784\n",
            "step: 450, loss: 0.00021937601559329778\n",
            "step: 460, loss: 0.010074689984321594\n",
            "step: 470, loss: 7.007303065620363e-05\n",
            "step: 480, loss: 0.009461279958486557\n",
            "step: 490, loss: 0.000247608550125733\n",
            "step: 500, loss: 0.008420647121965885\n",
            "step: 510, loss: 0.0013716586399823427\n",
            "step: 520, loss: 0.00011602242011576891\n",
            "step: 530, loss: 0.00018200530030298978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9105312208760485, f1=0.8967772069126577, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.97460849955678e-05\n",
            "step: 10, loss: 8.14693485153839e-05\n",
            "step: 20, loss: 0.052774522453546524\n",
            "step: 30, loss: 0.0032493053004145622\n",
            "step: 40, loss: 0.0015448906924575567\n",
            "step: 50, loss: 0.02280329540371895\n",
            "step: 60, loss: 0.001460028812289238\n",
            "step: 70, loss: 0.001903186785057187\n",
            "step: 80, loss: 0.0024458379484713078\n",
            "step: 90, loss: 0.0007741920999251306\n",
            "step: 100, loss: 0.000828603224363178\n",
            "step: 110, loss: 0.00036368417204357684\n",
            "step: 120, loss: 0.00034234108170494437\n",
            "step: 130, loss: 0.00021369129535742104\n",
            "step: 140, loss: 0.06661307066679001\n",
            "step: 150, loss: 0.00021022545115556568\n",
            "step: 160, loss: 7.546530832769349e-05\n",
            "step: 170, loss: 0.0001614147622603923\n",
            "step: 180, loss: 0.0004933848977088928\n",
            "step: 190, loss: 0.0010242339922115207\n",
            "step: 200, loss: 0.00029796166927553713\n",
            "step: 210, loss: 0.0002919115941040218\n",
            "step: 220, loss: 0.00017078670498449355\n",
            "step: 230, loss: 7.642123819096014e-05\n",
            "step: 240, loss: 0.0010845898650586605\n",
            "step: 250, loss: 0.0008216479327529669\n",
            "step: 260, loss: 0.00018258762429468334\n",
            "step: 270, loss: 0.003268919885158539\n",
            "step: 280, loss: 0.006662317086011171\n",
            "step: 290, loss: 0.004936544690281153\n",
            "step: 300, loss: 0.043091997504234314\n",
            "step: 310, loss: 0.0002650014066603035\n",
            "step: 320, loss: 7.679367263335735e-05\n",
            "step: 330, loss: 0.00021815770014654845\n",
            "step: 340, loss: 0.0001304163597524166\n",
            "step: 350, loss: 0.0010462601203471422\n",
            "step: 360, loss: 0.005229260306805372\n",
            "step: 370, loss: 0.00045377176138572395\n",
            "step: 380, loss: 0.00014933405327610672\n",
            "step: 390, loss: 0.06566817313432693\n",
            "step: 400, loss: 0.00027821699040941894\n",
            "step: 410, loss: 0.007096480578184128\n",
            "step: 420, loss: 0.013733072206377983\n",
            "step: 430, loss: 0.0036107178311794996\n",
            "step: 440, loss: 0.04889993369579315\n",
            "step: 450, loss: 0.00023194460663944483\n",
            "step: 460, loss: 0.00033138005528599024\n",
            "step: 470, loss: 3.678564462461509e-05\n",
            "step: 480, loss: 0.002382115926593542\n",
            "step: 490, loss: 0.0021021494176238775\n",
            "step: 500, loss: 0.0009986197110265493\n",
            "step: 510, loss: 0.0013678919058293104\n",
            "step: 520, loss: 4.2214396671624854e-05\n",
            "step: 530, loss: 0.015458393841981888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9145220588235294, f1=0.9015639374425022, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00078775075962767\n",
            "step: 10, loss: 0.0026306549552828074\n",
            "step: 20, loss: 0.00306111597456038\n",
            "step: 30, loss: 0.0007773960824124515\n",
            "step: 40, loss: 7.130208541639149e-05\n",
            "step: 50, loss: 0.007943719625473022\n",
            "step: 60, loss: 0.0005996141117066145\n",
            "step: 70, loss: 0.00014908387674950063\n",
            "step: 80, loss: 3.455837577348575e-05\n",
            "step: 90, loss: 0.00020227496861480176\n",
            "step: 100, loss: 0.0014098206302151084\n",
            "step: 110, loss: 0.20001405477523804\n",
            "step: 120, loss: 0.0345681793987751\n",
            "step: 130, loss: 0.015327511355280876\n",
            "step: 140, loss: 0.00014639433356933296\n",
            "step: 150, loss: 0.0018822327256202698\n",
            "step: 160, loss: 2.3770631742081605e-05\n",
            "step: 170, loss: 0.00010543897951720282\n",
            "step: 180, loss: 0.0001734977267915383\n",
            "step: 190, loss: 0.001292289001867175\n",
            "step: 200, loss: 0.00016623119881842285\n",
            "step: 210, loss: 0.00014317623572424054\n",
            "step: 220, loss: 0.0017074085772037506\n",
            "step: 230, loss: 0.0004558066721074283\n",
            "step: 240, loss: 0.0012097125872969627\n",
            "step: 250, loss: 0.0001414008584106341\n",
            "step: 260, loss: 0.00018848455511033535\n",
            "step: 270, loss: 0.00010354626283515245\n",
            "step: 280, loss: 0.0001956548949237913\n",
            "step: 290, loss: 0.0031006946228444576\n",
            "step: 300, loss: 0.00010127530549652874\n",
            "step: 310, loss: 0.0010271266801282763\n",
            "step: 320, loss: 0.02574244514107704\n",
            "step: 330, loss: 0.00030423171119764447\n",
            "step: 340, loss: 0.00024359379312954843\n",
            "step: 350, loss: 0.00034441036405041814\n",
            "step: 360, loss: 9.818997205002233e-05\n",
            "step: 370, loss: 0.00031017803121358156\n",
            "step: 380, loss: 0.0034730995539575815\n",
            "step: 390, loss: 0.0030654179863631725\n",
            "step: 400, loss: 0.0006015911931172013\n",
            "step: 410, loss: 0.0005170984077267349\n",
            "step: 420, loss: 9.571530245011672e-05\n",
            "step: 430, loss: 0.14264874160289764\n",
            "step: 440, loss: 0.0006662991363555193\n",
            "step: 450, loss: 0.00028600639780052006\n",
            "step: 460, loss: 0.008868738077580929\n",
            "step: 470, loss: 0.004190518520772457\n",
            "step: 480, loss: 0.0003587030805647373\n",
            "step: 490, loss: 0.012986204586923122\n",
            "step: 500, loss: 6.129020766820759e-05\n",
            "step: 510, loss: 0.003878260962665081\n",
            "step: 520, loss: 0.0048503936268389225\n",
            "step: 530, loss: 4.4188629544805735e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9115582592419279, f1=0.896519285042333, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005876236828044057\n",
            "step: 10, loss: 3.320588803035207e-05\n",
            "step: 20, loss: 0.0007089218124747276\n",
            "step: 30, loss: 4.2092920921277255e-05\n",
            "step: 40, loss: 0.0006795040098950267\n",
            "step: 50, loss: 0.07961481809616089\n",
            "step: 60, loss: 7.480999920517206e-05\n",
            "step: 70, loss: 0.0026466669514775276\n",
            "step: 80, loss: 4.2083102016476914e-05\n",
            "step: 90, loss: 0.00017675297567620873\n",
            "step: 100, loss: 0.01120050810277462\n",
            "step: 110, loss: 0.0001446077658329159\n",
            "step: 120, loss: 0.004434784408658743\n",
            "step: 130, loss: 0.00191485695540905\n",
            "step: 140, loss: 0.00011395322508178651\n",
            "step: 150, loss: 6.019444117555395e-05\n",
            "step: 160, loss: 3.8293721445370466e-05\n",
            "step: 170, loss: 9.981430048355833e-05\n",
            "step: 180, loss: 0.0012019608402624726\n",
            "step: 190, loss: 0.00017635671247262508\n",
            "step: 200, loss: 0.00013329244393389672\n",
            "step: 210, loss: 0.0005249240784905851\n",
            "step: 220, loss: 0.0005575857940129936\n",
            "step: 230, loss: 0.028785547241568565\n",
            "step: 240, loss: 0.00012455822434276342\n",
            "step: 250, loss: 0.0029388079419732094\n",
            "step: 260, loss: 0.0004998167860321701\n",
            "step: 270, loss: 0.0019084591185674071\n",
            "step: 280, loss: 0.000456327194115147\n",
            "step: 290, loss: 0.0020066658034920692\n",
            "step: 300, loss: 0.00023746326041873544\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 310, loss: 0.04258139431476593\n",
            "step: 320, loss: 7.004759390838444e-05\n",
            "step: 330, loss: 0.005075342021882534\n",
            "step: 340, loss: 0.0005406843265518546\n",
            "step: 350, loss: 0.00033930130302906036\n",
            "step: 360, loss: 0.00026086982688866556\n",
            "step: 370, loss: 9.945653437171131e-05\n",
            "step: 380, loss: 0.0010539772920310497\n",
            "step: 390, loss: 0.010888975113630295\n",
            "step: 400, loss: 9.556891745887697e-05\n",
            "step: 410, loss: 0.0061929416842758656\n",
            "step: 420, loss: 0.011001824401319027\n",
            "step: 430, loss: 0.00020068195590283722\n",
            "step: 440, loss: 0.0025521935895085335\n",
            "step: 450, loss: 0.0024825904984027147\n",
            "step: 460, loss: 0.0003233302559237927\n",
            "step: 470, loss: 0.000183278345502913\n",
            "step: 480, loss: 0.0002579485299065709\n",
            "step: 490, loss: 6.389322516042739e-05\n",
            "step: 500, loss: 6.935115379747003e-05\n",
            "step: 510, loss: 0.00025663842097856104\n",
            "step: 520, loss: 0.0002396057971054688\n",
            "step: 530, loss: 6.344404391711578e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.912882298424467, f1=0.8956197576887232, best_f1=0.9034965034965035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.8578902351437137e-05\n",
            "step: 10, loss: 0.0004827669763471931\n",
            "step: 20, loss: 0.0001481641666032374\n",
            "step: 30, loss: 0.0003507582878228277\n",
            "step: 40, loss: 0.11316309869289398\n",
            "step: 50, loss: 6.1002512666163966e-05\n",
            "step: 60, loss: 3.815297895926051e-05\n",
            "step: 70, loss: 0.0012344607384875417\n",
            "step: 80, loss: 5.966416938463226e-05\n",
            "step: 90, loss: 0.000831498415209353\n",
            "step: 100, loss: 0.0010408585658296943\n",
            "step: 110, loss: 0.005102951545268297\n",
            "step: 120, loss: 0.00011696251749526709\n",
            "step: 130, loss: 0.006030930206179619\n",
            "step: 140, loss: 0.00017752678832039237\n",
            "step: 150, loss: 0.00013363728066906333\n",
            "step: 160, loss: 0.00037693994818255305\n",
            "step: 170, loss: 0.001272568479180336\n",
            "step: 180, loss: 6.534404383273795e-05\n",
            "step: 190, loss: 0.0002234018757008016\n",
            "step: 200, loss: 0.0002801746013574302\n",
            "step: 210, loss: 0.004022262990474701\n",
            "step: 220, loss: 8.011861064005643e-05\n",
            "step: 230, loss: 0.00038769186357967556\n",
            "step: 240, loss: 9.240317740477622e-05\n",
            "step: 250, loss: 0.00022509740665555\n",
            "step: 260, loss: 0.02222595550119877\n",
            "step: 270, loss: 0.012571834027767181\n",
            "step: 280, loss: 2.8013133487547748e-05\n",
            "step: 290, loss: 6.692624447168782e-05\n",
            "step: 300, loss: 3.571989145711996e-05\n",
            "step: 310, loss: 0.001971642253920436\n",
            "step: 320, loss: 0.00022466132941190153\n",
            "step: 330, loss: 0.00045274387230165303\n",
            "step: 340, loss: 0.00040846518822945654\n",
            "step: 350, loss: 8.288903336506337e-05\n",
            "step: 360, loss: 0.001694018254056573\n",
            "step: 370, loss: 5.19565655849874e-05\n",
            "step: 380, loss: 0.0017517601372674108\n",
            "step: 390, loss: 0.0026436843909323215\n",
            "step: 400, loss: 0.00023063740809448063\n",
            "step: 410, loss: 0.0008156555704772472\n",
            "step: 420, loss: 0.000478476780699566\n",
            "step: 430, loss: 0.00015569893002975732\n",
            "step: 440, loss: 0.00024043403391260654\n",
            "step: 450, loss: 4.691313733928837e-05\n",
            "step: 460, loss: 0.0005203045438975096\n",
            "step: 470, loss: 0.00010350149386795238\n",
            "step: 480, loss: 9.750796016305685e-05\n",
            "step: 490, loss: 9.374100773129612e-05\n",
            "step: 500, loss: 0.000523418071679771\n",
            "step: 510, loss: 0.0015809816541150212\n",
            "step: 520, loss: 7.126281707314774e-05\n",
            "step: 530, loss: 0.0004749197978526354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9144434222631096, f1=0.8986175115207373, best_f1=0.9034965034965035\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 192.09it/s]\n",
            "load_f1 = 0.9134300421151146\n",
            "real_f1 = 0.9121495327102803\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b049245-d20f-4eb7-d96e-02e248267f8c"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5537285208702087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3846153846153846, f1=0.36363636363636365, best_f1=0.36363636363636365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.503345787525177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.43478260869565216, f1=0.5294117647058824, best_f1=0.5294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4230138063430786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.46428571428571436, f1=0.3939393939393939, best_f1=0.3939393939393939\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.35748857259750366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.3333333333333333, f1=0.4324324324324324, best_f1=0.3939393939393939\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3738439977169037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5098039215686275, f1=0.4905660377358491, best_f1=0.4905660377358491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2785224914550781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.5217391304347826, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2556169927120209\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.5714285714285714, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08991910517215729\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5, f1=0.4615384615384615, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004510669037699699\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.6250000000000001, f1=0.6153846153846153, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1470484435558319\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5925925925925927, f1=0.6470588235294117, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0342162661254406\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5714285714285714, f1=0.689655172413793, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025465503334999084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5454545454545454, f1=0.6666666666666666, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.034106917679309845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6060606060606061, f1=0.5945945945945946, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018168674781918526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6060606060606061, f1=0.5945945945945946, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.040347546339035034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6060606060606061, f1=0.5945945945945946, best_f1=0.6153846153846153\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 134489.66it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.6250000000000001\n",
            "real_f1 = 0.6250000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 272.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56ef153-acc9-4f61-8ada-a89c3abe7d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6389184594154358\n",
            "step: 10, loss: 0.6617250442504883\n",
            "step: 20, loss: 0.48337918519973755\n",
            "step: 30, loss: 0.38339993357658386\n",
            "step: 40, loss: 0.1523703783750534\n",
            "step: 50, loss: 0.04787345230579376\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.06595179438591003\n",
            "step: 70, loss: 0.07814402878284454\n",
            "step: 80, loss: 0.1384209543466568\n",
            "step: 90, loss: 0.05373376980423927\n",
            "step: 100, loss: 0.008892862126231194\n",
            "step: 110, loss: 0.09345519542694092\n",
            "step: 120, loss: 0.021016893908381462\n",
            "step: 130, loss: 0.049464982002973557\n",
            "step: 140, loss: 0.007662048563361168\n",
            "step: 150, loss: 0.047531675547361374\n",
            "step: 160, loss: 0.010834548622369766\n",
            "step: 170, loss: 0.028583776205778122\n",
            "step: 180, loss: 0.16198502480983734\n",
            "step: 190, loss: 0.02003425732254982\n",
            "step: 200, loss: 0.00976658146828413\n",
            "step: 210, loss: 0.004992729984223843\n",
            "step: 220, loss: 0.01361821498721838\n",
            "step: 230, loss: 0.034417033195495605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9556313993174061, f1=0.9519450800915331, best_f1=0.9519450800915331\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04342492297291756\n",
            "step: 10, loss: 0.006201000884175301\n",
            "step: 20, loss: 0.0714723989367485\n",
            "step: 30, loss: 0.1712387651205063\n",
            "step: 40, loss: 0.04957818239927292\n",
            "step: 50, loss: 0.004125166218727827\n",
            "step: 60, loss: 0.004797420464456081\n",
            "step: 70, loss: 0.20010517537593842\n",
            "step: 80, loss: 0.1142694354057312\n",
            "step: 90, loss: 0.009328545071184635\n",
            "step: 100, loss: 0.010786606930196285\n",
            "step: 110, loss: 0.05352421849966049\n",
            "step: 120, loss: 0.004157153889536858\n",
            "step: 130, loss: 0.0360470674932003\n",
            "step: 140, loss: 0.013308316469192505\n",
            "step: 150, loss: 0.01267241407185793\n",
            "step: 160, loss: 0.10497376322746277\n",
            "step: 170, loss: 0.008626987226307392\n",
            "step: 180, loss: 0.004909321665763855\n",
            "step: 190, loss: 0.02608383633196354\n",
            "step: 200, loss: 0.009319483302533627\n",
            "step: 210, loss: 0.0008947817841544747\n",
            "step: 220, loss: 0.021810313686728477\n",
            "step: 230, loss: 0.08747311681509018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9830890642615557, f1=0.9749430523917996, best_f1=0.9749430523917996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009413440711796284\n",
            "step: 10, loss: 0.008040261454880238\n",
            "step: 20, loss: 0.0033978333231061697\n",
            "step: 30, loss: 0.0018353326013311744\n",
            "step: 40, loss: 0.11347100138664246\n",
            "step: 50, loss: 0.021666062995791435\n",
            "step: 60, loss: 0.00113878701813519\n",
            "step: 70, loss: 0.015448889695107937\n",
            "step: 80, loss: 0.0011711426777765155\n",
            "step: 90, loss: 0.1493402123451233\n",
            "step: 100, loss: 0.00219991453923285\n",
            "step: 110, loss: 0.004374295007437468\n",
            "step: 120, loss: 0.015516545623540878\n",
            "step: 130, loss: 0.01585960015654564\n",
            "step: 140, loss: 0.04801279306411743\n",
            "step: 150, loss: 0.002550339326262474\n",
            "step: 160, loss: 0.004030116833746433\n",
            "step: 170, loss: 0.003906077705323696\n",
            "step: 180, loss: 0.011536824516952038\n",
            "step: 190, loss: 0.0181549321860075\n",
            "step: 200, loss: 0.008764710277318954\n",
            "step: 210, loss: 0.0009177479660138488\n",
            "step: 220, loss: 0.0008660274324938655\n",
            "step: 230, loss: 0.0005470538162626326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9767441860465117, f1=0.9732142857142857, best_f1=0.9749430523917996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035508659202605486\n",
            "step: 10, loss: 0.00029923825059086084\n",
            "step: 20, loss: 0.0004901661886833608\n",
            "step: 30, loss: 0.0007450731936842203\n",
            "step: 40, loss: 0.011375986970961094\n",
            "step: 50, loss: 0.001070691621862352\n",
            "step: 60, loss: 0.02579382248222828\n",
            "step: 70, loss: 0.000510183977894485\n",
            "step: 80, loss: 0.02073846198618412\n",
            "step: 90, loss: 0.0012855305103585124\n",
            "step: 100, loss: 0.06270480155944824\n",
            "step: 110, loss: 0.0018800720572471619\n",
            "step: 120, loss: 0.0195096954703331\n",
            "step: 130, loss: 0.0025301810819655657\n",
            "step: 140, loss: 0.016325898468494415\n",
            "step: 150, loss: 0.015139742754399776\n",
            "step: 160, loss: 0.06509371101856232\n",
            "step: 170, loss: 0.0458909310400486\n",
            "step: 180, loss: 0.0013497619656845927\n",
            "step: 190, loss: 0.010489738546311855\n",
            "step: 200, loss: 0.0011253474513068795\n",
            "step: 210, loss: 0.036591943353414536\n",
            "step: 220, loss: 0.0012819498078897595\n",
            "step: 230, loss: 0.003678193548694253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9840546697038726, f1=0.9749430523917996, best_f1=0.9749430523917996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005653771222569048\n",
            "step: 10, loss: 0.0019333361415192485\n",
            "step: 20, loss: 0.0034225862473249435\n",
            "step: 30, loss: 0.0020621842704713345\n",
            "step: 40, loss: 0.014157694764435291\n",
            "step: 50, loss: 0.00033659671316854656\n",
            "step: 60, loss: 0.002276303479447961\n",
            "step: 70, loss: 0.02228379249572754\n",
            "step: 80, loss: 0.0009973846608772874\n",
            "step: 90, loss: 0.006276021711528301\n",
            "step: 100, loss: 0.0011512661585584283\n",
            "step: 110, loss: 0.0012259874492883682\n",
            "step: 120, loss: 0.0004001940251328051\n",
            "step: 130, loss: 0.0037496532313525677\n",
            "step: 140, loss: 0.006379581987857819\n",
            "step: 150, loss: 0.030887186527252197\n",
            "step: 160, loss: 0.0008523687138222158\n",
            "step: 170, loss: 0.008442881517112255\n",
            "step: 180, loss: 0.07050206512212753\n",
            "step: 190, loss: 0.0007886509411036968\n",
            "step: 200, loss: 0.09650193899869919\n",
            "step: 210, loss: 0.0012647738913074136\n",
            "step: 220, loss: 0.011643406003713608\n",
            "step: 230, loss: 0.0006184933008626103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9865168539325843, f1=0.9775784753363228, best_f1=0.9775784753363228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001247528474777937\n",
            "step: 10, loss: 0.00047569014714099467\n",
            "step: 20, loss: 0.0018200945341959596\n",
            "step: 30, loss: 0.0008307772222906351\n",
            "step: 40, loss: 0.0006136991432867944\n",
            "step: 50, loss: 0.000210209094802849\n",
            "step: 60, loss: 0.00034104479709640145\n",
            "step: 70, loss: 0.00022240735415834934\n",
            "step: 80, loss: 0.00046171084977686405\n",
            "step: 90, loss: 0.010422125458717346\n",
            "step: 100, loss: 0.07643965631723404\n",
            "step: 110, loss: 0.0008965259185060859\n",
            "step: 120, loss: 0.00027439265977591276\n",
            "step: 130, loss: 0.004785190336406231\n",
            "step: 140, loss: 0.00023197052360046655\n",
            "step: 150, loss: 0.006487716920673847\n",
            "step: 160, loss: 0.03775160387158394\n",
            "step: 170, loss: 0.00024496507830917835\n",
            "step: 180, loss: 0.017958611249923706\n",
            "step: 190, loss: 0.010134967975318432\n",
            "step: 200, loss: 0.004735399503260851\n",
            "step: 210, loss: 0.005091678351163864\n",
            "step: 220, loss: 0.000358696561306715\n",
            "step: 230, loss: 0.1100442036986351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9853768278965129, f1=0.9774266365688488, best_f1=0.9775784753363228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0041060615330934525\n",
            "step: 10, loss: 0.0001441210333723575\n",
            "step: 20, loss: 0.00026321274344809353\n",
            "step: 30, loss: 0.001723736640997231\n",
            "step: 40, loss: 0.00012842340220231563\n",
            "step: 50, loss: 0.0002156613045372069\n",
            "step: 60, loss: 0.002301787491887808\n",
            "step: 70, loss: 0.0029175400268286467\n",
            "step: 80, loss: 0.00016099598724395037\n",
            "step: 90, loss: 5.848207001690753e-05\n",
            "step: 100, loss: 8.794127643341199e-05\n",
            "step: 110, loss: 0.013886183500289917\n",
            "step: 120, loss: 7.767023635096848e-05\n",
            "step: 130, loss: 0.0001232721988344565\n",
            "step: 140, loss: 0.000276696402579546\n",
            "step: 150, loss: 0.0010105398250743747\n",
            "step: 160, loss: 0.06066618114709854\n",
            "step: 170, loss: 0.00026264548068866134\n",
            "step: 180, loss: 0.00029092031763866544\n",
            "step: 190, loss: 0.0004611910553649068\n",
            "step: 200, loss: 0.006417838390916586\n",
            "step: 210, loss: 7.612485933350399e-05\n",
            "step: 220, loss: 0.0002464957651682198\n",
            "step: 230, loss: 0.002561919391155243\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9898534385569334, f1=0.9807909604519773, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001765219058142975\n",
            "step: 10, loss: 0.006996434647589922\n",
            "step: 20, loss: 0.0006693355389870703\n",
            "step: 30, loss: 0.00016267220780719072\n",
            "step: 40, loss: 0.0003260948578827083\n",
            "step: 50, loss: 0.0011698442976921797\n",
            "step: 60, loss: 0.0004016877501271665\n",
            "step: 70, loss: 0.0003262005338910967\n",
            "step: 80, loss: 0.00018686139083001763\n",
            "step: 90, loss: 0.00023475312627851963\n",
            "step: 100, loss: 0.00012875105312559754\n",
            "step: 110, loss: 0.00013554308679886162\n",
            "step: 120, loss: 0.0074895229190588\n",
            "step: 130, loss: 0.0019415834685787559\n",
            "step: 140, loss: 0.00010853923595277593\n",
            "step: 150, loss: 0.0002710046246647835\n",
            "step: 160, loss: 0.00011982230353169143\n",
            "step: 170, loss: 7.416187872877344e-05\n",
            "step: 180, loss: 0.00024069714709185064\n",
            "step: 190, loss: 0.00011372831795597449\n",
            "step: 200, loss: 0.0004890659474767745\n",
            "step: 210, loss: 0.0001513638999313116\n",
            "step: 220, loss: 0.00022892901324667037\n",
            "step: 230, loss: 0.0003222145896870643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9853107344632768, f1=0.9773242630385486, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.336303340503946e-05\n",
            "step: 10, loss: 0.0002903903659898788\n",
            "step: 20, loss: 0.0001976394996745512\n",
            "step: 30, loss: 8.690721006132662e-05\n",
            "step: 40, loss: 0.016471097245812416\n",
            "step: 50, loss: 0.00027710196445696056\n",
            "step: 60, loss: 0.0001552088069729507\n",
            "step: 70, loss: 0.0004686036263592541\n",
            "step: 80, loss: 0.000803487200755626\n",
            "step: 90, loss: 7.98079781816341e-05\n",
            "step: 100, loss: 0.07819847762584686\n",
            "step: 110, loss: 5.715682345908135e-05\n",
            "step: 120, loss: 0.0002366385597269982\n",
            "step: 130, loss: 0.12462932616472244\n",
            "step: 140, loss: 0.021235719323158264\n",
            "step: 150, loss: 0.001995289698243141\n",
            "step: 160, loss: 9.062284516403452e-05\n",
            "step: 170, loss: 8.38524429127574e-05\n",
            "step: 180, loss: 0.02364582009613514\n",
            "step: 190, loss: 0.002138395793735981\n",
            "step: 200, loss: 6.319962267298251e-05\n",
            "step: 210, loss: 6.781492265872657e-05\n",
            "step: 220, loss: 0.000674273120239377\n",
            "step: 230, loss: 0.00014437397476285696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9842342342342343, f1=0.9707207207207207, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013115776237100363\n",
            "step: 10, loss: 0.00015960237942636013\n",
            "step: 20, loss: 0.0005078121903352439\n",
            "step: 30, loss: 8.745226659812033e-05\n",
            "step: 40, loss: 0.00018342857947573066\n",
            "step: 50, loss: 0.00011191795056220144\n",
            "step: 60, loss: 0.00021720088261645287\n",
            "step: 70, loss: 0.0003753102500922978\n",
            "step: 80, loss: 9.523233893560246e-05\n",
            "step: 90, loss: 0.0007635020883753896\n",
            "step: 100, loss: 0.00017337824101559818\n",
            "step: 110, loss: 0.00013429162208922207\n",
            "step: 120, loss: 0.00011178467684658244\n",
            "step: 130, loss: 6.852267688373104e-05\n",
            "step: 140, loss: 0.05397268012166023\n",
            "step: 150, loss: 0.002786267315968871\n",
            "step: 160, loss: 3.8331447285600007e-05\n",
            "step: 170, loss: 8.145047468133271e-05\n",
            "step: 180, loss: 0.00027522165328264236\n",
            "step: 190, loss: 0.05045264586806297\n",
            "step: 200, loss: 3.784741056733765e-05\n",
            "step: 210, loss: 6.14127202425152e-05\n",
            "step: 220, loss: 0.0008105204324238002\n",
            "step: 230, loss: 0.00011308400280540809\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9787234042553192, f1=0.9753363228699552, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07976101338863373\n",
            "step: 10, loss: 5.5373697250615805e-05\n",
            "step: 20, loss: 0.006650098133832216\n",
            "step: 30, loss: 0.007419077213853598\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.004796585999429226\n",
            "step: 50, loss: 8.560967398807406e-05\n",
            "step: 60, loss: 0.002886236412450671\n",
            "step: 70, loss: 0.00020500508253462613\n",
            "step: 80, loss: 0.00011137709225295112\n",
            "step: 90, loss: 0.00011498968524392694\n",
            "step: 100, loss: 0.00018327395082451403\n",
            "step: 110, loss: 5.590174259850755e-05\n",
            "step: 120, loss: 6.869190838187933e-05\n",
            "step: 130, loss: 6.0401605878723785e-05\n",
            "step: 140, loss: 6.611904973397031e-05\n",
            "step: 150, loss: 0.0034398760180920362\n",
            "step: 160, loss: 6.713598850183189e-05\n",
            "step: 170, loss: 0.0035651924554258585\n",
            "step: 180, loss: 6.407814362319186e-05\n",
            "step: 190, loss: 5.45689654245507e-05\n",
            "step: 200, loss: 5.6635555665707216e-05\n",
            "step: 210, loss: 8.870998135535046e-05\n",
            "step: 220, loss: 5.666624929290265e-05\n",
            "step: 230, loss: 6.743729318259284e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9818594104308391, f1=0.9750566893424036, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.9869296819670126e-05\n",
            "step: 10, loss: 9.465343464398757e-05\n",
            "step: 20, loss: 7.835251017240807e-05\n",
            "step: 30, loss: 9.288513683713973e-05\n",
            "step: 40, loss: 0.004413362592458725\n",
            "step: 50, loss: 0.0062937261536717415\n",
            "step: 60, loss: 0.008744166232645512\n",
            "step: 70, loss: 0.00010875705629587173\n",
            "step: 80, loss: 0.0004339797014836222\n",
            "step: 90, loss: 8.284511568490416e-05\n",
            "step: 100, loss: 7.369406375801191e-05\n",
            "step: 110, loss: 5.825351399835199e-05\n",
            "step: 120, loss: 8.266569784609601e-05\n",
            "step: 130, loss: 6.862147711217403e-05\n",
            "step: 140, loss: 5.556667747441679e-05\n",
            "step: 150, loss: 0.00046874943654984236\n",
            "step: 160, loss: 6.198125629452989e-05\n",
            "step: 170, loss: 0.00010009611287387088\n",
            "step: 180, loss: 8.170148066710681e-05\n",
            "step: 190, loss: 0.005652001593261957\n",
            "step: 200, loss: 3.5787590604741126e-05\n",
            "step: 210, loss: 5.1674633141374215e-05\n",
            "step: 220, loss: 5.763195440522395e-05\n",
            "step: 230, loss: 7.717539847362787e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9842342342342343, f1=0.9750566893424036, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.339982352685183e-05\n",
            "step: 10, loss: 0.0017141588032245636\n",
            "step: 20, loss: 0.002276371465995908\n",
            "step: 30, loss: 7.637315866304561e-05\n",
            "step: 40, loss: 0.00045032487832941115\n",
            "step: 50, loss: 0.00012830027844756842\n",
            "step: 60, loss: 7.422352064168081e-05\n",
            "step: 70, loss: 3.6761968658538535e-05\n",
            "step: 80, loss: 5.262708509690128e-05\n",
            "step: 90, loss: 0.00016018228780012578\n",
            "step: 100, loss: 2.6858622732106596e-05\n",
            "step: 110, loss: 0.008941391482949257\n",
            "step: 120, loss: 0.0017220211448147893\n",
            "step: 130, loss: 0.00013180356472730637\n",
            "step: 140, loss: 3.9829101297073066e-05\n",
            "step: 150, loss: 3.833919618045911e-05\n",
            "step: 160, loss: 0.000795534928329289\n",
            "step: 170, loss: 8.082784188445657e-05\n",
            "step: 180, loss: 5.444525959319435e-05\n",
            "step: 190, loss: 0.00014389125863090158\n",
            "step: 200, loss: 0.006490643136203289\n",
            "step: 210, loss: 3.25170076393988e-05\n",
            "step: 220, loss: 5.68238647247199e-05\n",
            "step: 230, loss: 4.344909393694252e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.987598647125141, f1=0.9774266365688488, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.0257800719700754e-05\n",
            "step: 10, loss: 0.006632282864302397\n",
            "step: 20, loss: 0.00013310274516697973\n",
            "step: 30, loss: 0.00026719667948782444\n",
            "step: 40, loss: 0.0020047163125127554\n",
            "step: 50, loss: 6.460247823270038e-05\n",
            "step: 60, loss: 0.00016900515765883029\n",
            "step: 70, loss: 4.5059070544084534e-05\n",
            "step: 80, loss: 4.397139491629787e-05\n",
            "step: 90, loss: 4.210532279103063e-05\n",
            "step: 100, loss: 4.4179421820444986e-05\n",
            "step: 110, loss: 0.0001408426760463044\n",
            "step: 120, loss: 2.239228706457652e-05\n",
            "step: 130, loss: 9.364934521727264e-05\n",
            "step: 140, loss: 0.0003689866862259805\n",
            "step: 150, loss: 3.934407141059637e-05\n",
            "step: 160, loss: 0.03207571059465408\n",
            "step: 170, loss: 5.651870014844462e-05\n",
            "step: 180, loss: 3.9895749068818986e-05\n",
            "step: 190, loss: 5.1912029448430985e-05\n",
            "step: 200, loss: 6.569344986928627e-05\n",
            "step: 210, loss: 0.0001815089926822111\n",
            "step: 220, loss: 0.00018044895841740072\n",
            "step: 230, loss: 0.0001359035522909835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9863636363636363, f1=0.9738339021615473, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.558643948053941e-05\n",
            "step: 10, loss: 3.437103077885695e-05\n",
            "step: 20, loss: 4.4525921111926436e-05\n",
            "step: 30, loss: 7.057406764943153e-05\n",
            "step: 40, loss: 0.0003096394066233188\n",
            "step: 50, loss: 0.021272175014019012\n",
            "step: 60, loss: 3.9575894334120676e-05\n",
            "step: 70, loss: 6.447229679906741e-05\n",
            "step: 80, loss: 0.0001183835047413595\n",
            "step: 90, loss: 4.697397889685817e-05\n",
            "step: 100, loss: 0.004011720884591341\n",
            "step: 110, loss: 9.431866783415899e-05\n",
            "step: 120, loss: 7.638841634616256e-05\n",
            "step: 130, loss: 3.313176421215758e-05\n",
            "step: 140, loss: 3.346307858009823e-05\n",
            "step: 150, loss: 0.00011941747652599588\n",
            "step: 160, loss: 0.0001648177276365459\n",
            "step: 170, loss: 4.24528589064721e-05\n",
            "step: 180, loss: 4.9393591325497255e-05\n",
            "step: 190, loss: 5.341726136975922e-05\n",
            "step: 200, loss: 4.3579402699833736e-05\n",
            "step: 210, loss: 4.430290209711529e-05\n",
            "step: 220, loss: 3.7679328670492396e-05\n",
            "step: 230, loss: 5.075382432551123e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9863636363636363, f1=0.9726651480637813, best_f1=0.9807909604519773\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 197.52it/s]\n",
            "load_f1 = 0.9863945578231292\n",
            "real_f1 = 0.9864253393665158\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 258.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d14443-ed08-426e-a6de-37be564e7ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6421183943748474\n",
            "step: 10, loss: 0.6995790600776672\n",
            "step: 20, loss: 0.6058841347694397\n",
            "step: 30, loss: 0.3197649419307709\n",
            "step: 40, loss: 0.2713649868965149\n",
            "step: 50, loss: 0.2516390085220337\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.679408848285675\n",
            "step: 70, loss: 0.27800214290618896\n",
            "step: 80, loss: 0.052117928862571716\n",
            "step: 90, loss: 0.424496054649353\n",
            "step: 100, loss: 0.11637584865093231\n",
            "step: 110, loss: 0.1040760725736618\n",
            "step: 120, loss: 0.16227833926677704\n",
            "step: 130, loss: 0.1160588189959526\n",
            "step: 140, loss: 0.16380897164344788\n",
            "step: 150, loss: 0.18809163570404053\n",
            "step: 160, loss: 0.04103270545601845\n",
            "step: 170, loss: 0.26856333017349243\n",
            "step: 180, loss: 0.1332913488149643\n",
            "step: 190, loss: 0.05380196124315262\n",
            "step: 200, loss: 0.14468629658222198\n",
            "step: 210, loss: 0.06936771422624588\n",
            "step: 220, loss: 0.26232823729515076\n",
            "step: 230, loss: 0.20701025426387787\n",
            "step: 240, loss: 0.11325754225254059\n",
            "step: 250, loss: 0.09236881136894226\n",
            "step: 260, loss: 0.41953715682029724\n",
            "step: 270, loss: 0.043145932257175446\n",
            "step: 280, loss: 0.09062140434980392\n",
            "step: 290, loss: 0.0857018232345581\n",
            "step: 300, loss: 0.0630778893828392\n",
            "step: 310, loss: 0.2225126177072525\n",
            "step: 320, loss: 0.1418837010860443\n",
            "step: 330, loss: 0.06267087161540985\n",
            "step: 340, loss: 0.1708649843931198\n",
            "step: 350, loss: 0.054169122129678726\n",
            "step: 360, loss: 0.036410946398973465\n",
            "step: 370, loss: 0.16299636662006378\n",
            "step: 380, loss: 0.012924149632453918\n",
            "step: 390, loss: 0.3910031318664551\n",
            "step: 400, loss: 0.2679772675037384\n",
            "step: 410, loss: 0.06670437008142471\n",
            "step: 420, loss: 0.10219503939151764\n",
            "step: 430, loss: 0.20843036472797394\n",
            "step: 440, loss: 0.059280648827552795\n",
            "step: 450, loss: 0.041594695299863815\n",
            "step: 460, loss: 0.024200303480029106\n",
            "step: 470, loss: 0.04488743841648102\n",
            "step: 480, loss: 0.14717774093151093\n",
            "step: 490, loss: 0.3019404113292694\n",
            "step: 500, loss: 0.10322026908397675\n",
            "step: 510, loss: 0.14267446100711823\n",
            "step: 520, loss: 0.031807780265808105\n",
            "step: 530, loss: 0.01412262860685587\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8805206880520688, f1=0.8839981659789088, best_f1=0.8839981659789088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20008733868598938\n",
            "step: 10, loss: 0.14765706658363342\n",
            "step: 20, loss: 0.01554082054644823\n",
            "step: 30, loss: 0.014959963969886303\n",
            "step: 40, loss: 0.07016542553901672\n",
            "step: 50, loss: 0.17874997854232788\n",
            "step: 60, loss: 0.029129650443792343\n",
            "step: 70, loss: 0.06266877055168152\n",
            "step: 80, loss: 0.1823686808347702\n",
            "step: 90, loss: 0.09609533101320267\n",
            "step: 100, loss: 0.1751047670841217\n",
            "step: 110, loss: 0.058328062295913696\n",
            "step: 120, loss: 0.13430652022361755\n",
            "step: 130, loss: 0.09947343170642853\n",
            "step: 140, loss: 0.0451570563018322\n",
            "step: 150, loss: 0.08646640181541443\n",
            "step: 160, loss: 0.10795057564973831\n",
            "step: 170, loss: 0.06261748820543289\n",
            "step: 180, loss: 0.03127169981598854\n",
            "step: 190, loss: 0.027891673147678375\n",
            "step: 200, loss: 0.04309195280075073\n",
            "step: 210, loss: 0.03841289505362511\n",
            "step: 220, loss: 0.09594997018575668\n",
            "step: 230, loss: 0.011061075143516064\n",
            "step: 240, loss: 0.03179028630256653\n",
            "step: 250, loss: 0.02923828735947609\n",
            "step: 260, loss: 0.004542454611510038\n",
            "step: 270, loss: 0.24263595044612885\n",
            "step: 280, loss: 0.10136362165212631\n",
            "step: 290, loss: 0.13415347039699554\n",
            "step: 300, loss: 0.03077983483672142\n",
            "step: 310, loss: 0.08932148665189743\n",
            "step: 320, loss: 0.24387158453464508\n",
            "step: 330, loss: 0.11128108203411102\n",
            "step: 340, loss: 0.01271548867225647\n",
            "step: 350, loss: 0.0038324170745909214\n",
            "step: 360, loss: 0.14977505803108215\n",
            "step: 370, loss: 0.1294868290424347\n",
            "step: 380, loss: 0.18478482961654663\n",
            "step: 390, loss: 0.1024661585688591\n",
            "step: 400, loss: 0.061028625816106796\n",
            "step: 410, loss: 0.015615259297192097\n",
            "step: 420, loss: 0.04466495290398598\n",
            "step: 430, loss: 0.10303568094968796\n",
            "step: 440, loss: 0.0738116130232811\n",
            "step: 450, loss: 0.07059214264154434\n",
            "step: 460, loss: 0.026717405766248703\n",
            "step: 470, loss: 0.06706325709819794\n",
            "step: 480, loss: 0.16970719397068024\n",
            "step: 490, loss: 0.04030390456318855\n",
            "step: 500, loss: 0.31226709485054016\n",
            "step: 510, loss: 0.030290530994534492\n",
            "step: 520, loss: 0.03405417129397392\n",
            "step: 530, loss: 0.022001171484589577\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9136385087305333, f1=0.9110386585933862, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11133322864770889\n",
            "step: 10, loss: 0.0314088799059391\n",
            "step: 20, loss: 0.11916080117225647\n",
            "step: 30, loss: 0.08529753237962723\n",
            "step: 40, loss: 0.012435915879905224\n",
            "step: 50, loss: 0.17156511545181274\n",
            "step: 60, loss: 0.014752830378711224\n",
            "step: 70, loss: 0.02679400146007538\n",
            "step: 80, loss: 0.02188771590590477\n",
            "step: 90, loss: 0.0072515239007771015\n",
            "step: 100, loss: 0.025901200249791145\n",
            "step: 110, loss: 0.0186820887029171\n",
            "step: 120, loss: 0.03384280204772949\n",
            "step: 130, loss: 0.006153373047709465\n",
            "step: 140, loss: 0.017642298713326454\n",
            "step: 150, loss: 0.03231294825673103\n",
            "step: 160, loss: 0.04689835384488106\n",
            "step: 170, loss: 0.1474829614162445\n",
            "step: 180, loss: 0.02719896286725998\n",
            "step: 190, loss: 0.12522439658641815\n",
            "step: 200, loss: 0.07341361045837402\n",
            "step: 210, loss: 0.02814071998000145\n",
            "step: 220, loss: 0.19755366444587708\n",
            "step: 230, loss: 0.0656396672129631\n",
            "step: 240, loss: 0.10897857695817947\n",
            "step: 250, loss: 0.11751477420330048\n",
            "step: 260, loss: 0.02231142483651638\n",
            "step: 270, loss: 0.014808163978159428\n",
            "step: 280, loss: 0.1306544989347458\n",
            "step: 290, loss: 0.004855015780776739\n",
            "step: 300, loss: 0.06780495494604111\n",
            "step: 310, loss: 0.06948604434728622\n",
            "step: 320, loss: 0.033381327986717224\n",
            "step: 330, loss: 0.0187408234924078\n",
            "step: 340, loss: 0.018932919949293137\n",
            "step: 350, loss: 0.005653982516378164\n",
            "step: 360, loss: 0.06365828961133957\n",
            "step: 370, loss: 0.017108822241425514\n",
            "step: 380, loss: 0.051862411201000214\n",
            "step: 390, loss: 0.1518184095621109\n",
            "step: 400, loss: 0.08538372069597244\n",
            "step: 410, loss: 0.012913231737911701\n",
            "step: 420, loss: 0.12404289841651917\n",
            "step: 430, loss: 0.06413472443819046\n",
            "step: 440, loss: 0.021685147657990456\n",
            "step: 450, loss: 0.06255479902029037\n",
            "step: 460, loss: 0.02820744179189205\n",
            "step: 470, loss: 0.10914332419633865\n",
            "step: 480, loss: 0.05539077892899513\n",
            "step: 490, loss: 0.016409706324338913\n",
            "step: 500, loss: 0.055109284818172455\n",
            "step: 510, loss: 0.016850585117936134\n",
            "step: 520, loss: 0.23423655331134796\n",
            "step: 530, loss: 0.0472731739282608\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9064546304957904, f1=0.904363974001857, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0176222026348114\n",
            "step: 10, loss: 0.014656643383204937\n",
            "step: 20, loss: 0.002382960868999362\n",
            "step: 30, loss: 0.004833536688238382\n",
            "step: 40, loss: 0.010605007410049438\n",
            "step: 50, loss: 0.0459066741168499\n",
            "step: 60, loss: 0.030005773529410362\n",
            "step: 70, loss: 0.03944934904575348\n",
            "step: 80, loss: 0.04000858590006828\n",
            "step: 90, loss: 0.17409229278564453\n",
            "step: 100, loss: 0.09159214049577713\n",
            "step: 110, loss: 0.1408403068780899\n",
            "step: 120, loss: 0.040494050830602646\n",
            "step: 130, loss: 0.07796952873468399\n",
            "step: 140, loss: 0.0033645813819020987\n",
            "step: 150, loss: 0.007714849431067705\n",
            "step: 160, loss: 0.1998731940984726\n",
            "step: 170, loss: 0.054740842431783676\n",
            "step: 180, loss: 0.014317610301077366\n",
            "step: 190, loss: 0.027514822781085968\n",
            "step: 200, loss: 0.158567413687706\n",
            "step: 210, loss: 0.11601319164037704\n",
            "step: 220, loss: 0.020718451589345932\n",
            "step: 230, loss: 0.030956992879509926\n",
            "step: 240, loss: 0.013415921479463577\n",
            "step: 250, loss: 0.08537545800209045\n",
            "step: 260, loss: 0.010814519599080086\n",
            "step: 270, loss: 0.03729984909296036\n",
            "step: 280, loss: 0.13037899136543274\n",
            "step: 290, loss: 0.019299915060400963\n",
            "step: 300, loss: 0.007722924463450909\n",
            "step: 310, loss: 0.014600833877921104\n",
            "step: 320, loss: 0.00975812878459692\n",
            "step: 330, loss: 0.18026678264141083\n",
            "step: 340, loss: 0.05549932271242142\n",
            "step: 350, loss: 0.02072390355169773\n",
            "step: 360, loss: 0.0885450541973114\n",
            "step: 370, loss: 0.003625981044024229\n",
            "step: 380, loss: 0.008933785371482372\n",
            "step: 390, loss: 0.004333481192588806\n",
            "step: 400, loss: 0.013652737252414227\n",
            "step: 410, loss: 0.03366442024707794\n",
            "step: 420, loss: 0.019414102658629417\n",
            "step: 430, loss: 0.17957621812820435\n",
            "step: 440, loss: 0.01042608916759491\n",
            "step: 450, loss: 0.033339135348796844\n",
            "step: 460, loss: 0.06916292011737823\n",
            "step: 470, loss: 0.019884275272488594\n",
            "step: 480, loss: 0.03323392942547798\n",
            "step: 490, loss: 0.0013498755870386958\n",
            "step: 500, loss: 0.012209564447402954\n",
            "step: 510, loss: 0.02152399905025959\n",
            "step: 520, loss: 0.06027406454086304\n",
            "step: 530, loss: 0.015798967331647873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9005090236001851, f1=0.8991674375578168, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041906509548425674\n",
            "step: 10, loss: 0.07248084992170334\n",
            "step: 20, loss: 0.006297219078987837\n",
            "step: 30, loss: 0.00432035094127059\n",
            "step: 40, loss: 0.06559421867132187\n",
            "step: 50, loss: 0.005774305202066898\n",
            "step: 60, loss: 0.07638311386108398\n",
            "step: 70, loss: 0.008964503183960915\n",
            "step: 80, loss: 0.0009346548467874527\n",
            "step: 90, loss: 0.07939144223928452\n",
            "step: 100, loss: 0.0028958767652511597\n",
            "step: 110, loss: 0.009304646402597427\n",
            "step: 120, loss: 0.007788783870637417\n",
            "step: 130, loss: 0.0010242164134979248\n",
            "step: 140, loss: 0.008586565032601357\n",
            "step: 150, loss: 0.0011852829484269023\n",
            "step: 160, loss: 0.007403763942420483\n",
            "step: 170, loss: 0.006036612670868635\n",
            "step: 180, loss: 0.0021355533972382545\n",
            "step: 190, loss: 0.0020545825827866793\n",
            "step: 200, loss: 0.0011980661656707525\n",
            "step: 210, loss: 0.00880391988903284\n",
            "step: 220, loss: 0.04685370996594429\n",
            "step: 230, loss: 0.00364682893268764\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 240, loss: 0.0015446157194674015\n",
            "step: 250, loss: 0.007418077904731035\n",
            "step: 260, loss: 0.026709582656621933\n",
            "step: 270, loss: 0.03802258148789406\n",
            "step: 280, loss: 0.010731329210102558\n",
            "step: 290, loss: 0.125783309340477\n",
            "step: 300, loss: 0.020489608868956566\n",
            "step: 310, loss: 0.004768455866724253\n",
            "step: 320, loss: 0.028641507029533386\n",
            "step: 330, loss: 0.005671055056154728\n",
            "step: 340, loss: 0.0023975130170583725\n",
            "step: 350, loss: 0.0007616751827299595\n",
            "step: 360, loss: 0.032008878886699677\n",
            "step: 370, loss: 0.0559818372130394\n",
            "step: 380, loss: 0.0042708637192845345\n",
            "step: 390, loss: 0.007283355109393597\n",
            "step: 400, loss: 0.018623489886522293\n",
            "step: 410, loss: 0.06791634857654572\n",
            "step: 420, loss: 0.00822922307997942\n",
            "step: 430, loss: 0.007047269959002733\n",
            "step: 440, loss: 0.1229095309972763\n",
            "step: 450, loss: 0.15813925862312317\n",
            "step: 460, loss: 0.015805184841156006\n",
            "step: 470, loss: 0.002564621390774846\n",
            "step: 480, loss: 0.007043006829917431\n",
            "step: 490, loss: 0.001240032841451466\n",
            "step: 500, loss: 0.005430764053016901\n",
            "step: 510, loss: 0.02720893733203411\n",
            "step: 520, loss: 0.07667100429534912\n",
            "step: 530, loss: 0.01044459082186222\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.893461362597165, f1=0.9012915129151292, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010381587781012058\n",
            "step: 10, loss: 0.001397965126670897\n",
            "step: 20, loss: 0.15348173677921295\n",
            "step: 30, loss: 0.024692337960004807\n",
            "step: 40, loss: 0.013222410343587399\n",
            "step: 50, loss: 0.0006974615971557796\n",
            "step: 60, loss: 0.0003243243554607034\n",
            "step: 70, loss: 0.00028659726376645267\n",
            "step: 80, loss: 0.0010723741725087166\n",
            "step: 90, loss: 0.00016416743164882064\n",
            "step: 100, loss: 0.003354474436491728\n",
            "step: 110, loss: 0.03272305428981781\n",
            "step: 120, loss: 0.0005049642059020698\n",
            "step: 130, loss: 0.014998250640928745\n",
            "step: 140, loss: 0.004873654805123806\n",
            "step: 150, loss: 0.014294939115643501\n",
            "step: 160, loss: 0.003355575492605567\n",
            "step: 170, loss: 0.0005617736023850739\n",
            "step: 180, loss: 0.0006329663447104394\n",
            "step: 190, loss: 0.007124622818082571\n",
            "step: 200, loss: 0.0026829682756215334\n",
            "step: 210, loss: 0.009584969840943813\n",
            "step: 220, loss: 0.02568047121167183\n",
            "step: 230, loss: 0.0013828420778736472\n",
            "step: 240, loss: 0.06795831769704819\n",
            "step: 250, loss: 0.007625112775713205\n",
            "step: 260, loss: 0.008885785937309265\n",
            "step: 270, loss: 0.05638045072555542\n",
            "step: 280, loss: 0.0030669791158288717\n",
            "step: 290, loss: 0.0023569113109260798\n",
            "step: 300, loss: 0.005271392408758402\n",
            "step: 310, loss: 0.004115608055144548\n",
            "step: 320, loss: 0.00020564069563988596\n",
            "step: 330, loss: 0.0003112927370239049\n",
            "step: 340, loss: 0.0010278798872604966\n",
            "step: 350, loss: 0.0015038878191262484\n",
            "step: 360, loss: 0.007017635740339756\n",
            "step: 370, loss: 0.015463240444660187\n",
            "step: 380, loss: 0.0019163333345204592\n",
            "step: 390, loss: 0.024501746520400047\n",
            "step: 400, loss: 0.04354168102145195\n",
            "step: 410, loss: 0.002379309618845582\n",
            "step: 420, loss: 0.0028270878829061985\n",
            "step: 430, loss: 0.0010832464322447777\n",
            "step: 440, loss: 0.014979901723563671\n",
            "step: 450, loss: 0.003525958862155676\n",
            "step: 460, loss: 0.005017329938709736\n",
            "step: 470, loss: 0.037662573158741\n",
            "step: 480, loss: 0.003897111862897873\n",
            "step: 490, loss: 0.012422401458024979\n",
            "step: 500, loss: 0.005931271705776453\n",
            "step: 510, loss: 0.003577057272195816\n",
            "step: 520, loss: 0.0107789421454072\n",
            "step: 530, loss: 0.10782032459974289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9013953488372093, f1=0.8959107806691451, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012953934259712696\n",
            "step: 10, loss: 0.003229910973459482\n",
            "step: 20, loss: 0.001477081561461091\n",
            "step: 30, loss: 0.0007440935587510467\n",
            "step: 40, loss: 0.0013638341333717108\n",
            "step: 50, loss: 0.007639977615326643\n",
            "step: 60, loss: 0.0017430043080821633\n",
            "step: 70, loss: 0.0008977458928711712\n",
            "step: 80, loss: 0.0016804892802610993\n",
            "step: 90, loss: 0.009531316347420216\n",
            "step: 100, loss: 0.0571146197617054\n",
            "step: 110, loss: 0.0004508172278292477\n",
            "step: 120, loss: 0.006238567642867565\n",
            "step: 130, loss: 0.005288966931402683\n",
            "step: 140, loss: 0.000607315159868449\n",
            "step: 150, loss: 0.0010030283592641354\n",
            "step: 160, loss: 0.0002614394179545343\n",
            "step: 170, loss: 0.031872231513261795\n",
            "step: 180, loss: 0.0003760058607440442\n",
            "step: 190, loss: 0.0003110631078016013\n",
            "step: 200, loss: 0.002712561748921871\n",
            "step: 210, loss: 0.007078133523464203\n",
            "step: 220, loss: 0.0021179141476750374\n",
            "step: 230, loss: 0.004019244108349085\n",
            "step: 240, loss: 0.03200826048851013\n",
            "step: 250, loss: 0.0018088252982124686\n",
            "step: 260, loss: 0.013130580075085163\n",
            "step: 270, loss: 0.0013623659033328295\n",
            "step: 280, loss: 0.003914074506610632\n",
            "step: 290, loss: 0.015767047181725502\n",
            "step: 300, loss: 0.0005235964199528098\n",
            "step: 310, loss: 0.0007392773986794055\n",
            "step: 320, loss: 0.0029235773254185915\n",
            "step: 330, loss: 0.0007404144271276891\n",
            "step: 340, loss: 0.0047920155338943005\n",
            "step: 350, loss: 0.03492523729801178\n",
            "step: 360, loss: 0.002458566101267934\n",
            "step: 370, loss: 0.0013128904392942786\n",
            "step: 380, loss: 0.00302308052778244\n",
            "step: 390, loss: 0.00016593914187978953\n",
            "step: 400, loss: 0.0012874723179265857\n",
            "step: 410, loss: 0.0031609944999217987\n",
            "step: 420, loss: 0.0001320415030932054\n",
            "step: 430, loss: 7.420865586027503e-05\n",
            "step: 440, loss: 0.06822704523801804\n",
            "step: 450, loss: 0.0011051562614738941\n",
            "step: 460, loss: 0.009024790488183498\n",
            "step: 470, loss: 0.0008923602872528136\n",
            "step: 480, loss: 0.1847154051065445\n",
            "step: 490, loss: 0.07556650787591934\n",
            "step: 500, loss: 0.00400968873873353\n",
            "step: 510, loss: 0.011896847747266293\n",
            "step: 520, loss: 0.00691658491268754\n",
            "step: 530, loss: 0.024502204731106758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9050543221539914, f1=0.8994808872109485, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004701008554548025\n",
            "step: 10, loss: 0.000747853540815413\n",
            "step: 20, loss: 0.0002490698825567961\n",
            "step: 30, loss: 0.005752027966082096\n",
            "step: 40, loss: 0.0009827420581132174\n",
            "step: 50, loss: 0.05269772931933403\n",
            "step: 60, loss: 0.002372696530073881\n",
            "step: 70, loss: 0.002257322194054723\n",
            "step: 80, loss: 0.004629182629287243\n",
            "step: 90, loss: 0.0005645881174132228\n",
            "step: 100, loss: 0.0002573887468315661\n",
            "step: 110, loss: 0.0001688704069238156\n",
            "step: 120, loss: 0.0018263008678331971\n",
            "step: 130, loss: 0.00015738687943667173\n",
            "step: 140, loss: 0.026939496397972107\n",
            "step: 150, loss: 0.0009381232084706426\n",
            "step: 160, loss: 0.007757557090371847\n",
            "step: 170, loss: 0.00883416272699833\n",
            "step: 180, loss: 0.002959827659651637\n",
            "step: 190, loss: 0.005395802669227123\n",
            "step: 200, loss: 0.001978703774511814\n",
            "step: 210, loss: 0.0013824740890413523\n",
            "step: 220, loss: 0.001065244898200035\n",
            "step: 230, loss: 0.0011386770056560636\n",
            "step: 240, loss: 0.0016238264506682754\n",
            "step: 250, loss: 9.096676512854174e-05\n",
            "step: 260, loss: 5.8929534134222195e-05\n",
            "step: 270, loss: 0.0002794179890770465\n",
            "step: 280, loss: 0.0002789598365779966\n",
            "step: 290, loss: 0.0002508426259737462\n",
            "step: 300, loss: 0.000900502607692033\n",
            "step: 310, loss: 0.005109009332954884\n",
            "step: 320, loss: 0.06144782900810242\n",
            "step: 330, loss: 8.4844978118781e-05\n",
            "step: 340, loss: 5.4401421948568895e-05\n",
            "step: 350, loss: 7.197360537247732e-05\n",
            "step: 360, loss: 0.004556639585644007\n",
            "step: 370, loss: 0.0004421162884682417\n",
            "step: 380, loss: 0.0010050575947389007\n",
            "step: 390, loss: 0.09228521585464478\n",
            "step: 400, loss: 0.00038622028660029173\n",
            "step: 410, loss: 7.893027941463515e-05\n",
            "step: 420, loss: 0.0011400915682315826\n",
            "step: 430, loss: 0.05881895497441292\n",
            "step: 440, loss: 0.045497827231884\n",
            "step: 450, loss: 0.029567716643214226\n",
            "step: 460, loss: 0.007798568811267614\n",
            "step: 470, loss: 0.0013905331725254655\n",
            "step: 480, loss: 0.00018915344844572246\n",
            "step: 490, loss: 0.0008220889721997082\n",
            "step: 500, loss: 0.012848912738263607\n",
            "step: 510, loss: 0.0005905778962187469\n",
            "step: 520, loss: 0.00949020218104124\n",
            "step: 530, loss: 0.0005724729271605611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9019607843137254, f1=0.8994857410004675, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015521423192694783\n",
            "step: 10, loss: 0.0004743530007544905\n",
            "step: 20, loss: 0.00039268372347578406\n",
            "step: 30, loss: 0.0006150699337013066\n",
            "step: 40, loss: 0.00018168757378589362\n",
            "step: 50, loss: 0.0006120430771261454\n",
            "step: 60, loss: 0.0001891387510113418\n",
            "step: 70, loss: 0.0001761371677275747\n",
            "step: 80, loss: 0.04920051246881485\n",
            "step: 90, loss: 0.006606161128729582\n",
            "step: 100, loss: 0.00011025486310245469\n",
            "step: 110, loss: 7.953720341902226e-05\n",
            "step: 120, loss: 0.00018086179625242949\n",
            "step: 130, loss: 0.016220830380916595\n",
            "step: 140, loss: 0.00016000738833099604\n",
            "step: 150, loss: 8.190477092284709e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.0023670480586588383\n",
            "step: 170, loss: 0.0120537793263793\n",
            "step: 180, loss: 0.0005745809758082032\n",
            "step: 190, loss: 7.674529479118064e-05\n",
            "step: 200, loss: 0.00013205198047216982\n",
            "step: 210, loss: 0.000986958504654467\n",
            "step: 220, loss: 0.0025872173719108105\n",
            "step: 230, loss: 0.00014576951798517257\n",
            "step: 240, loss: 0.009466761723160744\n",
            "step: 250, loss: 6.879949796712026e-05\n",
            "step: 260, loss: 0.0034492406994104385\n",
            "step: 270, loss: 8.215332491090521e-05\n",
            "step: 280, loss: 8.99720544111915e-05\n",
            "step: 290, loss: 0.1954873502254486\n",
            "step: 300, loss: 0.0005471086478792131\n",
            "step: 310, loss: 0.0425892248749733\n",
            "step: 320, loss: 7.263939187396318e-05\n",
            "step: 330, loss: 0.000770552025642246\n",
            "step: 340, loss: 0.0004944056272506714\n",
            "step: 350, loss: 0.014357835054397583\n",
            "step: 360, loss: 6.871293589938432e-05\n",
            "step: 370, loss: 6.76411873428151e-05\n",
            "step: 380, loss: 0.12197710573673248\n",
            "step: 390, loss: 9.527731162961572e-05\n",
            "step: 400, loss: 0.022396525368094444\n",
            "step: 410, loss: 8.096696546999738e-05\n",
            "step: 420, loss: 0.0001123626425396651\n",
            "step: 430, loss: 0.0001423616340616718\n",
            "step: 440, loss: 0.001385149429552257\n",
            "step: 450, loss: 6.076516001485288e-05\n",
            "step: 460, loss: 0.00032199808629229665\n",
            "step: 470, loss: 2.8888822271255776e-05\n",
            "step: 480, loss: 8.181224256986752e-05\n",
            "step: 490, loss: 0.015059621073305607\n",
            "step: 500, loss: 0.0006363976281136274\n",
            "step: 510, loss: 0.0002029921452049166\n",
            "step: 520, loss: 0.0024405813310295343\n",
            "step: 530, loss: 8.022625115700066e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9047619047619049, f1=0.9072635906806761, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005524276290088892\n",
            "step: 10, loss: 4.131819150643423e-05\n",
            "step: 20, loss: 3.266232306486927e-05\n",
            "step: 30, loss: 3.914746775990352e-05\n",
            "step: 40, loss: 0.006353192497044802\n",
            "step: 50, loss: 4.2652460251701996e-05\n",
            "step: 60, loss: 4.172064654994756e-05\n",
            "step: 70, loss: 4.869724580203183e-05\n",
            "step: 80, loss: 4.449639163794927e-05\n",
            "step: 90, loss: 6.227455742191523e-05\n",
            "step: 100, loss: 3.163423025398515e-05\n",
            "step: 110, loss: 5.750155833084136e-05\n",
            "step: 120, loss: 0.0029187360778450966\n",
            "step: 130, loss: 0.0002801061200443655\n",
            "step: 140, loss: 0.008070878684520721\n",
            "step: 150, loss: 0.003944296855479479\n",
            "step: 160, loss: 0.006311206612735987\n",
            "step: 170, loss: 0.0013673445209860802\n",
            "step: 180, loss: 0.0006554963765665889\n",
            "step: 190, loss: 0.0065975673496723175\n",
            "step: 200, loss: 0.01139528676867485\n",
            "step: 210, loss: 0.0007435818552039564\n",
            "step: 220, loss: 0.008809467777609825\n",
            "step: 230, loss: 0.002367014065384865\n",
            "step: 240, loss: 0.00029429158894345164\n",
            "step: 250, loss: 0.0003326826263219118\n",
            "step: 260, loss: 0.0017934024799615145\n",
            "step: 270, loss: 0.00047761667519807816\n",
            "step: 280, loss: 0.008278603665530682\n",
            "step: 290, loss: 0.0002993041416630149\n",
            "step: 300, loss: 3.826117972494103e-05\n",
            "step: 310, loss: 5.3111394663574174e-05\n",
            "step: 320, loss: 0.0018611373379826546\n",
            "step: 330, loss: 9.12389878067188e-05\n",
            "step: 340, loss: 0.0016140480292961001\n",
            "step: 350, loss: 0.010330650955438614\n",
            "step: 360, loss: 0.026632066816091537\n",
            "step: 370, loss: 0.0021383147686719894\n",
            "step: 380, loss: 0.014983500353991985\n",
            "step: 390, loss: 0.0013607689179480076\n",
            "step: 400, loss: 0.0014691282995045185\n",
            "step: 410, loss: 0.00016818681615404785\n",
            "step: 420, loss: 0.0019325728062540293\n",
            "step: 430, loss: 6.737450894434005e-05\n",
            "step: 440, loss: 0.012971889227628708\n",
            "step: 450, loss: 0.00010554893378866836\n",
            "step: 460, loss: 0.00039779514190740883\n",
            "step: 470, loss: 0.0012703222455456853\n",
            "step: 480, loss: 0.0006772240740247071\n",
            "step: 490, loss: 0.0004273222293704748\n",
            "step: 500, loss: 0.032655611634254456\n",
            "step: 510, loss: 0.003163197310641408\n",
            "step: 520, loss: 0.00010220407421002164\n",
            "step: 530, loss: 8.157424599630758e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9049645390070921, f1=0.899624765478424, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008676313445903361\n",
            "step: 10, loss: 0.0006851975340396166\n",
            "step: 20, loss: 0.00016965520626399666\n",
            "step: 30, loss: 0.002202762523666024\n",
            "step: 40, loss: 0.01457873173058033\n",
            "step: 50, loss: 0.00827951543033123\n",
            "step: 60, loss: 0.10949289798736572\n",
            "step: 70, loss: 0.00015370994515251368\n",
            "step: 80, loss: 5.083488940726966e-05\n",
            "step: 90, loss: 0.00014331706915982068\n",
            "step: 100, loss: 0.00011411857849452645\n",
            "step: 110, loss: 0.0007467767572961748\n",
            "step: 120, loss: 0.008030190132558346\n",
            "step: 130, loss: 0.0012494346592575312\n",
            "step: 140, loss: 0.0002891597105190158\n",
            "step: 150, loss: 0.0014395692851394415\n",
            "step: 160, loss: 6.766614387743175e-05\n",
            "step: 170, loss: 6.105282955104485e-05\n",
            "step: 180, loss: 0.0008487323648296297\n",
            "step: 190, loss: 0.0008385212859138846\n",
            "step: 200, loss: 0.0002944681327790022\n",
            "step: 210, loss: 0.00011318797623971477\n",
            "step: 220, loss: 0.0009123371564783156\n",
            "step: 230, loss: 0.00014548914623446763\n",
            "step: 240, loss: 0.0001000093761831522\n",
            "step: 250, loss: 6.51989466859959e-05\n",
            "step: 260, loss: 3.435330290812999e-05\n",
            "step: 270, loss: 0.00011668509978335351\n",
            "step: 280, loss: 0.014529834501445293\n",
            "step: 290, loss: 8.19670021883212e-05\n",
            "step: 300, loss: 0.000226349409786053\n",
            "step: 310, loss: 4.4794101995648816e-05\n",
            "step: 320, loss: 5.499663529917598e-05\n",
            "step: 330, loss: 9.474795660935342e-05\n",
            "step: 340, loss: 6.289568409556523e-05\n",
            "step: 350, loss: 0.00011626882769633085\n",
            "step: 360, loss: 2.6128600438823923e-05\n",
            "step: 370, loss: 0.0001762302708812058\n",
            "step: 380, loss: 2.904539360315539e-05\n",
            "step: 390, loss: 3.391002974240109e-05\n",
            "step: 400, loss: 5.3535284678218886e-05\n",
            "step: 410, loss: 0.00219617853872478\n",
            "step: 420, loss: 0.00022536408505402505\n",
            "step: 430, loss: 3.63798244507052e-05\n",
            "step: 440, loss: 0.000312906748149544\n",
            "step: 450, loss: 5.1432110922178254e-05\n",
            "step: 460, loss: 0.07604539394378662\n",
            "step: 470, loss: 3.69305198546499e-05\n",
            "step: 480, loss: 0.0001205264707095921\n",
            "step: 490, loss: 0.0007117890636436641\n",
            "step: 500, loss: 0.00019556177721824497\n",
            "step: 510, loss: 4.327644273871556e-05\n",
            "step: 520, loss: 9.75517978076823e-05\n",
            "step: 530, loss: 0.00022600029478780925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8963503649635037, f1=0.8793436293436293, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.525386172346771e-05\n",
            "step: 10, loss: 2.9443719540722668e-05\n",
            "step: 20, loss: 8.91874369699508e-05\n",
            "step: 30, loss: 3.5101598768960685e-05\n",
            "step: 40, loss: 0.006304269190877676\n",
            "step: 50, loss: 0.004015741404145956\n",
            "step: 60, loss: 0.00023556224186904728\n",
            "step: 70, loss: 0.0006701695965602994\n",
            "step: 80, loss: 2.7592037440626882e-05\n",
            "step: 90, loss: 8.17969412310049e-05\n",
            "step: 100, loss: 0.004596303217113018\n",
            "step: 110, loss: 5.970377605990507e-05\n",
            "step: 120, loss: 3.17305421049241e-05\n",
            "step: 130, loss: 0.001228134031407535\n",
            "step: 140, loss: 0.0014703803462907672\n",
            "step: 150, loss: 5.492682248586789e-05\n",
            "step: 160, loss: 6.0098936955910176e-05\n",
            "step: 170, loss: 8.654796693008393e-05\n",
            "step: 180, loss: 4.392215487314388e-05\n",
            "step: 190, loss: 4.141864337725565e-05\n",
            "step: 200, loss: 0.0025979606434702873\n",
            "step: 210, loss: 0.00012295546184759587\n",
            "step: 220, loss: 7.767311035422608e-05\n",
            "step: 230, loss: 3.231120717828162e-05\n",
            "step: 240, loss: 9.510204108664766e-05\n",
            "step: 250, loss: 3.4226744901388884e-05\n",
            "step: 260, loss: 4.916509715258144e-05\n",
            "step: 270, loss: 7.54372522351332e-05\n",
            "step: 280, loss: 0.0017708134837448597\n",
            "step: 290, loss: 0.0007976074703037739\n",
            "step: 300, loss: 0.0012471232330426574\n",
            "step: 310, loss: 0.000434413377661258\n",
            "step: 320, loss: 3.798383113462478e-05\n",
            "step: 330, loss: 6.382331775967032e-05\n",
            "step: 340, loss: 6.006129115121439e-05\n",
            "step: 350, loss: 0.0018881751457229257\n",
            "step: 360, loss: 0.00013040051271673292\n",
            "step: 370, loss: 0.0003307789156679064\n",
            "step: 380, loss: 8.8680608314462e-05\n",
            "step: 390, loss: 0.00025736266979947686\n",
            "step: 400, loss: 4.4820248149335384e-05\n",
            "step: 410, loss: 0.00017747105448506773\n",
            "step: 420, loss: 0.02805846929550171\n",
            "step: 430, loss: 0.011089909821748734\n",
            "step: 440, loss: 9.772148769116029e-05\n",
            "step: 450, loss: 0.00022864325728733093\n",
            "step: 460, loss: 4.317805723985657e-05\n",
            "step: 470, loss: 0.00037116423482075334\n",
            "step: 480, loss: 0.00042619919986464083\n",
            "step: 490, loss: 0.014076890423893929\n",
            "step: 500, loss: 3.85129897040315e-05\n",
            "step: 510, loss: 0.0008898564847186208\n",
            "step: 520, loss: 0.0002672789851203561\n",
            "step: 530, loss: 6.75161209073849e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9096098953377736, f1=0.9006181645268663, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.530167204095051e-05\n",
            "step: 10, loss: 0.0001024241209961474\n",
            "step: 20, loss: 0.00042126711923629045\n",
            "step: 30, loss: 5.974684972898103e-05\n",
            "step: 40, loss: 3.537036900524981e-05\n",
            "step: 50, loss: 0.0018851141212508082\n",
            "step: 60, loss: 0.000488787773065269\n",
            "step: 70, loss: 4.12580520787742e-05\n",
            "step: 80, loss: 0.00010066706454381347\n",
            "step: 90, loss: 0.0014269277453422546\n",
            "step: 100, loss: 6.674870382994413e-05\n",
            "step: 110, loss: 3.897371425409801e-05\n",
            "step: 120, loss: 2.382256388955284e-05\n",
            "step: 130, loss: 5.039535972173326e-05\n",
            "step: 140, loss: 3.624188684625551e-05\n",
            "step: 150, loss: 0.00023651207447983325\n",
            "step: 160, loss: 4.3973512219963595e-05\n",
            "step: 170, loss: 0.0043534450232982635\n",
            "step: 180, loss: 8.805078687146306e-05\n",
            "step: 190, loss: 0.00010353376273997128\n",
            "step: 200, loss: 4.8717905883677304e-05\n",
            "step: 210, loss: 3.088510493398644e-05\n",
            "step: 220, loss: 8.750405686441809e-05\n",
            "step: 230, loss: 7.314908725675195e-05\n",
            "step: 240, loss: 0.0017805384704843163\n",
            "step: 250, loss: 3.0602044716943055e-05\n",
            "step: 260, loss: 2.878781560866628e-05\n",
            "step: 270, loss: 3.253898466937244e-05\n",
            "step: 280, loss: 3.998144165962003e-05\n",
            "step: 290, loss: 3.1428164220415056e-05\n",
            "step: 300, loss: 4.640278348233551e-05\n",
            "step: 310, loss: 0.000928649737033993\n",
            "step: 320, loss: 0.00127583765424788\n",
            "step: 330, loss: 2.6582916689221747e-05\n",
            "step: 340, loss: 4.986633939552121e-05\n",
            "step: 350, loss: 3.449854557402432e-05\n",
            "step: 360, loss: 3.827160981018096e-05\n",
            "step: 370, loss: 4.46114172518719e-05\n",
            "step: 380, loss: 0.00013176097127143294\n",
            "step: 390, loss: 8.799511124379933e-05\n",
            "step: 400, loss: 2.383365972491447e-05\n",
            "step: 410, loss: 3.158548861392774e-05\n",
            "step: 420, loss: 0.00018270053260494024\n",
            "step: 430, loss: 0.00010483295045560226\n",
            "step: 440, loss: 9.265793778467923e-05\n",
            "step: 450, loss: 3.6498568078968674e-05\n",
            "step: 460, loss: 0.0007317697745747864\n",
            "step: 470, loss: 1.8890643332269974e-05\n",
            "step: 480, loss: 1.9661627447931096e-05\n",
            "step: 490, loss: 0.00012804257858078927\n",
            "step: 500, loss: 1.5496982086915523e-05\n",
            "step: 510, loss: 0.00012904613686259836\n",
            "step: 520, loss: 0.00838453695178032\n",
            "step: 530, loss: 2.236610271211248e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9060402684563759, f1=0.8977650974797908, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001879075716715306\n",
            "step: 10, loss: 5.0281829317100346e-05\n",
            "step: 20, loss: 0.00018034572713077068\n",
            "step: 30, loss: 7.383129559457302e-05\n",
            "step: 40, loss: 3.67821121471934e-05\n",
            "step: 50, loss: 6.984344508964568e-05\n",
            "step: 60, loss: 3.610967178246938e-05\n",
            "step: 70, loss: 4.2806648707482964e-05\n",
            "step: 80, loss: 0.00026506639551371336\n",
            "step: 90, loss: 3.2892858143895864e-05\n",
            "step: 100, loss: 0.00018273040768690407\n",
            "step: 110, loss: 2.2444393835030496e-05\n",
            "step: 120, loss: 3.9563336031278595e-05\n",
            "step: 130, loss: 0.0007489445270039141\n",
            "step: 140, loss: 0.00022063724463805556\n",
            "step: 150, loss: 2.8355505492072552e-05\n",
            "step: 160, loss: 4.899982741335407e-05\n",
            "step: 170, loss: 2.6933128538075835e-05\n",
            "step: 180, loss: 0.0005363999516703188\n",
            "step: 190, loss: 0.0005667750956490636\n",
            "step: 200, loss: 7.040639320621267e-05\n",
            "step: 210, loss: 0.00010047260002465919\n",
            "step: 220, loss: 3.4884353226516396e-05\n",
            "step: 230, loss: 0.00014892089529894292\n",
            "step: 240, loss: 0.0002466789446771145\n",
            "step: 250, loss: 7.10891472408548e-05\n",
            "step: 260, loss: 2.91866763291182e-05\n",
            "step: 270, loss: 2.789009886328131e-05\n",
            "step: 280, loss: 3.877202470903285e-05\n",
            "step: 290, loss: 0.0006571727572008967\n",
            "step: 300, loss: 2.593856152088847e-05\n",
            "step: 310, loss: 0.00013692385982722044\n",
            "step: 320, loss: 0.00010879508772632107\n",
            "step: 330, loss: 0.049714308232069016\n",
            "step: 340, loss: 8.991877984954044e-05\n",
            "step: 350, loss: 4.0475479181623086e-05\n",
            "step: 360, loss: 0.001993398880586028\n",
            "step: 370, loss: 3.338076203363016e-05\n",
            "step: 380, loss: 3.996131272288039e-05\n",
            "step: 390, loss: 0.022813603281974792\n",
            "step: 400, loss: 6.0853504692204297e-05\n",
            "step: 410, loss: 2.301432323292829e-05\n",
            "step: 420, loss: 2.3003054593573324e-05\n",
            "step: 430, loss: 3.8726284401491284e-05\n",
            "step: 440, loss: 0.002235251944512129\n",
            "step: 450, loss: 4.388084926176816e-05\n",
            "step: 460, loss: 3.495920464047231e-05\n",
            "step: 470, loss: 0.0001639610854908824\n",
            "step: 480, loss: 0.0001767695212038234\n",
            "step: 490, loss: 2.6839670681511052e-05\n",
            "step: 500, loss: 1.95984284800943e-05\n",
            "step: 510, loss: 0.00023550588230136782\n",
            "step: 520, loss: 1.7232961909030564e-05\n",
            "step: 530, loss: 3.300800744909793e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9078080903104421, f1=0.9039812646370025, best_f1=0.9110386585933862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.6575518859317526e-05\n",
            "step: 10, loss: 0.0002910557668656111\n",
            "step: 20, loss: 2.290263000759296e-05\n",
            "step: 30, loss: 0.0009789889445528388\n",
            "step: 40, loss: 0.027383161708712578\n",
            "step: 50, loss: 5.025218706578016e-05\n",
            "step: 60, loss: 1.474078726459993e-05\n",
            "step: 70, loss: 0.00021863561414647847\n",
            "step: 80, loss: 0.0004995037452317774\n",
            "step: 90, loss: 3.956828368245624e-05\n",
            "step: 100, loss: 0.002054460346698761\n",
            "step: 110, loss: 0.0016024120850488544\n",
            "step: 120, loss: 0.0013309863861650229\n",
            "step: 130, loss: 0.018934620544314384\n",
            "step: 140, loss: 2.9289471058291383e-05\n",
            "step: 150, loss: 3.8859929190948606e-05\n",
            "step: 160, loss: 9.664092067396268e-05\n",
            "step: 170, loss: 2.5294018996646628e-05\n",
            "step: 180, loss: 2.1092115275678225e-05\n",
            "step: 190, loss: 5.321435310179368e-05\n",
            "step: 200, loss: 0.00030000830884091556\n",
            "step: 210, loss: 0.0002753364678937942\n",
            "step: 220, loss: 2.1557694708462805e-05\n",
            "step: 230, loss: 5.238388985162601e-05\n",
            "step: 240, loss: 2.246674739581067e-05\n",
            "step: 250, loss: 2.8046590159647167e-05\n",
            "step: 260, loss: 4.250480196787976e-05\n",
            "step: 270, loss: 3.0017410608706996e-05\n",
            "step: 280, loss: 0.0003507058136165142\n",
            "step: 290, loss: 2.1013926016166806e-05\n",
            "step: 300, loss: 3.038151771761477e-05\n",
            "step: 310, loss: 0.0005897688679397106\n",
            "step: 320, loss: 0.0009929003426805139\n",
            "step: 330, loss: 3.2874006137717515e-05\n",
            "step: 340, loss: 2.5803830794757232e-05\n",
            "step: 350, loss: 1.728483766783029e-05\n",
            "step: 360, loss: 4.7382767661474645e-05\n",
            "step: 370, loss: 1.6368727301596664e-05\n",
            "step: 380, loss: 2.626269997563213e-05\n",
            "step: 390, loss: 8.167007763404399e-05\n",
            "step: 400, loss: 4.759906732942909e-05\n",
            "step: 410, loss: 2.571870390966069e-05\n",
            "step: 420, loss: 4.1607534512877464e-05\n",
            "step: 430, loss: 0.0007693520747125149\n",
            "step: 440, loss: 2.533130464144051e-05\n",
            "step: 450, loss: 2.7040379791287705e-05\n",
            "step: 460, loss: 3.180884959874675e-05\n",
            "step: 470, loss: 0.000394217116991058\n",
            "step: 480, loss: 1.8115812054020353e-05\n",
            "step: 490, loss: 2.545425741118379e-05\n",
            "step: 500, loss: 0.004938072524964809\n",
            "step: 510, loss: 3.670740261441097e-05\n",
            "step: 520, loss: 2.0857462004642002e-05\n",
            "step: 530, loss: 2.249657154607121e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9075391180654339, f1=0.9035916824196598, best_f1=0.9110386585933862\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 254.21it/s]\n",
            "load_f1 = 0.9147141518275539\n",
            "real_f1 = 0.9094287041337669\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 265.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca998e8-1737-4053-d938-c06585823944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5426797866821289\n",
            "step: 10, loss: 0.3923998475074768\n",
            "step: 20, loss: 0.3950772285461426\n",
            "step: 30, loss: 0.30407851934432983\n",
            "step: 40, loss: 0.15551745891571045\n",
            "step: 50, loss: 0.4896179437637329\n",
            "step: 60, loss: 0.30525586009025574\n",
            "step: 70, loss: 0.2600313425064087\n",
            "step: 80, loss: 0.22365477681159973\n",
            "step: 90, loss: 0.48140695691108704\n",
            "step: 100, loss: 0.5283660292625427\n",
            "step: 110, loss: 0.3026462495326996\n",
            "step: 120, loss: 0.30361634492874146\n",
            "step: 130, loss: 0.4376980662345886\n",
            "step: 140, loss: 0.22929660975933075\n",
            "step: 150, loss: 0.36233311891555786\n",
            "step: 160, loss: 0.2941146790981293\n",
            "step: 170, loss: 0.355407178401947\n",
            "step: 180, loss: 0.21316996216773987\n",
            "step: 190, loss: 0.2522808611392975\n",
            "step: 200, loss: 0.349767804145813\n",
            "step: 210, loss: 0.25462666153907776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.41234567901234576, f1=0.42051282051282046, best_f1=0.42051282051282046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17819936573505402\n",
            "step: 10, loss: 0.4076669216156006\n",
            "step: 20, loss: 0.2884460389614105\n",
            "step: 30, loss: 0.3635062873363495\n",
            "step: 40, loss: 0.22958897054195404\n",
            "step: 50, loss: 0.15024986863136292\n",
            "step: 60, loss: 0.47098755836486816\n",
            "step: 70, loss: 0.153959259390831\n",
            "step: 80, loss: 0.2959614098072052\n",
            "step: 90, loss: 0.11744856834411621\n",
            "step: 100, loss: 0.0408531092107296\n",
            "step: 110, loss: 0.1835208237171173\n",
            "step: 120, loss: 0.1610432118177414\n",
            "step: 130, loss: 0.055518172681331635\n",
            "step: 140, loss: 0.22950586676597595\n",
            "step: 150, loss: 0.24987098574638367\n",
            "step: 160, loss: 0.2344844937324524\n",
            "step: 170, loss: 0.16472913324832916\n",
            "step: 180, loss: 0.2561550736427307\n",
            "step: 190, loss: 0.25989991426467896\n",
            "step: 200, loss: 0.058524757623672485\n",
            "step: 210, loss: 0.22734874486923218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4333333333333333, f1=0.49218750000000006, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08735973387956619\n",
            "step: 10, loss: 0.34220045804977417\n",
            "step: 20, loss: 0.19989433884620667\n",
            "step: 30, loss: 0.21439698338508606\n",
            "step: 40, loss: 0.08840638399124146\n",
            "step: 50, loss: 0.10576336830854416\n",
            "step: 60, loss: 0.12677665054798126\n",
            "step: 70, loss: 0.33543044328689575\n",
            "step: 80, loss: 0.1408807933330536\n",
            "step: 90, loss: 0.12940475344657898\n",
            "step: 100, loss: 0.13640831410884857\n",
            "step: 110, loss: 0.3022952973842621\n",
            "step: 120, loss: 0.1825467199087143\n",
            "step: 130, loss: 0.14052678644657135\n",
            "step: 140, loss: 0.13494467735290527\n",
            "step: 150, loss: 0.245692640542984\n",
            "step: 160, loss: 0.04371216148138046\n",
            "step: 170, loss: 0.1540675014257431\n",
            "step: 180, loss: 0.15874351561069489\n",
            "step: 190, loss: 0.3344349265098572\n",
            "step: 200, loss: 0.15391311049461365\n",
            "step: 210, loss: 0.10943134129047394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.481675392670157, f1=0.5501730103806229, best_f1=0.5501730103806229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21451805531978607\n",
            "step: 10, loss: 0.21123136579990387\n",
            "step: 20, loss: 0.07878239452838898\n",
            "step: 30, loss: 0.15986643731594086\n",
            "step: 40, loss: 0.052230652421712875\n",
            "step: 50, loss: 0.21481625735759735\n",
            "step: 60, loss: 0.23839086294174194\n",
            "step: 70, loss: 0.20703968405723572\n",
            "step: 80, loss: 0.11679672449827194\n",
            "step: 90, loss: 0.04422552138566971\n",
            "step: 100, loss: 0.3668478727340698\n",
            "step: 110, loss: 0.17491081357002258\n",
            "step: 120, loss: 0.18916144967079163\n",
            "step: 130, loss: 0.10921388119459152\n",
            "step: 140, loss: 0.24669653177261353\n",
            "step: 150, loss: 0.06275449693202972\n",
            "step: 160, loss: 0.06088748201727867\n",
            "step: 170, loss: 0.20628094673156738\n",
            "step: 180, loss: 0.23743271827697754\n",
            "step: 190, loss: 0.1325361132621765\n",
            "step: 200, loss: 0.24484850466251373\n",
            "step: 210, loss: 0.282761812210083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.49612403100775193, f1=0.505091649694501, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14892442524433136\n",
            "step: 10, loss: 0.06270725280046463\n",
            "step: 20, loss: 0.33654868602752686\n",
            "step: 30, loss: 0.03935299441218376\n",
            "step: 40, loss: 0.06843771785497665\n",
            "step: 50, loss: 0.14941994845867157\n",
            "step: 60, loss: 0.08641224354505539\n",
            "step: 70, loss: 0.03504486009478569\n",
            "step: 80, loss: 0.03760456293821335\n",
            "step: 90, loss: 0.07081558555364609\n",
            "step: 100, loss: 0.021763980388641357\n",
            "step: 110, loss: 0.19513729214668274\n",
            "step: 120, loss: 0.09140560030937195\n",
            "step: 130, loss: 0.11524120718240738\n",
            "step: 140, loss: 0.16212211549282074\n",
            "step: 150, loss: 0.0978723019361496\n",
            "step: 160, loss: 0.11911187320947647\n",
            "step: 170, loss: 0.08491872996091843\n",
            "step: 180, loss: 0.07323506474494934\n",
            "step: 190, loss: 0.02328369952738285\n",
            "step: 200, loss: 0.20881254971027374\n",
            "step: 210, loss: 0.03794702887535095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4747081712062257, f1=0.5156250000000001, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07848545163869858\n",
            "step: 10, loss: 0.027983125299215317\n",
            "step: 20, loss: 0.07775142788887024\n",
            "step: 30, loss: 0.004306306131184101\n",
            "step: 40, loss: 0.05558066815137863\n",
            "step: 50, loss: 0.008415297605097294\n",
            "step: 60, loss: 0.033098042011260986\n",
            "step: 70, loss: 0.00888659618794918\n",
            "step: 80, loss: 0.008771886117756367\n",
            "step: 90, loss: 0.20177745819091797\n",
            "step: 100, loss: 0.010179208591580391\n",
            "step: 110, loss: 0.06651108711957932\n",
            "step: 120, loss: 0.04549430310726166\n",
            "step: 130, loss: 0.07970115542411804\n",
            "step: 140, loss: 0.042611442506313324\n",
            "step: 150, loss: 0.051211901009082794\n",
            "step: 160, loss: 0.004759099800139666\n",
            "step: 170, loss: 0.17464500665664673\n",
            "step: 180, loss: 0.1754651963710785\n",
            "step: 190, loss: 0.14118723571300507\n",
            "step: 200, loss: 0.05590846762061119\n",
            "step: 210, loss: 0.03986458480358124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4729458917835671, f1=0.5000000000000001, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07782814651727676\n",
            "step: 10, loss: 0.018723521381616592\n",
            "step: 20, loss: 0.0351259745657444\n",
            "step: 30, loss: 0.08481337130069733\n",
            "step: 40, loss: 0.013196388259530067\n",
            "step: 50, loss: 0.17918719351291656\n",
            "step: 60, loss: 0.01157052256166935\n",
            "step: 70, loss: 0.03734205290675163\n",
            "step: 80, loss: 0.1152462437748909\n",
            "step: 90, loss: 0.03200991451740265\n",
            "step: 100, loss: 0.003628677921369672\n",
            "step: 110, loss: 0.21404916048049927\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.10006233304738998\n",
            "step: 130, loss: 0.08105587214231491\n",
            "step: 140, loss: 0.053723566234111786\n",
            "step: 150, loss: 0.04881145805120468\n",
            "step: 160, loss: 0.015621690079569817\n",
            "step: 170, loss: 0.001384107512421906\n",
            "step: 180, loss: 0.038077834993600845\n",
            "step: 190, loss: 0.00578369852155447\n",
            "step: 200, loss: 0.016077658161520958\n",
            "step: 210, loss: 0.05945159122347832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4714828897338402, f1=0.5265225933202357, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02164926566183567\n",
            "step: 10, loss: 0.05822917819023132\n",
            "step: 20, loss: 0.0025538827758282423\n",
            "step: 30, loss: 0.00611715717241168\n",
            "step: 40, loss: 0.09965914487838745\n",
            "step: 50, loss: 0.004360053688287735\n",
            "step: 60, loss: 0.005616145674139261\n",
            "step: 70, loss: 0.02029598504304886\n",
            "step: 80, loss: 0.035412829369306564\n",
            "step: 90, loss: 0.03789268061518669\n",
            "step: 100, loss: 0.111116923391819\n",
            "step: 110, loss: 0.130131334066391\n",
            "step: 120, loss: 0.02298450842499733\n",
            "step: 130, loss: 0.03069094941020012\n",
            "step: 140, loss: 0.06964189559221268\n",
            "step: 150, loss: 0.08773119747638702\n",
            "step: 160, loss: 0.3261536657810211\n",
            "step: 170, loss: 0.005899264942854643\n",
            "step: 180, loss: 0.06650515645742416\n",
            "step: 190, loss: 0.009789623320102692\n",
            "step: 200, loss: 0.0030400315299630165\n",
            "step: 210, loss: 0.08760897070169449\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.48163265306122444, f1=0.5409836065573771, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.039012789726257324\n",
            "step: 10, loss: 0.017656484618782997\n",
            "step: 20, loss: 0.006377456244081259\n",
            "step: 30, loss: 0.010090194642543793\n",
            "step: 40, loss: 0.1197633296251297\n",
            "step: 50, loss: 0.04368444159626961\n",
            "step: 60, loss: 0.08369807153940201\n",
            "step: 70, loss: 0.02228277549147606\n",
            "step: 80, loss: 0.0016394879203289747\n",
            "step: 90, loss: 0.0016079063061624765\n",
            "step: 100, loss: 0.009774786420166492\n",
            "step: 110, loss: 0.004713958129286766\n",
            "step: 120, loss: 0.0002979193232022226\n",
            "step: 130, loss: 0.008427446708083153\n",
            "step: 140, loss: 0.003709869459271431\n",
            "step: 150, loss: 0.061958275735378265\n",
            "step: 160, loss: 0.04424503073096275\n",
            "step: 170, loss: 0.05959422513842583\n",
            "step: 180, loss: 0.008964551612734795\n",
            "step: 190, loss: 0.0008269580430351198\n",
            "step: 200, loss: 0.06743938475847244\n",
            "step: 210, loss: 0.0026386112440377474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.4222222222222222, f1=0.48106904231625836, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08231578767299652\n",
            "step: 10, loss: 0.027477331459522247\n",
            "step: 20, loss: 0.004044689238071442\n",
            "step: 30, loss: 0.0571649968624115\n",
            "step: 40, loss: 0.005517024081200361\n",
            "step: 50, loss: 0.013484789058566093\n",
            "step: 60, loss: 0.001250320696271956\n",
            "step: 70, loss: 0.015206036157906055\n",
            "step: 80, loss: 0.01703428104519844\n",
            "step: 90, loss: 0.005543222185224295\n",
            "step: 100, loss: 0.04498358815908432\n",
            "step: 110, loss: 0.0008644724148325622\n",
            "step: 120, loss: 0.11407414078712463\n",
            "step: 130, loss: 0.0170478206127882\n",
            "step: 140, loss: 0.00036279112100601196\n",
            "step: 150, loss: 0.012391861528158188\n",
            "step: 160, loss: 0.0027786321006715298\n",
            "step: 170, loss: 0.0013204109854996204\n",
            "step: 180, loss: 0.021040627732872963\n",
            "step: 190, loss: 0.03897082805633545\n",
            "step: 200, loss: 0.0019116714829578996\n",
            "step: 210, loss: 0.0392032153904438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.46189376443418007, f1=0.5046296296296297, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005312936380505562\n",
            "step: 10, loss: 0.04680482670664787\n",
            "step: 20, loss: 0.001249426626600325\n",
            "step: 30, loss: 0.04293788596987724\n",
            "step: 40, loss: 0.006174099165946245\n",
            "step: 50, loss: 0.021346716210246086\n",
            "step: 60, loss: 0.0014023862313479185\n",
            "step: 70, loss: 0.0016508324770256877\n",
            "step: 80, loss: 0.015383332967758179\n",
            "step: 90, loss: 0.014797812327742577\n",
            "step: 100, loss: 0.021555010229349136\n",
            "step: 110, loss: 0.008500649593770504\n",
            "step: 120, loss: 0.029028022661805153\n",
            "step: 130, loss: 0.0022770778741687536\n",
            "step: 140, loss: 0.0013822013279423118\n",
            "step: 150, loss: 0.00012411335774231702\n",
            "step: 160, loss: 0.003914321307092905\n",
            "step: 170, loss: 0.001786491833627224\n",
            "step: 180, loss: 0.018365759402513504\n",
            "step: 190, loss: 0.0015768560115247965\n",
            "step: 200, loss: 0.024192288517951965\n",
            "step: 210, loss: 0.0022031180560588837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.45585215605749485, f1=0.5220883534136546, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001655207364819944\n",
            "step: 10, loss: 0.007558510173112154\n",
            "step: 20, loss: 0.0564366951584816\n",
            "step: 30, loss: 0.0035009668208658695\n",
            "step: 40, loss: 0.01663283072412014\n",
            "step: 50, loss: 0.00024393032072111964\n",
            "step: 60, loss: 0.005604021716862917\n",
            "step: 70, loss: 0.01993144303560257\n",
            "step: 80, loss: 0.0002491529448889196\n",
            "step: 90, loss: 0.029563257470726967\n",
            "step: 100, loss: 0.0023146390449255705\n",
            "step: 110, loss: 0.00522651057690382\n",
            "step: 120, loss: 0.0008352509466931224\n",
            "step: 130, loss: 0.0008349798736162484\n",
            "step: 140, loss: 0.0002833227044902742\n",
            "step: 150, loss: 0.002494232030585408\n",
            "step: 160, loss: 0.0003075031563639641\n",
            "step: 170, loss: 0.00018096662824973464\n",
            "step: 180, loss: 0.01673831418156624\n",
            "step: 190, loss: 0.0011949193431064487\n",
            "step: 200, loss: 0.0002586132031865418\n",
            "step: 210, loss: 0.0024699338246136904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.44052863436123346, f1=0.5161290322580645, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015394758433103561\n",
            "step: 10, loss: 0.0002151781227439642\n",
            "step: 20, loss: 0.0002111108769895509\n",
            "step: 30, loss: 0.001651874976232648\n",
            "step: 40, loss: 0.00017491051403339952\n",
            "step: 50, loss: 0.0008790877182036638\n",
            "step: 60, loss: 0.0005668403464369476\n",
            "step: 70, loss: 0.002955404343083501\n",
            "step: 80, loss: 0.0013564785476773977\n",
            "step: 90, loss: 0.00023272463295143098\n",
            "step: 100, loss: 0.0001713066449156031\n",
            "step: 110, loss: 0.00022641710529569536\n",
            "step: 120, loss: 0.022184329107403755\n",
            "step: 130, loss: 0.00020380491332616657\n",
            "step: 140, loss: 0.00014093164645601064\n",
            "step: 150, loss: 0.0012819968396797776\n",
            "step: 160, loss: 0.0005831727758049965\n",
            "step: 170, loss: 0.018146540969610214\n",
            "step: 180, loss: 0.0013225963339209557\n",
            "step: 190, loss: 0.0010838336311280727\n",
            "step: 200, loss: 0.008309069089591503\n",
            "step: 210, loss: 0.0007335490663535893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.43280182232346237, f1=0.49777777777777776, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007095240871421993\n",
            "step: 10, loss: 0.04485756903886795\n",
            "step: 20, loss: 0.0013171248137950897\n",
            "step: 30, loss: 0.004214869346469641\n",
            "step: 40, loss: 0.00014857131463941187\n",
            "step: 50, loss: 0.0013864220818504691\n",
            "step: 60, loss: 0.007738546002656221\n",
            "step: 70, loss: 0.0006319441017694771\n",
            "step: 80, loss: 0.0012670642463490367\n",
            "step: 90, loss: 0.0001706987532088533\n",
            "step: 100, loss: 0.0005300985067151487\n",
            "step: 110, loss: 0.0001973291509784758\n",
            "step: 120, loss: 0.0015170825645327568\n",
            "step: 130, loss: 0.0009758021333254874\n",
            "step: 140, loss: 0.01429979782551527\n",
            "step: 150, loss: 0.0002626234490890056\n",
            "step: 160, loss: 0.00018237385665997863\n",
            "step: 170, loss: 0.00021152796398382634\n",
            "step: 180, loss: 0.09181705117225647\n",
            "step: 190, loss: 0.0007036340539343655\n",
            "step: 200, loss: 0.0008369316346943378\n",
            "step: 210, loss: 0.0060887872241437435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.4403669724770642, f1=0.49549549549549554, best_f1=0.505091649694501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026720267487689853\n",
            "step: 10, loss: 0.00023107031302060932\n",
            "step: 20, loss: 0.0004475457826629281\n",
            "step: 30, loss: 0.00795670971274376\n",
            "step: 40, loss: 0.0005005674320273101\n",
            "step: 50, loss: 0.00023686785425525159\n",
            "step: 60, loss: 0.05950000137090683\n",
            "step: 70, loss: 0.003180940169841051\n",
            "step: 80, loss: 0.00027246124227531254\n",
            "step: 90, loss: 0.00287640537135303\n",
            "step: 100, loss: 0.00019920972408726811\n",
            "step: 110, loss: 0.00013743434101343155\n",
            "step: 120, loss: 0.00021299743093550205\n",
            "step: 130, loss: 0.00020418014901224524\n",
            "step: 140, loss: 0.00010988357098540291\n",
            "step: 150, loss: 0.00010398389713373035\n",
            "step: 160, loss: 0.0022685816511511803\n",
            "step: 170, loss: 0.0006768582388758659\n",
            "step: 180, loss: 0.0010077544720843434\n",
            "step: 190, loss: 0.0001915024040499702\n",
            "step: 200, loss: 0.018787279725074768\n",
            "step: 210, loss: 0.0014967890456318855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.4409090909090909, f1=0.5077262693156732, best_f1=0.505091649694501\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 433.88it/s]\n",
            "load_f1 = 0.48565965583174\n",
            "real_f1 = 0.4980544747081712\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 265.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3171a66-7808-4d12-c31a-a0374615d7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5551677346229553\n",
            "step: 10, loss: 0.38359037041664124\n",
            "step: 20, loss: 0.29695695638656616\n",
            "step: 30, loss: 0.44376951456069946\n",
            "step: 40, loss: 0.42356643080711365\n",
            "step: 50, loss: 0.3159785270690918\n",
            "step: 60, loss: 0.3166933059692383\n",
            "step: 70, loss: 0.3018368184566498\n",
            "step: 80, loss: 0.23721438646316528\n",
            "step: 90, loss: 0.29733479022979736\n",
            "step: 100, loss: 0.27518168091773987\n",
            "step: 110, loss: 0.5228491425514221\n",
            "step: 120, loss: 0.21038775146007538\n",
            "step: 130, loss: 0.16287943720817566\n",
            "step: 140, loss: 0.09186311066150665\n",
            "step: 150, loss: 0.13551662862300873\n",
            "step: 160, loss: 0.15972062945365906\n",
            "step: 170, loss: 0.25266143679618835\n",
            "step: 180, loss: 0.0436091274023056\n",
            "step: 190, loss: 0.3854607045650482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.640552995391705, f1=0.6428571428571428, best_f1=0.6428571428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2221403270959854\n",
            "step: 10, loss: 0.10855066776275635\n",
            "step: 20, loss: 0.16377851366996765\n",
            "step: 30, loss: 0.1345938742160797\n",
            "step: 40, loss: 0.14333857595920563\n",
            "step: 50, loss: 0.07474817335605621\n",
            "step: 60, loss: 0.26793864369392395\n",
            "step: 70, loss: 0.22370386123657227\n",
            "step: 80, loss: 0.1473010778427124\n",
            "step: 90, loss: 0.11567679047584534\n",
            "step: 100, loss: 0.022125648334622383\n",
            "step: 110, loss: 0.2648455500602722\n",
            "step: 120, loss: 0.2390345185995102\n",
            "step: 130, loss: 0.13395695388317108\n",
            "step: 140, loss: 0.08726227283477783\n",
            "step: 150, loss: 0.1918906271457672\n",
            "step: 160, loss: 0.06935739517211914\n",
            "step: 170, loss: 0.18284252285957336\n",
            "step: 180, loss: 0.13743746280670166\n",
            "step: 190, loss: 0.05020506680011749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7354497354497355, f1=0.7098445595854923, best_f1=0.7098445595854923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04093864932656288\n",
            "step: 10, loss: 0.24724172055721283\n",
            "step: 20, loss: 0.21029512584209442\n",
            "step: 30, loss: 0.2690320312976837\n",
            "step: 40, loss: 0.11733613908290863\n",
            "step: 50, loss: 0.2099025845527649\n",
            "step: 60, loss: 0.014912514947354794\n",
            "step: 70, loss: 0.11107493191957474\n",
            "step: 80, loss: 0.02799360081553459\n",
            "step: 90, loss: 0.089402936398983\n",
            "step: 100, loss: 0.10025516897439957\n",
            "step: 110, loss: 0.009232338517904282\n",
            "step: 120, loss: 0.08510759472846985\n",
            "step: 130, loss: 0.014243008568882942\n",
            "step: 140, loss: 0.05518047511577606\n",
            "step: 150, loss: 0.11189482361078262\n",
            "step: 160, loss: 0.02781740017235279\n",
            "step: 170, loss: 0.2009870558977127\n",
            "step: 180, loss: 0.018007805570960045\n",
            "step: 190, loss: 0.11911854147911072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7857142857142857, f1=0.7675675675675676, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05300365760922432\n",
            "step: 10, loss: 0.15596450865268707\n",
            "step: 20, loss: 0.07253675162792206\n",
            "step: 30, loss: 0.09168107062578201\n",
            "step: 40, loss: 0.08551938086748123\n",
            "step: 50, loss: 0.043563008308410645\n",
            "step: 60, loss: 0.1160314679145813\n",
            "step: 70, loss: 0.07791784405708313\n",
            "step: 80, loss: 0.13772980868816376\n",
            "step: 90, loss: 0.05422919988632202\n",
            "step: 100, loss: 0.1084827110171318\n",
            "step: 110, loss: 0.047379299998283386\n",
            "step: 120, loss: 0.010984751395881176\n",
            "step: 130, loss: 0.04561397060751915\n",
            "step: 140, loss: 0.01655276119709015\n",
            "step: 150, loss: 0.01694018766283989\n",
            "step: 160, loss: 0.017888305708765984\n",
            "step: 170, loss: 0.17542608082294464\n",
            "step: 180, loss: 0.022888291627168655\n",
            "step: 190, loss: 0.09803203493356705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7712765957446809, f1=0.761904761904762, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.044922854751348495\n",
            "step: 10, loss: 0.015385533683001995\n",
            "step: 20, loss: 0.040202200412750244\n",
            "step: 30, loss: 0.011408987455070019\n",
            "step: 40, loss: 0.02331758663058281\n",
            "step: 50, loss: 0.19494660198688507\n",
            "step: 60, loss: 0.08943741768598557\n",
            "step: 70, loss: 0.009465666487812996\n",
            "step: 80, loss: 0.014021703042089939\n",
            "step: 90, loss: 0.009402510710060596\n",
            "step: 100, loss: 0.0025155607145279646\n",
            "step: 110, loss: 0.004278331529349089\n",
            "step: 120, loss: 0.001849177642725408\n",
            "step: 130, loss: 0.011202544905245304\n",
            "step: 140, loss: 0.37154102325439453\n",
            "step: 150, loss: 0.09226171672344208\n",
            "step: 160, loss: 0.14680345356464386\n",
            "step: 170, loss: 0.07251042872667313\n",
            "step: 180, loss: 0.12537366151809692\n",
            "step: 190, loss: 0.2049197107553482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7232876712328767, f1=0.7353760445682451, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06896695494651794\n",
            "step: 10, loss: 0.03170429542660713\n",
            "step: 20, loss: 0.0036820622626692057\n",
            "step: 30, loss: 0.10992524772882462\n",
            "step: 40, loss: 0.005419062916189432\n",
            "step: 50, loss: 0.02198847010731697\n",
            "step: 60, loss: 0.0019710955675691366\n",
            "step: 70, loss: 0.015240645967423916\n",
            "step: 80, loss: 0.011301778256893158\n",
            "step: 90, loss: 0.005617510061711073\n",
            "step: 100, loss: 0.0005458440282382071\n",
            "step: 110, loss: 0.005616160575300455\n",
            "step: 120, loss: 0.19570060074329376\n",
            "step: 130, loss: 0.05123564973473549\n",
            "step: 140, loss: 0.005030622705817223\n",
            "step: 150, loss: 0.005234931129962206\n",
            "step: 160, loss: 0.021166659891605377\n",
            "step: 170, loss: 0.05919670686125755\n",
            "step: 180, loss: 0.075951486825943\n",
            "step: 190, loss: 0.11114110797643661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7642276422764227, f1=0.7527472527472527, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009750358760356903\n",
            "step: 10, loss: 0.005962917115539312\n",
            "step: 20, loss: 0.021210290491580963\n",
            "step: 30, loss: 0.014474866911768913\n",
            "step: 40, loss: 0.04917248710989952\n",
            "step: 50, loss: 0.07992348819971085\n",
            "step: 60, loss: 0.0020647633355110884\n",
            "step: 70, loss: 0.0020983752328902483\n",
            "step: 80, loss: 0.05156882852315903\n",
            "step: 90, loss: 0.0010492000728845596\n",
            "step: 100, loss: 0.0014665351482108235\n",
            "step: 110, loss: 0.003760933643206954\n",
            "step: 120, loss: 0.026601428166031837\n",
            "step: 130, loss: 0.001510730478912592\n",
            "step: 140, loss: 0.024633260443806648\n",
            "step: 150, loss: 0.1977282464504242\n",
            "step: 160, loss: 0.10265371948480606\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.022301912307739258\n",
            "step: 180, loss: 0.0023347476962953806\n",
            "step: 190, loss: 0.0016348259523510933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.752808988764045, f1=0.7272727272727273, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007292592898011208\n",
            "step: 10, loss: 0.005497207399457693\n",
            "step: 20, loss: 0.0009310589521192014\n",
            "step: 30, loss: 0.0022910558618605137\n",
            "step: 40, loss: 0.0005309623084031045\n",
            "step: 50, loss: 0.000429505598731339\n",
            "step: 60, loss: 0.0009296414791606367\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.0017058803932741284\n",
            "step: 80, loss: 0.00031516465242020786\n",
            "step: 90, loss: 0.003367889206856489\n",
            "step: 100, loss: 0.042944904416799545\n",
            "step: 110, loss: 0.0007353078108280897\n",
            "step: 120, loss: 0.0020421352237462997\n",
            "step: 130, loss: 0.03720752149820328\n",
            "step: 140, loss: 0.010813808999955654\n",
            "step: 150, loss: 0.05591299384832382\n",
            "step: 160, loss: 0.022632071748375893\n",
            "step: 170, loss: 0.015405556187033653\n",
            "step: 180, loss: 0.07490944862365723\n",
            "step: 190, loss: 0.0021211798302829266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7828418230563003, f1=0.7439353099730459, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03373706713318825\n",
            "step: 10, loss: 0.02128874510526657\n",
            "step: 20, loss: 0.08927545696496964\n",
            "step: 30, loss: 0.009721996262669563\n",
            "step: 40, loss: 0.0005860962555743754\n",
            "step: 50, loss: 0.00043523142812773585\n",
            "step: 60, loss: 0.00030168838566169143\n",
            "step: 70, loss: 0.034403447061777115\n",
            "step: 80, loss: 0.00036271955468691885\n",
            "step: 90, loss: 0.012175235897302628\n",
            "step: 100, loss: 0.03080364689230919\n",
            "step: 110, loss: 0.0055426014587283134\n",
            "step: 120, loss: 0.005022811237722635\n",
            "step: 130, loss: 0.016906777396798134\n",
            "step: 140, loss: 0.022617755457758904\n",
            "step: 150, loss: 0.021280279383063316\n",
            "step: 160, loss: 0.0006786949234083295\n",
            "step: 170, loss: 0.0005030982429161668\n",
            "step: 180, loss: 0.03570754826068878\n",
            "step: 190, loss: 0.09027548879384995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7828418230563003, f1=0.7634408602150539, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006290496094152331\n",
            "step: 10, loss: 0.0011984106386080384\n",
            "step: 20, loss: 0.0007069961866363883\n",
            "step: 30, loss: 0.00043042234028689563\n",
            "step: 40, loss: 0.0015255825128406286\n",
            "step: 50, loss: 0.0008250540122389793\n",
            "step: 60, loss: 0.0061111655086278915\n",
            "step: 70, loss: 0.019111553207039833\n",
            "step: 80, loss: 0.01853160932660103\n",
            "step: 90, loss: 0.0004750483203679323\n",
            "step: 100, loss: 0.002086711348965764\n",
            "step: 110, loss: 0.0003779087564907968\n",
            "step: 120, loss: 0.1554163098335266\n",
            "step: 130, loss: 0.0003171016869600862\n",
            "step: 140, loss: 0.0007106553530320525\n",
            "step: 150, loss: 0.012983487918972969\n",
            "step: 160, loss: 0.007816221565008163\n",
            "step: 170, loss: 0.00028649321757256985\n",
            "step: 180, loss: 0.0021060339640825987\n",
            "step: 190, loss: 0.00033806206192821264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7726027397260273, f1=0.7380281690140845, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033828968298621476\n",
            "step: 10, loss: 0.004776752088218927\n",
            "step: 20, loss: 0.0028422437608242035\n",
            "step: 30, loss: 0.007857165299355984\n",
            "step: 40, loss: 0.0006839917041361332\n",
            "step: 50, loss: 0.0033321750815957785\n",
            "step: 60, loss: 0.00022052160056773573\n",
            "step: 70, loss: 0.00016133137978613377\n",
            "step: 80, loss: 0.00018312076281290501\n",
            "step: 90, loss: 0.00019277801038697362\n",
            "step: 100, loss: 0.00028952083084732294\n",
            "step: 110, loss: 0.00028869142988696694\n",
            "step: 120, loss: 0.00020423783280421048\n",
            "step: 130, loss: 0.0003049120132345706\n",
            "step: 140, loss: 0.0003075219865422696\n",
            "step: 150, loss: 0.0002282688074046746\n",
            "step: 160, loss: 0.00023255647101905197\n",
            "step: 170, loss: 0.00018740688392426819\n",
            "step: 180, loss: 0.0012648575939238071\n",
            "step: 190, loss: 0.0002768216363620013\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7774647887323942, f1=0.7428571428571429, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.347918628714979e-05\n",
            "step: 10, loss: 0.00020531169138848782\n",
            "step: 20, loss: 0.00012918970605824143\n",
            "step: 30, loss: 0.0003385838062968105\n",
            "step: 40, loss: 0.0004447795799933374\n",
            "step: 50, loss: 0.0002791833830997348\n",
            "step: 60, loss: 0.00011336930037941784\n",
            "step: 70, loss: 9.98809773591347e-05\n",
            "step: 80, loss: 0.0001466138637624681\n",
            "step: 90, loss: 0.0001639363035792485\n",
            "step: 100, loss: 0.0002112120419042185\n",
            "step: 110, loss: 0.0001577071234351024\n",
            "step: 120, loss: 0.00019264592265244573\n",
            "step: 130, loss: 0.0025633724872022867\n",
            "step: 140, loss: 0.007478238083422184\n",
            "step: 150, loss: 0.00019488493853714317\n",
            "step: 160, loss: 0.00022421858739107847\n",
            "step: 170, loss: 0.000417455768911168\n",
            "step: 180, loss: 0.0001409105898346752\n",
            "step: 190, loss: 0.00011888190783793107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7811634349030471, f1=0.7457627118644068, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000430869753472507\n",
            "step: 10, loss: 0.00017145044694188982\n",
            "step: 20, loss: 0.0010004742071032524\n",
            "step: 30, loss: 0.0008660274324938655\n",
            "step: 40, loss: 0.0001363895135000348\n",
            "step: 50, loss: 0.00019430987595114857\n",
            "step: 60, loss: 0.00034322438295930624\n",
            "step: 70, loss: 0.002529968973249197\n",
            "step: 80, loss: 0.00016289713676087558\n",
            "step: 90, loss: 0.00043444643961265683\n",
            "step: 100, loss: 0.00019439475727267563\n",
            "step: 110, loss: 9.54652568907477e-05\n",
            "step: 120, loss: 0.0002131026703864336\n",
            "step: 130, loss: 0.00010361584281781688\n",
            "step: 140, loss: 9.010804205900058e-05\n",
            "step: 150, loss: 0.00011410920706111938\n",
            "step: 160, loss: 0.0002313855366082862\n",
            "step: 170, loss: 0.000273957964964211\n",
            "step: 180, loss: 0.00011210668890271336\n",
            "step: 190, loss: 0.00037149229319766164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7810026385224274, f1=0.7465940054495912, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000575369456782937\n",
            "step: 10, loss: 0.0002111146313836798\n",
            "step: 20, loss: 0.00034412628156132996\n",
            "step: 30, loss: 9.29335001274012e-05\n",
            "step: 40, loss: 0.00808777566999197\n",
            "step: 50, loss: 0.0010430009569972754\n",
            "step: 60, loss: 0.00016054423758760095\n",
            "step: 70, loss: 0.0001207397217513062\n",
            "step: 80, loss: 0.00037134604644961655\n",
            "step: 90, loss: 0.00046604813542217016\n",
            "step: 100, loss: 0.0034909467212855816\n",
            "step: 110, loss: 0.00018146811635233462\n",
            "step: 120, loss: 0.0004372734110802412\n",
            "step: 130, loss: 0.0001560931996209547\n",
            "step: 140, loss: 0.000295478937914595\n",
            "step: 150, loss: 0.0001415312581229955\n",
            "step: 160, loss: 0.00032256124541163445\n",
            "step: 170, loss: 0.0001936815824592486\n",
            "step: 180, loss: 0.00012146853259764612\n",
            "step: 190, loss: 0.0003203271480742842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.786096256684492, f1=0.7431693989071039, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008161081932485104\n",
            "step: 10, loss: 0.00021856525563634932\n",
            "step: 20, loss: 0.00013450018013827503\n",
            "step: 30, loss: 0.0012072203680872917\n",
            "step: 40, loss: 0.00018584023928269744\n",
            "step: 50, loss: 0.00028290157206356525\n",
            "step: 60, loss: 0.00010189716704189777\n",
            "step: 70, loss: 0.0003195599711034447\n",
            "step: 80, loss: 0.0006400900892913342\n",
            "step: 90, loss: 0.00017574543016962707\n",
            "step: 100, loss: 0.0041572535410523415\n",
            "step: 110, loss: 0.00023803333169780672\n",
            "step: 120, loss: 0.00010936654143733904\n",
            "step: 130, loss: 0.00014850379375275224\n",
            "step: 140, loss: 0.0005771336727775633\n",
            "step: 150, loss: 0.00039717109757475555\n",
            "step: 160, loss: 0.00019458284077700227\n",
            "step: 170, loss: 0.00013581950042862445\n",
            "step: 180, loss: 0.0003800082195084542\n",
            "step: 190, loss: 0.00014857295900583267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.7925531914893618, f1=0.7465940054495912, best_f1=0.7465940054495912\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 231.38it/s]\n",
            "load_f1 = 0.7890410958904108\n",
            "real_f1 = 0.7713498622589531\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 250.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf89698e-f986-4947-bc36-5dc47f4857ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6136382818222046\n",
            "step: 10, loss: 0.3752635717391968\n",
            "step: 20, loss: 0.3068855106830597\n",
            "step: 30, loss: 0.37660157680511475\n",
            "step: 40, loss: 0.27966034412384033\n",
            "step: 50, loss: 0.23037798702716827\n",
            "step: 60, loss: 0.30434393882751465\n",
            "step: 70, loss: 0.3531155586242676\n",
            "step: 80, loss: 0.32512933015823364\n",
            "step: 90, loss: 0.15566490590572357\n",
            "step: 100, loss: 0.23830054700374603\n",
            "step: 110, loss: 0.20853351056575775\n",
            "step: 120, loss: 0.14859884977340698\n",
            "step: 130, loss: 0.0649643987417221\n",
            "step: 140, loss: 0.2281384915113449\n",
            "step: 150, loss: 0.26947423815727234\n",
            "step: 160, loss: 0.13255757093429565\n",
            "step: 170, loss: 0.3162281811237335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7281553398058251, f1=0.6714628297362111, best_f1=0.6714628297362111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.036242179572582245\n",
            "step: 10, loss: 0.11107733845710754\n",
            "step: 20, loss: 0.1057780459523201\n",
            "step: 30, loss: 0.21576346457004547\n",
            "step: 40, loss: 0.12152107059955597\n",
            "step: 50, loss: 0.09643299877643585\n",
            "step: 60, loss: 0.41632628440856934\n",
            "step: 70, loss: 0.09719308465719223\n",
            "step: 80, loss: 0.04123036190867424\n",
            "step: 90, loss: 0.11312873661518097\n",
            "step: 100, loss: 0.1294376403093338\n",
            "step: 110, loss: 0.08647368848323822\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.10153007507324219\n",
            "step: 130, loss: 0.10523241013288498\n",
            "step: 140, loss: 0.22545203566551208\n",
            "step: 150, loss: 0.21166828274726868\n",
            "step: 160, loss: 0.07018417119979858\n",
            "step: 170, loss: 0.06087721139192581\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7524271844660194, f1=0.7294685990338163, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13905741274356842\n",
            "step: 10, loss: 0.05953509733080864\n",
            "step: 20, loss: 0.05399194359779358\n",
            "step: 30, loss: 0.07503440231084824\n",
            "step: 40, loss: 0.08895828574895859\n",
            "step: 50, loss: 0.08294324576854706\n",
            "step: 60, loss: 0.11327610909938812\n",
            "step: 70, loss: 0.09562437236309052\n",
            "step: 80, loss: 0.08622594177722931\n",
            "step: 90, loss: 0.1608278453350067\n",
            "step: 100, loss: 0.025483854115009308\n",
            "step: 110, loss: 0.20195835828781128\n",
            "step: 120, loss: 0.07559292763471603\n",
            "step: 130, loss: 0.11598372459411621\n",
            "step: 140, loss: 0.13114763796329498\n",
            "step: 150, loss: 0.014833083376288414\n",
            "step: 160, loss: 0.06634525209665298\n",
            "step: 170, loss: 0.05638795346021652\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.745945945945946, f1=0.7433155080213903, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11892732977867126\n",
            "step: 10, loss: 0.03574776649475098\n",
            "step: 20, loss: 0.009932862594723701\n",
            "step: 30, loss: 0.04237498342990875\n",
            "step: 40, loss: 0.001962742069736123\n",
            "step: 50, loss: 0.10910174995660782\n",
            "step: 60, loss: 0.07557125389575958\n",
            "step: 70, loss: 0.06339041143655777\n",
            "step: 80, loss: 0.12360205501317978\n",
            "step: 90, loss: 0.07473418861627579\n",
            "step: 100, loss: 0.5073763728141785\n",
            "step: 110, loss: 0.06123225390911102\n",
            "step: 120, loss: 0.009880597703158855\n",
            "step: 130, loss: 0.1798718124628067\n",
            "step: 140, loss: 0.04904408007860184\n",
            "step: 150, loss: 0.02201320417225361\n",
            "step: 160, loss: 0.13331389427185059\n",
            "step: 170, loss: 0.01399607490748167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.730478589420655, f1=0.6947890818858561, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018580660223960876\n",
            "step: 10, loss: 0.03721736744046211\n",
            "step: 20, loss: 0.010900291614234447\n",
            "step: 30, loss: 0.040283091366291046\n",
            "step: 40, loss: 0.015274765901267529\n",
            "step: 50, loss: 0.040939319878816605\n",
            "step: 60, loss: 0.01577875204384327\n",
            "step: 70, loss: 0.23692308366298676\n",
            "step: 80, loss: 0.06684126704931259\n",
            "step: 90, loss: 0.10213816165924072\n",
            "step: 100, loss: 0.05511562153697014\n",
            "step: 110, loss: 0.05012292042374611\n",
            "step: 120, loss: 0.01960013434290886\n",
            "step: 130, loss: 0.06582170724868774\n",
            "step: 140, loss: 0.07388903200626373\n",
            "step: 150, loss: 0.05606750398874283\n",
            "step: 160, loss: 0.03125794231891632\n",
            "step: 170, loss: 0.09486236423254013\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7118644067796611, f1=0.6848072562358277, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011037887074053288\n",
            "step: 10, loss: 0.020505085587501526\n",
            "step: 20, loss: 0.04473388195037842\n",
            "step: 30, loss: 0.025697069242596626\n",
            "step: 40, loss: 0.01657099276781082\n",
            "step: 50, loss: 0.11525344103574753\n",
            "step: 60, loss: 0.06176977604627609\n",
            "step: 70, loss: 0.054364610463380814\n",
            "step: 80, loss: 0.03600839897990227\n",
            "step: 90, loss: 0.01877853460609913\n",
            "step: 100, loss: 0.003729012329131365\n",
            "step: 110, loss: 0.005265751387923956\n",
            "step: 120, loss: 0.04053587466478348\n",
            "step: 130, loss: 0.002705428982153535\n",
            "step: 140, loss: 0.04675706475973129\n",
            "step: 150, loss: 0.11883704364299774\n",
            "step: 160, loss: 0.07123296707868576\n",
            "step: 170, loss: 0.00922603253275156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7259615384615385, f1=0.7058823529411764, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029241524171084166\n",
            "step: 10, loss: 0.01683959923684597\n",
            "step: 20, loss: 0.012110122479498386\n",
            "step: 30, loss: 0.05361044034361839\n",
            "step: 40, loss: 0.025731857866048813\n",
            "step: 50, loss: 0.03576033189892769\n",
            "step: 60, loss: 0.008382371626794338\n",
            "step: 70, loss: 0.013215750455856323\n",
            "step: 80, loss: 0.0023470469750463963\n",
            "step: 90, loss: 0.00043265620479360223\n",
            "step: 100, loss: 0.006412746850401163\n",
            "step: 110, loss: 0.05877241864800453\n",
            "step: 120, loss: 0.05057545378804207\n",
            "step: 130, loss: 0.0891028419137001\n",
            "step: 140, loss: 0.006285690236836672\n",
            "step: 150, loss: 0.023829080164432526\n",
            "step: 160, loss: 0.013160414062440395\n",
            "step: 170, loss: 0.049793507903814316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7210526315789473, f1=0.7376623376623377, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003632546868175268\n",
            "step: 10, loss: 0.004323170054703951\n",
            "step: 20, loss: 0.001580391894094646\n",
            "step: 30, loss: 0.034204501658678055\n",
            "step: 40, loss: 0.0003508379450067878\n",
            "step: 50, loss: 0.00268789054825902\n",
            "step: 60, loss: 0.002802557311952114\n",
            "step: 70, loss: 0.02467915415763855\n",
            "step: 80, loss: 0.010346595197916031\n",
            "step: 90, loss: 0.006067960523068905\n",
            "step: 100, loss: 0.0357830747961998\n",
            "step: 110, loss: 0.027352385222911835\n",
            "step: 120, loss: 0.0012073492398485541\n",
            "step: 130, loss: 0.001068966812454164\n",
            "step: 140, loss: 0.0006005583563819528\n",
            "step: 150, loss: 0.0018562785116955638\n",
            "step: 160, loss: 0.010641630738973618\n",
            "step: 170, loss: 0.020235218107700348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7070707070707071, f1=0.6904761904761905, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023444803431630135\n",
            "step: 10, loss: 0.01692323572933674\n",
            "step: 20, loss: 0.03893419727683067\n",
            "step: 30, loss: 0.005962858907878399\n",
            "step: 40, loss: 0.016843808814883232\n",
            "step: 50, loss: 0.001180503168143332\n",
            "step: 60, loss: 0.002662557177245617\n",
            "step: 70, loss: 0.03397315368056297\n",
            "step: 80, loss: 0.00928837526589632\n",
            "step: 90, loss: 0.013962341472506523\n",
            "step: 100, loss: 0.007804979104548693\n",
            "step: 110, loss: 0.13527537882328033\n",
            "step: 120, loss: 0.003185845213010907\n",
            "step: 130, loss: 0.01476262602955103\n",
            "step: 140, loss: 0.001722769346088171\n",
            "step: 150, loss: 0.013433174230158329\n",
            "step: 160, loss: 0.09735722094774246\n",
            "step: 170, loss: 0.03915967047214508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7305699481865284, f1=0.731829573934837, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07929570227861404\n",
            "step: 10, loss: 0.004424956627190113\n",
            "step: 20, loss: 0.029191771522164345\n",
            "step: 30, loss: 0.0009593503782525659\n",
            "step: 40, loss: 0.00027129179215990007\n",
            "step: 50, loss: 0.04683028906583786\n",
            "step: 60, loss: 0.0002323035878362134\n",
            "step: 70, loss: 0.00047028460539877415\n",
            "step: 80, loss: 0.0005807930720038712\n",
            "step: 90, loss: 0.0014759802725166082\n",
            "step: 100, loss: 0.0003390490310266614\n",
            "step: 110, loss: 0.0032763127237558365\n",
            "step: 120, loss: 0.0006977160228416324\n",
            "step: 130, loss: 0.07081225514411926\n",
            "step: 140, loss: 0.0294790156185627\n",
            "step: 150, loss: 0.019237587228417397\n",
            "step: 160, loss: 0.0013541651424020529\n",
            "step: 170, loss: 0.00020318325550761074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7008086253369273, f1=0.7195767195767196, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016156090423464775\n",
            "step: 10, loss: 0.04547657445073128\n",
            "step: 20, loss: 0.0009994310094043612\n",
            "step: 30, loss: 0.00018911277584265918\n",
            "step: 40, loss: 0.00025773816742002964\n",
            "step: 50, loss: 0.019446110352873802\n",
            "step: 60, loss: 0.001477473066188395\n",
            "step: 70, loss: 0.00012025044270558283\n",
            "step: 80, loss: 0.0004950332804583013\n",
            "step: 90, loss: 0.007754865568131208\n",
            "step: 100, loss: 0.1434900015592575\n",
            "step: 110, loss: 0.002059886697679758\n",
            "step: 120, loss: 0.0008541603456251323\n",
            "step: 130, loss: 0.00016806474013719708\n",
            "step: 140, loss: 0.000349832815118134\n",
            "step: 150, loss: 0.0008959340048022568\n",
            "step: 160, loss: 0.0010713626397773623\n",
            "step: 170, loss: 0.0008968728361651301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7008086253369273, f1=0.7324675324675325, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00032441545045003295\n",
            "step: 10, loss: 0.0033650521654635668\n",
            "step: 20, loss: 0.0005659344606101513\n",
            "step: 30, loss: 0.00038237194530665874\n",
            "step: 40, loss: 0.0008568759076297283\n",
            "step: 50, loss: 0.0005166362971067429\n",
            "step: 60, loss: 0.001419323612935841\n",
            "step: 70, loss: 0.056952573359012604\n",
            "step: 80, loss: 0.00020488942391239107\n",
            "step: 90, loss: 0.0011408027494326234\n",
            "step: 100, loss: 0.017691651359200478\n",
            "step: 110, loss: 0.00538815651088953\n",
            "step: 120, loss: 0.0008717197342775762\n",
            "step: 130, loss: 0.001847284846007824\n",
            "step: 140, loss: 0.00147532531991601\n",
            "step: 150, loss: 0.005149048753082752\n",
            "step: 160, loss: 0.00027857880922965705\n",
            "step: 170, loss: 0.000899118953384459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6925207756232686, f1=0.7335092348284961, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014245304046198726\n",
            "step: 10, loss: 0.00029713864205405116\n",
            "step: 20, loss: 0.011634009890258312\n",
            "step: 30, loss: 0.00020910931925754994\n",
            "step: 40, loss: 0.0005186396301724017\n",
            "step: 50, loss: 0.0006417762488126755\n",
            "step: 60, loss: 0.0006212581065483391\n",
            "step: 70, loss: 0.0006116931326687336\n",
            "step: 80, loss: 0.00048647765652276576\n",
            "step: 90, loss: 0.00026430084835737944\n",
            "step: 100, loss: 0.001608199905604124\n",
            "step: 110, loss: 0.0002170923980884254\n",
            "step: 120, loss: 0.010729474015533924\n",
            "step: 130, loss: 0.00014275313878897578\n",
            "step: 140, loss: 0.0001812096597859636\n",
            "step: 150, loss: 0.0018702119123190641\n",
            "step: 160, loss: 0.0021281293593347073\n",
            "step: 170, loss: 0.0005780864739790559\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7016574585635358, f1=0.7229551451187335, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009703070973046124\n",
            "step: 10, loss: 0.00013368572399485856\n",
            "step: 20, loss: 0.00017179767019115388\n",
            "step: 30, loss: 0.005606450606137514\n",
            "step: 40, loss: 0.00026607626932673156\n",
            "step: 50, loss: 0.0030709232669323683\n",
            "step: 60, loss: 9.346521983388811e-05\n",
            "step: 70, loss: 0.0006806238670833409\n",
            "step: 80, loss: 0.0004324323672335595\n",
            "step: 90, loss: 0.0001332537067355588\n",
            "step: 100, loss: 0.0017854004399850965\n",
            "step: 110, loss: 0.00020862228120677173\n",
            "step: 120, loss: 0.0012613695580512285\n",
            "step: 130, loss: 0.0004795871500391513\n",
            "step: 140, loss: 0.00014973235374782234\n",
            "step: 150, loss: 0.0004921674844808877\n",
            "step: 160, loss: 8.152038208208978e-05\n",
            "step: 170, loss: 0.00011517975508468226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7055555555555555, f1=0.738544474393531, best_f1=0.7294685990338163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00031533758738078177\n",
            "step: 10, loss: 0.0004957924829795957\n",
            "step: 20, loss: 0.001799883204512298\n",
            "step: 30, loss: 0.00029347019153647125\n",
            "step: 40, loss: 0.0007361680036410689\n",
            "step: 50, loss: 8.380127837881446e-05\n",
            "step: 60, loss: 0.0015384610742330551\n",
            "step: 70, loss: 0.00012401278945617378\n",
            "step: 80, loss: 0.0049828733317554\n",
            "step: 90, loss: 0.0005504398723132908\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.009405652061104774\n",
            "step: 110, loss: 0.00016874952416401356\n",
            "step: 120, loss: 0.025405706837773323\n",
            "step: 130, loss: 0.00023142203281167895\n",
            "step: 140, loss: 0.0003673938917927444\n",
            "step: 150, loss: 0.0002480291877873242\n",
            "step: 160, loss: 0.00021004959126003087\n",
            "step: 170, loss: 0.00023631479416508228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7058823529411764, f1=0.738544474393531, best_f1=0.7294685990338163\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 235.97it/s]\n",
            "load_f1 = 0.7589498806682576\n",
            "real_f1 = 0.7517730496453902\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 193.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1dad7b6-4344-49fe-d0c6-48021cf774dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6192374229431152\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.597267210483551\n",
            "step: 20, loss: 0.5394448637962341\n",
            "step: 30, loss: 0.5171781182289124\n",
            "step: 40, loss: 0.36119452118873596\n",
            "step: 50, loss: 0.17636150121688843\n",
            "step: 60, loss: 0.23745065927505493\n",
            "step: 70, loss: 0.20133274793624878\n",
            "step: 80, loss: 0.08308546990156174\n",
            "step: 90, loss: 0.06850156933069229\n",
            "step: 100, loss: 0.010220480151474476\n",
            "step: 110, loss: 0.15546591579914093\n",
            "step: 120, loss: 0.05979273095726967\n",
            "step: 130, loss: 0.05053778365254402\n",
            "step: 140, loss: 0.04779361933469772\n",
            "step: 150, loss: 0.09860529750585556\n",
            "step: 160, loss: 0.011975212022662163\n",
            "step: 170, loss: 0.21630778908729553\n",
            "step: 180, loss: 0.2070765644311905\n",
            "step: 190, loss: 0.1239706352353096\n",
            "step: 200, loss: 0.10929886251688004\n",
            "step: 210, loss: 0.02621220052242279\n",
            "step: 220, loss: 0.1796300709247589\n",
            "step: 230, loss: 0.09730477631092072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9401330376940134, f1=0.9361702127659575, best_f1=0.9361702127659575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021602654829621315\n",
            "step: 10, loss: 0.02358368970453739\n",
            "step: 20, loss: 0.31484007835388184\n",
            "step: 30, loss: 0.17102093994617462\n",
            "step: 40, loss: 0.11015377938747406\n",
            "step: 50, loss: 0.00829608179628849\n",
            "step: 60, loss: 0.04810884967446327\n",
            "step: 70, loss: 0.12798620760440826\n",
            "step: 80, loss: 0.09878485649824142\n",
            "step: 90, loss: 0.12859505414962769\n",
            "step: 100, loss: 0.12216494232416153\n",
            "step: 110, loss: 0.2065652757883072\n",
            "step: 120, loss: 0.1433434933423996\n",
            "step: 130, loss: 0.1100386530160904\n",
            "step: 140, loss: 0.014153712429106236\n",
            "step: 150, loss: 0.034401535987854004\n",
            "step: 160, loss: 0.1118466705083847\n",
            "step: 170, loss: 0.02172342874109745\n",
            "step: 180, loss: 0.055773865431547165\n",
            "step: 190, loss: 0.010463513433933258\n",
            "step: 200, loss: 0.02588501013815403\n",
            "step: 210, loss: 0.0026519515085965395\n",
            "step: 220, loss: 0.032827313989400864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9419642857142857, f1=0.9228998849252014, best_f1=0.9228998849252014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12792013585567474\n",
            "step: 10, loss: 0.052666403353214264\n",
            "step: 20, loss: 0.07030916213989258\n",
            "step: 30, loss: 0.026775751262903214\n",
            "step: 40, loss: 0.0072591411881148815\n",
            "step: 50, loss: 0.019829895347356796\n",
            "step: 60, loss: 0.006519021466374397\n",
            "step: 70, loss: 0.18595822155475616\n",
            "step: 80, loss: 0.0033091462682932615\n",
            "step: 90, loss: 0.14166834950447083\n",
            "step: 100, loss: 0.042549729347229004\n",
            "step: 110, loss: 0.008393160067498684\n",
            "step: 120, loss: 0.006145324092358351\n",
            "step: 130, loss: 0.01033099927008152\n",
            "step: 140, loss: 0.003221845254302025\n",
            "step: 150, loss: 0.0263877771794796\n",
            "step: 160, loss: 0.010580495931208134\n",
            "step: 170, loss: 0.020861594006419182\n",
            "step: 180, loss: 0.021061507984995842\n",
            "step: 190, loss: 0.05702004209160805\n",
            "step: 200, loss: 0.028562244027853012\n",
            "step: 210, loss: 0.0041435775347054005\n",
            "step: 220, loss: 0.051696110516786575\n",
            "step: 230, loss: 0.009380968287587166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9500554938956715, f1=0.9414414414414415, best_f1=0.9414414414414415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04623585194349289\n",
            "step: 10, loss: 0.0016593532636761665\n",
            "step: 20, loss: 0.015224194154143333\n",
            "step: 30, loss: 0.005614478141069412\n",
            "step: 40, loss: 0.004514869302511215\n",
            "step: 50, loss: 0.002851094352081418\n",
            "step: 60, loss: 0.004062525928020477\n",
            "step: 70, loss: 0.0013076207833364606\n",
            "step: 80, loss: 0.005530132912099361\n",
            "step: 90, loss: 0.06208471208810806\n",
            "step: 100, loss: 0.03966662660241127\n",
            "step: 110, loss: 0.05122245103120804\n",
            "step: 120, loss: 0.05910513922572136\n",
            "step: 130, loss: 0.04087114706635475\n",
            "step: 140, loss: 0.012831274420022964\n",
            "step: 150, loss: 0.20316541194915771\n",
            "step: 160, loss: 0.0080532506108284\n",
            "step: 170, loss: 0.028359999880194664\n",
            "step: 180, loss: 0.046071987599134445\n",
            "step: 190, loss: 0.0010371883399784565\n",
            "step: 200, loss: 0.0020332704298198223\n",
            "step: 210, loss: 0.006648211739957333\n",
            "step: 220, loss: 0.000638107187114656\n",
            "step: 230, loss: 0.011221016757190228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9448275862068966, f1=0.9433526011560694, best_f1=0.9414414414414415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010265017626807094\n",
            "step: 10, loss: 0.00796507578343153\n",
            "step: 20, loss: 0.005000915378332138\n",
            "step: 30, loss: 0.0010109922150149941\n",
            "step: 40, loss: 0.0008959921542555094\n",
            "step: 50, loss: 0.0014531795168295503\n",
            "step: 60, loss: 0.11828725785017014\n",
            "step: 70, loss: 0.012392068281769753\n",
            "step: 80, loss: 0.0006763305864296854\n",
            "step: 90, loss: 0.048538390547037125\n",
            "step: 100, loss: 0.0003404008166398853\n",
            "step: 110, loss: 0.0003088300582021475\n",
            "step: 120, loss: 0.01734551601111889\n",
            "step: 130, loss: 0.08279100805521011\n",
            "step: 140, loss: 0.009595402516424656\n",
            "step: 150, loss: 0.0056921085342764854\n",
            "step: 160, loss: 0.00500135775655508\n",
            "step: 170, loss: 0.07389097660779953\n",
            "step: 180, loss: 0.019160915166139603\n",
            "step: 190, loss: 0.11215809732675552\n",
            "step: 200, loss: 0.002160137752071023\n",
            "step: 210, loss: 0.002600285457447171\n",
            "step: 220, loss: 0.11974424868822098\n",
            "step: 230, loss: 0.0010942231165245175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9484304932735426, f1=0.9446327683615819, best_f1=0.9414414414414415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09845485538244247\n",
            "step: 10, loss: 0.006768021732568741\n",
            "step: 20, loss: 0.017974959686398506\n",
            "step: 30, loss: 0.000449918006779626\n",
            "step: 40, loss: 0.00038444335223175585\n",
            "step: 50, loss: 0.006430627778172493\n",
            "step: 60, loss: 0.0013551837764680386\n",
            "step: 70, loss: 0.0019934263546019793\n",
            "step: 80, loss: 0.002811969257891178\n",
            "step: 90, loss: 0.0013906116364523768\n",
            "step: 100, loss: 0.018982086330652237\n",
            "step: 110, loss: 0.0010881503112614155\n",
            "step: 120, loss: 0.018388858065009117\n",
            "step: 130, loss: 0.0009881739970296621\n",
            "step: 140, loss: 0.007716981694102287\n",
            "step: 150, loss: 0.0006362285930663347\n",
            "step: 160, loss: 0.0016765452455729246\n",
            "step: 170, loss: 0.0003799708210863173\n",
            "step: 180, loss: 0.006041708402335644\n",
            "step: 190, loss: 0.027992000803351402\n",
            "step: 200, loss: 0.00026403716765344143\n",
            "step: 210, loss: 0.0007387488731183112\n",
            "step: 220, loss: 0.0002579468709882349\n",
            "step: 230, loss: 0.10463567823171616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9579646017699115, f1=0.9395973154362416, best_f1=0.9395973154362416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06293725967407227\n",
            "step: 10, loss: 0.0003571865672711283\n",
            "step: 20, loss: 0.0007503183442167938\n",
            "step: 30, loss: 0.0001985951530514285\n",
            "step: 40, loss: 0.0008658805745653808\n",
            "step: 50, loss: 0.00020192931697238237\n",
            "step: 60, loss: 0.00041477399645373225\n",
            "step: 70, loss: 0.009640965610742569\n",
            "step: 80, loss: 0.05036235228180885\n",
            "step: 90, loss: 0.00014702729822602123\n",
            "step: 100, loss: 0.06660943478345871\n",
            "step: 110, loss: 0.0003684107505250722\n",
            "step: 120, loss: 0.0001698801788734272\n",
            "step: 130, loss: 0.0063896626234054565\n",
            "step: 140, loss: 0.0029183239676058292\n",
            "step: 150, loss: 0.0006013571401126683\n",
            "step: 160, loss: 0.07555725425481796\n",
            "step: 170, loss: 0.00611201161518693\n",
            "step: 180, loss: 0.01711127534508705\n",
            "step: 190, loss: 0.00039019997348077595\n",
            "step: 200, loss: 0.13991887867450714\n",
            "step: 210, loss: 0.0005685937940143049\n",
            "step: 220, loss: 0.0009862721199169755\n",
            "step: 230, loss: 0.000745572498999536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9571106094808126, f1=0.9440000000000001, best_f1=0.9395973154362416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016857525333762169\n",
            "step: 10, loss: 0.0015629589324817061\n",
            "step: 20, loss: 0.0004104462277609855\n",
            "step: 30, loss: 0.000632257666438818\n",
            "step: 40, loss: 0.017588673159480095\n",
            "step: 50, loss: 0.001093595870770514\n",
            "step: 60, loss: 0.0025640076491981745\n",
            "step: 70, loss: 0.00033713155426084995\n",
            "step: 80, loss: 0.0008136958349496126\n",
            "step: 90, loss: 0.00015819842519704252\n",
            "step: 100, loss: 0.0030260435305535793\n",
            "step: 110, loss: 0.0007954707252793014\n",
            "step: 120, loss: 0.05621642246842384\n",
            "step: 130, loss: 0.03353030979633331\n",
            "step: 140, loss: 0.0015323514817282557\n",
            "step: 150, loss: 0.0006205764366313815\n",
            "step: 160, loss: 0.0005882765399292111\n",
            "step: 170, loss: 0.0014253866393119097\n",
            "step: 180, loss: 0.0007258960977196693\n",
            "step: 190, loss: 0.0012150516267865896\n",
            "step: 200, loss: 0.009265446104109287\n",
            "step: 210, loss: 0.06644199043512344\n",
            "step: 220, loss: 0.0031312773935496807\n",
            "step: 230, loss: 0.00042017639498226345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9425028184892897, f1=0.9419795221843003, best_f1=0.9395973154362416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004044073633849621\n",
            "step: 10, loss: 0.003407808020710945\n",
            "step: 20, loss: 0.0011596346739679575\n",
            "step: 30, loss: 0.0002273138816235587\n",
            "step: 40, loss: 0.020944032818078995\n",
            "step: 50, loss: 0.0001501339575042948\n",
            "step: 60, loss: 0.000441274227341637\n",
            "step: 70, loss: 0.0002123824378941208\n",
            "step: 80, loss: 0.000956738309469074\n",
            "step: 90, loss: 0.0005079020047560334\n",
            "step: 100, loss: 0.0009632721194066107\n",
            "step: 110, loss: 0.0001331191451754421\n",
            "step: 120, loss: 0.000130232801893726\n",
            "step: 130, loss: 0.0001529761211713776\n",
            "step: 140, loss: 0.00011742604692699388\n",
            "step: 150, loss: 0.007789317984133959\n",
            "step: 160, loss: 0.0002031233161687851\n",
            "step: 170, loss: 0.0008674691780470312\n",
            "step: 180, loss: 0.0022025718353688717\n",
            "step: 190, loss: 0.000742247502785176\n",
            "step: 200, loss: 0.0040336549282073975\n",
            "step: 210, loss: 0.0002557203406468034\n",
            "step: 220, loss: 0.0006782071432098746\n",
            "step: 230, loss: 0.00940820574760437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9536423841059603, f1=0.9443207126948775, best_f1=0.9395973154362416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00031633165781386197\n",
            "step: 10, loss: 0.001020333031192422\n",
            "step: 20, loss: 0.00018852333596441895\n",
            "step: 30, loss: 0.00028047620435245335\n",
            "step: 40, loss: 0.00030775374034419656\n",
            "step: 50, loss: 0.0005671365070156753\n",
            "step: 60, loss: 0.0004174507921561599\n",
            "step: 70, loss: 0.000150100706377998\n",
            "step: 80, loss: 0.00016171076276805252\n",
            "step: 90, loss: 0.0005754553130827844\n",
            "step: 100, loss: 0.00022946472745388746\n",
            "step: 110, loss: 0.0031277891248464584\n",
            "step: 120, loss: 0.00012183981743874028\n",
            "step: 130, loss: 0.0008482764242216945\n",
            "step: 140, loss: 0.03877493739128113\n",
            "step: 150, loss: 0.04291857033967972\n",
            "step: 160, loss: 0.000147237500641495\n",
            "step: 170, loss: 0.00013186593423597515\n",
            "step: 180, loss: 0.005449497140944004\n",
            "step: 190, loss: 0.000629477493930608\n",
            "step: 200, loss: 8.566284668631852e-05\n",
            "step: 210, loss: 6.840858986834064e-05\n",
            "step: 220, loss: 0.0001925275573739782\n",
            "step: 230, loss: 0.0005229470552876592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9609810479375697, f1=0.941834451901566, best_f1=0.941834451901566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016454188153147697\n",
            "step: 10, loss: 0.0001328248472418636\n",
            "step: 20, loss: 0.001993667334318161\n",
            "step: 30, loss: 0.18534667789936066\n",
            "step: 40, loss: 0.0008548739715479314\n",
            "step: 50, loss: 0.00017562233551871032\n",
            "step: 60, loss: 0.0008386599947698414\n",
            "step: 70, loss: 0.0001702490699244663\n",
            "step: 80, loss: 0.0001006187085295096\n",
            "step: 90, loss: 0.00010980281513184309\n",
            "step: 100, loss: 0.00013599637895822525\n",
            "step: 110, loss: 0.0079466188326478\n",
            "step: 120, loss: 0.00010537495109019801\n",
            "step: 130, loss: 5.740886990679428e-05\n",
            "step: 140, loss: 0.0011717929737642407\n",
            "step: 150, loss: 0.00018308026483282447\n",
            "step: 160, loss: 7.64666183385998e-05\n",
            "step: 170, loss: 0.0017782084178179502\n",
            "step: 180, loss: 0.0067823706194758415\n",
            "step: 190, loss: 0.00010371541429776698\n",
            "step: 200, loss: 0.00016169172886293381\n",
            "step: 210, loss: 6.0131511418148875e-05\n",
            "step: 220, loss: 8.91808231244795e-05\n",
            "step: 230, loss: 0.00010720072168624029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9552572706935123, f1=0.9403824521934758, best_f1=0.941834451901566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013515989121515304\n",
            "step: 10, loss: 9.038137795869261e-05\n",
            "step: 20, loss: 0.00013235240476205945\n",
            "step: 30, loss: 0.000688319792971015\n",
            "step: 40, loss: 0.0001225862215505913\n",
            "step: 50, loss: 0.00018648534023668617\n",
            "step: 60, loss: 0.00016315936227329075\n",
            "step: 70, loss: 7.980535883689299e-05\n",
            "step: 80, loss: 8.90537048690021e-05\n",
            "step: 90, loss: 7.47829326428473e-05\n",
            "step: 100, loss: 6.209911953192204e-05\n",
            "step: 110, loss: 0.0001417384046362713\n",
            "step: 120, loss: 0.00010983838001266122\n",
            "step: 130, loss: 0.00015774730127304792\n",
            "step: 140, loss: 0.00018030735373031348\n",
            "step: 150, loss: 8.095982047962025e-05\n",
            "step: 160, loss: 9.463392052566633e-05\n",
            "step: 170, loss: 9.231571311829612e-05\n",
            "step: 180, loss: 0.00017856888007372618\n",
            "step: 190, loss: 0.0002249311946798116\n",
            "step: 200, loss: 8.754820009926334e-05\n",
            "step: 210, loss: 0.0001087379569071345\n",
            "step: 220, loss: 0.00011644836922641844\n",
            "step: 230, loss: 0.0004467648104764521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9573991031390134, f1=0.9379932356257046, best_f1=0.941834451901566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009480954613536596\n",
            "step: 10, loss: 0.0021253034938126802\n",
            "step: 20, loss: 9.833250805968419e-05\n",
            "step: 30, loss: 0.00020142982248216867\n",
            "step: 40, loss: 0.0004406693624332547\n",
            "step: 50, loss: 0.029667483642697334\n",
            "step: 60, loss: 0.00021327304420992732\n",
            "step: 70, loss: 0.10080042481422424\n",
            "step: 80, loss: 0.00032595047377981246\n",
            "step: 90, loss: 0.00011058411473641172\n",
            "step: 100, loss: 0.013699379749596119\n",
            "step: 110, loss: 0.0033505093306303024\n",
            "step: 120, loss: 0.00021078884310554713\n",
            "step: 130, loss: 0.0002603689208626747\n",
            "step: 140, loss: 0.00015644678205717355\n",
            "step: 150, loss: 0.00012998068996239454\n",
            "step: 160, loss: 0.00016892097482923418\n",
            "step: 170, loss: 0.0002329289127374068\n",
            "step: 180, loss: 8.249421807704493e-05\n",
            "step: 190, loss: 0.00012384731962811202\n",
            "step: 200, loss: 7.468482363037765e-05\n",
            "step: 210, loss: 4.073302625329234e-05\n",
            "step: 220, loss: 0.0005345841054804623\n",
            "step: 230, loss: 0.00013182869588490576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9584736251402918, f1=0.9427609427609427, best_f1=0.941834451901566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017614694661460817\n",
            "step: 10, loss: 0.00040846315096132457\n",
            "step: 20, loss: 7.17362345312722e-05\n",
            "step: 30, loss: 0.0001387672673445195\n",
            "step: 40, loss: 0.0016822899924591184\n",
            "step: 50, loss: 0.00031255726935341954\n",
            "step: 60, loss: 7.026587991276756e-05\n",
            "step: 70, loss: 0.0001254765666089952\n",
            "step: 80, loss: 8.171355875674635e-05\n",
            "step: 90, loss: 0.0003793720679823309\n",
            "step: 100, loss: 9.080159361474216e-05\n",
            "step: 110, loss: 7.912238652352244e-05\n",
            "step: 120, loss: 3.4110522392438725e-05\n",
            "step: 130, loss: 6.832400686107576e-05\n",
            "step: 140, loss: 9.906468767439947e-05\n",
            "step: 150, loss: 4.845934017794207e-05\n",
            "step: 160, loss: 0.00018191781418863684\n",
            "step: 170, loss: 0.00012453498493414372\n",
            "step: 180, loss: 7.150598685257137e-05\n",
            "step: 190, loss: 7.455565355485305e-05\n",
            "step: 200, loss: 4.706968684331514e-05\n",
            "step: 210, loss: 0.00015109045489225537\n",
            "step: 220, loss: 0.001071673003025353\n",
            "step: 230, loss: 0.0002249642857350409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9584736251402918, f1=0.9452513966480447, best_f1=0.941834451901566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.435822484083474e-05\n",
            "step: 10, loss: 4.1332386899739504e-05\n",
            "step: 20, loss: 6.760745600331575e-05\n",
            "step: 30, loss: 7.880419434513897e-05\n",
            "step: 40, loss: 7.937342161312699e-05\n",
            "step: 50, loss: 0.01917242258787155\n",
            "step: 60, loss: 0.00020022675744257867\n",
            "step: 70, loss: 0.0019284480949863791\n",
            "step: 80, loss: 0.00019017753947991878\n",
            "step: 90, loss: 0.00026357383467257023\n",
            "step: 100, loss: 7.579351949971169e-05\n",
            "step: 110, loss: 5.742721623391844e-05\n",
            "step: 120, loss: 0.00015806735609658062\n",
            "step: 130, loss: 6.054988261894323e-05\n",
            "step: 140, loss: 8.573295053793117e-05\n",
            "step: 150, loss: 0.00023206767218653113\n",
            "step: 160, loss: 3.844323873636313e-05\n",
            "step: 170, loss: 6.217490590643138e-05\n",
            "step: 180, loss: 0.0001379170862492174\n",
            "step: 190, loss: 0.0002006852300837636\n",
            "step: 200, loss: 0.0005769772105850279\n",
            "step: 210, loss: 0.025541165843605995\n",
            "step: 220, loss: 0.002728105057030916\n",
            "step: 230, loss: 6.136508454801515e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9594594594594594, f1=0.9423728813559321, best_f1=0.941834451901566\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 159.99it/s]\n",
            "load_f1 = 0.9608062709966406\n",
            "real_f1 = 0.9588431590656283\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5cd9b9-a1d2-48dd-e3b1-56cf46c92059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6312988996505737\n",
            "step: 10, loss: 0.6120545268058777\n",
            "step: 20, loss: 0.5849272608757019\n",
            "step: 30, loss: 0.38294240832328796\n",
            "step: 40, loss: 0.37277650833129883\n",
            "step: 50, loss: 0.17793136835098267\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.17053000628948212\n",
            "step: 70, loss: 0.2606331706047058\n",
            "step: 80, loss: 0.19594521820545197\n",
            "step: 90, loss: 0.3203250765800476\n",
            "step: 100, loss: 0.22131787240505219\n",
            "step: 110, loss: 0.08648928254842758\n",
            "step: 120, loss: 0.11615309864282608\n",
            "step: 130, loss: 0.11199084669351578\n",
            "step: 140, loss: 0.1105596199631691\n",
            "step: 150, loss: 0.06963010877370834\n",
            "step: 160, loss: 0.08014663308858871\n",
            "step: 170, loss: 0.2961997389793396\n",
            "step: 180, loss: 0.10005684196949005\n",
            "step: 190, loss: 0.025555722415447235\n",
            "step: 200, loss: 0.18905536830425262\n",
            "step: 210, loss: 0.07123304903507233\n",
            "step: 220, loss: 0.2086721658706665\n",
            "step: 230, loss: 0.23081597685813904\n",
            "step: 240, loss: 0.20672421157360077\n",
            "step: 250, loss: 0.09918347001075745\n",
            "step: 260, loss: 0.060434482991695404\n",
            "step: 270, loss: 0.0143789853900671\n",
            "step: 280, loss: 0.10697359591722488\n",
            "step: 290, loss: 0.16587094962596893\n",
            "step: 300, loss: 0.056003063917160034\n",
            "step: 310, loss: 0.19659112393856049\n",
            "step: 320, loss: 0.08217141032218933\n",
            "step: 330, loss: 0.07505617290735245\n",
            "step: 340, loss: 0.18367835879325867\n",
            "step: 350, loss: 0.04619000852108002\n",
            "step: 360, loss: 0.10206036269664764\n",
            "step: 370, loss: 0.1216365322470665\n",
            "step: 380, loss: 0.05186821147799492\n",
            "step: 390, loss: 0.07757783681154251\n",
            "step: 400, loss: 0.36001312732696533\n",
            "step: 410, loss: 0.08900579810142517\n",
            "step: 420, loss: 0.20354539155960083\n",
            "step: 430, loss: 0.151458278298378\n",
            "step: 440, loss: 0.08696899563074112\n",
            "step: 450, loss: 0.006797574460506439\n",
            "step: 460, loss: 0.06720329821109772\n",
            "step: 470, loss: 0.13491079211235046\n",
            "step: 480, loss: 0.18444105982780457\n",
            "step: 490, loss: 0.14394702017307281\n",
            "step: 500, loss: 0.06621420383453369\n",
            "step: 510, loss: 0.10721459239721298\n",
            "step: 520, loss: 0.0883471742272377\n",
            "step: 530, loss: 0.006052887067198753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.913003663003663, f1=0.8923499770957398, best_f1=0.8923499770957398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11305545270442963\n",
            "step: 10, loss: 0.15229228138923645\n",
            "step: 20, loss: 0.07153095304965973\n",
            "step: 30, loss: 0.08310869336128235\n",
            "step: 40, loss: 0.05879473686218262\n",
            "step: 50, loss: 0.146795392036438\n",
            "step: 60, loss: 0.08028260618448257\n",
            "step: 70, loss: 0.06063029170036316\n",
            "step: 80, loss: 0.12396625429391861\n",
            "step: 90, loss: 0.1027035266160965\n",
            "step: 100, loss: 0.028267860412597656\n",
            "step: 110, loss: 0.01986747235059738\n",
            "step: 120, loss: 0.04510263726115227\n",
            "step: 130, loss: 0.09240365773439407\n",
            "step: 140, loss: 0.0231282077729702\n",
            "step: 150, loss: 0.1773117333650589\n",
            "step: 160, loss: 0.06647161394357681\n",
            "step: 170, loss: 0.23635746538639069\n",
            "step: 180, loss: 0.009736026637256145\n",
            "step: 190, loss: 0.08305622637271881\n",
            "step: 200, loss: 0.022811289876699448\n",
            "step: 210, loss: 0.18900223076343536\n",
            "step: 220, loss: 0.08740471303462982\n",
            "step: 230, loss: 0.059232961386442184\n",
            "step: 240, loss: 0.05392644554376602\n",
            "step: 250, loss: 0.13166972994804382\n",
            "step: 260, loss: 0.08939006924629211\n",
            "step: 270, loss: 0.2770560085773468\n",
            "step: 280, loss: 0.19335998594760895\n",
            "step: 290, loss: 0.03394879773259163\n",
            "step: 300, loss: 0.14351342618465424\n",
            "step: 310, loss: 0.041352011263370514\n",
            "step: 320, loss: 0.1562167853116989\n",
            "step: 330, loss: 0.13899211585521698\n",
            "step: 340, loss: 0.08546307682991028\n",
            "step: 350, loss: 0.011348743923008442\n",
            "step: 360, loss: 0.07288603484630585\n",
            "step: 370, loss: 0.25682732462882996\n",
            "step: 380, loss: 0.08678384125232697\n",
            "step: 390, loss: 0.13939814269542694\n",
            "step: 400, loss: 0.04330313578248024\n",
            "step: 410, loss: 0.03218091279268265\n",
            "step: 420, loss: 0.02307196520268917\n",
            "step: 430, loss: 0.22337114810943604\n",
            "step: 440, loss: 0.1395205408334732\n",
            "step: 450, loss: 0.12499570101499557\n",
            "step: 460, loss: 0.025294912979006767\n",
            "step: 470, loss: 0.027754249051213264\n",
            "step: 480, loss: 0.20959994196891785\n",
            "step: 490, loss: 0.05026626959443092\n",
            "step: 500, loss: 0.41876840591430664\n",
            "step: 510, loss: 0.03487929701805115\n",
            "step: 520, loss: 0.16151748597621918\n",
            "step: 530, loss: 0.04522519186139107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9181258873639375, f1=0.9005681818181818, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031016001477837563\n",
            "step: 10, loss: 0.029693765565752983\n",
            "step: 20, loss: 0.09549643099308014\n",
            "step: 30, loss: 0.1563584953546524\n",
            "step: 40, loss: 0.04945315793156624\n",
            "step: 50, loss: 0.19116738438606262\n",
            "step: 60, loss: 0.04421960934996605\n",
            "step: 70, loss: 0.013662501238286495\n",
            "step: 80, loss: 0.002956658136099577\n",
            "step: 90, loss: 0.030283527448773384\n",
            "step: 100, loss: 0.007975936867296696\n",
            "step: 110, loss: 0.044260863214731216\n",
            "step: 120, loss: 0.0016222380800172687\n",
            "step: 130, loss: 0.0026013976894319057\n",
            "step: 140, loss: 0.011346772313117981\n",
            "step: 150, loss: 0.13989698886871338\n",
            "step: 160, loss: 0.04320318624377251\n",
            "step: 170, loss: 0.06611044704914093\n",
            "step: 180, loss: 0.05332259088754654\n",
            "step: 190, loss: 0.07449214160442352\n",
            "step: 200, loss: 0.11808319389820099\n",
            "step: 210, loss: 0.0815882533788681\n",
            "step: 220, loss: 0.07044297456741333\n",
            "step: 230, loss: 0.1668318212032318\n",
            "step: 240, loss: 0.00894970539957285\n",
            "step: 250, loss: 0.13657288253307343\n",
            "step: 260, loss: 0.04042649641633034\n",
            "step: 270, loss: 0.018261082470417023\n",
            "step: 280, loss: 0.22486862540245056\n",
            "step: 290, loss: 0.008312651887536049\n",
            "step: 300, loss: 0.013183430768549442\n",
            "step: 310, loss: 0.043125275522470474\n",
            "step: 320, loss: 0.05662455037236214\n",
            "step: 330, loss: 0.03462713956832886\n",
            "step: 340, loss: 0.012729895301163197\n",
            "step: 350, loss: 0.08194644749164581\n",
            "step: 360, loss: 0.0067095039412379265\n",
            "step: 370, loss: 0.034975919872522354\n",
            "step: 380, loss: 0.00768465967848897\n",
            "step: 390, loss: 0.011450052261352539\n",
            "step: 400, loss: 0.04957668483257294\n",
            "step: 410, loss: 0.009186419658362865\n",
            "step: 420, loss: 0.24368806183338165\n",
            "step: 430, loss: 0.029198719188570976\n",
            "step: 440, loss: 0.024410221725702286\n",
            "step: 450, loss: 0.03795856237411499\n",
            "step: 460, loss: 0.020546993240714073\n",
            "step: 470, loss: 0.05856271833181381\n",
            "step: 480, loss: 0.015570420771837234\n",
            "step: 490, loss: 0.01077294535934925\n",
            "step: 500, loss: 0.12738068401813507\n",
            "step: 510, loss: 0.009545812383294106\n",
            "step: 520, loss: 0.07107733935117722\n",
            "step: 530, loss: 0.0717330276966095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9138012246820537, f1=0.8915318744053282, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022538531571626663\n",
            "step: 10, loss: 0.037278398871421814\n",
            "step: 20, loss: 0.001016333349980414\n",
            "step: 30, loss: 0.01953766494989395\n",
            "step: 40, loss: 0.055764514952898026\n",
            "step: 50, loss: 0.006387483328580856\n",
            "step: 60, loss: 0.008199410513043404\n",
            "step: 70, loss: 0.004025967791676521\n",
            "step: 80, loss: 0.002371171023696661\n",
            "step: 90, loss: 0.1686495840549469\n",
            "step: 100, loss: 0.0039262366481125355\n",
            "step: 110, loss: 0.06476486474275589\n",
            "step: 120, loss: 0.0014123163418844342\n",
            "step: 130, loss: 0.0007426138618029654\n",
            "step: 140, loss: 0.0008113610674627125\n",
            "step: 150, loss: 0.026251990348100662\n",
            "step: 160, loss: 0.085244320333004\n",
            "step: 170, loss: 0.0848381295800209\n",
            "step: 180, loss: 0.0015504079638049006\n",
            "step: 190, loss: 0.020991051569581032\n",
            "step: 200, loss: 0.003955488558858633\n",
            "step: 210, loss: 0.06352987885475159\n",
            "step: 220, loss: 0.003663974115625024\n",
            "step: 230, loss: 0.0045556253753602505\n",
            "step: 240, loss: 0.00847413670271635\n",
            "step: 250, loss: 0.05843978002667427\n",
            "step: 260, loss: 0.0025386016350239515\n",
            "step: 270, loss: 0.052015453577041626\n",
            "step: 280, loss: 0.1287452131509781\n",
            "step: 290, loss: 0.010530262254178524\n",
            "step: 300, loss: 0.0008414026233367622\n",
            "step: 310, loss: 0.008850193582475185\n",
            "step: 320, loss: 0.006421247962862253\n",
            "step: 330, loss: 0.01515323854982853\n",
            "step: 340, loss: 0.01951196976006031\n",
            "step: 350, loss: 0.05244719237089157\n",
            "step: 360, loss: 0.05728236585855484\n",
            "step: 370, loss: 0.007413393352180719\n",
            "step: 380, loss: 0.0033177677541971207\n",
            "step: 390, loss: 0.0032982905395329\n",
            "step: 400, loss: 0.05648469552397728\n",
            "step: 410, loss: 0.0015152875566855073\n",
            "step: 420, loss: 0.14740674197673798\n",
            "step: 430, loss: 0.04684118926525116\n",
            "step: 440, loss: 0.013112715445458889\n",
            "step: 450, loss: 0.020559515804052353\n",
            "step: 460, loss: 0.010557946749031544\n",
            "step: 470, loss: 0.03422988951206207\n",
            "step: 480, loss: 0.11202484369277954\n",
            "step: 490, loss: 0.006273996084928513\n",
            "step: 500, loss: 0.006728064268827438\n",
            "step: 510, loss: 0.026427708566188812\n",
            "step: 520, loss: 0.0380120724439621\n",
            "step: 530, loss: 0.0544663704931736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9145259224661373, f1=0.9014615747289015, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06367892771959305\n",
            "step: 10, loss: 0.09834224730730057\n",
            "step: 20, loss: 0.00909350998699665\n",
            "step: 30, loss: 0.005762360990047455\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.14653746783733368\n",
            "step: 50, loss: 0.002226076787337661\n",
            "step: 60, loss: 0.11363694816827774\n",
            "step: 70, loss: 0.01799924299120903\n",
            "step: 80, loss: 0.008626868948340416\n",
            "step: 90, loss: 0.0694662407040596\n",
            "step: 100, loss: 0.008425022475421429\n",
            "step: 110, loss: 0.0011746177915483713\n",
            "step: 120, loss: 0.0984334871172905\n",
            "step: 130, loss: 0.0005582415033131838\n",
            "step: 140, loss: 0.0021649289410561323\n",
            "step: 150, loss: 0.004416324198246002\n",
            "step: 160, loss: 0.0007262262515723705\n",
            "step: 170, loss: 0.019370675086975098\n",
            "step: 180, loss: 0.001357095199637115\n",
            "step: 190, loss: 0.03648838773369789\n",
            "step: 200, loss: 0.006638785358518362\n",
            "step: 210, loss: 0.027766967192292213\n",
            "step: 220, loss: 0.011332766152918339\n",
            "step: 230, loss: 0.02200070023536682\n",
            "step: 240, loss: 0.005779515020549297\n",
            "step: 250, loss: 0.07460721582174301\n",
            "step: 260, loss: 0.008417056873440742\n",
            "step: 270, loss: 0.0050382767803967\n",
            "step: 280, loss: 0.010271595790982246\n",
            "step: 290, loss: 0.21948400139808655\n",
            "step: 300, loss: 0.024017583578824997\n",
            "step: 310, loss: 0.002083300380036235\n",
            "step: 320, loss: 0.07664375007152557\n",
            "step: 330, loss: 0.05871360003948212\n",
            "step: 340, loss: 0.0029011585284024477\n",
            "step: 350, loss: 0.0007540606311522424\n",
            "step: 360, loss: 0.0067685917019844055\n",
            "step: 370, loss: 0.02860182337462902\n",
            "step: 380, loss: 0.006291455589234829\n",
            "step: 390, loss: 0.00044679566053673625\n",
            "step: 400, loss: 0.009436710737645626\n",
            "step: 410, loss: 0.012626893818378448\n",
            "step: 420, loss: 0.0015561410691589117\n",
            "step: 430, loss: 0.012783823534846306\n",
            "step: 440, loss: 0.24412430822849274\n",
            "step: 450, loss: 0.05246780440211296\n",
            "step: 460, loss: 0.0031237625516951084\n",
            "step: 470, loss: 0.0018155827419832349\n",
            "step: 480, loss: 0.003912054002285004\n",
            "step: 490, loss: 0.0006713010952807963\n",
            "step: 500, loss: 0.00630223099142313\n",
            "step: 510, loss: 0.048610568046569824\n",
            "step: 520, loss: 0.01101086474955082\n",
            "step: 530, loss: 0.05384765565395355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8980952380952382, f1=0.8864503816793893, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031991302967071533\n",
            "step: 10, loss: 0.0005165531183592975\n",
            "step: 20, loss: 0.12752148509025574\n",
            "step: 30, loss: 0.004158813040703535\n",
            "step: 40, loss: 0.004099155310541391\n",
            "step: 50, loss: 0.0013380462769418955\n",
            "step: 60, loss: 0.139240562915802\n",
            "step: 70, loss: 0.015984104946255684\n",
            "step: 80, loss: 0.00990466121584177\n",
            "step: 90, loss: 0.009274952113628387\n",
            "step: 100, loss: 0.0039835041388869286\n",
            "step: 110, loss: 0.03128312528133392\n",
            "step: 120, loss: 0.011097203008830547\n",
            "step: 130, loss: 0.001387601369060576\n",
            "step: 140, loss: 0.015628674998879433\n",
            "step: 150, loss: 0.0003766172449104488\n",
            "step: 160, loss: 0.0020157883409410715\n",
            "step: 170, loss: 0.06548310816287994\n",
            "step: 180, loss: 0.0017874727491289377\n",
            "step: 190, loss: 0.0014144309097900987\n",
            "step: 200, loss: 0.027173127979040146\n",
            "step: 210, loss: 0.0012947152135893703\n",
            "step: 220, loss: 0.0015712257008999586\n",
            "step: 230, loss: 0.0004603344714269042\n",
            "step: 240, loss: 0.003270823508501053\n",
            "step: 250, loss: 0.0003764788853004575\n",
            "step: 260, loss: 0.00022685667499899864\n",
            "step: 270, loss: 0.15691280364990234\n",
            "step: 280, loss: 0.003065458731725812\n",
            "step: 290, loss: 0.0005753978621214628\n",
            "step: 300, loss: 0.0026093740016222\n",
            "step: 310, loss: 0.003978602588176727\n",
            "step: 320, loss: 0.0035488714929670095\n",
            "step: 330, loss: 0.007832012139260769\n",
            "step: 340, loss: 0.06885653734207153\n",
            "step: 350, loss: 0.03804619610309601\n",
            "step: 360, loss: 0.01733354665338993\n",
            "step: 370, loss: 0.017749378457665443\n",
            "step: 380, loss: 0.009627070277929306\n",
            "step: 390, loss: 0.005876166746020317\n",
            "step: 400, loss: 0.02174387127161026\n",
            "step: 410, loss: 0.015447588637471199\n",
            "step: 420, loss: 0.11577188968658447\n",
            "step: 430, loss: 0.00439140060916543\n",
            "step: 440, loss: 0.0006742941914126277\n",
            "step: 450, loss: 0.004556441679596901\n",
            "step: 460, loss: 0.0003957226581405848\n",
            "step: 470, loss: 0.0005109530175104737\n",
            "step: 480, loss: 0.015412931330502033\n",
            "step: 490, loss: 0.00045011434121988714\n",
            "step: 500, loss: 0.00936537142843008\n",
            "step: 510, loss: 0.0007825492648407817\n",
            "step: 520, loss: 0.006670070346444845\n",
            "step: 530, loss: 0.020505623891949654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9037106622827618, f1=0.886783514921838, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014739183825440705\n",
            "step: 10, loss: 0.18337851762771606\n",
            "step: 20, loss: 0.0011373715242370963\n",
            "step: 30, loss: 0.011227548122406006\n",
            "step: 40, loss: 0.0019879986066371202\n",
            "step: 50, loss: 0.09355413913726807\n",
            "step: 60, loss: 0.0015540615422651172\n",
            "step: 70, loss: 0.004838928114622831\n",
            "step: 80, loss: 0.059558916836977005\n",
            "step: 90, loss: 0.007386140059679747\n",
            "step: 100, loss: 0.000541446846909821\n",
            "step: 110, loss: 0.0031261369585990906\n",
            "step: 120, loss: 0.0002743565128184855\n",
            "step: 130, loss: 0.0008884160197339952\n",
            "step: 140, loss: 0.01066612172871828\n",
            "step: 150, loss: 0.00771903432905674\n",
            "step: 160, loss: 0.0013948369305580854\n",
            "step: 170, loss: 0.0007170294993557036\n",
            "step: 180, loss: 0.00030318062636069953\n",
            "step: 190, loss: 0.20494507253170013\n",
            "step: 200, loss: 0.010875605046749115\n",
            "step: 210, loss: 0.012767067179083824\n",
            "step: 220, loss: 0.0006340679829008877\n",
            "step: 230, loss: 0.0018426422029733658\n",
            "step: 240, loss: 0.003306909929960966\n",
            "step: 250, loss: 0.002016800455749035\n",
            "step: 260, loss: 0.00021471831132657826\n",
            "step: 270, loss: 0.00021052395459264517\n",
            "step: 280, loss: 0.0011779608903452754\n",
            "step: 290, loss: 0.002100408310070634\n",
            "step: 300, loss: 0.0028668008744716644\n",
            "step: 310, loss: 0.0001383631315547973\n",
            "step: 320, loss: 0.00012335697829257697\n",
            "step: 330, loss: 0.0006424038438126445\n",
            "step: 340, loss: 0.007422857917845249\n",
            "step: 350, loss: 0.0031126709654927254\n",
            "step: 360, loss: 0.003907713573426008\n",
            "step: 370, loss: 0.000960119825322181\n",
            "step: 380, loss: 0.003780276281759143\n",
            "step: 390, loss: 0.0002546731266193092\n",
            "step: 400, loss: 0.0016679356340318918\n",
            "step: 410, loss: 0.0029660894069820642\n",
            "step: 420, loss: 0.00012117554433643818\n",
            "step: 430, loss: 0.00023283690097741783\n",
            "step: 440, loss: 0.03919306769967079\n",
            "step: 450, loss: 0.00364487711340189\n",
            "step: 460, loss: 0.000489572761580348\n",
            "step: 470, loss: 0.006237827241420746\n",
            "step: 480, loss: 0.04295346513390541\n",
            "step: 490, loss: 0.0015192005084827542\n",
            "step: 500, loss: 0.0003341458213981241\n",
            "step: 510, loss: 0.004332198761403561\n",
            "step: 520, loss: 0.02803516574203968\n",
            "step: 530, loss: 0.021037602797150612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9140698772426818, f1=0.8898508898508899, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001074297004379332\n",
            "step: 10, loss: 0.003644264303147793\n",
            "step: 20, loss: 0.0006642626249231398\n",
            "step: 30, loss: 0.000665127532556653\n",
            "step: 40, loss: 0.07342851161956787\n",
            "step: 50, loss: 0.01611611433327198\n",
            "step: 60, loss: 0.0007113544270396233\n",
            "step: 70, loss: 0.001449744449928403\n",
            "step: 80, loss: 0.10949136316776276\n",
            "step: 90, loss: 0.0002530768688302487\n",
            "step: 100, loss: 0.0018889844650402665\n",
            "step: 110, loss: 0.0006997347227297723\n",
            "step: 120, loss: 0.20848067104816437\n",
            "step: 130, loss: 0.0008963398286141455\n",
            "step: 140, loss: 0.0016909635160118341\n",
            "step: 150, loss: 0.0015421988209709525\n",
            "step: 160, loss: 0.0767889991402626\n",
            "step: 170, loss: 0.015623362734913826\n",
            "step: 180, loss: 0.0029089662712067366\n",
            "step: 190, loss: 0.0017719202442094684\n",
            "step: 200, loss: 0.00603528693318367\n",
            "step: 210, loss: 0.0003927029320038855\n",
            "step: 220, loss: 0.004190640524029732\n",
            "step: 230, loss: 0.021710962057113647\n",
            "step: 240, loss: 0.002057307865470648\n",
            "step: 250, loss: 0.000249202101258561\n",
            "step: 260, loss: 0.03199220821261406\n",
            "step: 270, loss: 0.013421478681266308\n",
            "step: 280, loss: 0.05119415372610092\n",
            "step: 290, loss: 0.0011754874140024185\n",
            "step: 300, loss: 0.002590422285720706\n",
            "step: 310, loss: 0.00010209763422608376\n",
            "step: 320, loss: 0.046603139489889145\n",
            "step: 330, loss: 0.00012330309255048633\n",
            "step: 340, loss: 0.003121984424069524\n",
            "step: 350, loss: 0.016036562621593475\n",
            "step: 360, loss: 0.0002512555511202663\n",
            "step: 370, loss: 0.0001726622140267864\n",
            "step: 380, loss: 0.0004898954066447914\n",
            "step: 390, loss: 0.001714494195766747\n",
            "step: 400, loss: 0.00010664881119737402\n",
            "step: 410, loss: 0.00012071152741555125\n",
            "step: 420, loss: 0.0005666168872267008\n",
            "step: 430, loss: 0.009544582106173038\n",
            "step: 440, loss: 0.01039167307317257\n",
            "step: 450, loss: 0.00012258286005817354\n",
            "step: 460, loss: 0.0015024684835225344\n",
            "step: 470, loss: 0.00018620496848598123\n",
            "step: 480, loss: 0.00015108197112567723\n",
            "step: 490, loss: 0.0018328914884477854\n",
            "step: 500, loss: 0.012814669869840145\n",
            "step: 510, loss: 0.0001599111274117604\n",
            "step: 520, loss: 0.002212973078712821\n",
            "step: 530, loss: 8.926507871365175e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9170593779453347, f1=0.8934348239771647, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015057733980938792\n",
            "step: 10, loss: 0.0009879773715510964\n",
            "step: 20, loss: 0.0009467718773521483\n",
            "step: 30, loss: 0.001502510509453714\n",
            "step: 40, loss: 0.0011274563148617744\n",
            "step: 50, loss: 7.710942736594006e-05\n",
            "step: 60, loss: 5.604400939773768e-05\n",
            "step: 70, loss: 6.517820293083787e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.006985502317547798\n",
            "step: 90, loss: 0.00011166165495524183\n",
            "step: 100, loss: 0.004533527418971062\n",
            "step: 110, loss: 9.266142296837643e-05\n",
            "step: 120, loss: 0.000684081285726279\n",
            "step: 130, loss: 0.003267135936766863\n",
            "step: 140, loss: 0.0003030105144716799\n",
            "step: 150, loss: 7.93580838944763e-05\n",
            "step: 160, loss: 0.0008808259153738618\n",
            "step: 170, loss: 0.0026136855594813824\n",
            "step: 180, loss: 6.203730299603194e-05\n",
            "step: 190, loss: 0.00021458846458699554\n",
            "step: 200, loss: 0.00021462092990987003\n",
            "step: 210, loss: 0.00037100317422300577\n",
            "step: 220, loss: 0.00019036269804928452\n",
            "step: 230, loss: 0.0001881328789750114\n",
            "step: 240, loss: 8.949844777816907e-05\n",
            "step: 250, loss: 0.0003469125076662749\n",
            "step: 260, loss: 0.004943342413753271\n",
            "step: 270, loss: 6.117023440310732e-05\n",
            "step: 280, loss: 0.0003319739771541208\n",
            "step: 290, loss: 0.000868295319378376\n",
            "step: 300, loss: 7.347852806560695e-05\n",
            "step: 310, loss: 0.0287235826253891\n",
            "step: 320, loss: 9.314106137026101e-05\n",
            "step: 330, loss: 0.0003014870162587613\n",
            "step: 340, loss: 8.045372669585049e-05\n",
            "step: 350, loss: 0.00022762508888263255\n",
            "step: 360, loss: 9.242469241144136e-05\n",
            "step: 370, loss: 9.044050239026546e-05\n",
            "step: 380, loss: 0.0013955117901787162\n",
            "step: 390, loss: 0.009962080977857113\n",
            "step: 400, loss: 0.0003450678486842662\n",
            "step: 410, loss: 0.05169619619846344\n",
            "step: 420, loss: 0.00035982075496576726\n",
            "step: 430, loss: 0.0006077808793634176\n",
            "step: 440, loss: 0.0006114688003435731\n",
            "step: 450, loss: 0.004486437421292067\n",
            "step: 460, loss: 0.001243386883288622\n",
            "step: 470, loss: 0.00027869342011399567\n",
            "step: 480, loss: 8.423202234553173e-05\n",
            "step: 490, loss: 0.01492652390152216\n",
            "step: 500, loss: 0.0012763573322445154\n",
            "step: 510, loss: 0.009404348209500313\n",
            "step: 520, loss: 8.276075823232532e-05\n",
            "step: 530, loss: 0.0009393560467287898\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9136960600375235, f1=0.895032802249297, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.8765887186164036e-05\n",
            "step: 10, loss: 6.342010601656511e-05\n",
            "step: 20, loss: 4.026537499157712e-05\n",
            "step: 30, loss: 5.4461979743791744e-05\n",
            "step: 40, loss: 0.005910925101488829\n",
            "step: 50, loss: 0.010535500943660736\n",
            "step: 60, loss: 0.00012686551781371236\n",
            "step: 70, loss: 3.5180568374926224e-05\n",
            "step: 80, loss: 4.174064815742895e-05\n",
            "step: 90, loss: 5.2855670219287276e-05\n",
            "step: 100, loss: 3.46331435139291e-05\n",
            "step: 110, loss: 5.3820254834135994e-05\n",
            "step: 120, loss: 0.00018047119374386966\n",
            "step: 130, loss: 8.325683302246034e-05\n",
            "step: 140, loss: 0.0002665536303538829\n",
            "step: 150, loss: 9.198034967994317e-05\n",
            "step: 160, loss: 6.329462485155091e-05\n",
            "step: 170, loss: 0.00014977114915382117\n",
            "step: 180, loss: 0.00017346924869343638\n",
            "step: 190, loss: 0.004861611407250166\n",
            "step: 200, loss: 0.000794108840636909\n",
            "step: 210, loss: 0.00022404214541893452\n",
            "step: 220, loss: 0.03133158013224602\n",
            "step: 230, loss: 9.961240721167997e-05\n",
            "step: 240, loss: 0.0007355646812357008\n",
            "step: 250, loss: 0.0010716193355619907\n",
            "step: 260, loss: 0.0004482263175304979\n",
            "step: 270, loss: 3.6957186239305884e-05\n",
            "step: 280, loss: 5.268466338748112e-05\n",
            "step: 290, loss: 0.0012435272801667452\n",
            "step: 300, loss: 0.12056494504213333\n",
            "step: 310, loss: 6.980368198128417e-05\n",
            "step: 320, loss: 0.07403085380792618\n",
            "step: 330, loss: 3.3742257073754445e-05\n",
            "step: 340, loss: 0.0005802945815958083\n",
            "step: 350, loss: 0.00046765367733314633\n",
            "step: 360, loss: 0.0016569859581068158\n",
            "step: 370, loss: 0.000140256219310686\n",
            "step: 380, loss: 0.0039482032880187035\n",
            "step: 390, loss: 0.004320201929658651\n",
            "step: 400, loss: 0.0018637534230947495\n",
            "step: 410, loss: 0.00036406697472557425\n",
            "step: 420, loss: 0.0005903568235225976\n",
            "step: 430, loss: 6.979315367061645e-05\n",
            "step: 440, loss: 0.0026210895739495754\n",
            "step: 450, loss: 0.00010965659748762846\n",
            "step: 460, loss: 0.0006659994833171368\n",
            "step: 470, loss: 0.00015615923621226102\n",
            "step: 480, loss: 4.356934368843213e-05\n",
            "step: 490, loss: 6.907583156134933e-05\n",
            "step: 500, loss: 0.009920618496835232\n",
            "step: 510, loss: 0.00014972513599786907\n",
            "step: 520, loss: 0.0012542230542749166\n",
            "step: 530, loss: 3.978062159148976e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.912, f1=0.8976303317535544, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026862790808081627\n",
            "step: 10, loss: 0.0015570848481729627\n",
            "step: 20, loss: 7.837265002308413e-05\n",
            "step: 30, loss: 9.737965592648834e-05\n",
            "step: 40, loss: 3.780944098252803e-05\n",
            "step: 50, loss: 9.712979954201728e-05\n",
            "step: 60, loss: 0.0036227023229002953\n",
            "step: 70, loss: 0.00039325476973317564\n",
            "step: 80, loss: 0.0006416525575332344\n",
            "step: 90, loss: 7.528246351284906e-05\n",
            "step: 100, loss: 0.0004066140973009169\n",
            "step: 110, loss: 7.765692134853452e-05\n",
            "step: 120, loss: 4.860326225752942e-05\n",
            "step: 130, loss: 0.0010638892417773604\n",
            "step: 140, loss: 8.149279892677441e-05\n",
            "step: 150, loss: 0.00012130776303820312\n",
            "step: 160, loss: 4.605590584105812e-05\n",
            "step: 170, loss: 3.3913744118763134e-05\n",
            "step: 180, loss: 7.996754720807076e-05\n",
            "step: 190, loss: 0.0004626686277333647\n",
            "step: 200, loss: 0.0002533129299990833\n",
            "step: 210, loss: 0.00018377319793216884\n",
            "step: 220, loss: 3.1328832847066224e-05\n",
            "step: 230, loss: 3.0560113373212516e-05\n",
            "step: 240, loss: 3.0479410270345397e-05\n",
            "step: 250, loss: 0.00019334348326083273\n",
            "step: 260, loss: 8.379311475437135e-05\n",
            "step: 270, loss: 0.00041128668817691505\n",
            "step: 280, loss: 0.00015555144636891782\n",
            "step: 290, loss: 0.00020994678197894245\n",
            "step: 300, loss: 3.6234603612683713e-05\n",
            "step: 310, loss: 0.00036331542651169\n",
            "step: 320, loss: 0.0010765183251351118\n",
            "step: 330, loss: 7.789513620082289e-05\n",
            "step: 340, loss: 6.583805225091055e-05\n",
            "step: 350, loss: 5.406378841144033e-05\n",
            "step: 360, loss: 0.0002609499788377434\n",
            "step: 370, loss: 0.0006208647391758859\n",
            "step: 380, loss: 0.00011278628517175093\n",
            "step: 390, loss: 7.112978346413001e-05\n",
            "step: 400, loss: 5.076901652500965e-05\n",
            "step: 410, loss: 0.000766577897593379\n",
            "step: 420, loss: 0.0004513306193985045\n",
            "step: 430, loss: 0.00037765741581097245\n",
            "step: 440, loss: 0.0003368455800227821\n",
            "step: 450, loss: 0.00014412702876143157\n",
            "step: 460, loss: 0.00019777686975430697\n",
            "step: 470, loss: 0.00015105344937182963\n",
            "step: 480, loss: 0.00039235380245372653\n",
            "step: 490, loss: 0.0001515205076429993\n",
            "step: 500, loss: 0.0009484555921517313\n",
            "step: 510, loss: 0.0005622321623377502\n",
            "step: 520, loss: 0.00045590163790620863\n",
            "step: 530, loss: 0.0006657494232058525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9092609915809168, f1=0.8987816307403936, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004645221401005983\n",
            "step: 10, loss: 4.046285175718367e-05\n",
            "step: 20, loss: 0.0004898622282780707\n",
            "step: 30, loss: 5.073330248706043e-05\n",
            "step: 40, loss: 4.5133947423892096e-05\n",
            "step: 50, loss: 0.0006465790793299675\n",
            "step: 60, loss: 4.187813465250656e-05\n",
            "step: 70, loss: 0.00017257590661756694\n",
            "step: 80, loss: 0.000264236907241866\n",
            "step: 90, loss: 9.726163989398628e-05\n",
            "step: 100, loss: 0.0004024810914415866\n",
            "step: 110, loss: 0.0005763708031736314\n",
            "step: 120, loss: 5.8288020227337256e-05\n",
            "step: 130, loss: 0.0013672279892489314\n",
            "step: 140, loss: 0.00019905628869310021\n",
            "step: 150, loss: 4.9954171117860824e-05\n",
            "step: 160, loss: 5.027649604016915e-05\n",
            "step: 170, loss: 9.53081835177727e-05\n",
            "step: 180, loss: 0.00033635986619628966\n",
            "step: 190, loss: 0.0006336223450489342\n",
            "step: 200, loss: 4.5923577999928966e-05\n",
            "step: 210, loss: 4.805703429155983e-05\n",
            "step: 220, loss: 0.0002836530329659581\n",
            "step: 230, loss: 6.614446465391666e-05\n",
            "step: 240, loss: 0.002964322455227375\n",
            "step: 250, loss: 5.0782415200956166e-05\n",
            "step: 260, loss: 5.3863335779169574e-05\n",
            "step: 270, loss: 0.0001489875139668584\n",
            "step: 280, loss: 3.0129400329315104e-05\n",
            "step: 290, loss: 0.0001950897421920672\n",
            "step: 300, loss: 0.00013402807235252112\n",
            "step: 310, loss: 4.325179907027632e-05\n",
            "step: 320, loss: 4.44449542555958e-05\n",
            "step: 330, loss: 3.386866228538565e-05\n",
            "step: 340, loss: 5.357724876375869e-05\n",
            "step: 350, loss: 0.0012606356758624315\n",
            "step: 360, loss: 0.0001356404391117394\n",
            "step: 370, loss: 0.2376595288515091\n",
            "step: 380, loss: 0.00038509684964083135\n",
            "step: 390, loss: 4.961000377079472e-05\n",
            "step: 400, loss: 0.0004480724746827036\n",
            "step: 410, loss: 0.1288650929927826\n",
            "step: 420, loss: 0.0034298799000680447\n",
            "step: 430, loss: 0.00014916181680746377\n",
            "step: 440, loss: 0.09266512095928192\n",
            "step: 450, loss: 0.0107141537591815\n",
            "step: 460, loss: 4.818346133106388e-05\n",
            "step: 470, loss: 4.2176106944680214e-05\n",
            "step: 480, loss: 0.0010080524953082204\n",
            "step: 490, loss: 0.0032599212136119604\n",
            "step: 500, loss: 7.87758908700198e-05\n",
            "step: 510, loss: 0.0002624249318614602\n",
            "step: 520, loss: 9.133601997746155e-05\n",
            "step: 530, loss: 0.0005122878937982023\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.915764705882353, f1=0.8973995271867612, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015205575618892908\n",
            "step: 10, loss: 0.0010598558001220226\n",
            "step: 20, loss: 0.0002796074841171503\n",
            "step: 30, loss: 0.00010317101987311617\n",
            "step: 40, loss: 4.241799615556374e-05\n",
            "step: 50, loss: 0.004288441967219114\n",
            "step: 60, loss: 3.4650609450181946e-05\n",
            "step: 70, loss: 3.689024015329778e-05\n",
            "step: 80, loss: 0.00012051864905515686\n",
            "step: 90, loss: 4.957336204824969e-05\n",
            "step: 100, loss: 3.069572994718328e-05\n",
            "step: 110, loss: 0.012793119065463543\n",
            "step: 120, loss: 4.9776015657698736e-05\n",
            "step: 130, loss: 3.51835296896752e-05\n",
            "step: 140, loss: 6.874444079585373e-05\n",
            "step: 150, loss: 0.00012230820721015334\n",
            "step: 160, loss: 6.335152284009382e-05\n",
            "step: 170, loss: 3.515445496304892e-05\n",
            "step: 180, loss: 0.001122908084653318\n",
            "step: 190, loss: 0.00010251544154016301\n",
            "step: 200, loss: 4.970948430127464e-05\n",
            "step: 210, loss: 3.575410300982185e-05\n",
            "step: 220, loss: 5.4029278544476256e-05\n",
            "step: 230, loss: 4.619134051608853e-05\n",
            "step: 240, loss: 0.0005589399370364845\n",
            "step: 250, loss: 4.783358963322826e-05\n",
            "step: 260, loss: 6.233510066522285e-05\n",
            "step: 270, loss: 3.9234673749888316e-05\n",
            "step: 280, loss: 6.713434413541108e-05\n",
            "step: 290, loss: 0.0011457018554210663\n",
            "step: 300, loss: 7.177166844485328e-05\n",
            "step: 310, loss: 0.00021797252702526748\n",
            "step: 320, loss: 8.574975800001994e-05\n",
            "step: 330, loss: 5.5514585255878046e-05\n",
            "step: 340, loss: 5.6043329095700756e-05\n",
            "step: 350, loss: 0.0001239521079696715\n",
            "step: 360, loss: 5.3049800044391304e-05\n",
            "step: 370, loss: 0.00018583284690976143\n",
            "step: 380, loss: 2.77305025520036e-05\n",
            "step: 390, loss: 0.0002981455181725323\n",
            "step: 400, loss: 0.00014708892558701336\n",
            "step: 410, loss: 0.00023758540919516236\n",
            "step: 420, loss: 3.2297277357429266e-05\n",
            "step: 430, loss: 0.0002620640443637967\n",
            "step: 440, loss: 0.0003820652491413057\n",
            "step: 450, loss: 2.7916750696022063e-05\n",
            "step: 460, loss: 5.133591912453994e-05\n",
            "step: 470, loss: 1.9721463104360737e-05\n",
            "step: 480, loss: 2.1084866602905095e-05\n",
            "step: 490, loss: 3.721046232385561e-05\n",
            "step: 500, loss: 4.0240000089397654e-05\n",
            "step: 510, loss: 3.612229193095118e-05\n",
            "step: 520, loss: 0.0003546687075868249\n",
            "step: 530, loss: 5.017447620048188e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9148737137511693, f1=0.9026798307475318, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.995739386184141e-05\n",
            "step: 10, loss: 2.99317944154609e-05\n",
            "step: 20, loss: 2.363651765335817e-05\n",
            "step: 30, loss: 2.3960716134752147e-05\n",
            "step: 40, loss: 2.5987150365835987e-05\n",
            "step: 50, loss: 0.00035860075149685144\n",
            "step: 60, loss: 1.9036020603380166e-05\n",
            "step: 70, loss: 0.00048607756616547704\n",
            "step: 80, loss: 9.005260653793812e-05\n",
            "step: 90, loss: 4.4331431126920506e-05\n",
            "step: 100, loss: 4.801843169843778e-05\n",
            "step: 110, loss: 3.688264041556977e-05\n",
            "step: 120, loss: 0.001516450080089271\n",
            "step: 130, loss: 0.008664865046739578\n",
            "step: 140, loss: 3.219622522010468e-05\n",
            "step: 150, loss: 2.26606207434088e-05\n",
            "step: 160, loss: 0.00014763603394385427\n",
            "step: 170, loss: 2.5152703528874554e-05\n",
            "step: 180, loss: 3.2990239560604095e-05\n",
            "step: 190, loss: 2.9313341656234115e-05\n",
            "step: 200, loss: 6.312198820523918e-05\n",
            "step: 210, loss: 2.72871257038787e-05\n",
            "step: 220, loss: 0.00018930000078398734\n",
            "step: 230, loss: 9.603664511814713e-05\n",
            "step: 240, loss: 3.4792399674188346e-05\n",
            "step: 250, loss: 3.066578574362211e-05\n",
            "step: 260, loss: 3.542809281498194e-05\n",
            "step: 270, loss: 2.1844803995918483e-05\n",
            "step: 280, loss: 2.089861845888663e-05\n",
            "step: 290, loss: 8.032674668356776e-05\n",
            "step: 300, loss: 0.00010561551607679576\n",
            "step: 310, loss: 6.0356462199706584e-05\n",
            "step: 320, loss: 0.00034044080530293286\n",
            "step: 330, loss: 7.580497913295403e-05\n",
            "step: 340, loss: 2.669488821993582e-05\n",
            "step: 350, loss: 0.012409936636686325\n",
            "step: 360, loss: 2.487320489308331e-05\n",
            "step: 370, loss: 3.7753747164970264e-05\n",
            "step: 380, loss: 2.8106742320233025e-05\n",
            "step: 390, loss: 0.00949680432677269\n",
            "step: 400, loss: 2.464611316099763e-05\n",
            "step: 410, loss: 2.1610085241263732e-05\n",
            "step: 420, loss: 3.728849333128892e-05\n",
            "step: 430, loss: 5.1521798013709486e-05\n",
            "step: 440, loss: 7.800190360285342e-05\n",
            "step: 450, loss: 0.00010982140520354733\n",
            "step: 460, loss: 2.3200760551844724e-05\n",
            "step: 470, loss: 2.974084964080248e-05\n",
            "step: 480, loss: 3.5727447539102286e-05\n",
            "step: 490, loss: 3.0456463719019666e-05\n",
            "step: 500, loss: 1.942341805261094e-05\n",
            "step: 510, loss: 0.0001479663624195382\n",
            "step: 520, loss: 3.785390799748711e-05\n",
            "step: 530, loss: 2.3208205675473437e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9136385087305333, f1=0.9012753897024091, best_f1=0.9005681818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2749962226953357e-05\n",
            "step: 10, loss: 8.594375685788691e-05\n",
            "step: 20, loss: 3.107521843048744e-05\n",
            "step: 30, loss: 7.229580660350621e-05\n",
            "step: 40, loss: 3.539257886586711e-05\n",
            "step: 50, loss: 0.00034811344812624156\n",
            "step: 60, loss: 1.9449485989753157e-05\n",
            "step: 70, loss: 2.4444854716421105e-05\n",
            "step: 80, loss: 2.6273844923707657e-05\n",
            "step: 90, loss: 0.00017293334531132132\n",
            "step: 100, loss: 0.00010076842590933666\n",
            "step: 110, loss: 2.9603959774249233e-05\n",
            "step: 120, loss: 3.578310861485079e-05\n",
            "step: 130, loss: 0.005453836172819138\n",
            "step: 140, loss: 2.0008274077554233e-05\n",
            "step: 150, loss: 3.374975858605467e-05\n",
            "step: 160, loss: 2.6344543584855273e-05\n",
            "step: 170, loss: 2.222845796495676e-05\n",
            "step: 180, loss: 1.676736428635195e-05\n",
            "step: 190, loss: 2.537622094678227e-05\n",
            "step: 200, loss: 4.5839806261938065e-05\n",
            "step: 210, loss: 3.5625998862087727e-05\n",
            "step: 220, loss: 2.5334811653010547e-05\n",
            "step: 230, loss: 7.845590153010562e-05\n",
            "step: 240, loss: 2.3100174075807445e-05\n",
            "step: 250, loss: 7.115996413631365e-05\n",
            "step: 260, loss: 2.2388676370610483e-05\n",
            "step: 270, loss: 2.3133656213758513e-05\n",
            "step: 280, loss: 2.2701527996105142e-05\n",
            "step: 290, loss: 2.0693718397524208e-05\n",
            "step: 300, loss: 3.365152588230558e-05\n",
            "step: 310, loss: 3.8260684959823266e-05\n",
            "step: 320, loss: 7.941704825498164e-05\n",
            "step: 330, loss: 5.558469638344832e-05\n",
            "step: 340, loss: 2.4269786081276834e-05\n",
            "step: 350, loss: 1.9289225747343153e-05\n",
            "step: 360, loss: 3.98768279410433e-05\n",
            "step: 370, loss: 2.2295422240858898e-05\n",
            "step: 380, loss: 2.235141619166825e-05\n",
            "step: 390, loss: 4.6660996304126456e-05\n",
            "step: 400, loss: 2.2690406694891863e-05\n",
            "step: 410, loss: 0.0036327382549643517\n",
            "step: 420, loss: 2.0924608179484494e-05\n",
            "step: 430, loss: 8.229284139815718e-05\n",
            "step: 440, loss: 9.134544234257191e-05\n",
            "step: 450, loss: 3.977260712417774e-05\n",
            "step: 460, loss: 2.3066608264343813e-05\n",
            "step: 470, loss: 4.8712314310250804e-05\n",
            "step: 480, loss: 1.987043651752174e-05\n",
            "step: 490, loss: 2.1788908270536922e-05\n",
            "step: 500, loss: 7.130094309104607e-05\n",
            "step: 510, loss: 3.084295894950628e-05\n",
            "step: 520, loss: 2.3621614673174918e-05\n",
            "step: 530, loss: 2.497751484042965e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9140733859730609, f1=0.9010782934833569, best_f1=0.9005681818181818\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:30, 188.59it/s]\n",
            "load_f1 = 0.9195075757575758\n",
            "real_f1 = 0.9152863227638429\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673b5023-e05b-4352-a0e6-602abe3c2966"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=76eb5becc83d5a086bb88d38716bd9135cedbf72058d0cf6c3559295df529a43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h2l3tupy/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd97448-2c04-494b-9a19-7d46c348dc27"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 457kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.77MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 68.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5580651760101318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3684210526315789, f1=0.33766233766233766, best_f1=0.33766233766233766\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.487705796957016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5555555555555556, f1=0.48, best_f1=0.48\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48420560359954834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.5, f1=0.44067796610169485, best_f1=0.48\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3797960877418518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6222222222222222, f1=0.4905660377358491, best_f1=0.4905660377358491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25421980023384094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6060606060606061, f1=0.5454545454545454, best_f1=0.4905660377358491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.34076476097106934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5384615384615384, f1=0.6206896551724138, best_f1=0.4905660377358491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3629540503025055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6486486486486486, f1=0.55, best_f1=0.55\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14020290970802307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.75, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008622473105788231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6666666666666666, f1=0.5384615384615384, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12401348352432251\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6956521739130435, f1=0.5384615384615384, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04934275895357132\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7200000000000001, f1=0.6000000000000001, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04014194756746292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7741935483870968, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03465431183576584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.7878787878787878, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.047165825963020325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8125000000000001, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07806573808193207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8125000000000001, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 133923.39it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7058823529411764\n",
            "real_f1 = 0.6857142857142857\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 257.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb7f7cc-de19-400d-fbf7-9e89b1ec87aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6152023673057556\n",
            "step: 10, loss: 0.642973005771637\n",
            "step: 20, loss: 0.47908568382263184\n",
            "step: 30, loss: 0.33732637763023376\n",
            "step: 40, loss: 0.22306329011917114\n",
            "step: 50, loss: 0.16794683039188385\n",
            "step: 60, loss: 0.12328655272722244\n",
            "step: 70, loss: 0.03905951604247093\n",
            "step: 80, loss: 0.06941882520914078\n",
            "step: 90, loss: 0.04841301962733269\n",
            "step: 100, loss: 0.004194248467683792\n",
            "step: 110, loss: 0.2970150411128998\n",
            "step: 120, loss: 0.04597296193242073\n",
            "step: 130, loss: 0.029168453067541122\n",
            "step: 140, loss: 0.00946805253624916\n",
            "step: 150, loss: 0.07201418280601501\n",
            "step: 160, loss: 0.020635556429624557\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.20727062225341797\n",
            "step: 180, loss: 0.16266989707946777\n",
            "step: 190, loss: 0.04804529994726181\n",
            "step: 200, loss: 0.023387182503938675\n",
            "step: 210, loss: 0.05062030628323555\n",
            "step: 220, loss: 0.02660377323627472\n",
            "step: 230, loss: 0.05675648897886276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9719416386083053, f1=0.9706546275395034, best_f1=0.9706546275395034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1134919673204422\n",
            "step: 10, loss: 0.006605489645153284\n",
            "step: 20, loss: 0.10926666110754013\n",
            "step: 30, loss: 0.18808215856552124\n",
            "step: 40, loss: 0.07511946558952332\n",
            "step: 50, loss: 0.01607067696750164\n",
            "step: 60, loss: 0.011250887997448444\n",
            "step: 70, loss: 0.18302884697914124\n",
            "step: 80, loss: 0.007688598241657019\n",
            "step: 90, loss: 0.005607320461422205\n",
            "step: 100, loss: 0.006958906073123217\n",
            "step: 110, loss: 0.15910880267620087\n",
            "step: 120, loss: 0.0020396483596414328\n",
            "step: 130, loss: 0.010465122759342194\n",
            "step: 140, loss: 0.026741860434412956\n",
            "step: 150, loss: 0.09666755050420761\n",
            "step: 160, loss: 0.035107821226119995\n",
            "step: 170, loss: 0.0034125763922929764\n",
            "step: 180, loss: 0.0050445920787751675\n",
            "step: 190, loss: 0.07382607460021973\n",
            "step: 200, loss: 0.008446523919701576\n",
            "step: 210, loss: 0.0010954281315207481\n",
            "step: 220, loss: 0.05454663187265396\n",
            "step: 230, loss: 0.02339094877243042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9766407119021134, f1=0.9688195991091313, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012628396973013878\n",
            "step: 10, loss: 0.012472317554056644\n",
            "step: 20, loss: 0.005277307238429785\n",
            "step: 30, loss: 0.0015672520967200398\n",
            "step: 40, loss: 0.08124203234910965\n",
            "step: 50, loss: 0.09487494826316833\n",
            "step: 60, loss: 0.000975185539573431\n",
            "step: 70, loss: 0.05059468746185303\n",
            "step: 80, loss: 0.0003397518885321915\n",
            "step: 90, loss: 0.15499071776866913\n",
            "step: 100, loss: 0.000493288564030081\n",
            "step: 110, loss: 0.0021832669153809547\n",
            "step: 120, loss: 0.03289487585425377\n",
            "step: 130, loss: 0.003403279697522521\n",
            "step: 140, loss: 0.04337836429476738\n",
            "step: 150, loss: 0.06494028121232986\n",
            "step: 160, loss: 0.008172111585736275\n",
            "step: 170, loss: 0.0033379453234374523\n",
            "step: 180, loss: 0.10597190260887146\n",
            "step: 190, loss: 0.03616920858621597\n",
            "step: 200, loss: 0.024507025256752968\n",
            "step: 210, loss: 0.0005018914234824479\n",
            "step: 220, loss: 0.0010960126528516412\n",
            "step: 230, loss: 0.0010388686787337065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9659090909090909, f1=0.9655963302752294, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030850248876959085\n",
            "step: 10, loss: 0.0003233612223993987\n",
            "step: 20, loss: 0.0002934943186119199\n",
            "step: 30, loss: 0.0001795912248780951\n",
            "step: 40, loss: 0.0012395278317853808\n",
            "step: 50, loss: 0.0014830139698460698\n",
            "step: 60, loss: 0.008968710899353027\n",
            "step: 70, loss: 0.0017852711025625467\n",
            "step: 80, loss: 0.007622533943504095\n",
            "step: 90, loss: 0.024081028997898102\n",
            "step: 100, loss: 0.0043403636664152145\n",
            "step: 110, loss: 0.0014084617141634226\n",
            "step: 120, loss: 0.014088444411754608\n",
            "step: 130, loss: 0.0011082038981840014\n",
            "step: 140, loss: 0.0009416933753527701\n",
            "step: 150, loss: 0.1553025245666504\n",
            "step: 160, loss: 0.03940892592072487\n",
            "step: 170, loss: 0.028937172144651413\n",
            "step: 180, loss: 0.0009928479557856917\n",
            "step: 190, loss: 0.04063193500041962\n",
            "step: 200, loss: 0.0013092408189550042\n",
            "step: 210, loss: 0.12553507089614868\n",
            "step: 220, loss: 0.003965489566326141\n",
            "step: 230, loss: 0.048259977251291275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9796380090497738, f1=0.9808773903262092, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002573311096057296\n",
            "step: 10, loss: 0.0029836164321750402\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.045654501765966415\n",
            "step: 30, loss: 0.00029814179288223386\n",
            "step: 40, loss: 0.0003422588633839041\n",
            "step: 50, loss: 0.0002155730762751773\n",
            "step: 60, loss: 0.0002966538304463029\n",
            "step: 70, loss: 0.00019582251843530685\n",
            "step: 80, loss: 0.0004470002604648471\n",
            "step: 90, loss: 0.000784163421485573\n",
            "step: 100, loss: 0.00029462159727700055\n",
            "step: 110, loss: 0.0002236179861938581\n",
            "step: 120, loss: 0.00020779127953574061\n",
            "step: 130, loss: 0.0004684515006374568\n",
            "step: 140, loss: 0.08108212798833847\n",
            "step: 150, loss: 0.1367213875055313\n",
            "step: 160, loss: 0.005343434400856495\n",
            "step: 170, loss: 0.012113839387893677\n",
            "step: 180, loss: 0.019560175016522408\n",
            "step: 190, loss: 0.027233976870775223\n",
            "step: 200, loss: 0.0014534093206748366\n",
            "step: 210, loss: 0.0014427014393731952\n",
            "step: 220, loss: 0.004398318938910961\n",
            "step: 230, loss: 0.00267969723790884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9729119638826186, f1=0.972972972972973, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006143142003566027\n",
            "step: 10, loss: 0.0022012023255228996\n",
            "step: 20, loss: 0.039387594908475876\n",
            "step: 30, loss: 0.00027870663325302303\n",
            "step: 40, loss: 0.0009535423596389592\n",
            "step: 50, loss: 0.00024771972675807774\n",
            "step: 60, loss: 0.00027513678651303053\n",
            "step: 70, loss: 0.0005399013170972466\n",
            "step: 80, loss: 0.05333775654435158\n",
            "step: 90, loss: 0.04187118634581566\n",
            "step: 100, loss: 0.04811769723892212\n",
            "step: 110, loss: 0.0026248206850141287\n",
            "step: 120, loss: 0.0224298182874918\n",
            "step: 130, loss: 0.0017983062425628304\n",
            "step: 140, loss: 0.007895401678979397\n",
            "step: 150, loss: 0.01155738066881895\n",
            "step: 160, loss: 0.10109933465719223\n",
            "step: 170, loss: 0.00046728577581234276\n",
            "step: 180, loss: 0.03788599371910095\n",
            "step: 190, loss: 0.011873210780322552\n",
            "step: 200, loss: 0.0004582286346703768\n",
            "step: 210, loss: 0.09045626223087311\n",
            "step: 220, loss: 0.0004492232983466238\n",
            "step: 230, loss: 0.10660668462514877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9764309764309763, f1=0.9743589743589743, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003284905571490526\n",
            "step: 10, loss: 0.00042978659621439874\n",
            "step: 20, loss: 0.0012761215912178159\n",
            "step: 30, loss: 0.00771896680817008\n",
            "step: 40, loss: 0.0005047654267400503\n",
            "step: 50, loss: 0.00012247431732248515\n",
            "step: 60, loss: 0.00011421951785450801\n",
            "step: 70, loss: 0.012532681226730347\n",
            "step: 80, loss: 0.00021310480951797217\n",
            "step: 90, loss: 0.0003992079582531005\n",
            "step: 100, loss: 0.001272610854357481\n",
            "step: 110, loss: 0.001944702584296465\n",
            "step: 120, loss: 0.0002757494803518057\n",
            "step: 130, loss: 0.0001529179426142946\n",
            "step: 140, loss: 0.0005087650497443974\n",
            "step: 150, loss: 0.0017699608579277992\n",
            "step: 160, loss: 0.06661542505025864\n",
            "step: 170, loss: 0.002014779020100832\n",
            "step: 180, loss: 0.00021507182100322098\n",
            "step: 190, loss: 0.0003153465804643929\n",
            "step: 200, loss: 0.0018056684639304876\n",
            "step: 210, loss: 6.079454760765657e-05\n",
            "step: 220, loss: 0.00020462799875531346\n",
            "step: 230, loss: 0.00013255832891445607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.979591836734694, f1=0.9740698985343857, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.800145663670264e-05\n",
            "step: 10, loss: 0.00013921638310421258\n",
            "step: 20, loss: 0.0048362030647695065\n",
            "step: 30, loss: 0.0002514634979888797\n",
            "step: 40, loss: 0.0002439594391034916\n",
            "step: 50, loss: 0.00031594137544743717\n",
            "step: 60, loss: 0.00036289324634708464\n",
            "step: 70, loss: 0.019082706421613693\n",
            "step: 80, loss: 0.04907650873064995\n",
            "step: 90, loss: 0.0001769417867762968\n",
            "step: 100, loss: 0.00013431439583655447\n",
            "step: 110, loss: 0.0001322216121479869\n",
            "step: 120, loss: 0.0013449226971715689\n",
            "step: 130, loss: 4.898485349258408e-05\n",
            "step: 140, loss: 5.9835248976014555e-05\n",
            "step: 150, loss: 0.0004924678360112011\n",
            "step: 160, loss: 9.836194658419117e-05\n",
            "step: 170, loss: 7.570208981633186e-05\n",
            "step: 180, loss: 0.0005328080151230097\n",
            "step: 190, loss: 0.00011477332009235397\n",
            "step: 200, loss: 0.00014801652287133038\n",
            "step: 210, loss: 8.838741632644087e-05\n",
            "step: 220, loss: 9.100775787374005e-05\n",
            "step: 230, loss: 0.00016850886458996683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9775784753363228, f1=0.9730337078651685, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07272788137197495\n",
            "step: 10, loss: 0.0030842386186122894\n",
            "step: 20, loss: 0.0009928279323503375\n",
            "step: 30, loss: 0.00017563486471772194\n",
            "step: 40, loss: 0.004487267229706049\n",
            "step: 50, loss: 6.255198240978643e-05\n",
            "step: 60, loss: 0.0002481777628418058\n",
            "step: 70, loss: 0.0007157461950555444\n",
            "step: 80, loss: 0.006758770439773798\n",
            "step: 90, loss: 0.000919120735488832\n",
            "step: 100, loss: 0.03649481385946274\n",
            "step: 110, loss: 0.00019266425806563348\n",
            "step: 120, loss: 3.92221991205588e-05\n",
            "step: 130, loss: 0.0002457774244248867\n",
            "step: 140, loss: 0.00019773068197537214\n",
            "step: 150, loss: 0.0008940441184677184\n",
            "step: 160, loss: 0.00012201886420371011\n",
            "step: 170, loss: 0.00016972990124486387\n",
            "step: 180, loss: 0.007404760457575321\n",
            "step: 190, loss: 0.0002954124065581709\n",
            "step: 200, loss: 0.00018152117263525724\n",
            "step: 210, loss: 0.00018214517331216484\n",
            "step: 220, loss: 0.0005048344028182328\n",
            "step: 230, loss: 0.00011026555148418993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9763779527559054, f1=0.972972972972973, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006742983241565526\n",
            "step: 10, loss: 7.472455035895109e-05\n",
            "step: 20, loss: 0.00013119909272063524\n",
            "step: 30, loss: 0.00012759093078784645\n",
            "step: 40, loss: 5.3588988521369174e-05\n",
            "step: 50, loss: 4.4444663217291236e-05\n",
            "step: 60, loss: 8.30345816211775e-05\n",
            "step: 70, loss: 0.00022432075638789684\n",
            "step: 80, loss: 7.058295159367844e-05\n",
            "step: 90, loss: 0.00030447158496826887\n",
            "step: 100, loss: 0.0001922132942127064\n",
            "step: 110, loss: 7.928773993626237e-05\n",
            "step: 120, loss: 7.073193410178646e-05\n",
            "step: 130, loss: 6.856227992102504e-05\n",
            "step: 140, loss: 0.08114824444055557\n",
            "step: 150, loss: 0.007622345816344023\n",
            "step: 160, loss: 0.001475038006901741\n",
            "step: 170, loss: 3.802268111030571e-05\n",
            "step: 180, loss: 7.96044259914197e-05\n",
            "step: 190, loss: 0.00154072733130306\n",
            "step: 200, loss: 3.889799700118601e-05\n",
            "step: 210, loss: 5.489652539836243e-05\n",
            "step: 220, loss: 5.406217314884998e-05\n",
            "step: 230, loss: 0.00021309168369043618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9808342728297633, f1=0.9752252252252253, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.1365022954996675e-05\n",
            "step: 10, loss: 6.244950782274827e-05\n",
            "step: 20, loss: 0.005470974836498499\n",
            "step: 30, loss: 0.011929260566830635\n",
            "step: 40, loss: 6.261100497795269e-05\n",
            "step: 50, loss: 9.209438576363027e-05\n",
            "step: 60, loss: 6.87385763740167e-05\n",
            "step: 70, loss: 7.898407784523442e-05\n",
            "step: 80, loss: 4.335646008257754e-05\n",
            "step: 90, loss: 3.966520671383478e-05\n",
            "step: 100, loss: 5.045926081947982e-05\n",
            "step: 110, loss: 0.00039494462544098496\n",
            "step: 120, loss: 0.00011087229358963668\n",
            "step: 130, loss: 0.00013797437713947147\n",
            "step: 140, loss: 0.0001681863359408453\n",
            "step: 150, loss: 0.0017929155146703124\n",
            "step: 160, loss: 0.00010209815081907436\n",
            "step: 170, loss: 0.0004924061358906329\n",
            "step: 180, loss: 9.260993829229847e-05\n",
            "step: 190, loss: 0.00011227049981243908\n",
            "step: 200, loss: 9.325963037554175e-05\n",
            "step: 210, loss: 6.600052438443527e-05\n",
            "step: 220, loss: 6.399302219506353e-05\n",
            "step: 230, loss: 0.00011007746070390567\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9797752808988766, f1=0.9729119638826186, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.915057045058347e-05\n",
            "step: 10, loss: 0.0002853100304491818\n",
            "step: 20, loss: 6.685442349407822e-05\n",
            "step: 30, loss: 7.472898141713813e-05\n",
            "step: 40, loss: 0.0001390490069752559\n",
            "step: 50, loss: 0.008390413597226143\n",
            "step: 60, loss: 0.00017489930905867368\n",
            "step: 70, loss: 6.552245758939534e-05\n",
            "step: 80, loss: 7.874878065194935e-05\n",
            "step: 90, loss: 4.634031211026013e-05\n",
            "step: 100, loss: 0.00030970462830737233\n",
            "step: 110, loss: 0.0001400786713929847\n",
            "step: 120, loss: 5.4274467402137816e-05\n",
            "step: 130, loss: 8.080504630925134e-05\n",
            "step: 140, loss: 6.460761505877599e-05\n",
            "step: 150, loss: 6.719423254253343e-05\n",
            "step: 160, loss: 7.306145562324673e-05\n",
            "step: 170, loss: 7.052408182062209e-05\n",
            "step: 180, loss: 6.0975751694059e-05\n",
            "step: 190, loss: 0.00012390484334900975\n",
            "step: 200, loss: 4.1833081922959536e-05\n",
            "step: 210, loss: 0.00011122236173832789\n",
            "step: 220, loss: 4.217226160108112e-05\n",
            "step: 230, loss: 6.214497261680663e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9796380090497738, f1=0.9739524348810873, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.983011113945395e-05\n",
            "step: 10, loss: 0.0005079961847513914\n",
            "step: 20, loss: 9.91438064374961e-05\n",
            "step: 30, loss: 7.08340885466896e-05\n",
            "step: 40, loss: 6.413453229470178e-05\n",
            "step: 50, loss: 8.180810982594267e-05\n",
            "step: 60, loss: 6.619618216063827e-05\n",
            "step: 70, loss: 0.0001266994804609567\n",
            "step: 80, loss: 7.106044358806685e-05\n",
            "step: 90, loss: 0.0011031817412003875\n",
            "step: 100, loss: 3.9755035686539486e-05\n",
            "step: 110, loss: 0.1529414802789688\n",
            "step: 120, loss: 0.0008784226956777275\n",
            "step: 130, loss: 0.00017987797036767006\n",
            "step: 140, loss: 6.304193811956793e-05\n",
            "step: 150, loss: 5.453899211715907e-05\n",
            "step: 160, loss: 0.002840946661308408\n",
            "step: 170, loss: 0.00010445933730807155\n",
            "step: 180, loss: 8.73687895364128e-05\n",
            "step: 190, loss: 5.8500059822108597e-05\n",
            "step: 200, loss: 0.00317385490052402\n",
            "step: 210, loss: 7.661611743969843e-05\n",
            "step: 220, loss: 0.0014230618253350258\n",
            "step: 230, loss: 0.00010729907808126882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9796380090497738, f1=0.9728506787330317, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.587126386468299e-05\n",
            "step: 10, loss: 0.009114964865148067\n",
            "step: 20, loss: 8.548131154384464e-05\n",
            "step: 30, loss: 0.0001548653090139851\n",
            "step: 40, loss: 0.0010277745313942432\n",
            "step: 50, loss: 0.00020910415332764387\n",
            "step: 60, loss: 0.0001709893549559638\n",
            "step: 70, loss: 0.0023769736289978027\n",
            "step: 80, loss: 4.441106284502894e-05\n",
            "step: 90, loss: 7.684188312850893e-05\n",
            "step: 100, loss: 7.553654722869396e-05\n",
            "step: 110, loss: 6.204845703905448e-05\n",
            "step: 120, loss: 2.6586887543089688e-05\n",
            "step: 130, loss: 4.586721115629189e-05\n",
            "step: 140, loss: 5.5406246247002855e-05\n",
            "step: 150, loss: 0.0011005926644429564\n",
            "step: 160, loss: 0.0033007788006216288\n",
            "step: 170, loss: 4.519377398537472e-05\n",
            "step: 180, loss: 7.71130362409167e-05\n",
            "step: 190, loss: 5.778158811153844e-05\n",
            "step: 200, loss: 4.82863761135377e-05\n",
            "step: 210, loss: 0.00011884041305165738\n",
            "step: 220, loss: 4.070364229846746e-05\n",
            "step: 230, loss: 0.000103725069493521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9751693002257337, f1=0.9683257918552037, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.6067081964574754e-05\n",
            "step: 10, loss: 3.130986442556605e-05\n",
            "step: 20, loss: 0.00045350557775236666\n",
            "step: 30, loss: 0.00014837904018349946\n",
            "step: 40, loss: 0.00020012620370835066\n",
            "step: 50, loss: 0.04022315517067909\n",
            "step: 60, loss: 0.0001912261504912749\n",
            "step: 70, loss: 0.00010620113607728854\n",
            "step: 80, loss: 5.0669183110585436e-05\n",
            "step: 90, loss: 0.0001329927472397685\n",
            "step: 100, loss: 5.336863250704482e-05\n",
            "step: 110, loss: 0.0002086762833641842\n",
            "step: 120, loss: 4.3930569518124685e-05\n",
            "step: 130, loss: 8.294328290503472e-05\n",
            "step: 140, loss: 3.7166060792515054e-05\n",
            "step: 150, loss: 0.00020898182992823422\n",
            "step: 160, loss: 3.67851389455609e-05\n",
            "step: 170, loss: 3.2226416806224734e-05\n",
            "step: 180, loss: 0.001550908898934722\n",
            "step: 190, loss: 0.00037626922130584717\n",
            "step: 200, loss: 0.00012531243555713445\n",
            "step: 210, loss: 3.884606121573597e-05\n",
            "step: 220, loss: 0.00045123809832148254\n",
            "step: 230, loss: 4.257042019162327e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9761634506242906, f1=0.9704545454545453, best_f1=0.9752252252252253\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 158.73it/s]\n",
            "load_f1 = 0.9797297297297298\n",
            "real_f1 = 0.9751693002257337\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 183.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9a4732-a73d-4943-c846-bb295662521f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6203928589820862\n",
            "step: 10, loss: 0.5956630706787109\n",
            "step: 20, loss: 0.5956918001174927\n",
            "step: 30, loss: 0.3407902717590332\n",
            "step: 40, loss: 0.21666613221168518\n",
            "step: 50, loss: 0.2774430215358734\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.1029977798461914\n",
            "step: 70, loss: 0.19061754643917084\n",
            "step: 80, loss: 0.04065834730863571\n",
            "step: 90, loss: 0.4285522699356079\n",
            "step: 100, loss: 0.3212401270866394\n",
            "step: 110, loss: 0.05595703050494194\n",
            "step: 120, loss: 0.17775672674179077\n",
            "step: 130, loss: 0.19709612429141998\n",
            "step: 140, loss: 0.12181587517261505\n",
            "step: 150, loss: 0.1851647049188614\n",
            "step: 160, loss: 0.0774395614862442\n",
            "step: 170, loss: 0.16738298535346985\n",
            "step: 180, loss: 0.06013816222548485\n",
            "step: 190, loss: 0.06696939468383789\n",
            "step: 200, loss: 0.18566705286502838\n",
            "step: 210, loss: 0.04108394309878349\n",
            "step: 220, loss: 0.31758418679237366\n",
            "step: 230, loss: 0.18663586676120758\n",
            "step: 240, loss: 0.07860952615737915\n",
            "step: 250, loss: 0.08661098033189774\n",
            "step: 260, loss: 0.32025450468063354\n",
            "step: 270, loss: 0.05141540616750717\n",
            "step: 280, loss: 0.10472384095191956\n",
            "step: 290, loss: 0.1078319177031517\n",
            "step: 300, loss: 0.10784848779439926\n",
            "step: 310, loss: 0.23922163248062134\n",
            "step: 320, loss: 0.11708678305149078\n",
            "step: 330, loss: 0.0450480692088604\n",
            "step: 340, loss: 0.11280544847249985\n",
            "step: 350, loss: 0.04474456235766411\n",
            "step: 360, loss: 0.0515446811914444\n",
            "step: 370, loss: 0.15784992277622223\n",
            "step: 380, loss: 0.015614660456776619\n",
            "step: 390, loss: 0.3859425187110901\n",
            "step: 400, loss: 0.23812979459762573\n",
            "step: 410, loss: 0.06368624418973923\n",
            "step: 420, loss: 0.19381043314933777\n",
            "step: 430, loss: 0.15462428331375122\n",
            "step: 440, loss: 0.029763052240014076\n",
            "step: 450, loss: 0.0583718977868557\n",
            "step: 460, loss: 0.03451903164386749\n",
            "step: 470, loss: 0.07134488970041275\n",
            "step: 480, loss: 0.12916772067546844\n",
            "step: 490, loss: 0.2863258421421051\n",
            "step: 500, loss: 0.05490024760365486\n",
            "step: 510, loss: 0.15002501010894775\n",
            "step: 520, loss: 0.03965567424893379\n",
            "step: 530, loss: 0.020651385188102722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8894052044609666, f1=0.8921523634694815, best_f1=0.8921523634694815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17673049867153168\n",
            "step: 10, loss: 0.11850208789110184\n",
            "step: 20, loss: 0.022296080365777016\n",
            "step: 30, loss: 0.031363632529973984\n",
            "step: 40, loss: 0.2073325514793396\n",
            "step: 50, loss: 0.2408607304096222\n",
            "step: 60, loss: 0.036921799182891846\n",
            "step: 70, loss: 0.09085609018802643\n",
            "step: 80, loss: 0.2643811106681824\n",
            "step: 90, loss: 0.024047549813985825\n",
            "step: 100, loss: 0.1541527509689331\n",
            "step: 110, loss: 0.015019550919532776\n",
            "step: 120, loss: 0.12280485033988953\n",
            "step: 130, loss: 0.1362505406141281\n",
            "step: 140, loss: 0.05599187687039375\n",
            "step: 150, loss: 0.10231215506792068\n",
            "step: 160, loss: 0.0807800143957138\n",
            "step: 170, loss: 0.08637659251689911\n",
            "step: 180, loss: 0.020576929673552513\n",
            "step: 190, loss: 0.05276418477296829\n",
            "step: 200, loss: 0.04831460490822792\n",
            "step: 210, loss: 0.021045222878456116\n",
            "step: 220, loss: 0.06639345735311508\n",
            "step: 230, loss: 0.05346333608031273\n",
            "step: 240, loss: 0.03130370005965233\n",
            "step: 250, loss: 0.06608964502811432\n",
            "step: 260, loss: 0.016177747398614883\n",
            "step: 270, loss: 0.23710939288139343\n",
            "step: 280, loss: 0.15686464309692383\n",
            "step: 290, loss: 0.07056015729904175\n",
            "step: 300, loss: 0.056118499487638474\n",
            "step: 310, loss: 0.0306426789611578\n",
            "step: 320, loss: 0.25500476360321045\n",
            "step: 330, loss: 0.07010539621114731\n",
            "step: 340, loss: 0.01535874791443348\n",
            "step: 350, loss: 0.0015296153724193573\n",
            "step: 360, loss: 0.10407102853059769\n",
            "step: 370, loss: 0.12147407978773117\n",
            "step: 380, loss: 0.1589396446943283\n",
            "step: 390, loss: 0.06826496124267578\n",
            "step: 400, loss: 0.0768948569893837\n",
            "step: 410, loss: 0.060298334807157516\n",
            "step: 420, loss: 0.04609429091215134\n",
            "step: 430, loss: 0.038734227418899536\n",
            "step: 440, loss: 0.029458610340952873\n",
            "step: 450, loss: 0.04531741887331009\n",
            "step: 460, loss: 0.03757837787270546\n",
            "step: 470, loss: 0.05328746140003204\n",
            "step: 480, loss: 0.13538099825382233\n",
            "step: 490, loss: 0.03620292618870735\n",
            "step: 500, loss: 0.3302943706512451\n",
            "step: 510, loss: 0.025609754025936127\n",
            "step: 520, loss: 0.05416293814778328\n",
            "step: 530, loss: 0.018254157155752182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9130841121495327, f1=0.907906976744186, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0656229704618454\n",
            "step: 10, loss: 0.014680192805826664\n",
            "step: 20, loss: 0.18334539234638214\n",
            "step: 30, loss: 0.06550410389900208\n",
            "step: 40, loss: 0.026685478165745735\n",
            "step: 50, loss: 0.06927380710840225\n",
            "step: 60, loss: 0.030475322157144547\n",
            "step: 70, loss: 0.03563292324542999\n",
            "step: 80, loss: 0.0025986209511756897\n",
            "step: 90, loss: 0.02408052794635296\n",
            "step: 100, loss: 0.054339345544576645\n",
            "step: 110, loss: 0.003651284845545888\n",
            "step: 120, loss: 0.02822660654783249\n",
            "step: 130, loss: 0.0004531680897343904\n",
            "step: 140, loss: 0.17558646202087402\n",
            "step: 150, loss: 0.06698139011859894\n",
            "step: 160, loss: 0.02985456772148609\n",
            "step: 170, loss: 0.14936232566833496\n",
            "step: 180, loss: 0.06578250229358673\n",
            "step: 190, loss: 0.0661688894033432\n",
            "step: 200, loss: 0.04191194474697113\n",
            "step: 210, loss: 0.020192669704556465\n",
            "step: 220, loss: 0.16313078999519348\n",
            "step: 230, loss: 0.0508696511387825\n",
            "step: 240, loss: 0.01841559074819088\n",
            "step: 250, loss: 0.05909213051199913\n",
            "step: 260, loss: 0.024024898186326027\n",
            "step: 270, loss: 0.04019007831811905\n",
            "step: 280, loss: 0.1517118513584137\n",
            "step: 290, loss: 0.034489382058382034\n",
            "step: 300, loss: 0.044386912137269974\n",
            "step: 310, loss: 0.03925461694598198\n",
            "step: 320, loss: 0.017658766359090805\n",
            "step: 330, loss: 0.012707439251244068\n",
            "step: 340, loss: 0.00973082147538662\n",
            "step: 350, loss: 0.010470293462276459\n",
            "step: 360, loss: 0.05203299969434738\n",
            "step: 370, loss: 0.029970955103635788\n",
            "step: 380, loss: 0.05176863819360733\n",
            "step: 390, loss: 0.10136513411998749\n",
            "step: 400, loss: 0.013416064903140068\n",
            "step: 410, loss: 0.012445506639778614\n",
            "step: 420, loss: 0.09031267464160919\n",
            "step: 430, loss: 0.01779058948159218\n",
            "step: 440, loss: 0.023976091295480728\n",
            "step: 450, loss: 0.03341711685061455\n",
            "step: 460, loss: 0.029223766177892685\n",
            "step: 470, loss: 0.018844278529286385\n",
            "step: 480, loss: 0.013256553560495377\n",
            "step: 490, loss: 0.01049773022532463\n",
            "step: 500, loss: 0.04225611314177513\n",
            "step: 510, loss: 0.007631917484104633\n",
            "step: 520, loss: 0.12952029705047607\n",
            "step: 530, loss: 0.07152814418077469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9057665260196905, f1=0.8975791433891994, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03353516384959221\n",
            "step: 10, loss: 0.006592847872525454\n",
            "step: 20, loss: 0.02204366959631443\n",
            "step: 30, loss: 0.02849036455154419\n",
            "step: 40, loss: 0.020877325907349586\n",
            "step: 50, loss: 0.0012942645698785782\n",
            "step: 60, loss: 0.003432227997109294\n",
            "step: 70, loss: 0.01887158863246441\n",
            "step: 80, loss: 0.031544364988803864\n",
            "step: 90, loss: 0.1687779724597931\n",
            "step: 100, loss: 0.021077051758766174\n",
            "step: 110, loss: 0.14530543982982635\n",
            "step: 120, loss: 0.08087769150733948\n",
            "step: 130, loss: 0.001786744687706232\n",
            "step: 140, loss: 0.003496959339827299\n",
            "step: 150, loss: 0.0032628236804157495\n",
            "step: 160, loss: 0.19998852908611298\n",
            "step: 170, loss: 0.006451488938182592\n",
            "step: 180, loss: 0.01291837077587843\n",
            "step: 190, loss: 0.021578652784228325\n",
            "step: 200, loss: 0.015064024366438389\n",
            "step: 210, loss: 0.014406724832952023\n",
            "step: 220, loss: 0.010567707009613514\n",
            "step: 230, loss: 0.041600003838539124\n",
            "step: 240, loss: 0.005482962355017662\n",
            "step: 250, loss: 0.04679363965988159\n",
            "step: 260, loss: 0.004476682282984257\n",
            "step: 270, loss: 0.02820878103375435\n",
            "step: 280, loss: 0.3219447135925293\n",
            "step: 290, loss: 0.1686958372592926\n",
            "step: 300, loss: 0.004173868335783482\n",
            "step: 310, loss: 0.021678932011127472\n",
            "step: 320, loss: 0.009919504635035992\n",
            "step: 330, loss: 0.07296358793973923\n",
            "step: 340, loss: 0.018259229138493538\n",
            "step: 350, loss: 0.007616249844431877\n",
            "step: 360, loss: 0.05933232977986336\n",
            "step: 370, loss: 0.006649344693869352\n",
            "step: 380, loss: 0.009567067958414555\n",
            "step: 390, loss: 0.004400509875267744\n",
            "step: 400, loss: 0.000887950649484992\n",
            "step: 410, loss: 0.0017772488063201308\n",
            "step: 420, loss: 0.07975087314844131\n",
            "step: 430, loss: 0.10394899547100067\n",
            "step: 440, loss: 0.0029089783784002066\n",
            "step: 450, loss: 0.012085688300430775\n",
            "step: 460, loss: 0.14197999238967896\n",
            "step: 470, loss: 0.052806273102760315\n",
            "step: 480, loss: 0.00837006140500307\n",
            "step: 490, loss: 0.002976265037432313\n",
            "step: 500, loss: 0.0027047665789723396\n",
            "step: 510, loss: 0.018644364550709724\n",
            "step: 520, loss: 0.027247266843914986\n",
            "step: 530, loss: 0.0761466920375824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9043151969981239, f1=0.8886810102899906, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04649779573082924\n",
            "step: 10, loss: 0.046793367713689804\n",
            "step: 20, loss: 0.004402705933898687\n",
            "step: 30, loss: 0.005431700497865677\n",
            "step: 40, loss: 0.017087819054722786\n",
            "step: 50, loss: 0.04996394366025925\n",
            "step: 60, loss: 0.1064944639801979\n",
            "step: 70, loss: 0.028600111603736877\n",
            "step: 80, loss: 0.009443500079214573\n",
            "step: 90, loss: 0.04450186714529991\n",
            "step: 100, loss: 0.0008584714378230274\n",
            "step: 110, loss: 0.015509175136685371\n",
            "step: 120, loss: 0.01076600980013609\n",
            "step: 130, loss: 0.0003414626116864383\n",
            "step: 140, loss: 0.005138979759067297\n",
            "step: 150, loss: 0.03876461461186409\n",
            "step: 160, loss: 0.0010072062723338604\n",
            "step: 170, loss: 0.011460518464446068\n",
            "step: 180, loss: 0.0012928476789966226\n",
            "step: 190, loss: 0.002069752663373947\n",
            "step: 200, loss: 0.002599061466753483\n",
            "step: 210, loss: 0.05518863722681999\n",
            "step: 220, loss: 0.005342953838407993\n",
            "step: 230, loss: 0.01210671104490757\n",
            "step: 240, loss: 0.03010203130543232\n",
            "step: 250, loss: 0.08481896668672562\n",
            "step: 260, loss: 0.020281247794628143\n",
            "step: 270, loss: 0.0029210352804511786\n",
            "step: 280, loss: 0.030998336151242256\n",
            "step: 290, loss: 0.20965369045734406\n",
            "step: 300, loss: 0.006623988971114159\n",
            "step: 310, loss: 0.014979755505919456\n",
            "step: 320, loss: 0.1477387398481369\n",
            "step: 330, loss: 0.007902687415480614\n",
            "step: 340, loss: 0.004220015835016966\n",
            "step: 350, loss: 0.000954215123783797\n",
            "step: 360, loss: 0.002320877742022276\n",
            "step: 370, loss: 0.02153163030743599\n",
            "step: 380, loss: 0.0604398287832737\n",
            "step: 390, loss: 0.00028877536533400416\n",
            "step: 400, loss: 0.025101302191615105\n",
            "step: 410, loss: 0.0055748093873262405\n",
            "step: 420, loss: 0.004120148718357086\n",
            "step: 430, loss: 0.0031333058141171932\n",
            "step: 440, loss: 0.14039458334445953\n",
            "step: 450, loss: 0.010162405669689178\n",
            "step: 460, loss: 0.0015783135313540697\n",
            "step: 470, loss: 0.007803116925060749\n",
            "step: 480, loss: 0.0023016072809696198\n",
            "step: 490, loss: 0.006762573961168528\n",
            "step: 500, loss: 0.13018062710762024\n",
            "step: 510, loss: 0.02139541693031788\n",
            "step: 520, loss: 0.05483652651309967\n",
            "step: 530, loss: 0.009183441288769245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9093484419263457, f1=0.9010367577756833, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007097164634615183\n",
            "step: 10, loss: 0.00043430112418718636\n",
            "step: 20, loss: 0.06318381428718567\n",
            "step: 30, loss: 0.0009201960056088865\n",
            "step: 40, loss: 0.08963903039693832\n",
            "step: 50, loss: 0.010568267665803432\n",
            "step: 60, loss: 0.00043033939437009394\n",
            "step: 70, loss: 0.00045018261880613863\n",
            "step: 80, loss: 0.0015712975291535258\n",
            "step: 90, loss: 0.0003514218842610717\n",
            "step: 100, loss: 0.008873219601809978\n",
            "step: 110, loss: 0.00018667406402528286\n",
            "step: 120, loss: 8.699952741153538e-05\n",
            "step: 130, loss: 0.0005848250002600253\n",
            "step: 140, loss: 0.006524394731968641\n",
            "step: 150, loss: 0.0013045478845015168\n",
            "step: 160, loss: 0.0024931614752858877\n",
            "step: 170, loss: 0.0030491091310977936\n",
            "step: 180, loss: 0.0007763973553664982\n",
            "step: 190, loss: 0.00368082569912076\n",
            "step: 200, loss: 0.0019407665822654963\n",
            "step: 210, loss: 0.014341533184051514\n",
            "step: 220, loss: 0.009833596646785736\n",
            "step: 230, loss: 0.0035073780454695225\n",
            "step: 240, loss: 0.03674260526895523\n",
            "step: 250, loss: 0.0022654852364212275\n",
            "step: 260, loss: 0.0002771934377960861\n",
            "step: 270, loss: 0.01834707520902157\n",
            "step: 280, loss: 0.08340664952993393\n",
            "step: 290, loss: 0.0039887940511107445\n",
            "step: 300, loss: 0.005706240888684988\n",
            "step: 310, loss: 0.0010395270073786378\n",
            "step: 320, loss: 0.0010213463101536036\n",
            "step: 330, loss: 0.0007274713134393096\n",
            "step: 340, loss: 0.059652335941791534\n",
            "step: 350, loss: 0.0077919624745845795\n",
            "step: 360, loss: 0.006135732866823673\n",
            "step: 370, loss: 0.02480163984000683\n",
            "step: 380, loss: 0.00012867318582721055\n",
            "step: 390, loss: 0.009060335345566273\n",
            "step: 400, loss: 0.05479762330651283\n",
            "step: 410, loss: 0.0005236480501480401\n",
            "step: 420, loss: 0.007391876541078091\n",
            "step: 430, loss: 0.00011212180834263563\n",
            "step: 440, loss: 0.006963890977203846\n",
            "step: 450, loss: 0.0014583806041628122\n",
            "step: 460, loss: 0.0015608163084834814\n",
            "step: 470, loss: 0.0446413978934288\n",
            "step: 480, loss: 0.006107311230152845\n",
            "step: 490, loss: 0.010246988385915756\n",
            "step: 500, loss: 0.02640393190085888\n",
            "step: 510, loss: 0.008018814958631992\n",
            "step: 520, loss: 0.005203367210924625\n",
            "step: 530, loss: 0.018189148977398872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8698254364089776, f1=0.8650246305418717, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007003589998930693\n",
            "step: 10, loss: 0.06703511625528336\n",
            "step: 20, loss: 0.009680020622909069\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.010342423804104328\n",
            "step: 40, loss: 0.06456290930509567\n",
            "step: 50, loss: 0.03799606114625931\n",
            "step: 60, loss: 0.0009591329144313931\n",
            "step: 70, loss: 0.04144798591732979\n",
            "step: 80, loss: 0.0022351485677063465\n",
            "step: 90, loss: 0.0003032982349395752\n",
            "step: 100, loss: 0.0008109543123282492\n",
            "step: 110, loss: 0.022038482129573822\n",
            "step: 120, loss: 0.000134759015054442\n",
            "step: 130, loss: 0.00048080930719152093\n",
            "step: 140, loss: 0.00012479812721721828\n",
            "step: 150, loss: 0.0028075582813471556\n",
            "step: 160, loss: 0.014474156312644482\n",
            "step: 170, loss: 0.008093430660665035\n",
            "step: 180, loss: 0.0008092163479886949\n",
            "step: 190, loss: 0.0006828279001638293\n",
            "step: 200, loss: 0.0023677183780819178\n",
            "step: 210, loss: 0.04986673220992088\n",
            "step: 220, loss: 0.00045798346400260925\n",
            "step: 230, loss: 0.000415063404943794\n",
            "step: 240, loss: 0.17604242265224457\n",
            "step: 250, loss: 0.001288966042920947\n",
            "step: 260, loss: 0.00043163340887986124\n",
            "step: 270, loss: 0.0012410138733685017\n",
            "step: 280, loss: 0.004031619522720575\n",
            "step: 290, loss: 0.08485060185194016\n",
            "step: 300, loss: 0.0012959556188434362\n",
            "step: 310, loss: 0.000780905713327229\n",
            "step: 320, loss: 0.00022586261911783367\n",
            "step: 330, loss: 0.02612793631851673\n",
            "step: 340, loss: 0.007332670968025923\n",
            "step: 350, loss: 0.001595870591700077\n",
            "step: 360, loss: 0.00013253815995994955\n",
            "step: 370, loss: 0.00025721630663610995\n",
            "step: 380, loss: 0.00025154545437544584\n",
            "step: 390, loss: 0.00015771928883623332\n",
            "step: 400, loss: 0.00012645173410419375\n",
            "step: 410, loss: 0.00577740790322423\n",
            "step: 420, loss: 0.02110503613948822\n",
            "step: 430, loss: 5.73932848055847e-05\n",
            "step: 440, loss: 0.0008775625028647482\n",
            "step: 450, loss: 0.0004149508604314178\n",
            "step: 460, loss: 7.405014184769243e-05\n",
            "step: 470, loss: 0.0006372568313963711\n",
            "step: 480, loss: 0.05368507653474808\n",
            "step: 490, loss: 0.0018681437941268086\n",
            "step: 500, loss: 0.03547088801860809\n",
            "step: 510, loss: 0.0010748793138191104\n",
            "step: 520, loss: 0.0015564317582175136\n",
            "step: 530, loss: 0.001956926891580224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9038112522686025, f1=0.902461257976299, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03744813799858093\n",
            "step: 10, loss: 0.005273173563182354\n",
            "step: 20, loss: 0.002738777780905366\n",
            "step: 30, loss: 0.08842939138412476\n",
            "step: 40, loss: 0.00033845490543171763\n",
            "step: 50, loss: 0.06851015239953995\n",
            "step: 60, loss: 0.0010993318865075707\n",
            "step: 70, loss: 0.0012232671724632382\n",
            "step: 80, loss: 0.0014289767714217305\n",
            "step: 90, loss: 0.030217742547392845\n",
            "step: 100, loss: 0.0010320758447051048\n",
            "step: 110, loss: 0.0005751218996010721\n",
            "step: 120, loss: 0.0030361551325768232\n",
            "step: 130, loss: 0.0001974858168978244\n",
            "step: 140, loss: 0.000318357371725142\n",
            "step: 150, loss: 0.0032966313883662224\n",
            "step: 160, loss: 0.0005788921262137592\n",
            "step: 170, loss: 0.0019726769533008337\n",
            "step: 180, loss: 0.0009451636578887701\n",
            "step: 190, loss: 0.011135276407003403\n",
            "step: 200, loss: 0.003057154593989253\n",
            "step: 210, loss: 0.001963770017027855\n",
            "step: 220, loss: 0.00040565821109339595\n",
            "step: 230, loss: 0.0015448424965143204\n",
            "step: 240, loss: 0.005641024559736252\n",
            "step: 250, loss: 7.303973688976839e-05\n",
            "step: 260, loss: 4.07899497076869e-05\n",
            "step: 270, loss: 0.007535536307841539\n",
            "step: 280, loss: 0.026853235438466072\n",
            "step: 290, loss: 0.0002465834841132164\n",
            "step: 300, loss: 0.10509040951728821\n",
            "step: 310, loss: 0.0004684123850893229\n",
            "step: 320, loss: 0.08630082756280899\n",
            "step: 330, loss: 0.01103284116834402\n",
            "step: 340, loss: 0.0003467428614385426\n",
            "step: 350, loss: 0.03482377156615257\n",
            "step: 360, loss: 0.00361900357529521\n",
            "step: 370, loss: 0.04599525406956673\n",
            "step: 380, loss: 0.003625014331191778\n",
            "step: 390, loss: 0.07388181239366531\n",
            "step: 400, loss: 0.0004451691056601703\n",
            "step: 410, loss: 0.00016514411254320294\n",
            "step: 420, loss: 0.00036810923484154046\n",
            "step: 430, loss: 0.04089663177728653\n",
            "step: 440, loss: 0.015799451619386673\n",
            "step: 450, loss: 0.00021424159058369696\n",
            "step: 460, loss: 0.0008006665157154202\n",
            "step: 470, loss: 0.004865577444434166\n",
            "step: 480, loss: 0.0002241030742879957\n",
            "step: 490, loss: 0.005466548725962639\n",
            "step: 500, loss: 0.0013514168094843626\n",
            "step: 510, loss: 0.001756887068040669\n",
            "step: 520, loss: 0.0024218924809247255\n",
            "step: 530, loss: 0.00014416061458177865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9067041725269573, f1=0.9010270774976656, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004598097875714302\n",
            "step: 10, loss: 0.0006197154289111495\n",
            "step: 20, loss: 0.0008747333195060492\n",
            "step: 30, loss: 0.006807386875152588\n",
            "step: 40, loss: 5.5690052249701694e-05\n",
            "step: 50, loss: 0.002492243889719248\n",
            "step: 60, loss: 5.561108628171496e-05\n",
            "step: 70, loss: 0.0004697063995990902\n",
            "step: 80, loss: 0.019245782867074013\n",
            "step: 90, loss: 0.0020466449204832315\n",
            "step: 100, loss: 0.029938848689198494\n",
            "step: 110, loss: 0.0083073815330863\n",
            "step: 120, loss: 0.00042955257231369615\n",
            "step: 130, loss: 0.0002942630962934345\n",
            "step: 140, loss: 0.0002682960475794971\n",
            "step: 150, loss: 0.000542782130651176\n",
            "step: 160, loss: 0.0012348211603239179\n",
            "step: 170, loss: 0.07632629573345184\n",
            "step: 180, loss: 0.00041341324686072767\n",
            "step: 190, loss: 5.679806417902e-05\n",
            "step: 200, loss: 0.0002920165716204792\n",
            "step: 210, loss: 7.076412293827161e-05\n",
            "step: 220, loss: 0.07116734981536865\n",
            "step: 230, loss: 0.0007179688545875251\n",
            "step: 240, loss: 0.00044546640128828585\n",
            "step: 250, loss: 0.0009384016739204526\n",
            "step: 260, loss: 0.0008973692310974002\n",
            "step: 270, loss: 0.0009820314589887857\n",
            "step: 280, loss: 0.0009578831377439201\n",
            "step: 290, loss: 0.026141300797462463\n",
            "step: 300, loss: 0.0001563061523484066\n",
            "step: 310, loss: 0.009391573257744312\n",
            "step: 320, loss: 0.0002408196305623278\n",
            "step: 330, loss: 0.12340768426656723\n",
            "step: 340, loss: 0.0013694643275812268\n",
            "step: 350, loss: 0.00017412954184692353\n",
            "step: 360, loss: 0.0015333020128309727\n",
            "step: 370, loss: 0.00026409272686578333\n",
            "step: 380, loss: 0.0001174499120679684\n",
            "step: 390, loss: 0.000167490667081438\n",
            "step: 400, loss: 0.0003088793600909412\n",
            "step: 410, loss: 3.646173718152568e-05\n",
            "step: 420, loss: 6.853398372186348e-05\n",
            "step: 430, loss: 8.4783008787781e-05\n",
            "step: 440, loss: 0.00023741314362268895\n",
            "step: 450, loss: 0.00011421633826103061\n",
            "step: 460, loss: 9.337843948742375e-05\n",
            "step: 470, loss: 3.253195245633833e-05\n",
            "step: 480, loss: 4.5609151129610837e-05\n",
            "step: 490, loss: 0.013635894283652306\n",
            "step: 500, loss: 7.539673242717981e-05\n",
            "step: 510, loss: 9.649269486544654e-05\n",
            "step: 520, loss: 4.0987513784784824e-05\n",
            "step: 530, loss: 0.0006004719762131572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.906468124709167, f1=0.9023041474654377, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.715479634702206e-05\n",
            "step: 10, loss: 0.0007243562722578645\n",
            "step: 20, loss: 0.07000479102134705\n",
            "step: 30, loss: 3.758272578124888e-05\n",
            "step: 40, loss: 0.0005423637921921909\n",
            "step: 50, loss: 4.4715347030432895e-05\n",
            "step: 60, loss: 5.0842932978412136e-05\n",
            "step: 70, loss: 0.00029455136973410845\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 2.9492286557797343e-05\n",
            "step: 90, loss: 0.00017994949303101748\n",
            "step: 100, loss: 0.00097120605641976\n",
            "step: 110, loss: 4.21455260948278e-05\n",
            "step: 120, loss: 0.00032321581966243684\n",
            "step: 130, loss: 0.00909137912094593\n",
            "step: 140, loss: 0.01764713227748871\n",
            "step: 150, loss: 0.01067001186311245\n",
            "step: 160, loss: 5.465357389766723e-05\n",
            "step: 170, loss: 0.00013220588152762502\n",
            "step: 180, loss: 0.00029195594834163785\n",
            "step: 190, loss: 4.315561818657443e-05\n",
            "step: 200, loss: 0.00012278323993086815\n",
            "step: 210, loss: 0.00013621142716147006\n",
            "step: 220, loss: 0.00019125152903143317\n",
            "step: 230, loss: 0.00011444420670159161\n",
            "step: 240, loss: 0.00019515692838467658\n",
            "step: 250, loss: 0.0002915819932240993\n",
            "step: 260, loss: 0.003880760632455349\n",
            "step: 270, loss: 8.148855704348534e-05\n",
            "step: 280, loss: 6.190451676957309e-05\n",
            "step: 290, loss: 0.00303435605019331\n",
            "step: 300, loss: 3.5034474421991035e-05\n",
            "step: 310, loss: 0.0006713205366395414\n",
            "step: 320, loss: 0.008919217623770237\n",
            "step: 330, loss: 0.006242799106985331\n",
            "step: 340, loss: 0.009267936460673809\n",
            "step: 350, loss: 3.8011356082279235e-05\n",
            "step: 360, loss: 5.0566835852805525e-05\n",
            "step: 370, loss: 0.00014800456119701266\n",
            "step: 380, loss: 8.847173739923164e-05\n",
            "step: 390, loss: 0.009819644503295422\n",
            "step: 400, loss: 3.24834072671365e-05\n",
            "step: 410, loss: 5.570549183175899e-05\n",
            "step: 420, loss: 8.324395457748324e-05\n",
            "step: 430, loss: 2.5957011530408636e-05\n",
            "step: 440, loss: 0.0004991673631593585\n",
            "step: 450, loss: 2.58118689089315e-05\n",
            "step: 460, loss: 9.555602446198463e-05\n",
            "step: 470, loss: 8.575192623538896e-05\n",
            "step: 480, loss: 0.00014165914035402238\n",
            "step: 490, loss: 0.0002081673446809873\n",
            "step: 500, loss: 0.018253013491630554\n",
            "step: 510, loss: 4.162975528743118e-05\n",
            "step: 520, loss: 0.00014875653141643852\n",
            "step: 530, loss: 0.00023797660833224654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9064885496183206, f1=0.8964200477326969, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.081291965325363e-05\n",
            "step: 10, loss: 0.0025651496835052967\n",
            "step: 20, loss: 6.741225661244243e-05\n",
            "step: 30, loss: 0.00025323216686956584\n",
            "step: 40, loss: 7.429214019794017e-05\n",
            "step: 50, loss: 0.0015767216682434082\n",
            "step: 60, loss: 0.01819775253534317\n",
            "step: 70, loss: 0.02727392502129078\n",
            "step: 80, loss: 4.019670086563565e-05\n",
            "step: 90, loss: 0.00013868566020391881\n",
            "step: 100, loss: 0.004339035600423813\n",
            "step: 110, loss: 0.0003929919912479818\n",
            "step: 120, loss: 2.825921001203824e-05\n",
            "step: 130, loss: 6.398349069058895e-05\n",
            "step: 140, loss: 0.0003628600970841944\n",
            "step: 150, loss: 4.1838138713501394e-05\n",
            "step: 160, loss: 3.146612652926706e-05\n",
            "step: 170, loss: 4.0790171624394134e-05\n",
            "step: 180, loss: 0.00027479504933580756\n",
            "step: 190, loss: 0.0032147751189768314\n",
            "step: 200, loss: 0.0004104043764527887\n",
            "step: 210, loss: 0.005016573704779148\n",
            "step: 220, loss: 9.030909859575331e-05\n",
            "step: 230, loss: 2.411306013527792e-05\n",
            "step: 240, loss: 0.013562125153839588\n",
            "step: 250, loss: 8.066402369877324e-05\n",
            "step: 260, loss: 5.921621777815744e-05\n",
            "step: 270, loss: 0.026852751150727272\n",
            "step: 280, loss: 0.00048080080887302756\n",
            "step: 290, loss: 0.0004969134461134672\n",
            "step: 300, loss: 0.00016959263302851468\n",
            "step: 310, loss: 4.2734282033052295e-05\n",
            "step: 320, loss: 0.0001144821144407615\n",
            "step: 330, loss: 0.0008062546257860959\n",
            "step: 340, loss: 0.0003733958292286843\n",
            "step: 350, loss: 7.142642425606027e-05\n",
            "step: 360, loss: 3.3792945032473654e-05\n",
            "step: 370, loss: 5.190061710891314e-05\n",
            "step: 380, loss: 2.9820130293956026e-05\n",
            "step: 390, loss: 2.7536523703020066e-05\n",
            "step: 400, loss: 3.319778261356987e-05\n",
            "step: 410, loss: 8.280971087515354e-05\n",
            "step: 420, loss: 0.00013244197180029005\n",
            "step: 430, loss: 3.171979551552795e-05\n",
            "step: 440, loss: 4.84401207359042e-05\n",
            "step: 450, loss: 0.0008212481043301523\n",
            "step: 460, loss: 0.014873804524540901\n",
            "step: 470, loss: 0.00013361609308049083\n",
            "step: 480, loss: 0.0003047179488930851\n",
            "step: 490, loss: 5.201433668844402e-05\n",
            "step: 500, loss: 0.00011745494703063741\n",
            "step: 510, loss: 3.69892550224904e-05\n",
            "step: 520, loss: 0.039620768278837204\n",
            "step: 530, loss: 0.001577003626152873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9038010323791646, f1=0.9014479215319944, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.0467814212897792e-05\n",
            "step: 10, loss: 2.6452609745319933e-05\n",
            "step: 20, loss: 0.007990064099431038\n",
            "step: 30, loss: 6.109716196078807e-05\n",
            "step: 40, loss: 3.1458686862606555e-05\n",
            "step: 50, loss: 0.006931259296834469\n",
            "step: 60, loss: 0.00012784203863702714\n",
            "step: 70, loss: 0.0001162027328973636\n",
            "step: 80, loss: 0.00029480073135346174\n",
            "step: 90, loss: 0.0003251723537687212\n",
            "step: 100, loss: 0.11836361885070801\n",
            "step: 110, loss: 0.00021221187489572912\n",
            "step: 120, loss: 2.8742697395500727e-05\n",
            "step: 130, loss: 7.879212353145704e-05\n",
            "step: 140, loss: 5.0269434723304585e-05\n",
            "step: 150, loss: 0.00012331240577623248\n",
            "step: 160, loss: 4.843120768782683e-05\n",
            "step: 170, loss: 0.0029972626361995935\n",
            "step: 180, loss: 2.5826828277786262e-05\n",
            "step: 190, loss: 3.6323192034615204e-05\n",
            "step: 200, loss: 8.44937821966596e-05\n",
            "step: 210, loss: 4.173124034423381e-05\n",
            "step: 220, loss: 0.0001884972007246688\n",
            "step: 230, loss: 3.0177348889992572e-05\n",
            "step: 240, loss: 0.00022562970116268843\n",
            "step: 250, loss: 2.4552737158956006e-05\n",
            "step: 260, loss: 5.400657028076239e-05\n",
            "step: 270, loss: 5.684009374817833e-05\n",
            "step: 280, loss: 5.13661143486388e-05\n",
            "step: 290, loss: 0.002952966606244445\n",
            "step: 300, loss: 0.0002066579763777554\n",
            "step: 310, loss: 0.00014484288112726063\n",
            "step: 320, loss: 8.834216714603826e-05\n",
            "step: 330, loss: 0.0002153700334019959\n",
            "step: 340, loss: 0.0001708849158603698\n",
            "step: 350, loss: 0.0030172253027558327\n",
            "step: 360, loss: 8.542503928765655e-05\n",
            "step: 370, loss: 4.867528332397342e-05\n",
            "step: 380, loss: 5.763319495599717e-05\n",
            "step: 390, loss: 4.0987513784784824e-05\n",
            "step: 400, loss: 0.020445657894015312\n",
            "step: 410, loss: 8.498416718794033e-05\n",
            "step: 420, loss: 0.0003158478648401797\n",
            "step: 430, loss: 0.00798824056982994\n",
            "step: 440, loss: 0.0001608507300261408\n",
            "step: 450, loss: 5.975125532131642e-05\n",
            "step: 460, loss: 3.3652653655735776e-05\n",
            "step: 470, loss: 6.693789327982813e-05\n",
            "step: 480, loss: 0.00017594634846318513\n",
            "step: 490, loss: 7.086930418154225e-05\n",
            "step: 500, loss: 0.00032459208159707487\n",
            "step: 510, loss: 8.133615483529866e-05\n",
            "step: 520, loss: 5.798911661258899e-05\n",
            "step: 530, loss: 3.438662315602414e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9115128449096099, f1=0.8944866920152091, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.4567215556744486e-05\n",
            "step: 10, loss: 0.0003799335681833327\n",
            "step: 20, loss: 2.839342960214708e-05\n",
            "step: 30, loss: 6.285351264523342e-05\n",
            "step: 40, loss: 4.8759273340692744e-05\n",
            "step: 50, loss: 0.0005485267611220479\n",
            "step: 60, loss: 0.0005747061804868281\n",
            "step: 70, loss: 5.645591954817064e-05\n",
            "step: 80, loss: 4.921135769109242e-05\n",
            "step: 90, loss: 0.0001725159672787413\n",
            "step: 100, loss: 0.00022950171842239797\n",
            "step: 110, loss: 1.8082249880535528e-05\n",
            "step: 120, loss: 3.9531627407995984e-05\n",
            "step: 130, loss: 3.078098961850628e-05\n",
            "step: 140, loss: 3.454981560935266e-05\n",
            "step: 150, loss: 4.4368633098201826e-05\n",
            "step: 160, loss: 4.039308623760007e-05\n",
            "step: 170, loss: 4.911808355245739e-05\n",
            "step: 180, loss: 7.671162893529981e-05\n",
            "step: 190, loss: 0.00021668720000889152\n",
            "step: 200, loss: 0.0001701967412373051\n",
            "step: 210, loss: 0.0001848704123403877\n",
            "step: 220, loss: 3.728749288711697e-05\n",
            "step: 230, loss: 4.10340289818123e-05\n",
            "step: 240, loss: 0.0007354443077929318\n",
            "step: 250, loss: 1.8849685147870332e-05\n",
            "step: 260, loss: 2.4317942006746307e-05\n",
            "step: 270, loss: 2.4508015485480428e-05\n",
            "step: 280, loss: 2.6708594305091538e-05\n",
            "step: 290, loss: 4.4980701204622164e-05\n",
            "step: 300, loss: 5.4843927500769496e-05\n",
            "step: 310, loss: 4.811702820006758e-05\n",
            "step: 320, loss: 0.0008109326590783894\n",
            "step: 330, loss: 2.4727856725803576e-05\n",
            "step: 340, loss: 6.662256055278704e-05\n",
            "step: 350, loss: 0.030849333852529526\n",
            "step: 360, loss: 4.4124357373220846e-05\n",
            "step: 370, loss: 5.712005804525688e-05\n",
            "step: 380, loss: 3.5886430850951e-05\n",
            "step: 390, loss: 5.118290937389247e-05\n",
            "step: 400, loss: 1.903964948724024e-05\n",
            "step: 410, loss: 3.2110267056850716e-05\n",
            "step: 420, loss: 2.2656724468106404e-05\n",
            "step: 430, loss: 7.171651668613777e-05\n",
            "step: 440, loss: 0.0001236946991411969\n",
            "step: 450, loss: 0.002919713268056512\n",
            "step: 460, loss: 0.00012283850810490549\n",
            "step: 470, loss: 8.668781811138615e-05\n",
            "step: 480, loss: 1.9326527763041668e-05\n",
            "step: 490, loss: 4.293804886401631e-05\n",
            "step: 500, loss: 3.316570655442774e-05\n",
            "step: 510, loss: 0.00010173898044740781\n",
            "step: 520, loss: 0.0023510130122303963\n",
            "step: 530, loss: 1.9669223547680303e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9096958174904942, f1=0.8971076339497392, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.6953922467073426e-05\n",
            "step: 10, loss: 1.9147502825944684e-05\n",
            "step: 20, loss: 2.304415284015704e-05\n",
            "step: 30, loss: 4.051625364809297e-05\n",
            "step: 40, loss: 4.877046012552455e-05\n",
            "step: 50, loss: 0.0001917577610583976\n",
            "step: 60, loss: 0.00012900863657705486\n",
            "step: 70, loss: 1.9158578652422875e-05\n",
            "step: 80, loss: 3.7189314753049985e-05\n",
            "step: 90, loss: 2.671679249033332e-05\n",
            "step: 100, loss: 3.083585397689603e-05\n",
            "step: 110, loss: 0.08584386110305786\n",
            "step: 120, loss: 2.0868708816124126e-05\n",
            "step: 130, loss: 9.911884262692183e-05\n",
            "step: 140, loss: 2.271963057864923e-05\n",
            "step: 150, loss: 0.0005925916484557092\n",
            "step: 160, loss: 2.7585077987168916e-05\n",
            "step: 170, loss: 3.974654828198254e-05\n",
            "step: 180, loss: 2.754727211140562e-05\n",
            "step: 190, loss: 6.660680810455233e-05\n",
            "step: 200, loss: 3.859897697111592e-05\n",
            "step: 210, loss: 9.164456423604861e-05\n",
            "step: 220, loss: 1.9449427782092243e-05\n",
            "step: 230, loss: 4.908811752102338e-05\n",
            "step: 240, loss: 2.6895515475189313e-05\n",
            "step: 250, loss: 3.942466719308868e-05\n",
            "step: 260, loss: 2.3677150238654576e-05\n",
            "step: 270, loss: 2.851957469829358e-05\n",
            "step: 280, loss: 0.000369972171029076\n",
            "step: 290, loss: 0.003451727097854018\n",
            "step: 300, loss: 2.111822323058732e-05\n",
            "step: 310, loss: 5.832535680383444e-05\n",
            "step: 320, loss: 7.117716450011358e-05\n",
            "step: 330, loss: 0.002169401617720723\n",
            "step: 340, loss: 0.00012538822193164378\n",
            "step: 350, loss: 7.612331683048978e-05\n",
            "step: 360, loss: 5.020895696361549e-05\n",
            "step: 370, loss: 5.242408951744437e-05\n",
            "step: 380, loss: 7.180296961450949e-05\n",
            "step: 390, loss: 0.006864104885607958\n",
            "step: 400, loss: 8.865356358001009e-05\n",
            "step: 410, loss: 1.678594526310917e-05\n",
            "step: 420, loss: 0.021187396720051765\n",
            "step: 430, loss: 8.476684160996228e-05\n",
            "step: 440, loss: 0.00022617136710323393\n",
            "step: 450, loss: 0.0004816438886336982\n",
            "step: 460, loss: 0.00012659186904784292\n",
            "step: 470, loss: 0.0003476636193227023\n",
            "step: 480, loss: 2.5658533559180796e-05\n",
            "step: 490, loss: 0.03165891766548157\n",
            "step: 500, loss: 1.6409710951847956e-05\n",
            "step: 510, loss: 0.0006326469010673463\n",
            "step: 520, loss: 4.682780854636803e-05\n",
            "step: 530, loss: 2.093206421704963e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9097744360902255, f1=0.8990136214185063, best_f1=0.907906976744186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2123955204733647e-05\n",
            "step: 10, loss: 0.0002543407026678324\n",
            "step: 20, loss: 6.462400779128075e-05\n",
            "step: 30, loss: 2.2883945348439738e-05\n",
            "step: 40, loss: 0.009490042924880981\n",
            "step: 50, loss: 6.173301517264917e-05\n",
            "step: 60, loss: 1.7747064703144133e-05\n",
            "step: 70, loss: 3.151281998725608e-05\n",
            "step: 80, loss: 0.0005914279608987272\n",
            "step: 90, loss: 0.002680284669622779\n",
            "step: 100, loss: 6.23565647401847e-05\n",
            "step: 110, loss: 0.0003040523442905396\n",
            "step: 120, loss: 0.00155977054964751\n",
            "step: 130, loss: 0.0031226053833961487\n",
            "step: 140, loss: 1.98219386220444e-05\n",
            "step: 150, loss: 3.845838364213705e-05\n",
            "step: 160, loss: 0.0007427555974572897\n",
            "step: 170, loss: 6.818433030275628e-05\n",
            "step: 180, loss: 1.5750349120935425e-05\n",
            "step: 190, loss: 8.178925781976432e-05\n",
            "step: 200, loss: 4.178916788077913e-05\n",
            "step: 210, loss: 6.0428286815294996e-05\n",
            "step: 220, loss: 8.169822103809565e-05\n",
            "step: 230, loss: 5.485356814460829e-05\n",
            "step: 240, loss: 0.0004334877012297511\n",
            "step: 250, loss: 2.766982106550131e-05\n",
            "step: 260, loss: 5.469026291393675e-05\n",
            "step: 270, loss: 4.5084652811056e-05\n",
            "step: 280, loss: 2.5591840312699787e-05\n",
            "step: 290, loss: 0.0003985149960499257\n",
            "step: 300, loss: 2.1103191102156416e-05\n",
            "step: 310, loss: 0.0001188963942695409\n",
            "step: 320, loss: 0.0002859432715922594\n",
            "step: 330, loss: 2.0887258870061487e-05\n",
            "step: 340, loss: 1.8875773093895987e-05\n",
            "step: 350, loss: 1.3034687071922235e-05\n",
            "step: 360, loss: 0.0005008774460293353\n",
            "step: 370, loss: 1.418576266587479e-05\n",
            "step: 380, loss: 7.598836964461952e-05\n",
            "step: 390, loss: 1.8641077986103483e-05\n",
            "step: 400, loss: 4.5020027755526826e-05\n",
            "step: 410, loss: 8.097948739305139e-05\n",
            "step: 420, loss: 1.9192379113519564e-05\n",
            "step: 430, loss: 0.00011925349826924503\n",
            "step: 440, loss: 2.490665610821452e-05\n",
            "step: 450, loss: 1.680076093180105e-05\n",
            "step: 460, loss: 2.753262197074946e-05\n",
            "step: 470, loss: 8.78496648510918e-05\n",
            "step: 480, loss: 1.3030932677793317e-05\n",
            "step: 490, loss: 1.9240806068410166e-05\n",
            "step: 500, loss: 7.150138117140159e-05\n",
            "step: 510, loss: 7.829145761206746e-05\n",
            "step: 520, loss: 2.3684520783717744e-05\n",
            "step: 530, loss: 1.9337672711117193e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9110486891385768, f1=0.90014064697609, best_f1=0.907906976744186\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:31, 181.39it/s]\n",
            "load_f1 = 0.9111826967326276\n",
            "real_f1 = 0.9099264705882353\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 180.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c4a020-12fa-4b43-c695-82c76890976e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5483188629150391\n",
            "step: 10, loss: 0.39112716913223267\n",
            "step: 20, loss: 0.3592701256275177\n",
            "step: 30, loss: 0.310160756111145\n",
            "step: 40, loss: 0.19110792875289917\n",
            "step: 50, loss: 0.4393254220485687\n",
            "step: 60, loss: 0.28172147274017334\n",
            "step: 70, loss: 0.2521669566631317\n",
            "step: 80, loss: 0.18054641783237457\n",
            "step: 90, loss: 0.44780194759368896\n",
            "step: 100, loss: 0.49234819412231445\n",
            "step: 110, loss: 0.275777131319046\n",
            "step: 120, loss: 0.28153789043426514\n",
            "step: 130, loss: 0.3191806674003601\n",
            "step: 140, loss: 0.23763032257556915\n",
            "step: 150, loss: 0.246140718460083\n",
            "step: 160, loss: 0.2969309091567993\n",
            "step: 170, loss: 0.28456273674964905\n",
            "step: 180, loss: 0.1342092901468277\n",
            "step: 190, loss: 0.23065736889839172\n",
            "step: 200, loss: 0.3565085232257843\n",
            "step: 210, loss: 0.28061747550964355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.42689075630252094, f1=0.4825291181364393, best_f1=0.4825291181364393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17592360079288483\n",
            "step: 10, loss: 0.32136163115501404\n",
            "step: 20, loss: 0.26048094034194946\n",
            "step: 30, loss: 0.42373278737068176\n",
            "step: 40, loss: 0.21594516932964325\n",
            "step: 50, loss: 0.14349104464054108\n",
            "step: 60, loss: 0.45761168003082275\n",
            "step: 70, loss: 0.10866077989339828\n",
            "step: 80, loss: 0.2643962502479553\n",
            "step: 90, loss: 0.17249959707260132\n",
            "step: 100, loss: 0.03747202455997467\n",
            "step: 110, loss: 0.19175437092781067\n",
            "step: 120, loss: 0.1478605568408966\n",
            "step: 130, loss: 0.0662771463394165\n",
            "step: 140, loss: 0.2600002586841583\n",
            "step: 150, loss: 0.300148606300354\n",
            "step: 160, loss: 0.18532179296016693\n",
            "step: 170, loss: 0.19148415327072144\n",
            "step: 180, loss: 0.23884481191635132\n",
            "step: 190, loss: 0.28407397866249084\n",
            "step: 200, loss: 0.060276489704847336\n",
            "step: 210, loss: 0.1669456660747528\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4927536231884058, f1=0.5326460481099656, best_f1=0.5326460481099656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.061123743653297424\n",
            "step: 10, loss: 0.17951901257038116\n",
            "step: 20, loss: 0.25437796115875244\n",
            "step: 30, loss: 0.21894362568855286\n",
            "step: 40, loss: 0.05147017911076546\n",
            "step: 50, loss: 0.12928655743598938\n",
            "step: 60, loss: 0.20532634854316711\n",
            "step: 70, loss: 0.2488166242837906\n",
            "step: 80, loss: 0.1493535190820694\n",
            "step: 90, loss: 0.0677238181233406\n",
            "step: 100, loss: 0.37492576241493225\n",
            "step: 110, loss: 0.23653928935527802\n",
            "step: 120, loss: 0.14444813132286072\n",
            "step: 130, loss: 0.22639767825603485\n",
            "step: 140, loss: 0.060752883553504944\n",
            "step: 150, loss: 0.3008284270763397\n",
            "step: 160, loss: 0.06103229522705078\n",
            "step: 170, loss: 0.12858234345912933\n",
            "step: 180, loss: 0.06492402404546738\n",
            "step: 190, loss: 0.3098655045032501\n",
            "step: 200, loss: 0.16778743267059326\n",
            "step: 210, loss: 0.1633566915988922\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5034013605442177, f1=0.5495652173913044, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2457180917263031\n",
            "step: 10, loss: 0.16976352035999298\n",
            "step: 20, loss: 0.06281518936157227\n",
            "step: 30, loss: 0.034646663814783096\n",
            "step: 40, loss: 0.016136042773723602\n",
            "step: 50, loss: 0.19907675683498383\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.11328615248203278\n",
            "step: 70, loss: 0.16774338483810425\n",
            "step: 80, loss: 0.12698210775852203\n",
            "step: 90, loss: 0.043678976595401764\n",
            "step: 100, loss: 0.3043454885482788\n",
            "step: 110, loss: 0.13029366731643677\n",
            "step: 120, loss: 0.286465585231781\n",
            "step: 130, loss: 0.037940721958875656\n",
            "step: 140, loss: 0.2298945188522339\n",
            "step: 150, loss: 0.10278904438018799\n",
            "step: 160, loss: 0.05831747129559517\n",
            "step: 170, loss: 0.16081523895263672\n",
            "step: 180, loss: 0.4968874156475067\n",
            "step: 190, loss: 0.19321541488170624\n",
            "step: 200, loss: 0.18598762154579163\n",
            "step: 210, loss: 0.2612646818161011\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.48704663212435234, f1=0.5390070921985817, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10974933207035065\n",
            "step: 10, loss: 0.08107291907072067\n",
            "step: 20, loss: 0.19366610050201416\n",
            "step: 30, loss: 0.07422488927841187\n",
            "step: 40, loss: 0.10229698568582535\n",
            "step: 50, loss: 0.1882661134004593\n",
            "step: 60, loss: 0.11225685477256775\n",
            "step: 70, loss: 0.024206720292568207\n",
            "step: 80, loss: 0.03646409511566162\n",
            "step: 90, loss: 0.05686299502849579\n",
            "step: 100, loss: 0.014499866403639317\n",
            "step: 110, loss: 0.35422396659851074\n",
            "step: 120, loss: 0.12010897696018219\n",
            "step: 130, loss: 0.11459162086248398\n",
            "step: 140, loss: 0.18662485480308533\n",
            "step: 150, loss: 0.1325116753578186\n",
            "step: 160, loss: 0.18872500956058502\n",
            "step: 170, loss: 0.15987247228622437\n",
            "step: 180, loss: 0.10287914425134659\n",
            "step: 190, loss: 0.06016924977302551\n",
            "step: 200, loss: 0.12206082791090012\n",
            "step: 210, loss: 0.03155854344367981\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.49056603773584906, f1=0.5401709401709401, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027360746636986732\n",
            "step: 10, loss: 0.12549595534801483\n",
            "step: 20, loss: 0.020814865827560425\n",
            "step: 30, loss: 0.001443990389816463\n",
            "step: 40, loss: 0.053083766251802444\n",
            "step: 50, loss: 0.004140131641179323\n",
            "step: 60, loss: 0.14695769548416138\n",
            "step: 70, loss: 0.008929332718253136\n",
            "step: 80, loss: 0.028452571481466293\n",
            "step: 90, loss: 0.16339422762393951\n",
            "step: 100, loss: 0.004145897924900055\n",
            "step: 110, loss: 0.010586568154394627\n",
            "step: 120, loss: 0.03988558053970337\n",
            "step: 130, loss: 0.2025052160024643\n",
            "step: 140, loss: 0.09680500626564026\n",
            "step: 150, loss: 0.028014488518238068\n",
            "step: 160, loss: 0.05320383235812187\n",
            "step: 170, loss: 0.11887655407190323\n",
            "step: 180, loss: 0.08956947177648544\n",
            "step: 190, loss: 0.07059872150421143\n",
            "step: 200, loss: 0.09273654222488403\n",
            "step: 210, loss: 0.04989178851246834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4564459930313589, f1=0.5159010600706714, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019381260499358177\n",
            "step: 10, loss: 0.08939484506845474\n",
            "step: 20, loss: 0.0675668865442276\n",
            "step: 30, loss: 0.09879879653453827\n",
            "step: 40, loss: 0.013561815023422241\n",
            "step: 50, loss: 0.2024102509021759\n",
            "step: 60, loss: 0.025719575583934784\n",
            "step: 70, loss: 0.012758774682879448\n",
            "step: 80, loss: 0.09855744987726212\n",
            "step: 90, loss: 0.11985883116722107\n",
            "step: 100, loss: 0.005115697160363197\n",
            "step: 110, loss: 0.17156538367271423\n",
            "step: 120, loss: 0.06943763047456741\n",
            "step: 130, loss: 0.038067858666181564\n",
            "step: 140, loss: 0.03129391372203827\n",
            "step: 150, loss: 0.02262812852859497\n",
            "step: 160, loss: 0.004404081031680107\n",
            "step: 170, loss: 0.0037812944501638412\n",
            "step: 180, loss: 0.017844995483756065\n",
            "step: 190, loss: 0.004538544919341803\n",
            "step: 200, loss: 0.0025854504201561213\n",
            "step: 210, loss: 0.01669134385883808\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.43340857787810383, f1=0.5023474178403756, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008050496689975262\n",
            "step: 10, loss: 0.10780119895935059\n",
            "step: 20, loss: 0.011305198073387146\n",
            "step: 30, loss: 0.0030377504881471395\n",
            "step: 40, loss: 0.018894486129283905\n",
            "step: 50, loss: 0.006257252302020788\n",
            "step: 60, loss: 0.04925892874598503\n",
            "step: 70, loss: 0.010714668780565262\n",
            "step: 80, loss: 0.07473457604646683\n",
            "step: 90, loss: 0.018512947484850883\n",
            "step: 100, loss: 0.019160529598593712\n",
            "step: 110, loss: 0.022938812151551247\n",
            "step: 120, loss: 0.0038844896480441093\n",
            "step: 130, loss: 0.003034529509022832\n",
            "step: 140, loss: 0.004722025711089373\n",
            "step: 150, loss: 0.014396683312952518\n",
            "step: 160, loss: 0.06853161752223969\n",
            "step: 170, loss: 0.009695881977677345\n",
            "step: 180, loss: 0.17145639657974243\n",
            "step: 190, loss: 0.01258703414350748\n",
            "step: 200, loss: 0.01770460419356823\n",
            "step: 210, loss: 0.07534687221050262\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4424379232505643, f1=0.488262910798122, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021899497136473656\n",
            "step: 10, loss: 0.01603076048195362\n",
            "step: 20, loss: 0.015169416554272175\n",
            "step: 30, loss: 0.0067380876280367374\n",
            "step: 40, loss: 0.050860460847616196\n",
            "step: 50, loss: 0.011247918009757996\n",
            "step: 60, loss: 0.05460711941123009\n",
            "step: 70, loss: 0.008504031226038933\n",
            "step: 80, loss: 0.002577386796474457\n",
            "step: 90, loss: 0.0010394009295850992\n",
            "step: 100, loss: 0.005187541246414185\n",
            "step: 110, loss: 0.007695329375565052\n",
            "step: 120, loss: 0.00045905227307230234\n",
            "step: 130, loss: 0.020816057920455933\n",
            "step: 140, loss: 0.020741119980812073\n",
            "step: 150, loss: 0.08867024630308151\n",
            "step: 160, loss: 0.0011419201036915183\n",
            "step: 170, loss: 0.001213323324918747\n",
            "step: 180, loss: 0.07081428915262222\n",
            "step: 190, loss: 0.0008632439421489835\n",
            "step: 200, loss: 0.08108383417129517\n",
            "step: 210, loss: 0.0029881037771701813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.44921875, f1=0.5096525096525096, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013618886470794678\n",
            "step: 10, loss: 0.004422257654368877\n",
            "step: 20, loss: 0.008458462543785572\n",
            "step: 30, loss: 0.01872064732015133\n",
            "step: 40, loss: 0.005114738363772631\n",
            "step: 50, loss: 0.004912805277854204\n",
            "step: 60, loss: 0.005040478892624378\n",
            "step: 70, loss: 0.02472073957324028\n",
            "step: 80, loss: 0.00558092538267374\n",
            "step: 90, loss: 0.03744924068450928\n",
            "step: 100, loss: 0.025426382198929787\n",
            "step: 110, loss: 0.0026574430521577597\n",
            "step: 120, loss: 0.07138659060001373\n",
            "step: 130, loss: 0.054908156394958496\n",
            "step: 140, loss: 0.0007944648386910558\n",
            "step: 150, loss: 0.02184680849313736\n",
            "step: 160, loss: 0.005981167778372765\n",
            "step: 170, loss: 0.006083168089389801\n",
            "step: 180, loss: 0.005000079050660133\n",
            "step: 190, loss: 0.0010193443158641458\n",
            "step: 200, loss: 0.004860476590692997\n",
            "step: 210, loss: 0.008428463712334633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4247787610619469, f1=0.5178571428571429, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003962431102991104\n",
            "step: 10, loss: 0.00786309503018856\n",
            "step: 20, loss: 0.0681886151432991\n",
            "step: 30, loss: 0.0026255433913320303\n",
            "step: 40, loss: 0.003367218654602766\n",
            "step: 50, loss: 0.002372868126258254\n",
            "step: 60, loss: 0.0015501207672059536\n",
            "step: 70, loss: 0.05130873993039131\n",
            "step: 80, loss: 0.003237874014303088\n",
            "step: 90, loss: 0.0027370182797312737\n",
            "step: 100, loss: 0.09913662075996399\n",
            "step: 110, loss: 0.0011793682351708412\n",
            "step: 120, loss: 0.002246113494038582\n",
            "step: 130, loss: 0.008742082864046097\n",
            "step: 140, loss: 0.0245478767901659\n",
            "step: 150, loss: 0.0006595721933990717\n",
            "step: 160, loss: 0.03332987427711487\n",
            "step: 170, loss: 0.0008875790517777205\n",
            "step: 180, loss: 0.006925896275788546\n",
            "step: 190, loss: 0.0005794812459498644\n",
            "step: 200, loss: 0.0009309818269684911\n",
            "step: 210, loss: 0.002559009240940213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.44303797468354433, f1=0.510548523206751, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015412485226988792\n",
            "step: 10, loss: 0.004476815927773714\n",
            "step: 20, loss: 0.0017108895117416978\n",
            "step: 30, loss: 0.0030176681466400623\n",
            "step: 40, loss: 0.006353017874062061\n",
            "step: 50, loss: 0.0010536168701946735\n",
            "step: 60, loss: 0.004732794128358364\n",
            "step: 70, loss: 0.008356746286153793\n",
            "step: 80, loss: 0.0009383065043948591\n",
            "step: 90, loss: 0.040872588753700256\n",
            "step: 100, loss: 0.0015596768353134394\n",
            "step: 110, loss: 0.0014418135397136211\n",
            "step: 120, loss: 0.00663688313215971\n",
            "step: 130, loss: 0.00030324666295200586\n",
            "step: 140, loss: 0.0014117341488599777\n",
            "step: 150, loss: 0.001648225006647408\n",
            "step: 160, loss: 0.04922918230295181\n",
            "step: 170, loss: 0.0008393042953684926\n",
            "step: 180, loss: 0.005056667607277632\n",
            "step: 190, loss: 0.0010305453324690461\n",
            "step: 200, loss: 0.001142905093729496\n",
            "step: 210, loss: 0.0019356774864718318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.4353448275862069, f1=0.5052631578947369, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017617283388972282\n",
            "step: 10, loss: 0.0005784744862467051\n",
            "step: 20, loss: 0.0007428304525092244\n",
            "step: 30, loss: 0.00721797626465559\n",
            "step: 40, loss: 0.0011012105969712138\n",
            "step: 50, loss: 0.005955886095762253\n",
            "step: 60, loss: 0.0005946154706180096\n",
            "step: 70, loss: 0.006273441482335329\n",
            "step: 80, loss: 0.0011618011631071568\n",
            "step: 90, loss: 0.02943609096109867\n",
            "step: 100, loss: 0.0003272672474849969\n",
            "step: 110, loss: 0.0056977891363203526\n",
            "step: 120, loss: 0.014605996198952198\n",
            "step: 130, loss: 0.013655472546815872\n",
            "step: 140, loss: 0.0009528196533210576\n",
            "step: 150, loss: 0.00025571201695129275\n",
            "step: 160, loss: 0.0002573594683781266\n",
            "step: 170, loss: 0.008182535879313946\n",
            "step: 180, loss: 0.0021002190187573433\n",
            "step: 190, loss: 0.00031685573048889637\n",
            "step: 200, loss: 0.006232531275600195\n",
            "step: 210, loss: 0.0005465690628625453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.42494226327944573, f1=0.4919540229885057, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006886639748699963\n",
            "step: 10, loss: 0.0055926209315657616\n",
            "step: 20, loss: 0.005972490645945072\n",
            "step: 30, loss: 0.001334338798187673\n",
            "step: 40, loss: 0.0001623747666599229\n",
            "step: 50, loss: 0.001482086256146431\n",
            "step: 60, loss: 0.0003085599164478481\n",
            "step: 70, loss: 0.0008905419963411987\n",
            "step: 80, loss: 0.0004037385224364698\n",
            "step: 90, loss: 0.0003448172938078642\n",
            "step: 100, loss: 0.0032276292331516743\n",
            "step: 110, loss: 0.00018947452190332115\n",
            "step: 120, loss: 0.00030816387152299285\n",
            "step: 130, loss: 0.002397957956418395\n",
            "step: 140, loss: 0.04396524280309677\n",
            "step: 150, loss: 0.00018076406558975577\n",
            "step: 160, loss: 0.0007345859776251018\n",
            "step: 170, loss: 0.00011643861944321543\n",
            "step: 180, loss: 0.0037651644088327885\n",
            "step: 190, loss: 0.00025704348809085786\n",
            "step: 200, loss: 0.0015299738151952624\n",
            "step: 210, loss: 0.002205905271694064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.42028985507246375, f1=0.4879227053140097, best_f1=0.5495652173913044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002809266734402627\n",
            "step: 10, loss: 0.0003978674940299243\n",
            "step: 20, loss: 0.03690759465098381\n",
            "step: 30, loss: 0.014903713949024677\n",
            "step: 40, loss: 0.0001997423096327111\n",
            "step: 50, loss: 0.0002579293795861304\n",
            "step: 60, loss: 0.0056727877818048\n",
            "step: 70, loss: 0.001002845005132258\n",
            "step: 80, loss: 0.0006407427717931569\n",
            "step: 90, loss: 0.00012081822933396325\n",
            "step: 100, loss: 0.0005031322361901402\n",
            "step: 110, loss: 0.0001658390974625945\n",
            "step: 120, loss: 0.00020013318862766027\n",
            "step: 130, loss: 0.00011727453238563612\n",
            "step: 140, loss: 0.00017453249893151224\n",
            "step: 150, loss: 0.00016063638031482697\n",
            "step: 160, loss: 0.00025995736359618604\n",
            "step: 170, loss: 0.002085875952616334\n",
            "step: 180, loss: 0.000386740401154384\n",
            "step: 190, loss: 0.026346923783421516\n",
            "step: 200, loss: 0.006861276458948851\n",
            "step: 210, loss: 0.0010134493932127953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.41964285714285715, f1=0.5022222222222222, best_f1=0.5495652173913044\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 263.21it/s]\n",
            "load_f1 = 0.5084112149532711\n",
            "real_f1 = 0.5037593984962406\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 177.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47367929-6a1e-47e2-be5a-8760d15ca284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5303205251693726\n",
            "step: 10, loss: 0.3862031400203705\n",
            "step: 20, loss: 0.3000407814979553\n",
            "step: 30, loss: 0.4201473891735077\n",
            "step: 40, loss: 0.4309053421020508\n",
            "step: 50, loss: 0.30702048540115356\n",
            "step: 60, loss: 0.30840006470680237\n",
            "step: 70, loss: 0.29379430413246155\n",
            "step: 80, loss: 0.23465479910373688\n",
            "step: 90, loss: 0.30534401535987854\n",
            "step: 100, loss: 0.2937978208065033\n",
            "step: 110, loss: 0.5041799545288086\n",
            "step: 120, loss: 0.11110654473304749\n",
            "step: 130, loss: 0.186003640294075\n",
            "step: 140, loss: 0.06869842112064362\n",
            "step: 150, loss: 0.1489877700805664\n",
            "step: 160, loss: 0.06780638545751572\n",
            "step: 170, loss: 0.1872253268957138\n",
            "step: 180, loss: 0.02607361227273941\n",
            "step: 190, loss: 0.35784393548965454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5773195876288659, f1=0.5985037406483791, best_f1=0.5985037406483791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28591683506965637\n",
            "step: 10, loss: 0.145869180560112\n",
            "step: 20, loss: 0.21385493874549866\n",
            "step: 30, loss: 0.08955886960029602\n",
            "step: 40, loss: 0.16626335680484772\n",
            "step: 50, loss: 0.09010424464941025\n",
            "step: 60, loss: 0.22417403757572174\n",
            "step: 70, loss: 0.23516686260700226\n",
            "step: 80, loss: 0.14150165021419525\n",
            "step: 90, loss: 0.1331881582736969\n",
            "step: 100, loss: 0.024404115974903107\n",
            "step: 110, loss: 0.21301217377185822\n",
            "step: 120, loss: 0.25804221630096436\n",
            "step: 130, loss: 0.08044274896383286\n",
            "step: 140, loss: 0.09036274254322052\n",
            "step: 150, loss: 0.2133331000804901\n",
            "step: 160, loss: 0.08742763102054596\n",
            "step: 170, loss: 0.18500211834907532\n",
            "step: 180, loss: 0.062234655022621155\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.016578562557697296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7300771208226221, f1=0.7411167512690356, best_f1=0.7411167512690356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03719903528690338\n",
            "step: 10, loss: 0.06716660410165787\n",
            "step: 20, loss: 0.06275662034749985\n",
            "step: 30, loss: 0.12883122265338898\n",
            "step: 40, loss: 0.11648838967084885\n",
            "step: 50, loss: 0.20688562095165253\n",
            "step: 60, loss: 0.017238186672329903\n",
            "step: 70, loss: 0.10417954623699188\n",
            "step: 80, loss: 0.1854383796453476\n",
            "step: 90, loss: 0.11522508412599564\n",
            "step: 100, loss: 0.05747559666633606\n",
            "step: 110, loss: 0.014306631870567799\n",
            "step: 120, loss: 0.020368224009871483\n",
            "step: 130, loss: 0.019999828189611435\n",
            "step: 140, loss: 0.03740323334932327\n",
            "step: 150, loss: 0.16300293803215027\n",
            "step: 160, loss: 0.04295025020837784\n",
            "step: 170, loss: 0.19994889199733734\n",
            "step: 180, loss: 0.03676706179976463\n",
            "step: 190, loss: 0.04219254478812218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7747252747252747, f1=0.7676240208877285, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05002250894904137\n",
            "step: 10, loss: 0.17702455818653107\n",
            "step: 20, loss: 0.03348145633935928\n",
            "step: 30, loss: 0.09420575946569443\n",
            "step: 40, loss: 0.19201742112636566\n",
            "step: 50, loss: 0.09959989786148071\n",
            "step: 60, loss: 0.08560305088758469\n",
            "step: 70, loss: 0.15136556327342987\n",
            "step: 80, loss: 0.08013269305229187\n",
            "step: 90, loss: 0.004803596995770931\n",
            "step: 100, loss: 0.2507260739803314\n",
            "step: 110, loss: 0.00904111098498106\n",
            "step: 120, loss: 0.07846061885356903\n",
            "step: 130, loss: 0.1570775955915451\n",
            "step: 140, loss: 0.08874472975730896\n",
            "step: 150, loss: 0.025010602548718452\n",
            "step: 160, loss: 0.029959874227643013\n",
            "step: 170, loss: 0.08173482120037079\n",
            "step: 180, loss: 0.026556646451354027\n",
            "step: 190, loss: 0.12218278646469116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.717557251908397, f1=0.734375, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07836845517158508\n",
            "step: 10, loss: 0.0023956485092639923\n",
            "step: 20, loss: 0.12649273872375488\n",
            "step: 30, loss: 0.1342843919992447\n",
            "step: 40, loss: 0.017550788819789886\n",
            "step: 50, loss: 0.11549561470746994\n",
            "step: 60, loss: 0.06791265308856964\n",
            "step: 70, loss: 0.011130064725875854\n",
            "step: 80, loss: 0.007786758244037628\n",
            "step: 90, loss: 0.016038991510868073\n",
            "step: 100, loss: 0.0010965714463964105\n",
            "step: 110, loss: 0.03381528705358505\n",
            "step: 120, loss: 0.1822485625743866\n",
            "step: 130, loss: 0.09884287416934967\n",
            "step: 140, loss: 0.050422314554452896\n",
            "step: 150, loss: 0.10322815179824829\n",
            "step: 160, loss: 0.01159605011343956\n",
            "step: 170, loss: 0.0012249342398718\n",
            "step: 180, loss: 0.20252858102321625\n",
            "step: 190, loss: 0.024417931213974953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7138810198300283, f1=0.7486033519553073, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024625705555081367\n",
            "step: 10, loss: 0.24287134408950806\n",
            "step: 20, loss: 0.0025539586786180735\n",
            "step: 30, loss: 0.01120533887296915\n",
            "step: 40, loss: 0.05618206784129143\n",
            "step: 50, loss: 0.04405607655644417\n",
            "step: 60, loss: 0.08590944856405258\n",
            "step: 70, loss: 0.0069051203317940235\n",
            "step: 80, loss: 0.07901962846517563\n",
            "step: 90, loss: 0.004332950804382563\n",
            "step: 100, loss: 0.0003590500564314425\n",
            "step: 110, loss: 0.0011390276486054063\n",
            "step: 120, loss: 0.037309419363737106\n",
            "step: 130, loss: 0.004973005969077349\n",
            "step: 140, loss: 0.04367877542972565\n",
            "step: 150, loss: 0.00047690374776721\n",
            "step: 160, loss: 0.2817081809043884\n",
            "step: 170, loss: 0.1348668932914734\n",
            "step: 180, loss: 0.00299365958198905\n",
            "step: 190, loss: 0.0066113946959376335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7107438016528925, f1=0.7169811320754718, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007772132754325867\n",
            "step: 10, loss: 0.0021096717100590467\n",
            "step: 20, loss: 0.020376112312078476\n",
            "step: 30, loss: 0.012908389791846275\n",
            "step: 40, loss: 0.003534263465553522\n",
            "step: 50, loss: 0.11854131519794464\n",
            "step: 60, loss: 0.007964099757373333\n",
            "step: 70, loss: 0.00932723842561245\n",
            "step: 80, loss: 0.0019214511848986149\n",
            "step: 90, loss: 0.0011272537522017956\n",
            "step: 100, loss: 0.0005319851916283369\n",
            "step: 110, loss: 0.008474232628941536\n",
            "step: 120, loss: 0.0008044381393119693\n",
            "step: 130, loss: 0.0011215033009648323\n",
            "step: 140, loss: 0.0030981225427240133\n",
            "step: 150, loss: 0.017039256170392036\n",
            "step: 160, loss: 0.01060337945818901\n",
            "step: 170, loss: 0.000977954943664372\n",
            "step: 180, loss: 0.006277138367295265\n",
            "step: 190, loss: 0.0033233186695724726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7211267605633802, f1=0.7450980392156863, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031475943978875875\n",
            "step: 10, loss: 0.001936306362040341\n",
            "step: 20, loss: 0.0009698989451862872\n",
            "step: 30, loss: 0.0006351750926114619\n",
            "step: 40, loss: 0.00043408898636698723\n",
            "step: 50, loss: 0.0003665784897748381\n",
            "step: 60, loss: 0.0007558532524853945\n",
            "step: 70, loss: 0.000783981813583523\n",
            "step: 80, loss: 0.0001367814838886261\n",
            "step: 90, loss: 0.0008935311925597489\n",
            "step: 100, loss: 0.0020545090083032846\n",
            "step: 110, loss: 0.0008867750293575227\n",
            "step: 120, loss: 0.007057614158838987\n",
            "step: 130, loss: 0.0006965810898691416\n",
            "step: 140, loss: 0.0037188134156167507\n",
            "step: 150, loss: 0.006051268428564072\n",
            "step: 160, loss: 0.0009397607063874602\n",
            "step: 170, loss: 0.0006702614482492208\n",
            "step: 180, loss: 0.003930775914341211\n",
            "step: 190, loss: 0.00047288535279221833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7248677248677249, f1=0.7329842931937172, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029140856117010117\n",
            "step: 10, loss: 0.0005930840270593762\n",
            "step: 20, loss: 0.024477681145071983\n",
            "step: 30, loss: 0.002454555593430996\n",
            "step: 40, loss: 0.0022752743680030107\n",
            "step: 50, loss: 0.0006252101738937199\n",
            "step: 60, loss: 0.022429976612329483\n",
            "step: 70, loss: 0.0003969655663240701\n",
            "step: 80, loss: 0.0008381074294447899\n",
            "step: 90, loss: 0.008518459275364876\n",
            "step: 100, loss: 0.01661757193505764\n",
            "step: 110, loss: 0.0004394679272081703\n",
            "step: 120, loss: 0.015053720213472843\n",
            "step: 130, loss: 0.0005761653883382678\n",
            "step: 140, loss: 0.0004585389979183674\n",
            "step: 150, loss: 0.00034478501765988767\n",
            "step: 160, loss: 0.00019401188183110207\n",
            "step: 170, loss: 0.00044287709170021117\n",
            "step: 180, loss: 0.01349679846316576\n",
            "step: 190, loss: 0.0004062673542648554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7089947089947092, f1=0.7329842931937172, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014720149338245392\n",
            "step: 10, loss: 0.0007197451195679605\n",
            "step: 20, loss: 0.00044452407746575773\n",
            "step: 30, loss: 0.00012374793004710227\n",
            "step: 40, loss: 0.0007510874420404434\n",
            "step: 50, loss: 0.0008586428011767566\n",
            "step: 60, loss: 0.000316714373184368\n",
            "step: 70, loss: 0.0009416628745384514\n",
            "step: 80, loss: 0.00017050221504177898\n",
            "step: 90, loss: 0.00016449950635433197\n",
            "step: 100, loss: 0.0003855424583889544\n",
            "step: 110, loss: 0.00037261380930431187\n",
            "step: 120, loss: 0.003848637454211712\n",
            "step: 130, loss: 0.0010659967083483934\n",
            "step: 140, loss: 0.0002805473341140896\n",
            "step: 150, loss: 0.0004073463787790388\n",
            "step: 160, loss: 0.0005921833217144012\n",
            "step: 170, loss: 0.00020556777599267662\n",
            "step: 180, loss: 0.0001497876801295206\n",
            "step: 190, loss: 0.00023716116265859455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7326203208556149, f1=0.7272727272727273, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019972192239947617\n",
            "step: 10, loss: 0.0003164858790114522\n",
            "step: 20, loss: 0.0010634275386109948\n",
            "step: 30, loss: 0.00033548136707395315\n",
            "step: 40, loss: 0.00029118574457243085\n",
            "step: 50, loss: 0.0005876439972780645\n",
            "step: 60, loss: 0.0002188633952755481\n",
            "step: 70, loss: 0.00020433295867405832\n",
            "step: 80, loss: 0.0016425995854660869\n",
            "step: 90, loss: 0.00013918810873292387\n",
            "step: 100, loss: 0.0001500679354649037\n",
            "step: 110, loss: 0.0010789224179461598\n",
            "step: 120, loss: 0.00029121930128894746\n",
            "step: 130, loss: 0.0028066642116755247\n",
            "step: 140, loss: 0.0005200132727622986\n",
            "step: 150, loss: 0.00012354635691735893\n",
            "step: 160, loss: 0.00034074895665980875\n",
            "step: 170, loss: 0.0002529648190829903\n",
            "step: 180, loss: 0.0002812393067870289\n",
            "step: 190, loss: 0.0005992516526021063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7302452316076293, f1=0.745945945945946, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019132021407131106\n",
            "step: 10, loss: 0.0017126970924437046\n",
            "step: 20, loss: 0.0005218239384703338\n",
            "step: 30, loss: 0.00033483869628980756\n",
            "step: 40, loss: 0.000371974369045347\n",
            "step: 50, loss: 0.0009404337033629417\n",
            "step: 60, loss: 0.00023959166719578207\n",
            "step: 70, loss: 0.00018337850633542985\n",
            "step: 80, loss: 0.0005142401205375791\n",
            "step: 90, loss: 0.0002977528783958405\n",
            "step: 100, loss: 0.0010597953805699944\n",
            "step: 110, loss: 0.00036474273656494915\n",
            "step: 120, loss: 0.0002583330206107348\n",
            "step: 130, loss: 0.0008664560155011714\n",
            "step: 140, loss: 0.0007691021892242134\n",
            "step: 150, loss: 0.00022881616314407438\n",
            "step: 160, loss: 9.082483302336186e-05\n",
            "step: 170, loss: 0.00041959501686505973\n",
            "step: 180, loss: 0.00012664738460443914\n",
            "step: 190, loss: 0.00017584397573955357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7422680412371134, f1=0.7538461538461538, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003650809230748564\n",
            "step: 10, loss: 0.00023627815244253725\n",
            "step: 20, loss: 0.002295487094670534\n",
            "step: 30, loss: 0.0004670721828006208\n",
            "step: 40, loss: 0.00022722204448655248\n",
            "step: 50, loss: 0.0005249777459539473\n",
            "step: 60, loss: 0.0010673115029931068\n",
            "step: 70, loss: 0.0006123686907812953\n",
            "step: 80, loss: 0.0009140143520198762\n",
            "step: 90, loss: 0.0014421935193240643\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.056940071284770966\n",
            "step: 110, loss: 0.00019708585750777274\n",
            "step: 120, loss: 0.00015329214511439204\n",
            "step: 130, loss: 0.00010132935858564451\n",
            "step: 140, loss: 0.00030247410177253187\n",
            "step: 150, loss: 9.678552305558696e-05\n",
            "step: 160, loss: 0.0015889003407210112\n",
            "step: 170, loss: 0.0002531249774619937\n",
            "step: 180, loss: 0.00013469357509166002\n",
            "step: 190, loss: 0.0008530404884368181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7252747252747253, f1=0.7417582417582418, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020041741663590074\n",
            "step: 10, loss: 0.00012276943016331643\n",
            "step: 20, loss: 0.00045106065226718783\n",
            "step: 30, loss: 0.00011553082731552422\n",
            "step: 40, loss: 0.0002839801018126309\n",
            "step: 50, loss: 0.000278048450127244\n",
            "step: 60, loss: 0.00015152375272009522\n",
            "step: 70, loss: 0.00010983379615936428\n",
            "step: 80, loss: 0.0009490444208495319\n",
            "step: 90, loss: 0.0010317665291950107\n",
            "step: 100, loss: 0.0010051729623228312\n",
            "step: 110, loss: 0.00023604561283718795\n",
            "step: 120, loss: 0.0006784358993172646\n",
            "step: 130, loss: 0.0005158467683941126\n",
            "step: 140, loss: 0.00018750854360405356\n",
            "step: 150, loss: 0.00014193844981491566\n",
            "step: 160, loss: 0.00027085107285529375\n",
            "step: 170, loss: 0.0003591044805943966\n",
            "step: 180, loss: 0.00029821021598763764\n",
            "step: 190, loss: 0.00015628758410457522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7423822714681441, f1=0.7493261455525607, best_f1=0.7676240208877285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003675827174447477\n",
            "step: 10, loss: 0.00024071680672932416\n",
            "step: 20, loss: 0.000159253177116625\n",
            "step: 30, loss: 0.00048742443323135376\n",
            "step: 40, loss: 0.00015995411376934499\n",
            "step: 50, loss: 0.00021423482394311577\n",
            "step: 60, loss: 0.00015925026673357934\n",
            "step: 70, loss: 0.00023514157510362566\n",
            "step: 80, loss: 0.0014494325732812285\n",
            "step: 90, loss: 9.549382230034098e-05\n",
            "step: 100, loss: 0.0002686823718249798\n",
            "step: 110, loss: 8.343060471815988e-05\n",
            "step: 120, loss: 0.0007505331886932254\n",
            "step: 130, loss: 0.0008853247272782028\n",
            "step: 140, loss: 0.0006045598420314491\n",
            "step: 150, loss: 0.0003964526695199311\n",
            "step: 160, loss: 0.00024849598412401974\n",
            "step: 170, loss: 0.00014852384629193693\n",
            "step: 180, loss: 0.001262521487660706\n",
            "step: 190, loss: 0.0005585203180089593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7423822714681441, f1=0.745945945945946, best_f1=0.7676240208877285\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 166.63it/s]\n",
            "load_f1 = 0.6914893617021276\n",
            "real_f1 = 0.6494845360824743\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 178.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733f4ec1-06c4-479b-8a53-0074426df97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6319571733474731\n",
            "step: 10, loss: 0.3642005920410156\n",
            "step: 20, loss: 0.2886134088039398\n",
            "step: 30, loss: 0.3748997151851654\n",
            "step: 40, loss: 0.27916574478149414\n",
            "step: 50, loss: 0.26424115896224976\n",
            "step: 60, loss: 0.3484301269054413\n",
            "step: 70, loss: 0.36188673973083496\n",
            "step: 80, loss: 0.31054261326789856\n",
            "step: 90, loss: 0.17663311958312988\n",
            "step: 100, loss: 0.24566039443016052\n",
            "step: 110, loss: 0.21750427782535553\n",
            "step: 120, loss: 0.07266655564308167\n",
            "step: 130, loss: 0.057942964136600494\n",
            "step: 140, loss: 0.20291438698768616\n",
            "step: 150, loss: 0.22627349197864532\n",
            "step: 160, loss: 0.12006838619709015\n",
            "step: 170, loss: 0.27095547318458557\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6370757180156658, f1=0.5861182519280206, best_f1=0.5861182519280206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25044289231300354\n",
            "step: 10, loss: 0.298211008310318\n",
            "step: 20, loss: 0.06463055312633514\n",
            "step: 30, loss: 0.23173704743385315\n",
            "step: 40, loss: 0.07754459977149963\n",
            "step: 50, loss: 0.1530933678150177\n",
            "step: 60, loss: 0.3556568920612335\n",
            "step: 70, loss: 0.07889925688505173\n",
            "step: 80, loss: 0.09676633775234222\n",
            "step: 90, loss: 0.12659624218940735\n",
            "step: 100, loss: 0.19526785612106323\n",
            "step: 110, loss: 0.10268349945545197\n",
            "step: 120, loss: 0.19877488911151886\n",
            "step: 130, loss: 0.08647836744785309\n",
            "step: 140, loss: 0.2153802216053009\n",
            "step: 150, loss: 0.1808393895626068\n",
            "step: 160, loss: 0.14520908892154694\n",
            "step: 170, loss: 0.04605618864297867\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7174447174447175, f1=0.6838407494145199, best_f1=0.6838407494145199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12001394480466843\n",
            "step: 10, loss: 0.09213340282440186\n",
            "step: 20, loss: 0.03474653139710426\n",
            "step: 30, loss: 0.08562447130680084\n",
            "step: 40, loss: 0.1216849833726883\n",
            "step: 50, loss: 0.145889014005661\n",
            "step: 60, loss: 0.18642063438892365\n",
            "step: 70, loss: 0.19567571580410004\n",
            "step: 80, loss: 0.09787274152040482\n",
            "step: 90, loss: 0.1582944244146347\n",
            "step: 100, loss: 0.028428778052330017\n",
            "step: 110, loss: 0.08481057733297348\n",
            "step: 120, loss: 0.07043958455324173\n",
            "step: 130, loss: 0.06444214284420013\n",
            "step: 140, loss: 0.11224721372127533\n",
            "step: 150, loss: 0.05290435254573822\n",
            "step: 160, loss: 0.09970194846391678\n",
            "step: 170, loss: 0.09670715779066086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7154046997389034, f1=0.7146529562982005, best_f1=0.6838407494145199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01240132562816143\n",
            "step: 10, loss: 0.13856150209903717\n",
            "step: 20, loss: 0.05675071105360985\n",
            "step: 30, loss: 0.13919180631637573\n",
            "step: 40, loss: 0.008014261722564697\n",
            "step: 50, loss: 0.18263451755046844\n",
            "step: 60, loss: 0.19077783823013306\n",
            "step: 70, loss: 0.061525680124759674\n",
            "step: 80, loss: 0.1350145787000656\n",
            "step: 90, loss: 0.025269145146012306\n",
            "step: 100, loss: 0.20004849135875702\n",
            "step: 110, loss: 0.06465837359428406\n",
            "step: 120, loss: 0.018989797681570053\n",
            "step: 130, loss: 0.17112784087657928\n",
            "step: 140, loss: 0.04155602678656578\n",
            "step: 150, loss: 0.09736144542694092\n",
            "step: 160, loss: 0.045548539608716965\n",
            "step: 170, loss: 0.14799964427947998\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7196029776674938, f1=0.6944444444444445, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04172138497233391\n",
            "step: 10, loss: 0.0347561314702034\n",
            "step: 20, loss: 0.02965640462934971\n",
            "step: 30, loss: 0.005770252086222172\n",
            "step: 40, loss: 0.014603989198803902\n",
            "step: 50, loss: 0.19737207889556885\n",
            "step: 60, loss: 0.05465118587017059\n",
            "step: 70, loss: 0.23210695385932922\n",
            "step: 80, loss: 0.11997847259044647\n",
            "step: 90, loss: 0.13537763059139252\n",
            "step: 100, loss: 0.11445767432451248\n",
            "step: 110, loss: 0.07785426080226898\n",
            "step: 120, loss: 0.030599232763051987\n",
            "step: 130, loss: 0.14433392882347107\n",
            "step: 140, loss: 0.023885594680905342\n",
            "step: 150, loss: 0.17060303688049316\n",
            "step: 160, loss: 0.10382168740034103\n",
            "step: 170, loss: 0.11144702881574631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6870588235294117, f1=0.7069351230425055, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011222245171666145\n",
            "step: 10, loss: 0.08977460116147995\n",
            "step: 20, loss: 0.008689038455486298\n",
            "step: 30, loss: 0.02224016934633255\n",
            "step: 40, loss: 0.06284385919570923\n",
            "step: 50, loss: 0.1214369609951973\n",
            "step: 60, loss: 0.04183569177985191\n",
            "step: 70, loss: 0.03932677209377289\n",
            "step: 80, loss: 0.045384861528873444\n",
            "step: 90, loss: 0.009358972311019897\n",
            "step: 100, loss: 0.03385908901691437\n",
            "step: 110, loss: 0.002053172793239355\n",
            "step: 120, loss: 0.007391799241304398\n",
            "step: 130, loss: 0.007926597259938717\n",
            "step: 140, loss: 0.19589251279830933\n",
            "step: 150, loss: 0.051615044474601746\n",
            "step: 160, loss: 0.062180086970329285\n",
            "step: 170, loss: 0.026266802102327347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6870229007633587, f1=0.6794258373205743, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00482971454039216\n",
            "step: 10, loss: 0.03930843994021416\n",
            "step: 20, loss: 0.004694982897490263\n",
            "step: 30, loss: 0.03468982130289078\n",
            "step: 40, loss: 0.06383955478668213\n",
            "step: 50, loss: 0.05290244147181511\n",
            "step: 60, loss: 0.00425699632614851\n",
            "step: 70, loss: 0.007998989894986153\n",
            "step: 80, loss: 0.018730632960796356\n",
            "step: 90, loss: 0.0011357904877513647\n",
            "step: 100, loss: 0.1554650068283081\n",
            "step: 110, loss: 0.01382453739643097\n",
            "step: 120, loss: 0.004047485999763012\n",
            "step: 130, loss: 0.10872554779052734\n",
            "step: 140, loss: 0.009338350966572762\n",
            "step: 150, loss: 0.01039922796189785\n",
            "step: 160, loss: 0.0032371897250413895\n",
            "step: 170, loss: 0.04632631316781044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6802030456852792, f1=0.6617283950617283, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015588066540658474\n",
            "step: 10, loss: 0.006026907358318567\n",
            "step: 20, loss: 0.02728433907032013\n",
            "step: 30, loss: 0.3408445417881012\n",
            "step: 40, loss: 0.017038894817233086\n",
            "step: 50, loss: 0.0027912179939448833\n",
            "step: 60, loss: 0.023671770468354225\n",
            "step: 70, loss: 0.036590322852134705\n",
            "step: 80, loss: 0.0035704155452549458\n",
            "step: 90, loss: 0.0029918733052909374\n",
            "step: 100, loss: 0.1708890199661255\n",
            "step: 110, loss: 0.0826750174164772\n",
            "step: 120, loss: 0.01623537577688694\n",
            "step: 130, loss: 0.03542787581682205\n",
            "step: 140, loss: 0.012791801244020462\n",
            "step: 150, loss: 0.008371589705348015\n",
            "step: 160, loss: 0.006677614059299231\n",
            "step: 170, loss: 0.007266081869602203\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6969696969696969, f1=0.7070217917675544, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038514535408467054\n",
            "step: 10, loss: 0.003993174061179161\n",
            "step: 20, loss: 0.004283763002604246\n",
            "step: 30, loss: 0.0756089985370636\n",
            "step: 40, loss: 0.17921939492225647\n",
            "step: 50, loss: 0.0010539545910432935\n",
            "step: 60, loss: 0.03149138763546944\n",
            "step: 70, loss: 0.0012737520737573504\n",
            "step: 80, loss: 0.010785059072077274\n",
            "step: 90, loss: 0.01760842837393284\n",
            "step: 100, loss: 0.07643269002437592\n",
            "step: 110, loss: 0.001895107445307076\n",
            "step: 120, loss: 0.026085250079631805\n",
            "step: 130, loss: 0.06264110654592514\n",
            "step: 140, loss: 0.03546600416302681\n",
            "step: 150, loss: 0.003103244351223111\n",
            "step: 160, loss: 0.03614344820380211\n",
            "step: 170, loss: 0.1723269820213318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.707774798927614, f1=0.6909090909090909, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05770310014486313\n",
            "step: 10, loss: 0.004403893835842609\n",
            "step: 20, loss: 0.13230766355991364\n",
            "step: 30, loss: 0.09531198441982269\n",
            "step: 40, loss: 0.0033555258996784687\n",
            "step: 50, loss: 0.07760195434093475\n",
            "step: 60, loss: 0.005199769511818886\n",
            "step: 70, loss: 0.004897296894341707\n",
            "step: 80, loss: 0.0035691228695213795\n",
            "step: 90, loss: 0.007954183034598827\n",
            "step: 100, loss: 0.006685251370072365\n",
            "step: 110, loss: 0.019544582813978195\n",
            "step: 120, loss: 0.0024146686773747206\n",
            "step: 130, loss: 0.009498139843344688\n",
            "step: 140, loss: 0.05485466122627258\n",
            "step: 150, loss: 0.07967973500490189\n",
            "step: 160, loss: 0.0011161435395479202\n",
            "step: 170, loss: 0.0008185422630049288\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7010309278350515, f1=0.683291770573566, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002428429201245308\n",
            "step: 10, loss: 0.06697893142700195\n",
            "step: 20, loss: 0.0012822604039683938\n",
            "step: 30, loss: 0.001683258917182684\n",
            "step: 40, loss: 0.00044581954716704786\n",
            "step: 50, loss: 0.0055776676163077354\n",
            "step: 60, loss: 0.00582482572644949\n",
            "step: 70, loss: 0.0004727877676486969\n",
            "step: 80, loss: 0.001293362583965063\n",
            "step: 90, loss: 0.0022289101034402847\n",
            "step: 100, loss: 0.0003740156826097518\n",
            "step: 110, loss: 0.033918749541044235\n",
            "step: 120, loss: 0.00044297578278928995\n",
            "step: 130, loss: 0.00032558635575696826\n",
            "step: 140, loss: 0.0025650751776993275\n",
            "step: 150, loss: 0.0006674132309854031\n",
            "step: 160, loss: 0.017316052690148354\n",
            "step: 170, loss: 0.02593991719186306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6868686868686869, f1=0.6714975845410628, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007277446798980236\n",
            "step: 10, loss: 0.0008259073947556317\n",
            "step: 20, loss: 0.0037351769860833883\n",
            "step: 30, loss: 0.00030420266557484865\n",
            "step: 40, loss: 0.008324508555233479\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.09688466042280197\n",
            "step: 60, loss: 0.0012775642098858953\n",
            "step: 70, loss: 0.043593116104602814\n",
            "step: 80, loss: 0.0003248131542932242\n",
            "step: 90, loss: 0.0017907473957166076\n",
            "step: 100, loss: 0.0010514426976442337\n",
            "step: 110, loss: 0.011487002484500408\n",
            "step: 120, loss: 0.016364719718694687\n",
            "step: 130, loss: 0.0018254731548950076\n",
            "step: 140, loss: 0.009686280973255634\n",
            "step: 150, loss: 0.001271773362532258\n",
            "step: 160, loss: 0.0005203665350563824\n",
            "step: 170, loss: 0.0006945973145775497\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6723646723646722, f1=0.6595174262734584, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004469336417969316\n",
            "step: 10, loss: 0.00238011940382421\n",
            "step: 20, loss: 0.00912928581237793\n",
            "step: 30, loss: 0.0001812233531381935\n",
            "step: 40, loss: 0.0004902396467514336\n",
            "step: 50, loss: 0.00023555349616799504\n",
            "step: 60, loss: 0.0015382496640086174\n",
            "step: 70, loss: 0.004656173288822174\n",
            "step: 80, loss: 0.0011457190848886967\n",
            "step: 90, loss: 0.0001430170377716422\n",
            "step: 100, loss: 0.2115388661623001\n",
            "step: 110, loss: 0.00018275990441907197\n",
            "step: 120, loss: 0.014756505377590656\n",
            "step: 130, loss: 0.00023184504243545234\n",
            "step: 140, loss: 0.003160382155328989\n",
            "step: 150, loss: 0.006324188318103552\n",
            "step: 160, loss: 0.004628084134310484\n",
            "step: 170, loss: 0.001195740420371294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6724137931034483, f1=0.6880000000000001, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007289705099537969\n",
            "step: 10, loss: 0.0008844103431329131\n",
            "step: 20, loss: 0.008091759867966175\n",
            "step: 30, loss: 0.07060319930315018\n",
            "step: 40, loss: 0.0002403081307420507\n",
            "step: 50, loss: 0.0007927267579361796\n",
            "step: 60, loss: 0.0003296229406259954\n",
            "step: 70, loss: 0.00023612195218447596\n",
            "step: 80, loss: 0.024735160171985626\n",
            "step: 90, loss: 0.00011388210987206548\n",
            "step: 100, loss: 0.00036004031426273286\n",
            "step: 110, loss: 0.00020202971063554287\n",
            "step: 120, loss: 0.0006544529460370541\n",
            "step: 130, loss: 0.0003088151279371232\n",
            "step: 140, loss: 0.001145881717093289\n",
            "step: 150, loss: 0.0013433977728709579\n",
            "step: 160, loss: 9.800514089874923e-05\n",
            "step: 170, loss: 0.0001243258302565664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6721763085399449, f1=0.6910994764397905, best_f1=0.6944444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002881412801798433\n",
            "step: 10, loss: 0.0009140034671872854\n",
            "step: 20, loss: 0.06770621240139008\n",
            "step: 30, loss: 0.0004738546849694103\n",
            "step: 40, loss: 0.0011736896121874452\n",
            "step: 50, loss: 0.000651294132694602\n",
            "step: 60, loss: 0.040954262018203735\n",
            "step: 70, loss: 0.005705642513930798\n",
            "step: 80, loss: 0.0004797691071871668\n",
            "step: 90, loss: 0.00021256966283544898\n",
            "step: 100, loss: 0.006990406196564436\n",
            "step: 110, loss: 0.0004924663226120174\n",
            "step: 120, loss: 0.028711970895528793\n",
            "step: 130, loss: 0.0009108165395446122\n",
            "step: 140, loss: 0.00020972049969714135\n",
            "step: 150, loss: 0.01640263944864273\n",
            "step: 160, loss: 0.12863659858703613\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 170, loss: 0.0002696315059438348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6703296703296703, f1=0.6822916666666666, best_f1=0.6944444444444445\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:09, 210.61it/s]\n",
            "load_f1 = 0.7053140096618358\n",
            "real_f1 = 0.7241379310344829\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6da1bb-97f3-49df-8cd8-80192473b1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6031379699707031\n",
            "step: 10, loss: 0.6395144462585449\n",
            "step: 20, loss: 0.4851710796356201\n",
            "step: 30, loss: 0.4518936276435852\n",
            "step: 40, loss: 0.28168749809265137\n",
            "step: 50, loss: 0.08428388088941574\n",
            "step: 60, loss: 0.08016380667686462\n",
            "step: 70, loss: 0.2860534191131592\n",
            "step: 80, loss: 0.07184000313282013\n",
            "step: 90, loss: 0.14025017619132996\n",
            "step: 100, loss: 0.009048317559063435\n",
            "step: 110, loss: 0.11347726732492447\n",
            "step: 120, loss: 0.0385235920548439\n",
            "step: 130, loss: 0.015684951096773148\n",
            "step: 140, loss: 0.02152738906443119\n",
            "step: 150, loss: 0.06368913501501083\n",
            "step: 160, loss: 0.04158054292201996\n",
            "step: 170, loss: 0.24465620517730713\n",
            "step: 180, loss: 0.08318504691123962\n",
            "step: 190, loss: 0.01856108382344246\n",
            "step: 200, loss: 0.09117037802934647\n",
            "step: 210, loss: 0.013791278935968876\n",
            "step: 220, loss: 0.02512577921152115\n",
            "step: 230, loss: 0.19296960532665253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.941834451901566, f1=0.9362186788154897, best_f1=0.9362186788154897\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01392364501953125\n",
            "step: 10, loss: 0.03160727396607399\n",
            "step: 20, loss: 0.2596651017665863\n",
            "step: 30, loss: 0.15986406803131104\n",
            "step: 40, loss: 0.13687764108181\n",
            "step: 50, loss: 0.012287762016057968\n",
            "step: 60, loss: 0.016190823167562485\n",
            "step: 70, loss: 0.05321013927459717\n",
            "step: 80, loss: 0.04578004404902458\n",
            "step: 90, loss: 0.2633776366710663\n",
            "step: 100, loss: 0.023373985663056374\n",
            "step: 110, loss: 0.19603654742240906\n",
            "step: 120, loss: 0.13089410960674286\n",
            "step: 130, loss: 0.09958405047655106\n",
            "step: 140, loss: 0.0037621057126671076\n",
            "step: 150, loss: 0.05480048805475235\n",
            "step: 160, loss: 0.014990024268627167\n",
            "step: 170, loss: 0.007407966535538435\n",
            "step: 180, loss: 0.04076734185218811\n",
            "step: 190, loss: 0.0052473219111561775\n",
            "step: 200, loss: 0.03724247217178345\n",
            "step: 210, loss: 0.0038774877320975065\n",
            "step: 220, loss: 0.0744168683886528\n",
            "step: 230, loss: 0.222891703248024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9552572706935123, f1=0.9410430839002268, best_f1=0.9410430839002268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028919517993927002\n",
            "step: 10, loss: 0.022276056930422783\n",
            "step: 20, loss: 0.12418389320373535\n",
            "step: 30, loss: 0.012226833961904049\n",
            "step: 40, loss: 0.04972351714968681\n",
            "step: 50, loss: 0.10197560489177704\n",
            "step: 60, loss: 0.004376474767923355\n",
            "step: 70, loss: 0.04562724009156227\n",
            "step: 80, loss: 0.0013634926872327924\n",
            "step: 90, loss: 0.11751794070005417\n",
            "step: 100, loss: 0.03739327937364578\n",
            "step: 110, loss: 0.0056849876418709755\n",
            "step: 120, loss: 0.009360251948237419\n",
            "step: 130, loss: 0.003514124546200037\n",
            "step: 140, loss: 0.05623365566134453\n",
            "step: 150, loss: 0.023737885057926178\n",
            "step: 160, loss: 0.019434882327914238\n",
            "step: 170, loss: 0.005188248120248318\n",
            "step: 180, loss: 0.08724739402532578\n",
            "step: 190, loss: 0.003070216393098235\n",
            "step: 200, loss: 0.07335541397333145\n",
            "step: 210, loss: 0.024481039494276047\n",
            "step: 220, loss: 0.030829355120658875\n",
            "step: 230, loss: 0.0015570969553664327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9601769911504424, f1=0.951739618406285, best_f1=0.951739618406285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039375293999910355\n",
            "step: 10, loss: 0.0019487030804157257\n",
            "step: 20, loss: 0.0023363411892205477\n",
            "step: 30, loss: 0.0020748923998326063\n",
            "step: 40, loss: 0.0024054802488535643\n",
            "step: 50, loss: 0.0005806658882647753\n",
            "step: 60, loss: 0.0010324738686904311\n",
            "step: 70, loss: 0.0014774785377085209\n",
            "step: 80, loss: 0.001110757701098919\n",
            "step: 90, loss: 0.0017707105726003647\n",
            "step: 100, loss: 0.00398022448644042\n",
            "step: 110, loss: 0.0023957001976668835\n",
            "step: 120, loss: 0.004990576300770044\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.0635017454624176\n",
            "step: 140, loss: 0.0026253615505993366\n",
            "step: 150, loss: 0.21686267852783203\n",
            "step: 160, loss: 0.019668100401759148\n",
            "step: 170, loss: 0.04588816314935684\n",
            "step: 180, loss: 0.0006536812870763242\n",
            "step: 190, loss: 0.0023462011013180017\n",
            "step: 200, loss: 0.02726718969643116\n",
            "step: 210, loss: 0.00268554431386292\n",
            "step: 220, loss: 0.0039055675733834505\n",
            "step: 230, loss: 0.08697099983692169\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9600886917960089, f1=0.9496080627099663, best_f1=0.951739618406285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019286867463961244\n",
            "step: 10, loss: 0.0006001087604090571\n",
            "step: 20, loss: 0.0038345842622220516\n",
            "step: 30, loss: 0.0008806852856650949\n",
            "step: 40, loss: 0.0017882243264466524\n",
            "step: 50, loss: 0.0007032065186649561\n",
            "step: 60, loss: 0.21346797049045563\n",
            "step: 70, loss: 0.001215155702084303\n",
            "step: 80, loss: 0.0013435548171401024\n",
            "step: 90, loss: 0.0031858193688094616\n",
            "step: 100, loss: 0.0010748454369604588\n",
            "step: 110, loss: 0.0008938164100982249\n",
            "step: 120, loss: 0.00027602899353951216\n",
            "step: 130, loss: 0.007652962114661932\n",
            "step: 140, loss: 0.005954355001449585\n",
            "step: 150, loss: 0.0010108217829838395\n",
            "step: 160, loss: 0.0010350244119763374\n",
            "step: 170, loss: 0.007319845724850893\n",
            "step: 180, loss: 0.050428036600351334\n",
            "step: 190, loss: 0.13629329204559326\n",
            "step: 200, loss: 0.005918328650295734\n",
            "step: 210, loss: 0.002503909170627594\n",
            "step: 220, loss: 0.015515236184000969\n",
            "step: 230, loss: 0.0011204632464796305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9603624009060023, f1=0.9450800915331807, best_f1=0.9450800915331807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004043166060000658\n",
            "step: 10, loss: 0.007819402031600475\n",
            "step: 20, loss: 0.005204575601965189\n",
            "step: 30, loss: 0.0004511185979936272\n",
            "step: 40, loss: 0.0005584486061707139\n",
            "step: 50, loss: 0.0009124736534431577\n",
            "step: 60, loss: 0.0004108596476726234\n",
            "step: 70, loss: 0.020010508596897125\n",
            "step: 80, loss: 0.0028338709380477667\n",
            "step: 90, loss: 0.004965753294527531\n",
            "step: 100, loss: 0.02672550641000271\n",
            "step: 110, loss: 0.0013844526838511229\n",
            "step: 120, loss: 0.0006614943267777562\n",
            "step: 130, loss: 0.0010884266812354326\n",
            "step: 140, loss: 0.0002813464670907706\n",
            "step: 150, loss: 0.00704239634796977\n",
            "step: 160, loss: 0.0013932830188423395\n",
            "step: 170, loss: 0.0005820182850584388\n",
            "step: 180, loss: 0.002916609635576606\n",
            "step: 190, loss: 0.0003840719582512975\n",
            "step: 200, loss: 0.0004893068107776344\n",
            "step: 210, loss: 0.00037068116944283247\n",
            "step: 220, loss: 0.0005634092376567423\n",
            "step: 230, loss: 0.09922551363706589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9585666293393057, f1=0.9537767756482525, best_f1=0.9450800915331807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003188162110745907\n",
            "step: 10, loss: 0.0012470112415030599\n",
            "step: 20, loss: 0.000975678616669029\n",
            "step: 30, loss: 0.0005006862338632345\n",
            "step: 40, loss: 0.0013225047150626779\n",
            "step: 50, loss: 0.000954324146732688\n",
            "step: 60, loss: 0.0006842829752713442\n",
            "step: 70, loss: 0.026211746037006378\n",
            "step: 80, loss: 0.002734798239544034\n",
            "step: 90, loss: 0.0002244873030576855\n",
            "step: 100, loss: 0.004522125702351332\n",
            "step: 110, loss: 0.00033624196657910943\n",
            "step: 120, loss: 0.0022377350833266973\n",
            "step: 130, loss: 0.0002950537600554526\n",
            "step: 140, loss: 0.002040265826508403\n",
            "step: 150, loss: 0.0012544832425191998\n",
            "step: 160, loss: 0.1018514335155487\n",
            "step: 170, loss: 0.023466939106583595\n",
            "step: 180, loss: 0.10583256930112839\n",
            "step: 190, loss: 0.0020854880567640066\n",
            "step: 200, loss: 0.06679078191518784\n",
            "step: 210, loss: 0.0036171083338558674\n",
            "step: 220, loss: 0.07512229681015015\n",
            "step: 230, loss: 0.0022499996703118086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9572072072072072, f1=0.9453302961275627, best_f1=0.9450800915331807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012627814430743456\n",
            "step: 10, loss: 0.000859764579217881\n",
            "step: 20, loss: 0.0004961717058904469\n",
            "step: 30, loss: 0.0006523014162667096\n",
            "step: 40, loss: 0.005670711863785982\n",
            "step: 50, loss: 0.00042413995834067464\n",
            "step: 60, loss: 0.0008143538143485785\n",
            "step: 70, loss: 0.0010022082133218646\n",
            "step: 80, loss: 0.0035793909337371588\n",
            "step: 90, loss: 0.00028631248278543353\n",
            "step: 100, loss: 0.0017403494566679\n",
            "step: 110, loss: 0.004537269007414579\n",
            "step: 120, loss: 0.00258272304199636\n",
            "step: 130, loss: 0.0023533774074167013\n",
            "step: 140, loss: 0.00039865588769316673\n",
            "step: 150, loss: 0.00020083699200768024\n",
            "step: 160, loss: 0.0003131064586341381\n",
            "step: 170, loss: 0.000551500532310456\n",
            "step: 180, loss: 0.0003776292141992599\n",
            "step: 190, loss: 0.002708052285015583\n",
            "step: 200, loss: 0.00044406281085684896\n",
            "step: 210, loss: 0.0003240909136366099\n",
            "step: 220, loss: 0.00039580839802511036\n",
            "step: 230, loss: 0.00019320538558531553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9615384615384616, f1=0.9437428243398392, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023468512517865747\n",
            "step: 10, loss: 0.0006079564918763936\n",
            "step: 20, loss: 0.000907513196580112\n",
            "step: 30, loss: 6.208675767993554e-05\n",
            "step: 40, loss: 0.056735455989837646\n",
            "step: 50, loss: 7.380809256574139e-05\n",
            "step: 60, loss: 0.00032711762469261885\n",
            "step: 70, loss: 0.0009654348250478506\n",
            "step: 80, loss: 0.00016059599875006825\n",
            "step: 90, loss: 0.0034859429579228163\n",
            "step: 100, loss: 0.00025212904438376427\n",
            "step: 110, loss: 0.00018908927449956536\n",
            "step: 120, loss: 0.0004856935702264309\n",
            "step: 130, loss: 0.00025399221340194345\n",
            "step: 140, loss: 0.00023822631919756532\n",
            "step: 150, loss: 0.012500963173806667\n",
            "step: 160, loss: 0.0008735501323826611\n",
            "step: 170, loss: 0.008813789114356041\n",
            "step: 180, loss: 0.0021541116293519735\n",
            "step: 190, loss: 0.0008190224762074649\n",
            "step: 200, loss: 0.0009085024357773364\n",
            "step: 210, loss: 0.0009267203859053552\n",
            "step: 220, loss: 0.0006455217371694744\n",
            "step: 230, loss: 0.00804334506392479\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9578713968957872, f1=0.9454949944382648, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003699498192872852\n",
            "step: 10, loss: 0.0014034447958692908\n",
            "step: 20, loss: 0.0002205623168265447\n",
            "step: 30, loss: 0.00028324511367827654\n",
            "step: 40, loss: 0.00019363056344445795\n",
            "step: 50, loss: 0.02029695361852646\n",
            "step: 60, loss: 0.000189742335351184\n",
            "step: 70, loss: 0.0002371486771153286\n",
            "step: 80, loss: 0.002441721735522151\n",
            "step: 90, loss: 0.0057478975504636765\n",
            "step: 100, loss: 0.00020186665642540902\n",
            "step: 110, loss: 0.00021525412739720196\n",
            "step: 120, loss: 0.0002586777263786644\n",
            "step: 130, loss: 0.0004943434614688158\n",
            "step: 140, loss: 0.04362647980451584\n",
            "step: 150, loss: 0.016739021986722946\n",
            "step: 160, loss: 0.00014007158461026847\n",
            "step: 170, loss: 7.493374141631648e-05\n",
            "step: 180, loss: 0.00033846095902845263\n",
            "step: 190, loss: 0.0008006296120584011\n",
            "step: 200, loss: 0.0003533612471073866\n",
            "step: 210, loss: 4.122514292248525e-05\n",
            "step: 220, loss: 0.00015730602899566293\n",
            "step: 230, loss: 0.0001514967152616009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9611542730299667, f1=0.9513274336283186, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0062250131741166115\n",
            "step: 10, loss: 0.0006065224879421294\n",
            "step: 20, loss: 0.00017329270485788584\n",
            "step: 30, loss: 0.00792425312101841\n",
            "step: 40, loss: 0.00023545343719888479\n",
            "step: 50, loss: 0.00023073684133123606\n",
            "step: 60, loss: 0.0034478437155485153\n",
            "step: 70, loss: 0.0022673809435218573\n",
            "step: 80, loss: 9.845834574662149e-05\n",
            "step: 90, loss: 0.0001150895914179273\n",
            "step: 100, loss: 0.00039914369699545205\n",
            "step: 110, loss: 0.00031606011907570064\n",
            "step: 120, loss: 0.0006419881829060614\n",
            "step: 130, loss: 0.0012686800910159945\n",
            "step: 140, loss: 0.0005549454363062978\n",
            "step: 150, loss: 0.0017262604087591171\n",
            "step: 160, loss: 0.0001311220257775858\n",
            "step: 170, loss: 0.0006354896468110383\n",
            "step: 180, loss: 0.0003704876289702952\n",
            "step: 190, loss: 0.00043940701289102435\n",
            "step: 200, loss: 0.00024558737641200423\n",
            "step: 210, loss: 7.792279939167202e-05\n",
            "step: 220, loss: 0.00019943166989833117\n",
            "step: 230, loss: 0.0002039407117990777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9543937708565072, f1=0.9452513966480447, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00025177127099595964\n",
            "step: 10, loss: 7.98700493760407e-05\n",
            "step: 20, loss: 0.007781794294714928\n",
            "step: 30, loss: 0.006342113018035889\n",
            "step: 40, loss: 0.00023988046450540423\n",
            "step: 50, loss: 0.000277009850833565\n",
            "step: 60, loss: 0.00023004625109024346\n",
            "step: 70, loss: 0.00010934981401078403\n",
            "step: 80, loss: 0.00016258454706985503\n",
            "step: 90, loss: 0.00014099245890974998\n",
            "step: 100, loss: 6.824478623457253e-05\n",
            "step: 110, loss: 0.00010182779078604653\n",
            "step: 120, loss: 0.00015427192556671798\n",
            "step: 130, loss: 0.0002072320057777688\n",
            "step: 140, loss: 0.0001259842683793977\n",
            "step: 150, loss: 0.00010593779006740078\n",
            "step: 160, loss: 0.00017398083582520485\n",
            "step: 170, loss: 0.0001303239696426317\n",
            "step: 180, loss: 0.010336140170693398\n",
            "step: 190, loss: 9.249307186109945e-05\n",
            "step: 200, loss: 0.00014221682795323431\n",
            "step: 210, loss: 0.0002304171648574993\n",
            "step: 220, loss: 0.00018564797937870026\n",
            "step: 230, loss: 0.028775779530405998\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9567147613762487, f1=0.9519553072625698, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004086430184543133\n",
            "step: 10, loss: 0.0007963841198943555\n",
            "step: 20, loss: 0.0008389002177864313\n",
            "step: 30, loss: 0.00012562074698507786\n",
            "step: 40, loss: 0.0002347846020711586\n",
            "step: 50, loss: 0.00015952711692079902\n",
            "step: 60, loss: 0.0008259573951363564\n",
            "step: 70, loss: 7.212725904537365e-05\n",
            "step: 80, loss: 0.00010146988643100485\n",
            "step: 90, loss: 0.0001696472172625363\n",
            "step: 100, loss: 0.00010635569924488664\n",
            "step: 110, loss: 0.0035049046855419874\n",
            "step: 120, loss: 0.0002281602646689862\n",
            "step: 130, loss: 0.00015510397497564554\n",
            "step: 140, loss: 0.00016193880583159626\n",
            "step: 150, loss: 0.0010105184046551585\n",
            "step: 160, loss: 0.00034558004699647427\n",
            "step: 170, loss: 0.000133568377350457\n",
            "step: 180, loss: 9.025300096254796e-05\n",
            "step: 190, loss: 0.00017394727910868824\n",
            "step: 200, loss: 8.380268991459161e-05\n",
            "step: 210, loss: 4.033757795696147e-05\n",
            "step: 220, loss: 0.00021690120047423989\n",
            "step: 230, loss: 7.22959084669128e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9553571428571428, f1=0.946067415730337, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002517151879146695\n",
            "step: 10, loss: 0.0001310377847403288\n",
            "step: 20, loss: 0.0006152056157588959\n",
            "step: 30, loss: 0.00012680416693910956\n",
            "step: 40, loss: 0.0026849096175283194\n",
            "step: 50, loss: 9.884955215966329e-05\n",
            "step: 60, loss: 0.00011550471390364692\n",
            "step: 70, loss: 0.005377591587603092\n",
            "step: 80, loss: 0.0002115554379997775\n",
            "step: 90, loss: 0.0002508146280888468\n",
            "step: 100, loss: 7.46311925468035e-05\n",
            "step: 110, loss: 7.038266630843282e-05\n",
            "step: 120, loss: 4.307348353904672e-05\n",
            "step: 130, loss: 0.001676828251220286\n",
            "step: 140, loss: 8.454519411316141e-05\n",
            "step: 150, loss: 5.854406117578037e-05\n",
            "step: 160, loss: 0.0016529355198144913\n",
            "step: 170, loss: 0.00017345805827062577\n",
            "step: 180, loss: 0.00019177993817720562\n",
            "step: 190, loss: 7.589710730826482e-05\n",
            "step: 200, loss: 5.106938260723837e-05\n",
            "step: 210, loss: 6.937274883966893e-05\n",
            "step: 220, loss: 6.615062739001587e-05\n",
            "step: 230, loss: 0.00490990374237299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9599109131403119, f1=0.9499443826473859, best_f1=0.9437428243398392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.428950731176883e-05\n",
            "step: 10, loss: 6.059036240912974e-05\n",
            "step: 20, loss: 0.0003290711611043662\n",
            "step: 30, loss: 0.00010443662176840007\n",
            "step: 40, loss: 0.00019685234292410314\n",
            "step: 50, loss: 0.04553942382335663\n",
            "step: 60, loss: 0.00023243596660904586\n",
            "step: 70, loss: 0.0010984324617311358\n",
            "step: 80, loss: 8.461552351946011e-05\n",
            "step: 90, loss: 8.522660937160254e-05\n",
            "step: 100, loss: 6.954609125386924e-05\n",
            "step: 110, loss: 9.543069609208032e-05\n",
            "step: 120, loss: 9.890382352750748e-05\n",
            "step: 130, loss: 7.656248635612428e-05\n",
            "step: 140, loss: 4.846606680075638e-05\n",
            "step: 150, loss: 0.0002769990242086351\n",
            "step: 160, loss: 4.3631080188788474e-05\n",
            "step: 170, loss: 7.443955837516114e-05\n",
            "step: 180, loss: 4.1917654016288e-05\n",
            "step: 190, loss: 0.00031894363928586245\n",
            "step: 200, loss: 0.0003086219367105514\n",
            "step: 210, loss: 0.0004757953865919262\n",
            "step: 220, loss: 0.00012162249186076224\n",
            "step: 230, loss: 7.032047869870439e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9588431590656283, f1=0.9500554938956715, best_f1=0.9437428243398392\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 192.12it/s]\n",
            "load_f1 = 0.9620535714285715\n",
            "real_f1 = 0.9577777777777777\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 235.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5391d0-aed8-47be-99bb-f33cdacafa81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6345385313034058\n",
            "step: 10, loss: 0.5159788727760315\n",
            "step: 20, loss: 0.5931004285812378\n",
            "step: 30, loss: 0.3431259095668793\n",
            "step: 40, loss: 0.3452569842338562\n",
            "step: 50, loss: 0.2996799647808075\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.22765102982521057\n",
            "step: 70, loss: 0.23037375509738922\n",
            "step: 80, loss: 0.1413046270608902\n",
            "step: 90, loss: 0.31314384937286377\n",
            "step: 100, loss: 0.22369693219661713\n",
            "step: 110, loss: 0.09800390154123306\n",
            "step: 120, loss: 0.13428767025470734\n",
            "step: 130, loss: 0.1631496399641037\n",
            "step: 140, loss: 0.0863686129450798\n",
            "step: 150, loss: 0.05076254904270172\n",
            "step: 160, loss: 0.0705532357096672\n",
            "step: 170, loss: 0.2859067916870117\n",
            "step: 180, loss: 0.05679988116025925\n",
            "step: 190, loss: 0.019781971350312233\n",
            "step: 200, loss: 0.17236925661563873\n",
            "step: 210, loss: 0.0519067719578743\n",
            "step: 220, loss: 0.20721620321273804\n",
            "step: 230, loss: 0.13942179083824158\n",
            "step: 240, loss: 0.09976700693368912\n",
            "step: 250, loss: 0.09312956780195236\n",
            "step: 260, loss: 0.062252700328826904\n",
            "step: 270, loss: 0.04090512543916702\n",
            "step: 280, loss: 0.18595346808433533\n",
            "step: 290, loss: 0.30160561203956604\n",
            "step: 300, loss: 0.10853982716798782\n",
            "step: 310, loss: 0.18380101025104523\n",
            "step: 320, loss: 0.07394029945135117\n",
            "step: 330, loss: 0.04933370277285576\n",
            "step: 340, loss: 0.17994646728038788\n",
            "step: 350, loss: 0.06874911487102509\n",
            "step: 360, loss: 0.09396405518054962\n",
            "step: 370, loss: 0.10824308544397354\n",
            "step: 380, loss: 0.0333951972424984\n",
            "step: 390, loss: 0.14608217775821686\n",
            "step: 400, loss: 0.2794446349143982\n",
            "step: 410, loss: 0.0994172990322113\n",
            "step: 420, loss: 0.16915026307106018\n",
            "step: 430, loss: 0.20114783942699432\n",
            "step: 440, loss: 0.09981073439121246\n",
            "step: 450, loss: 0.00991132203489542\n",
            "step: 460, loss: 0.08980310708284378\n",
            "step: 470, loss: 0.11010853201150894\n",
            "step: 480, loss: 0.14469105005264282\n",
            "step: 490, loss: 0.2282792627811432\n",
            "step: 500, loss: 0.09226910024881363\n",
            "step: 510, loss: 0.14934997260570526\n",
            "step: 520, loss: 0.2697022557258606\n",
            "step: 530, loss: 0.02729497291147709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9067641681901281, f1=0.8931860036832413, best_f1=0.8931860036832413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1297970712184906\n",
            "step: 10, loss: 0.12176104635000229\n",
            "step: 20, loss: 0.10667318105697632\n",
            "step: 30, loss: 0.1161021739244461\n",
            "step: 40, loss: 0.04303897172212601\n",
            "step: 50, loss: 0.07592345774173737\n",
            "step: 60, loss: 0.049467604607343674\n",
            "step: 70, loss: 0.05676442012190819\n",
            "step: 80, loss: 0.06961308419704437\n",
            "step: 90, loss: 0.05212937667965889\n",
            "step: 100, loss: 0.05721186846494675\n",
            "step: 110, loss: 0.024939224123954773\n",
            "step: 120, loss: 0.05967044457793236\n",
            "step: 130, loss: 0.10777202248573303\n",
            "step: 140, loss: 0.10405401885509491\n",
            "step: 150, loss: 0.05712760239839554\n",
            "step: 160, loss: 0.03692019730806351\n",
            "step: 170, loss: 0.09184528887271881\n",
            "step: 180, loss: 0.010978949256241322\n",
            "step: 190, loss: 0.05827793478965759\n",
            "step: 200, loss: 0.024434037506580353\n",
            "step: 210, loss: 0.09479187428951263\n",
            "step: 220, loss: 0.04099050164222717\n",
            "step: 230, loss: 0.04752489551901817\n",
            "step: 240, loss: 0.028789175674319267\n",
            "step: 250, loss: 0.22742146253585815\n",
            "step: 260, loss: 0.024253079667687416\n",
            "step: 270, loss: 0.24112984538078308\n",
            "step: 280, loss: 0.1962643712759018\n",
            "step: 290, loss: 0.028782367706298828\n",
            "step: 300, loss: 0.0925590842962265\n",
            "step: 310, loss: 0.03146238252520561\n",
            "step: 320, loss: 0.1843540519475937\n",
            "step: 330, loss: 0.19272997975349426\n",
            "step: 340, loss: 0.07221384346485138\n",
            "step: 350, loss: 0.03916364535689354\n",
            "step: 360, loss: 0.09560562670230865\n",
            "step: 370, loss: 0.19742199778556824\n",
            "step: 380, loss: 0.11260980367660522\n",
            "step: 390, loss: 0.19608673453330994\n",
            "step: 400, loss: 0.02135448344051838\n",
            "step: 410, loss: 0.09618993103504181\n",
            "step: 420, loss: 0.026998300105333328\n",
            "step: 430, loss: 0.22829782962799072\n",
            "step: 440, loss: 0.12894001603126526\n",
            "step: 450, loss: 0.05193041265010834\n",
            "step: 460, loss: 0.09953662008047104\n",
            "step: 470, loss: 0.03979341685771942\n",
            "step: 480, loss: 0.15147246420383453\n",
            "step: 490, loss: 0.03275294229388237\n",
            "step: 500, loss: 0.5400466322898865\n",
            "step: 510, loss: 0.0367853119969368\n",
            "step: 520, loss: 0.12258786708116531\n",
            "step: 530, loss: 0.09555791318416595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.908235294117647, f1=0.9002364066193854, best_f1=0.9002364066193854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0767800360918045\n",
            "step: 10, loss: 0.09259124845266342\n",
            "step: 20, loss: 0.0495271235704422\n",
            "step: 30, loss: 0.26746612787246704\n",
            "step: 40, loss: 0.0230867899954319\n",
            "step: 50, loss: 0.16216401755809784\n",
            "step: 60, loss: 0.05473768338561058\n",
            "step: 70, loss: 0.03925517946481705\n",
            "step: 80, loss: 0.006715036928653717\n",
            "step: 90, loss: 0.04146776348352432\n",
            "step: 100, loss: 0.02179226279258728\n",
            "step: 110, loss: 0.002379094250500202\n",
            "step: 120, loss: 0.01759437844157219\n",
            "step: 130, loss: 0.004072373267263174\n",
            "step: 140, loss: 0.017164321616292\n",
            "step: 150, loss: 0.07640637457370758\n",
            "step: 160, loss: 0.009216570295393467\n",
            "step: 170, loss: 0.09696120768785477\n",
            "step: 180, loss: 0.06885220110416412\n",
            "step: 190, loss: 0.04847704991698265\n",
            "step: 200, loss: 0.0153276976197958\n",
            "step: 210, loss: 0.028813179582357407\n",
            "step: 220, loss: 0.06390254199504852\n",
            "step: 230, loss: 0.28438472747802734\n",
            "step: 240, loss: 0.0031115247402340174\n",
            "step: 250, loss: 0.03987603634595871\n",
            "step: 260, loss: 0.0205225870013237\n",
            "step: 270, loss: 0.022196004167199135\n",
            "step: 280, loss: 0.2078208327293396\n",
            "step: 290, loss: 0.007804475259035826\n",
            "step: 300, loss: 0.12049132585525513\n",
            "step: 310, loss: 0.035007674247026443\n",
            "step: 320, loss: 0.042947255074977875\n",
            "step: 330, loss: 0.015897002071142197\n",
            "step: 340, loss: 0.010643357411026955\n",
            "step: 350, loss: 0.019028298556804657\n",
            "step: 360, loss: 0.03255254775285721\n",
            "step: 370, loss: 0.013550067320466042\n",
            "step: 380, loss: 0.005147553980350494\n",
            "step: 390, loss: 0.024470722302794456\n",
            "step: 400, loss: 0.1590900868177414\n",
            "step: 410, loss: 0.015072374604642391\n",
            "step: 420, loss: 0.2630383372306824\n",
            "step: 430, loss: 0.010259999893605709\n",
            "step: 440, loss: 0.07859810441732407\n",
            "step: 450, loss: 0.12051332741975784\n",
            "step: 460, loss: 0.08320201188325882\n",
            "step: 470, loss: 0.034559376537799835\n",
            "step: 480, loss: 0.02945765294134617\n",
            "step: 490, loss: 0.022369258105754852\n",
            "step: 500, loss: 0.12409017980098724\n",
            "step: 510, loss: 0.014923984184861183\n",
            "step: 520, loss: 0.05362401902675629\n",
            "step: 530, loss: 0.0747075006365776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.912082745651152, f1=0.8944099378881988, best_f1=0.8944099378881988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008499042131006718\n",
            "step: 10, loss: 0.008325438015162945\n",
            "step: 20, loss: 0.017380500212311745\n",
            "step: 30, loss: 0.009126865305006504\n",
            "step: 40, loss: 0.04010215401649475\n",
            "step: 50, loss: 0.03553945943713188\n",
            "step: 60, loss: 0.006586793344467878\n",
            "step: 70, loss: 0.004099797457456589\n",
            "step: 80, loss: 0.003751944750547409\n",
            "step: 90, loss: 0.1672637015581131\n",
            "step: 100, loss: 0.00903908722102642\n",
            "step: 110, loss: 0.15919935703277588\n",
            "step: 120, loss: 0.0019498032052069902\n",
            "step: 130, loss: 0.002475582994520664\n",
            "step: 140, loss: 0.004589851479977369\n",
            "step: 150, loss: 0.018469564616680145\n",
            "step: 160, loss: 0.06064090505242348\n",
            "step: 170, loss: 0.004836102947592735\n",
            "step: 180, loss: 0.0012690932489931583\n",
            "step: 190, loss: 0.03329590708017349\n",
            "step: 200, loss: 0.009394606575369835\n",
            "step: 210, loss: 0.046146899461746216\n",
            "step: 220, loss: 0.0071780625730752945\n",
            "step: 230, loss: 0.01597127877175808\n",
            "step: 240, loss: 0.004680955782532692\n",
            "step: 250, loss: 0.05700678750872612\n",
            "step: 260, loss: 0.0022197160869836807\n",
            "step: 270, loss: 0.04535209387540817\n",
            "step: 280, loss: 0.21195867657661438\n",
            "step: 290, loss: 0.025626741349697113\n",
            "step: 300, loss: 0.004644455853849649\n",
            "step: 310, loss: 0.004963350016623735\n",
            "step: 320, loss: 0.0108055854216218\n",
            "step: 330, loss: 0.021290190517902374\n",
            "step: 340, loss: 0.05742461606860161\n",
            "step: 350, loss: 0.12710030376911163\n",
            "step: 360, loss: 0.0900988057255745\n",
            "step: 370, loss: 0.005927878897637129\n",
            "step: 380, loss: 0.010796040296554565\n",
            "step: 390, loss: 0.005154358223080635\n",
            "step: 400, loss: 0.02862093411386013\n",
            "step: 410, loss: 0.0008702185587026179\n",
            "step: 420, loss: 0.08580722659826279\n",
            "step: 430, loss: 0.06089651584625244\n",
            "step: 440, loss: 0.0022630123421549797\n",
            "step: 450, loss: 0.008185058832168579\n",
            "step: 460, loss: 0.008338574320077896\n",
            "step: 470, loss: 0.046225808560848236\n",
            "step: 480, loss: 0.04310920089483261\n",
            "step: 490, loss: 0.010161244310438633\n",
            "step: 500, loss: 0.0024882443249225616\n",
            "step: 510, loss: 0.028400063514709473\n",
            "step: 520, loss: 0.03319086134433746\n",
            "step: 530, loss: 0.041965920478105545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9072261072261073, f1=0.8899430740037951, best_f1=0.8944099378881988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03445682302117348\n",
            "step: 10, loss: 0.01999519020318985\n",
            "step: 20, loss: 0.003906242549419403\n",
            "step: 30, loss: 0.004001666326075792\n",
            "step: 40, loss: 0.04595369100570679\n",
            "step: 50, loss: 0.024317516013979912\n",
            "step: 60, loss: 0.09414973855018616\n",
            "step: 70, loss: 0.024932237342000008\n",
            "step: 80, loss: 0.008136275224387646\n",
            "step: 90, loss: 0.02557588554918766\n",
            "step: 100, loss: 0.0347147136926651\n",
            "step: 110, loss: 0.0016749163623899221\n",
            "step: 120, loss: 0.05267760157585144\n",
            "step: 130, loss: 0.0020467801950871944\n",
            "step: 140, loss: 0.003363917348906398\n",
            "step: 150, loss: 0.10932005196809769\n",
            "step: 160, loss: 0.0008929428295232356\n",
            "step: 170, loss: 0.031720083206892014\n",
            "step: 180, loss: 0.0009285951964557171\n",
            "step: 190, loss: 0.00449293153360486\n",
            "step: 200, loss: 0.017756709828972816\n",
            "step: 210, loss: 0.06895583868026733\n",
            "step: 220, loss: 0.0031013451516628265\n",
            "step: 230, loss: 0.02513963170349598\n",
            "step: 240, loss: 0.043276891112327576\n",
            "step: 250, loss: 0.04618023708462715\n",
            "step: 260, loss: 0.0025880122557282448\n",
            "step: 270, loss: 0.010830337181687355\n",
            "step: 280, loss: 0.007806032430380583\n",
            "step: 290, loss: 0.208930104970932\n",
            "step: 300, loss: 0.03937363997101784\n",
            "step: 310, loss: 0.00972413457930088\n",
            "step: 320, loss: 0.07521802932024002\n",
            "step: 330, loss: 0.01639087311923504\n",
            "step: 340, loss: 0.0031206447165459394\n",
            "step: 350, loss: 0.0013627088628709316\n",
            "step: 360, loss: 0.0039001612458378077\n",
            "step: 370, loss: 0.0029813102446496487\n",
            "step: 380, loss: 0.004460127558559179\n",
            "step: 390, loss: 0.00213326932862401\n",
            "step: 400, loss: 0.0011027674190700054\n",
            "step: 410, loss: 0.007287752814590931\n",
            "step: 420, loss: 0.0006139019387774169\n",
            "step: 430, loss: 0.1486971229314804\n",
            "step: 440, loss: 0.179969921708107\n",
            "step: 450, loss: 0.011231125332415104\n",
            "step: 460, loss: 0.004497077316045761\n",
            "step: 470, loss: 0.03633009269833565\n",
            "step: 480, loss: 0.016462404280900955\n",
            "step: 490, loss: 0.003463469911366701\n",
            "step: 500, loss: 0.04566683992743492\n",
            "step: 510, loss: 0.008569885976612568\n",
            "step: 520, loss: 0.00778389535844326\n",
            "step: 530, loss: 0.0008788554114289582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9111111111111111, f1=0.8932400932400931, best_f1=0.8944099378881988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004449534695595503\n",
            "step: 10, loss: 0.0004503014497458935\n",
            "step: 20, loss: 0.13122524321079254\n",
            "step: 30, loss: 0.002606580499559641\n",
            "step: 40, loss: 0.006317175459116697\n",
            "step: 50, loss: 0.010005541145801544\n",
            "step: 60, loss: 0.0012756913201883435\n",
            "step: 70, loss: 0.0009119705064222217\n",
            "step: 80, loss: 0.014675899408757687\n",
            "step: 90, loss: 0.0003623013326432556\n",
            "step: 100, loss: 0.14486517012119293\n",
            "step: 110, loss: 0.027819698676466942\n",
            "step: 120, loss: 0.005792984273284674\n",
            "step: 130, loss: 0.004270556848496199\n",
            "step: 140, loss: 0.00865299440920353\n",
            "step: 150, loss: 0.0012288432335481048\n",
            "step: 160, loss: 0.0019091577269136906\n",
            "step: 170, loss: 0.001449735020287335\n",
            "step: 180, loss: 0.0006759064854122698\n",
            "step: 190, loss: 0.013389325700700283\n",
            "step: 200, loss: 0.010476130992174149\n",
            "step: 210, loss: 0.0023250060621649027\n",
            "step: 220, loss: 0.0010475533781573176\n",
            "step: 230, loss: 0.0016632708720862865\n",
            "step: 240, loss: 0.04200168326497078\n",
            "step: 250, loss: 0.004966967273503542\n",
            "step: 260, loss: 0.0003428615164011717\n",
            "step: 270, loss: 0.0013601050013676286\n",
            "step: 280, loss: 0.0009884071769192815\n",
            "step: 290, loss: 0.00039075847598724067\n",
            "step: 300, loss: 0.0004962278180755675\n",
            "step: 310, loss: 0.0010300506837666035\n",
            "step: 320, loss: 0.0015178339090198278\n",
            "step: 330, loss: 0.0014609888894483447\n",
            "step: 340, loss: 0.0443820022046566\n",
            "step: 350, loss: 0.0028169522993266582\n",
            "step: 360, loss: 0.004646338988095522\n",
            "step: 370, loss: 0.0031775704119354486\n",
            "step: 380, loss: 0.000339863559929654\n",
            "step: 390, loss: 0.004031868651509285\n",
            "step: 400, loss: 0.14433667063713074\n",
            "step: 410, loss: 0.0880168229341507\n",
            "step: 420, loss: 0.02476143091917038\n",
            "step: 430, loss: 0.0013658857205882668\n",
            "step: 440, loss: 0.0006460821023210883\n",
            "step: 450, loss: 0.0005717584281228483\n",
            "step: 460, loss: 0.0011738475877791643\n",
            "step: 470, loss: 0.0005673015839420259\n",
            "step: 480, loss: 0.0011858321959152818\n",
            "step: 490, loss: 0.007445620372891426\n",
            "step: 500, loss: 0.0023434069007635117\n",
            "step: 510, loss: 0.02280420809984207\n",
            "step: 520, loss: 0.0011516499798744917\n",
            "step: 530, loss: 0.002705085091292858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9083969465648856, f1=0.8818137964302943, best_f1=0.8944099378881988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00025044314679689705\n",
            "step: 10, loss: 0.0004061848158016801\n",
            "step: 20, loss: 0.001335824839770794\n",
            "step: 30, loss: 0.0008527265163138509\n",
            "step: 40, loss: 0.00617134477943182\n",
            "step: 50, loss: 0.0050085559487342834\n",
            "step: 60, loss: 0.0007466660463251173\n",
            "step: 70, loss: 0.00033340559457428753\n",
            "step: 80, loss: 0.005214403383433819\n",
            "step: 90, loss: 0.0005238659796305001\n",
            "step: 100, loss: 0.00040051789255812764\n",
            "step: 110, loss: 0.006995262578129768\n",
            "step: 120, loss: 0.002682599239051342\n",
            "step: 130, loss: 0.08341220766305923\n",
            "step: 140, loss: 6.665248656645417e-05\n",
            "step: 150, loss: 0.0011732700513675809\n",
            "step: 160, loss: 0.0001616103108972311\n",
            "step: 170, loss: 0.0015709124272689223\n",
            "step: 180, loss: 0.0002570578071754426\n",
            "step: 190, loss: 0.009640974923968315\n",
            "step: 200, loss: 0.00040595553582534194\n",
            "step: 210, loss: 0.014825894497334957\n",
            "step: 220, loss: 0.00165260280482471\n",
            "step: 230, loss: 0.014640148729085922\n",
            "step: 240, loss: 0.0020671836100518703\n",
            "step: 250, loss: 0.08286258578300476\n",
            "step: 260, loss: 0.0002570589422248304\n",
            "step: 270, loss: 0.0026491787284612656\n",
            "step: 280, loss: 0.0006049229414202273\n",
            "step: 290, loss: 0.0009963074699044228\n",
            "step: 300, loss: 0.2320452481508255\n",
            "step: 310, loss: 0.0025579421781003475\n",
            "step: 320, loss: 0.000618698017206043\n",
            "step: 330, loss: 0.005303935147821903\n",
            "step: 340, loss: 0.005940405651926994\n",
            "step: 350, loss: 0.000491646584123373\n",
            "step: 360, loss: 0.008164425380527973\n",
            "step: 370, loss: 0.0006243972457014024\n",
            "step: 380, loss: 0.0002820958034135401\n",
            "step: 390, loss: 0.00021777309302706271\n",
            "step: 400, loss: 0.00036427087616175413\n",
            "step: 410, loss: 0.003521945793181658\n",
            "step: 420, loss: 0.003456655889749527\n",
            "step: 430, loss: 0.006967911496758461\n",
            "step: 440, loss: 0.0007361914031207561\n",
            "step: 450, loss: 0.019948331639170647\n",
            "step: 460, loss: 0.006426415406167507\n",
            "step: 470, loss: 0.00027242579380981624\n",
            "step: 480, loss: 0.05668797716498375\n",
            "step: 490, loss: 0.012163490056991577\n",
            "step: 500, loss: 0.0008651715470477939\n",
            "step: 510, loss: 0.0012223549420014024\n",
            "step: 520, loss: 0.06211160123348236\n",
            "step: 530, loss: 0.0012928241631016135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.913144449605202, f1=0.9038912330051571, best_f1=0.9038912330051571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002349399495869875\n",
            "step: 10, loss: 0.0023497575893998146\n",
            "step: 20, loss: 0.00016778033750597388\n",
            "step: 30, loss: 0.014491303823888302\n",
            "step: 40, loss: 0.004725146573036909\n",
            "step: 50, loss: 0.024494972079992294\n",
            "step: 60, loss: 0.00018562246987130493\n",
            "step: 70, loss: 0.00013842048065271229\n",
            "step: 80, loss: 0.008216220885515213\n",
            "step: 90, loss: 0.00029043335234746337\n",
            "step: 100, loss: 0.00013454785221256316\n",
            "step: 110, loss: 0.16483156383037567\n",
            "step: 120, loss: 0.0006506711943075061\n",
            "step: 130, loss: 0.00016769388457760215\n",
            "step: 140, loss: 0.000950803398154676\n",
            "step: 150, loss: 0.00024873760412447155\n",
            "step: 160, loss: 0.0022500744089484215\n",
            "step: 170, loss: 0.0011380473151803017\n",
            "step: 180, loss: 8.360248466487974e-05\n",
            "step: 190, loss: 0.002226270502433181\n",
            "step: 200, loss: 0.0014610171783715487\n",
            "step: 210, loss: 0.00029705456108786166\n",
            "step: 220, loss: 0.0014187708729878068\n",
            "step: 230, loss: 0.0003562140045687556\n",
            "step: 240, loss: 0.0008180763688869774\n",
            "step: 250, loss: 0.005781116429716349\n",
            "step: 260, loss: 0.00038683952880091965\n",
            "step: 270, loss: 0.005035795271396637\n",
            "step: 280, loss: 0.027199214324355125\n",
            "step: 290, loss: 0.03314600884914398\n",
            "step: 300, loss: 0.0031112851575016975\n",
            "step: 310, loss: 0.00024271439178846776\n",
            "step: 320, loss: 0.1319313943386078\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 330, loss: 0.0004132980538997799\n",
            "step: 340, loss: 0.0009593864087946713\n",
            "step: 350, loss: 0.0002708043612074107\n",
            "step: 360, loss: 0.0033416152000427246\n",
            "step: 370, loss: 0.0012456387048587203\n",
            "step: 380, loss: 0.005161578301340342\n",
            "step: 390, loss: 0.017933432012796402\n",
            "step: 400, loss: 0.0005495662335306406\n",
            "step: 410, loss: 0.00045497005339711905\n",
            "step: 420, loss: 0.002170708728954196\n",
            "step: 430, loss: 0.02802407369017601\n",
            "step: 440, loss: 0.0022748075425624847\n",
            "step: 450, loss: 0.006901554763317108\n",
            "step: 460, loss: 0.00014681287575513124\n",
            "step: 470, loss: 0.0007365681231021881\n",
            "step: 480, loss: 0.0002736441674642265\n",
            "step: 490, loss: 0.0011341498466208577\n",
            "step: 500, loss: 0.06769923120737076\n",
            "step: 510, loss: 0.07875583320856094\n",
            "step: 520, loss: 0.003987311851233244\n",
            "step: 530, loss: 0.00015358885866589844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9187411930483794, f1=0.889947594092425, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015389852924272418\n",
            "step: 10, loss: 9.928735380526632e-05\n",
            "step: 20, loss: 0.0006311328615993261\n",
            "step: 30, loss: 0.0034648380242288113\n",
            "step: 40, loss: 0.00025116270990110934\n",
            "step: 50, loss: 0.00015247953706420958\n",
            "step: 60, loss: 0.00017920020036399364\n",
            "step: 70, loss: 4.028594048577361e-05\n",
            "step: 80, loss: 0.00021500600269064307\n",
            "step: 90, loss: 0.0013528912095353007\n",
            "step: 100, loss: 6.209599087014794e-05\n",
            "step: 110, loss: 4.9695772759150714e-05\n",
            "step: 120, loss: 0.008853926323354244\n",
            "step: 130, loss: 0.007361988537013531\n",
            "step: 140, loss: 0.00017574429512023926\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 150, loss: 0.0002517862303648144\n",
            "step: 160, loss: 0.00011120025010313839\n",
            "step: 170, loss: 0.002388553461059928\n",
            "step: 180, loss: 0.00021892438235227019\n",
            "step: 190, loss: 0.002936912002041936\n",
            "step: 200, loss: 0.013268006965517998\n",
            "step: 210, loss: 0.009460082277655602\n",
            "step: 220, loss: 0.0004288560012355447\n",
            "step: 230, loss: 0.00744528928771615\n",
            "step: 240, loss: 0.00011219615407753736\n",
            "step: 250, loss: 0.0004921869840472937\n",
            "step: 260, loss: 0.0008639739826321602\n",
            "step: 270, loss: 0.0001238436670973897\n",
            "step: 280, loss: 0.0028611000161617994\n",
            "step: 290, loss: 0.009457272477447987\n",
            "step: 300, loss: 0.00014398052007891238\n",
            "step: 310, loss: 0.00032785063376650214\n",
            "step: 320, loss: 0.0002851183235179633\n",
            "step: 330, loss: 0.00039976471452973783\n",
            "step: 340, loss: 0.0043961466290056705\n",
            "step: 350, loss: 0.07424214482307434\n",
            "step: 360, loss: 0.00017513500642962754\n",
            "step: 370, loss: 0.00034524311195127666\n",
            "step: 380, loss: 0.00020145170856267214\n",
            "step: 390, loss: 0.003722775261849165\n",
            "step: 400, loss: 0.0014471783069893718\n",
            "step: 410, loss: 0.00016248297470156103\n",
            "step: 420, loss: 9.161894558928907e-05\n",
            "step: 430, loss: 8.408936992054805e-05\n",
            "step: 440, loss: 0.00015127783990465105\n",
            "step: 450, loss: 0.002677884651347995\n",
            "step: 460, loss: 0.0035219318233430386\n",
            "step: 470, loss: 0.23010286688804626\n",
            "step: 480, loss: 0.00015136078582145274\n",
            "step: 490, loss: 0.0001337494031758979\n",
            "step: 500, loss: 0.004487516358494759\n",
            "step: 510, loss: 0.029645023867487907\n",
            "step: 520, loss: 0.0006073230761103332\n",
            "step: 530, loss: 0.00010697665129555389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9113805970149255, f1=0.8901408450704226, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021461817959789187\n",
            "step: 10, loss: 4.9917580327019095e-05\n",
            "step: 20, loss: 0.00012318379594944417\n",
            "step: 30, loss: 0.004075329750776291\n",
            "step: 40, loss: 0.00019366535707376897\n",
            "step: 50, loss: 0.00015605389489792287\n",
            "step: 60, loss: 9.962977492250502e-05\n",
            "step: 70, loss: 7.408553938148543e-05\n",
            "step: 80, loss: 0.00013824008055962622\n",
            "step: 90, loss: 0.001672322046943009\n",
            "step: 100, loss: 8.287890523206443e-05\n",
            "step: 110, loss: 5.671848703059368e-05\n",
            "step: 120, loss: 0.0012600445188581944\n",
            "step: 130, loss: 0.00018152942357119173\n",
            "step: 140, loss: 0.00013959530042484403\n",
            "step: 150, loss: 0.0001847779203671962\n",
            "step: 160, loss: 8.261114271590486e-05\n",
            "step: 170, loss: 0.00039136307896114886\n",
            "step: 180, loss: 0.00022743834415450692\n",
            "step: 190, loss: 0.0005675561842508614\n",
            "step: 200, loss: 0.0007492689765058458\n",
            "step: 210, loss: 0.0008244640193879604\n",
            "step: 220, loss: 0.04041646048426628\n",
            "step: 230, loss: 0.04373852163553238\n",
            "step: 240, loss: 0.0003853653615806252\n",
            "step: 250, loss: 0.00020224342006258667\n",
            "step: 260, loss: 0.0011891323374584317\n",
            "step: 270, loss: 0.00029534738860093057\n",
            "step: 280, loss: 0.001407568110153079\n",
            "step: 290, loss: 0.00012113218690501526\n",
            "step: 300, loss: 0.0005571968504227698\n",
            "step: 310, loss: 0.00012381187116261572\n",
            "step: 320, loss: 0.0007892398862168193\n",
            "step: 330, loss: 0.0003750785253942013\n",
            "step: 340, loss: 0.0012197927571833134\n",
            "step: 350, loss: 0.006476098205894232\n",
            "step: 360, loss: 0.00012498689466156065\n",
            "step: 370, loss: 0.0010144597617909312\n",
            "step: 380, loss: 0.025364622473716736\n",
            "step: 390, loss: 0.009085563011467457\n",
            "step: 400, loss: 0.0004309635842218995\n",
            "step: 410, loss: 0.008837143890559673\n",
            "step: 420, loss: 0.005555488634854555\n",
            "step: 430, loss: 0.0003876538830809295\n",
            "step: 440, loss: 0.007549551781266928\n",
            "step: 450, loss: 0.0003563159261830151\n",
            "step: 460, loss: 0.00015309915761463344\n",
            "step: 470, loss: 0.00015895810793153942\n",
            "step: 480, loss: 0.0027588484808802605\n",
            "step: 490, loss: 0.0005773674347437918\n",
            "step: 500, loss: 0.07050382345914841\n",
            "step: 510, loss: 0.0001698032720014453\n",
            "step: 520, loss: 0.0001550184824736789\n",
            "step: 530, loss: 0.0002936673117801547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.913556920170052, f1=0.89058039961941, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02111021988093853\n",
            "step: 10, loss: 0.0011768217664211988\n",
            "step: 20, loss: 0.0001976555649889633\n",
            "step: 30, loss: 6.44249957986176e-05\n",
            "step: 40, loss: 0.00011734857980627567\n",
            "step: 50, loss: 0.00014975109661463648\n",
            "step: 60, loss: 0.0008422690443694592\n",
            "step: 70, loss: 8.032030746107921e-05\n",
            "step: 80, loss: 0.0010406498331576586\n",
            "step: 90, loss: 0.0004944023094139993\n",
            "step: 100, loss: 0.0010081232758238912\n",
            "step: 110, loss: 0.00011938439274672419\n",
            "step: 120, loss: 0.0003095013089478016\n",
            "step: 130, loss: 6.270306039368734e-05\n",
            "step: 140, loss: 7.885642116889358e-05\n",
            "step: 150, loss: 8.540981798432767e-05\n",
            "step: 160, loss: 0.00011241518222959712\n",
            "step: 170, loss: 4.992314279661514e-05\n",
            "step: 180, loss: 0.0006235938635654747\n",
            "step: 190, loss: 0.0005781258805654943\n",
            "step: 200, loss: 0.003053235588595271\n",
            "step: 210, loss: 0.0028142656665295362\n",
            "step: 220, loss: 4.143767000641674e-05\n",
            "step: 230, loss: 2.5730125344125554e-05\n",
            "step: 240, loss: 0.00012899631110485643\n",
            "step: 250, loss: 0.00036957176052965224\n",
            "step: 260, loss: 8.68267088662833e-05\n",
            "step: 270, loss: 0.00015749363228678703\n",
            "step: 280, loss: 0.0003287859435658902\n",
            "step: 290, loss: 0.0001972472236957401\n",
            "step: 300, loss: 0.00014471652684733272\n",
            "step: 310, loss: 0.00010136430501006544\n",
            "step: 320, loss: 0.00010350941010983661\n",
            "step: 330, loss: 0.0009598037577234209\n",
            "step: 340, loss: 7.830112735973671e-05\n",
            "step: 350, loss: 7.39971874281764e-05\n",
            "step: 360, loss: 6.011468212818727e-05\n",
            "step: 370, loss: 0.0003026325721293688\n",
            "step: 380, loss: 4.720401193480939e-05\n",
            "step: 390, loss: 3.690996891236864e-05\n",
            "step: 400, loss: 2.7275866159470752e-05\n",
            "step: 410, loss: 0.00011902152618858963\n",
            "step: 420, loss: 0.00012295150372665375\n",
            "step: 430, loss: 9.190943819703534e-05\n",
            "step: 440, loss: 0.00012096231512259692\n",
            "step: 450, loss: 0.00021136719442438334\n",
            "step: 460, loss: 0.001286514918319881\n",
            "step: 470, loss: 3.9702463254798204e-05\n",
            "step: 480, loss: 0.00013937879703007638\n",
            "step: 490, loss: 5.70999582123477e-05\n",
            "step: 500, loss: 0.0011171390069648623\n",
            "step: 510, loss: 5.084829172119498e-05\n",
            "step: 520, loss: 0.00014991391799412668\n",
            "step: 530, loss: 6.31698640063405e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9135460009246417, f1=0.8953703703703704, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003404116432648152\n",
            "step: 10, loss: 4.9549664254300296e-05\n",
            "step: 20, loss: 0.00023950946342665702\n",
            "step: 30, loss: 0.00014367714175023139\n",
            "step: 40, loss: 0.00010599356755847111\n",
            "step: 50, loss: 0.001389363664202392\n",
            "step: 60, loss: 0.00020533811766654253\n",
            "step: 70, loss: 8.835098560666665e-05\n",
            "step: 80, loss: 0.00021774330525659025\n",
            "step: 90, loss: 8.097830141196027e-05\n",
            "step: 100, loss: 0.00011417666974011809\n",
            "step: 110, loss: 7.953298336360604e-05\n",
            "step: 120, loss: 2.9235079637146555e-05\n",
            "step: 130, loss: 3.0170356694725342e-05\n",
            "step: 140, loss: 6.5217463998124e-05\n",
            "step: 150, loss: 0.0001035171007970348\n",
            "step: 160, loss: 4.005975279142149e-05\n",
            "step: 170, loss: 0.00038689534994773567\n",
            "step: 180, loss: 5.4372438171412796e-05\n",
            "step: 190, loss: 0.00010930919961538166\n",
            "step: 200, loss: 6.572675192728639e-05\n",
            "step: 210, loss: 9.856825636234134e-05\n",
            "step: 220, loss: 0.00022331053332891315\n",
            "step: 230, loss: 4.424508006195538e-05\n",
            "step: 240, loss: 5.003048136131838e-05\n",
            "step: 250, loss: 6.867313641123474e-05\n",
            "step: 260, loss: 3.0211218472686596e-05\n",
            "step: 270, loss: 8.307973621413112e-05\n",
            "step: 280, loss: 0.0008166661718860269\n",
            "step: 290, loss: 0.0006354317883960903\n",
            "step: 300, loss: 0.001995209837332368\n",
            "step: 310, loss: 5.706663796445355e-05\n",
            "step: 320, loss: 5.478324965224601e-05\n",
            "step: 330, loss: 4.534568506642245e-05\n",
            "step: 340, loss: 0.0002329457929590717\n",
            "step: 350, loss: 0.0001000734482659027\n",
            "step: 360, loss: 0.002868547337129712\n",
            "step: 370, loss: 0.001583907869644463\n",
            "step: 380, loss: 8.201748278224841e-05\n",
            "step: 390, loss: 0.0028158584609627724\n",
            "step: 400, loss: 3.3592747058719397e-05\n",
            "step: 410, loss: 9.977760055335239e-05\n",
            "step: 420, loss: 0.000412529829191044\n",
            "step: 430, loss: 0.00011311750131426379\n",
            "step: 440, loss: 0.0038392001297324896\n",
            "step: 450, loss: 0.002595327328890562\n",
            "step: 460, loss: 2.6556963348411955e-05\n",
            "step: 470, loss: 4.7064913815120235e-05\n",
            "step: 480, loss: 0.00045280990889295936\n",
            "step: 490, loss: 0.0031165245454758406\n",
            "step: 500, loss: 4.558470391202718e-05\n",
            "step: 510, loss: 0.0003561972116585821\n",
            "step: 520, loss: 3.70800044038333e-05\n",
            "step: 530, loss: 0.0004871632845606655\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9042553191489362, f1=0.886100386100386, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015745146083645523\n",
            "step: 10, loss: 9.304402919951826e-05\n",
            "step: 20, loss: 3.381955320946872e-05\n",
            "step: 30, loss: 0.0007307616760954261\n",
            "step: 40, loss: 3.5481032682582736e-05\n",
            "step: 50, loss: 0.0014686435461044312\n",
            "step: 60, loss: 5.746229726355523e-05\n",
            "step: 70, loss: 0.02731066197156906\n",
            "step: 80, loss: 2.8038935852237046e-05\n",
            "step: 90, loss: 8.963628351921216e-05\n",
            "step: 100, loss: 6.49827197776176e-05\n",
            "step: 110, loss: 2.9030334189883433e-05\n",
            "step: 120, loss: 4.551086749415845e-05\n",
            "step: 130, loss: 2.3830169084249064e-05\n",
            "step: 140, loss: 4.227560930303298e-05\n",
            "step: 150, loss: 0.0014924018178135157\n",
            "step: 160, loss: 0.0005593406967818737\n",
            "step: 170, loss: 0.00025444693164899945\n",
            "step: 180, loss: 0.0011282551568001509\n",
            "step: 190, loss: 0.0017732041887938976\n",
            "step: 200, loss: 0.0006270114681683481\n",
            "step: 210, loss: 0.00012371221964713186\n",
            "step: 220, loss: 0.000161341144121252\n",
            "step: 230, loss: 0.024482646957039833\n",
            "step: 240, loss: 7.695711246924475e-05\n",
            "step: 250, loss: 0.00022634475317317992\n",
            "step: 260, loss: 8.777147741056979e-05\n",
            "step: 270, loss: 0.0005056242225691676\n",
            "step: 280, loss: 4.3806765461340547e-05\n",
            "step: 290, loss: 0.0016594841144979\n",
            "step: 300, loss: 3.29529975715559e-05\n",
            "step: 310, loss: 0.00022640109818894416\n",
            "step: 320, loss: 5.7157845731126145e-05\n",
            "step: 330, loss: 0.00026171133504249156\n",
            "step: 340, loss: 6.714831397403032e-05\n",
            "step: 350, loss: 9.201405919156969e-05\n",
            "step: 360, loss: 5.1146063924534246e-05\n",
            "step: 370, loss: 0.06446170061826706\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 380, loss: 0.0003439753782004118\n",
            "step: 390, loss: 0.002040534047409892\n",
            "step: 400, loss: 0.0006849839701317251\n",
            "step: 410, loss: 0.0014939485117793083\n",
            "step: 420, loss: 0.0001298888528253883\n",
            "step: 430, loss: 0.00017974649381358176\n",
            "step: 440, loss: 0.0010405133944004774\n",
            "step: 450, loss: 0.00037506394437514246\n",
            "step: 460, loss: 0.004927139729261398\n",
            "step: 470, loss: 3.5272296372568235e-05\n",
            "step: 480, loss: 3.7232272006804124e-05\n",
            "step: 490, loss: 7.956875197123736e-05\n",
            "step: 500, loss: 2.4141785615938716e-05\n",
            "step: 510, loss: 0.0001354128762613982\n",
            "step: 520, loss: 0.004116499796509743\n",
            "step: 530, loss: 3.056776768062264e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9166269652215341, f1=0.8964525407478429, best_f1=0.889947594092425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001526606356492266\n",
            "step: 10, loss: 2.2864855054649524e-05\n",
            "step: 20, loss: 4.016093225800432e-05\n",
            "step: 30, loss: 4.5231241529108956e-05\n",
            "step: 40, loss: 0.00468703405931592\n",
            "step: 50, loss: 0.00012616468302439898\n",
            "step: 60, loss: 0.00023621792206540704\n",
            "step: 70, loss: 0.00011371474101906642\n",
            "step: 80, loss: 4.729598731501028e-05\n",
            "step: 90, loss: 0.00031319676782004535\n",
            "step: 100, loss: 6.455371476477012e-05\n",
            "step: 110, loss: 7.781365275150165e-05\n",
            "step: 120, loss: 3.925488636014052e-05\n",
            "step: 130, loss: 0.0003843719605356455\n",
            "step: 140, loss: 3.105607538600452e-05\n",
            "step: 150, loss: 8.650263043818995e-05\n",
            "step: 160, loss: 4.015272133983672e-05\n",
            "step: 170, loss: 0.0001378385495627299\n",
            "step: 180, loss: 7.402622577501461e-05\n",
            "step: 190, loss: 0.0002154657559003681\n",
            "step: 200, loss: 6.418829434551299e-05\n",
            "step: 210, loss: 2.5361105144838803e-05\n",
            "step: 220, loss: 0.00026888586580753326\n",
            "step: 230, loss: 0.0001845984807005152\n",
            "step: 240, loss: 3.4728778700809926e-05\n",
            "step: 250, loss: 4.995870403945446e-05\n",
            "step: 260, loss: 6.69581422698684e-05\n",
            "step: 270, loss: 3.3660235203569755e-05\n",
            "step: 280, loss: 6.152755668153986e-05\n",
            "step: 290, loss: 0.0004258162807673216\n",
            "step: 300, loss: 0.001284162630327046\n",
            "step: 310, loss: 0.00039667863165959716\n",
            "step: 320, loss: 0.0001371239632135257\n",
            "step: 330, loss: 0.00018212666327599436\n",
            "step: 340, loss: 5.159999636816792e-05\n",
            "step: 350, loss: 0.00012943956244271249\n",
            "step: 360, loss: 0.00037842255551368\n",
            "step: 370, loss: 4.1345181671204045e-05\n",
            "step: 380, loss: 8.012894977582619e-05\n",
            "step: 390, loss: 0.0013282395666465163\n",
            "step: 400, loss: 0.00026169285411015153\n",
            "step: 410, loss: 7.041277422104031e-05\n",
            "step: 420, loss: 3.37548553943634e-05\n",
            "step: 430, loss: 7.602661935379729e-05\n",
            "step: 440, loss: 0.0007081864168867469\n",
            "step: 450, loss: 0.0002722225326579064\n",
            "step: 460, loss: 3.916819332516752e-05\n",
            "step: 470, loss: 3.835623647319153e-05\n",
            "step: 480, loss: 0.00011851538874907419\n",
            "step: 490, loss: 3.401615686016157e-05\n",
            "step: 500, loss: 1.938965215231292e-05\n",
            "step: 510, loss: 0.00024447261239401996\n",
            "step: 520, loss: 7.824298518244177e-05\n",
            "step: 530, loss: 3.270643355790526e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9189957366177167, f1=0.8956646021915198, best_f1=0.8956646021915198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.069894930580631e-05\n",
            "step: 10, loss: 0.000161455842317082\n",
            "step: 20, loss: 8.132925722748041e-05\n",
            "step: 30, loss: 0.00025478930911049247\n",
            "step: 40, loss: 8.377403719350696e-05\n",
            "step: 50, loss: 0.00017424052930437028\n",
            "step: 60, loss: 1.8622364223119803e-05\n",
            "step: 70, loss: 0.0002314460143679753\n",
            "step: 80, loss: 0.0004985517007298768\n",
            "step: 90, loss: 6.481949822045863e-05\n",
            "step: 100, loss: 7.066282705636695e-05\n",
            "step: 110, loss: 5.032658009440638e-05\n",
            "step: 120, loss: 2.9495780836441554e-05\n",
            "step: 130, loss: 0.0013322783634066582\n",
            "step: 140, loss: 0.00014256307622417808\n",
            "step: 150, loss: 0.00015319319209083915\n",
            "step: 160, loss: 8.690202957950532e-05\n",
            "step: 170, loss: 0.0006415120442397892\n",
            "step: 180, loss: 2.2600666852667928e-05\n",
            "step: 190, loss: 0.0005030771135352552\n",
            "step: 200, loss: 2.828879951266572e-05\n",
            "step: 210, loss: 9.353106725029647e-05\n",
            "step: 220, loss: 5.293632057146169e-05\n",
            "step: 230, loss: 5.883479389012791e-05\n",
            "step: 240, loss: 0.0040060714818537235\n",
            "step: 250, loss: 3.836063842754811e-05\n",
            "step: 260, loss: 2.766690704447683e-05\n",
            "step: 270, loss: 3.165623274981044e-05\n",
            "step: 280, loss: 0.0001830161054385826\n",
            "step: 290, loss: 2.494380714779254e-05\n",
            "step: 300, loss: 4.4052543671568856e-05\n",
            "step: 310, loss: 0.00039586317143402994\n",
            "step: 320, loss: 0.005367023404687643\n",
            "step: 330, loss: 6.27185363555327e-05\n",
            "step: 340, loss: 2.6381669158581644e-05\n",
            "step: 350, loss: 1.640597838559188e-05\n",
            "step: 360, loss: 0.0015970732783898711\n",
            "step: 370, loss: 0.0020748034585267305\n",
            "step: 380, loss: 3.647966877906583e-05\n",
            "step: 390, loss: 5.453384073916823e-05\n",
            "step: 400, loss: 3.4047938243020326e-05\n",
            "step: 410, loss: 0.00012536977010313421\n",
            "step: 420, loss: 0.00014161197759676725\n",
            "step: 430, loss: 0.0001265190076082945\n",
            "step: 440, loss: 5.0983624532818794e-05\n",
            "step: 450, loss: 2.6351588530815206e-05\n",
            "step: 460, loss: 3.998633474111557e-05\n",
            "step: 470, loss: 2.9401928259176202e-05\n",
            "step: 480, loss: 4.06507751904428e-05\n",
            "step: 490, loss: 2.2150126824271865e-05\n",
            "step: 500, loss: 3.170039417454973e-05\n",
            "step: 510, loss: 0.00012333244376350194\n",
            "step: 520, loss: 3.890774678438902e-05\n",
            "step: 530, loss: 3.142894274787977e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9203956665096561, f1=0.8980169971671387, best_f1=0.8980169971671387\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 232.26it/s]\n",
            "load_f1 = 0.9186155285313378\n",
            "real_f1 = 0.9163533834586467\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 242.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6de960-6b6f-48ab-e955-a37c891a250f"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5551786422729492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.37333333333333335, f1=0.33766233766233766, best_f1=0.33766233766233766\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5203631520271301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.42424242424242425, f1=0.3835616438356164, best_f1=0.3835616438356164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5861638784408569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5306122448979592, f1=0.3934426229508196, best_f1=0.3934426229508196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3425706923007965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5777777777777778, f1=0.4705882352941177, best_f1=0.4705882352941177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.36081892251968384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6341463414634146, f1=0.4878048780487805, best_f1=0.4878048780487805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29483652114868164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7142857142857143, f1=0.5806451612903226, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2356729656457901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7272727272727273, f1=0.55, best_f1=0.55\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05478467792272568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7142857142857143, f1=0.5625000000000001, best_f1=0.55\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00521625904366374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7647058823529412, f1=0.6111111111111112, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.062225230038166046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7027027027027025, f1=0.5641025641025641, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009794116951525211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7200000000000001, f1=0.5517241379310344, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05906094238162041\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.742857142857143, f1=0.5641025641025641, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020768988877534866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.742857142857143, f1=0.5641025641025641, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013823959976434708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7647058823529412, f1=0.5641025641025641, best_f1=0.6111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030742648988962173\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7647058823529412, f1=0.5641025641025641, best_f1=0.6111111111111112\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 138642.09it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.6923076923076924\n",
            "real_f1 = 0.6923076923076924\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 247.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171cc2a7-b311-43e6-fe06-dcb5be5af3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6125273108482361\n",
            "step: 10, loss: 0.634732723236084\n",
            "step: 20, loss: 0.48264914751052856\n",
            "step: 30, loss: 0.37067699432373047\n",
            "step: 40, loss: 0.15597350895404816\n",
            "step: 50, loss: 0.33545631170272827\n",
            "step: 60, loss: 0.04662356525659561\n",
            "step: 70, loss: 0.024408916011452675\n",
            "step: 80, loss: 0.13388077914714813\n",
            "step: 90, loss: 0.21603842079639435\n",
            "step: 100, loss: 0.01286372635513544\n",
            "step: 110, loss: 0.1714010387659073\n",
            "step: 120, loss: 0.03181290999054909\n",
            "step: 130, loss: 0.0228571817278862\n",
            "step: 140, loss: 0.0061898622661828995\n",
            "step: 150, loss: 0.08743108063936234\n",
            "step: 160, loss: 0.011198822408914566\n",
            "step: 170, loss: 0.024940330535173416\n",
            "step: 180, loss: 0.1438705027103424\n",
            "step: 190, loss: 0.14274956285953522\n",
            "step: 200, loss: 0.02236882410943508\n",
            "step: 210, loss: 0.004620740655809641\n",
            "step: 220, loss: 0.06944581121206284\n",
            "step: 230, loss: 0.048176731914281845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.963882618510158, f1=0.9625425652667423, best_f1=0.9625425652667423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03449138626456261\n",
            "step: 10, loss: 0.0035242901649326086\n",
            "step: 20, loss: 0.07605284452438354\n",
            "step: 30, loss: 0.19910404086112976\n",
            "step: 40, loss: 0.017395203933119774\n",
            "step: 50, loss: 0.006793178617954254\n",
            "step: 60, loss: 0.004923023283481598\n",
            "step: 70, loss: 0.15433919429779053\n",
            "step: 80, loss: 0.022104522213339806\n",
            "step: 90, loss: 0.016230657696723938\n",
            "step: 100, loss: 0.0994827151298523\n",
            "step: 110, loss: 0.06646080315113068\n",
            "step: 120, loss: 0.031216736882925034\n",
            "step: 130, loss: 0.020358191803097725\n",
            "step: 140, loss: 0.03473959118127823\n",
            "step: 150, loss: 0.011558104306459427\n",
            "step: 160, loss: 0.0028204198461025953\n",
            "step: 170, loss: 0.022912971675395966\n",
            "step: 180, loss: 0.014498892240226269\n",
            "step: 190, loss: 0.017853576689958572\n",
            "step: 200, loss: 0.002065579406917095\n",
            "step: 210, loss: 0.0008340222993865609\n",
            "step: 220, loss: 0.09280554950237274\n",
            "step: 230, loss: 0.09112107008695602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9798206278026906, f1=0.9730337078651685, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008639758452773094\n",
            "step: 10, loss: 0.003373668296262622\n",
            "step: 20, loss: 0.0072344038635492325\n",
            "step: 30, loss: 0.003261468606069684\n",
            "step: 40, loss: 0.0993417352437973\n",
            "step: 50, loss: 0.04753194376826286\n",
            "step: 60, loss: 0.004276916850358248\n",
            "step: 70, loss: 0.10581675916910172\n",
            "step: 80, loss: 0.0008523485739715397\n",
            "step: 90, loss: 0.1360769271850586\n",
            "step: 100, loss: 0.0012634432641789317\n",
            "step: 110, loss: 0.0018622373463585973\n",
            "step: 120, loss: 0.016055045649409294\n",
            "step: 130, loss: 0.0011884871637448668\n",
            "step: 140, loss: 0.058881524950265884\n",
            "step: 150, loss: 0.025226986035704613\n",
            "step: 160, loss: 0.0019915227312594652\n",
            "step: 170, loss: 0.00471445033326745\n",
            "step: 180, loss: 0.023233307525515556\n",
            "step: 190, loss: 0.019532231613993645\n",
            "step: 200, loss: 0.040428485721349716\n",
            "step: 210, loss: 0.0013366129714995623\n",
            "step: 220, loss: 0.0012954672565683722\n",
            "step: 230, loss: 0.0010143450926989317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9754464285714286, f1=0.9732142857142857, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013093689922243357\n",
            "step: 10, loss: 0.0003755372017621994\n",
            "step: 20, loss: 0.004208176862448454\n",
            "step: 30, loss: 0.009115096181631088\n",
            "step: 40, loss: 0.01641671359539032\n",
            "step: 50, loss: 0.005156326573342085\n",
            "step: 60, loss: 0.0395493283867836\n",
            "step: 70, loss: 0.00042582247988320887\n",
            "step: 80, loss: 0.009618241339921951\n",
            "step: 90, loss: 0.0008346524555236101\n",
            "step: 100, loss: 0.0010335530387237668\n",
            "step: 110, loss: 0.001435709884390235\n",
            "step: 120, loss: 0.03737922012805939\n",
            "step: 130, loss: 0.0024665528908371925\n",
            "step: 140, loss: 0.027104660868644714\n",
            "step: 150, loss: 0.10361763834953308\n",
            "step: 160, loss: 0.004091465380042791\n",
            "step: 170, loss: 0.17546454071998596\n",
            "step: 180, loss: 0.0007796854479238391\n",
            "step: 190, loss: 0.01473039947450161\n",
            "step: 200, loss: 0.0009342360426671803\n",
            "step: 210, loss: 0.09205443412065506\n",
            "step: 220, loss: 0.0013719445560127497\n",
            "step: 230, loss: 0.0020995272789150476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9757709251101322, f1=0.9649890590809628, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011607721680775285\n",
            "step: 10, loss: 0.00782892107963562\n",
            "step: 20, loss: 0.037098679691553116\n",
            "step: 30, loss: 0.00045101146679371595\n",
            "step: 40, loss: 0.0002659203018993139\n",
            "step: 50, loss: 0.00047101982636377215\n",
            "step: 60, loss: 0.0007552462047897279\n",
            "step: 70, loss: 0.00019178990623913705\n",
            "step: 80, loss: 0.001598771894350648\n",
            "step: 90, loss: 0.0003008981002494693\n",
            "step: 100, loss: 0.0008166742045432329\n",
            "step: 110, loss: 0.00019008545496035367\n",
            "step: 120, loss: 0.00037482636980712414\n",
            "step: 130, loss: 0.009593593887984753\n",
            "step: 140, loss: 0.008788051083683968\n",
            "step: 150, loss: 0.015309691429138184\n",
            "step: 160, loss: 0.0037202350795269012\n",
            "step: 170, loss: 0.003069077618420124\n",
            "step: 180, loss: 0.11289866268634796\n",
            "step: 190, loss: 0.0005789376446045935\n",
            "step: 200, loss: 0.04977957531809807\n",
            "step: 210, loss: 0.0033428766764700413\n",
            "step: 220, loss: 0.002379925223067403\n",
            "step: 230, loss: 0.0002483751450199634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9730337078651685, f1=0.9777777777777777, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001607892569154501\n",
            "step: 10, loss: 0.010885201394557953\n",
            "step: 20, loss: 0.004166122525930405\n",
            "step: 30, loss: 0.0010889994446188211\n",
            "step: 40, loss: 0.001360714202746749\n",
            "step: 50, loss: 0.03767545893788338\n",
            "step: 60, loss: 0.00019519656780175865\n",
            "step: 70, loss: 0.0008439291850663722\n",
            "step: 80, loss: 0.0009751093457452953\n",
            "step: 90, loss: 0.006544723175466061\n",
            "step: 100, loss: 0.030971093103289604\n",
            "step: 110, loss: 0.04281361401081085\n",
            "step: 120, loss: 0.0004070907016284764\n",
            "step: 130, loss: 0.008809477090835571\n",
            "step: 140, loss: 0.0075585199519991875\n",
            "step: 150, loss: 0.0013264210429042578\n",
            "step: 160, loss: 0.014085362665355206\n",
            "step: 170, loss: 0.0040975287556648254\n",
            "step: 180, loss: 0.05494081974029541\n",
            "step: 190, loss: 0.0018045371398329735\n",
            "step: 200, loss: 0.0011145866010338068\n",
            "step: 210, loss: 0.0017107243184000254\n",
            "step: 220, loss: 0.0004072347073815763\n",
            "step: 230, loss: 0.029694009572267532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9763779527559054, f1=0.9751693002257337, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019787998870015144\n",
            "step: 10, loss: 9.946034697350115e-05\n",
            "step: 20, loss: 0.00025518020265735686\n",
            "step: 30, loss: 0.0002594663528725505\n",
            "step: 40, loss: 0.00032137450762093067\n",
            "step: 50, loss: 0.0003699635562952608\n",
            "step: 60, loss: 0.000748339225538075\n",
            "step: 70, loss: 0.030943134799599648\n",
            "step: 80, loss: 0.0005708155804313719\n",
            "step: 90, loss: 0.0012621150817722082\n",
            "step: 100, loss: 0.000265562062850222\n",
            "step: 110, loss: 0.0010227069724351168\n",
            "step: 120, loss: 0.0006022921297699213\n",
            "step: 130, loss: 0.0008827627752907574\n",
            "step: 140, loss: 0.005129074677824974\n",
            "step: 150, loss: 0.001647222088649869\n",
            "step: 160, loss: 0.09777458757162094\n",
            "step: 170, loss: 0.0008839906076900661\n",
            "step: 180, loss: 0.0003091710968874395\n",
            "step: 190, loss: 0.0018328444566577673\n",
            "step: 200, loss: 0.06617086380720139\n",
            "step: 210, loss: 0.0004392415867187083\n",
            "step: 220, loss: 0.0005553118535317481\n",
            "step: 230, loss: 0.001644135918468237\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9729119638826186, f1=0.9683972911963882, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000585924310144037\n",
            "step: 10, loss: 0.021533535793423653\n",
            "step: 20, loss: 0.00014904244744684547\n",
            "step: 30, loss: 0.00015236393664963543\n",
            "step: 40, loss: 0.0026699211448431015\n",
            "step: 50, loss: 0.0008456922369077802\n",
            "step: 60, loss: 0.0101921446621418\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.0004056961042806506\n",
            "step: 80, loss: 0.0002688123204279691\n",
            "step: 90, loss: 8.046348375501111e-05\n",
            "step: 100, loss: 0.0004361363244242966\n",
            "step: 110, loss: 0.00015628825349267572\n",
            "step: 120, loss: 0.04338430240750313\n",
            "step: 130, loss: 0.000979208038188517\n",
            "step: 140, loss: 0.000273028650553897\n",
            "step: 150, loss: 0.011321723461151123\n",
            "step: 160, loss: 0.0001232421345775947\n",
            "step: 170, loss: 0.0002896888763643801\n",
            "step: 180, loss: 0.0006927576032467186\n",
            "step: 190, loss: 0.00023239188885781914\n",
            "step: 200, loss: 0.00014466750144492835\n",
            "step: 210, loss: 0.00013214141654316336\n",
            "step: 220, loss: 0.00012104208872187883\n",
            "step: 230, loss: 0.00013071011926513165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9743589743589743, f1=0.9722530521642618, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018082499445881695\n",
            "step: 10, loss: 0.00016993182362057269\n",
            "step: 20, loss: 0.0007032356807030737\n",
            "step: 30, loss: 3.9307251427089795e-05\n",
            "step: 40, loss: 0.023064885288476944\n",
            "step: 50, loss: 5.013382178731263e-05\n",
            "step: 60, loss: 9.287639113608748e-05\n",
            "step: 70, loss: 0.0005080051487311721\n",
            "step: 80, loss: 0.0006531900144182146\n",
            "step: 90, loss: 7.249110785778612e-05\n",
            "step: 100, loss: 0.00021977751748636365\n",
            "step: 110, loss: 8.527059253538027e-05\n",
            "step: 120, loss: 3.847735933959484e-05\n",
            "step: 130, loss: 0.001056784181855619\n",
            "step: 140, loss: 0.0003156095917802304\n",
            "step: 150, loss: 0.06459996849298477\n",
            "step: 160, loss: 0.00043831937364302576\n",
            "step: 170, loss: 0.0002595716214273125\n",
            "step: 180, loss: 0.007869930937886238\n",
            "step: 190, loss: 0.00042988997302018106\n",
            "step: 200, loss: 0.0049122171476483345\n",
            "step: 210, loss: 0.0032553470227867365\n",
            "step: 220, loss: 0.00019230431644245982\n",
            "step: 230, loss: 0.0004990277229808271\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9680365296803655, f1=0.9681093394077448, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028655147179961205\n",
            "step: 10, loss: 0.0001326439087279141\n",
            "step: 20, loss: 0.0012397492537274957\n",
            "step: 30, loss: 0.00030531291849911213\n",
            "step: 40, loss: 0.00012941742897965014\n",
            "step: 50, loss: 0.00050439836923033\n",
            "step: 60, loss: 0.003960700239986181\n",
            "step: 70, loss: 0.0012088401708751917\n",
            "step: 80, loss: 8.564864401705563e-05\n",
            "step: 90, loss: 0.0036175663117319345\n",
            "step: 100, loss: 0.00021624584042001516\n",
            "step: 110, loss: 0.002810389269143343\n",
            "step: 120, loss: 0.0003510891692712903\n",
            "step: 130, loss: 0.00036297127371653914\n",
            "step: 140, loss: 0.05609610676765442\n",
            "step: 150, loss: 0.00908343493938446\n",
            "step: 160, loss: 0.0099077969789505\n",
            "step: 170, loss: 5.8438330597709864e-05\n",
            "step: 180, loss: 0.0007637699018232524\n",
            "step: 190, loss: 0.08078867942094803\n",
            "step: 200, loss: 0.00043403799645602703\n",
            "step: 210, loss: 0.00022183007968124002\n",
            "step: 220, loss: 0.0004024274239782244\n",
            "step: 230, loss: 0.038544390350580215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9730941704035874, f1=0.9743016759776536, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.082709584617987e-05\n",
            "step: 10, loss: 0.00034815858816727996\n",
            "step: 20, loss: 0.009160144254565239\n",
            "step: 30, loss: 0.007618098519742489\n",
            "step: 40, loss: 0.0010197034571319818\n",
            "step: 50, loss: 0.000751265324652195\n",
            "step: 60, loss: 0.0004319300933275372\n",
            "step: 70, loss: 0.024562230333685875\n",
            "step: 80, loss: 0.0007184858550317585\n",
            "step: 90, loss: 7.791447569616139e-05\n",
            "step: 100, loss: 0.00010775140253826976\n",
            "step: 110, loss: 0.0002952012000605464\n",
            "step: 120, loss: 0.000107263978861738\n",
            "step: 130, loss: 0.0003135384467896074\n",
            "step: 140, loss: 0.0021705098915845156\n",
            "step: 150, loss: 0.006647143047302961\n",
            "step: 160, loss: 8.060748950811103e-05\n",
            "step: 170, loss: 0.0008701553451828659\n",
            "step: 180, loss: 0.008721905760467052\n",
            "step: 190, loss: 4.3743508285842836e-05\n",
            "step: 200, loss: 7.770375668769702e-05\n",
            "step: 210, loss: 7.78487155912444e-05\n",
            "step: 220, loss: 0.0001951821323018521\n",
            "step: 230, loss: 0.0007567350403405726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9730337078651685, f1=0.9752252252252253, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.0578153857495636e-05\n",
            "step: 10, loss: 4.968493522028439e-05\n",
            "step: 20, loss: 4.513595558819361e-05\n",
            "step: 30, loss: 9.369882172904909e-05\n",
            "step: 40, loss: 0.0020162700675427914\n",
            "step: 50, loss: 0.005558932665735483\n",
            "step: 60, loss: 0.0017953802598640323\n",
            "step: 70, loss: 8.840008376864716e-05\n",
            "step: 80, loss: 0.00010667923925211653\n",
            "step: 90, loss: 3.569846012396738e-05\n",
            "step: 100, loss: 0.0002167539350921288\n",
            "step: 110, loss: 6.661543739028275e-05\n",
            "step: 120, loss: 0.0001890489220386371\n",
            "step: 130, loss: 9.409159974893555e-05\n",
            "step: 140, loss: 7.547594577772543e-05\n",
            "step: 150, loss: 0.0028453092090785503\n",
            "step: 160, loss: 0.00021284635295160115\n",
            "step: 170, loss: 7.920097414171323e-05\n",
            "step: 180, loss: 0.00011355476453900337\n",
            "step: 190, loss: 6.694899639114738e-05\n",
            "step: 200, loss: 2.4880751880118623e-05\n",
            "step: 210, loss: 4.119645745959133e-05\n",
            "step: 220, loss: 0.002501735696569085\n",
            "step: 230, loss: 0.0030976622365415096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9743589743589743, f1=0.9744160177975528, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019896957091987133\n",
            "step: 10, loss: 0.00018761609680950642\n",
            "step: 20, loss: 0.0022615455090999603\n",
            "step: 30, loss: 0.0011208084179088473\n",
            "step: 40, loss: 0.12012497335672379\n",
            "step: 50, loss: 9.66516017797403e-05\n",
            "step: 60, loss: 3.823845327133313e-05\n",
            "step: 70, loss: 3.387539982213639e-05\n",
            "step: 80, loss: 3.867389386869036e-05\n",
            "step: 90, loss: 0.0004848790413234383\n",
            "step: 100, loss: 4.9339225370204076e-05\n",
            "step: 110, loss: 0.005670326761901379\n",
            "step: 120, loss: 8.095484372461215e-05\n",
            "step: 130, loss: 8.522148709744215e-05\n",
            "step: 140, loss: 3.758263846975751e-05\n",
            "step: 150, loss: 2.836006933648605e-05\n",
            "step: 160, loss: 0.00038166638114489615\n",
            "step: 170, loss: 6.332749035209417e-05\n",
            "step: 180, loss: 3.661058144643903e-05\n",
            "step: 190, loss: 4.059881757711992e-05\n",
            "step: 200, loss: 0.0030685788951814175\n",
            "step: 210, loss: 2.451550608384423e-05\n",
            "step: 220, loss: 7.298430864466354e-05\n",
            "step: 230, loss: 3.574219226720743e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9764837625979844, f1=0.9764309764309763, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.207823803881183e-05\n",
            "step: 10, loss: 0.01126464270055294\n",
            "step: 20, loss: 0.0001288648636545986\n",
            "step: 30, loss: 0.0006197839393280447\n",
            "step: 40, loss: 0.0005366250406950712\n",
            "step: 50, loss: 5.945978773524985e-05\n",
            "step: 60, loss: 4.061816798639484e-05\n",
            "step: 70, loss: 0.00016805788618512452\n",
            "step: 80, loss: 0.0005472922930493951\n",
            "step: 90, loss: 3.449474388617091e-05\n",
            "step: 100, loss: 4.3041804019594565e-05\n",
            "step: 110, loss: 3.908125654561445e-05\n",
            "step: 120, loss: 2.2910164261702448e-05\n",
            "step: 130, loss: 3.299772652098909e-05\n",
            "step: 140, loss: 4.0667018765816465e-05\n",
            "step: 150, loss: 2.7462168873171322e-05\n",
            "step: 160, loss: 0.005897959228605032\n",
            "step: 170, loss: 2.3290140234166756e-05\n",
            "step: 180, loss: 4.888570765615441e-05\n",
            "step: 190, loss: 3.502632534946315e-05\n",
            "step: 200, loss: 4.819318201043643e-05\n",
            "step: 210, loss: 0.00028195496997796\n",
            "step: 220, loss: 0.0007639891118742526\n",
            "step: 230, loss: 0.00029167826869525015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9751693002257337, f1=0.9763779527559054, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.6825815186602995e-05\n",
            "step: 10, loss: 2.7737132768379524e-05\n",
            "step: 20, loss: 6.162388308439404e-05\n",
            "step: 30, loss: 4.683159932028502e-05\n",
            "step: 40, loss: 0.0002474795328453183\n",
            "step: 50, loss: 0.02606639824807644\n",
            "step: 60, loss: 2.9306022042874247e-05\n",
            "step: 70, loss: 8.063529821811244e-05\n",
            "step: 80, loss: 0.0097003523260355\n",
            "step: 90, loss: 8.505635923938826e-05\n",
            "step: 100, loss: 5.371090810513124e-05\n",
            "step: 110, loss: 0.0048848786391317844\n",
            "step: 120, loss: 0.00021831806225236505\n",
            "step: 130, loss: 2.91050091618672e-05\n",
            "step: 140, loss: 2.7816096917376854e-05\n",
            "step: 150, loss: 6.428708002204075e-05\n",
            "step: 160, loss: 0.0001923644740600139\n",
            "step: 170, loss: 0.00010464182560099289\n",
            "step: 180, loss: 4.6916858991608024e-05\n",
            "step: 190, loss: 0.00012126524961786345\n",
            "step: 200, loss: 8.638860163046047e-05\n",
            "step: 210, loss: 0.00014564265438821167\n",
            "step: 220, loss: 0.0006828467594459653\n",
            "step: 230, loss: 0.0001540316443424672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.976324689966178, f1=0.9763779527559054, best_f1=0.9730337078651685\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 193.12it/s]\n",
            "load_f1 = 0.9797297297297298\n",
            "real_f1 = 0.9808773903262092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 234.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbe1e15-92b2-4c99-df2a-05a1ef810a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 395kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 347kB/s]\n",
            "Downloading: 100% 440M/440M [00:24<00:00, 17.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6516682505607605\n",
            "step: 10, loss: 0.5442603230476379\n",
            "step: 20, loss: 0.5805392265319824\n",
            "step: 30, loss: 0.354943186044693\n",
            "step: 40, loss: 0.43120574951171875\n",
            "step: 50, loss: 0.3120923638343811\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.09215760976076126\n",
            "step: 70, loss: 0.27884456515312195\n",
            "step: 80, loss: 0.0733758956193924\n",
            "step: 90, loss: 0.47419053316116333\n",
            "step: 100, loss: 0.10777720808982849\n",
            "step: 110, loss: 0.09580127149820328\n",
            "step: 120, loss: 0.1123201847076416\n",
            "step: 130, loss: 0.12251872569322586\n",
            "step: 140, loss: 0.15984408557415009\n",
            "step: 150, loss: 0.13865146040916443\n",
            "step: 160, loss: 0.04552578181028366\n",
            "step: 170, loss: 0.25079792737960815\n",
            "step: 180, loss: 0.05609835311770439\n",
            "step: 190, loss: 0.05890258774161339\n",
            "step: 200, loss: 0.13212767243385315\n",
            "step: 210, loss: 0.0864730030298233\n",
            "step: 220, loss: 0.325448602437973\n",
            "step: 230, loss: 0.15821203589439392\n",
            "step: 240, loss: 0.09702880680561066\n",
            "step: 250, loss: 0.11176594346761703\n",
            "step: 260, loss: 0.26485079526901245\n",
            "step: 270, loss: 0.027711452916264534\n",
            "step: 280, loss: 0.11221151053905487\n",
            "step: 290, loss: 0.1523686647415161\n",
            "step: 300, loss: 0.053268443793058395\n",
            "step: 310, loss: 0.17913196980953217\n",
            "step: 320, loss: 0.0817926675081253\n",
            "step: 330, loss: 0.045366544276475906\n",
            "step: 340, loss: 0.12988236546516418\n",
            "step: 350, loss: 0.034080374985933304\n",
            "step: 360, loss: 0.05461651459336281\n",
            "step: 370, loss: 0.21469518542289734\n",
            "step: 380, loss: 0.020308375358581543\n",
            "step: 390, loss: 0.37779682874679565\n",
            "step: 400, loss: 0.22145172953605652\n",
            "step: 410, loss: 0.0811118483543396\n",
            "step: 420, loss: 0.15984764695167542\n",
            "step: 430, loss: 0.14339867234230042\n",
            "step: 440, loss: 0.04561661556363106\n",
            "step: 450, loss: 0.029530050233006477\n",
            "step: 460, loss: 0.019099729135632515\n",
            "step: 470, loss: 0.09894194453954697\n",
            "step: 480, loss: 0.12412215024232864\n",
            "step: 490, loss: 0.24672026932239532\n",
            "step: 500, loss: 0.13427023589611053\n",
            "step: 510, loss: 0.0851416140794754\n",
            "step: 520, loss: 0.04163914918899536\n",
            "step: 530, loss: 0.036168716847896576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9001865671641792, f1=0.8962308050255934, best_f1=0.8962308050255934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18727123737335205\n",
            "step: 10, loss: 0.12514831125736237\n",
            "step: 20, loss: 0.03884948045015335\n",
            "step: 30, loss: 0.028027797117829323\n",
            "step: 40, loss: 0.11921899020671844\n",
            "step: 50, loss: 0.1521671712398529\n",
            "step: 60, loss: 0.038016147911548615\n",
            "step: 70, loss: 0.08387494087219238\n",
            "step: 80, loss: 0.22234179079532623\n",
            "step: 90, loss: 0.034747861325740814\n",
            "step: 100, loss: 0.12771742045879364\n",
            "step: 110, loss: 0.07001998275518417\n",
            "step: 120, loss: 0.13377292454242706\n",
            "step: 130, loss: 0.15545545518398285\n",
            "step: 140, loss: 0.0590069517493248\n",
            "step: 150, loss: 0.13714823126792908\n",
            "step: 160, loss: 0.021254075691103935\n",
            "step: 170, loss: 0.0721975788474083\n",
            "step: 180, loss: 0.03301473706960678\n",
            "step: 190, loss: 0.05148835852742195\n",
            "step: 200, loss: 0.023888645693659782\n",
            "step: 210, loss: 0.0598914809525013\n",
            "step: 220, loss: 0.08789756149053574\n",
            "step: 230, loss: 0.009201704524457455\n",
            "step: 240, loss: 0.042390793561935425\n",
            "step: 250, loss: 0.07698938995599747\n",
            "step: 260, loss: 0.01227183174341917\n",
            "step: 270, loss: 0.2684674859046936\n",
            "step: 280, loss: 0.07074382901191711\n",
            "step: 290, loss: 0.08378026634454727\n",
            "step: 300, loss: 0.03767503425478935\n",
            "step: 310, loss: 0.11072684079408646\n",
            "step: 320, loss: 0.19647139310836792\n",
            "step: 330, loss: 0.06642482429742813\n",
            "step: 340, loss: 0.043532196432352066\n",
            "step: 350, loss: 0.004251889418810606\n",
            "step: 360, loss: 0.06392479687929153\n",
            "step: 370, loss: 0.1327272206544876\n",
            "step: 380, loss: 0.19990655779838562\n",
            "step: 390, loss: 0.1615966409444809\n",
            "step: 400, loss: 0.04357128217816353\n",
            "step: 410, loss: 0.010058870539069176\n",
            "step: 420, loss: 0.062362439930438995\n",
            "step: 430, loss: 0.035506147891283035\n",
            "step: 440, loss: 0.045352593064308167\n",
            "step: 450, loss: 0.0999252200126648\n",
            "step: 460, loss: 0.043159130960702896\n",
            "step: 470, loss: 0.013273250311613083\n",
            "step: 480, loss: 0.1657121628522873\n",
            "step: 490, loss: 0.02465713582932949\n",
            "step: 500, loss: 0.30680060386657715\n",
            "step: 510, loss: 0.04114522039890289\n",
            "step: 520, loss: 0.12404020875692368\n",
            "step: 530, loss: 0.023780034855008125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9096045197740114, f1=0.9001865671641792, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08650670945644379\n",
            "step: 10, loss: 0.044175148010253906\n",
            "step: 20, loss: 0.1772846281528473\n",
            "step: 30, loss: 0.05118848755955696\n",
            "step: 40, loss: 0.03791843354701996\n",
            "step: 50, loss: 0.13140372931957245\n",
            "step: 60, loss: 0.048562247306108475\n",
            "step: 70, loss: 0.011762753129005432\n",
            "step: 80, loss: 0.00736711360514164\n",
            "step: 90, loss: 0.02990984544157982\n",
            "step: 100, loss: 0.07369312644004822\n",
            "step: 110, loss: 0.0175878144800663\n",
            "step: 120, loss: 0.023784037679433823\n",
            "step: 130, loss: 0.0036749732680618763\n",
            "step: 140, loss: 0.03635983541607857\n",
            "step: 150, loss: 0.09108547121286392\n",
            "step: 160, loss: 0.023320410400629044\n",
            "step: 170, loss: 0.1171141043305397\n",
            "step: 180, loss: 0.04509983956813812\n",
            "step: 190, loss: 0.03163988143205643\n",
            "step: 200, loss: 0.11868558824062347\n",
            "step: 210, loss: 0.027212202548980713\n",
            "step: 220, loss: 0.14726832509040833\n",
            "step: 230, loss: 0.09413886070251465\n",
            "step: 240, loss: 0.05272320285439491\n",
            "step: 250, loss: 0.04826754331588745\n",
            "step: 260, loss: 0.025786811485886574\n",
            "step: 270, loss: 0.007249115966260433\n",
            "step: 280, loss: 0.13909709453582764\n",
            "step: 290, loss: 0.007489943411201239\n",
            "step: 300, loss: 0.05073947086930275\n",
            "step: 310, loss: 0.09635603427886963\n",
            "step: 320, loss: 0.01161795761436224\n",
            "step: 330, loss: 0.03468291461467743\n",
            "step: 340, loss: 0.11880150437355042\n",
            "step: 350, loss: 0.006507757119834423\n",
            "step: 360, loss: 0.03151267021894455\n",
            "step: 370, loss: 0.021062003448605537\n",
            "step: 380, loss: 0.01682894304394722\n",
            "step: 390, loss: 0.09857942909002304\n",
            "step: 400, loss: 0.03007015772163868\n",
            "step: 410, loss: 0.020101027563214302\n",
            "step: 420, loss: 0.1136523187160492\n",
            "step: 430, loss: 0.022435953840613365\n",
            "step: 440, loss: 0.007535920944064856\n",
            "step: 450, loss: 0.017144273966550827\n",
            "step: 460, loss: 0.09781556576490402\n",
            "step: 470, loss: 0.22453024983406067\n",
            "step: 480, loss: 0.06658809632062912\n",
            "step: 490, loss: 0.056157030165195465\n",
            "step: 500, loss: 0.008106104098260403\n",
            "step: 510, loss: 0.019388431683182716\n",
            "step: 520, loss: 0.07955234497785568\n",
            "step: 530, loss: 0.1162550300359726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9066543438077633, f1=0.8960074280408542, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021881727501749992\n",
            "step: 10, loss: 0.004962571896612644\n",
            "step: 20, loss: 0.006401402410119772\n",
            "step: 30, loss: 0.03853942081332207\n",
            "step: 40, loss: 0.021822353824973106\n",
            "step: 50, loss: 0.0542147159576416\n",
            "step: 60, loss: 0.044324930757284164\n",
            "step: 70, loss: 0.10388968884944916\n",
            "step: 80, loss: 0.0117641007527709\n",
            "step: 90, loss: 0.17964288592338562\n",
            "step: 100, loss: 0.04451309144496918\n",
            "step: 110, loss: 0.07071875780820847\n",
            "step: 120, loss: 0.0027340035885572433\n",
            "step: 130, loss: 0.0026846646796911955\n",
            "step: 140, loss: 0.005228231195360422\n",
            "step: 150, loss: 0.03734550625085831\n",
            "step: 160, loss: 0.16002888977527618\n",
            "step: 170, loss: 0.013710063882172108\n",
            "step: 180, loss: 0.02436267025768757\n",
            "step: 190, loss: 0.036349087953567505\n",
            "step: 200, loss: 0.016854021698236465\n",
            "step: 210, loss: 0.016384819522500038\n",
            "step: 220, loss: 0.018056070432066917\n",
            "step: 230, loss: 0.01815945655107498\n",
            "step: 240, loss: 0.025157101452350616\n",
            "step: 250, loss: 0.034887153655290604\n",
            "step: 260, loss: 0.005412920843809843\n",
            "step: 270, loss: 0.006662154104560614\n",
            "step: 280, loss: 0.07899673283100128\n",
            "step: 290, loss: 0.092631496489048\n",
            "step: 300, loss: 0.002036913763731718\n",
            "step: 310, loss: 0.007387756835669279\n",
            "step: 320, loss: 0.04351196065545082\n",
            "step: 330, loss: 0.06293956190347672\n",
            "step: 340, loss: 0.03418156877160072\n",
            "step: 350, loss: 0.02386840432882309\n",
            "step: 360, loss: 0.1066058874130249\n",
            "step: 370, loss: 0.0034725258592516184\n",
            "step: 380, loss: 0.01297271903604269\n",
            "step: 390, loss: 0.15821895003318787\n",
            "step: 400, loss: 0.01841052435338497\n",
            "step: 410, loss: 0.01743965595960617\n",
            "step: 420, loss: 0.00311901792883873\n",
            "step: 430, loss: 0.2066938579082489\n",
            "step: 440, loss: 0.0039011952467262745\n",
            "step: 450, loss: 0.0029760212637484074\n",
            "step: 460, loss: 0.10907460004091263\n",
            "step: 470, loss: 0.014821324497461319\n",
            "step: 480, loss: 0.020936105400323868\n",
            "step: 490, loss: 0.006861590780317783\n",
            "step: 500, loss: 0.0045210979878902435\n",
            "step: 510, loss: 0.17379331588745117\n",
            "step: 520, loss: 0.07557747513055801\n",
            "step: 530, loss: 0.06734079122543335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9038010323791646, f1=0.8959020254357042, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06234800070524216\n",
            "step: 10, loss: 0.05198837071657181\n",
            "step: 20, loss: 0.009055341593921185\n",
            "step: 30, loss: 0.004390450660139322\n",
            "step: 40, loss: 0.018661612644791603\n",
            "step: 50, loss: 0.017178736627101898\n",
            "step: 60, loss: 0.13876129686832428\n",
            "step: 70, loss: 0.04946108162403107\n",
            "step: 80, loss: 0.003979919012635946\n",
            "step: 90, loss: 0.008862336166203022\n",
            "step: 100, loss: 0.0007377120782621205\n",
            "step: 110, loss: 0.0006393186049535871\n",
            "step: 120, loss: 0.00695380661636591\n",
            "step: 130, loss: 0.001359764952212572\n",
            "step: 140, loss: 0.005395587533712387\n",
            "step: 150, loss: 0.002214656677097082\n",
            "step: 160, loss: 0.01013004407286644\n",
            "step: 170, loss: 0.016901567578315735\n",
            "step: 180, loss: 0.0022610535379499197\n",
            "step: 190, loss: 0.003357032546773553\n",
            "step: 200, loss: 0.010566947050392628\n",
            "step: 210, loss: 0.09834110736846924\n",
            "step: 220, loss: 0.003159309271723032\n",
            "step: 230, loss: 0.01950533129274845\n",
            "step: 240, loss: 0.02693227306008339\n",
            "step: 250, loss: 0.00968747679144144\n",
            "step: 260, loss: 0.01632056199014187\n",
            "step: 270, loss: 0.057040125131607056\n",
            "step: 280, loss: 0.0706232413649559\n",
            "step: 290, loss: 0.12963725626468658\n",
            "step: 300, loss: 0.03659576550126076\n",
            "step: 310, loss: 0.0766153559088707\n",
            "step: 320, loss: 0.06268524378538132\n",
            "step: 330, loss: 0.022656038403511047\n",
            "step: 340, loss: 0.01889481209218502\n",
            "step: 350, loss: 0.015765925869345665\n",
            "step: 360, loss: 0.02804413251578808\n",
            "step: 370, loss: 0.04239439219236374\n",
            "step: 380, loss: 0.0210331492125988\n",
            "step: 390, loss: 0.020128212869167328\n",
            "step: 400, loss: 0.008174861781299114\n",
            "step: 410, loss: 0.052343688905239105\n",
            "step: 420, loss: 0.002475901274010539\n",
            "step: 430, loss: 0.019253859296441078\n",
            "step: 440, loss: 0.01793147623538971\n",
            "step: 450, loss: 0.11206533014774323\n",
            "step: 460, loss: 0.0006226467667147517\n",
            "step: 470, loss: 0.0010503209196031094\n",
            "step: 480, loss: 0.024347729980945587\n",
            "step: 490, loss: 0.0005402425304055214\n",
            "step: 500, loss: 0.08228352665901184\n",
            "step: 510, loss: 0.041682954877614975\n",
            "step: 520, loss: 0.10599327087402344\n",
            "step: 530, loss: 0.005637845024466515\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9002298850574711, f1=0.8948587308939323, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007972949533723295\n",
            "step: 10, loss: 0.014760933816432953\n",
            "step: 20, loss: 0.06419245898723602\n",
            "step: 30, loss: 0.01987736113369465\n",
            "step: 40, loss: 0.03309580311179161\n",
            "step: 50, loss: 0.001229586428962648\n",
            "step: 60, loss: 0.0010162824764847755\n",
            "step: 70, loss: 0.020677903667092323\n",
            "step: 80, loss: 0.06152312457561493\n",
            "step: 90, loss: 0.00202169013209641\n",
            "step: 100, loss: 0.008878345601260662\n",
            "step: 110, loss: 0.004423990845680237\n",
            "step: 120, loss: 0.0010813501430675387\n",
            "step: 130, loss: 0.042356766760349274\n",
            "step: 140, loss: 0.020086314529180527\n",
            "step: 150, loss: 0.0012762006372213364\n",
            "step: 160, loss: 0.0008261220064014196\n",
            "step: 170, loss: 0.007426067721098661\n",
            "step: 180, loss: 0.09918694198131561\n",
            "step: 190, loss: 0.003845100523903966\n",
            "step: 200, loss: 0.006690799258649349\n",
            "step: 210, loss: 0.024541044607758522\n",
            "step: 220, loss: 0.04905351623892784\n",
            "step: 230, loss: 0.0014237182913348079\n",
            "step: 240, loss: 0.076429083943367\n",
            "step: 250, loss: 0.005407672841101885\n",
            "step: 260, loss: 0.002900676103308797\n",
            "step: 270, loss: 0.04230000078678131\n",
            "step: 280, loss: 0.016899926587939262\n",
            "step: 290, loss: 0.002922364976257086\n",
            "step: 300, loss: 0.0382685624063015\n",
            "step: 310, loss: 0.01635373942553997\n",
            "step: 320, loss: 0.00094269507098943\n",
            "step: 330, loss: 0.0007973629399202764\n",
            "step: 340, loss: 0.030945373699069023\n",
            "step: 350, loss: 0.013318298384547234\n",
            "step: 360, loss: 0.004401795100420713\n",
            "step: 370, loss: 0.14290910959243774\n",
            "step: 380, loss: 0.00028863479383289814\n",
            "step: 390, loss: 0.009633122943341732\n",
            "step: 400, loss: 0.0090206079185009\n",
            "step: 410, loss: 0.0034457077272236347\n",
            "step: 420, loss: 0.019732261076569557\n",
            "step: 430, loss: 0.002081755083054304\n",
            "step: 440, loss: 0.005959570873528719\n",
            "step: 450, loss: 0.005505356937646866\n",
            "step: 460, loss: 0.008239807561039925\n",
            "step: 470, loss: 0.022039368748664856\n",
            "step: 480, loss: 0.000743542448617518\n",
            "step: 490, loss: 0.013436397537589073\n",
            "step: 500, loss: 0.0023093887139111757\n",
            "step: 510, loss: 0.0049505578354001045\n",
            "step: 520, loss: 0.003996171057224274\n",
            "step: 530, loss: 0.1359521895647049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9042995839112344, f1=0.903435468895079, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037182907108217478\n",
            "step: 10, loss: 0.012729565612971783\n",
            "step: 20, loss: 0.007523941341787577\n",
            "step: 30, loss: 0.012221158482134342\n",
            "step: 40, loss: 0.12593509256839752\n",
            "step: 50, loss: 0.041698817163705826\n",
            "step: 60, loss: 0.00888262502849102\n",
            "step: 70, loss: 0.0019695607479661703\n",
            "step: 80, loss: 0.02543737180531025\n",
            "step: 90, loss: 0.004851169418543577\n",
            "step: 100, loss: 0.007883034646511078\n",
            "step: 110, loss: 0.0018699390348047018\n",
            "step: 120, loss: 0.017813790589571\n",
            "step: 130, loss: 0.011069083586335182\n",
            "step: 140, loss: 0.009177698753774166\n",
            "step: 150, loss: 0.008160402998328209\n",
            "step: 160, loss: 0.00022064118820708245\n",
            "step: 170, loss: 0.007610981352627277\n",
            "step: 180, loss: 0.0006145818624645472\n",
            "step: 190, loss: 0.0005070537445135415\n",
            "step: 200, loss: 0.00037453469121828675\n",
            "step: 210, loss: 0.002852892968803644\n",
            "step: 220, loss: 0.0034845625050365925\n",
            "step: 230, loss: 0.0027860249392688274\n",
            "step: 240, loss: 0.003824575338512659\n",
            "step: 250, loss: 0.004058892372995615\n",
            "step: 260, loss: 0.0007870797999203205\n",
            "step: 270, loss: 0.02760918438434601\n",
            "step: 280, loss: 0.0013049846747890115\n",
            "step: 290, loss: 0.0031938087195158005\n",
            "step: 300, loss: 0.0034938184544444084\n",
            "step: 310, loss: 0.00045440898975357413\n",
            "step: 320, loss: 0.0008194519905373454\n",
            "step: 330, loss: 0.010260188952088356\n",
            "step: 340, loss: 0.015529262833297253\n",
            "step: 350, loss: 0.0017310602124780416\n",
            "step: 360, loss: 0.007567902095615864\n",
            "step: 370, loss: 0.0005248228553682566\n",
            "step: 380, loss: 0.003049260936677456\n",
            "step: 390, loss: 0.00023511532344855368\n",
            "step: 400, loss: 0.00039136281702667475\n",
            "step: 410, loss: 0.003475914243608713\n",
            "step: 420, loss: 0.00267986161634326\n",
            "step: 430, loss: 0.00035076745552942157\n",
            "step: 440, loss: 0.003701386973261833\n",
            "step: 450, loss: 0.0015118415467441082\n",
            "step: 460, loss: 0.001170757575891912\n",
            "step: 470, loss: 0.001630681101232767\n",
            "step: 480, loss: 0.30816447734832764\n",
            "step: 490, loss: 0.03320273384451866\n",
            "step: 500, loss: 0.00185650575440377\n",
            "step: 510, loss: 0.007479420863091946\n",
            "step: 520, loss: 0.0040651182644069195\n",
            "step: 530, loss: 0.007166590541601181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9001883239171375, f1=0.895536562203229, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008511016494594514\n",
            "step: 10, loss: 0.0007651773630641401\n",
            "step: 20, loss: 0.000336375436745584\n",
            "step: 30, loss: 0.0013359440490603447\n",
            "step: 40, loss: 0.00014967568858992308\n",
            "step: 50, loss: 0.20884504914283752\n",
            "step: 60, loss: 0.0005118579138070345\n",
            "step: 70, loss: 0.0003033093817066401\n",
            "step: 80, loss: 0.0013233220670372248\n",
            "step: 90, loss: 0.000981807243078947\n",
            "step: 100, loss: 0.03134314343333244\n",
            "step: 110, loss: 0.000521885056514293\n",
            "step: 120, loss: 0.003994151018559933\n",
            "step: 130, loss: 0.0032538643572479486\n",
            "step: 140, loss: 0.003124895039945841\n",
            "step: 150, loss: 0.002934791147708893\n",
            "step: 160, loss: 0.0063347285613417625\n",
            "step: 170, loss: 0.001716942759230733\n",
            "step: 180, loss: 0.0005752784782089293\n",
            "step: 190, loss: 0.004232028499245644\n",
            "step: 200, loss: 0.004835488740354776\n",
            "step: 210, loss: 0.028054941445589066\n",
            "step: 220, loss: 0.009874164126813412\n",
            "step: 230, loss: 0.019860904663801193\n",
            "step: 240, loss: 0.004469159059226513\n",
            "step: 250, loss: 0.11076641827821732\n",
            "step: 260, loss: 0.0006820371490903199\n",
            "step: 270, loss: 0.0034374764654785395\n",
            "step: 280, loss: 0.01667569950222969\n",
            "step: 290, loss: 0.006346337962895632\n",
            "step: 300, loss: 0.0012994520366191864\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 310, loss: 0.0006261547096073627\n",
            "step: 320, loss: 0.1633630394935608\n",
            "step: 330, loss: 0.009764902293682098\n",
            "step: 340, loss: 0.00016011891420930624\n",
            "step: 350, loss: 0.0015543471090495586\n",
            "step: 360, loss: 0.023372551426291466\n",
            "step: 370, loss: 0.0005627580685541034\n",
            "step: 380, loss: 0.001493207411840558\n",
            "step: 390, loss: 0.12202434986829758\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 0.018815677613019943\n",
            "step: 410, loss: 0.00373090710490942\n",
            "step: 420, loss: 0.0025288795586675406\n",
            "step: 430, loss: 0.04852035269141197\n",
            "step: 440, loss: 0.01801927387714386\n",
            "step: 450, loss: 0.0003461475425865501\n",
            "step: 460, loss: 0.0032709145452827215\n",
            "step: 470, loss: 0.0005362137453630567\n",
            "step: 480, loss: 0.004789689555764198\n",
            "step: 490, loss: 0.011539501138031483\n",
            "step: 500, loss: 0.0008638477884232998\n",
            "step: 510, loss: 0.00011654097033897415\n",
            "step: 520, loss: 0.0006416017422452569\n",
            "step: 530, loss: 0.017017286270856857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9076233183856501, f1=0.8935788055680288, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032365107908844948\n",
            "step: 10, loss: 0.00019944690575357527\n",
            "step: 20, loss: 0.001759332837536931\n",
            "step: 30, loss: 0.0015193312428891659\n",
            "step: 40, loss: 0.012072018347680569\n",
            "step: 50, loss: 0.0017829900607466698\n",
            "step: 60, loss: 0.05464686080813408\n",
            "step: 70, loss: 0.007356193382292986\n",
            "step: 80, loss: 0.039380915462970734\n",
            "step: 90, loss: 0.00011636658746283501\n",
            "step: 100, loss: 0.0022259384859353304\n",
            "step: 110, loss: 0.0004432521527633071\n",
            "step: 120, loss: 0.016123997047543526\n",
            "step: 130, loss: 0.00577665027230978\n",
            "step: 140, loss: 0.003330050967633724\n",
            "step: 150, loss: 0.015602074563503265\n",
            "step: 160, loss: 0.0004338570579420775\n",
            "step: 170, loss: 0.04258248955011368\n",
            "step: 180, loss: 0.008645497262477875\n",
            "step: 190, loss: 0.0028033254202455282\n",
            "step: 200, loss: 0.0029638963751494884\n",
            "step: 210, loss: 0.0002918118261732161\n",
            "step: 220, loss: 0.10331638157367706\n",
            "step: 230, loss: 0.00036752078449353576\n",
            "step: 240, loss: 0.007253124378621578\n",
            "step: 250, loss: 0.0010675139492377639\n",
            "step: 260, loss: 0.006060771178454161\n",
            "step: 270, loss: 0.0002636149001773447\n",
            "step: 280, loss: 0.00016564704128541052\n",
            "step: 290, loss: 0.07102254778146744\n",
            "step: 300, loss: 0.00026338110910728574\n",
            "step: 310, loss: 0.014186156913638115\n",
            "step: 320, loss: 0.0007174713537096977\n",
            "step: 330, loss: 0.0032761257607489824\n",
            "step: 340, loss: 0.0005808615242131054\n",
            "step: 350, loss: 0.0005431747413240373\n",
            "step: 360, loss: 0.00042667650268413126\n",
            "step: 370, loss: 0.0004574827034957707\n",
            "step: 380, loss: 0.00037434755358844995\n",
            "step: 390, loss: 0.013325809501111507\n",
            "step: 400, loss: 0.0124280396848917\n",
            "step: 410, loss: 0.00043853186070919037\n",
            "step: 420, loss: 6.8814551923424e-05\n",
            "step: 430, loss: 0.00015227538824547082\n",
            "step: 440, loss: 0.0017716799629852176\n",
            "step: 450, loss: 0.005371576640754938\n",
            "step: 460, loss: 0.003609459614381194\n",
            "step: 470, loss: 0.000173010746948421\n",
            "step: 480, loss: 0.00018139970779884607\n",
            "step: 490, loss: 0.011674979701638222\n",
            "step: 500, loss: 0.0005632582469843328\n",
            "step: 510, loss: 0.004834021907299757\n",
            "step: 520, loss: 0.00023929221788421273\n",
            "step: 530, loss: 0.00022783367603551596\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9095890410958903, f1=0.9025826914363388, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005105077871121466\n",
            "step: 10, loss: 5.563894228544086e-05\n",
            "step: 20, loss: 0.00011706569057423621\n",
            "step: 30, loss: 0.0021153653506189585\n",
            "step: 40, loss: 0.01188261341303587\n",
            "step: 50, loss: 0.00013607183063868433\n",
            "step: 60, loss: 9.072382817976177e-05\n",
            "step: 70, loss: 0.0002723304205574095\n",
            "step: 80, loss: 0.002893652068451047\n",
            "step: 90, loss: 0.0010397388832643628\n",
            "step: 100, loss: 3.5672372177941725e-05\n",
            "step: 110, loss: 0.00023837162007112056\n",
            "step: 120, loss: 0.006110957823693752\n",
            "step: 130, loss: 0.000170900413650088\n",
            "step: 140, loss: 0.00430263951420784\n",
            "step: 150, loss: 0.0009801835985854268\n",
            "step: 160, loss: 0.00463111000135541\n",
            "step: 170, loss: 0.0010024188086390495\n",
            "step: 180, loss: 0.00040944275679066777\n",
            "step: 190, loss: 0.0022566996049135923\n",
            "step: 200, loss: 0.002245072042569518\n",
            "step: 210, loss: 0.0019302316941320896\n",
            "step: 220, loss: 0.021049275994300842\n",
            "step: 230, loss: 0.004834457766264677\n",
            "step: 240, loss: 0.0001318646245636046\n",
            "step: 250, loss: 0.003760015359148383\n",
            "step: 260, loss: 0.0038232519291341305\n",
            "step: 270, loss: 0.00040252431062981486\n",
            "step: 280, loss: 0.00028231702162884176\n",
            "step: 290, loss: 9.507332288194448e-05\n",
            "step: 300, loss: 2.7696962206391618e-05\n",
            "step: 310, loss: 0.00553681468591094\n",
            "step: 320, loss: 0.0023212034720927477\n",
            "step: 330, loss: 0.0001531373563921079\n",
            "step: 340, loss: 0.00016505480743944645\n",
            "step: 350, loss: 0.00016280225827358663\n",
            "step: 360, loss: 0.00011231720418436453\n",
            "step: 370, loss: 0.005244504194706678\n",
            "step: 380, loss: 0.07772433757781982\n",
            "step: 390, loss: 0.00016143821994774044\n",
            "step: 400, loss: 0.0005821958766318858\n",
            "step: 410, loss: 0.0004140716337133199\n",
            "step: 420, loss: 0.005041997414082289\n",
            "step: 430, loss: 4.212757994537242e-05\n",
            "step: 440, loss: 0.0009216234320774674\n",
            "step: 450, loss: 0.00010441328777233139\n",
            "step: 460, loss: 0.0007442766218446195\n",
            "step: 470, loss: 0.00013011319970246404\n",
            "step: 480, loss: 0.055772650986909866\n",
            "step: 490, loss: 0.004985416773706675\n",
            "step: 500, loss: 0.013978583738207817\n",
            "step: 510, loss: 0.0015706740086898208\n",
            "step: 520, loss: 0.004182510077953339\n",
            "step: 530, loss: 0.00016234125359915197\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9050486336266791, f1=0.9015398973401773, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003101142356172204\n",
            "step: 10, loss: 0.0006546176737174392\n",
            "step: 20, loss: 9.999765461543575e-05\n",
            "step: 30, loss: 0.0016594532644376159\n",
            "step: 40, loss: 0.02530290000140667\n",
            "step: 50, loss: 6.641996878897771e-05\n",
            "step: 60, loss: 0.005653532221913338\n",
            "step: 70, loss: 0.00015149735554587096\n",
            "step: 80, loss: 3.189151539118029e-05\n",
            "step: 90, loss: 7.067211117828265e-05\n",
            "step: 100, loss: 0.002656580414623022\n",
            "step: 110, loss: 0.0015145629877224565\n",
            "step: 120, loss: 0.00012895680265501142\n",
            "step: 130, loss: 0.00043370318599045277\n",
            "step: 140, loss: 0.0010571208549663424\n",
            "step: 150, loss: 0.0005209152586758137\n",
            "step: 160, loss: 0.00013811388635076582\n",
            "step: 170, loss: 0.001199849764816463\n",
            "step: 180, loss: 0.00014659928274340928\n",
            "step: 190, loss: 0.0018779649399220943\n",
            "step: 200, loss: 0.005755431950092316\n",
            "step: 210, loss: 0.0007132099126465619\n",
            "step: 220, loss: 0.00032544031273573637\n",
            "step: 230, loss: 0.00019510285346768796\n",
            "step: 240, loss: 0.0002457060618326068\n",
            "step: 250, loss: 3.4767021134030074e-05\n",
            "step: 260, loss: 3.539656972861849e-05\n",
            "step: 270, loss: 0.0563511922955513\n",
            "step: 280, loss: 0.0005267431843094528\n",
            "step: 290, loss: 0.001742239692248404\n",
            "step: 300, loss: 0.0007662518764846027\n",
            "step: 310, loss: 0.00033361310488544405\n",
            "step: 320, loss: 8.009850716916844e-05\n",
            "step: 330, loss: 0.16233780980110168\n",
            "step: 340, loss: 0.017560040578246117\n",
            "step: 350, loss: 0.0001767689682310447\n",
            "step: 360, loss: 6.439201388275251e-05\n",
            "step: 370, loss: 0.00017784615920390934\n",
            "step: 380, loss: 0.00021712607122026384\n",
            "step: 390, loss: 0.00031933048740029335\n",
            "step: 400, loss: 0.0002620200684759766\n",
            "step: 410, loss: 0.000508829252794385\n",
            "step: 420, loss: 0.002755658235400915\n",
            "step: 430, loss: 0.00021036532416474074\n",
            "step: 440, loss: 0.00013593326730187982\n",
            "step: 450, loss: 0.00022937534959055483\n",
            "step: 460, loss: 0.067533940076828\n",
            "step: 470, loss: 8.144809544319287e-05\n",
            "step: 480, loss: 0.0019730087369680405\n",
            "step: 490, loss: 0.0007159641827456653\n",
            "step: 500, loss: 0.004441472701728344\n",
            "step: 510, loss: 0.05691832676529884\n",
            "step: 520, loss: 0.00041272060479968786\n",
            "step: 530, loss: 0.0014050152385607362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8971698113207547, f1=0.8949858088930936, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003421858709771186\n",
            "step: 10, loss: 5.43322748853825e-05\n",
            "step: 20, loss: 6.41515216557309e-05\n",
            "step: 30, loss: 9.081723692361265e-05\n",
            "step: 40, loss: 3.28447058564052e-05\n",
            "step: 50, loss: 0.04212482273578644\n",
            "step: 60, loss: 0.005819225683808327\n",
            "step: 70, loss: 0.05464474484324455\n",
            "step: 80, loss: 0.0033450149931013584\n",
            "step: 90, loss: 6.631098221987486e-05\n",
            "step: 100, loss: 0.002100010635331273\n",
            "step: 110, loss: 0.00016973933088593185\n",
            "step: 120, loss: 0.0001649553159950301\n",
            "step: 130, loss: 0.0014281811891123652\n",
            "step: 140, loss: 0.0006991797126829624\n",
            "step: 150, loss: 0.00014267167716752738\n",
            "step: 160, loss: 6.182058132253587e-05\n",
            "step: 170, loss: 0.0002662704500835389\n",
            "step: 180, loss: 3.427524279686622e-05\n",
            "step: 190, loss: 0.00032088192529045045\n",
            "step: 200, loss: 0.0018379648681730032\n",
            "step: 210, loss: 0.00044037983752787113\n",
            "step: 220, loss: 0.00012956385035067797\n",
            "step: 230, loss: 5.8630768762668595e-05\n",
            "step: 240, loss: 0.0018029877683147788\n",
            "step: 250, loss: 0.0005219884915277362\n",
            "step: 260, loss: 0.0005516815581358969\n",
            "step: 270, loss: 0.0005563748418353498\n",
            "step: 280, loss: 0.0002551450452301651\n",
            "step: 290, loss: 0.0006990496185608208\n",
            "step: 300, loss: 0.03498071804642677\n",
            "step: 310, loss: 0.0008538489346392453\n",
            "step: 320, loss: 0.00021983686019666493\n",
            "step: 330, loss: 0.05572747066617012\n",
            "step: 340, loss: 0.003133621532469988\n",
            "step: 350, loss: 0.0005003591650165617\n",
            "step: 360, loss: 0.00014396845654118806\n",
            "step: 370, loss: 0.000388278451282531\n",
            "step: 380, loss: 0.00449087331071496\n",
            "step: 390, loss: 0.0001569433807162568\n",
            "step: 400, loss: 0.0004727641644421965\n",
            "step: 410, loss: 0.0021464424207806587\n",
            "step: 420, loss: 0.0032314255367964506\n",
            "step: 430, loss: 0.0009870154317468405\n",
            "step: 440, loss: 0.005555338691920042\n",
            "step: 450, loss: 0.0025741246063262224\n",
            "step: 460, loss: 0.0002476507506798953\n",
            "step: 470, loss: 0.09123009443283081\n",
            "step: 480, loss: 0.002816811203956604\n",
            "step: 490, loss: 0.0006282461690716445\n",
            "step: 500, loss: 0.0001654636289458722\n",
            "step: 510, loss: 0.16847100853919983\n",
            "step: 520, loss: 7.419998291879892e-05\n",
            "step: 530, loss: 0.00025680594262667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9012915129151292, f1=0.8988452655889146, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010942780412733555\n",
            "step: 10, loss: 0.00011073511268477887\n",
            "step: 20, loss: 0.010790865868330002\n",
            "step: 30, loss: 0.00028261897386983037\n",
            "step: 40, loss: 0.008628936484456062\n",
            "step: 50, loss: 0.02447483316063881\n",
            "step: 60, loss: 0.003000261727720499\n",
            "step: 70, loss: 7.699218986090273e-05\n",
            "step: 80, loss: 0.0012227409752085805\n",
            "step: 90, loss: 0.013097535818815231\n",
            "step: 100, loss: 0.00020975717052351683\n",
            "step: 110, loss: 0.0009880707366392016\n",
            "step: 120, loss: 0.0005740839405916631\n",
            "step: 130, loss: 6.250439764698967e-05\n",
            "step: 140, loss: 3.231214213883504e-05\n",
            "step: 150, loss: 0.0014001975068822503\n",
            "step: 160, loss: 0.00025458980235271156\n",
            "step: 170, loss: 8.331897697644308e-05\n",
            "step: 180, loss: 6.137338641565293e-05\n",
            "step: 190, loss: 6.797032983740792e-05\n",
            "step: 200, loss: 4.0182621887652203e-05\n",
            "step: 210, loss: 0.000234450853895396\n",
            "step: 220, loss: 6.83814796502702e-05\n",
            "step: 230, loss: 8.88392241904512e-05\n",
            "step: 240, loss: 8.784091915003955e-05\n",
            "step: 250, loss: 5.947773752268404e-05\n",
            "step: 260, loss: 0.00019786288612522185\n",
            "step: 270, loss: 5.625648554996587e-05\n",
            "step: 280, loss: 5.8743211411638185e-05\n",
            "step: 290, loss: 3.844713864964433e-05\n",
            "step: 300, loss: 0.0027712977025657892\n",
            "step: 310, loss: 0.0018502651946619153\n",
            "step: 320, loss: 0.0014315992593765259\n",
            "step: 330, loss: 7.665391603950411e-05\n",
            "step: 340, loss: 0.0032246760092675686\n",
            "step: 350, loss: 4.369047383079305e-05\n",
            "step: 360, loss: 3.74099072359968e-05\n",
            "step: 370, loss: 0.0005471889744512737\n",
            "step: 380, loss: 0.0003864279424306005\n",
            "step: 390, loss: 0.000617943296674639\n",
            "step: 400, loss: 0.00011397709749871865\n",
            "step: 410, loss: 0.0011419332586228848\n",
            "step: 420, loss: 3.271088280598633e-05\n",
            "step: 430, loss: 0.00024529380607418716\n",
            "step: 440, loss: 0.0004512008454184979\n",
            "step: 450, loss: 0.0012543504126369953\n",
            "step: 460, loss: 0.0033637480810284615\n",
            "step: 470, loss: 4.770655505126342e-05\n",
            "step: 480, loss: 6.537104491144419e-05\n",
            "step: 490, loss: 0.00019773488747887313\n",
            "step: 500, loss: 8.868631266523153e-05\n",
            "step: 510, loss: 0.00021016197570133954\n",
            "step: 520, loss: 0.01437081303447485\n",
            "step: 530, loss: 0.000534422870259732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9008419083255379, f1=0.9037106622827618, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017449954757466912\n",
            "step: 10, loss: 0.00012578791938722134\n",
            "step: 20, loss: 0.0001476160396123305\n",
            "step: 30, loss: 0.0011098457034677267\n",
            "step: 40, loss: 0.06431430578231812\n",
            "step: 50, loss: 0.0003373190702404827\n",
            "step: 60, loss: 3.719900632859208e-05\n",
            "step: 70, loss: 0.00014838657807558775\n",
            "step: 80, loss: 0.0007865023799240589\n",
            "step: 90, loss: 0.00011348428233759478\n",
            "step: 100, loss: 0.027335643768310547\n",
            "step: 110, loss: 4.7076573537196964e-05\n",
            "step: 120, loss: 0.00015399089897982776\n",
            "step: 130, loss: 0.0021892639342695475\n",
            "step: 140, loss: 0.000418120704125613\n",
            "step: 150, loss: 5.814810356241651e-05\n",
            "step: 160, loss: 0.0004685079329647124\n",
            "step: 170, loss: 0.059866707772016525\n",
            "step: 180, loss: 0.00011336020543240011\n",
            "step: 190, loss: 0.0025438901502639055\n",
            "step: 200, loss: 0.00025321514112874866\n",
            "step: 210, loss: 0.005313960835337639\n",
            "step: 220, loss: 0.000352448842022568\n",
            "step: 230, loss: 0.0002445682475809008\n",
            "step: 240, loss: 0.0002194096305174753\n",
            "step: 250, loss: 0.00018308179278392345\n",
            "step: 260, loss: 0.0004804704512935132\n",
            "step: 270, loss: 0.0007361077005043626\n",
            "step: 280, loss: 3.8867423427291214e-05\n",
            "step: 290, loss: 0.0036214643623679876\n",
            "step: 300, loss: 0.0007682321011088789\n",
            "step: 310, loss: 0.1155904084444046\n",
            "step: 320, loss: 0.0005684178322553635\n",
            "step: 330, loss: 0.108134925365448\n",
            "step: 340, loss: 2.7033427613787353e-05\n",
            "step: 350, loss: 0.00010808102524606511\n",
            "step: 360, loss: 0.021369028836488724\n",
            "step: 370, loss: 7.014313450781628e-05\n",
            "step: 380, loss: 0.0005350597202777863\n",
            "step: 390, loss: 0.006325055845081806\n",
            "step: 400, loss: 0.00022584162070415914\n",
            "step: 410, loss: 6.413146911654621e-05\n",
            "step: 420, loss: 0.0011281461920589209\n",
            "step: 430, loss: 0.04107372835278511\n",
            "step: 440, loss: 0.03042558766901493\n",
            "step: 450, loss: 0.009025519713759422\n",
            "step: 460, loss: 0.003151091979816556\n",
            "step: 470, loss: 0.002686128718778491\n",
            "step: 480, loss: 7.973105675773695e-05\n",
            "step: 490, loss: 3.7931575207039714e-05\n",
            "step: 500, loss: 9.025672625284642e-05\n",
            "step: 510, loss: 0.08690924197435379\n",
            "step: 520, loss: 0.0002109623164869845\n",
            "step: 530, loss: 6.215509347384796e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9054680259499536, f1=0.906030855539972, best_f1=0.9001865671641792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.5551725381519645e-05\n",
            "step: 10, loss: 5.11924663442187e-05\n",
            "step: 20, loss: 0.0001458828483009711\n",
            "step: 30, loss: 0.0001904760574689135\n",
            "step: 40, loss: 0.04489728808403015\n",
            "step: 50, loss: 0.00222382671199739\n",
            "step: 60, loss: 0.00010589348676148802\n",
            "step: 70, loss: 4.5989625505171716e-05\n",
            "step: 80, loss: 0.0003422696318011731\n",
            "step: 90, loss: 4.491328581934795e-05\n",
            "step: 100, loss: 0.00010166219726670533\n",
            "step: 110, loss: 0.01399518083781004\n",
            "step: 120, loss: 0.0002736139576882124\n",
            "step: 130, loss: 0.050402771681547165\n",
            "step: 140, loss: 8.106456516543403e-05\n",
            "step: 150, loss: 0.020074572414159775\n",
            "step: 160, loss: 0.00021154488786123693\n",
            "step: 170, loss: 0.0003198180056642741\n",
            "step: 180, loss: 0.00026398972840979695\n",
            "step: 190, loss: 0.00028670148458331823\n",
            "step: 200, loss: 0.00012007168697891757\n",
            "step: 210, loss: 0.0002463503915350884\n",
            "step: 220, loss: 0.015564316883683205\n",
            "step: 230, loss: 0.0003271533059887588\n",
            "step: 240, loss: 0.021966177970170975\n",
            "step: 250, loss: 0.003978559281677008\n",
            "step: 260, loss: 0.00037121973582543433\n",
            "step: 270, loss: 0.00040731122135184705\n",
            "step: 280, loss: 0.00012399826664477587\n",
            "step: 290, loss: 5.995462925056927e-05\n",
            "step: 300, loss: 0.027766931802034378\n",
            "step: 310, loss: 0.0003127002564724535\n",
            "step: 320, loss: 0.03353894501924515\n",
            "step: 330, loss: 4.400426405481994e-05\n",
            "step: 340, loss: 7.888576510595158e-05\n",
            "step: 350, loss: 0.002224139403551817\n",
            "step: 360, loss: 0.0011509746545925736\n",
            "step: 370, loss: 6.206931720953435e-05\n",
            "step: 380, loss: 0.0004254210798535496\n",
            "step: 390, loss: 0.026730960234999657\n",
            "step: 400, loss: 0.0019082850776612759\n",
            "step: 410, loss: 0.00018683211237657815\n",
            "step: 420, loss: 0.00038486532866954803\n",
            "step: 430, loss: 0.0009460730943828821\n",
            "step: 440, loss: 0.007035884540528059\n",
            "step: 450, loss: 5.760123531217687e-05\n",
            "step: 460, loss: 0.0020875493064522743\n",
            "step: 470, loss: 0.0003655474865809083\n",
            "step: 480, loss: 3.8664162275381386e-05\n",
            "step: 490, loss: 0.0020943733397871256\n",
            "step: 500, loss: 0.00017200522415805608\n",
            "step: 510, loss: 0.00045385034172795713\n",
            "step: 520, loss: 3.207710324204527e-05\n",
            "step: 530, loss: 2.6270210582879372e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9061784897025172, f1=0.9049150206706478, best_f1=0.9001865671641792\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:32, 175.35it/s]\n",
            "load_f1 = 0.9089193015573384\n",
            "real_f1 = 0.9061032863849765\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6434cbd2-e9bc-4589-f431-fb37fbff5af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5603142976760864\n",
            "step: 10, loss: 0.36666882038116455\n",
            "step: 20, loss: 0.38503167033195496\n",
            "step: 30, loss: 0.2950451672077179\n",
            "step: 40, loss: 0.1882036030292511\n",
            "step: 50, loss: 0.43594881892204285\n",
            "step: 60, loss: 0.3095202147960663\n",
            "step: 70, loss: 0.2673228979110718\n",
            "step: 80, loss: 0.19683505594730377\n",
            "step: 90, loss: 0.48345255851745605\n",
            "step: 100, loss: 0.5528759956359863\n",
            "step: 110, loss: 0.2940233647823334\n",
            "step: 120, loss: 0.2764367163181305\n",
            "step: 130, loss: 0.3492065966129303\n",
            "step: 140, loss: 0.23523515462875366\n",
            "step: 150, loss: 0.2982051372528076\n",
            "step: 160, loss: 0.37269696593284607\n",
            "step: 170, loss: 0.3443286716938019\n",
            "step: 180, loss: 0.18641380965709686\n",
            "step: 190, loss: 0.2288391888141632\n",
            "step: 200, loss: 0.32214874029159546\n",
            "step: 210, loss: 0.19597022235393524\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4175824175824176, f1=0.4444444444444444, best_f1=0.4444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15005554258823395\n",
            "step: 10, loss: 0.3149722218513489\n",
            "step: 20, loss: 0.2975628077983856\n",
            "step: 30, loss: 0.3469946086406708\n",
            "step: 40, loss: 0.2540143132209778\n",
            "step: 50, loss: 0.16440320014953613\n",
            "step: 60, loss: 0.41868430376052856\n",
            "step: 70, loss: 0.132035031914711\n",
            "step: 80, loss: 0.3709642291069031\n",
            "step: 90, loss: 0.1883493959903717\n",
            "step: 100, loss: 0.07340341806411743\n",
            "step: 110, loss: 0.24218221008777618\n",
            "step: 120, loss: 0.13777215778827667\n",
            "step: 130, loss: 0.02989266626536846\n",
            "step: 140, loss: 0.28500109910964966\n",
            "step: 150, loss: 0.35375601053237915\n",
            "step: 160, loss: 0.27919405698776245\n",
            "step: 170, loss: 0.2021656483411789\n",
            "step: 180, loss: 0.2841641306877136\n",
            "step: 190, loss: 0.25904586911201477\n",
            "step: 200, loss: 0.05565865337848663\n",
            "step: 210, loss: 0.22819538414478302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.45396825396825397, f1=0.4871794871794871, best_f1=0.4871794871794871\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12037896364927292\n",
            "step: 10, loss: 0.2633518874645233\n",
            "step: 20, loss: 0.30321165919303894\n",
            "step: 30, loss: 0.258314311504364\n",
            "step: 40, loss: 0.10814705491065979\n",
            "step: 50, loss: 0.10449633002281189\n",
            "step: 60, loss: 0.27178695797920227\n",
            "step: 70, loss: 0.3842102885246277\n",
            "step: 80, loss: 0.14447258412837982\n",
            "step: 90, loss: 0.21645820140838623\n",
            "step: 100, loss: 0.2453952133655548\n",
            "step: 110, loss: 0.29357847571372986\n",
            "step: 120, loss: 0.24320314824581146\n",
            "step: 130, loss: 0.14202068746089935\n",
            "step: 140, loss: 0.18406878411769867\n",
            "step: 150, loss: 0.3414132297039032\n",
            "step: 160, loss: 0.06808461993932724\n",
            "step: 170, loss: 0.1644599735736847\n",
            "step: 180, loss: 0.17851293087005615\n",
            "step: 190, loss: 0.26943787932395935\n",
            "step: 200, loss: 0.18440011143684387\n",
            "step: 210, loss: 0.1546393781900406\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4689165186500889, f1=0.5533453887884268, best_f1=0.5533453887884268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1759122908115387\n",
            "step: 10, loss: 0.20628531277179718\n",
            "step: 20, loss: 0.24770981073379517\n",
            "step: 30, loss: 0.13634979724884033\n",
            "step: 40, loss: 0.10114428400993347\n",
            "step: 50, loss: 0.1808720976114273\n",
            "step: 60, loss: 0.29100295901298523\n",
            "step: 70, loss: 0.2807995676994324\n",
            "step: 80, loss: 0.1572612226009369\n",
            "step: 90, loss: 0.05437071621417999\n",
            "step: 100, loss: 0.2533729672431946\n",
            "step: 110, loss: 0.26518163084983826\n",
            "step: 120, loss: 0.12367476522922516\n",
            "step: 130, loss: 0.11531735211610794\n",
            "step: 140, loss: 0.21007320284843445\n",
            "step: 150, loss: 0.09725422412157059\n",
            "step: 160, loss: 0.04910008981823921\n",
            "step: 170, loss: 0.2625880241394043\n",
            "step: 180, loss: 0.5227002501487732\n",
            "step: 190, loss: 0.2567097544670105\n",
            "step: 200, loss: 0.20956899225711823\n",
            "step: 210, loss: 0.20550581812858582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.48822269807280516, f1=0.52, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15224336087703705\n",
            "step: 10, loss: 0.2100471407175064\n",
            "step: 20, loss: 0.31190839409828186\n",
            "step: 30, loss: 0.13531875610351562\n",
            "step: 40, loss: 0.03542237728834152\n",
            "step: 50, loss: 0.09262314438819885\n",
            "step: 60, loss: 0.08322994410991669\n",
            "step: 70, loss: 0.12809431552886963\n",
            "step: 80, loss: 0.12935705482959747\n",
            "step: 90, loss: 0.07993782311677933\n",
            "step: 100, loss: 0.021113138645887375\n",
            "step: 110, loss: 0.25112491846084595\n",
            "step: 120, loss: 0.10266582667827606\n",
            "step: 130, loss: 0.15696707367897034\n",
            "step: 140, loss: 0.16787973046302795\n",
            "step: 150, loss: 0.14178933203220367\n",
            "step: 160, loss: 0.1428210586309433\n",
            "step: 170, loss: 0.21911898255348206\n",
            "step: 180, loss: 0.10956905037164688\n",
            "step: 190, loss: 0.048308879137039185\n",
            "step: 200, loss: 0.18872523307800293\n",
            "step: 210, loss: 0.033164042979478836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.46399999999999997, f1=0.5296950240770465, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04855899140238762\n",
            "step: 10, loss: 0.11805061995983124\n",
            "step: 20, loss: 0.09175628423690796\n",
            "step: 30, loss: 0.06840275228023529\n",
            "step: 40, loss: 0.04336833208799362\n",
            "step: 50, loss: 0.007430600468069315\n",
            "step: 60, loss: 0.31951919198036194\n",
            "step: 70, loss: 0.01189244445413351\n",
            "step: 80, loss: 0.09869330376386642\n",
            "step: 90, loss: 0.21099241077899933\n",
            "step: 100, loss: 0.01919412985444069\n",
            "step: 110, loss: 0.13956309854984283\n",
            "step: 120, loss: 0.06998101621866226\n",
            "step: 130, loss: 0.14667953550815582\n",
            "step: 140, loss: 0.09124105423688889\n",
            "step: 150, loss: 0.034398917108774185\n",
            "step: 160, loss: 0.022212523967027664\n",
            "step: 170, loss: 0.25189194083213806\n",
            "step: 180, loss: 0.16062703728675842\n",
            "step: 190, loss: 0.10622561722993851\n",
            "step: 200, loss: 0.02558109723031521\n",
            "step: 210, loss: 0.07814373821020126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4745166959578207, f1=0.5065666041275797, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010611608624458313\n",
            "step: 10, loss: 0.08622027933597565\n",
            "step: 20, loss: 0.0312496330589056\n",
            "step: 30, loss: 0.008778839372098446\n",
            "step: 40, loss: 0.05726603418588638\n",
            "step: 50, loss: 0.17291942238807678\n",
            "step: 60, loss: 0.08082418143749237\n",
            "step: 70, loss: 0.0569775253534317\n",
            "step: 80, loss: 0.02636174112558365\n",
            "step: 90, loss: 0.044230084866285324\n",
            "step: 100, loss: 0.00945740845054388\n",
            "step: 110, loss: 0.24227473139762878\n",
            "step: 120, loss: 0.16955187916755676\n",
            "step: 130, loss: 0.027337871491909027\n",
            "step: 140, loss: 0.00849417969584465\n",
            "step: 150, loss: 0.057410866022109985\n",
            "step: 160, loss: 0.08801393955945969\n",
            "step: 170, loss: 0.048876870423555374\n",
            "step: 180, loss: 0.03823940455913544\n",
            "step: 190, loss: 0.16206659376621246\n",
            "step: 200, loss: 0.03679549694061279\n",
            "step: 210, loss: 0.10709558427333832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.45960502692998206, f1=0.5065176908752328, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.052634332329034805\n",
            "step: 10, loss: 0.03825884684920311\n",
            "step: 20, loss: 0.03217284381389618\n",
            "step: 30, loss: 0.01908666267991066\n",
            "step: 40, loss: 0.13267840445041656\n",
            "step: 50, loss: 0.025864819064736366\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.11006324738264084\n",
            "step: 70, loss: 0.13361303508281708\n",
            "step: 80, loss: 0.07967530190944672\n",
            "step: 90, loss: 0.03274637088179588\n",
            "step: 100, loss: 0.11509286612272263\n",
            "step: 110, loss: 0.11825942993164062\n",
            "step: 120, loss: 0.010085396468639374\n",
            "step: 130, loss: 0.0066104610450565815\n",
            "step: 140, loss: 0.016952181234955788\n",
            "step: 150, loss: 0.15196512639522552\n",
            "step: 160, loss: 0.07721802592277527\n",
            "step: 170, loss: 0.07482615113258362\n",
            "step: 180, loss: 0.0669717937707901\n",
            "step: 190, loss: 0.014927428215742111\n",
            "step: 200, loss: 0.012682772241532803\n",
            "step: 210, loss: 0.1549387276172638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.45901639344262296, f1=0.49019607843137253, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06883305311203003\n",
            "step: 10, loss: 0.09131260961294174\n",
            "step: 20, loss: 0.013441899791359901\n",
            "step: 30, loss: 0.03663768246769905\n",
            "step: 40, loss: 0.039064664393663406\n",
            "step: 50, loss: 0.041956428438425064\n",
            "step: 60, loss: 0.11185549944639206\n",
            "step: 70, loss: 0.009658575057983398\n",
            "step: 80, loss: 0.005921050440520048\n",
            "step: 90, loss: 0.02183365635573864\n",
            "step: 100, loss: 0.11215405911207199\n",
            "step: 110, loss: 0.09230181574821472\n",
            "step: 120, loss: 0.036027777940034866\n",
            "step: 130, loss: 0.053908493369817734\n",
            "step: 140, loss: 0.10002695024013519\n",
            "step: 150, loss: 0.22642090916633606\n",
            "step: 160, loss: 0.014289802871644497\n",
            "step: 170, loss: 0.03429523855447769\n",
            "step: 180, loss: 0.006571552250534296\n",
            "step: 190, loss: 0.003235441166907549\n",
            "step: 200, loss: 0.042923279106616974\n",
            "step: 210, loss: 0.010062080807983875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.4338235294117647, f1=0.48275862068965514, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.040368176996707916\n",
            "step: 10, loss: 0.013260009698569775\n",
            "step: 20, loss: 0.003030425636097789\n",
            "step: 30, loss: 0.15991009771823883\n",
            "step: 40, loss: 0.006496886257082224\n",
            "step: 50, loss: 0.0256675835698843\n",
            "step: 60, loss: 0.1452576220035553\n",
            "step: 70, loss: 0.10945295542478561\n",
            "step: 80, loss: 0.056387919932603836\n",
            "step: 90, loss: 0.05892856419086456\n",
            "step: 100, loss: 0.012852579355239868\n",
            "step: 110, loss: 0.06753519177436829\n",
            "step: 120, loss: 0.11205055564641953\n",
            "step: 130, loss: 0.016216447576880455\n",
            "step: 140, loss: 0.035694532096385956\n",
            "step: 150, loss: 0.10696498304605484\n",
            "step: 160, loss: 0.02652730792760849\n",
            "step: 170, loss: 0.03957125172019005\n",
            "step: 180, loss: 0.09493769705295563\n",
            "step: 190, loss: 0.1740482598543167\n",
            "step: 200, loss: 0.0018003208097070456\n",
            "step: 210, loss: 0.11529024690389633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4521072796934866, f1=0.46034816247582205, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00627094553783536\n",
            "step: 10, loss: 0.09059964120388031\n",
            "step: 20, loss: 0.04808780178427696\n",
            "step: 30, loss: 0.007028873544186354\n",
            "step: 40, loss: 0.008630679920315742\n",
            "step: 50, loss: 0.0035396211314946413\n",
            "step: 60, loss: 0.003811864648014307\n",
            "step: 70, loss: 0.016624849289655685\n",
            "step: 80, loss: 0.015919536352157593\n",
            "step: 90, loss: 0.005448623560369015\n",
            "step: 100, loss: 0.006698727607727051\n",
            "step: 110, loss: 0.03467273339629173\n",
            "step: 120, loss: 0.023654883727431297\n",
            "step: 130, loss: 0.0032660518772900105\n",
            "step: 140, loss: 0.021580561995506287\n",
            "step: 150, loss: 0.0007295591058209538\n",
            "step: 160, loss: 0.011786136776208878\n",
            "step: 170, loss: 0.002104173181578517\n",
            "step: 180, loss: 0.005981935653835535\n",
            "step: 190, loss: 0.0014771174173802137\n",
            "step: 200, loss: 0.004251736681908369\n",
            "step: 210, loss: 0.0010810426902025938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4671532846715329, f1=0.4789762340036563, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0415378212928772\n",
            "step: 10, loss: 0.003254712326452136\n",
            "step: 20, loss: 0.003959663677960634\n",
            "step: 30, loss: 0.004622709937393665\n",
            "step: 40, loss: 0.05495740473270416\n",
            "step: 50, loss: 0.0029369075782597065\n",
            "step: 60, loss: 0.22913233935832977\n",
            "step: 70, loss: 0.00041856139432638884\n",
            "step: 80, loss: 0.04698508232831955\n",
            "step: 90, loss: 0.026639223098754883\n",
            "step: 100, loss: 0.04777710139751434\n",
            "step: 110, loss: 0.004396986681967974\n",
            "step: 120, loss: 0.05010238662362099\n",
            "step: 130, loss: 0.0019125055987387896\n",
            "step: 140, loss: 0.002703011268749833\n",
            "step: 150, loss: 0.003968921024352312\n",
            "step: 160, loss: 0.006808183155953884\n",
            "step: 170, loss: 0.0029495935887098312\n",
            "step: 180, loss: 0.006117160432040691\n",
            "step: 190, loss: 0.10983137041330338\n",
            "step: 200, loss: 0.00923120230436325\n",
            "step: 210, loss: 0.004556534346193075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.4394250513347023, f1=0.4551148225469729, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027730893343687057\n",
            "step: 10, loss: 0.0004826000367756933\n",
            "step: 20, loss: 0.004234304651618004\n",
            "step: 30, loss: 0.022421278059482574\n",
            "step: 40, loss: 0.011976909823715687\n",
            "step: 50, loss: 0.0011160324793308973\n",
            "step: 60, loss: 0.0022672698833048344\n",
            "step: 70, loss: 0.020890282467007637\n",
            "step: 80, loss: 0.05663115903735161\n",
            "step: 90, loss: 0.0008259625174105167\n",
            "step: 100, loss: 0.054300207644701004\n",
            "step: 110, loss: 0.012136614881455898\n",
            "step: 120, loss: 0.00859695952385664\n",
            "step: 130, loss: 0.0029208653140813112\n",
            "step: 140, loss: 0.03630983829498291\n",
            "step: 150, loss: 0.003556040581315756\n",
            "step: 160, loss: 0.0035780721809715033\n",
            "step: 170, loss: 0.08355532586574554\n",
            "step: 180, loss: 0.015361586585640907\n",
            "step: 190, loss: 0.008698448538780212\n",
            "step: 200, loss: 0.11452627927064896\n",
            "step: 210, loss: 0.005531803239136934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.45833333333333337, f1=0.4448669201520913, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010976763442158699\n",
            "step: 10, loss: 0.01317105907946825\n",
            "step: 20, loss: 0.005117708817124367\n",
            "step: 30, loss: 0.0007260615238919854\n",
            "step: 40, loss: 0.03337622061371803\n",
            "step: 50, loss: 0.0023526481818407774\n",
            "step: 60, loss: 0.0076688374392688274\n",
            "step: 70, loss: 0.004217250272631645\n",
            "step: 80, loss: 0.002240107161924243\n",
            "step: 90, loss: 0.061534393578767776\n",
            "step: 100, loss: 0.007149806246161461\n",
            "step: 110, loss: 0.002641316968947649\n",
            "step: 120, loss: 0.0033065860625356436\n",
            "step: 130, loss: 0.004037213046103716\n",
            "step: 140, loss: 0.02834009937942028\n",
            "step: 150, loss: 0.008226846344769001\n",
            "step: 160, loss: 0.11648331582546234\n",
            "step: 170, loss: 0.0010018819011747837\n",
            "step: 180, loss: 0.06485776603221893\n",
            "step: 190, loss: 0.001863346784375608\n",
            "step: 200, loss: 0.08540981262922287\n",
            "step: 210, loss: 0.03362058848142624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.44313725490196076, f1=0.45940594059405937, best_f1=0.52\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009309621527791023\n",
            "step: 10, loss: 0.0017946595326066017\n",
            "step: 20, loss: 0.055928051471710205\n",
            "step: 30, loss: 0.05367287993431091\n",
            "step: 40, loss: 0.007627400103956461\n",
            "step: 50, loss: 0.0031206991989165545\n",
            "step: 60, loss: 0.054228488355875015\n",
            "step: 70, loss: 0.0031291248742491007\n",
            "step: 80, loss: 0.0053047118708491325\n",
            "step: 90, loss: 0.01986517384648323\n",
            "step: 100, loss: 0.001967281335964799\n",
            "step: 110, loss: 0.0013066749088466167\n",
            "step: 120, loss: 0.015611999668180943\n",
            "step: 130, loss: 0.010408393107354641\n",
            "step: 140, loss: 0.0014922904083505273\n",
            "step: 150, loss: 0.003149823285639286\n",
            "step: 160, loss: 0.0022829407826066017\n",
            "step: 170, loss: 0.0005138495471328497\n",
            "step: 180, loss: 0.0006991632981225848\n",
            "step: 190, loss: 0.01590416021645069\n",
            "step: 200, loss: 0.051853202283382416\n",
            "step: 210, loss: 0.018396664410829544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.4474708171206226, f1=0.45940594059405937, best_f1=0.52\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 241.71it/s]\n",
            "load_f1 = 0.48375451263537905\n",
            "real_f1 = 0.48363636363636364\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 176.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b824fa-d4c1-4ee9-d085-deb529ac6646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5476108193397522\n",
            "step: 10, loss: 0.3836674392223358\n",
            "step: 20, loss: 0.3001682758331299\n",
            "step: 30, loss: 0.40133601427078247\n",
            "step: 40, loss: 0.4373561441898346\n",
            "step: 50, loss: 0.3090874254703522\n",
            "step: 60, loss: 0.28447261452674866\n",
            "step: 70, loss: 0.2916218340396881\n",
            "step: 80, loss: 0.23680011928081512\n",
            "step: 90, loss: 0.29648295044898987\n",
            "step: 100, loss: 0.3198568820953369\n",
            "step: 110, loss: 0.3691559135913849\n",
            "step: 120, loss: 0.1119728535413742\n",
            "step: 130, loss: 0.18179108202457428\n",
            "step: 140, loss: 0.0702705830335617\n",
            "step: 150, loss: 0.18741412460803986\n",
            "step: 160, loss: 0.15221019089221954\n",
            "step: 170, loss: 0.24256330728530884\n",
            "step: 180, loss: 0.04553528502583504\n",
            "step: 190, loss: 0.2332042157649994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6601941747572816, f1=0.676470588235294, best_f1=0.676470588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20300674438476562\n",
            "step: 10, loss: 0.11758013814687729\n",
            "step: 20, loss: 0.2606046795845032\n",
            "step: 30, loss: 0.16066379845142365\n",
            "step: 40, loss: 0.13403992354869843\n",
            "step: 50, loss: 0.03786816820502281\n",
            "step: 60, loss: 0.18465930223464966\n",
            "step: 70, loss: 0.33036261796951294\n",
            "step: 80, loss: 0.11250382661819458\n",
            "step: 90, loss: 0.12546144425868988\n",
            "step: 100, loss: 0.010572666302323341\n",
            "step: 110, loss: 0.25642988085746765\n",
            "step: 120, loss: 0.23013979196548462\n",
            "step: 130, loss: 0.1875699758529663\n",
            "step: 140, loss: 0.1399373710155487\n",
            "step: 150, loss: 0.17611832916736603\n",
            "step: 160, loss: 0.1582515835762024\n",
            "step: 170, loss: 0.17331410944461823\n",
            "step: 180, loss: 0.10871793329715729\n",
            "step: 190, loss: 0.04819580912590027\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7532467532467532, f1=0.7604166666666666, best_f1=0.7604166666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16419516503810883\n",
            "step: 10, loss: 0.16401232779026031\n",
            "step: 20, loss: 0.06975893676280975\n",
            "step: 30, loss: 0.1209322139620781\n",
            "step: 40, loss: 0.1329733431339264\n",
            "step: 50, loss: 0.2773014307022095\n",
            "step: 60, loss: 0.07089672982692719\n",
            "step: 70, loss: 0.10307082533836365\n",
            "step: 80, loss: 0.05755574256181717\n",
            "step: 90, loss: 0.14585727453231812\n",
            "step: 100, loss: 0.11578354239463806\n",
            "step: 110, loss: 0.1090267226099968\n",
            "step: 120, loss: 0.08404242247343063\n",
            "step: 130, loss: 0.023577041923999786\n",
            "step: 140, loss: 0.019421571865677834\n",
            "step: 150, loss: 0.11229224503040314\n",
            "step: 160, loss: 0.012231813743710518\n",
            "step: 170, loss: 0.23007714748382568\n",
            "step: 180, loss: 0.029985293745994568\n",
            "step: 190, loss: 0.07583452761173248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7814207650273224, f1=0.7733333333333333, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04377760365605354\n",
            "step: 10, loss: 0.2085021734237671\n",
            "step: 20, loss: 0.15145501494407654\n",
            "step: 30, loss: 0.08476202189922333\n",
            "step: 40, loss: 0.14070159196853638\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.05450260266661644\n",
            "step: 60, loss: 0.18195828795433044\n",
            "step: 70, loss: 0.04446347802877426\n",
            "step: 80, loss: 0.22734051942825317\n",
            "step: 90, loss: 0.0748639702796936\n",
            "step: 100, loss: 0.12799988687038422\n",
            "step: 110, loss: 0.009716206230223179\n",
            "step: 120, loss: 0.054329708218574524\n",
            "step: 130, loss: 0.12533603608608246\n",
            "step: 140, loss: 0.0485055074095726\n",
            "step: 150, loss: 0.049716148525476456\n",
            "step: 160, loss: 0.004764493554830551\n",
            "step: 170, loss: 0.1873626559972763\n",
            "step: 180, loss: 0.08002225309610367\n",
            "step: 190, loss: 0.08023517578840256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7794871794871795, f1=0.7487179487179487, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05244222655892372\n",
            "step: 10, loss: 0.006302101071923971\n",
            "step: 20, loss: 0.06946009397506714\n",
            "step: 30, loss: 0.020600855350494385\n",
            "step: 40, loss: 0.07532258331775665\n",
            "step: 50, loss: 0.05168318748474121\n",
            "step: 60, loss: 0.0631629154086113\n",
            "step: 70, loss: 0.019197402521967888\n",
            "step: 80, loss: 0.003217745805159211\n",
            "step: 90, loss: 0.011946937069296837\n",
            "step: 100, loss: 0.0024607954546809196\n",
            "step: 110, loss: 0.0048562902957201\n",
            "step: 120, loss: 0.018508372828364372\n",
            "step: 130, loss: 0.04069555550813675\n",
            "step: 140, loss: 0.03775991499423981\n",
            "step: 150, loss: 0.11197112500667572\n",
            "step: 160, loss: 0.02218516729772091\n",
            "step: 170, loss: 0.004090507049113512\n",
            "step: 180, loss: 0.03712107241153717\n",
            "step: 190, loss: 0.10369753837585449\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7580645161290323, f1=0.724233983286908, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005001209210604429\n",
            "step: 10, loss: 0.008195621892809868\n",
            "step: 20, loss: 0.002566884970292449\n",
            "step: 30, loss: 0.0011932130437344313\n",
            "step: 40, loss: 0.08367675542831421\n",
            "step: 50, loss: 0.007955270819365978\n",
            "step: 60, loss: 0.0008331738645210862\n",
            "step: 70, loss: 0.021416518837213516\n",
            "step: 80, loss: 0.12819284200668335\n",
            "step: 90, loss: 0.036245640367269516\n",
            "step: 100, loss: 0.00047504555550403893\n",
            "step: 110, loss: 0.025891665369272232\n",
            "step: 120, loss: 0.1206723153591156\n",
            "step: 130, loss: 0.0043769218027591705\n",
            "step: 140, loss: 0.011760788969695568\n",
            "step: 150, loss: 0.050005920231342316\n",
            "step: 160, loss: 0.0772458016872406\n",
            "step: 170, loss: 0.04128820821642876\n",
            "step: 180, loss: 0.025801455602049828\n",
            "step: 190, loss: 0.004128210712224245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7479224376731302, f1=0.7094972067039106, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010088887996971607\n",
            "step: 10, loss: 0.015290606766939163\n",
            "step: 20, loss: 0.04917389154434204\n",
            "step: 30, loss: 0.029755182564258575\n",
            "step: 40, loss: 0.002004713285714388\n",
            "step: 50, loss: 0.05594419687986374\n",
            "step: 60, loss: 0.02261979877948761\n",
            "step: 70, loss: 0.0013580848462879658\n",
            "step: 80, loss: 0.11422105878591537\n",
            "step: 90, loss: 0.001844294834882021\n",
            "step: 100, loss: 0.0003935388522222638\n",
            "step: 110, loss: 0.024563593789935112\n",
            "step: 120, loss: 0.006569486577063799\n",
            "step: 130, loss: 0.07802609354257584\n",
            "step: 140, loss: 0.024180583655834198\n",
            "step: 150, loss: 0.03248148784041405\n",
            "step: 160, loss: 0.3143850862979889\n",
            "step: 170, loss: 0.03907296061515808\n",
            "step: 180, loss: 0.026105228811502457\n",
            "step: 190, loss: 0.009364535100758076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7513513513513514, f1=0.747191011235955, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015821419656276703\n",
            "step: 10, loss: 0.03463771566748619\n",
            "step: 20, loss: 0.11432412266731262\n",
            "step: 30, loss: 0.010424422100186348\n",
            "step: 40, loss: 0.0008687812951393425\n",
            "step: 50, loss: 0.0012628216063603759\n",
            "step: 60, loss: 0.001089634490199387\n",
            "step: 70, loss: 0.007854178547859192\n",
            "step: 80, loss: 0.0013471698621287942\n",
            "step: 90, loss: 0.07500284910202026\n",
            "step: 100, loss: 0.02026766538619995\n",
            "step: 110, loss: 0.0005845400155521929\n",
            "step: 120, loss: 0.004496371373534203\n",
            "step: 130, loss: 0.0029098966624587774\n",
            "step: 140, loss: 0.06851150840520859\n",
            "step: 150, loss: 0.009326175786554813\n",
            "step: 160, loss: 0.0010405994253233075\n",
            "step: 170, loss: 0.003186864545568824\n",
            "step: 180, loss: 0.0030412091873586178\n",
            "step: 190, loss: 0.010820893570780754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7556675062972291, f1=0.7512953367875648, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009421895956620574\n",
            "step: 10, loss: 0.02320513315498829\n",
            "step: 20, loss: 0.06611136347055435\n",
            "step: 30, loss: 0.002143907593563199\n",
            "step: 40, loss: 0.00022837132564745843\n",
            "step: 50, loss: 0.11427812278270721\n",
            "step: 60, loss: 0.0004939265782013535\n",
            "step: 70, loss: 0.000693333859089762\n",
            "step: 80, loss: 0.0007930430001579225\n",
            "step: 90, loss: 0.005953256506472826\n",
            "step: 100, loss: 0.01577134244143963\n",
            "step: 110, loss: 0.025030061602592468\n",
            "step: 120, loss: 0.06828510016202927\n",
            "step: 130, loss: 0.0027623118367046118\n",
            "step: 140, loss: 0.014109320007264614\n",
            "step: 150, loss: 0.010266635566949844\n",
            "step: 160, loss: 0.0020606659818440676\n",
            "step: 170, loss: 0.00042318477062508464\n",
            "step: 180, loss: 0.1841755211353302\n",
            "step: 190, loss: 0.0059188553132116795\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7708333333333334, f1=0.7405405405405405, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020993612706661224\n",
            "step: 10, loss: 0.12149467319250107\n",
            "step: 20, loss: 0.000797910732217133\n",
            "step: 30, loss: 0.0015981529140844941\n",
            "step: 40, loss: 0.004744702950119972\n",
            "step: 50, loss: 0.004650600720196962\n",
            "step: 60, loss: 0.00035837452742271125\n",
            "step: 70, loss: 0.0006562244379892945\n",
            "step: 80, loss: 0.0076970504596829414\n",
            "step: 90, loss: 0.0018120176391676068\n",
            "step: 100, loss: 0.014616432599723339\n",
            "step: 110, loss: 0.001116889645345509\n",
            "step: 120, loss: 0.014291157945990562\n",
            "step: 130, loss: 0.0011876170756295323\n",
            "step: 140, loss: 0.04453081637620926\n",
            "step: 150, loss: 0.017326144501566887\n",
            "step: 160, loss: 0.001827784115448594\n",
            "step: 170, loss: 0.0013394642155617476\n",
            "step: 180, loss: 0.0011355889728292823\n",
            "step: 190, loss: 0.002398004289716482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7553191489361701, f1=0.7547169811320755, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004109145607799292\n",
            "step: 10, loss: 0.0028224969282746315\n",
            "step: 20, loss: 0.00222354126162827\n",
            "step: 30, loss: 0.010742150247097015\n",
            "step: 40, loss: 0.0014451813185587525\n",
            "step: 50, loss: 0.06474462896585464\n",
            "step: 60, loss: 0.001863220240920782\n",
            "step: 70, loss: 0.0008168938802555203\n",
            "step: 80, loss: 0.0009600811172276735\n",
            "step: 90, loss: 0.001139993080869317\n",
            "step: 100, loss: 0.0004675397649407387\n",
            "step: 110, loss: 0.001239714794792235\n",
            "step: 120, loss: 0.001596861518919468\n",
            "step: 130, loss: 0.007260431069880724\n",
            "step: 140, loss: 0.0008446322754025459\n",
            "step: 150, loss: 0.0003531461989041418\n",
            "step: 160, loss: 0.0009096928406506777\n",
            "step: 170, loss: 0.0023797890171408653\n",
            "step: 180, loss: 0.0011529254261404276\n",
            "step: 190, loss: 0.0017287717200815678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7611548556430445, f1=0.753315649867374, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00032473832834511995\n",
            "step: 10, loss: 0.0011820318177342415\n",
            "step: 20, loss: 0.0009519311133772135\n",
            "step: 30, loss: 0.00150700518861413\n",
            "step: 40, loss: 0.027057308703660965\n",
            "step: 50, loss: 0.0036110880319029093\n",
            "step: 60, loss: 0.0010784954065456986\n",
            "step: 70, loss: 0.004510669969022274\n",
            "step: 80, loss: 0.0011713167186826468\n",
            "step: 90, loss: 0.0021921349689364433\n",
            "step: 100, loss: 0.0008794492459855974\n",
            "step: 110, loss: 0.02341325767338276\n",
            "step: 120, loss: 0.0007297990960069001\n",
            "step: 130, loss: 0.01702912151813507\n",
            "step: 140, loss: 0.10742849856615067\n",
            "step: 150, loss: 0.0008280125912278891\n",
            "step: 160, loss: 0.000409287546062842\n",
            "step: 170, loss: 0.0008697360171936452\n",
            "step: 180, loss: 0.001482417923398316\n",
            "step: 190, loss: 0.0018127256771549582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7613636363636365, f1=0.7485380116959065, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015712716849520802\n",
            "step: 10, loss: 0.005291924811899662\n",
            "step: 20, loss: 0.003906567115336657\n",
            "step: 30, loss: 0.00786955002695322\n",
            "step: 40, loss: 0.0007526496774517\n",
            "step: 50, loss: 0.026172934100031853\n",
            "step: 60, loss: 0.0022299890406429768\n",
            "step: 70, loss: 0.003919829614460468\n",
            "step: 80, loss: 0.0022891336120665073\n",
            "step: 90, loss: 0.004459437448531389\n",
            "step: 100, loss: 0.0004928825655952096\n",
            "step: 110, loss: 0.0006648823618888855\n",
            "step: 120, loss: 0.0005080448463559151\n",
            "step: 130, loss: 0.0003290420863777399\n",
            "step: 140, loss: 0.0002829233999364078\n",
            "step: 150, loss: 0.00042032450437545776\n",
            "step: 160, loss: 0.001810411922633648\n",
            "step: 170, loss: 0.010923103429377079\n",
            "step: 180, loss: 0.0005933705833740532\n",
            "step: 190, loss: 0.0022968638222664595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7660668380462724, f1=0.7591623036649214, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018794931238517165\n",
            "step: 10, loss: 0.0018351763719692826\n",
            "step: 20, loss: 0.0017049201996996999\n",
            "step: 30, loss: 0.0004231571510899812\n",
            "step: 40, loss: 0.0006246128468774259\n",
            "step: 50, loss: 0.0008170484798029065\n",
            "step: 60, loss: 0.00033684720983728766\n",
            "step: 70, loss: 0.000305190565995872\n",
            "step: 80, loss: 0.0007633411441929638\n",
            "step: 90, loss: 0.0004981064703315496\n",
            "step: 100, loss: 0.0035376872401684523\n",
            "step: 110, loss: 0.0005345772369764745\n",
            "step: 120, loss: 0.0011341789504513144\n",
            "step: 130, loss: 0.001996226143091917\n",
            "step: 140, loss: 0.0009290472371503711\n",
            "step: 150, loss: 0.001075263018719852\n",
            "step: 160, loss: 0.007548003923147917\n",
            "step: 170, loss: 0.18062669038772583\n",
            "step: 180, loss: 0.0012853057123720646\n",
            "step: 190, loss: 0.000696102564688772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7657430730478589, f1=0.7544303797468355, best_f1=0.7733333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0216436218470335\n",
            "step: 10, loss: 0.019080236554145813\n",
            "step: 20, loss: 0.032796528190374374\n",
            "step: 30, loss: 0.003912733867764473\n",
            "step: 40, loss: 0.00032225658651441336\n",
            "step: 50, loss: 0.00041459081694483757\n",
            "step: 60, loss: 0.033542752265930176\n",
            "step: 70, loss: 0.01633651740849018\n",
            "step: 80, loss: 0.01926889270544052\n",
            "step: 90, loss: 0.0026547221932560205\n",
            "step: 100, loss: 0.0006079343729652464\n",
            "step: 110, loss: 0.003648376790806651\n",
            "step: 120, loss: 0.0018224997911602259\n",
            "step: 130, loss: 0.0003779701073653996\n",
            "step: 140, loss: 0.002320900559425354\n",
            "step: 150, loss: 0.0012834996450692415\n",
            "step: 160, loss: 0.00039714283775538206\n",
            "step: 170, loss: 0.0006548103410750628\n",
            "step: 180, loss: 0.0005369610153138638\n",
            "step: 190, loss: 0.003050813917070627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7700258397932817, f1=0.7539267015706806, best_f1=0.7733333333333333\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 160.19it/s]\n",
            "load_f1 = 0.7214854111405835\n",
            "real_f1 = 0.7015706806282722\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 176.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8caaa494-486d-46c9-aacb-111a6f6af179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6165744662284851\n",
            "step: 10, loss: 0.3687102496623993\n",
            "step: 20, loss: 0.30875706672668457\n",
            "step: 30, loss: 0.3907177150249481\n",
            "step: 40, loss: 0.28262069821357727\n",
            "step: 50, loss: 0.25831300020217896\n",
            "step: 60, loss: 0.24509304761886597\n",
            "step: 70, loss: 0.3771314322948456\n",
            "step: 80, loss: 0.3846713602542877\n",
            "step: 90, loss: 0.26410335302352905\n",
            "step: 100, loss: 0.2664520740509033\n",
            "step: 110, loss: 0.25776904821395874\n",
            "step: 120, loss: 0.09186646342277527\n",
            "step: 130, loss: 0.06763725727796555\n",
            "step: 140, loss: 0.25295522809028625\n",
            "step: 150, loss: 0.35514819622039795\n",
            "step: 160, loss: 0.164027139544487\n",
            "step: 170, loss: 0.2913610339164734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6652267818574514, f1=0.6098081023454158, best_f1=0.6098081023454158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15148545801639557\n",
            "step: 10, loss: 0.13722816109657288\n",
            "step: 20, loss: 0.05376318842172623\n",
            "step: 30, loss: 0.20245185494422913\n",
            "step: 40, loss: 0.11500225216150284\n",
            "step: 50, loss: 0.12971222400665283\n",
            "step: 60, loss: 0.3542972803115845\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.08057165145874023\n",
            "step: 80, loss: 0.13726577162742615\n",
            "step: 90, loss: 0.11182127147912979\n",
            "step: 100, loss: 0.19807040691375732\n",
            "step: 110, loss: 0.10835840553045273\n",
            "step: 120, loss: 0.10697413235902786\n",
            "step: 130, loss: 0.06557872891426086\n",
            "step: 140, loss: 0.2827779948711395\n",
            "step: 150, loss: 0.1896727979183197\n",
            "step: 160, loss: 0.13454435765743256\n",
            "step: 170, loss: 0.09140650928020477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7182044887780548, f1=0.6898263027295285, best_f1=0.6898263027295285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1318625807762146\n",
            "step: 10, loss: 0.19465714693069458\n",
            "step: 20, loss: 0.024529553949832916\n",
            "step: 30, loss: 0.07818819582462311\n",
            "step: 40, loss: 0.11272766441106796\n",
            "step: 50, loss: 0.11966900527477264\n",
            "step: 60, loss: 0.25148993730545044\n",
            "step: 70, loss: 0.2067977339029312\n",
            "step: 80, loss: 0.09210427105426788\n",
            "step: 90, loss: 0.13573536276817322\n",
            "step: 100, loss: 0.008174563758075237\n",
            "step: 110, loss: 0.20702724158763885\n",
            "step: 120, loss: 0.0815286636352539\n",
            "step: 130, loss: 0.1609739363193512\n",
            "step: 140, loss: 0.10735742747783661\n",
            "step: 150, loss: 0.026757989078760147\n",
            "step: 160, loss: 0.09709354490041733\n",
            "step: 170, loss: 0.0693373903632164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7354497354497355, f1=0.731829573934837, best_f1=0.731829573934837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007734957616776228\n",
            "step: 10, loss: 0.12581120431423187\n",
            "step: 20, loss: 0.1781364381313324\n",
            "step: 30, loss: 0.11441954970359802\n",
            "step: 40, loss: 0.010684964247047901\n",
            "step: 50, loss: 0.2994500994682312\n",
            "step: 60, loss: 0.14808911085128784\n",
            "step: 70, loss: 0.11398173123598099\n",
            "step: 80, loss: 0.08794277906417847\n",
            "step: 90, loss: 0.09174685180187225\n",
            "step: 100, loss: 0.28627994656562805\n",
            "step: 110, loss: 0.09272298961877823\n",
            "step: 120, loss: 0.04605848714709282\n",
            "step: 130, loss: 0.18176348507404327\n",
            "step: 140, loss: 0.09850011765956879\n",
            "step: 150, loss: 0.17477761209011078\n",
            "step: 160, loss: 0.08763975650072098\n",
            "step: 170, loss: 0.012660828419029713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7342465753424658, f1=0.7183462532299741, best_f1=0.731829573934837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07286972552537918\n",
            "step: 10, loss: 0.014609753154218197\n",
            "step: 20, loss: 0.04111155495047569\n",
            "step: 30, loss: 0.14772649109363556\n",
            "step: 40, loss: 0.06634248793125153\n",
            "step: 50, loss: 0.24812881648540497\n",
            "step: 60, loss: 0.06567411124706268\n",
            "step: 70, loss: 0.45413103699684143\n",
            "step: 80, loss: 0.1427159607410431\n",
            "step: 90, loss: 0.1188163235783577\n",
            "step: 100, loss: 0.06915626674890518\n",
            "step: 110, loss: 0.13011394441127777\n",
            "step: 120, loss: 0.022548966109752655\n",
            "step: 130, loss: 0.13299289345741272\n",
            "step: 140, loss: 0.04807819798588753\n",
            "step: 150, loss: 0.0728679746389389\n",
            "step: 160, loss: 0.08566264063119888\n",
            "step: 170, loss: 0.21110357344150543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7095115681233933, f1=0.712871287128713, best_f1=0.731829573934837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006224621552973986\n",
            "step: 10, loss: 0.04980183392763138\n",
            "step: 20, loss: 0.005832298658788204\n",
            "step: 30, loss: 0.05749956890940666\n",
            "step: 40, loss: 0.00743865454569459\n",
            "step: 50, loss: 0.18002742528915405\n",
            "step: 60, loss: 0.04559442028403282\n",
            "step: 70, loss: 0.11970579624176025\n",
            "step: 80, loss: 0.12196437269449234\n",
            "step: 90, loss: 0.1497727483510971\n",
            "step: 100, loss: 0.0942973792552948\n",
            "step: 110, loss: 0.003443132620304823\n",
            "step: 120, loss: 0.04664458706974983\n",
            "step: 130, loss: 0.04041968286037445\n",
            "step: 140, loss: 0.021839605644345284\n",
            "step: 150, loss: 0.02791237086057663\n",
            "step: 160, loss: 0.11328544467687607\n",
            "step: 170, loss: 0.023008624091744423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7440633245382585, f1=0.6812652068126521, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003319420153275132\n",
            "step: 10, loss: 0.015664134174585342\n",
            "step: 20, loss: 0.001913935411721468\n",
            "step: 30, loss: 0.010261589661240578\n",
            "step: 40, loss: 0.02140977792441845\n",
            "step: 50, loss: 0.03734055534005165\n",
            "step: 60, loss: 0.055504582822322845\n",
            "step: 70, loss: 0.01582157239317894\n",
            "step: 80, loss: 0.13666388392448425\n",
            "step: 90, loss: 0.005818639416247606\n",
            "step: 100, loss: 0.061955131590366364\n",
            "step: 110, loss: 0.11575474590063095\n",
            "step: 120, loss: 0.03437570482492447\n",
            "step: 130, loss: 0.1401916742324829\n",
            "step: 140, loss: 0.023056760430336\n",
            "step: 150, loss: 0.027978813275694847\n",
            "step: 160, loss: 0.03312402591109276\n",
            "step: 170, loss: 0.09159255027770996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7219251336898396, f1=0.6911764705882352, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018802262842655182\n",
            "step: 10, loss: 0.007635799236595631\n",
            "step: 20, loss: 0.0010000471957027912\n",
            "step: 30, loss: 0.03664612025022507\n",
            "step: 40, loss: 0.004204050172120333\n",
            "step: 50, loss: 0.017261000350117683\n",
            "step: 60, loss: 0.019608039408922195\n",
            "step: 70, loss: 0.030800271779298782\n",
            "step: 80, loss: 0.013370418921113014\n",
            "step: 90, loss: 0.04980875551700592\n",
            "step: 100, loss: 0.07915966957807541\n",
            "step: 110, loss: 0.04657420143485069\n",
            "step: 120, loss: 0.0016619381494820118\n",
            "step: 130, loss: 0.009432741440832615\n",
            "step: 140, loss: 0.013161428272724152\n",
            "step: 150, loss: 0.05808284133672714\n",
            "step: 160, loss: 0.006525831762701273\n",
            "step: 170, loss: 0.04723585024476051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7146666666666667, f1=0.6865671641791046, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0710742250084877\n",
            "step: 10, loss: 0.017867859452962875\n",
            "step: 20, loss: 0.007701158989220858\n",
            "step: 30, loss: 0.03673524409532547\n",
            "step: 40, loss: 0.0024990695528686047\n",
            "step: 50, loss: 0.0012678081402555108\n",
            "step: 60, loss: 0.004003297071903944\n",
            "step: 70, loss: 0.07424932718276978\n",
            "step: 80, loss: 0.02390962466597557\n",
            "step: 90, loss: 0.04906357079744339\n",
            "step: 100, loss: 0.001448155613616109\n",
            "step: 110, loss: 0.001595428679138422\n",
            "step: 120, loss: 0.0013137178029865026\n",
            "step: 130, loss: 0.020175063982605934\n",
            "step: 140, loss: 0.00869427714496851\n",
            "step: 150, loss: 0.0731622725725174\n",
            "step: 160, loss: 0.07043398171663284\n",
            "step: 170, loss: 0.026775088161230087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6963788300835654, f1=0.6649484536082474, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0842069536447525\n",
            "step: 10, loss: 0.0010173845803365111\n",
            "step: 20, loss: 0.03575995936989784\n",
            "step: 30, loss: 0.005582205951213837\n",
            "step: 40, loss: 0.0008652857504785061\n",
            "step: 50, loss: 0.04664987325668335\n",
            "step: 60, loss: 0.002319718012586236\n",
            "step: 70, loss: 0.0027914054226130247\n",
            "step: 80, loss: 0.006558613386005163\n",
            "step: 90, loss: 0.0014594817766919732\n",
            "step: 100, loss: 0.0003113462298642844\n",
            "step: 110, loss: 0.007500129286199808\n",
            "step: 120, loss: 0.034442488104104996\n",
            "step: 130, loss: 0.05558916553854942\n",
            "step: 140, loss: 0.03560260310769081\n",
            "step: 150, loss: 0.010408308357000351\n",
            "step: 160, loss: 0.038295019418001175\n",
            "step: 170, loss: 0.008549480699002743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7142857142857143, f1=0.6960784313725491, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11123666167259216\n",
            "step: 10, loss: 0.020825659856200218\n",
            "step: 20, loss: 0.02539539523422718\n",
            "step: 30, loss: 0.0006682202802039683\n",
            "step: 40, loss: 0.058161456137895584\n",
            "step: 50, loss: 0.008157548494637012\n",
            "step: 60, loss: 0.010536868125200272\n",
            "step: 70, loss: 0.0083592738956213\n",
            "step: 80, loss: 0.0015898304991424084\n",
            "step: 90, loss: 0.002218155423179269\n",
            "step: 100, loss: 0.009419032372534275\n",
            "step: 110, loss: 0.006519257556647062\n",
            "step: 120, loss: 0.0037038701120764017\n",
            "step: 130, loss: 0.0052422937005758286\n",
            "step: 140, loss: 0.0005412446917034686\n",
            "step: 150, loss: 0.08565006405115128\n",
            "step: 160, loss: 0.0051059904508292675\n",
            "step: 170, loss: 0.01780877821147442\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6927374301675977, f1=0.6700507614213198, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0045167249627411366\n",
            "step: 10, loss: 0.001876979717053473\n",
            "step: 20, loss: 0.005741169676184654\n",
            "step: 30, loss: 0.007355926092714071\n",
            "step: 40, loss: 0.006106162443757057\n",
            "step: 50, loss: 0.025686083361506462\n",
            "step: 60, loss: 0.054832424968481064\n",
            "step: 70, loss: 0.013594276271760464\n",
            "step: 80, loss: 0.0009269932634197176\n",
            "step: 90, loss: 0.01857070066034794\n",
            "step: 100, loss: 0.006994871888309717\n",
            "step: 110, loss: 0.017931032925844193\n",
            "step: 120, loss: 0.0074211303144693375\n",
            "step: 130, loss: 0.014628888107836246\n",
            "step: 140, loss: 0.061035312712192535\n",
            "step: 150, loss: 0.1947575956583023\n",
            "step: 160, loss: 0.005065571051090956\n",
            "step: 170, loss: 0.0005206321948207915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6943765281173595, f1=0.6915887850467289, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015606630593538284\n",
            "step: 10, loss: 0.11610884964466095\n",
            "step: 20, loss: 0.004887885879725218\n",
            "step: 30, loss: 0.004722590558230877\n",
            "step: 40, loss: 0.045718129724264145\n",
            "step: 50, loss: 0.003291957313194871\n",
            "step: 60, loss: 0.007730313576757908\n",
            "step: 70, loss: 0.004666922148317099\n",
            "step: 80, loss: 0.00181186490226537\n",
            "step: 90, loss: 0.0068716066889464855\n",
            "step: 100, loss: 0.0019346955232322216\n",
            "step: 110, loss: 0.0006537640583701432\n",
            "step: 120, loss: 0.009745887480676174\n",
            "step: 130, loss: 0.0010849963873624802\n",
            "step: 140, loss: 0.12898509204387665\n",
            "step: 150, loss: 0.0010601230897009373\n",
            "step: 160, loss: 0.011647433042526245\n",
            "step: 170, loss: 0.02677142061293125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.681948424068768, f1=0.6927083333333334, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003659407142549753\n",
            "step: 10, loss: 0.0006640874780714512\n",
            "step: 20, loss: 0.0011504541616886854\n",
            "step: 30, loss: 0.00500338152050972\n",
            "step: 40, loss: 0.0008427802822552621\n",
            "step: 50, loss: 0.0011013956973329186\n",
            "step: 60, loss: 0.003721010871231556\n",
            "step: 70, loss: 0.0023651740048080683\n",
            "step: 80, loss: 0.003876244183629751\n",
            "step: 90, loss: 0.0006001276196911931\n",
            "step: 100, loss: 0.020526209846138954\n",
            "step: 110, loss: 0.028972361236810684\n",
            "step: 120, loss: 0.005013955757021904\n",
            "step: 130, loss: 0.0012471235822886229\n",
            "step: 140, loss: 0.0010426483349874616\n",
            "step: 150, loss: 0.0024357738438993692\n",
            "step: 160, loss: 0.0013106223195791245\n",
            "step: 170, loss: 0.0007770496886223555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6842105263157895, f1=0.6960784313725491, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00046392439980991185\n",
            "step: 10, loss: 0.002737775444984436\n",
            "step: 20, loss: 0.09190026670694351\n",
            "step: 30, loss: 0.0009702685056254268\n",
            "step: 40, loss: 0.0019111745059490204\n",
            "step: 50, loss: 0.002653430914506316\n",
            "step: 60, loss: 0.003672615624964237\n",
            "step: 70, loss: 0.00041102594695985317\n",
            "step: 80, loss: 0.004218451213091612\n",
            "step: 90, loss: 0.0012539192102849483\n",
            "step: 100, loss: 0.0019327295012772083\n",
            "step: 110, loss: 0.013026931323111057\n",
            "step: 120, loss: 0.010857226327061653\n",
            "step: 130, loss: 0.0008170371875166893\n",
            "step: 140, loss: 0.003802448743954301\n",
            "step: 150, loss: 0.0009539689053781331\n",
            "step: 160, loss: 0.001485512126237154\n",
            "step: 170, loss: 0.0015258811181411147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6891191709844561, f1=0.7021791767554479, best_f1=0.6812652068126521\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:09, 204.66it/s]\n",
            "load_f1 = 0.5853658536585367\n",
            "real_f1 = 0.5508474576271186\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 176.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6659ee3a-9105-4cae-a140-32d11bdedea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6112583875656128\n",
            "step: 10, loss: 0.6425053477287292\n",
            "step: 20, loss: 0.5024682283401489\n",
            "step: 30, loss: 0.49299857020378113\n",
            "step: 40, loss: 0.33945152163505554\n",
            "step: 50, loss: 0.09824777394533157\n",
            "step: 60, loss: 0.08160559833049774\n",
            "step: 70, loss: 0.1350330114364624\n",
            "step: 80, loss: 0.0736512690782547\n",
            "step: 90, loss: 0.10158059000968933\n",
            "step: 100, loss: 0.003985240124166012\n",
            "step: 110, loss: 0.17033912241458893\n",
            "step: 120, loss: 0.04715033248066902\n",
            "step: 130, loss: 0.008792468346655369\n",
            "step: 140, loss: 0.032375186681747437\n",
            "step: 150, loss: 0.08091802895069122\n",
            "step: 160, loss: 0.03708646446466446\n",
            "step: 170, loss: 0.34693121910095215\n",
            "step: 180, loss: 0.3127778172492981\n",
            "step: 190, loss: 0.07206883281469345\n",
            "step: 200, loss: 0.068061463534832\n",
            "step: 210, loss: 0.006968977861106396\n",
            "step: 220, loss: 0.015121906995773315\n",
            "step: 230, loss: 0.09445174038410187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9411764705882353, f1=0.9388209121245829, best_f1=0.9388209121245829\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0054883998818695545\n",
            "step: 10, loss: 0.004746670834720135\n",
            "step: 20, loss: 0.2383190393447876\n",
            "step: 30, loss: 0.1518149971961975\n",
            "step: 40, loss: 0.1456727683544159\n",
            "step: 50, loss: 0.010085509158670902\n",
            "step: 60, loss: 0.015918508172035217\n",
            "step: 70, loss: 0.11594723910093307\n",
            "step: 80, loss: 0.06442676484584808\n",
            "step: 90, loss: 0.06138131022453308\n",
            "step: 100, loss: 0.12494660913944244\n",
            "step: 110, loss: 0.21599242091178894\n",
            "step: 120, loss: 0.1504804491996765\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.08933646976947784\n",
            "step: 140, loss: 0.015073010697960854\n",
            "step: 150, loss: 0.030097048729658127\n",
            "step: 160, loss: 0.03942212462425232\n",
            "step: 170, loss: 0.010473741218447685\n",
            "step: 180, loss: 0.05961298570036888\n",
            "step: 190, loss: 0.039277657866477966\n",
            "step: 200, loss: 0.008935874328017235\n",
            "step: 210, loss: 0.005853372626006603\n",
            "step: 220, loss: 0.05233799293637276\n",
            "step: 230, loss: 0.25609201192855835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9589345172031077, f1=0.9552572706935123, best_f1=0.9552572706935123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027092942968010902\n",
            "step: 10, loss: 0.05225331708788872\n",
            "step: 20, loss: 0.033197950571775436\n",
            "step: 30, loss: 0.022337252274155617\n",
            "step: 40, loss: 0.07995156943798065\n",
            "step: 50, loss: 0.06210964173078537\n",
            "step: 60, loss: 0.034119658172130585\n",
            "step: 70, loss: 0.02064487896859646\n",
            "step: 80, loss: 0.01880904659628868\n",
            "step: 90, loss: 0.03140558302402496\n",
            "step: 100, loss: 0.0668257549405098\n",
            "step: 110, loss: 0.0038710779044777155\n",
            "step: 120, loss: 0.0069852182641625404\n",
            "step: 130, loss: 0.0007567614084109664\n",
            "step: 140, loss: 0.008255494758486748\n",
            "step: 150, loss: 0.06366995722055435\n",
            "step: 160, loss: 0.06824727356433868\n",
            "step: 170, loss: 0.021875331178307533\n",
            "step: 180, loss: 0.02778995782136917\n",
            "step: 190, loss: 0.007403744384646416\n",
            "step: 200, loss: 0.012689029797911644\n",
            "step: 210, loss: 0.023953642696142197\n",
            "step: 220, loss: 0.0028621105011552572\n",
            "step: 230, loss: 0.0014978778781369328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9581497797356827, f1=0.9565217391304347, best_f1=0.9552572706935123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002128481399267912\n",
            "step: 10, loss: 0.002102195518091321\n",
            "step: 20, loss: 0.00857420265674591\n",
            "step: 30, loss: 0.014491681009531021\n",
            "step: 40, loss: 0.01312005240470171\n",
            "step: 50, loss: 0.004489723592996597\n",
            "step: 60, loss: 0.003617602400481701\n",
            "step: 70, loss: 0.0025545204989612103\n",
            "step: 80, loss: 0.002769271144643426\n",
            "step: 90, loss: 0.020934859290719032\n",
            "step: 100, loss: 0.004403913859277964\n",
            "step: 110, loss: 0.012806234881281853\n",
            "step: 120, loss: 0.02158365771174431\n",
            "step: 130, loss: 0.1281825304031372\n",
            "step: 140, loss: 0.002378285164013505\n",
            "step: 150, loss: 0.2585284411907196\n",
            "step: 160, loss: 0.060101110488176346\n",
            "step: 170, loss: 0.024404427036643028\n",
            "step: 180, loss: 0.0009801206178963184\n",
            "step: 190, loss: 0.0022900686599314213\n",
            "step: 200, loss: 0.004465287551283836\n",
            "step: 210, loss: 0.0016358408611267805\n",
            "step: 220, loss: 0.001095120212994516\n",
            "step: 230, loss: 0.013611667789518833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9565217391304347, f1=0.9511111111111112, best_f1=0.9552572706935123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07453760504722595\n",
            "step: 10, loss: 0.001501749036833644\n",
            "step: 20, loss: 0.006373519543558359\n",
            "step: 30, loss: 0.0007305326289497316\n",
            "step: 40, loss: 0.0014573123771697283\n",
            "step: 50, loss: 0.02483501099050045\n",
            "step: 60, loss: 0.051665112376213074\n",
            "step: 70, loss: 0.0017697729635983706\n",
            "step: 80, loss: 0.0005908524617552757\n",
            "step: 90, loss: 0.007639922667294741\n",
            "step: 100, loss: 0.00026029208675026894\n",
            "step: 110, loss: 0.0002736237074714154\n",
            "step: 120, loss: 0.0003951505059376359\n",
            "step: 130, loss: 0.03634939342737198\n",
            "step: 140, loss: 0.005677782464772463\n",
            "step: 150, loss: 0.005102585535496473\n",
            "step: 160, loss: 0.0003508070658426732\n",
            "step: 170, loss: 0.00901932455599308\n",
            "step: 180, loss: 0.00033581574098207057\n",
            "step: 190, loss: 0.11229947209358215\n",
            "step: 200, loss: 0.037799522280693054\n",
            "step: 210, loss: 0.011302044615149498\n",
            "step: 220, loss: 0.0042775836773216724\n",
            "step: 230, loss: 0.001181307015940547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9615806805708014, f1=0.9592959295929592, best_f1=0.9592959295929592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010161417536437511\n",
            "step: 10, loss: 0.0013098687632009387\n",
            "step: 20, loss: 0.01041490864008665\n",
            "step: 30, loss: 0.0033209885004907846\n",
            "step: 40, loss: 0.00027649669209495187\n",
            "step: 50, loss: 0.000373556453268975\n",
            "step: 60, loss: 0.00016307497571688145\n",
            "step: 70, loss: 0.0012659062631428242\n",
            "step: 80, loss: 0.0005924021825194359\n",
            "step: 90, loss: 0.07483331114053726\n",
            "step: 100, loss: 0.13347235321998596\n",
            "step: 110, loss: 0.0017763329669833183\n",
            "step: 120, loss: 0.057900115847587585\n",
            "step: 130, loss: 0.015267172828316689\n",
            "step: 140, loss: 0.0011296411976218224\n",
            "step: 150, loss: 0.02741832844913006\n",
            "step: 160, loss: 0.004367985296994448\n",
            "step: 170, loss: 0.007115104701370001\n",
            "step: 180, loss: 0.007658190093934536\n",
            "step: 190, loss: 0.057818781584501266\n",
            "step: 200, loss: 0.001843275036662817\n",
            "step: 210, loss: 0.0012697218917310238\n",
            "step: 220, loss: 0.01334310881793499\n",
            "step: 230, loss: 0.08035188168287277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9542920847268673, f1=0.9502262443438914, best_f1=0.9592959295929592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012967672199010849\n",
            "step: 10, loss: 0.0013730202335864305\n",
            "step: 20, loss: 0.0038845252711325884\n",
            "step: 30, loss: 0.02867252193391323\n",
            "step: 40, loss: 0.008632884360849857\n",
            "step: 50, loss: 0.00036581663880497217\n",
            "step: 60, loss: 0.0008908368181437254\n",
            "step: 70, loss: 0.011061522178351879\n",
            "step: 80, loss: 0.00320099457167089\n",
            "step: 90, loss: 0.000136314207338728\n",
            "step: 100, loss: 0.00038212802610360086\n",
            "step: 110, loss: 0.0017681081080809236\n",
            "step: 120, loss: 0.002453394467011094\n",
            "step: 130, loss: 0.004980150144547224\n",
            "step: 140, loss: 0.00029323133639991283\n",
            "step: 150, loss: 0.003074508858844638\n",
            "step: 160, loss: 0.08970427513122559\n",
            "step: 170, loss: 0.011120768263936043\n",
            "step: 180, loss: 0.003750344505533576\n",
            "step: 190, loss: 0.000582112988922745\n",
            "step: 200, loss: 0.12755347788333893\n",
            "step: 210, loss: 7.813864067429677e-05\n",
            "step: 220, loss: 0.07442910224199295\n",
            "step: 230, loss: 0.08824358135461807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9588431590656283, f1=0.953229398663697, best_f1=0.9592959295929592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002375858457526192\n",
            "step: 10, loss: 0.07718590646982193\n",
            "step: 20, loss: 0.001225322368554771\n",
            "step: 30, loss: 0.00473802350461483\n",
            "step: 40, loss: 0.05004673823714256\n",
            "step: 50, loss: 0.0007141176029108465\n",
            "step: 60, loss: 0.0004542275273706764\n",
            "step: 70, loss: 0.0017568678595125675\n",
            "step: 80, loss: 0.0003470703086350113\n",
            "step: 90, loss: 0.00020648882491514087\n",
            "step: 100, loss: 0.04269363731145859\n",
            "step: 110, loss: 0.0016841592732816935\n",
            "step: 120, loss: 0.025312121957540512\n",
            "step: 130, loss: 0.005010436289012432\n",
            "step: 140, loss: 0.000435110239777714\n",
            "step: 150, loss: 0.00030767059070058167\n",
            "step: 160, loss: 0.001226886291988194\n",
            "step: 170, loss: 0.0007747897179797292\n",
            "step: 180, loss: 0.0725911557674408\n",
            "step: 190, loss: 0.004548244643956423\n",
            "step: 200, loss: 0.002362368628382683\n",
            "step: 210, loss: 0.0022854015696793795\n",
            "step: 220, loss: 0.000546671508345753\n",
            "step: 230, loss: 0.09005814045667648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9683257918552037, f1=0.9477272727272728, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009008195484057069\n",
            "step: 10, loss: 0.006636475678533316\n",
            "step: 20, loss: 0.001219981350004673\n",
            "step: 30, loss: 9.510829113423824e-05\n",
            "step: 40, loss: 0.01686665415763855\n",
            "step: 50, loss: 0.00015994218119885772\n",
            "step: 60, loss: 0.0002871298638638109\n",
            "step: 70, loss: 0.0001357321598334238\n",
            "step: 80, loss: 8.845824049785733e-05\n",
            "step: 90, loss: 0.00028026284417137504\n",
            "step: 100, loss: 0.0005614759866148233\n",
            "step: 110, loss: 0.0006097727455198765\n",
            "step: 120, loss: 0.0019330017967149615\n",
            "step: 130, loss: 0.00691326754167676\n",
            "step: 140, loss: 0.0003852092777378857\n",
            "step: 150, loss: 0.14178387820720673\n",
            "step: 160, loss: 0.00021997479780111462\n",
            "step: 170, loss: 0.0003496247227303684\n",
            "step: 180, loss: 0.005569511093199253\n",
            "step: 190, loss: 0.0020658313296735287\n",
            "step: 200, loss: 0.000668024702463299\n",
            "step: 210, loss: 0.0013546643313020468\n",
            "step: 220, loss: 0.0001413434511050582\n",
            "step: 230, loss: 0.0004365343484096229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9627959413754228, f1=0.9532497149372862, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023467213031835854\n",
            "step: 10, loss: 0.000356759614078328\n",
            "step: 20, loss: 0.004093323368579149\n",
            "step: 30, loss: 8.936470840126276e-05\n",
            "step: 40, loss: 0.0001241543359356001\n",
            "step: 50, loss: 0.0009013499948196113\n",
            "step: 60, loss: 0.0005235809949226677\n",
            "step: 70, loss: 0.0002297440660186112\n",
            "step: 80, loss: 7.859031757107005e-05\n",
            "step: 90, loss: 0.003040907671675086\n",
            "step: 100, loss: 7.571111927973107e-05\n",
            "step: 110, loss: 9.122284973273054e-05\n",
            "step: 120, loss: 0.0002974549133796245\n",
            "step: 130, loss: 0.02381480112671852\n",
            "step: 140, loss: 0.057502150535583496\n",
            "step: 150, loss: 0.01424513477832079\n",
            "step: 160, loss: 0.0011886009015142918\n",
            "step: 170, loss: 5.298212272464298e-05\n",
            "step: 180, loss: 0.00036640255711972713\n",
            "step: 190, loss: 0.011015078984200954\n",
            "step: 200, loss: 5.441206303657964e-05\n",
            "step: 210, loss: 9.095874702325091e-05\n",
            "step: 220, loss: 0.00014643445319961756\n",
            "step: 230, loss: 0.05812162905931473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9620535714285715, f1=0.953125, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002557651314418763\n",
            "step: 10, loss: 0.00016782335296738893\n",
            "step: 20, loss: 0.0012651474680751562\n",
            "step: 30, loss: 0.006614319514483213\n",
            "step: 40, loss: 0.00010366813512519002\n",
            "step: 50, loss: 0.00048693138523958623\n",
            "step: 60, loss: 0.00011873492621816695\n",
            "step: 70, loss: 0.0002597629209049046\n",
            "step: 80, loss: 5.1149469072697684e-05\n",
            "step: 90, loss: 6.654221215285361e-05\n",
            "step: 100, loss: 7.345630001509562e-05\n",
            "step: 110, loss: 0.0006068412913009524\n",
            "step: 120, loss: 0.000114080285129603\n",
            "step: 130, loss: 0.00019690915360115469\n",
            "step: 140, loss: 0.0002706922823563218\n",
            "step: 150, loss: 0.012597236782312393\n",
            "step: 160, loss: 4.8743222578195855e-05\n",
            "step: 170, loss: 0.0003383269358891994\n",
            "step: 180, loss: 7.10313834133558e-05\n",
            "step: 190, loss: 6.384873267961666e-05\n",
            "step: 200, loss: 0.00019371161761227995\n",
            "step: 210, loss: 3.974002174800262e-05\n",
            "step: 220, loss: 5.5436492402805015e-05\n",
            "step: 230, loss: 4.4235999666852877e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.961883408071749, f1=0.9534619750283768, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.549835911253467e-05\n",
            "step: 10, loss: 0.0003852683585137129\n",
            "step: 20, loss: 0.00016720571147743613\n",
            "step: 30, loss: 7.767155329929665e-05\n",
            "step: 40, loss: 0.07959844172000885\n",
            "step: 50, loss: 0.004191108047962189\n",
            "step: 60, loss: 0.0005493620410561562\n",
            "step: 70, loss: 0.00011988209735136479\n",
            "step: 80, loss: 9.325014252681285e-05\n",
            "step: 90, loss: 6.994366412982345e-05\n",
            "step: 100, loss: 4.626409281627275e-05\n",
            "step: 110, loss: 0.00023880283697508276\n",
            "step: 120, loss: 4.735718903248198e-05\n",
            "step: 130, loss: 0.00021445401944220066\n",
            "step: 140, loss: 0.00049094099085778\n",
            "step: 150, loss: 0.00012147127563366666\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 9.606201638234779e-05\n",
            "step: 170, loss: 0.0007084383978508413\n",
            "step: 180, loss: 0.00501065980643034\n",
            "step: 190, loss: 0.0015563147608190775\n",
            "step: 200, loss: 3.3720152714522555e-05\n",
            "step: 210, loss: 5.23415110365022e-05\n",
            "step: 220, loss: 0.00010230124462395906\n",
            "step: 230, loss: 0.0075393542647361755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9584736251402918, f1=0.9466515323496028, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006144444341771305\n",
            "step: 10, loss: 0.00010855899745365605\n",
            "step: 20, loss: 0.00015476878616027534\n",
            "step: 30, loss: 5.443485497380607e-05\n",
            "step: 40, loss: 0.0014388726558536291\n",
            "step: 50, loss: 6.871324876556173e-05\n",
            "step: 60, loss: 0.0008439699886366725\n",
            "step: 70, loss: 5.0108319555874914e-05\n",
            "step: 80, loss: 7.22624099580571e-05\n",
            "step: 90, loss: 0.0008837233181111515\n",
            "step: 100, loss: 0.000133522305986844\n",
            "step: 110, loss: 0.042940422892570496\n",
            "step: 120, loss: 0.0004628312890417874\n",
            "step: 130, loss: 0.00010289045894751325\n",
            "step: 140, loss: 0.00015412690117955208\n",
            "step: 150, loss: 4.705579704022966e-05\n",
            "step: 160, loss: 0.00022932360298000276\n",
            "step: 170, loss: 0.00020540445984806865\n",
            "step: 180, loss: 0.0007001168560236692\n",
            "step: 190, loss: 4.7685596655355766e-05\n",
            "step: 200, loss: 0.00037845922634005547\n",
            "step: 210, loss: 3.413773447391577e-05\n",
            "step: 220, loss: 4.884679947281256e-05\n",
            "step: 230, loss: 4.855894439970143e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9609810479375697, f1=0.9518477043673013, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.929003262077458e-05\n",
            "step: 10, loss: 0.00014473979535978287\n",
            "step: 20, loss: 0.000862862216308713\n",
            "step: 30, loss: 5.783838059869595e-05\n",
            "step: 40, loss: 0.006566113792359829\n",
            "step: 50, loss: 0.003829807974398136\n",
            "step: 60, loss: 9.552703704684973e-05\n",
            "step: 70, loss: 8.009788143681362e-05\n",
            "step: 80, loss: 5.3277686674846336e-05\n",
            "step: 90, loss: 0.0003189978888258338\n",
            "step: 100, loss: 0.0003449555370025337\n",
            "step: 110, loss: 0.0019372425740584731\n",
            "step: 120, loss: 3.394006489543244e-05\n",
            "step: 130, loss: 0.00012562509800773114\n",
            "step: 140, loss: 3.8495782064273953e-05\n",
            "step: 150, loss: 3.2870855648070574e-05\n",
            "step: 160, loss: 9.924601909006014e-05\n",
            "step: 170, loss: 6.126508378656581e-05\n",
            "step: 180, loss: 0.0010994464391842484\n",
            "step: 190, loss: 3.27818124787882e-05\n",
            "step: 200, loss: 7.06591090420261e-05\n",
            "step: 210, loss: 6.177992327138782e-05\n",
            "step: 220, loss: 0.00014880088565405458\n",
            "step: 230, loss: 0.0012485483894124627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9615384615384616, f1=0.9519450800915331, best_f1=0.9477272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.148540054098703e-05\n",
            "step: 10, loss: 4.4409651309251785e-05\n",
            "step: 20, loss: 4.2484414734644815e-05\n",
            "step: 30, loss: 0.0002534427912905812\n",
            "step: 40, loss: 5.680646063410677e-05\n",
            "step: 50, loss: 0.0464373417198658\n",
            "step: 60, loss: 4.905600144411437e-05\n",
            "step: 70, loss: 0.001490571303293109\n",
            "step: 80, loss: 0.00023419628269039094\n",
            "step: 90, loss: 0.0003835927345789969\n",
            "step: 100, loss: 4.336413985583931e-05\n",
            "step: 110, loss: 3.425314207561314e-05\n",
            "step: 120, loss: 0.0003442020097281784\n",
            "step: 130, loss: 5.413438702817075e-05\n",
            "step: 140, loss: 0.003133351681753993\n",
            "step: 150, loss: 0.004276635590940714\n",
            "step: 160, loss: 7.984452531673014e-05\n",
            "step: 170, loss: 3.0349119697348215e-05\n",
            "step: 180, loss: 9.752465120982379e-05\n",
            "step: 190, loss: 0.0012140849139541388\n",
            "step: 200, loss: 0.0007098286296240985\n",
            "step: 210, loss: 0.0002674812276381999\n",
            "step: 220, loss: 0.0020343014039099216\n",
            "step: 230, loss: 5.5874646932352334e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9594594594594594, f1=0.9533560864618885, best_f1=0.9477272727272728\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 152.92it/s]\n",
            "load_f1 = 0.9641255605381166\n",
            "real_f1 = 0.9619686800894856\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 175.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37ef086-2ac6-4e0e-c08c-3ce2d905d1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 450kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 69.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6308624744415283\n",
            "step: 10, loss: 0.6032471656799316\n",
            "step: 20, loss: 0.5994514226913452\n",
            "step: 30, loss: 0.3534665107727051\n",
            "step: 40, loss: 0.3783387839794159\n",
            "step: 50, loss: 0.22612157464027405\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.10589089244604111\n",
            "step: 70, loss: 0.2236054241657257\n",
            "step: 80, loss: 0.10979318618774414\n",
            "step: 90, loss: 0.40887370705604553\n",
            "step: 100, loss: 0.19607095420360565\n",
            "step: 110, loss: 0.129455104470253\n",
            "step: 120, loss: 0.11008865386247635\n",
            "step: 130, loss: 0.179279625415802\n",
            "step: 140, loss: 0.11163455992937088\n",
            "step: 150, loss: 0.09006695449352264\n",
            "step: 160, loss: 0.1105445921421051\n",
            "step: 170, loss: 0.30319690704345703\n",
            "step: 180, loss: 0.08209290355443954\n",
            "step: 190, loss: 0.019269302487373352\n",
            "step: 200, loss: 0.15609294176101685\n",
            "step: 210, loss: 0.060745444148778915\n",
            "step: 220, loss: 0.2706945538520813\n",
            "step: 230, loss: 0.1904507726430893\n",
            "step: 240, loss: 0.08724727481603622\n",
            "step: 250, loss: 0.10056265443563461\n",
            "step: 260, loss: 0.06593586504459381\n",
            "step: 270, loss: 0.023828720673918724\n",
            "step: 280, loss: 0.19308306276798248\n",
            "step: 290, loss: 0.2809520363807678\n",
            "step: 300, loss: 0.06415525078773499\n",
            "step: 310, loss: 0.23609450459480286\n",
            "step: 320, loss: 0.05800242722034454\n",
            "step: 330, loss: 0.09501051157712936\n",
            "step: 340, loss: 0.21608494222164154\n",
            "step: 350, loss: 0.046079427003860474\n",
            "step: 360, loss: 0.0917193815112114\n",
            "step: 370, loss: 0.10223643481731415\n",
            "step: 380, loss: 0.011865193955600262\n",
            "step: 390, loss: 0.07146729528903961\n",
            "step: 400, loss: 0.3023937940597534\n",
            "step: 410, loss: 0.06558353453874588\n",
            "step: 420, loss: 0.09069307893514633\n",
            "step: 430, loss: 0.1747521013021469\n",
            "step: 440, loss: 0.09228198230266571\n",
            "step: 450, loss: 0.06431342661380768\n",
            "step: 460, loss: 0.00969283189624548\n",
            "step: 470, loss: 0.1327700912952423\n",
            "step: 480, loss: 0.07971636950969696\n",
            "step: 490, loss: 0.0853620246052742\n",
            "step: 500, loss: 0.08713164180517197\n",
            "step: 510, loss: 0.11188672482967377\n",
            "step: 520, loss: 0.13075077533721924\n",
            "step: 530, loss: 0.008883076719939709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9072261072261073, f1=0.8876351669017396, best_f1=0.8876351669017396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18230444192886353\n",
            "step: 10, loss: 0.14306297898292542\n",
            "step: 20, loss: 0.20137865841388702\n",
            "step: 30, loss: 0.08887867629528046\n",
            "step: 40, loss: 0.06548857688903809\n",
            "step: 50, loss: 0.11013713479042053\n",
            "step: 60, loss: 0.07806499302387238\n",
            "step: 70, loss: 0.048743657767772675\n",
            "step: 80, loss: 0.06817272305488586\n",
            "step: 90, loss: 0.07117559760808945\n",
            "step: 100, loss: 0.09879474341869354\n",
            "step: 110, loss: 0.10872165858745575\n",
            "step: 120, loss: 0.08267752081155777\n",
            "step: 130, loss: 0.11053754389286041\n",
            "step: 140, loss: 0.0682210773229599\n",
            "step: 150, loss: 0.13777834177017212\n",
            "step: 160, loss: 0.023302676156163216\n",
            "step: 170, loss: 0.13033680617809296\n",
            "step: 180, loss: 0.0454958900809288\n",
            "step: 190, loss: 0.06711029261350632\n",
            "step: 200, loss: 0.029953598976135254\n",
            "step: 210, loss: 0.086685411632061\n",
            "step: 220, loss: 0.08574916422367096\n",
            "step: 230, loss: 0.05620792880654335\n",
            "step: 240, loss: 0.061646413058042526\n",
            "step: 250, loss: 0.10141347348690033\n",
            "step: 260, loss: 0.051630258560180664\n",
            "step: 270, loss: 0.1753477156162262\n",
            "step: 280, loss: 0.16053202748298645\n",
            "step: 290, loss: 0.02872966229915619\n",
            "step: 300, loss: 0.2454231083393097\n",
            "step: 310, loss: 0.04407922923564911\n",
            "step: 320, loss: 0.1952347457408905\n",
            "step: 330, loss: 0.09094320982694626\n",
            "step: 340, loss: 0.07831069082021713\n",
            "step: 350, loss: 0.015131842344999313\n",
            "step: 360, loss: 0.148757204413414\n",
            "step: 370, loss: 0.1635657250881195\n",
            "step: 380, loss: 0.11317852139472961\n",
            "step: 390, loss: 0.1463436633348465\n",
            "step: 400, loss: 0.01757032424211502\n",
            "step: 410, loss: 0.06074519827961922\n",
            "step: 420, loss: 0.02724740095436573\n",
            "step: 430, loss: 0.15838950872421265\n",
            "step: 440, loss: 0.08900714665651321\n",
            "step: 450, loss: 0.1686776876449585\n",
            "step: 460, loss: 0.02454710565507412\n",
            "step: 470, loss: 0.02230921946465969\n",
            "step: 480, loss: 0.18457531929016113\n",
            "step: 490, loss: 0.016655782237648964\n",
            "step: 500, loss: 0.3514931797981262\n",
            "step: 510, loss: 0.03749142214655876\n",
            "step: 520, loss: 0.10233378410339355\n",
            "step: 530, loss: 0.10562456399202347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9101019462465245, f1=0.8976377952755905, best_f1=0.8976377952755905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09090369194746017\n",
            "step: 10, loss: 0.05051650479435921\n",
            "step: 20, loss: 0.14046929776668549\n",
            "step: 30, loss: 0.2116367518901825\n",
            "step: 40, loss: 0.05286232382059097\n",
            "step: 50, loss: 0.16880419850349426\n",
            "step: 60, loss: 0.07688988745212555\n",
            "step: 70, loss: 0.016828911378979683\n",
            "step: 80, loss: 0.017465464770793915\n",
            "step: 90, loss: 0.03802984580397606\n",
            "step: 100, loss: 0.01624738983809948\n",
            "step: 110, loss: 0.004249974153935909\n",
            "step: 120, loss: 0.02563210017979145\n",
            "step: 130, loss: 0.0025322865694761276\n",
            "step: 140, loss: 0.012266874313354492\n",
            "step: 150, loss: 0.1898767352104187\n",
            "step: 160, loss: 0.005419933702796698\n",
            "step: 170, loss: 0.132664293050766\n",
            "step: 180, loss: 0.028715714812278748\n",
            "step: 190, loss: 0.025567667558789253\n",
            "step: 200, loss: 0.061706580221652985\n",
            "step: 210, loss: 0.0473773367702961\n",
            "step: 220, loss: 0.11505324393510818\n",
            "step: 230, loss: 0.2111319899559021\n",
            "step: 240, loss: 0.14227616786956787\n",
            "step: 250, loss: 0.008944236673414707\n",
            "step: 260, loss: 0.07475295662879944\n",
            "step: 270, loss: 0.022482501342892647\n",
            "step: 280, loss: 0.2703956365585327\n",
            "step: 290, loss: 0.00650831637904048\n",
            "step: 300, loss: 0.05522974207997322\n",
            "step: 310, loss: 0.0450286865234375\n",
            "step: 320, loss: 0.04817254841327667\n",
            "step: 330, loss: 0.00792610365897417\n",
            "step: 340, loss: 0.013992128893733025\n",
            "step: 350, loss: 0.03602888435125351\n",
            "step: 360, loss: 0.025534575805068016\n",
            "step: 370, loss: 0.016985880210995674\n",
            "step: 380, loss: 0.002880462910979986\n",
            "step: 390, loss: 0.015180072747170925\n",
            "step: 400, loss: 0.08383985608816147\n",
            "step: 410, loss: 0.03227343410253525\n",
            "step: 420, loss: 0.14809182286262512\n",
            "step: 430, loss: 0.010795806534588337\n",
            "step: 440, loss: 0.07860787212848663\n",
            "step: 450, loss: 0.051675789058208466\n",
            "step: 460, loss: 0.044970132410526276\n",
            "step: 470, loss: 0.0541149377822876\n",
            "step: 480, loss: 0.04470234364271164\n",
            "step: 490, loss: 0.005834745708853006\n",
            "step: 500, loss: 0.11362612992525101\n",
            "step: 510, loss: 0.011050084605813026\n",
            "step: 520, loss: 0.05607800558209419\n",
            "step: 530, loss: 0.12204436212778091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9153992395437264, f1=0.8953655040611561, best_f1=0.8953655040611561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0066430349834263325\n",
            "step: 10, loss: 0.027809735387563705\n",
            "step: 20, loss: 0.004516819026321173\n",
            "step: 30, loss: 0.011271648108959198\n",
            "step: 40, loss: 0.0685976967215538\n",
            "step: 50, loss: 0.01387912780046463\n",
            "step: 60, loss: 0.013557527214288712\n",
            "step: 70, loss: 0.02534766122698784\n",
            "step: 80, loss: 0.004326429218053818\n",
            "step: 90, loss: 0.15591412782669067\n",
            "step: 100, loss: 0.019829824566841125\n",
            "step: 110, loss: 0.0886169895529747\n",
            "step: 120, loss: 0.00245283218100667\n",
            "step: 130, loss: 0.0021528112702071667\n",
            "step: 140, loss: 0.004986746236681938\n",
            "step: 150, loss: 0.046438705176115036\n",
            "step: 160, loss: 0.0035173564683645964\n",
            "step: 170, loss: 0.005279794801026583\n",
            "step: 180, loss: 0.01854580268263817\n",
            "step: 190, loss: 0.086207315325737\n",
            "step: 200, loss: 0.02681104652583599\n",
            "step: 210, loss: 0.04812709614634514\n",
            "step: 220, loss: 0.007310991641134024\n",
            "step: 230, loss: 0.008267893455922604\n",
            "step: 240, loss: 0.0021908998023718596\n",
            "step: 250, loss: 0.06312011182308197\n",
            "step: 260, loss: 0.010400092229247093\n",
            "step: 270, loss: 0.061282768845558167\n",
            "step: 280, loss: 0.15071019530296326\n",
            "step: 290, loss: 0.012388370931148529\n",
            "step: 300, loss: 0.0014819566858932376\n",
            "step: 310, loss: 0.006693318020552397\n",
            "step: 320, loss: 0.0378216952085495\n",
            "step: 330, loss: 0.0630405992269516\n",
            "step: 340, loss: 0.2916943430900574\n",
            "step: 350, loss: 0.04358921945095062\n",
            "step: 360, loss: 0.07297121733427048\n",
            "step: 370, loss: 0.003690264653414488\n",
            "step: 380, loss: 0.026814214885234833\n",
            "step: 390, loss: 0.03694860264658928\n",
            "step: 400, loss: 0.001381085254251957\n",
            "step: 410, loss: 0.02422146126627922\n",
            "step: 420, loss: 0.23613262176513672\n",
            "step: 430, loss: 0.0866832360625267\n",
            "step: 440, loss: 0.004209062084555626\n",
            "step: 450, loss: 0.017103102058172226\n",
            "step: 460, loss: 0.005303812213242054\n",
            "step: 470, loss: 0.039907604455947876\n",
            "step: 480, loss: 0.023829763755202293\n",
            "step: 490, loss: 0.019351962953805923\n",
            "step: 500, loss: 0.012563557364046574\n",
            "step: 510, loss: 0.09942575544118881\n",
            "step: 520, loss: 0.09480052441358566\n",
            "step: 530, loss: 0.042420394718647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9087574507106833, f1=0.9040333796940195, best_f1=0.8953655040611561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19698697328567505\n",
            "step: 10, loss: 0.04114644601941109\n",
            "step: 20, loss: 0.05112839490175247\n",
            "step: 30, loss: 0.003441245760768652\n",
            "step: 40, loss: 0.1142912432551384\n",
            "step: 50, loss: 0.001889569335617125\n",
            "step: 60, loss: 0.14990022778511047\n",
            "step: 70, loss: 0.05975887551903725\n",
            "step: 80, loss: 0.07479606568813324\n",
            "step: 90, loss: 0.0048789335414767265\n",
            "step: 100, loss: 0.007001224905252457\n",
            "step: 110, loss: 0.0009265068219974637\n",
            "step: 120, loss: 0.01891174353659153\n",
            "step: 130, loss: 0.0017293645069003105\n",
            "step: 140, loss: 0.00674563879147172\n",
            "step: 150, loss: 0.0549921877682209\n",
            "step: 160, loss: 0.0022861238103359938\n",
            "step: 170, loss: 0.007045604754239321\n",
            "step: 180, loss: 0.004460544791072607\n",
            "step: 190, loss: 0.01654401421546936\n",
            "step: 200, loss: 0.022402871400117874\n",
            "step: 210, loss: 0.037443362176418304\n",
            "step: 220, loss: 0.010975463315844536\n",
            "step: 230, loss: 0.026925817131996155\n",
            "step: 240, loss: 0.012880288995802402\n",
            "step: 250, loss: 0.014495009556412697\n",
            "step: 260, loss: 0.005816080141812563\n",
            "step: 270, loss: 0.012859560549259186\n",
            "step: 280, loss: 0.0021992544643580914\n",
            "step: 290, loss: 0.236911341547966\n",
            "step: 300, loss: 0.034711651504039764\n",
            "step: 310, loss: 0.009963475167751312\n",
            "step: 320, loss: 0.1562647968530655\n",
            "step: 330, loss: 0.046950288116931915\n",
            "step: 340, loss: 0.0026724543422460556\n",
            "step: 350, loss: 0.0008621826418675482\n",
            "step: 360, loss: 0.005734287202358246\n",
            "step: 370, loss: 0.014865242876112461\n",
            "step: 380, loss: 0.0011756597086787224\n",
            "step: 390, loss: 0.000926606822758913\n",
            "step: 400, loss: 0.03421005606651306\n",
            "step: 410, loss: 0.014734147116541862\n",
            "step: 420, loss: 0.0012469551293179393\n",
            "step: 430, loss: 0.10551777482032776\n",
            "step: 440, loss: 0.17245620489120483\n",
            "step: 450, loss: 0.00886040274053812\n",
            "step: 460, loss: 0.0050578126683831215\n",
            "step: 470, loss: 0.005866053979843855\n",
            "step: 480, loss: 0.03171348571777344\n",
            "step: 490, loss: 0.001454837154597044\n",
            "step: 500, loss: 0.07025256007909775\n",
            "step: 510, loss: 0.0030387442093342543\n",
            "step: 520, loss: 0.03448103740811348\n",
            "step: 530, loss: 0.009390738792717457\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9168207024029574, f1=0.8997214484679666, best_f1=0.8997214484679666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015101785538718104\n",
            "step: 10, loss: 0.002478862414136529\n",
            "step: 20, loss: 0.11003152281045914\n",
            "step: 30, loss: 0.006985130254179239\n",
            "step: 40, loss: 0.0017004329711198807\n",
            "step: 50, loss: 0.0033593117259442806\n",
            "step: 60, loss: 0.0001971826422959566\n",
            "step: 70, loss: 0.0012164624640718102\n",
            "step: 80, loss: 0.0514473021030426\n",
            "step: 90, loss: 0.005910306703299284\n",
            "step: 100, loss: 0.023439977318048477\n",
            "step: 110, loss: 0.011378182098269463\n",
            "step: 120, loss: 0.024989865720272064\n",
            "step: 130, loss: 0.001539392862468958\n",
            "step: 140, loss: 0.021524429321289062\n",
            "step: 150, loss: 0.006504649296402931\n",
            "step: 160, loss: 0.002761795185506344\n",
            "step: 170, loss: 0.007358884904533625\n",
            "step: 180, loss: 0.002614314202219248\n",
            "step: 190, loss: 0.006294431164860725\n",
            "step: 200, loss: 0.030139822512865067\n",
            "step: 210, loss: 0.0018709105206653476\n",
            "step: 220, loss: 0.0027594794519245625\n",
            "step: 230, loss: 0.004641438368707895\n",
            "step: 240, loss: 0.05480504408478737\n",
            "step: 250, loss: 0.004973806440830231\n",
            "step: 260, loss: 0.012785669416189194\n",
            "step: 270, loss: 0.014854823239147663\n",
            "step: 280, loss: 0.001810629852116108\n",
            "step: 290, loss: 0.0004599808598868549\n",
            "step: 300, loss: 0.00039926698082126677\n",
            "step: 310, loss: 0.0013919607736170292\n",
            "step: 320, loss: 0.0008702654740773141\n",
            "step: 330, loss: 0.004096830729395151\n",
            "step: 340, loss: 0.07715679705142975\n",
            "step: 350, loss: 0.024414919316768646\n",
            "step: 360, loss: 0.04565298929810524\n",
            "step: 370, loss: 0.0013741697184741497\n",
            "step: 380, loss: 0.0007200422114692628\n",
            "step: 390, loss: 0.11618383973836899\n",
            "step: 400, loss: 0.11270009726285934\n",
            "step: 410, loss: 0.007898246869444847\n",
            "step: 420, loss: 0.056369464844465256\n",
            "step: 430, loss: 0.001282812561839819\n",
            "step: 440, loss: 0.005339452065527439\n",
            "step: 450, loss: 0.0013114108005538583\n",
            "step: 460, loss: 0.008757714182138443\n",
            "step: 470, loss: 0.016695402562618256\n",
            "step: 480, loss: 0.022279368713498116\n",
            "step: 490, loss: 0.003732894780114293\n",
            "step: 500, loss: 0.0008756423485465348\n",
            "step: 510, loss: 0.007275558542460203\n",
            "step: 520, loss: 0.004005839116871357\n",
            "step: 530, loss: 0.019645147025585175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9031352363125877, f1=0.8772426817752597, best_f1=0.8997214484679666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031515348702669144\n",
            "step: 10, loss: 0.00924545805901289\n",
            "step: 20, loss: 0.17961455881595612\n",
            "step: 30, loss: 0.00861323717981577\n",
            "step: 40, loss: 0.03483874723315239\n",
            "step: 50, loss: 0.014020106755197048\n",
            "step: 60, loss: 0.011027703993022442\n",
            "step: 70, loss: 0.0010788252111524343\n",
            "step: 80, loss: 0.016148356720805168\n",
            "step: 90, loss: 0.0011790626449510455\n",
            "step: 100, loss: 0.00013245080481283367\n",
            "step: 110, loss: 0.00038339311140589416\n",
            "step: 120, loss: 0.0016973029123619199\n",
            "step: 130, loss: 0.016889410093426704\n",
            "step: 140, loss: 0.00032386419479735196\n",
            "step: 150, loss: 0.00275716301985085\n",
            "step: 160, loss: 0.09211436659097672\n",
            "step: 170, loss: 0.002797223860397935\n",
            "step: 180, loss: 0.0010187311563640833\n",
            "step: 190, loss: 0.13661330938339233\n",
            "step: 200, loss: 0.010134467855095863\n",
            "step: 210, loss: 0.00695136608555913\n",
            "step: 220, loss: 0.0009410791099071503\n",
            "step: 230, loss: 0.029287273064255714\n",
            "step: 240, loss: 0.0025969459675252438\n",
            "step: 250, loss: 0.0004650845075957477\n",
            "step: 260, loss: 0.004686142783612013\n",
            "step: 270, loss: 0.030994830653071404\n",
            "step: 280, loss: 0.10990392416715622\n",
            "step: 290, loss: 0.004669333342462778\n",
            "step: 300, loss: 0.025307755917310715\n",
            "step: 310, loss: 0.0004367638612166047\n",
            "step: 320, loss: 0.0008648757939226925\n",
            "step: 330, loss: 0.07851580530405045\n",
            "step: 340, loss: 0.0026654438115656376\n",
            "step: 350, loss: 0.0005350107094272971\n",
            "step: 360, loss: 0.002436259062960744\n",
            "step: 370, loss: 0.0021530338563024998\n",
            "step: 380, loss: 0.0016151946038007736\n",
            "step: 390, loss: 0.0005999439745210111\n",
            "step: 400, loss: 0.0037773705553263426\n",
            "step: 410, loss: 0.0017030773451551795\n",
            "step: 420, loss: 0.010874301195144653\n",
            "step: 430, loss: 0.0028280564583837986\n",
            "step: 440, loss: 0.0012089544907212257\n",
            "step: 450, loss: 0.002166761551052332\n",
            "step: 460, loss: 0.0022217051591724157\n",
            "step: 470, loss: 0.03783382102847099\n",
            "step: 480, loss: 0.2232976108789444\n",
            "step: 490, loss: 0.006647947244346142\n",
            "step: 500, loss: 0.005814340431243181\n",
            "step: 510, loss: 0.007022024132311344\n",
            "step: 520, loss: 0.12662021815776825\n",
            "step: 530, loss: 0.037307221442461014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.917397323488694, f1=0.9006010171058715, best_f1=0.9006010171058715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013784898445010185\n",
            "step: 10, loss: 0.01779925636947155\n",
            "step: 20, loss: 0.00038212264189496636\n",
            "step: 30, loss: 0.001509202760644257\n",
            "step: 40, loss: 0.000593398988712579\n",
            "step: 50, loss: 0.02469637617468834\n",
            "step: 60, loss: 0.0007074191817082465\n",
            "step: 70, loss: 0.0004301226290408522\n",
            "step: 80, loss: 0.00383172370493412\n",
            "step: 90, loss: 0.0030528386123478413\n",
            "step: 100, loss: 0.0627811998128891\n",
            "step: 110, loss: 0.0018292561871930957\n",
            "step: 120, loss: 0.04289949685335159\n",
            "step: 130, loss: 0.001766806934028864\n",
            "step: 140, loss: 0.01070067472755909\n",
            "step: 150, loss: 0.0004042889049742371\n",
            "step: 160, loss: 0.0051720160990953445\n",
            "step: 170, loss: 0.020923800766468048\n",
            "step: 180, loss: 0.006710372865200043\n",
            "step: 190, loss: 0.002775453496724367\n",
            "step: 200, loss: 0.001840410870499909\n",
            "step: 210, loss: 0.008254650980234146\n",
            "step: 220, loss: 0.005098004825413227\n",
            "step: 230, loss: 0.0497889444231987\n",
            "step: 240, loss: 0.0007880938937887549\n",
            "step: 250, loss: 0.008777955546975136\n",
            "step: 260, loss: 0.05641230195760727\n",
            "step: 270, loss: 0.04136605188250542\n",
            "step: 280, loss: 0.07160801440477371\n",
            "step: 290, loss: 0.016888462007045746\n",
            "step: 300, loss: 0.014897353947162628\n",
            "step: 310, loss: 0.002064456231892109\n",
            "step: 320, loss: 0.22029389441013336\n",
            "step: 330, loss: 0.0033292328007519245\n",
            "step: 340, loss: 0.003123251721262932\n",
            "step: 350, loss: 0.0018908934434875846\n",
            "step: 360, loss: 0.007102487608790398\n",
            "step: 370, loss: 0.004523665644228458\n",
            "step: 380, loss: 0.004714195150882006\n",
            "step: 390, loss: 0.0136777488514781\n",
            "step: 400, loss: 0.00027766742277890444\n",
            "step: 410, loss: 0.0002673873968888074\n",
            "step: 420, loss: 0.001377498498186469\n",
            "step: 430, loss: 0.02418537251651287\n",
            "step: 440, loss: 0.0034636922646313906\n",
            "step: 450, loss: 0.0002074568037642166\n",
            "step: 460, loss: 0.002977213589474559\n",
            "step: 470, loss: 0.0004710783832706511\n",
            "step: 480, loss: 0.0006142063066363335\n",
            "step: 490, loss: 0.09853112697601318\n",
            "step: 500, loss: 0.0006809444166719913\n",
            "step: 510, loss: 0.006708059925585985\n",
            "step: 520, loss: 0.004592966288328171\n",
            "step: 530, loss: 0.003128133248537779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9235048678720444, f1=0.8941724941724941, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029088398441672325\n",
            "step: 10, loss: 0.0009015353280119598\n",
            "step: 20, loss: 0.09525834769010544\n",
            "step: 30, loss: 0.0015663934173062444\n",
            "step: 40, loss: 0.0015379594406113029\n",
            "step: 50, loss: 0.0023508816957473755\n",
            "step: 60, loss: 0.0003483761101961136\n",
            "step: 70, loss: 0.00021664160885848105\n",
            "step: 80, loss: 0.002188457176089287\n",
            "step: 90, loss: 0.00036889046896249056\n",
            "step: 100, loss: 0.0004609638126567006\n",
            "step: 110, loss: 0.000688511470798403\n",
            "step: 120, loss: 0.01812032423913479\n",
            "step: 130, loss: 0.10832741111516953\n",
            "step: 140, loss: 0.04985828697681427\n",
            "step: 150, loss: 0.022276924923062325\n",
            "step: 160, loss: 0.002752905711531639\n",
            "step: 170, loss: 0.0023855865001678467\n",
            "step: 180, loss: 0.0009531673276796937\n",
            "step: 190, loss: 0.0063118417747318745\n",
            "step: 200, loss: 0.0012989339884370565\n",
            "step: 210, loss: 0.0022688594181090593\n",
            "step: 220, loss: 0.003243141109123826\n",
            "step: 230, loss: 0.004197880160063505\n",
            "step: 240, loss: 0.0005717940512113273\n",
            "step: 250, loss: 0.0005813332390971482\n",
            "step: 260, loss: 0.00856807641685009\n",
            "step: 270, loss: 0.000510913785547018\n",
            "step: 280, loss: 0.0020644140895456076\n",
            "step: 290, loss: 0.07220793515443802\n",
            "step: 300, loss: 0.00012951558164786547\n",
            "step: 310, loss: 0.000771035032812506\n",
            "step: 320, loss: 0.000324392196489498\n",
            "step: 330, loss: 0.00612950325012207\n",
            "step: 340, loss: 0.0019810288213193417\n",
            "step: 350, loss: 0.008640659973025322\n",
            "step: 360, loss: 0.00042651043622754514\n",
            "step: 370, loss: 0.002063347725197673\n",
            "step: 380, loss: 0.0005362993688322604\n",
            "step: 390, loss: 0.0035701736342161894\n",
            "step: 400, loss: 0.03143167123198509\n",
            "step: 410, loss: 0.001307621831074357\n",
            "step: 420, loss: 0.0011962155112996697\n",
            "step: 430, loss: 0.0006536361761391163\n",
            "step: 440, loss: 0.0019401764729991555\n",
            "step: 450, loss: 0.00045146053889766335\n",
            "step: 460, loss: 0.00021910197392571718\n",
            "step: 470, loss: 0.0005798537749797106\n",
            "step: 480, loss: 0.04346496984362602\n",
            "step: 490, loss: 0.0009246842819266021\n",
            "step: 500, loss: 0.0001871815911727026\n",
            "step: 510, loss: 0.002310340292751789\n",
            "step: 520, loss: 0.0006381258717738092\n",
            "step: 530, loss: 0.00015077581338118762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9131423374261028, f1=0.8932481751824817, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010257872054353356\n",
            "step: 10, loss: 0.00010399777966085821\n",
            "step: 20, loss: 4.512314262683503e-05\n",
            "step: 30, loss: 5.309411426424049e-05\n",
            "step: 40, loss: 0.00010022974311141297\n",
            "step: 50, loss: 0.000108095642644912\n",
            "step: 60, loss: 0.0034215631894767284\n",
            "step: 70, loss: 0.00018166577501688153\n",
            "step: 80, loss: 0.0020355554297566414\n",
            "step: 90, loss: 0.0010147785069420934\n",
            "step: 100, loss: 0.00015119221643544734\n",
            "step: 110, loss: 0.0001845216320361942\n",
            "step: 120, loss: 0.0001661337009863928\n",
            "step: 130, loss: 0.12027030438184738\n",
            "step: 140, loss: 0.001913155079819262\n",
            "step: 150, loss: 0.018109699711203575\n",
            "step: 160, loss: 0.0019218545639887452\n",
            "step: 170, loss: 0.00047687769983895123\n",
            "step: 180, loss: 0.04735395312309265\n",
            "step: 190, loss: 0.0004010460979770869\n",
            "step: 200, loss: 0.0003213545132894069\n",
            "step: 210, loss: 0.0007547424756921828\n",
            "step: 220, loss: 0.006733099929988384\n",
            "step: 230, loss: 0.002223188290372491\n",
            "step: 240, loss: 0.003309824038296938\n",
            "step: 250, loss: 0.00020722050976473838\n",
            "step: 260, loss: 0.0018644254887476563\n",
            "step: 270, loss: 0.0002058974641840905\n",
            "step: 280, loss: 0.0002100480196531862\n",
            "step: 290, loss: 0.005449139513075352\n",
            "step: 300, loss: 0.00019663793500512838\n",
            "step: 310, loss: 0.00024777062935754657\n",
            "step: 320, loss: 0.006685202941298485\n",
            "step: 330, loss: 0.0015415396774187684\n",
            "step: 340, loss: 0.009023305028676987\n",
            "step: 350, loss: 0.0023337334860116243\n",
            "step: 360, loss: 0.009630984626710415\n",
            "step: 370, loss: 0.0023325509391725063\n",
            "step: 380, loss: 0.001957076136022806\n",
            "step: 390, loss: 0.008733408525586128\n",
            "step: 400, loss: 0.008049550466239452\n",
            "step: 410, loss: 0.0035800342448055744\n",
            "step: 420, loss: 0.0009894933318719268\n",
            "step: 430, loss: 0.0015803711721673608\n",
            "step: 440, loss: 0.04016588628292084\n",
            "step: 450, loss: 0.00036614100099541247\n",
            "step: 460, loss: 0.0023618037812411785\n",
            "step: 470, loss: 0.0013918188633397222\n",
            "step: 480, loss: 0.0008047963492572308\n",
            "step: 490, loss: 0.005022969096899033\n",
            "step: 500, loss: 0.008903506211936474\n",
            "step: 510, loss: 0.00030753089231438935\n",
            "step: 520, loss: 0.0010081450454890728\n",
            "step: 530, loss: 0.0005834268522448838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9124594719777674, f1=0.890125173852573, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00037755409721285105\n",
            "step: 10, loss: 0.27231255173683167\n",
            "step: 20, loss: 0.007426001131534576\n",
            "step: 30, loss: 0.0060399253852665424\n",
            "step: 40, loss: 0.0038399354089051485\n",
            "step: 50, loss: 0.02993875928223133\n",
            "step: 60, loss: 0.0014773056609556079\n",
            "step: 70, loss: 0.0018438119441270828\n",
            "step: 80, loss: 0.0011974438093602657\n",
            "step: 90, loss: 0.001957011641934514\n",
            "step: 100, loss: 0.0008820410002954304\n",
            "step: 110, loss: 0.005000808276236057\n",
            "step: 120, loss: 0.00016521233192179352\n",
            "step: 130, loss: 0.00027885293820872903\n",
            "step: 140, loss: 0.0035246426705271006\n",
            "step: 150, loss: 0.0017888364382088184\n",
            "step: 160, loss: 0.002046645851805806\n",
            "step: 170, loss: 0.00031374243553727865\n",
            "step: 180, loss: 0.003954057116061449\n",
            "step: 190, loss: 0.008642596192657948\n",
            "step: 200, loss: 0.021221281960606575\n",
            "step: 210, loss: 0.0005235313437879086\n",
            "step: 220, loss: 0.0002519477275200188\n",
            "step: 230, loss: 7.049251871649176e-05\n",
            "step: 240, loss: 0.00012108738883398473\n",
            "step: 250, loss: 8.690361573826522e-05\n",
            "step: 260, loss: 0.00012172155402367935\n",
            "step: 270, loss: 0.19901826977729797\n",
            "step: 280, loss: 0.0007142307003960013\n",
            "step: 290, loss: 0.001350595150142908\n",
            "step: 300, loss: 0.006945055443793535\n",
            "step: 310, loss: 0.001114160637371242\n",
            "step: 320, loss: 0.00032023267704062164\n",
            "step: 330, loss: 0.0017730375984683633\n",
            "step: 340, loss: 0.0026577136013656855\n",
            "step: 350, loss: 0.014084292575716972\n",
            "step: 360, loss: 0.00012153664283687249\n",
            "step: 370, loss: 0.00157071347348392\n",
            "step: 380, loss: 0.0001135811471613124\n",
            "step: 390, loss: 4.055532917845994e-05\n",
            "step: 400, loss: 8.761355275055394e-05\n",
            "step: 410, loss: 0.00017235707491636276\n",
            "step: 420, loss: 0.0007681387360207736\n",
            "step: 430, loss: 4.582901601679623e-05\n",
            "step: 440, loss: 0.00033131238888017833\n",
            "step: 450, loss: 5.637381400447339e-05\n",
            "step: 460, loss: 0.12904441356658936\n",
            "step: 470, loss: 0.003273898968473077\n",
            "step: 480, loss: 0.0032530655153095722\n",
            "step: 490, loss: 0.002731436863541603\n",
            "step: 500, loss: 0.0007328545907512307\n",
            "step: 510, loss: 0.12394294142723083\n",
            "step: 520, loss: 0.00039301745709963143\n",
            "step: 530, loss: 0.00010964859393425286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9061488673139159, f1=0.8818897637795275, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0183677077293396\n",
            "step: 10, loss: 0.00026460603112354875\n",
            "step: 20, loss: 0.00011333671864122152\n",
            "step: 30, loss: 0.0009893412934616208\n",
            "step: 40, loss: 0.0004462438228074461\n",
            "step: 50, loss: 0.02249986305832863\n",
            "step: 60, loss: 0.0002675138530321419\n",
            "step: 70, loss: 0.11557510495185852\n",
            "step: 80, loss: 0.0005569180939346552\n",
            "step: 90, loss: 0.00017982367717195302\n",
            "step: 100, loss: 0.001732091885060072\n",
            "step: 110, loss: 0.001060761627741158\n",
            "step: 120, loss: 6.379465048667043e-05\n",
            "step: 130, loss: 8.088488539215177e-05\n",
            "step: 140, loss: 9.967045480152592e-05\n",
            "step: 150, loss: 0.00021504929463844746\n",
            "step: 160, loss: 0.0016970803262665868\n",
            "step: 170, loss: 0.00045572035014629364\n",
            "step: 180, loss: 0.00035324000054970384\n",
            "step: 190, loss: 0.0011763775255531073\n",
            "step: 200, loss: 9.850117930909619e-05\n",
            "step: 210, loss: 0.0001291071966988966\n",
            "step: 220, loss: 0.06942345201969147\n",
            "step: 230, loss: 0.0002399972581770271\n",
            "step: 240, loss: 0.0001476878533139825\n",
            "step: 250, loss: 0.002188286278396845\n",
            "step: 260, loss: 0.000536278763320297\n",
            "step: 270, loss: 0.03210017830133438\n",
            "step: 280, loss: 0.00037918216548860073\n",
            "step: 290, loss: 0.00068616186035797\n",
            "step: 300, loss: 0.029231050983071327\n",
            "step: 310, loss: 0.00042598749860189855\n",
            "step: 320, loss: 0.00028983812080696225\n",
            "step: 330, loss: 0.0026251371018588543\n",
            "step: 340, loss: 0.0005245704087428749\n",
            "step: 350, loss: 0.0004698276170529425\n",
            "step: 360, loss: 0.003944427240639925\n",
            "step: 370, loss: 0.0008925388101488352\n",
            "step: 380, loss: 0.0001467162510380149\n",
            "step: 390, loss: 0.0004724547907244414\n",
            "step: 400, loss: 0.00012032182712573558\n",
            "step: 410, loss: 0.00041155581129714847\n",
            "step: 420, loss: 0.002150849439203739\n",
            "step: 430, loss: 0.0034478334710001945\n",
            "step: 440, loss: 0.002781730378046632\n",
            "step: 450, loss: 0.0006822791765443981\n",
            "step: 460, loss: 9.992014383897185e-05\n",
            "step: 470, loss: 0.01588597148656845\n",
            "step: 480, loss: 0.001401621731929481\n",
            "step: 490, loss: 0.00020952885097358376\n",
            "step: 500, loss: 0.00032728808582760394\n",
            "step: 510, loss: 0.05893156677484512\n",
            "step: 520, loss: 5.769269046140835e-05\n",
            "step: 530, loss: 0.0014549592742696404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.909763998149005, f1=0.8893023255813953, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06833478063344955\n",
            "step: 10, loss: 0.004271986428648233\n",
            "step: 20, loss: 0.016468379646539688\n",
            "step: 30, loss: 0.0010633267229422927\n",
            "step: 40, loss: 0.00164611276704818\n",
            "step: 50, loss: 0.0009568171808496118\n",
            "step: 60, loss: 2.9749020541203208e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 0.00023852403683122247\n",
            "step: 80, loss: 0.00011745314986910671\n",
            "step: 90, loss: 7.625503349117935e-05\n",
            "step: 100, loss: 0.0009174743900075555\n",
            "step: 110, loss: 6.159744953038171e-05\n",
            "step: 120, loss: 0.0006937713478691876\n",
            "step: 130, loss: 0.003295470727607608\n",
            "step: 140, loss: 0.0003897121059708297\n",
            "step: 150, loss: 0.0006035657715983689\n",
            "step: 160, loss: 0.00013302733714226633\n",
            "step: 170, loss: 0.0003979481989517808\n",
            "step: 180, loss: 7.439805631292984e-05\n",
            "step: 190, loss: 0.002529309131205082\n",
            "step: 200, loss: 0.00011909309978364035\n",
            "step: 210, loss: 0.00011783946683863178\n",
            "step: 220, loss: 0.0005567393964156508\n",
            "step: 230, loss: 0.0001968245196621865\n",
            "step: 240, loss: 9.632627188693732e-05\n",
            "step: 250, loss: 5.1131068175891414e-05\n",
            "step: 260, loss: 0.0004285505274310708\n",
            "step: 270, loss: 0.0007450886769220233\n",
            "step: 280, loss: 0.000648795859888196\n",
            "step: 290, loss: 0.028241081163287163\n",
            "step: 300, loss: 0.00045757999760098755\n",
            "step: 310, loss: 0.002126437844708562\n",
            "step: 320, loss: 0.0033274665474891663\n",
            "step: 330, loss: 0.0006178246694616973\n",
            "step: 340, loss: 0.00022915517911314964\n",
            "step: 350, loss: 0.0003452032105997205\n",
            "step: 360, loss: 4.7568213631166145e-05\n",
            "step: 370, loss: 0.00047120993258431554\n",
            "step: 380, loss: 0.00018473199452273548\n",
            "step: 390, loss: 0.015500592067837715\n",
            "step: 400, loss: 0.00074092234717682\n",
            "step: 410, loss: 0.0008004317642189562\n",
            "step: 420, loss: 6.513725384138525e-05\n",
            "step: 430, loss: 0.004287296906113625\n",
            "step: 440, loss: 0.0018348200246691704\n",
            "step: 450, loss: 0.006240553688257933\n",
            "step: 460, loss: 0.0007981153903529048\n",
            "step: 470, loss: 0.00010040039342129603\n",
            "step: 480, loss: 0.00020030456653330475\n",
            "step: 490, loss: 0.00032437534537166357\n",
            "step: 500, loss: 8.193973189918324e-05\n",
            "step: 510, loss: 0.0006008294294588268\n",
            "step: 520, loss: 0.0023930019233375788\n",
            "step: 530, loss: 4.487158730626106e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9158964879852127, f1=0.8982259570494864, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.6482244215440005e-05\n",
            "step: 10, loss: 9.438127744942904e-05\n",
            "step: 20, loss: 0.0015871721552684903\n",
            "step: 30, loss: 0.00010277400724589825\n",
            "step: 40, loss: 0.0002213238039985299\n",
            "step: 50, loss: 0.000253525358857587\n",
            "step: 60, loss: 1.8510610971134156e-05\n",
            "step: 70, loss: 0.0015108126681298018\n",
            "step: 80, loss: 5.3952462621964514e-05\n",
            "step: 90, loss: 0.00021926139015704393\n",
            "step: 100, loss: 0.01707920990884304\n",
            "step: 110, loss: 5.1742481446126476e-05\n",
            "step: 120, loss: 6.965984357520938e-05\n",
            "step: 130, loss: 0.0022292854264378548\n",
            "step: 140, loss: 0.0002840344386640936\n",
            "step: 150, loss: 4.144493141211569e-05\n",
            "step: 160, loss: 4.1690669604577124e-05\n",
            "step: 170, loss: 7.442457717843354e-05\n",
            "step: 180, loss: 5.5552038247697055e-05\n",
            "step: 190, loss: 0.00021228192781563848\n",
            "step: 200, loss: 0.0008381931693293154\n",
            "step: 210, loss: 0.004098972305655479\n",
            "step: 220, loss: 0.00011191319208592176\n",
            "step: 230, loss: 0.1585128754377365\n",
            "step: 240, loss: 5.8643639931688085e-05\n",
            "step: 250, loss: 0.00032632696093060076\n",
            "step: 260, loss: 5.358775888453238e-05\n",
            "step: 270, loss: 0.00034917614539153874\n",
            "step: 280, loss: 0.00010665607260307297\n",
            "step: 290, loss: 0.002153924899175763\n",
            "step: 300, loss: 0.0016135333571583033\n",
            "step: 310, loss: 0.052411846816539764\n",
            "step: 320, loss: 0.000719848379958421\n",
            "step: 330, loss: 0.0048605226911604404\n",
            "step: 340, loss: 8.194445399567485e-05\n",
            "step: 350, loss: 0.0027536824345588684\n",
            "step: 360, loss: 0.0001228743785759434\n",
            "step: 370, loss: 0.00040597940096631646\n",
            "step: 380, loss: 0.055611129850149155\n",
            "step: 390, loss: 0.0032137997914105654\n",
            "step: 400, loss: 0.0011725027579814196\n",
            "step: 410, loss: 0.0005324971862137318\n",
            "step: 420, loss: 7.2865019319579e-05\n",
            "step: 430, loss: 0.00026119116228073835\n",
            "step: 440, loss: 0.001325343968346715\n",
            "step: 450, loss: 0.005449975840747356\n",
            "step: 460, loss: 0.00015180899936240166\n",
            "step: 470, loss: 0.00022115834872238338\n",
            "step: 480, loss: 0.003466630121693015\n",
            "step: 490, loss: 0.000134655085275881\n",
            "step: 500, loss: 0.0001470384158892557\n",
            "step: 510, loss: 0.00249160616658628\n",
            "step: 520, loss: 4.955637996317819e-05\n",
            "step: 530, loss: 4.9769550969358534e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9137291280148423, f1=0.8987400839944004, best_f1=0.8941724941724941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.25483483215794e-05\n",
            "step: 10, loss: 0.002303576096892357\n",
            "step: 20, loss: 0.00016271760978270322\n",
            "step: 30, loss: 0.0009357495582662523\n",
            "step: 40, loss: 0.00044786828220821917\n",
            "step: 50, loss: 0.001170504023320973\n",
            "step: 60, loss: 3.531221591401845e-05\n",
            "step: 70, loss: 0.00027194805443286896\n",
            "step: 80, loss: 0.00030651851557195187\n",
            "step: 90, loss: 0.00018460489809513092\n",
            "step: 100, loss: 4.348172296886332e-05\n",
            "step: 110, loss: 0.0013057102914899588\n",
            "step: 120, loss: 4.873567013419233e-05\n",
            "step: 130, loss: 0.021505389362573624\n",
            "step: 140, loss: 3.8727161154383793e-05\n",
            "step: 150, loss: 0.0004440412449184805\n",
            "step: 160, loss: 6.736896466463804e-05\n",
            "step: 170, loss: 0.000477478199172765\n",
            "step: 180, loss: 5.838947254233062e-05\n",
            "step: 190, loss: 0.00027051212964579463\n",
            "step: 200, loss: 0.00019622119725681841\n",
            "step: 210, loss: 0.00015874276868999004\n",
            "step: 220, loss: 0.0023201461881399155\n",
            "step: 230, loss: 0.00046612470760010183\n",
            "step: 240, loss: 0.0006141237099654973\n",
            "step: 250, loss: 8.749694097787142e-05\n",
            "step: 260, loss: 3.719107553479262e-05\n",
            "step: 270, loss: 0.00018858499242924154\n",
            "step: 280, loss: 0.00010193981870543212\n",
            "step: 290, loss: 1.9572335077100433e-05\n",
            "step: 300, loss: 0.00010173994087381288\n",
            "step: 310, loss: 0.0032888895366340876\n",
            "step: 320, loss: 0.0006662346422672272\n",
            "step: 330, loss: 4.6704430133104324e-05\n",
            "step: 340, loss: 8.16095998743549e-05\n",
            "step: 350, loss: 4.500229624682106e-05\n",
            "step: 360, loss: 0.001224661129526794\n",
            "step: 370, loss: 3.883178942487575e-05\n",
            "step: 380, loss: 0.00015693434397689998\n",
            "step: 390, loss: 0.00019026707741431892\n",
            "step: 400, loss: 0.0014338534092530608\n",
            "step: 410, loss: 0.009933257475495338\n",
            "step: 420, loss: 0.00018162875494454056\n",
            "step: 430, loss: 0.0016426292713731527\n",
            "step: 440, loss: 0.00015522385365329683\n",
            "step: 450, loss: 8.540595445083454e-05\n",
            "step: 460, loss: 0.02564260922372341\n",
            "step: 470, loss: 0.00023803174553904682\n",
            "step: 480, loss: 4.455312591744587e-05\n",
            "step: 490, loss: 6.701423262711614e-05\n",
            "step: 500, loss: 0.0011643131729215384\n",
            "step: 510, loss: 8.427782449871302e-05\n",
            "step: 520, loss: 0.0031007619109004736\n",
            "step: 530, loss: 0.00259570125490427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9136822773186409, f1=0.8981097279852466, best_f1=0.8941724941724941\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 238.67it/s]\n",
            "load_f1 = 0.9173210161662818\n",
            "real_f1 = 0.9176688251618872\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 246.93it/s]\n"
          ]
        }
      ]
    }
  ]
}