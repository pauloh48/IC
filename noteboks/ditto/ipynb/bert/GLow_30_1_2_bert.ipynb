{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLow_30_1_2_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "b011EMgogNFP",
        "pnXzXaaYhstq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187e0339-9412-4d57-f6a3-485293d908d4"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 17.02 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 60.9 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 63.8 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 9.02 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 30.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 76.6 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.0 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 63.8 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=5f7900927dba9d2075808d61dbee130f5a11a21360e3c545d4ed6edf9b51c2f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=143113a2d18da688f20d4b3bd5d73ea7655626dc18864177b88a1df023fd933d\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067f5680-8341-4253-d3bc-f31b3578a1a3"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10085, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 10085 (delta 98), reused 131 (delta 59), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10085/10085), 14.95 MiB | 13.49 MiB/s, done.\n",
            "Resolving deltas: 100% (6902/6902), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-ym70gx0y\n",
            "Created temporary directory: /tmp/pip-req-tracker-m20tyguz\n",
            "Initialized build tracking at /tmp/pip-req-tracker-m20tyguz\n",
            "Created build tracker: /tmp/pip-req-tracker-m20tyguz\n",
            "Entered build tracker: /tmp/pip-req-tracker-m20tyguz\n",
            "Created temporary directory: /tmp/pip-install-s6qfvm15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-1eeoeqsu\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-m20tyguz'\n",
            "    Running setup.py (path:/tmp/pip-req-build-1eeoeqsu/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-ro3w1yn1\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-ro3w1yn1/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-1eeoeqsu has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-m20tyguz'\n",
            "Created temporary directory: /tmp/pip-unpack-bj7ib6kq\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-kh1loa2k\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-kh1loa2k\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-1eeoeqsu/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-1eeoeqsu/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-kh1loa2k\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-kh1loa2k/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=0a88c20167858771da39d82a19025da35f5dc99eabdb71f388d80fa8ce10bde9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ym70gx0y/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-m20tyguz'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3673d963-e0c6-41d3-f713-74ff21e1cc4e"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 30.5 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.47-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 66.5 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.47\n",
            "  Downloading botocore-1.27.47-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 59.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.47->awscli) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.47->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.47 botocore-1.27.47 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da59355-b1c8-461c-b548-abefc8c15675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "fc3e898b-ce3c-4512-c4af-eafea0607e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 982, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 982 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (982/982), 251.89 MiB | 16.15 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1273/1273), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8761d37-43e7-4f16-9eb1-10afe702a89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/GLow_30_1_2/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b37c6c-1b67-4867-a0bb-93103242a9f1"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 352kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 352kB/s]\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 52.0MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5509892106056213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4571428571428571, f1=0.32432432432432434, best_f1=0.32432432432432434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5032119750976562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4761904761904762, f1=0.42857142857142855, best_f1=0.42857142857142855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.611190676689148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5263157894736842, f1=0.3939393939393939, best_f1=0.3939393939393939\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30005261301994324\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.64, f1=0.5625000000000001, best_f1=0.5625000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2698204219341278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6956521739130435, f1=0.5454545454545454, best_f1=0.5454545454545454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4255441427230835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8387096774193549, f1=0.7333333333333334, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05421486869454384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8666666666666666, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017690740525722504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8125000000000001, f1=0.7333333333333334, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034423889592289925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8387096774193549, f1=0.689655172413793, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011498814448714256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8666666666666666, f1=0.7096774193548386, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005369340535253286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8666666666666666, f1=0.7096774193548386, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00391433946788311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00423455098643899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027639735490083694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004610847216099501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.6875000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 134917.52it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7647058823529412\n",
            "real_f1 = 0.742857142857143\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 197.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a170ecb-e521-40be-96d7-e6cec5c3b068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.61360764503479\n",
            "step: 10, loss: 0.6060507893562317\n",
            "step: 20, loss: 0.29252126812934875\n",
            "step: 30, loss: 0.0836595743894577\n",
            "step: 40, loss: 0.265250563621521\n",
            "step: 50, loss: 0.031064342707395554\n",
            "step: 60, loss: 0.1298033595085144\n",
            "step: 70, loss: 0.02207045815885067\n",
            "step: 80, loss: 0.1375972032546997\n",
            "step: 90, loss: 0.13806623220443726\n",
            "step: 100, loss: 0.006056226324290037\n",
            "step: 110, loss: 0.0412595197558403\n",
            "step: 120, loss: 0.003394680330529809\n",
            "step: 130, loss: 0.007184511981904507\n",
            "step: 140, loss: 0.0015734051121398807\n",
            "step: 150, loss: 0.009520241059362888\n",
            "step: 160, loss: 0.008436452597379684\n",
            "step: 170, loss: 0.024883117526769638\n",
            "step: 180, loss: 0.008980232290923595\n",
            "step: 190, loss: 0.003581976518034935\n",
            "step: 200, loss: 0.17186276614665985\n",
            "step: 210, loss: 0.00949359592050314\n",
            "step: 220, loss: 0.015583659522235394\n",
            "step: 230, loss: 0.06920652836561203\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9830124575311437, f1=0.9748858447488584, best_f1=0.9748858447488584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001778988866135478\n",
            "step: 10, loss: 0.0010173831833526492\n",
            "step: 20, loss: 0.149968221783638\n",
            "step: 30, loss: 0.2044285088777542\n",
            "step: 40, loss: 0.07572926580905914\n",
            "step: 50, loss: 0.0038105833809822798\n",
            "step: 60, loss: 0.005344164092093706\n",
            "step: 70, loss: 0.11141209304332733\n",
            "step: 80, loss: 0.002032521413639188\n",
            "step: 90, loss: 0.030820343643426895\n",
            "step: 100, loss: 0.026891076937317848\n",
            "step: 110, loss: 0.020916398614645004\n",
            "step: 120, loss: 0.13848377764225006\n",
            "step: 130, loss: 0.08541326224803925\n",
            "step: 140, loss: 0.00373519747518003\n",
            "step: 150, loss: 0.012934011407196522\n",
            "step: 160, loss: 0.003929352853447199\n",
            "step: 170, loss: 0.0024371235631406307\n",
            "step: 180, loss: 0.0065992423333227634\n",
            "step: 190, loss: 0.004592441953718662\n",
            "step: 200, loss: 0.0014284143690019846\n",
            "step: 210, loss: 0.00048077941755764186\n",
            "step: 220, loss: 0.07583026587963104\n",
            "step: 230, loss: 0.052222855389118195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9899216125419933, f1=0.9887387387387387, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0145880663767457\n",
            "step: 10, loss: 0.0008764764061197639\n",
            "step: 20, loss: 0.04063619300723076\n",
            "step: 30, loss: 0.07550598680973053\n",
            "step: 40, loss: 0.1502288579940796\n",
            "step: 50, loss: 0.00995203573256731\n",
            "step: 60, loss: 0.0029036090709269047\n",
            "step: 70, loss: 0.016418475657701492\n",
            "step: 80, loss: 0.0009566176449880004\n",
            "step: 90, loss: 0.011895876377820969\n",
            "step: 100, loss: 0.0023834598250687122\n",
            "step: 110, loss: 0.0009680389193817973\n",
            "step: 120, loss: 0.005158033687621355\n",
            "step: 130, loss: 0.0015312707982957363\n",
            "step: 140, loss: 0.0016029230318963528\n",
            "step: 150, loss: 0.007489652372896671\n",
            "step: 160, loss: 0.004613358993083239\n",
            "step: 170, loss: 0.0036733457818627357\n",
            "step: 180, loss: 0.004752924665808678\n",
            "step: 190, loss: 0.003860162803903222\n",
            "step: 200, loss: 0.0041845873929560184\n",
            "step: 210, loss: 0.0013404831988736987\n",
            "step: 220, loss: 0.0006585965165868402\n",
            "step: 230, loss: 0.0004888717085123062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9921436588103255, f1=0.987709497206704, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009260995429940522\n",
            "step: 10, loss: 0.0005836343043483794\n",
            "step: 20, loss: 0.0007531323935836554\n",
            "step: 30, loss: 0.0007050394196994603\n",
            "step: 40, loss: 0.0019735614769160748\n",
            "step: 50, loss: 0.0015966343926265836\n",
            "step: 60, loss: 0.0007179290987551212\n",
            "step: 70, loss: 0.0012957908911630511\n",
            "step: 80, loss: 0.002054451499134302\n",
            "step: 90, loss: 0.0010225195437669754\n",
            "step: 100, loss: 0.00034843917819671333\n",
            "step: 110, loss: 0.0006075240671634674\n",
            "step: 120, loss: 0.041891634464263916\n",
            "step: 130, loss: 0.010944724082946777\n",
            "step: 140, loss: 0.0009645503014326096\n",
            "step: 150, loss: 0.15359826385974884\n",
            "step: 160, loss: 0.03613527491688728\n",
            "step: 170, loss: 0.008423724211752415\n",
            "step: 180, loss: 0.0013857270823791623\n",
            "step: 190, loss: 0.003168809926137328\n",
            "step: 200, loss: 0.0009153328719548881\n",
            "step: 210, loss: 0.1021098643541336\n",
            "step: 220, loss: 0.0005398401990532875\n",
            "step: 230, loss: 0.09273406118154526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9876819708846584, f1=0.984304932735426, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018040071008726954\n",
            "step: 10, loss: 0.0011056715156883001\n",
            "step: 20, loss: 0.0027762586250901222\n",
            "step: 30, loss: 0.011055098846554756\n",
            "step: 40, loss: 0.0010542746167629957\n",
            "step: 50, loss: 0.00040891391108743846\n",
            "step: 60, loss: 0.0032910299487411976\n",
            "step: 70, loss: 0.0003126460360363126\n",
            "step: 80, loss: 0.00030490089557133615\n",
            "step: 90, loss: 0.0013095347676426172\n",
            "step: 100, loss: 0.0014885059790685773\n",
            "step: 110, loss: 0.0015356160001829267\n",
            "step: 120, loss: 0.00014682784967590123\n",
            "step: 130, loss: 0.004203615710139275\n",
            "step: 140, loss: 0.0007379847229458392\n",
            "step: 150, loss: 0.042046431452035904\n",
            "step: 160, loss: 0.000754447013605386\n",
            "step: 170, loss: 0.015302612446248531\n",
            "step: 180, loss: 0.00771752092987299\n",
            "step: 190, loss: 0.003425250295549631\n",
            "step: 200, loss: 0.001573648420162499\n",
            "step: 210, loss: 0.0012247200356796384\n",
            "step: 220, loss: 0.004613311495631933\n",
            "step: 230, loss: 0.0007262814324349165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9875141884222476, f1=0.976054732041049, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005588412168435752\n",
            "step: 10, loss: 0.0007142314570955932\n",
            "step: 20, loss: 0.0009316775831393898\n",
            "step: 30, loss: 0.0011912856716662645\n",
            "step: 40, loss: 0.0004058245103806257\n",
            "step: 50, loss: 0.00024239761114586145\n",
            "step: 60, loss: 0.00022450262622442096\n",
            "step: 70, loss: 0.020429521799087524\n",
            "step: 80, loss: 0.01049130316823721\n",
            "step: 90, loss: 0.015370482578873634\n",
            "step: 100, loss: 0.015816735103726387\n",
            "step: 110, loss: 0.002717433264479041\n",
            "step: 120, loss: 0.0008530538179911673\n",
            "step: 130, loss: 0.0013184596318751574\n",
            "step: 140, loss: 0.0002630821836646646\n",
            "step: 150, loss: 0.0004346657660789788\n",
            "step: 160, loss: 0.0007227725582197309\n",
            "step: 170, loss: 0.0005477203521877527\n",
            "step: 180, loss: 0.018213365226984024\n",
            "step: 190, loss: 0.01418870035558939\n",
            "step: 200, loss: 0.0006809272454120219\n",
            "step: 210, loss: 0.0006750098546035588\n",
            "step: 220, loss: 0.000954474788159132\n",
            "step: 230, loss: 0.024846738204360008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9888392857142857, f1=0.9822616407982262, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006073236581869423\n",
            "step: 10, loss: 0.00034739854163490236\n",
            "step: 20, loss: 0.0019870048854500055\n",
            "step: 30, loss: 0.0011379417264834046\n",
            "step: 40, loss: 0.00022489936964120716\n",
            "step: 50, loss: 0.0004991954192519188\n",
            "step: 60, loss: 0.0008439971134066582\n",
            "step: 70, loss: 0.018087120726704597\n",
            "step: 80, loss: 0.003673804458230734\n",
            "step: 90, loss: 0.000179390495759435\n",
            "step: 100, loss: 0.0002536542306188494\n",
            "step: 110, loss: 0.00045365767437033355\n",
            "step: 120, loss: 0.00018104715854860842\n",
            "step: 130, loss: 0.0007800596067681909\n",
            "step: 140, loss: 0.0003777756937779486\n",
            "step: 150, loss: 0.002387842396274209\n",
            "step: 160, loss: 0.09835189580917358\n",
            "step: 170, loss: 0.004696089308708906\n",
            "step: 180, loss: 0.002858423627912998\n",
            "step: 190, loss: 0.0011876702774316072\n",
            "step: 200, loss: 0.07623787969350815\n",
            "step: 210, loss: 0.00021621248743031174\n",
            "step: 220, loss: 0.0015257832128554583\n",
            "step: 230, loss: 0.010907434858381748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9887133182844244, f1=0.9807037457434733, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006454878021031618\n",
            "step: 10, loss: 0.0017904323758557439\n",
            "step: 20, loss: 0.00023976605734787881\n",
            "step: 30, loss: 0.0010267908219248056\n",
            "step: 40, loss: 0.0004767374775838107\n",
            "step: 50, loss: 0.0008320307242684066\n",
            "step: 60, loss: 0.00018863870354834944\n",
            "step: 70, loss: 0.0012320661917328835\n",
            "step: 80, loss: 0.0009726324351504445\n",
            "step: 90, loss: 0.0002097706455970183\n",
            "step: 100, loss: 0.001443590852431953\n",
            "step: 110, loss: 0.0030390319880098104\n",
            "step: 120, loss: 0.0005823900573886931\n",
            "step: 130, loss: 0.0009897882118821144\n",
            "step: 140, loss: 0.0007755103870294988\n",
            "step: 150, loss: 0.0019238763488829136\n",
            "step: 160, loss: 0.0014514618087559938\n",
            "step: 170, loss: 0.0006012141820974648\n",
            "step: 180, loss: 0.0005461935070343316\n",
            "step: 190, loss: 0.0002510317717678845\n",
            "step: 200, loss: 0.00024350860621780157\n",
            "step: 210, loss: 0.003192767035216093\n",
            "step: 220, loss: 0.00029919121880084276\n",
            "step: 230, loss: 0.0002080962440231815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9899216125419933, f1=0.9844097995545658, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016832497203722596\n",
            "step: 10, loss: 0.0005323499208316207\n",
            "step: 20, loss: 0.0006506656063720584\n",
            "step: 30, loss: 8.814300963422284e-05\n",
            "step: 40, loss: 0.04501849785447121\n",
            "step: 50, loss: 0.0027115014381706715\n",
            "step: 60, loss: 0.0068714916706085205\n",
            "step: 70, loss: 0.011982230469584465\n",
            "step: 80, loss: 0.0028041619807481766\n",
            "step: 90, loss: 0.0008011508034542203\n",
            "step: 100, loss: 0.0006667404668405652\n",
            "step: 110, loss: 0.00045190146192908287\n",
            "step: 120, loss: 8.201618038583547e-05\n",
            "step: 130, loss: 9.108885569730774e-05\n",
            "step: 140, loss: 0.00011377887858543545\n",
            "step: 150, loss: 0.00031616969499737024\n",
            "step: 160, loss: 0.00020624836906790733\n",
            "step: 170, loss: 0.0009808932663872838\n",
            "step: 180, loss: 0.0005565992323681712\n",
            "step: 190, loss: 0.0004446184611879289\n",
            "step: 200, loss: 0.0001305760524701327\n",
            "step: 210, loss: 0.00023295250139199197\n",
            "step: 220, loss: 8.090186747722328e-05\n",
            "step: 230, loss: 0.0006242808303795755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9909706546275394, f1=0.9876265466816648, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040160404751077294\n",
            "step: 10, loss: 4.560279558063485e-05\n",
            "step: 20, loss: 0.00016653905913699418\n",
            "step: 30, loss: 0.01359280850738287\n",
            "step: 40, loss: 9.769391908776015e-05\n",
            "step: 50, loss: 0.00021185669174883515\n",
            "step: 60, loss: 0.0018472623778507113\n",
            "step: 70, loss: 0.00021024569286964834\n",
            "step: 80, loss: 0.00019273464567959309\n",
            "step: 90, loss: 0.0003737047372851521\n",
            "step: 100, loss: 0.000681539298966527\n",
            "step: 110, loss: 0.009923738427460194\n",
            "step: 120, loss: 0.0002991129586007446\n",
            "step: 130, loss: 0.00011337329488014802\n",
            "step: 140, loss: 0.02940143086016178\n",
            "step: 150, loss: 0.014705050736665726\n",
            "step: 160, loss: 4.3562642531469464e-05\n",
            "step: 170, loss: 0.00022615341003984213\n",
            "step: 180, loss: 0.00011954231740674004\n",
            "step: 190, loss: 0.012239170260727406\n",
            "step: 200, loss: 6.141424819361418e-05\n",
            "step: 210, loss: 6.663975364062935e-05\n",
            "step: 220, loss: 0.00012248785060364753\n",
            "step: 230, loss: 0.00011796412582043558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9898534385569334, f1=0.9853438556933484, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.412796159973368e-05\n",
            "step: 10, loss: 7.121812814148143e-05\n",
            "step: 20, loss: 8.706131484359503e-05\n",
            "step: 30, loss: 0.026016762480139732\n",
            "step: 40, loss: 0.00018203201761934906\n",
            "step: 50, loss: 6.749082240276039e-05\n",
            "step: 60, loss: 0.00033063662704080343\n",
            "step: 70, loss: 0.00011610580259002745\n",
            "step: 80, loss: 7.820593600627035e-05\n",
            "step: 90, loss: 4.158243245910853e-05\n",
            "step: 100, loss: 0.0001048026024363935\n",
            "step: 110, loss: 0.0008053880883380771\n",
            "step: 120, loss: 4.033874211017974e-05\n",
            "step: 130, loss: 3.654247848317027e-05\n",
            "step: 140, loss: 0.0004939441569149494\n",
            "step: 150, loss: 0.023761603981256485\n",
            "step: 160, loss: 7.268826448125765e-05\n",
            "step: 170, loss: 0.03758939355611801\n",
            "step: 180, loss: 6.862762529635802e-05\n",
            "step: 190, loss: 0.00010068721167044714\n",
            "step: 200, loss: 0.0002997994306497276\n",
            "step: 210, loss: 4.7609893954358995e-05\n",
            "step: 220, loss: 7.22430195310153e-05\n",
            "step: 230, loss: 0.00010661759006325155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9898534385569334, f1=0.9830124575311437, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.49828977859579e-05\n",
            "step: 10, loss: 0.00012243054516147822\n",
            "step: 20, loss: 5.8164321671938524e-05\n",
            "step: 30, loss: 7.776536222081631e-05\n",
            "step: 40, loss: 8.5350991867017e-05\n",
            "step: 50, loss: 0.0001070585785782896\n",
            "step: 60, loss: 0.00012211456487420946\n",
            "step: 70, loss: 5.818101271870546e-05\n",
            "step: 80, loss: 7.474248559447005e-05\n",
            "step: 90, loss: 6.11531431786716e-05\n",
            "step: 100, loss: 9.668692655395716e-05\n",
            "step: 110, loss: 6.830893835285679e-05\n",
            "step: 120, loss: 0.000195507105672732\n",
            "step: 130, loss: 7.637035741936415e-05\n",
            "step: 140, loss: 6.812602805439383e-05\n",
            "step: 150, loss: 6.367081368807703e-05\n",
            "step: 160, loss: 5.899298412259668e-05\n",
            "step: 170, loss: 0.00010684924200177193\n",
            "step: 180, loss: 7.854123396100476e-05\n",
            "step: 190, loss: 4.9154481530422345e-05\n",
            "step: 200, loss: 3.371465209056623e-05\n",
            "step: 210, loss: 8.840776717988774e-05\n",
            "step: 220, loss: 6.350656622089446e-05\n",
            "step: 230, loss: 0.04616161435842514\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9876543209876544, f1=0.9843400447427293, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011964143050136045\n",
            "step: 10, loss: 0.0007645345758646727\n",
            "step: 20, loss: 9.88761312328279e-05\n",
            "step: 30, loss: 8.201796299545094e-05\n",
            "step: 40, loss: 0.00010119413491338491\n",
            "step: 50, loss: 0.00011763598013203591\n",
            "step: 60, loss: 4.7196121158776805e-05\n",
            "step: 70, loss: 2.90669959213119e-05\n",
            "step: 80, loss: 5.2563500503310934e-05\n",
            "step: 90, loss: 7.946918049128726e-05\n",
            "step: 100, loss: 2.435517126286868e-05\n",
            "step: 110, loss: 0.00020708564261440188\n",
            "step: 120, loss: 0.03305528312921524\n",
            "step: 130, loss: 6.890839722473174e-05\n",
            "step: 140, loss: 8.192471432266757e-05\n",
            "step: 150, loss: 6.55801486573182e-05\n",
            "step: 160, loss: 0.0001195739969261922\n",
            "step: 170, loss: 0.000105190061731264\n",
            "step: 180, loss: 8.088609320111573e-05\n",
            "step: 190, loss: 9.434366802452132e-05\n",
            "step: 200, loss: 8.468616579193622e-05\n",
            "step: 210, loss: 0.000323501939419657\n",
            "step: 220, loss: 6.201273936312646e-05\n",
            "step: 230, loss: 9.061618766281754e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9876543209876544, f1=0.9843400447427293, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.721736720763147e-05\n",
            "step: 10, loss: 0.012067052535712719\n",
            "step: 20, loss: 4.755270492751151e-05\n",
            "step: 30, loss: 7.953868043841794e-05\n",
            "step: 40, loss: 0.01075134240090847\n",
            "step: 50, loss: 4.983349208487198e-05\n",
            "step: 60, loss: 5.878967931494117e-05\n",
            "step: 70, loss: 4.867156167165376e-05\n",
            "step: 80, loss: 5.6865334045141935e-05\n",
            "step: 90, loss: 0.0001545220147818327\n",
            "step: 100, loss: 4.5626442442880943e-05\n",
            "step: 110, loss: 7.885901868576184e-05\n",
            "step: 120, loss: 2.332703661522828e-05\n",
            "step: 130, loss: 4.644016735255718e-05\n",
            "step: 140, loss: 0.00011988452752120793\n",
            "step: 150, loss: 7.067836850183085e-05\n",
            "step: 160, loss: 0.0003830104833468795\n",
            "step: 170, loss: 4.13642410421744e-05\n",
            "step: 180, loss: 4.296370025258511e-05\n",
            "step: 190, loss: 5.5205888202181086e-05\n",
            "step: 200, loss: 3.707929135998711e-05\n",
            "step: 210, loss: 9.444751776754856e-05\n",
            "step: 220, loss: 4.276497566024773e-05\n",
            "step: 230, loss: 5.5286698625423014e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9898305084745763, f1=0.983050847457627, best_f1=0.987709497206704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.621380190248601e-05\n",
            "step: 10, loss: 2.5874544007820077e-05\n",
            "step: 20, loss: 0.00024226454843301326\n",
            "step: 30, loss: 3.7664467527065426e-05\n",
            "step: 40, loss: 7.766697672195733e-05\n",
            "step: 50, loss: 0.03977149724960327\n",
            "step: 60, loss: 5.1355404139030725e-05\n",
            "step: 70, loss: 5.286749728838913e-05\n",
            "step: 80, loss: 6.447064515668899e-05\n",
            "step: 90, loss: 6.021445369697176e-05\n",
            "step: 100, loss: 5.204162880545482e-05\n",
            "step: 110, loss: 3.5067834687652066e-05\n",
            "step: 120, loss: 5.388964927988127e-05\n",
            "step: 130, loss: 7.498609193135053e-05\n",
            "step: 140, loss: 4.42821801698301e-05\n",
            "step: 150, loss: 8.088039612630382e-05\n",
            "step: 160, loss: 2.746899554040283e-05\n",
            "step: 170, loss: 2.7402136765886098e-05\n",
            "step: 180, loss: 4.384453859529458e-05\n",
            "step: 190, loss: 0.0001607047743164003\n",
            "step: 200, loss: 9.855616372078657e-05\n",
            "step: 210, loss: 0.00011933585483347997\n",
            "step: 220, loss: 4.761904710903764e-05\n",
            "step: 230, loss: 4.723175516119227e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9887133182844244, f1=0.9853438556933484, best_f1=0.987709497206704\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 159.44it/s]\n",
            "load_f1 = 0.9921612541993281\n",
            "real_f1 = 0.9899216125419933\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca37d3e-7a77-4722-f43d-9b9dfdbd3e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 486kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.90MB/s]\n",
            "Downloading: 100% 440M/440M [00:09<00:00, 44.3MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.613934338092804\n",
            "step: 10, loss: 0.5082128643989563\n",
            "step: 20, loss: 0.4452504813671112\n",
            "step: 30, loss: 0.06997070461511612\n",
            "step: 40, loss: 0.11014629155397415\n",
            "step: 50, loss: 0.18721503019332886\n",
            "step: 60, loss: 0.06895910203456879\n",
            "step: 70, loss: 0.09611670672893524\n",
            "step: 80, loss: 0.0457177609205246\n",
            "step: 90, loss: 0.2640192210674286\n",
            "step: 100, loss: 0.07924122363328934\n",
            "step: 110, loss: 0.08022493869066238\n",
            "step: 120, loss: 0.12507089972496033\n",
            "step: 130, loss: 0.07762271910905838\n",
            "step: 140, loss: 0.10442501306533813\n",
            "step: 150, loss: 0.09120186418294907\n",
            "step: 160, loss: 0.0383390411734581\n",
            "step: 170, loss: 0.18734470009803772\n",
            "step: 180, loss: 0.12905855476856232\n",
            "step: 190, loss: 0.023265380412340164\n",
            "step: 200, loss: 0.13216833770275116\n",
            "step: 210, loss: 0.14655452966690063\n",
            "step: 220, loss: 0.24144193530082703\n",
            "step: 230, loss: 0.18649506568908691\n",
            "step: 240, loss: 0.07451999932527542\n",
            "step: 250, loss: 0.04067869856953621\n",
            "step: 260, loss: 0.07409151643514633\n",
            "step: 270, loss: 0.01939357817173004\n",
            "step: 280, loss: 0.04048396646976471\n",
            "step: 290, loss: 0.042394667863845825\n",
            "step: 300, loss: 0.027263274416327477\n",
            "step: 310, loss: 0.0906909853219986\n",
            "step: 320, loss: 0.06287535279989243\n",
            "step: 330, loss: 0.057860054075717926\n",
            "step: 340, loss: 0.03125019744038582\n",
            "step: 350, loss: 0.04064049571752548\n",
            "step: 360, loss: 0.03070683591067791\n",
            "step: 370, loss: 0.10907158255577087\n",
            "step: 380, loss: 0.11332546919584274\n",
            "step: 390, loss: 0.1589994579553604\n",
            "step: 400, loss: 0.3631631135940552\n",
            "step: 410, loss: 0.05283457413315773\n",
            "step: 420, loss: 0.053227271884679794\n",
            "step: 430, loss: 0.20440471172332764\n",
            "step: 440, loss: 0.031467255204916\n",
            "step: 450, loss: 0.007331761531531811\n",
            "step: 460, loss: 0.010424582287669182\n",
            "step: 470, loss: 0.15816155076026917\n",
            "step: 480, loss: 0.04498807713389397\n",
            "step: 490, loss: 0.05979424715042114\n",
            "step: 500, loss: 0.0762535110116005\n",
            "step: 510, loss: 0.08564624190330505\n",
            "step: 520, loss: 0.04503433406352997\n",
            "step: 530, loss: 0.0028658669907599688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.93202062822316, f1=0.927536231884058, best_f1=0.927536231884058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1956147700548172\n",
            "step: 10, loss: 0.14010795950889587\n",
            "step: 20, loss: 0.007230376824736595\n",
            "step: 30, loss: 0.02317636087536812\n",
            "step: 40, loss: 0.07165294140577316\n",
            "step: 50, loss: 0.08454151451587677\n",
            "step: 60, loss: 0.014049187302589417\n",
            "step: 70, loss: 0.03037022426724434\n",
            "step: 80, loss: 0.07194548100233078\n",
            "step: 90, loss: 0.041661448776721954\n",
            "step: 100, loss: 0.020001620054244995\n",
            "step: 110, loss: 0.0987481102347374\n",
            "step: 120, loss: 0.03953232988715172\n",
            "step: 130, loss: 0.1718534529209137\n",
            "step: 140, loss: 0.04623088985681534\n",
            "step: 150, loss: 0.06081540510058403\n",
            "step: 160, loss: 0.012563962489366531\n",
            "step: 170, loss: 0.03166091442108154\n",
            "step: 180, loss: 0.03495490178465843\n",
            "step: 190, loss: 0.054093074053525925\n",
            "step: 200, loss: 0.007727175951004028\n",
            "step: 210, loss: 0.0951017290353775\n",
            "step: 220, loss: 0.13484549522399902\n",
            "step: 230, loss: 0.005941242910921574\n",
            "step: 240, loss: 0.049584146589040756\n",
            "step: 250, loss: 0.09885513037443161\n",
            "step: 260, loss: 0.002990849083289504\n",
            "step: 270, loss: 0.21787047386169434\n",
            "step: 280, loss: 0.0030217573512345552\n",
            "step: 290, loss: 0.03377397730946541\n",
            "step: 300, loss: 0.1130533367395401\n",
            "step: 310, loss: 0.01932665891945362\n",
            "step: 320, loss: 0.05203250050544739\n",
            "step: 330, loss: 0.04138000309467316\n",
            "step: 340, loss: 0.02669086493551731\n",
            "step: 350, loss: 0.0010877667227759957\n",
            "step: 360, loss: 0.09356673061847687\n",
            "step: 370, loss: 0.1608896255493164\n",
            "step: 380, loss: 0.08814701437950134\n",
            "step: 390, loss: 0.04781879484653473\n",
            "step: 400, loss: 0.046957846730947495\n",
            "step: 410, loss: 0.01615333743393421\n",
            "step: 420, loss: 0.01300432812422514\n",
            "step: 430, loss: 0.018483096733689308\n",
            "step: 440, loss: 0.1576240211725235\n",
            "step: 450, loss: 0.020416073501110077\n",
            "step: 460, loss: 0.015114519745111465\n",
            "step: 470, loss: 0.009904108941555023\n",
            "step: 480, loss: 0.21572451293468475\n",
            "step: 490, loss: 0.019455745816230774\n",
            "step: 500, loss: 0.19059783220291138\n",
            "step: 510, loss: 0.012401782907545567\n",
            "step: 520, loss: 0.06397898495197296\n",
            "step: 530, loss: 0.05924451723694801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9356505401596994, f1=0.9373246024321795, best_f1=0.9373246024321795\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03346934914588928\n",
            "step: 10, loss: 0.0541888065636158\n",
            "step: 20, loss: 0.12902846932411194\n",
            "step: 30, loss: 0.12529242038726807\n",
            "step: 40, loss: 0.029717106372117996\n",
            "step: 50, loss: 0.13159780204296112\n",
            "step: 60, loss: 0.02439424768090248\n",
            "step: 70, loss: 0.010078513994812965\n",
            "step: 80, loss: 0.002970106201246381\n",
            "step: 90, loss: 0.005487777292728424\n",
            "step: 100, loss: 0.08866236358880997\n",
            "step: 110, loss: 0.0010315888794139028\n",
            "step: 120, loss: 0.004780990071594715\n",
            "step: 130, loss: 0.00698839919641614\n",
            "step: 140, loss: 0.025573445484042168\n",
            "step: 150, loss: 0.014395667240023613\n",
            "step: 160, loss: 0.0064708092249929905\n",
            "step: 170, loss: 0.060806598514318466\n",
            "step: 180, loss: 0.007326286286115646\n",
            "step: 190, loss: 0.013348439708352089\n",
            "step: 200, loss: 0.05384363234043121\n",
            "step: 210, loss: 0.039264075458049774\n",
            "step: 220, loss: 0.11485884338617325\n",
            "step: 230, loss: 0.06627652049064636\n",
            "step: 240, loss: 0.0031007127836346626\n",
            "step: 250, loss: 0.025571776553988457\n",
            "step: 260, loss: 0.008420799858868122\n",
            "step: 270, loss: 0.02386505715548992\n",
            "step: 280, loss: 0.11756488680839539\n",
            "step: 290, loss: 0.0018904379103332758\n",
            "step: 300, loss: 0.008340919390320778\n",
            "step: 310, loss: 0.05358602851629257\n",
            "step: 320, loss: 0.032012827694416046\n",
            "step: 330, loss: 0.0023750776890665293\n",
            "step: 340, loss: 0.014050230383872986\n",
            "step: 350, loss: 0.013914233073592186\n",
            "step: 360, loss: 0.05564792454242706\n",
            "step: 370, loss: 0.010071555152535439\n",
            "step: 380, loss: 0.013834935612976551\n",
            "step: 390, loss: 0.03584376350045204\n",
            "step: 400, loss: 0.012088505551218987\n",
            "step: 410, loss: 0.006354578770697117\n",
            "step: 420, loss: 0.14969882369041443\n",
            "step: 430, loss: 0.011920103803277016\n",
            "step: 440, loss: 0.002132501918822527\n",
            "step: 450, loss: 0.08592624217271805\n",
            "step: 460, loss: 0.006167762912809849\n",
            "step: 470, loss: 0.005790104623883963\n",
            "step: 480, loss: 0.0033562059979885817\n",
            "step: 490, loss: 0.008619189262390137\n",
            "step: 500, loss: 0.020189370959997177\n",
            "step: 510, loss: 0.008360831066966057\n",
            "step: 520, loss: 0.12516364455223083\n",
            "step: 530, loss: 0.04122161120176315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9375866851595007, f1=0.9364858599907279, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010036027990281582\n",
            "step: 10, loss: 0.0065558189526200294\n",
            "step: 20, loss: 0.004528503865003586\n",
            "step: 30, loss: 0.0010049828561022878\n",
            "step: 40, loss: 0.004993733484297991\n",
            "step: 50, loss: 0.0005125848692841828\n",
            "step: 60, loss: 0.0007800692692399025\n",
            "step: 70, loss: 0.00853730272501707\n",
            "step: 80, loss: 0.0029322232585400343\n",
            "step: 90, loss: 0.03672856092453003\n",
            "step: 100, loss: 0.06479310989379883\n",
            "step: 110, loss: 0.01806642860174179\n",
            "step: 120, loss: 0.00018564745550975204\n",
            "step: 130, loss: 0.0006582285859622061\n",
            "step: 140, loss: 0.0010411322582513094\n",
            "step: 150, loss: 0.0011261026374995708\n",
            "step: 160, loss: 0.04512978345155716\n",
            "step: 170, loss: 0.043578751385211945\n",
            "step: 180, loss: 0.0062391310930252075\n",
            "step: 190, loss: 0.0326557420194149\n",
            "step: 200, loss: 0.061354849487543106\n",
            "step: 210, loss: 0.024330290034413338\n",
            "step: 220, loss: 0.00154464493971318\n",
            "step: 230, loss: 0.02029651403427124\n",
            "step: 240, loss: 0.0073782941326498985\n",
            "step: 250, loss: 0.010680004954338074\n",
            "step: 260, loss: 0.008982696570456028\n",
            "step: 270, loss: 0.02917974255979061\n",
            "step: 280, loss: 0.015648556873202324\n",
            "step: 290, loss: 0.014416366815567017\n",
            "step: 300, loss: 0.00019843342306558043\n",
            "step: 310, loss: 0.07886991649866104\n",
            "step: 320, loss: 0.019532978534698486\n",
            "step: 330, loss: 0.010830240324139595\n",
            "step: 340, loss: 0.07274630665779114\n",
            "step: 350, loss: 0.03630658611655235\n",
            "step: 360, loss: 0.039976220577955246\n",
            "step: 370, loss: 0.007580047473311424\n",
            "step: 380, loss: 0.000982298399321735\n",
            "step: 390, loss: 0.01696426421403885\n",
            "step: 400, loss: 0.05888251215219498\n",
            "step: 410, loss: 0.003723912639543414\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 420, loss: 0.048240967094898224\n",
            "step: 430, loss: 0.10430088639259338\n",
            "step: 440, loss: 0.00722967553883791\n",
            "step: 450, loss: 0.0019129014108330011\n",
            "step: 460, loss: 0.0020404160022735596\n",
            "step: 470, loss: 0.007343672215938568\n",
            "step: 480, loss: 0.0038516575004905462\n",
            "step: 490, loss: 0.0010678413091227412\n",
            "step: 500, loss: 0.010023959912359715\n",
            "step: 510, loss: 0.003828303422778845\n",
            "step: 520, loss: 0.04377691075205803\n",
            "step: 530, loss: 0.09692911058664322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9317972350230416, f1=0.9349220898258479, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012839573435485363\n",
            "step: 10, loss: 0.0493207685649395\n",
            "step: 20, loss: 0.04077386111021042\n",
            "step: 30, loss: 0.0013693490764126182\n",
            "step: 40, loss: 0.053036294877529144\n",
            "step: 50, loss: 0.0037944503128528595\n",
            "step: 60, loss: 0.0046800789423286915\n",
            "step: 70, loss: 0.02637835405766964\n",
            "step: 80, loss: 0.00043273603660054505\n",
            "step: 90, loss: 0.0004761040036100894\n",
            "step: 100, loss: 0.007375899702310562\n",
            "step: 110, loss: 0.00038152484921738505\n",
            "step: 120, loss: 0.0004714365932159126\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.0005595898837782443\n",
            "step: 140, loss: 0.0004121988022234291\n",
            "step: 150, loss: 0.002262201625853777\n",
            "step: 160, loss: 0.00039949960773810744\n",
            "step: 170, loss: 0.044757865369319916\n",
            "step: 180, loss: 0.001883669407106936\n",
            "step: 190, loss: 0.0034446311183273792\n",
            "step: 200, loss: 0.003538762219250202\n",
            "step: 210, loss: 0.007229969836771488\n",
            "step: 220, loss: 0.0025058311875909567\n",
            "step: 230, loss: 0.004095837473869324\n",
            "step: 240, loss: 0.0023783687502145767\n",
            "step: 250, loss: 0.0019853352569043636\n",
            "step: 260, loss: 0.0010527988197281957\n",
            "step: 270, loss: 0.00016674063226673752\n",
            "step: 280, loss: 0.0038085924461483955\n",
            "step: 290, loss: 0.12465646117925644\n",
            "step: 300, loss: 0.003673934144899249\n",
            "step: 310, loss: 0.0011199722066521645\n",
            "step: 320, loss: 0.0745108351111412\n",
            "step: 330, loss: 0.02580823190510273\n",
            "step: 340, loss: 0.002064164262264967\n",
            "step: 350, loss: 0.006912263575941324\n",
            "step: 360, loss: 0.007277794647961855\n",
            "step: 370, loss: 0.00909241009503603\n",
            "step: 380, loss: 0.0013737418921664357\n",
            "step: 390, loss: 0.0002650823735166341\n",
            "step: 400, loss: 0.0011744149960577488\n",
            "step: 410, loss: 0.0002386741543887183\n",
            "step: 420, loss: 0.0013529341667890549\n",
            "step: 430, loss: 0.00039975406252779067\n",
            "step: 440, loss: 0.003147589275613427\n",
            "step: 450, loss: 0.004347576294094324\n",
            "step: 460, loss: 0.10527914762496948\n",
            "step: 470, loss: 0.02410237118601799\n",
            "step: 480, loss: 0.007835080847144127\n",
            "step: 490, loss: 0.0008592365775257349\n",
            "step: 500, loss: 0.0026379134505987167\n",
            "step: 510, loss: 0.009951031766831875\n",
            "step: 520, loss: 0.07097494602203369\n",
            "step: 530, loss: 0.003615850117057562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9366391184573002, f1=0.9402099497946144, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010396366007626057\n",
            "step: 10, loss: 0.002793720457702875\n",
            "step: 20, loss: 0.11026394367218018\n",
            "step: 30, loss: 0.0008188716601580381\n",
            "step: 40, loss: 0.0017879604129120708\n",
            "step: 50, loss: 0.014004328288137913\n",
            "step: 60, loss: 0.000283078319625929\n",
            "step: 70, loss: 0.0004212549247313291\n",
            "step: 80, loss: 0.007956408895552158\n",
            "step: 90, loss: 0.000473715306725353\n",
            "step: 100, loss: 0.0013079320779070258\n",
            "step: 110, loss: 0.01610811986029148\n",
            "step: 120, loss: 0.0032987461891025305\n",
            "step: 130, loss: 0.0002750915300566703\n",
            "step: 140, loss: 0.003113705664873123\n",
            "step: 150, loss: 0.0008387485868297517\n",
            "step: 160, loss: 0.0003085996431764215\n",
            "step: 170, loss: 0.001013087690807879\n",
            "step: 180, loss: 0.0013439857866615057\n",
            "step: 190, loss: 0.00033698894549161196\n",
            "step: 200, loss: 0.00024111953098326921\n",
            "step: 210, loss: 0.0011466031428426504\n",
            "step: 220, loss: 0.046398088335990906\n",
            "step: 230, loss: 0.027985168620944023\n",
            "step: 240, loss: 0.10077822208404541\n",
            "step: 250, loss: 0.0009960670722648501\n",
            "step: 260, loss: 0.01877768337726593\n",
            "step: 270, loss: 0.11677844077348709\n",
            "step: 280, loss: 0.004582786001265049\n",
            "step: 290, loss: 0.0004875946033280343\n",
            "step: 300, loss: 0.004140487406402826\n",
            "step: 310, loss: 0.0006182852666825056\n",
            "step: 320, loss: 0.0004775654524564743\n",
            "step: 330, loss: 0.005474055651575327\n",
            "step: 340, loss: 0.050843559205532074\n",
            "step: 350, loss: 0.011586022563278675\n",
            "step: 360, loss: 0.018565841019153595\n",
            "step: 370, loss: 0.0016359372530132532\n",
            "step: 380, loss: 0.0005920235416851938\n",
            "step: 390, loss: 0.029160238802433014\n",
            "step: 400, loss: 0.00029319405439309776\n",
            "step: 410, loss: 0.0004007990937680006\n",
            "step: 420, loss: 0.006374744698405266\n",
            "step: 430, loss: 0.0009057938004843891\n",
            "step: 440, loss: 0.00026845268439501524\n",
            "step: 450, loss: 0.000431251828558743\n",
            "step: 460, loss: 0.00016507912368979305\n",
            "step: 470, loss: 0.0003501226019579917\n",
            "step: 480, loss: 0.0020065505523234606\n",
            "step: 490, loss: 0.011343443766236305\n",
            "step: 500, loss: 0.0009669616119936109\n",
            "step: 510, loss: 0.03878592327237129\n",
            "step: 520, loss: 0.0011211598757654428\n",
            "step: 530, loss: 0.05348493158817291\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9266697804764129, f1=0.9357541899441341, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008534913649782538\n",
            "step: 10, loss: 0.003850545035675168\n",
            "step: 20, loss: 0.0006376230739988387\n",
            "step: 30, loss: 0.0015533401165157557\n",
            "step: 40, loss: 0.00020776537712663412\n",
            "step: 50, loss: 0.006746172439306974\n",
            "step: 60, loss: 0.00433209678158164\n",
            "step: 70, loss: 0.0002775152097456157\n",
            "step: 80, loss: 0.0053441585041582584\n",
            "step: 90, loss: 0.00015637799515388906\n",
            "step: 100, loss: 0.00013909919653087854\n",
            "step: 110, loss: 0.0042875539511442184\n",
            "step: 120, loss: 0.0040756696835160255\n",
            "step: 130, loss: 0.0003795068769250065\n",
            "step: 140, loss: 0.00017285517242271453\n",
            "step: 150, loss: 0.0006774748908355832\n",
            "step: 160, loss: 0.0009464994654990733\n",
            "step: 170, loss: 0.0010614024940878153\n",
            "step: 180, loss: 0.0022003338672220707\n",
            "step: 190, loss: 0.0028458419255912304\n",
            "step: 200, loss: 0.00015491642989218235\n",
            "step: 210, loss: 0.0007967435522004962\n",
            "step: 220, loss: 0.00021377517259679735\n",
            "step: 230, loss: 0.02284861169755459\n",
            "step: 240, loss: 0.0002659448073245585\n",
            "step: 250, loss: 0.00031806380138732493\n",
            "step: 260, loss: 6.0087440942879766e-05\n",
            "step: 270, loss: 9.366855374537408e-05\n",
            "step: 280, loss: 0.0003123840142507106\n",
            "step: 290, loss: 9.294680785387754e-05\n",
            "step: 300, loss: 0.0024208456743508577\n",
            "step: 310, loss: 0.07721994817256927\n",
            "step: 320, loss: 0.001036832807585597\n",
            "step: 330, loss: 0.004226783290505409\n",
            "step: 340, loss: 0.013594734482467175\n",
            "step: 350, loss: 0.000301238673273474\n",
            "step: 360, loss: 0.03400915488600731\n",
            "step: 370, loss: 0.0009042947785928845\n",
            "step: 380, loss: 0.0020022511016577482\n",
            "step: 390, loss: 9.418661647941917e-05\n",
            "step: 400, loss: 0.008458890952169895\n",
            "step: 410, loss: 0.0018016245448961854\n",
            "step: 420, loss: 0.0003077961737290025\n",
            "step: 430, loss: 5.532994327950291e-05\n",
            "step: 440, loss: 0.00014639945584349334\n",
            "step: 450, loss: 0.00012910277291666716\n",
            "step: 460, loss: 0.0011904799612239003\n",
            "step: 470, loss: 0.004262649919837713\n",
            "step: 480, loss: 0.2016754150390625\n",
            "step: 490, loss: 0.00019067231914959848\n",
            "step: 500, loss: 0.00035242928424850106\n",
            "step: 510, loss: 0.005133882164955139\n",
            "step: 520, loss: 0.01761259324848652\n",
            "step: 530, loss: 0.0053643835708498955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9347623485554521, f1=0.9377323420074348, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018206539971288294\n",
            "step: 10, loss: 0.012868838384747505\n",
            "step: 20, loss: 0.00015348580200225115\n",
            "step: 30, loss: 0.008971890434622765\n",
            "step: 40, loss: 0.0005534648080356419\n",
            "step: 50, loss: 0.0035205124877393246\n",
            "step: 60, loss: 0.0020258035510778427\n",
            "step: 70, loss: 0.00011267873924225569\n",
            "step: 80, loss: 0.01128922589123249\n",
            "step: 90, loss: 0.0005116018000990152\n",
            "step: 100, loss: 4.889712727162987e-05\n",
            "step: 110, loss: 0.0003761308907996863\n",
            "step: 120, loss: 0.054790809750556946\n",
            "step: 130, loss: 0.00012732576578855515\n",
            "step: 140, loss: 0.013537583872675896\n",
            "step: 150, loss: 0.000723730365280062\n",
            "step: 160, loss: 0.10025345534086227\n",
            "step: 170, loss: 0.19775685667991638\n",
            "step: 180, loss: 0.0002681297482922673\n",
            "step: 190, loss: 0.0034166653640568256\n",
            "step: 200, loss: 0.002010868163779378\n",
            "step: 210, loss: 0.0031014918349683285\n",
            "step: 220, loss: 0.002344634849578142\n",
            "step: 230, loss: 0.00012638873886317015\n",
            "step: 240, loss: 0.00046304488205350935\n",
            "step: 250, loss: 0.0007252034265547991\n",
            "step: 260, loss: 0.00011047320003854111\n",
            "step: 270, loss: 0.008527781814336777\n",
            "step: 280, loss: 0.03619414567947388\n",
            "step: 290, loss: 0.00013441122428048402\n",
            "step: 300, loss: 0.0010376116260886192\n",
            "step: 310, loss: 0.00019447928934823722\n",
            "step: 320, loss: 0.00043510159594006836\n",
            "step: 330, loss: 6.423712329706177e-05\n",
            "step: 340, loss: 8.58246858115308e-05\n",
            "step: 350, loss: 0.00016133183089550585\n",
            "step: 360, loss: 0.0031917900778353214\n",
            "step: 370, loss: 0.0001997996005229652\n",
            "step: 380, loss: 0.0005214763223193586\n",
            "step: 390, loss: 0.12901125848293304\n",
            "step: 400, loss: 0.0008633381803520024\n",
            "step: 410, loss: 0.00013105219113640487\n",
            "step: 420, loss: 0.002133836504071951\n",
            "step: 430, loss: 0.05257078632712364\n",
            "step: 440, loss: 0.0002277519815834239\n",
            "step: 450, loss: 0.00017868027498479933\n",
            "step: 460, loss: 0.000316916179144755\n",
            "step: 470, loss: 8.89099610503763e-05\n",
            "step: 480, loss: 8.756417082622647e-05\n",
            "step: 490, loss: 0.0011698753805831075\n",
            "step: 500, loss: 0.00014033864135853946\n",
            "step: 510, loss: 0.00015008196351118386\n",
            "step: 520, loss: 0.0017946739681065083\n",
            "step: 530, loss: 0.007545233704149723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9348946135831382, f1=0.935813953488372, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009197330102324486\n",
            "step: 10, loss: 7.613015623064712e-05\n",
            "step: 20, loss: 0.0003250973823014647\n",
            "step: 30, loss: 0.00093937071505934\n",
            "step: 40, loss: 0.0005452515324577689\n",
            "step: 50, loss: 2.780376235023141e-05\n",
            "step: 60, loss: 5.155291364644654e-05\n",
            "step: 70, loss: 0.003646361641585827\n",
            "step: 80, loss: 0.0005817731725983322\n",
            "step: 90, loss: 8.040168177103624e-05\n",
            "step: 100, loss: 7.257080869749188e-05\n",
            "step: 110, loss: 5.589974898612127e-05\n",
            "step: 120, loss: 0.0003953043487854302\n",
            "step: 130, loss: 0.0002663469931576401\n",
            "step: 140, loss: 8.906058792490512e-05\n",
            "step: 150, loss: 5.550196874537505e-05\n",
            "step: 160, loss: 0.001206117682158947\n",
            "step: 170, loss: 0.0036416472867131233\n",
            "step: 180, loss: 6.126991502242163e-05\n",
            "step: 190, loss: 0.00047457675100304186\n",
            "step: 200, loss: 5.084211807115935e-05\n",
            "step: 210, loss: 0.00018732687749434263\n",
            "step: 220, loss: 8.670843089930713e-05\n",
            "step: 230, loss: 6.12403528066352e-05\n",
            "step: 240, loss: 8.408571011386812e-05\n",
            "step: 250, loss: 5.9709731431212276e-05\n",
            "step: 260, loss: 0.004378249403089285\n",
            "step: 270, loss: 5.7752298744162545e-05\n",
            "step: 280, loss: 0.007220068946480751\n",
            "step: 290, loss: 0.05945190042257309\n",
            "step: 300, loss: 0.00013673539797309786\n",
            "step: 310, loss: 0.02529202401638031\n",
            "step: 320, loss: 0.00011448845179984346\n",
            "step: 330, loss: 0.0001895786845125258\n",
            "step: 340, loss: 0.00010102095257025212\n",
            "step: 350, loss: 0.0021284939721226692\n",
            "step: 360, loss: 4.936772893415764e-05\n",
            "step: 370, loss: 0.0022157924249768257\n",
            "step: 380, loss: 0.0008394194883294404\n",
            "step: 390, loss: 0.05942973122000694\n",
            "step: 400, loss: 0.022158833220601082\n",
            "step: 410, loss: 0.009444921277463436\n",
            "step: 420, loss: 5.38441636308562e-05\n",
            "step: 430, loss: 8.380404324270785e-05\n",
            "step: 440, loss: 0.0033312987070530653\n",
            "step: 450, loss: 0.00041719546425156295\n",
            "step: 460, loss: 0.00016389560187235475\n",
            "step: 470, loss: 0.003516304073855281\n",
            "step: 480, loss: 0.0002570610085967928\n",
            "step: 490, loss: 0.0007272197399288416\n",
            "step: 500, loss: 0.0023559383116662502\n",
            "step: 510, loss: 0.00030071751098148525\n",
            "step: 520, loss: 0.0014173304662108421\n",
            "step: 530, loss: 0.00019261091074440628\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9266445811642213, f1=0.9337717238139972, best_f1=0.9364858599907279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003528505621943623\n",
            "step: 10, loss: 6.845030293334275e-05\n",
            "step: 20, loss: 0.0002752391155809164\n",
            "step: 30, loss: 8.178374264389277e-05\n",
            "step: 40, loss: 0.00020149818737991154\n",
            "step: 50, loss: 0.0009968988597393036\n",
            "step: 60, loss: 0.00017280399333685637\n",
            "step: 70, loss: 9.30982205318287e-05\n",
            "step: 80, loss: 9.974028216674924e-05\n",
            "step: 90, loss: 0.0005101111019030213\n",
            "step: 100, loss: 0.00012255701585672796\n",
            "step: 110, loss: 5.3365325584309176e-05\n",
            "step: 120, loss: 0.00017148282495327294\n",
            "step: 130, loss: 0.00019300385611131787\n",
            "step: 140, loss: 0.01163062360137701\n",
            "step: 150, loss: 0.0014187500346451998\n",
            "step: 160, loss: 0.0002591057273093611\n",
            "step: 170, loss: 0.0001691480865702033\n",
            "step: 180, loss: 0.00014271019608713686\n",
            "step: 190, loss: 8.936748781707138e-05\n",
            "step: 200, loss: 0.0004736543050967157\n",
            "step: 210, loss: 0.003344177268445492\n",
            "step: 220, loss: 0.18883664906024933\n",
            "step: 230, loss: 8.705165237188339e-05\n",
            "step: 240, loss: 0.0001432945573469624\n",
            "step: 250, loss: 0.00011299422476440668\n",
            "step: 260, loss: 0.0011543100699782372\n",
            "step: 270, loss: 0.000959704746492207\n",
            "step: 280, loss: 0.00014640422887168825\n",
            "step: 290, loss: 0.00010216025111731142\n",
            "step: 300, loss: 8.180550503311679e-05\n",
            "step: 310, loss: 7.582149555673823e-05\n",
            "step: 320, loss: 0.0006477255374193192\n",
            "step: 330, loss: 5.49009746464435e-05\n",
            "step: 340, loss: 0.00021476979600265622\n",
            "step: 350, loss: 0.00531064672395587\n",
            "step: 360, loss: 0.00023330387193709612\n",
            "step: 370, loss: 0.0068403747864067554\n",
            "step: 380, loss: 0.00023970133042894304\n",
            "step: 390, loss: 0.00014352967264130712\n",
            "step: 400, loss: 0.0016609993763267994\n",
            "step: 410, loss: 0.029473386704921722\n",
            "step: 420, loss: 0.0001349167723674327\n",
            "step: 430, loss: 6.647852569585666e-05\n",
            "step: 440, loss: 0.019313538447022438\n",
            "step: 450, loss: 0.00022129577700980008\n",
            "step: 460, loss: 0.00019608855654951185\n",
            "step: 470, loss: 0.0001392869744449854\n",
            "step: 480, loss: 7.348298095166683e-05\n",
            "step: 490, loss: 0.00011934716167161241\n",
            "step: 500, loss: 0.004385172389447689\n",
            "step: 510, loss: 5.7223904150305316e-05\n",
            "step: 520, loss: 0.00019064053776673973\n",
            "step: 530, loss: 0.0016413796693086624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9398601398601399, f1=0.9453703703703704, best_f1=0.9453703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026088280603289604\n",
            "step: 10, loss: 0.00024582637706771493\n",
            "step: 20, loss: 0.00017898080113809556\n",
            "step: 30, loss: 0.00013054636656306684\n",
            "step: 40, loss: 0.0010877910535782576\n",
            "step: 50, loss: 0.001215091673657298\n",
            "step: 60, loss: 0.06667208671569824\n",
            "step: 70, loss: 0.0007215086370706558\n",
            "step: 80, loss: 0.00011206440103705972\n",
            "step: 90, loss: 0.00027385420980863273\n",
            "step: 100, loss: 0.0012538303853943944\n",
            "step: 110, loss: 0.00023444740509148687\n",
            "step: 120, loss: 4.1340139432577416e-05\n",
            "step: 130, loss: 0.00011579989222809672\n",
            "step: 140, loss: 0.03395124524831772\n",
            "step: 150, loss: 8.24354647193104e-05\n",
            "step: 160, loss: 0.009449001401662827\n",
            "step: 170, loss: 0.00019655746291391551\n",
            "step: 180, loss: 0.00013310502981767058\n",
            "step: 190, loss: 0.009019912220537663\n",
            "step: 200, loss: 0.010314377024769783\n",
            "step: 210, loss: 0.0003691008605528623\n",
            "step: 220, loss: 0.0029862921219319105\n",
            "step: 230, loss: 0.0004108032153453678\n",
            "step: 240, loss: 0.0005727591342292726\n",
            "step: 250, loss: 4.1389226680621505e-05\n",
            "step: 260, loss: 7.832699338905513e-05\n",
            "step: 270, loss: 0.00027681211940944195\n",
            "step: 280, loss: 0.0001315215340582654\n",
            "step: 290, loss: 9.896483243210241e-05\n",
            "step: 300, loss: 0.00016050244448706508\n",
            "step: 310, loss: 0.0007531174924224615\n",
            "step: 320, loss: 8.646656351629645e-05\n",
            "step: 330, loss: 0.00031528074759989977\n",
            "step: 340, loss: 0.0003487810608930886\n",
            "step: 350, loss: 0.0014570645289495587\n",
            "step: 360, loss: 0.0007904174271970987\n",
            "step: 370, loss: 0.00020526461594272405\n",
            "step: 380, loss: 6.328208110062405e-05\n",
            "step: 390, loss: 4.817174340132624e-05\n",
            "step: 400, loss: 2.7037331165047362e-05\n",
            "step: 410, loss: 5.737476021749899e-05\n",
            "step: 420, loss: 0.00011016060307156295\n",
            "step: 430, loss: 3.711373210535385e-05\n",
            "step: 440, loss: 5.1300463383086026e-05\n",
            "step: 450, loss: 0.000689059728756547\n",
            "step: 460, loss: 4.808003723155707e-05\n",
            "step: 470, loss: 5.652193067362532e-05\n",
            "step: 480, loss: 0.0012906717602163553\n",
            "step: 490, loss: 9.744527051225305e-05\n",
            "step: 500, loss: 0.0008215019479393959\n",
            "step: 510, loss: 5.413613689597696e-05\n",
            "step: 520, loss: 0.00019548155250959098\n",
            "step: 530, loss: 0.00011572824587346986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9332079021636878, f1=0.9361305361305362, best_f1=0.9453703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.28559903614223e-05\n",
            "step: 10, loss: 2.6210103897028603e-05\n",
            "step: 20, loss: 8.33989106467925e-05\n",
            "step: 30, loss: 0.0007514553726650774\n",
            "step: 40, loss: 0.00016407007933594286\n",
            "step: 50, loss: 0.05101954564452171\n",
            "step: 60, loss: 0.002887825248762965\n",
            "step: 70, loss: 9.197460894938558e-05\n",
            "step: 80, loss: 3.526731234160252e-05\n",
            "step: 90, loss: 0.0005101598799228668\n",
            "step: 100, loss: 0.0005079185939393938\n",
            "step: 110, loss: 0.00011025385174434632\n",
            "step: 120, loss: 2.005648821068462e-05\n",
            "step: 130, loss: 0.00048135468387044966\n",
            "step: 140, loss: 9.852150833467022e-05\n",
            "step: 150, loss: 4.383974737720564e-05\n",
            "step: 160, loss: 2.6947887818096206e-05\n",
            "step: 170, loss: 3.348099562572315e-05\n",
            "step: 180, loss: 3.004677637363784e-05\n",
            "step: 190, loss: 9.155164298135787e-05\n",
            "step: 200, loss: 0.00037530012195929885\n",
            "step: 210, loss: 3.529547757352702e-05\n",
            "step: 220, loss: 0.00015286101552192122\n",
            "step: 230, loss: 2.1378888050094247e-05\n",
            "step: 240, loss: 0.0001695889513939619\n",
            "step: 250, loss: 7.36885194783099e-05\n",
            "step: 260, loss: 4.9978072638623416e-05\n",
            "step: 270, loss: 8.219020674005151e-05\n",
            "step: 280, loss: 0.003779578721150756\n",
            "step: 290, loss: 5.2900955779477954e-05\n",
            "step: 300, loss: 0.0004583392583299428\n",
            "step: 310, loss: 0.00018898621783591807\n",
            "step: 320, loss: 1.8920396541943774e-05\n",
            "step: 330, loss: 4.152918700128794e-05\n",
            "step: 340, loss: 2.5059380277525634e-05\n",
            "step: 350, loss: 0.00020433194003999233\n",
            "step: 360, loss: 0.0006684482796117663\n",
            "step: 370, loss: 9.57736701820977e-05\n",
            "step: 380, loss: 0.0001544225960969925\n",
            "step: 390, loss: 0.016340386122465134\n",
            "step: 400, loss: 0.00017343703075312078\n",
            "step: 410, loss: 5.351311483536847e-05\n",
            "step: 420, loss: 0.007027212530374527\n",
            "step: 430, loss: 0.023030713200569153\n",
            "step: 440, loss: 2.812850652844645e-05\n",
            "step: 450, loss: 8.541863644495606e-05\n",
            "step: 460, loss: 0.00012725281703751534\n",
            "step: 470, loss: 6.020966247888282e-05\n",
            "step: 480, loss: 8.627366332802922e-05\n",
            "step: 490, loss: 0.0003156780148856342\n",
            "step: 500, loss: 3.995738006778993e-05\n",
            "step: 510, loss: 0.0064966860227286816\n",
            "step: 520, loss: 0.0002938823599833995\n",
            "step: 530, loss: 4.2006922740256414e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9354078264969354, f1=0.9400749063670412, best_f1=0.9453703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0069867572747170925\n",
            "step: 10, loss: 2.1978550648782402e-05\n",
            "step: 20, loss: 2.2591719243791886e-05\n",
            "step: 30, loss: 0.001010425272397697\n",
            "step: 40, loss: 3.852403096971102e-05\n",
            "step: 50, loss: 0.00010705285967560485\n",
            "step: 60, loss: 2.8865848435088992e-05\n",
            "step: 70, loss: 3.230285437894054e-05\n",
            "step: 80, loss: 5.4828553402330726e-05\n",
            "step: 90, loss: 0.025303808972239494\n",
            "step: 100, loss: 0.00020779561600647867\n",
            "step: 110, loss: 2.9943350455141626e-05\n",
            "step: 120, loss: 2.9748742235824466e-05\n",
            "step: 130, loss: 3.252423266530968e-05\n",
            "step: 140, loss: 3.142815694445744e-05\n",
            "step: 150, loss: 0.0003869189531542361\n",
            "step: 160, loss: 2.9255428671604022e-05\n",
            "step: 170, loss: 0.00015462585724890232\n",
            "step: 180, loss: 6.0776052123401314e-05\n",
            "step: 190, loss: 1.872288885351736e-05\n",
            "step: 200, loss: 3.8271617086138576e-05\n",
            "step: 210, loss: 3.039821058337111e-05\n",
            "step: 220, loss: 3.004091195180081e-05\n",
            "step: 230, loss: 1.557892574055586e-05\n",
            "step: 240, loss: 0.00015038893616292626\n",
            "step: 250, loss: 3.379500412847847e-05\n",
            "step: 260, loss: 9.962396143237129e-05\n",
            "step: 270, loss: 1.588801205798518e-05\n",
            "step: 280, loss: 2.9610702767968178e-05\n",
            "step: 290, loss: 3.340472903801128e-05\n",
            "step: 300, loss: 1.9542545487638563e-05\n",
            "step: 310, loss: 0.001608490594662726\n",
            "step: 320, loss: 0.0021312707103788853\n",
            "step: 330, loss: 0.001338658039458096\n",
            "step: 340, loss: 2.405669874860905e-05\n",
            "step: 350, loss: 2.831097663147375e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 360, loss: 5.537905963137746e-05\n",
            "step: 370, loss: 0.00020022249373141676\n",
            "step: 380, loss: 0.00022561097284778953\n",
            "step: 390, loss: 0.00019773848180193454\n",
            "step: 400, loss: 2.7643805879051797e-05\n",
            "step: 410, loss: 0.0012808385072275996\n",
            "step: 420, loss: 2.229130404884927e-05\n",
            "step: 430, loss: 0.023649537935853004\n",
            "step: 440, loss: 2.0625988327083178e-05\n",
            "step: 450, loss: 3.288373045506887e-05\n",
            "step: 460, loss: 0.001330300117842853\n",
            "step: 470, loss: 2.371645314269699e-05\n",
            "step: 480, loss: 5.4468244343297556e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 490, loss: 0.000681342207826674\n",
            "step: 500, loss: 0.00011310596892144531\n",
            "step: 510, loss: 0.015189441852271557\n",
            "step: 520, loss: 0.001005065510980785\n",
            "step: 530, loss: 4.191395782981999e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9310670443814919, f1=0.9400749063670412, best_f1=0.9453703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011343284859322011\n",
            "step: 10, loss: 1.7962176571018063e-05\n",
            "step: 20, loss: 1.6081754438346252e-05\n",
            "step: 30, loss: 1.7944372302736156e-05\n",
            "step: 40, loss: 6.974130519665778e-05\n",
            "step: 50, loss: 0.005358532071113586\n",
            "step: 60, loss: 3.874915637425147e-05\n",
            "step: 70, loss: 1.902065923786722e-05\n",
            "step: 80, loss: 0.00011833675671368837\n",
            "step: 90, loss: 2.351675539102871e-05\n",
            "step: 100, loss: 0.0003983063797932118\n",
            "step: 110, loss: 5.0462960643926635e-05\n",
            "step: 120, loss: 2.591531847428996e-05\n",
            "step: 130, loss: 0.012313400395214558\n",
            "step: 140, loss: 0.0009043316822499037\n",
            "step: 150, loss: 1.5213552615023218e-05\n",
            "step: 160, loss: 0.0006573151331394911\n",
            "step: 170, loss: 0.003227067878469825\n",
            "step: 180, loss: 3.45355729223229e-05\n",
            "step: 190, loss: 0.0012694899924099445\n",
            "step: 200, loss: 8.133373194141313e-05\n",
            "step: 210, loss: 2.1423413272714242e-05\n",
            "step: 220, loss: 2.1520292648347095e-05\n",
            "step: 230, loss: 9.740392124513164e-05\n",
            "step: 240, loss: 1.983288530027494e-05\n",
            "step: 250, loss: 3.687810749397613e-05\n",
            "step: 260, loss: 0.0006777202943339944\n",
            "step: 270, loss: 1.5031213479232974e-05\n",
            "step: 280, loss: 1.4092564924794715e-05\n",
            "step: 290, loss: 0.020047668367624283\n",
            "step: 300, loss: 2.9670536605408415e-05\n",
            "step: 310, loss: 0.018427779898047447\n",
            "step: 320, loss: 0.0002042718551820144\n",
            "step: 330, loss: 2.980045428557787e-05\n",
            "step: 340, loss: 0.0001878845359897241\n",
            "step: 350, loss: 7.30787796783261e-05\n",
            "step: 360, loss: 0.00032677041599527\n",
            "step: 370, loss: 1.9516277461661957e-05\n",
            "step: 380, loss: 2.5420460588065907e-05\n",
            "step: 390, loss: 0.02098827250301838\n",
            "step: 400, loss: 0.00021952114184387028\n",
            "step: 410, loss: 2.3613338271388784e-05\n",
            "step: 420, loss: 0.00780859449878335\n",
            "step: 430, loss: 0.13558706641197205\n",
            "step: 440, loss: 0.00017801232752390206\n",
            "step: 450, loss: 5.048049570177682e-05\n",
            "step: 460, loss: 9.958689770428464e-05\n",
            "step: 470, loss: 0.00016270442574750632\n",
            "step: 480, loss: 3.077154906350188e-05\n",
            "step: 490, loss: 3.969116005464457e-05\n",
            "step: 500, loss: 1.794352829165291e-05\n",
            "step: 510, loss: 0.00684317946434021\n",
            "step: 520, loss: 0.00027740001678466797\n",
            "step: 530, loss: 0.0009888290660455823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9324515824279641, f1=0.9360902255639098, best_f1=0.9453703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.509543578024022e-05\n",
            "step: 10, loss: 3.971498517785221e-05\n",
            "step: 20, loss: 1.9147395505569875e-05\n",
            "step: 30, loss: 8.107349276542664e-05\n",
            "step: 40, loss: 0.07351942360401154\n",
            "step: 50, loss: 0.00013193758786655962\n",
            "step: 60, loss: 9.879401659418363e-06\n",
            "step: 70, loss: 0.0004056655161548406\n",
            "step: 80, loss: 2.3393578885588795e-05\n",
            "step: 90, loss: 2.153152490791399e-05\n",
            "step: 100, loss: 2.112494985340163e-05\n",
            "step: 110, loss: 3.181569627486169e-05\n",
            "step: 120, loss: 6.512519030366093e-05\n",
            "step: 130, loss: 0.10380297154188156\n",
            "step: 140, loss: 3.409739292692393e-05\n",
            "step: 150, loss: 4.901258216705173e-05\n",
            "step: 160, loss: 5.1674760470632464e-05\n",
            "step: 170, loss: 2.060010410787072e-05\n",
            "step: 180, loss: 1.038228401739616e-05\n",
            "step: 190, loss: 4.782074029208161e-05\n",
            "step: 200, loss: 4.593242556438781e-05\n",
            "step: 210, loss: 0.0037274484056979418\n",
            "step: 220, loss: 3.0492637961287983e-05\n",
            "step: 230, loss: 0.0006253741448745131\n",
            "step: 240, loss: 9.913330723065883e-05\n",
            "step: 250, loss: 0.0005415150662884116\n",
            "step: 260, loss: 4.08561245421879e-05\n",
            "step: 270, loss: 2.261546796944458e-05\n",
            "step: 280, loss: 2.044003122136928e-05\n",
            "step: 290, loss: 1.4423962966247927e-05\n",
            "step: 300, loss: 3.7067777157062665e-05\n",
            "step: 310, loss: 3.9569429645780474e-05\n",
            "step: 320, loss: 4.177675145911053e-05\n",
            "step: 330, loss: 5.249411697150208e-05\n",
            "step: 340, loss: 2.4158796804840676e-05\n",
            "step: 350, loss: 1.4047667718841694e-05\n",
            "step: 360, loss: 0.0020915570203214884\n",
            "step: 370, loss: 1.5880519640631974e-05\n",
            "step: 380, loss: 0.0003338168316986412\n",
            "step: 390, loss: 4.997497671865858e-05\n",
            "step: 400, loss: 2.629195478220936e-05\n",
            "step: 410, loss: 2.1203306459938176e-05\n",
            "step: 420, loss: 4.439058102434501e-05\n",
            "step: 430, loss: 0.0003072820545639843\n",
            "step: 440, loss: 5.726010931539349e-05\n",
            "step: 450, loss: 1.2878126653959043e-05\n",
            "step: 460, loss: 5.086036981083453e-05\n",
            "step: 470, loss: 0.00315316254273057\n",
            "step: 480, loss: 1.0736186595750041e-05\n",
            "step: 490, loss: 1.3179879715607967e-05\n",
            "step: 500, loss: 3.19811915687751e-05\n",
            "step: 510, loss: 5.5342839914374053e-05\n",
            "step: 520, loss: 1.949744728335645e-05\n",
            "step: 530, loss: 0.0001341097813565284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.935285781766651, f1=0.9407894736842105, best_f1=0.9453703703703704\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:20, 280.33it/s]\n",
            "load_f1 = 0.9376181474480151\n",
            "real_f1 = 0.9346497414198403\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:15, 277.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128c2f22-83cd-4aa7-9e06-400485309a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5434144139289856\n",
            "step: 10, loss: 0.3823399543762207\n",
            "step: 20, loss: 0.37477490305900574\n",
            "step: 30, loss: 0.32258114218711853\n",
            "step: 40, loss: 0.17696405947208405\n",
            "step: 50, loss: 0.44034701585769653\n",
            "step: 60, loss: 0.18311454355716705\n",
            "step: 70, loss: 0.12624257802963257\n",
            "step: 80, loss: 0.21569672226905823\n",
            "step: 90, loss: 0.363359659910202\n",
            "step: 100, loss: 0.394257128238678\n",
            "step: 110, loss: 0.2372412234544754\n",
            "step: 120, loss: 0.19035498797893524\n",
            "step: 130, loss: 0.19953617453575134\n",
            "step: 140, loss: 0.21994824707508087\n",
            "step: 150, loss: 0.19273599982261658\n",
            "step: 160, loss: 0.22796499729156494\n",
            "step: 170, loss: 0.3086649775505066\n",
            "step: 180, loss: 0.08711553364992142\n",
            "step: 190, loss: 0.13265776634216309\n",
            "step: 200, loss: 0.2739883065223694\n",
            "step: 210, loss: 0.1839381605386734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6404293381037568, f1=0.6323268206039075, best_f1=0.6323268206039075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1075240895152092\n",
            "step: 10, loss: 0.16614452004432678\n",
            "step: 20, loss: 0.21451720595359802\n",
            "step: 30, loss: 0.2866430878639221\n",
            "step: 40, loss: 0.2459515780210495\n",
            "step: 50, loss: 0.14319941401481628\n",
            "step: 60, loss: 0.4001561403274536\n",
            "step: 70, loss: 0.11650519073009491\n",
            "step: 80, loss: 0.1990671157836914\n",
            "step: 90, loss: 0.09421484172344208\n",
            "step: 100, loss: 0.008277940563857555\n",
            "step: 110, loss: 0.07497961074113846\n",
            "step: 120, loss: 0.18063023686408997\n",
            "step: 130, loss: 0.034392595291137695\n",
            "step: 140, loss: 0.18351875245571136\n",
            "step: 150, loss: 0.23681522905826569\n",
            "step: 160, loss: 0.13535195589065552\n",
            "step: 170, loss: 0.1488286852836609\n",
            "step: 180, loss: 0.15940889716148376\n",
            "step: 190, loss: 0.182070791721344\n",
            "step: 200, loss: 0.04856667295098305\n",
            "step: 210, loss: 0.12433665990829468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6830265848670757, f1=0.6708333333333333, best_f1=0.6708333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05691620334982872\n",
            "step: 10, loss: 0.1504661589860916\n",
            "step: 20, loss: 0.16778451204299927\n",
            "step: 30, loss: 0.25222429633140564\n",
            "step: 40, loss: 0.11161648482084274\n",
            "step: 50, loss: 0.06341027468442917\n",
            "step: 60, loss: 0.24596813321113586\n",
            "step: 70, loss: 0.07870879024267197\n",
            "step: 80, loss: 0.21909897029399872\n",
            "step: 90, loss: 0.06598716229200363\n",
            "step: 100, loss: 0.2469601333141327\n",
            "step: 110, loss: 0.23451107740402222\n",
            "step: 120, loss: 0.09911496192216873\n",
            "step: 130, loss: 0.1133035197854042\n",
            "step: 140, loss: 0.1439017653465271\n",
            "step: 150, loss: 0.2673822343349457\n",
            "step: 160, loss: 0.03618275374174118\n",
            "step: 170, loss: 0.15383918583393097\n",
            "step: 180, loss: 0.13727037608623505\n",
            "step: 190, loss: 0.1931876838207245\n",
            "step: 200, loss: 0.0668308287858963\n",
            "step: 210, loss: 0.11872367560863495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7090558766859344, f1=0.6602687140115163, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13032038509845734\n",
            "step: 10, loss: 0.08147811144590378\n",
            "step: 20, loss: 0.13474252820014954\n",
            "step: 30, loss: 0.15347692370414734\n",
            "step: 40, loss: 0.020334392786026\n",
            "step: 50, loss: 0.17192529141902924\n",
            "step: 60, loss: 0.10283730179071426\n",
            "step: 70, loss: 0.3344650864601135\n",
            "step: 80, loss: 0.18116846680641174\n",
            "step: 90, loss: 0.027297867462038994\n",
            "step: 100, loss: 0.34588733315467834\n",
            "step: 110, loss: 0.2002459168434143\n",
            "step: 120, loss: 0.1327800154685974\n",
            "step: 130, loss: 0.4090738296508789\n",
            "step: 140, loss: 0.12034253031015396\n",
            "step: 150, loss: 0.048721540719270706\n",
            "step: 160, loss: 0.038944046944379807\n",
            "step: 170, loss: 0.09557562321424484\n",
            "step: 180, loss: 0.3871647119522095\n",
            "step: 190, loss: 0.06750144064426422\n",
            "step: 200, loss: 0.25029847025871277\n",
            "step: 210, loss: 0.15257437527179718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6725978647686832, f1=0.6323809523809524, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29246899485588074\n",
            "step: 10, loss: 0.0663321241736412\n",
            "step: 20, loss: 0.2531580626964569\n",
            "step: 30, loss: 0.046676263213157654\n",
            "step: 40, loss: 0.11544736474752426\n",
            "step: 50, loss: 0.0685892403125763\n",
            "step: 60, loss: 0.0877435952425003\n",
            "step: 70, loss: 0.19317007064819336\n",
            "step: 80, loss: 0.08835513889789581\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.0792083740234375\n",
            "step: 100, loss: 0.006859172601252794\n",
            "step: 110, loss: 0.25867781043052673\n",
            "step: 120, loss: 0.16783161461353302\n",
            "step: 130, loss: 0.11209680885076523\n",
            "step: 140, loss: 0.0689258873462677\n",
            "step: 150, loss: 0.14476346969604492\n",
            "step: 160, loss: 0.18658819794654846\n",
            "step: 170, loss: 0.08254426717758179\n",
            "step: 180, loss: 0.04942438751459122\n",
            "step: 190, loss: 0.022585127502679825\n",
            "step: 200, loss: 0.05238315090537071\n",
            "step: 210, loss: 0.03189175948500633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6845124282982792, f1=0.661567877629063, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.036258671432733536\n",
            "step: 10, loss: 0.044209130108356476\n",
            "step: 20, loss: 0.08524291217327118\n",
            "step: 30, loss: 0.01522210892289877\n",
            "step: 40, loss: 0.03478199243545532\n",
            "step: 50, loss: 0.03269393742084503\n",
            "step: 60, loss: 0.039633214473724365\n",
            "step: 70, loss: 0.012402401305735111\n",
            "step: 80, loss: 0.08546929061412811\n",
            "step: 90, loss: 0.1155882403254509\n",
            "step: 100, loss: 0.017045538872480392\n",
            "step: 110, loss: 0.016512887552380562\n",
            "step: 120, loss: 0.018426386639475822\n",
            "step: 130, loss: 0.23243509232997894\n",
            "step: 140, loss: 0.12982366979122162\n",
            "step: 150, loss: 0.012962986715137959\n",
            "step: 160, loss: 0.028802894055843353\n",
            "step: 170, loss: 0.20357458293437958\n",
            "step: 180, loss: 0.108008474111557\n",
            "step: 190, loss: 0.03348458930850029\n",
            "step: 200, loss: 0.03314198926091194\n",
            "step: 210, loss: 0.1333669126033783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6782945736434107, f1=0.6305220883534137, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008201025426387787\n",
            "step: 10, loss: 0.012597551569342613\n",
            "step: 20, loss: 0.01755892112851143\n",
            "step: 30, loss: 0.028729122132062912\n",
            "step: 40, loss: 0.06333138793706894\n",
            "step: 50, loss: 0.09282928705215454\n",
            "step: 60, loss: 0.030311260372400284\n",
            "step: 70, loss: 0.030941685661673546\n",
            "step: 80, loss: 0.04151032865047455\n",
            "step: 90, loss: 0.11013346165418625\n",
            "step: 100, loss: 0.005585697013884783\n",
            "step: 110, loss: 0.22471588850021362\n",
            "step: 120, loss: 0.07909556478261948\n",
            "step: 130, loss: 0.09482309222221375\n",
            "step: 140, loss: 0.03022785671055317\n",
            "step: 150, loss: 0.005826036911457777\n",
            "step: 160, loss: 0.05504532903432846\n",
            "step: 170, loss: 0.02035543881356716\n",
            "step: 180, loss: 0.022567622363567352\n",
            "step: 190, loss: 0.06585598737001419\n",
            "step: 200, loss: 0.12270057201385498\n",
            "step: 210, loss: 0.020176513120532036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6777777777777778, f1=0.6298342541436465, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03813336044549942\n",
            "step: 10, loss: 0.051950909197330475\n",
            "step: 20, loss: 0.030180664733052254\n",
            "step: 30, loss: 0.03145000338554382\n",
            "step: 40, loss: 0.01441612932831049\n",
            "step: 50, loss: 0.09283449500799179\n",
            "step: 60, loss: 0.22982068359851837\n",
            "step: 70, loss: 0.04564806446433067\n",
            "step: 80, loss: 0.0275464728474617\n",
            "step: 90, loss: 0.028527090325951576\n",
            "step: 100, loss: 0.029840078204870224\n",
            "step: 110, loss: 0.07626857608556747\n",
            "step: 120, loss: 0.11674680560827255\n",
            "step: 130, loss: 0.0034818865824490786\n",
            "step: 140, loss: 0.09638125449419022\n",
            "step: 150, loss: 0.013650788925588131\n",
            "step: 160, loss: 0.02280491404235363\n",
            "step: 170, loss: 0.005541782360523939\n",
            "step: 180, loss: 0.10646980255842209\n",
            "step: 190, loss: 0.024318719282746315\n",
            "step: 200, loss: 0.006971339229494333\n",
            "step: 210, loss: 0.12148264050483704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6749116607773852, f1=0.6443661971830986, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01086603756994009\n",
            "step: 10, loss: 0.044388722628355026\n",
            "step: 20, loss: 0.0015463503077626228\n",
            "step: 30, loss: 0.017910419031977654\n",
            "step: 40, loss: 0.0657251626253128\n",
            "step: 50, loss: 0.20473405718803406\n",
            "step: 60, loss: 0.09129220992326736\n",
            "step: 70, loss: 0.19022522866725922\n",
            "step: 80, loss: 0.036637865006923676\n",
            "step: 90, loss: 0.007282128091901541\n",
            "step: 100, loss: 0.021125206723809242\n",
            "step: 110, loss: 0.008844166062772274\n",
            "step: 120, loss: 0.007906586863100529\n",
            "step: 130, loss: 0.011861161328852177\n",
            "step: 140, loss: 0.03238208219408989\n",
            "step: 150, loss: 0.07858510315418243\n",
            "step: 160, loss: 0.001864359830506146\n",
            "step: 170, loss: 0.08528245985507965\n",
            "step: 180, loss: 0.026723338291049004\n",
            "step: 190, loss: 0.032848674803972244\n",
            "step: 200, loss: 0.11070495843887329\n",
            "step: 210, loss: 0.020563047379255295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6704761904761904, f1=0.6538461538461539, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00634409487247467\n",
            "step: 10, loss: 0.05429684743285179\n",
            "step: 20, loss: 0.0009528380469419062\n",
            "step: 30, loss: 0.0269283726811409\n",
            "step: 40, loss: 0.004339852835983038\n",
            "step: 50, loss: 0.062080591917037964\n",
            "step: 60, loss: 0.02531030960381031\n",
            "step: 70, loss: 0.027336139231920242\n",
            "step: 80, loss: 0.0117233507335186\n",
            "step: 90, loss: 0.047442540526390076\n",
            "step: 100, loss: 0.046145785599946976\n",
            "step: 110, loss: 0.024516146630048752\n",
            "step: 120, loss: 0.037365105003118515\n",
            "step: 130, loss: 0.0371159128844738\n",
            "step: 140, loss: 0.01666734553873539\n",
            "step: 150, loss: 0.1322038322687149\n",
            "step: 160, loss: 0.02982565574347973\n",
            "step: 170, loss: 0.030719708651304245\n",
            "step: 180, loss: 0.012354628182947636\n",
            "step: 190, loss: 0.060155972838401794\n",
            "step: 200, loss: 0.008012364618480206\n",
            "step: 210, loss: 0.046780772507190704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6679920477137177, f1=0.626984126984127, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08442655205726624\n",
            "step: 10, loss: 0.01795981638133526\n",
            "step: 20, loss: 0.025394072756171227\n",
            "step: 30, loss: 0.0015009924536570907\n",
            "step: 40, loss: 0.034022342413663864\n",
            "step: 50, loss: 0.03386262431740761\n",
            "step: 60, loss: 0.1297164261341095\n",
            "step: 70, loss: 0.007500261999666691\n",
            "step: 80, loss: 0.18360671401023865\n",
            "step: 90, loss: 0.05574227496981621\n",
            "step: 100, loss: 0.21540257334709167\n",
            "step: 110, loss: 0.034185633063316345\n",
            "step: 120, loss: 0.028818070888519287\n",
            "step: 130, loss: 0.04166979715228081\n",
            "step: 140, loss: 0.018840594217181206\n",
            "step: 150, loss: 0.0023004808463156223\n",
            "step: 160, loss: 0.04319607838988304\n",
            "step: 170, loss: 0.12787030637264252\n",
            "step: 180, loss: 0.00843667984008789\n",
            "step: 190, loss: 0.011640525422990322\n",
            "step: 200, loss: 0.0015326141146942973\n",
            "step: 210, loss: 0.004533256869763136\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6796875, f1=0.6455445544554456, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04774558171629906\n",
            "step: 10, loss: 0.0011328987311571836\n",
            "step: 20, loss: 0.03329800069332123\n",
            "step: 30, loss: 0.02573475055396557\n",
            "step: 40, loss: 0.029503632336854935\n",
            "step: 50, loss: 0.0102681964635849\n",
            "step: 60, loss: 0.024892613291740417\n",
            "step: 70, loss: 0.017998769879341125\n",
            "step: 80, loss: 0.019424743950366974\n",
            "step: 90, loss: 0.04704062268137932\n",
            "step: 100, loss: 0.10649888217449188\n",
            "step: 110, loss: 0.010094301775097847\n",
            "step: 120, loss: 0.004402150399982929\n",
            "step: 130, loss: 0.014233139343559742\n",
            "step: 140, loss: 0.0034054103307425976\n",
            "step: 150, loss: 0.030449680984020233\n",
            "step: 160, loss: 0.0013055920135229826\n",
            "step: 170, loss: 0.012780854478478432\n",
            "step: 180, loss: 0.009127200581133366\n",
            "step: 190, loss: 0.014651505276560783\n",
            "step: 200, loss: 0.002840791130438447\n",
            "step: 210, loss: 0.015225684270262718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6745562130177514, f1=0.6217821782178218, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06664949655532837\n",
            "step: 10, loss: 0.0005417956272140145\n",
            "step: 20, loss: 0.0815153419971466\n",
            "step: 30, loss: 0.22164411842823029\n",
            "step: 40, loss: 0.03726443648338318\n",
            "step: 50, loss: 0.009639261290431023\n",
            "step: 60, loss: 0.0021261433139443398\n",
            "step: 70, loss: 0.031758058816194534\n",
            "step: 80, loss: 0.004853283520787954\n",
            "step: 90, loss: 0.0008378481725230813\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.001558643183670938\n",
            "step: 110, loss: 0.003870993619784713\n",
            "step: 120, loss: 0.06911928951740265\n",
            "step: 130, loss: 0.004262561909854412\n",
            "step: 140, loss: 0.0018739630468189716\n",
            "step: 150, loss: 0.005796435289084911\n",
            "step: 160, loss: 0.006482153665274382\n",
            "step: 170, loss: 0.002456907881423831\n",
            "step: 180, loss: 0.08066383004188538\n",
            "step: 190, loss: 0.028921842575073242\n",
            "step: 200, loss: 0.0020825748797506094\n",
            "step: 210, loss: 0.0077072130516171455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6703910614525139, f1=0.632768361581921, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003595168935135007\n",
            "step: 10, loss: 0.03299511969089508\n",
            "step: 20, loss: 0.045291561633348465\n",
            "step: 30, loss: 0.003825356252491474\n",
            "step: 40, loss: 0.003134847618639469\n",
            "step: 50, loss: 0.04149796813726425\n",
            "step: 60, loss: 0.0012334410566836596\n",
            "step: 70, loss: 0.003942342475056648\n",
            "step: 80, loss: 0.0482274629175663\n",
            "step: 90, loss: 0.09953691810369492\n",
            "step: 100, loss: 0.008038888685405254\n",
            "step: 110, loss: 0.00548273790627718\n",
            "step: 120, loss: 0.03656699135899544\n",
            "step: 130, loss: 0.003206951543688774\n",
            "step: 140, loss: 0.04963789880275726\n",
            "step: 150, loss: 0.006694663781672716\n",
            "step: 160, loss: 0.011363727040588856\n",
            "step: 170, loss: 0.013950797729194164\n",
            "step: 180, loss: 0.008942659944295883\n",
            "step: 190, loss: 0.0012861958239227533\n",
            "step: 200, loss: 0.001840227167122066\n",
            "step: 210, loss: 0.011081070639193058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6741573033707865, f1=0.6259541984732825, best_f1=0.6602687140115163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00051690055988729\n",
            "step: 10, loss: 0.0005308325053192675\n",
            "step: 20, loss: 0.003378119319677353\n",
            "step: 30, loss: 0.030674301087856293\n",
            "step: 40, loss: 0.0063453419134020805\n",
            "step: 50, loss: 0.002676219679415226\n",
            "step: 60, loss: 0.04937714338302612\n",
            "step: 70, loss: 0.001819196273572743\n",
            "step: 80, loss: 0.0010219716932624578\n",
            "step: 90, loss: 0.013871180824935436\n",
            "step: 100, loss: 0.006441762670874596\n",
            "step: 110, loss: 0.001110854558646679\n",
            "step: 120, loss: 0.006191359832882881\n",
            "step: 130, loss: 0.009376094676554203\n",
            "step: 140, loss: 0.0009132529376074672\n",
            "step: 150, loss: 0.002283437643200159\n",
            "step: 160, loss: 0.004335750360041857\n",
            "step: 170, loss: 0.0006887535564601421\n",
            "step: 180, loss: 0.0002992021618410945\n",
            "step: 190, loss: 0.010078425519168377\n",
            "step: 200, loss: 0.004206703510135412\n",
            "step: 210, loss: 0.020142706111073494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6770428015564203, f1=0.6237816764132554, best_f1=0.6602687140115163\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 281.03it/s]\n",
            "load_f1 = 0.7017543859649122\n",
            "real_f1 = 0.69921875\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e43d03-1619-4ded-f048-7eccb34ec304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5713953971862793\n",
            "step: 10, loss: 0.36597007513046265\n",
            "step: 20, loss: 0.29090356826782227\n",
            "step: 30, loss: 0.4396257996559143\n",
            "step: 40, loss: 0.41501766443252563\n",
            "step: 50, loss: 0.28450602293014526\n",
            "step: 60, loss: 0.23495253920555115\n",
            "step: 70, loss: 0.2580818831920624\n",
            "step: 80, loss: 0.3696858584880829\n",
            "step: 90, loss: 0.2715051770210266\n",
            "step: 100, loss: 0.2673864960670471\n",
            "step: 110, loss: 0.28615814447402954\n",
            "step: 120, loss: 0.04424536973237991\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.1435668021440506\n",
            "step: 140, loss: 0.04594852030277252\n",
            "step: 150, loss: 0.14782533049583435\n",
            "step: 160, loss: 0.034631531685590744\n",
            "step: 170, loss: 0.2529412508010864\n",
            "step: 180, loss: 0.04719490557909012\n",
            "step: 190, loss: 0.18079321086406708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6386554621848739, f1=0.6551724137931034, best_f1=0.6551724137931034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15462927520275116\n",
            "step: 10, loss: 0.10671914368867874\n",
            "step: 20, loss: 0.155114084482193\n",
            "step: 30, loss: 0.08402692526578903\n",
            "step: 40, loss: 0.10692373663187027\n",
            "step: 50, loss: 0.09987255185842514\n",
            "step: 60, loss: 0.3403746783733368\n",
            "step: 70, loss: 0.2158690243959427\n",
            "step: 80, loss: 0.22331339120864868\n",
            "step: 90, loss: 0.15635773539543152\n",
            "step: 100, loss: 0.011298095807433128\n",
            "step: 110, loss: 0.07993175834417343\n",
            "step: 120, loss: 0.22605013847351074\n",
            "step: 130, loss: 0.038847699761390686\n",
            "step: 140, loss: 0.05829255282878876\n",
            "step: 150, loss: 0.11398463696241379\n",
            "step: 160, loss: 0.018169106915593147\n",
            "step: 170, loss: 0.16388072073459625\n",
            "step: 180, loss: 0.11644705384969711\n",
            "step: 190, loss: 0.08721638470888138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7863247863247862, f1=0.7877094972067039, best_f1=0.7877094972067039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027608459815382957\n",
            "step: 10, loss: 0.2068047672510147\n",
            "step: 20, loss: 0.18215614557266235\n",
            "step: 30, loss: 0.1409163475036621\n",
            "step: 40, loss: 0.12309923768043518\n",
            "step: 50, loss: 0.19497454166412354\n",
            "step: 60, loss: 0.032188307493925095\n",
            "step: 70, loss: 0.10452820360660553\n",
            "step: 80, loss: 0.17867770791053772\n",
            "step: 90, loss: 0.09748116135597229\n",
            "step: 100, loss: 0.07328921556472778\n",
            "step: 110, loss: 0.00998812634497881\n",
            "step: 120, loss: 0.021200435236096382\n",
            "step: 130, loss: 0.028454937040805817\n",
            "step: 140, loss: 0.031100083142518997\n",
            "step: 150, loss: 0.10852992534637451\n",
            "step: 160, loss: 0.07934604585170746\n",
            "step: 170, loss: 0.13658437132835388\n",
            "step: 180, loss: 0.04974547028541565\n",
            "step: 190, loss: 0.11717117577791214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7922437673130193, f1=0.7896253602305476, best_f1=0.7896253602305476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04117247089743614\n",
            "step: 10, loss: 0.1356584131717682\n",
            "step: 20, loss: 0.07406715303659439\n",
            "step: 30, loss: 0.08482196927070618\n",
            "step: 40, loss: 0.12145064771175385\n",
            "step: 50, loss: 0.017745008692145348\n",
            "step: 60, loss: 0.1667526662349701\n",
            "step: 70, loss: 0.11556793749332428\n",
            "step: 80, loss: 0.10841846466064453\n",
            "step: 90, loss: 0.017374811694025993\n",
            "step: 100, loss: 0.035746391862630844\n",
            "step: 110, loss: 0.0034292396157979965\n",
            "step: 120, loss: 0.0650574341416359\n",
            "step: 130, loss: 0.27608177065849304\n",
            "step: 140, loss: 0.019772082567214966\n",
            "step: 150, loss: 0.1074310839176178\n",
            "step: 160, loss: 0.017524605616927147\n",
            "step: 170, loss: 0.1388629972934723\n",
            "step: 180, loss: 0.12089361250400543\n",
            "step: 190, loss: 0.162501260638237\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7821522309711286, f1=0.7609254498714654, best_f1=0.7896253602305476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08281728625297546\n",
            "step: 10, loss: 0.03849559649825096\n",
            "step: 20, loss: 0.041657961905002594\n",
            "step: 30, loss: 0.01568567007780075\n",
            "step: 40, loss: 0.06963715702295303\n",
            "step: 50, loss: 0.04461590573191643\n",
            "step: 60, loss: 0.1246107742190361\n",
            "step: 70, loss: 0.030137578025460243\n",
            "step: 80, loss: 0.009864908643066883\n",
            "step: 90, loss: 0.0258504506200552\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.00662204110994935\n",
            "step: 110, loss: 0.00583443371579051\n",
            "step: 120, loss: 0.07067828625440598\n",
            "step: 130, loss: 0.0551731176674366\n",
            "step: 140, loss: 0.02511274442076683\n",
            "step: 150, loss: 0.01940545253455639\n",
            "step: 160, loss: 0.31233111023902893\n",
            "step: 170, loss: 0.1139197051525116\n",
            "step: 180, loss: 0.0370701365172863\n",
            "step: 190, loss: 0.07917950302362442\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7903225806451613, f1=0.7830687830687831, best_f1=0.7896253602305476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10054081678390503\n",
            "step: 10, loss: 0.044361673295497894\n",
            "step: 20, loss: 0.010160389356315136\n",
            "step: 30, loss: 0.00109615339897573\n",
            "step: 40, loss: 0.03498867154121399\n",
            "step: 50, loss: 0.0972534567117691\n",
            "step: 60, loss: 0.00943212304264307\n",
            "step: 70, loss: 0.003285443875938654\n",
            "step: 80, loss: 0.004601198248565197\n",
            "step: 90, loss: 0.032069165259599686\n",
            "step: 100, loss: 0.0022907163947820663\n",
            "step: 110, loss: 0.0382375605404377\n",
            "step: 120, loss: 0.0227949358522892\n",
            "step: 130, loss: 0.004230060614645481\n",
            "step: 140, loss: 0.027694690972566605\n",
            "step: 150, loss: 0.0021362740080803633\n",
            "step: 160, loss: 0.07327012717723846\n",
            "step: 170, loss: 0.04482700303196907\n",
            "step: 180, loss: 0.07534673064947128\n",
            "step: 190, loss: 0.021200649440288544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.772117962466488, f1=0.7659574468085106, best_f1=0.7896253602305476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003288496285676956\n",
            "step: 10, loss: 0.011541948653757572\n",
            "step: 20, loss: 0.031035451218485832\n",
            "step: 30, loss: 0.10196801275014877\n",
            "step: 40, loss: 0.006145255174487829\n",
            "step: 50, loss: 0.05757276341319084\n",
            "step: 60, loss: 0.06846005469560623\n",
            "step: 70, loss: 0.01766134984791279\n",
            "step: 80, loss: 0.0024279432836920023\n",
            "step: 90, loss: 0.017067966982722282\n",
            "step: 100, loss: 0.002226448617875576\n",
            "step: 110, loss: 0.008048581890761852\n",
            "step: 120, loss: 0.0024557386059314013\n",
            "step: 130, loss: 0.15148381888866425\n",
            "step: 140, loss: 0.004480068106204271\n",
            "step: 150, loss: 0.0010218415409326553\n",
            "step: 160, loss: 0.16791176795959473\n",
            "step: 170, loss: 0.0909385085105896\n",
            "step: 180, loss: 0.00238889642059803\n",
            "step: 190, loss: 0.001685967086814344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8062827225130891, f1=0.7946666666666666, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0063565275631845\n",
            "step: 10, loss: 0.02035815455019474\n",
            "step: 20, loss: 0.005952455569058657\n",
            "step: 30, loss: 0.014762070029973984\n",
            "step: 40, loss: 0.0021533512044698\n",
            "step: 50, loss: 0.00901661068201065\n",
            "step: 60, loss: 0.0017091460758820176\n",
            "step: 70, loss: 0.004638256039470434\n",
            "step: 80, loss: 0.0008145183674059808\n",
            "step: 90, loss: 0.0031912634149193764\n",
            "step: 100, loss: 0.011429854668676853\n",
            "step: 110, loss: 0.001698903040960431\n",
            "step: 120, loss: 0.005310780368745327\n",
            "step: 130, loss: 0.01751876249909401\n",
            "step: 140, loss: 0.024650221690535545\n",
            "step: 150, loss: 0.010251184925436974\n",
            "step: 160, loss: 0.006353522185236216\n",
            "step: 170, loss: 0.02789929322898388\n",
            "step: 180, loss: 0.04883226752281189\n",
            "step: 190, loss: 0.0022266595624387264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7774798927613942, f1=0.765498652291105, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0060684275813400745\n",
            "step: 10, loss: 0.03338444232940674\n",
            "step: 20, loss: 0.0031447343062609434\n",
            "step: 30, loss: 0.013660400174558163\n",
            "step: 40, loss: 0.03500019386410713\n",
            "step: 50, loss: 0.0045131198130548\n",
            "step: 60, loss: 0.002446766709908843\n",
            "step: 70, loss: 0.0026075770147144794\n",
            "step: 80, loss: 0.001843073288910091\n",
            "step: 90, loss: 0.01734914816915989\n",
            "step: 100, loss: 0.043989844620227814\n",
            "step: 110, loss: 0.001254464965313673\n",
            "step: 120, loss: 0.011176369152963161\n",
            "step: 130, loss: 0.0023328782990574837\n",
            "step: 140, loss: 0.005780401639640331\n",
            "step: 150, loss: 0.0046197520568966866\n",
            "step: 160, loss: 0.001015214715152979\n",
            "step: 170, loss: 0.0014972813660278916\n",
            "step: 180, loss: 0.07434837520122528\n",
            "step: 190, loss: 0.002931856317445636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7897435897435896, f1=0.7819548872180452, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019807424396276474\n",
            "step: 10, loss: 0.05824992060661316\n",
            "step: 20, loss: 0.031865015625953674\n",
            "step: 30, loss: 0.0018833972280845046\n",
            "step: 40, loss: 0.015832556411623955\n",
            "step: 50, loss: 0.0005754731246270239\n",
            "step: 60, loss: 0.000816097657661885\n",
            "step: 70, loss: 0.0008115021628327668\n",
            "step: 80, loss: 0.03136935457587242\n",
            "step: 90, loss: 0.018660662695765495\n",
            "step: 100, loss: 0.007297839038074017\n",
            "step: 110, loss: 0.01653478294610977\n",
            "step: 120, loss: 0.11227086186408997\n",
            "step: 130, loss: 0.006080503109842539\n",
            "step: 140, loss: 0.00301970262080431\n",
            "step: 150, loss: 0.041819978505373\n",
            "step: 160, loss: 0.01620326191186905\n",
            "step: 170, loss: 0.0022895177826285362\n",
            "step: 180, loss: 0.2327873408794403\n",
            "step: 190, loss: 0.0014106258749961853\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.783289817232376, f1=0.783289817232376, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014418390346691012\n",
            "step: 10, loss: 0.011065970174968243\n",
            "step: 20, loss: 0.02498861774802208\n",
            "step: 30, loss: 0.05318028852343559\n",
            "step: 40, loss: 0.001423319918103516\n",
            "step: 50, loss: 0.020419050008058548\n",
            "step: 60, loss: 0.00048713828437030315\n",
            "step: 70, loss: 0.014509234577417374\n",
            "step: 80, loss: 0.0010689669288694859\n",
            "step: 90, loss: 0.0037615953478962183\n",
            "step: 100, loss: 0.0009547404479235411\n",
            "step: 110, loss: 0.004425038117915392\n",
            "step: 120, loss: 0.0006417073309421539\n",
            "step: 130, loss: 0.0016537487972527742\n",
            "step: 140, loss: 0.0046671610325574875\n",
            "step: 150, loss: 0.0009090639650821686\n",
            "step: 160, loss: 0.0007742232410237193\n",
            "step: 170, loss: 0.0006685625994578004\n",
            "step: 180, loss: 0.0016024879878386855\n",
            "step: 190, loss: 0.0046195476315915585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.773067331670823, f1=0.7450980392156864, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001564758480526507\n",
            "step: 10, loss: 0.0052679977379739285\n",
            "step: 20, loss: 0.0010680381674319506\n",
            "step: 30, loss: 0.0005065581644885242\n",
            "step: 40, loss: 0.0007652103086002171\n",
            "step: 50, loss: 0.004546689800918102\n",
            "step: 60, loss: 0.0015438739210367203\n",
            "step: 70, loss: 0.0011170974466949701\n",
            "step: 80, loss: 0.0017852156888693571\n",
            "step: 90, loss: 0.0008792081498540938\n",
            "step: 100, loss: 0.0009127809898927808\n",
            "step: 110, loss: 0.0007510036812163889\n",
            "step: 120, loss: 0.0004347946378402412\n",
            "step: 130, loss: 0.0005159590509720147\n",
            "step: 140, loss: 0.0031053603161126375\n",
            "step: 150, loss: 0.0009550201939418912\n",
            "step: 160, loss: 0.0008113458752632141\n",
            "step: 170, loss: 0.001563416444696486\n",
            "step: 180, loss: 0.0005220940802246332\n",
            "step: 190, loss: 0.0008887051953934133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7989690721649483, f1=0.7788944723618091, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13375568389892578\n",
            "step: 10, loss: 0.0803937241435051\n",
            "step: 20, loss: 0.00524055166170001\n",
            "step: 30, loss: 0.0006165573140606284\n",
            "step: 40, loss: 0.0026473840698599815\n",
            "step: 50, loss: 0.0023588803596794605\n",
            "step: 60, loss: 0.0009515451965853572\n",
            "step: 70, loss: 0.001736028236337006\n",
            "step: 80, loss: 0.010810238309204578\n",
            "step: 90, loss: 0.0011118088150396943\n",
            "step: 100, loss: 0.0019483695505186915\n",
            "step: 110, loss: 0.01506292074918747\n",
            "step: 120, loss: 0.0020903206896036863\n",
            "step: 130, loss: 0.00037787959445267916\n",
            "step: 140, loss: 0.0003591335844248533\n",
            "step: 150, loss: 0.0005096212844364345\n",
            "step: 160, loss: 0.0007002914207987487\n",
            "step: 170, loss: 0.0007201214320957661\n",
            "step: 180, loss: 0.0021141599863767624\n",
            "step: 190, loss: 0.04032883048057556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7928388746803069, f1=0.775, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001620415598154068\n",
            "step: 10, loss: 0.00040764687582850456\n",
            "step: 20, loss: 0.0003274352930020541\n",
            "step: 30, loss: 0.0005048796301707625\n",
            "step: 40, loss: 0.0020006406120955944\n",
            "step: 50, loss: 0.0010631382465362549\n",
            "step: 60, loss: 0.004840065259486437\n",
            "step: 70, loss: 0.00037488600355573\n",
            "step: 80, loss: 0.0003542563645169139\n",
            "step: 90, loss: 0.06722576916217804\n",
            "step: 100, loss: 0.0021851882338523865\n",
            "step: 110, loss: 0.0034830558579415083\n",
            "step: 120, loss: 0.0031060767360031605\n",
            "step: 130, loss: 0.0017864821711555123\n",
            "step: 140, loss: 0.08998211473226547\n",
            "step: 150, loss: 0.0008153589442372322\n",
            "step: 160, loss: 0.0031459617894142866\n",
            "step: 170, loss: 0.0011529983021318913\n",
            "step: 180, loss: 0.0012179192854091525\n",
            "step: 190, loss: 0.0008590673096477985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7969151670951158, f1=0.789873417721519, best_f1=0.7946666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031081942841410637\n",
            "step: 10, loss: 0.000503536022733897\n",
            "step: 20, loss: 0.0007295070681720972\n",
            "step: 30, loss: 0.0008648857474327087\n",
            "step: 40, loss: 0.001694891368970275\n",
            "step: 50, loss: 0.010414054617285728\n",
            "step: 60, loss: 0.006975201424211264\n",
            "step: 70, loss: 0.0017490516183897853\n",
            "step: 80, loss: 0.000776509812567383\n",
            "step: 90, loss: 0.000833774683997035\n",
            "step: 100, loss: 0.0022931070998311043\n",
            "step: 110, loss: 0.0018416928360238671\n",
            "step: 120, loss: 0.001659812987782061\n",
            "step: 130, loss: 0.0008834523614495993\n",
            "step: 140, loss: 0.12071750313043594\n",
            "step: 150, loss: 0.0011006994172930717\n",
            "step: 160, loss: 0.0005740750930272043\n",
            "step: 170, loss: 0.012705820612609386\n",
            "step: 180, loss: 0.0013890624977648258\n",
            "step: 190, loss: 0.004342812113463879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7969151670951158, f1=0.7858942065491183, best_f1=0.7946666666666666\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 164.32it/s]\n",
            "load_f1 = 0.7374301675977653\n",
            "real_f1 = 0.7061994609164419\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 183.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f53d6e-9543-4f09-96fb-081e5dac95d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6508017778396606\n",
            "step: 10, loss: 0.37014082074165344\n",
            "step: 20, loss: 0.30151501297950745\n",
            "step: 30, loss: 0.3774060606956482\n",
            "step: 40, loss: 0.26833775639533997\n",
            "step: 50, loss: 0.2627197206020355\n",
            "step: 60, loss: 0.2626081109046936\n",
            "step: 70, loss: 0.37545379996299744\n",
            "step: 80, loss: 0.4021260142326355\n",
            "step: 90, loss: 0.23043376207351685\n",
            "step: 100, loss: 0.22389978170394897\n",
            "step: 110, loss: 0.26520395278930664\n",
            "step: 120, loss: 0.18283258378505707\n",
            "step: 130, loss: 0.0155501002445817\n",
            "step: 140, loss: 0.26080521941185\n",
            "step: 150, loss: 0.26983213424682617\n",
            "step: 160, loss: 0.131405770778656\n",
            "step: 170, loss: 0.2603968679904938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7428571428571429, f1=0.6924829157175398, best_f1=0.6924829157175398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09318426996469498\n",
            "step: 10, loss: 0.27214834094047546\n",
            "step: 20, loss: 0.10133615881204605\n",
            "step: 30, loss: 0.1970413476228714\n",
            "step: 40, loss: 0.03626849874854088\n",
            "step: 50, loss: 0.22509920597076416\n",
            "step: 60, loss: 0.18079440295696259\n",
            "step: 70, loss: 0.030362965539097786\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.1652430146932602\n",
            "step: 90, loss: 0.09869427233934402\n",
            "step: 100, loss: 0.12594100832939148\n",
            "step: 110, loss: 0.08240988105535507\n",
            "step: 120, loss: 0.28438976407051086\n",
            "step: 130, loss: 0.10478556156158447\n",
            "step: 140, loss: 0.29702168703079224\n",
            "step: 150, loss: 0.12379471957683563\n",
            "step: 160, loss: 0.16390804946422577\n",
            "step: 170, loss: 0.11179737001657486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7713004484304932, f1=0.7428571428571429, best_f1=0.7428571428571429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09332569688558578\n",
            "step: 10, loss: 0.06343653798103333\n",
            "step: 20, loss: 0.02359546534717083\n",
            "step: 30, loss: 0.3062361180782318\n",
            "step: 40, loss: 0.11452141404151917\n",
            "step: 50, loss: 0.03373045101761818\n",
            "step: 60, loss: 0.11460627615451813\n",
            "step: 70, loss: 0.05287321284413338\n",
            "step: 80, loss: 0.036168407648801804\n",
            "step: 90, loss: 0.08440648019313812\n",
            "step: 100, loss: 0.009607430547475815\n",
            "step: 110, loss: 0.0750541090965271\n",
            "step: 120, loss: 0.05009841173887253\n",
            "step: 130, loss: 0.07029586285352707\n",
            "step: 140, loss: 0.035264864563941956\n",
            "step: 150, loss: 0.22849905490875244\n",
            "step: 160, loss: 0.017936866730451584\n",
            "step: 170, loss: 0.20577335357666016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7659574468085106, f1=0.7808219178082191, best_f1=0.7428571428571429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012919265776872635\n",
            "step: 10, loss: 0.02094007097184658\n",
            "step: 20, loss: 0.08955798298120499\n",
            "step: 30, loss: 0.05750463530421257\n",
            "step: 40, loss: 0.006980641279369593\n",
            "step: 50, loss: 0.05468466505408287\n",
            "step: 60, loss: 0.09683454036712646\n",
            "step: 70, loss: 0.016501083970069885\n",
            "step: 80, loss: 0.08534888178110123\n",
            "step: 90, loss: 0.0661911740899086\n",
            "step: 100, loss: 0.24565835297107697\n",
            "step: 110, loss: 0.0313933826982975\n",
            "step: 120, loss: 0.03181761875748634\n",
            "step: 130, loss: 0.06261614710092545\n",
            "step: 140, loss: 0.031215060502290726\n",
            "step: 150, loss: 0.14348874986171722\n",
            "step: 160, loss: 0.05414145439863205\n",
            "step: 170, loss: 0.16945752501487732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8128078817733989, f1=0.7971698113207547, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.059104762971401215\n",
            "step: 10, loss: 0.013607158325612545\n",
            "step: 20, loss: 0.05217916890978813\n",
            "step: 30, loss: 0.0034521969500929117\n",
            "step: 40, loss: 0.015408781357109547\n",
            "step: 50, loss: 0.01403993833810091\n",
            "step: 60, loss: 0.0927409753203392\n",
            "step: 70, loss: 0.10347811877727509\n",
            "step: 80, loss: 0.042719561606645584\n",
            "step: 90, loss: 0.0627584233880043\n",
            "step: 100, loss: 0.016848847270011902\n",
            "step: 110, loss: 0.032945193350315094\n",
            "step: 120, loss: 0.0036046770401299\n",
            "step: 130, loss: 0.026608996093273163\n",
            "step: 140, loss: 0.024196309968829155\n",
            "step: 150, loss: 0.09739633649587631\n",
            "step: 160, loss: 0.04293755814433098\n",
            "step: 170, loss: 0.004460733849555254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7990867579908676, f1=0.7955555555555555, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009692475199699402\n",
            "step: 10, loss: 0.025939766317605972\n",
            "step: 20, loss: 0.001979382010176778\n",
            "step: 30, loss: 0.019940027967095375\n",
            "step: 40, loss: 0.005318109877407551\n",
            "step: 50, loss: 0.2319859266281128\n",
            "step: 60, loss: 0.07336561381816864\n",
            "step: 70, loss: 0.12984582781791687\n",
            "step: 80, loss: 0.047625817358493805\n",
            "step: 90, loss: 0.05133051797747612\n",
            "step: 100, loss: 0.0032184591982513666\n",
            "step: 110, loss: 0.008888894692063332\n",
            "step: 120, loss: 0.014039186760783195\n",
            "step: 130, loss: 0.005366511642932892\n",
            "step: 140, loss: 0.01639285311102867\n",
            "step: 150, loss: 0.00965900532901287\n",
            "step: 160, loss: 0.07310846447944641\n",
            "step: 170, loss: 0.01470182929188013\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7967914438502675, f1=0.7969924812030076, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0025829484220594168\n",
            "step: 10, loss: 0.0023685910273343325\n",
            "step: 20, loss: 0.011820957064628601\n",
            "step: 30, loss: 0.007684449665248394\n",
            "step: 40, loss: 0.012572857551276684\n",
            "step: 50, loss: 0.003681881818920374\n",
            "step: 60, loss: 0.0022688889876008034\n",
            "step: 70, loss: 0.0011295879958197474\n",
            "step: 80, loss: 0.0428287535905838\n",
            "step: 90, loss: 0.0011422333773225546\n",
            "step: 100, loss: 0.0005271583213470876\n",
            "step: 110, loss: 0.01395932026207447\n",
            "step: 120, loss: 0.01908322423696518\n",
            "step: 130, loss: 0.10477763414382935\n",
            "step: 140, loss: 0.02140052616596222\n",
            "step: 150, loss: 0.0005705048679374158\n",
            "step: 160, loss: 0.03465760871767998\n",
            "step: 170, loss: 0.023062974214553833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7716535433070866, f1=0.7960199004975125, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007844371721148491\n",
            "step: 10, loss: 0.0010299376444891095\n",
            "step: 20, loss: 0.0023941195104271173\n",
            "step: 30, loss: 0.007397486362606287\n",
            "step: 40, loss: 0.00015137023001443595\n",
            "step: 50, loss: 0.0022010228130966425\n",
            "step: 60, loss: 0.002297842875123024\n",
            "step: 70, loss: 0.005618109833449125\n",
            "step: 80, loss: 0.007044227793812752\n",
            "step: 90, loss: 0.004286285489797592\n",
            "step: 100, loss: 0.05562727898359299\n",
            "step: 110, loss: 0.06023945286870003\n",
            "step: 120, loss: 0.00831915345042944\n",
            "step: 130, loss: 0.0010879267938435078\n",
            "step: 140, loss: 0.003700357163324952\n",
            "step: 150, loss: 0.006168432999402285\n",
            "step: 160, loss: 0.07342823594808578\n",
            "step: 170, loss: 0.002280409215018153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.781725888324873, f1=0.7897196261682243, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025070548057556152\n",
            "step: 10, loss: 0.0022365818731486797\n",
            "step: 20, loss: 0.00022406893549486995\n",
            "step: 30, loss: 0.033337660133838654\n",
            "step: 40, loss: 0.00482949847355485\n",
            "step: 50, loss: 0.00026294219424016774\n",
            "step: 60, loss: 0.0036465059965848923\n",
            "step: 70, loss: 0.0006190433050505817\n",
            "step: 80, loss: 0.020706601440906525\n",
            "step: 90, loss: 0.044089339673519135\n",
            "step: 100, loss: 0.003031775588169694\n",
            "step: 110, loss: 0.005943763069808483\n",
            "step: 120, loss: 0.0036545065231621265\n",
            "step: 130, loss: 0.07264706492424011\n",
            "step: 140, loss: 0.009714096784591675\n",
            "step: 150, loss: 0.00015023231389932334\n",
            "step: 160, loss: 0.04975646361708641\n",
            "step: 170, loss: 0.003158397739753127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.787128712871287, f1=0.8179669030732861, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.047648847103118896\n",
            "step: 10, loss: 0.0045571839436888695\n",
            "step: 20, loss: 0.012623528949916363\n",
            "step: 30, loss: 0.00032272134558297694\n",
            "step: 40, loss: 0.017326388508081436\n",
            "step: 50, loss: 0.031150899827480316\n",
            "step: 60, loss: 0.00011645457561826333\n",
            "step: 70, loss: 0.011510123498737812\n",
            "step: 80, loss: 0.00011729726975318044\n",
            "step: 90, loss: 0.0008276844164356589\n",
            "step: 100, loss: 0.00013283042062539607\n",
            "step: 110, loss: 0.08267048746347427\n",
            "step: 120, loss: 0.002229271689429879\n",
            "step: 130, loss: 0.0004798964655492455\n",
            "step: 140, loss: 0.0217727217823267\n",
            "step: 150, loss: 0.053509317338466644\n",
            "step: 160, loss: 0.0068598780781030655\n",
            "step: 170, loss: 0.0027969495858997107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7642276422764228, f1=0.8061224489795918, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03261655941605568\n",
            "step: 10, loss: 0.00010773850226541981\n",
            "step: 20, loss: 0.0003137100429739803\n",
            "step: 30, loss: 0.00013690217747353017\n",
            "step: 40, loss: 7.789250958012417e-05\n",
            "step: 50, loss: 0.026243047788739204\n",
            "step: 60, loss: 0.002407355699688196\n",
            "step: 70, loss: 0.008456428535282612\n",
            "step: 80, loss: 0.0001612323831068352\n",
            "step: 90, loss: 0.00013716451940126717\n",
            "step: 100, loss: 0.0001871747663244605\n",
            "step: 110, loss: 0.01634659618139267\n",
            "step: 120, loss: 0.00030147621873766184\n",
            "step: 130, loss: 0.0001119593289331533\n",
            "step: 140, loss: 0.0035226482432335615\n",
            "step: 150, loss: 0.0023427370470017195\n",
            "step: 160, loss: 0.0022377038840204477\n",
            "step: 170, loss: 0.00015707571583334357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7990314769975787, f1=0.7935779816513762, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012950404197908938\n",
            "step: 10, loss: 0.00045638447045348585\n",
            "step: 20, loss: 7.668208854738623e-05\n",
            "step: 30, loss: 8.364645327674225e-05\n",
            "step: 40, loss: 0.001190792303532362\n",
            "step: 50, loss: 0.00028673664201050997\n",
            "step: 60, loss: 0.000435427064076066\n",
            "step: 70, loss: 0.05068981647491455\n",
            "step: 80, loss: 0.00010477196337888017\n",
            "step: 90, loss: 0.00030334541224874556\n",
            "step: 100, loss: 0.00125010940246284\n",
            "step: 110, loss: 0.014512897469103336\n",
            "step: 120, loss: 0.008462676778435707\n",
            "step: 130, loss: 0.0010573847685009241\n",
            "step: 140, loss: 0.0015685658436268568\n",
            "step: 150, loss: 0.0007948518032208085\n",
            "step: 160, loss: 0.00011829325376311317\n",
            "step: 170, loss: 0.0001178843667730689\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7792207792207791, f1=0.7990430622009569, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011540760286152363\n",
            "step: 10, loss: 9.197105100611225e-05\n",
            "step: 20, loss: 0.0007342586177401245\n",
            "step: 30, loss: 4.5442484406521544e-05\n",
            "step: 40, loss: 7.498218474211171e-05\n",
            "step: 50, loss: 0.0003240944934077561\n",
            "step: 60, loss: 0.0019679879769682884\n",
            "step: 70, loss: 0.03485048934817314\n",
            "step: 80, loss: 0.06671269237995148\n",
            "step: 90, loss: 8.609583892393857e-05\n",
            "step: 100, loss: 0.12202487885951996\n",
            "step: 110, loss: 0.00030886256718076766\n",
            "step: 120, loss: 0.05452214181423187\n",
            "step: 130, loss: 0.00016749992209952325\n",
            "step: 140, loss: 0.0005413363687694073\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.0007723423768766224\n",
            "step: 160, loss: 0.0013912295689806342\n",
            "step: 170, loss: 0.0003089234814979136\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7823834196891193, f1=0.8076923076923078, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002157351264031604\n",
            "step: 10, loss: 8.829720172798261e-05\n",
            "step: 20, loss: 0.00012963431072421372\n",
            "step: 30, loss: 0.0001308847713517025\n",
            "step: 40, loss: 8.805794641375542e-05\n",
            "step: 50, loss: 9.61672849371098e-05\n",
            "step: 60, loss: 8.243740012403578e-05\n",
            "step: 70, loss: 0.00032079964876174927\n",
            "step: 80, loss: 0.001254648668691516\n",
            "step: 90, loss: 0.00015864182205405086\n",
            "step: 100, loss: 5.9782560128951445e-05\n",
            "step: 110, loss: 0.00011992458166787401\n",
            "step: 120, loss: 0.001849773689173162\n",
            "step: 130, loss: 0.001533731585368514\n",
            "step: 140, loss: 0.00027497828705236316\n",
            "step: 150, loss: 5.870247332495637e-05\n",
            "step: 160, loss: 4.395346695673652e-05\n",
            "step: 170, loss: 0.00018369543249718845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7890818858560793, f1=0.810304449648712, best_f1=0.7971698113207547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012659290223382413\n",
            "step: 10, loss: 0.021372444927692413\n",
            "step: 20, loss: 0.025938719511032104\n",
            "step: 30, loss: 0.00041906186379492283\n",
            "step: 40, loss: 0.0007824511267244816\n",
            "step: 50, loss: 6.63423488731496e-05\n",
            "step: 60, loss: 0.003790121292695403\n",
            "step: 70, loss: 4.52674794360064e-05\n",
            "step: 80, loss: 0.03561288118362427\n",
            "step: 90, loss: 0.014140470884740353\n",
            "step: 100, loss: 0.00033336502383463085\n",
            "step: 110, loss: 7.006667146924883e-05\n",
            "step: 120, loss: 0.00011436192289693281\n",
            "step: 130, loss: 0.0005004284903407097\n",
            "step: 140, loss: 9.403364674653858e-05\n",
            "step: 150, loss: 0.0017260332824662328\n",
            "step: 160, loss: 7.680700946366414e-05\n",
            "step: 170, loss: 0.0011093338252976537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7704485488126649, f1=0.8148148148148149, best_f1=0.7971698113207547\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 226.91it/s]\n",
            "load_f1 = 0.40476190476190477\n",
            "real_f1 = 0.3724696356275304\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 183.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca9f5b0-d1a0-4371-8c56-3306cee67b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 522kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.65MB/s]\n",
            "Downloading: 100% 440M/440M [00:12<00:00, 36.4MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6374112367630005\n",
            "step: 10, loss: 0.6111491322517395\n",
            "step: 20, loss: 0.3884900212287903\n",
            "step: 30, loss: 0.10773272812366486\n",
            "step: 40, loss: 0.20705385506153107\n",
            "step: 50, loss: 0.0583997517824173\n",
            "step: 60, loss: 0.10728656500577927\n",
            "step: 70, loss: 0.059156373143196106\n",
            "step: 80, loss: 0.05844314768910408\n",
            "step: 90, loss: 0.10975848138332367\n",
            "step: 100, loss: 0.010257083922624588\n",
            "step: 110, loss: 0.2951129078865051\n",
            "step: 120, loss: 0.006196639034897089\n",
            "step: 130, loss: 0.006480564828962088\n",
            "step: 140, loss: 0.004214535001665354\n",
            "step: 150, loss: 0.04568469524383545\n",
            "step: 160, loss: 0.009131337516009808\n",
            "step: 170, loss: 0.14555776119232178\n",
            "step: 180, loss: 0.15463386476039886\n",
            "step: 190, loss: 0.04021920636296272\n",
            "step: 200, loss: 0.07479310780763626\n",
            "step: 210, loss: 0.004508759826421738\n",
            "step: 220, loss: 0.014682442881166935\n",
            "step: 230, loss: 0.022360071539878845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9642058165548099, f1=0.9592760180995475, best_f1=0.9592760180995475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009693719446659088\n",
            "step: 10, loss: 0.004716751631349325\n",
            "step: 20, loss: 0.20243461430072784\n",
            "step: 30, loss: 0.13871745765209198\n",
            "step: 40, loss: 0.03426488861441612\n",
            "step: 50, loss: 0.0035780263133347034\n",
            "step: 60, loss: 0.002102046739310026\n",
            "step: 70, loss: 0.05676734447479248\n",
            "step: 80, loss: 0.027119185775518417\n",
            "step: 90, loss: 0.0753648653626442\n",
            "step: 100, loss: 0.06941071152687073\n",
            "step: 110, loss: 0.05138611048460007\n",
            "step: 120, loss: 0.12378107011318207\n",
            "step: 130, loss: 0.005966637283563614\n",
            "step: 140, loss: 0.0015214952873066068\n",
            "step: 150, loss: 0.0713614895939827\n",
            "step: 160, loss: 0.01445830799639225\n",
            "step: 170, loss: 0.0007477056933566928\n",
            "step: 180, loss: 0.0030019923578947783\n",
            "step: 190, loss: 0.003011075546965003\n",
            "step: 200, loss: 0.0008431681781075895\n",
            "step: 210, loss: 0.0004772826796397567\n",
            "step: 220, loss: 0.13583604991436005\n",
            "step: 230, loss: 0.0025938530452549458\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.971815107102593, f1=0.972972972972973, best_f1=0.972972972972973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003151038195937872\n",
            "step: 10, loss: 0.0037254798226058483\n",
            "step: 20, loss: 0.048445042222738266\n",
            "step: 30, loss: 0.003410705365240574\n",
            "step: 40, loss: 0.11386196315288544\n",
            "step: 50, loss: 0.005112338345497847\n",
            "step: 60, loss: 0.002403060207143426\n",
            "step: 70, loss: 0.006269186269491911\n",
            "step: 80, loss: 0.0016555151669308543\n",
            "step: 90, loss: 0.025329889729619026\n",
            "step: 100, loss: 0.0008446025894954801\n",
            "step: 110, loss: 0.002279765671119094\n",
            "step: 120, loss: 0.0017453193431720138\n",
            "step: 130, loss: 0.0004660518025048077\n",
            "step: 140, loss: 0.0032061166130006313\n",
            "step: 150, loss: 0.0081625422462821\n",
            "step: 160, loss: 0.02509242855012417\n",
            "step: 170, loss: 0.030703090131282806\n",
            "step: 180, loss: 0.007178278639912605\n",
            "step: 190, loss: 0.009972725063562393\n",
            "step: 200, loss: 0.18378527462482452\n",
            "step: 210, loss: 0.006621772423386574\n",
            "step: 220, loss: 0.00827522948384285\n",
            "step: 230, loss: 0.09310071170330048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9733924611973392, f1=0.9698996655518396, best_f1=0.9698996655518396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026881496887654066\n",
            "step: 10, loss: 0.0016907872632145882\n",
            "step: 20, loss: 0.0008372999727725983\n",
            "step: 30, loss: 0.0019373359391465783\n",
            "step: 40, loss: 0.003889305517077446\n",
            "step: 50, loss: 0.004832930862903595\n",
            "step: 60, loss: 0.008800826966762543\n",
            "step: 70, loss: 0.000494733452796936\n",
            "step: 80, loss: 0.0014376427279785275\n",
            "step: 90, loss: 0.0019094857852905989\n",
            "step: 100, loss: 0.0011645450722426176\n",
            "step: 110, loss: 0.0007096805493347347\n",
            "step: 120, loss: 0.002709808060899377\n",
            "step: 130, loss: 0.00239715538918972\n",
            "step: 140, loss: 0.0005125892348587513\n",
            "step: 150, loss: 0.1796960085630417\n",
            "step: 160, loss: 0.14334706962108612\n",
            "step: 170, loss: 0.00496085686609149\n",
            "step: 180, loss: 0.0029577675741165876\n",
            "step: 190, loss: 0.001152673503383994\n",
            "step: 200, loss: 0.0016908959951251745\n",
            "step: 210, loss: 0.00538281723856926\n",
            "step: 220, loss: 0.0007190273609012365\n",
            "step: 230, loss: 0.018810363486409187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.977728285077951, f1=0.9732739420935412, best_f1=0.9732739420935412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026441436260938644\n",
            "step: 10, loss: 0.0008627603529021144\n",
            "step: 20, loss: 0.0004928801208734512\n",
            "step: 30, loss: 0.00020959442190360278\n",
            "step: 40, loss: 0.02752051316201687\n",
            "step: 50, loss: 0.00026576058007776737\n",
            "step: 60, loss: 0.1845388561487198\n",
            "step: 70, loss: 0.0011233786353841424\n",
            "step: 80, loss: 0.0032041803933680058\n",
            "step: 90, loss: 0.0018621855415403843\n",
            "step: 100, loss: 0.0007277477998286486\n",
            "step: 110, loss: 0.011228874325752258\n",
            "step: 120, loss: 0.006968355271965265\n",
            "step: 130, loss: 0.002676063682883978\n",
            "step: 140, loss: 0.006733624264597893\n",
            "step: 150, loss: 0.003188252681866288\n",
            "step: 160, loss: 0.00041303259786218405\n",
            "step: 170, loss: 0.0148519491776824\n",
            "step: 180, loss: 0.004187657963484526\n",
            "step: 190, loss: 0.08725697547197342\n",
            "step: 200, loss: 0.005746576935052872\n",
            "step: 210, loss: 0.005236005410552025\n",
            "step: 220, loss: 0.0008532306528650224\n",
            "step: 230, loss: 0.00032049359288066626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9798206278026906, f1=0.970917225950783, best_f1=0.970917225950783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022573198657482862\n",
            "step: 10, loss: 0.0011144415475428104\n",
            "step: 20, loss: 0.003650335595011711\n",
            "step: 30, loss: 0.00024297831987496465\n",
            "step: 40, loss: 0.00031760375713929534\n",
            "step: 50, loss: 0.0005847482243552804\n",
            "step: 60, loss: 0.00030291921575553715\n",
            "step: 70, loss: 0.002102380385622382\n",
            "step: 80, loss: 0.003713826183229685\n",
            "step: 90, loss: 0.00043499653111211956\n",
            "step: 100, loss: 0.005750415381044149\n",
            "step: 110, loss: 0.0007868606480769813\n",
            "step: 120, loss: 0.00047462419024668634\n",
            "step: 130, loss: 0.0006862377631478012\n",
            "step: 140, loss: 0.00041592857451178133\n",
            "step: 150, loss: 0.002255016006529331\n",
            "step: 160, loss: 0.002863791771233082\n",
            "step: 170, loss: 0.001316435169428587\n",
            "step: 180, loss: 0.008835085667669773\n",
            "step: 190, loss: 0.08578633517026901\n",
            "step: 200, loss: 0.0007113097817637026\n",
            "step: 210, loss: 0.0013745344476774335\n",
            "step: 220, loss: 0.0009466800838708878\n",
            "step: 230, loss: 0.023656100034713745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.980963045912654, f1=0.9763779527559054, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008249772363342345\n",
            "step: 10, loss: 0.00012726857676170766\n",
            "step: 20, loss: 0.00015605363296344876\n",
            "step: 30, loss: 0.00017532167839817703\n",
            "step: 40, loss: 0.00030041977879591286\n",
            "step: 50, loss: 0.00013618904631584883\n",
            "step: 60, loss: 0.00013988606224302202\n",
            "step: 70, loss: 0.0002760797506198287\n",
            "step: 80, loss: 0.0060876342467963696\n",
            "step: 90, loss: 0.00038940884405747056\n",
            "step: 100, loss: 0.00043649101280607283\n",
            "step: 110, loss: 0.0034898535814136267\n",
            "step: 120, loss: 0.003555585164576769\n",
            "step: 130, loss: 0.0009791539050638676\n",
            "step: 140, loss: 0.0010079757776111364\n",
            "step: 150, loss: 0.000350981397787109\n",
            "step: 160, loss: 0.06372598558664322\n",
            "step: 170, loss: 0.0003570764674805105\n",
            "step: 180, loss: 0.0008492153719998896\n",
            "step: 190, loss: 0.00026281457394361496\n",
            "step: 200, loss: 0.04901079460978508\n",
            "step: 210, loss: 0.0001206037777592428\n",
            "step: 220, loss: 0.0004461899516172707\n",
            "step: 230, loss: 0.008297998458147049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9797752808988766, f1=0.9764309764309763, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002323805820196867\n",
            "step: 10, loss: 0.001665815245360136\n",
            "step: 20, loss: 0.00026459331274963915\n",
            "step: 30, loss: 0.00032591939088888466\n",
            "step: 40, loss: 0.00044514387263916433\n",
            "step: 50, loss: 0.000549676304217428\n",
            "step: 60, loss: 0.0022223670966923237\n",
            "step: 70, loss: 0.0003440325381234288\n",
            "step: 80, loss: 0.00026991937193088233\n",
            "step: 90, loss: 7.222520071081817e-05\n",
            "step: 100, loss: 0.0009156595333479345\n",
            "step: 110, loss: 0.0030719900969415903\n",
            "step: 120, loss: 0.03811157867312431\n",
            "step: 130, loss: 8.653799159219489e-05\n",
            "step: 140, loss: 9.464777394896373e-05\n",
            "step: 150, loss: 0.0001756761921569705\n",
            "step: 160, loss: 0.00014438756625168025\n",
            "step: 170, loss: 0.000312784657580778\n",
            "step: 180, loss: 0.012109913863241673\n",
            "step: 190, loss: 0.0001434758014511317\n",
            "step: 200, loss: 0.0001503960374975577\n",
            "step: 210, loss: 0.0007158393273130059\n",
            "step: 220, loss: 0.0005704754730686545\n",
            "step: 230, loss: 0.00026445931871421635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9797297297297298, f1=0.9753363228699552, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008362606167793274\n",
            "step: 10, loss: 0.0016649169847369194\n",
            "step: 20, loss: 0.0005966824246570468\n",
            "step: 30, loss: 0.0006528440862894058\n",
            "step: 40, loss: 0.024640318006277084\n",
            "step: 50, loss: 0.00020092409977223724\n",
            "step: 60, loss: 0.00017221368034370244\n",
            "step: 70, loss: 0.0001670478959567845\n",
            "step: 80, loss: 0.00014663836918771267\n",
            "step: 90, loss: 0.0003378555120434612\n",
            "step: 100, loss: 0.0013914250303059816\n",
            "step: 110, loss: 7.511823787353933e-05\n",
            "step: 120, loss: 0.00010895268496824428\n",
            "step: 130, loss: 7.2386501415167e-05\n",
            "step: 140, loss: 5.552552829612978e-05\n",
            "step: 150, loss: 0.00017667155771050602\n",
            "step: 160, loss: 8.861959940986708e-05\n",
            "step: 170, loss: 0.00026918540243059397\n",
            "step: 180, loss: 0.00044155993964523077\n",
            "step: 190, loss: 9.431586659047753e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 200, loss: 0.00016058293113019317\n",
            "step: 210, loss: 0.00010335926344851032\n",
            "step: 220, loss: 0.00010536093759583309\n",
            "step: 230, loss: 0.013789921067655087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.978675645342312, f1=0.9742441209406495, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007878906326368451\n",
            "step: 10, loss: 8.83090469869785e-05\n",
            "step: 20, loss: 6.234750617295504e-05\n",
            "step: 30, loss: 0.0011958021204918623\n",
            "step: 40, loss: 7.362979522440583e-05\n",
            "step: 50, loss: 6.489466613857076e-05\n",
            "step: 60, loss: 7.23882403690368e-05\n",
            "step: 70, loss: 0.00019106731633655727\n",
            "step: 80, loss: 7.137422653613612e-05\n",
            "step: 90, loss: 0.00015148910460993648\n",
            "step: 100, loss: 0.014074564911425114\n",
            "step: 110, loss: 0.0007045866805128753\n",
            "step: 120, loss: 7.764371548546478e-05\n",
            "step: 130, loss: 6.162939826026559e-05\n",
            "step: 140, loss: 0.027337322011590004\n",
            "step: 150, loss: 0.025783823803067207\n",
            "step: 160, loss: 3.9142782043199986e-05\n",
            "step: 170, loss: 6.842023867648095e-05\n",
            "step: 180, loss: 0.00013015870354138315\n",
            "step: 190, loss: 0.0001288753846893087\n",
            "step: 200, loss: 5.312233042786829e-05\n",
            "step: 210, loss: 0.0006089732050895691\n",
            "step: 220, loss: 5.390239675762132e-05\n",
            "step: 230, loss: 6.980499892961234e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9776286353467561, f1=0.9753914988814317, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.5398577387677506e-05\n",
            "step: 10, loss: 7.869541877880692e-05\n",
            "step: 20, loss: 0.00015877805708441883\n",
            "step: 30, loss: 0.00033459000405855477\n",
            "step: 40, loss: 0.0017475336790084839\n",
            "step: 50, loss: 0.00015894253738224506\n",
            "step: 60, loss: 0.005205168854445219\n",
            "step: 70, loss: 0.0002504417789168656\n",
            "step: 80, loss: 0.00011664431804092601\n",
            "step: 90, loss: 0.0002905378642026335\n",
            "step: 100, loss: 6.943649350432679e-05\n",
            "step: 110, loss: 0.00020801587379537523\n",
            "step: 120, loss: 5.42692796443589e-05\n",
            "step: 130, loss: 4.364908818388358e-05\n",
            "step: 140, loss: 5.655962013406679e-05\n",
            "step: 150, loss: 0.00043658987851813436\n",
            "step: 160, loss: 6.683578249067068e-05\n",
            "step: 170, loss: 9.093889821087942e-05\n",
            "step: 180, loss: 0.0033208385575562716\n",
            "step: 190, loss: 9.073258843272924e-05\n",
            "step: 200, loss: 0.0005582260782830417\n",
            "step: 210, loss: 4.552717655315064e-05\n",
            "step: 220, loss: 0.0006226625991985202\n",
            "step: 230, loss: 0.00910127442330122\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9717514124293786, f1=0.9751693002257337, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002457438095007092\n",
            "step: 10, loss: 5.17776352353394e-05\n",
            "step: 20, loss: 7.084505341481417e-05\n",
            "step: 30, loss: 0.004830458667129278\n",
            "step: 40, loss: 0.0002801943337544799\n",
            "step: 50, loss: 0.004522052593529224\n",
            "step: 60, loss: 0.00012321160465944558\n",
            "step: 70, loss: 0.005525513086467981\n",
            "step: 80, loss: 0.0048625958152115345\n",
            "step: 90, loss: 0.00036152268876321614\n",
            "step: 100, loss: 3.8789203244959936e-05\n",
            "step: 110, loss: 0.00040249587618745863\n",
            "step: 120, loss: 0.0001275773101951927\n",
            "step: 130, loss: 8.824106771498919e-05\n",
            "step: 140, loss: 8.47189876367338e-05\n",
            "step: 150, loss: 0.00010984705295413733\n",
            "step: 160, loss: 0.00017382926307618618\n",
            "step: 170, loss: 8.66501868586056e-05\n",
            "step: 180, loss: 8.019598317332566e-05\n",
            "step: 190, loss: 5.572689406108111e-05\n",
            "step: 200, loss: 0.00010446326632518321\n",
            "step: 210, loss: 0.00011751570855267346\n",
            "step: 220, loss: 9.123125346377492e-05\n",
            "step: 230, loss: 0.02634175308048725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9775280898876404, f1=0.9742441209406495, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012910740042570978\n",
            "step: 10, loss: 0.0011707209050655365\n",
            "step: 20, loss: 0.0004266592441126704\n",
            "step: 30, loss: 0.00013416624278761446\n",
            "step: 40, loss: 4.853735663346015e-05\n",
            "step: 50, loss: 0.000996937626041472\n",
            "step: 60, loss: 0.0005185264744795859\n",
            "step: 70, loss: 0.00021283510432112962\n",
            "step: 80, loss: 6.173613655846566e-05\n",
            "step: 90, loss: 6.270000449148938e-05\n",
            "step: 100, loss: 3.729177115019411e-05\n",
            "step: 110, loss: 0.02927279658615589\n",
            "step: 120, loss: 6.459283758886158e-05\n",
            "step: 130, loss: 0.00018653178995009512\n",
            "step: 140, loss: 0.0013927725376561284\n",
            "step: 150, loss: 5.995689207338728e-05\n",
            "step: 160, loss: 7.478212501155213e-05\n",
            "step: 170, loss: 7.054676825646311e-05\n",
            "step: 180, loss: 5.7773111620917916e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.00023598611005581915\n",
            "step: 200, loss: 0.00041462347144261\n",
            "step: 210, loss: 4.5136424887459725e-05\n",
            "step: 220, loss: 4.2343290260760114e-05\n",
            "step: 230, loss: 5.2622464863816276e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.978675645342312, f1=0.9754464285714286, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007401106413453817\n",
            "step: 10, loss: 0.00040281793917529285\n",
            "step: 20, loss: 0.00036387238651514053\n",
            "step: 30, loss: 8.227104262914509e-05\n",
            "step: 40, loss: 0.0005731173441745341\n",
            "step: 50, loss: 0.00011780055501731113\n",
            "step: 60, loss: 8.158345735864714e-05\n",
            "step: 70, loss: 0.00010787024075398222\n",
            "step: 80, loss: 0.0003807793837040663\n",
            "step: 90, loss: 6.259629299165681e-05\n",
            "step: 100, loss: 6.89519292791374e-05\n",
            "step: 110, loss: 0.00016716925892978907\n",
            "step: 120, loss: 3.920759991160594e-05\n",
            "step: 130, loss: 4.344979242887348e-05\n",
            "step: 140, loss: 0.0002902352425735444\n",
            "step: 150, loss: 3.878925053868443e-05\n",
            "step: 160, loss: 0.00010355365520808846\n",
            "step: 170, loss: 2.0291332475608215e-05\n",
            "step: 180, loss: 6.412035872926936e-05\n",
            "step: 190, loss: 0.00011716688459273428\n",
            "step: 200, loss: 3.468032446107827e-05\n",
            "step: 210, loss: 0.0005392095190472901\n",
            "step: 220, loss: 5.420640081865713e-05\n",
            "step: 230, loss: 0.012526961974799633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9741863075196409, f1=0.9731543624161074, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.499582478776574e-05\n",
            "step: 10, loss: 2.8815929908887483e-05\n",
            "step: 20, loss: 4.291670848033391e-05\n",
            "step: 30, loss: 3.9769558497937396e-05\n",
            "step: 40, loss: 5.100760972709395e-05\n",
            "step: 50, loss: 0.01933087408542633\n",
            "step: 60, loss: 8.253670966951177e-05\n",
            "step: 70, loss: 6.868722994113341e-05\n",
            "step: 80, loss: 4.4192867790116e-05\n",
            "step: 90, loss: 0.0002602083550300449\n",
            "step: 100, loss: 5.0481336074881256e-05\n",
            "step: 110, loss: 3.2650859793648124e-05\n",
            "step: 120, loss: 0.004527286160737276\n",
            "step: 130, loss: 9.907638741424307e-05\n",
            "step: 140, loss: 4.9842365115182474e-05\n",
            "step: 150, loss: 0.00013426512305159122\n",
            "step: 160, loss: 3.511955583235249e-05\n",
            "step: 170, loss: 3.1354251404991373e-05\n",
            "step: 180, loss: 0.0001659712434047833\n",
            "step: 190, loss: 6.982959894230589e-05\n",
            "step: 200, loss: 6.207804835867137e-05\n",
            "step: 210, loss: 5.424936898634769e-05\n",
            "step: 220, loss: 0.00020539491379167885\n",
            "step: 230, loss: 3.9657203160459176e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9764837625979844, f1=0.9720670391061451, best_f1=0.9763779527559054\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 206.96it/s]\n",
            "load_f1 = 0.9787234042553192\n",
            "real_f1 = 0.9765363128491621\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 237.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91015920-dd56-48ff-e9d9-24e9b93d489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.622779130935669\n",
            "step: 10, loss: 0.5275973081588745\n",
            "step: 20, loss: 0.48310959339141846\n",
            "step: 30, loss: 0.11289907991886139\n",
            "step: 40, loss: 0.19237300753593445\n",
            "step: 50, loss: 0.1051773726940155\n",
            "step: 60, loss: 0.038916174322366714\n",
            "step: 70, loss: 0.1332879513502121\n",
            "step: 80, loss: 0.0595669187605381\n",
            "step: 90, loss: 0.16507588326931\n",
            "step: 100, loss: 0.008498692885041237\n",
            "step: 110, loss: 0.08463716506958008\n",
            "step: 120, loss: 0.11474382132291794\n",
            "step: 130, loss: 0.0652799978852272\n",
            "step: 140, loss: 0.1260744035243988\n",
            "step: 150, loss: 0.03715766966342926\n",
            "step: 160, loss: 0.011899762786924839\n",
            "step: 170, loss: 0.20283089578151703\n",
            "step: 180, loss: 0.055387552827596664\n",
            "step: 190, loss: 0.003894668072462082\n",
            "step: 200, loss: 0.15793925523757935\n",
            "step: 210, loss: 0.0726628303527832\n",
            "step: 220, loss: 0.22228150069713593\n",
            "step: 230, loss: 0.17135848104953766\n",
            "step: 240, loss: 0.043372441083192825\n",
            "step: 250, loss: 0.023197872564196587\n",
            "step: 260, loss: 0.052153635770082474\n",
            "step: 270, loss: 0.009614781476557255\n",
            "step: 280, loss: 0.03135520592331886\n",
            "step: 290, loss: 0.031743843108415604\n",
            "step: 300, loss: 0.029772324487566948\n",
            "step: 310, loss: 0.224617600440979\n",
            "step: 320, loss: 0.12615352869033813\n",
            "step: 330, loss: 0.0880129486322403\n",
            "step: 340, loss: 0.06111243739724159\n",
            "step: 350, loss: 0.03864394128322601\n",
            "step: 360, loss: 0.09316810220479965\n",
            "step: 370, loss: 0.06655983626842499\n",
            "step: 380, loss: 0.003490251023322344\n",
            "step: 390, loss: 0.16825971007347107\n",
            "step: 400, loss: 0.2993960976600647\n",
            "step: 410, loss: 0.07628637552261353\n",
            "step: 420, loss: 0.08534898608922958\n",
            "step: 430, loss: 0.15191909670829773\n",
            "step: 440, loss: 0.039742544293403625\n",
            "step: 450, loss: 0.0031632045283913612\n",
            "step: 460, loss: 0.004211868159472942\n",
            "step: 470, loss: 0.15681633353233337\n",
            "step: 480, loss: 0.08089405298233032\n",
            "step: 490, loss: 0.10740760713815689\n",
            "step: 500, loss: 0.07589790970087051\n",
            "step: 510, loss: 0.02808593586087227\n",
            "step: 520, loss: 0.22000066936016083\n",
            "step: 530, loss: 0.005513809621334076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9378214118747078, f1=0.9333333333333335, best_f1=0.9333333333333335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18094293773174286\n",
            "step: 10, loss: 0.14935418963432312\n",
            "step: 20, loss: 0.00870467722415924\n",
            "step: 30, loss: 0.008358423598110676\n",
            "step: 40, loss: 0.09689725190401077\n",
            "step: 50, loss: 0.06440630555152893\n",
            "step: 60, loss: 0.016148552298545837\n",
            "step: 70, loss: 0.027182072401046753\n",
            "step: 80, loss: 0.03061521053314209\n",
            "step: 90, loss: 0.011298046447336674\n",
            "step: 100, loss: 0.025286532938480377\n",
            "step: 110, loss: 0.00429791584610939\n",
            "step: 120, loss: 0.07362674921751022\n",
            "step: 130, loss: 0.08713658154010773\n",
            "step: 140, loss: 0.0268063023686409\n",
            "step: 150, loss: 0.058482490479946136\n",
            "step: 160, loss: 0.05283161997795105\n",
            "step: 170, loss: 0.011720342561602592\n",
            "step: 180, loss: 0.02963690087199211\n",
            "step: 190, loss: 0.0834307074546814\n",
            "step: 200, loss: 0.03853008523583412\n",
            "step: 210, loss: 0.05145689845085144\n",
            "step: 220, loss: 0.06053999438881874\n",
            "step: 230, loss: 0.0098965959623456\n",
            "step: 240, loss: 0.054112184792757034\n",
            "step: 250, loss: 0.009877145290374756\n",
            "step: 260, loss: 0.0021062230225652456\n",
            "step: 270, loss: 0.11531636118888855\n",
            "step: 280, loss: 0.02808830887079239\n",
            "step: 290, loss: 0.025152157992124557\n",
            "step: 300, loss: 0.18964961171150208\n",
            "step: 310, loss: 0.007027726620435715\n",
            "step: 320, loss: 0.09362034499645233\n",
            "step: 330, loss: 0.07972729206085205\n",
            "step: 340, loss: 0.02515587955713272\n",
            "step: 350, loss: 0.0014690498355776072\n",
            "step: 360, loss: 0.03319256007671356\n",
            "step: 370, loss: 0.06648119539022446\n",
            "step: 380, loss: 0.029195932671427727\n",
            "step: 390, loss: 0.09204292297363281\n",
            "step: 400, loss: 0.03838859498500824\n",
            "step: 410, loss: 0.09918512403964996\n",
            "step: 420, loss: 0.02702920511364937\n",
            "step: 430, loss: 0.0912967249751091\n",
            "step: 440, loss: 0.2084704339504242\n",
            "step: 450, loss: 0.052516911178827286\n",
            "step: 460, loss: 0.1318807750940323\n",
            "step: 470, loss: 0.12025365233421326\n",
            "step: 480, loss: 0.25759702920913696\n",
            "step: 490, loss: 0.01609579659998417\n",
            "step: 500, loss: 0.28244417905807495\n",
            "step: 510, loss: 0.026727793738245964\n",
            "step: 520, loss: 0.0678730383515358\n",
            "step: 530, loss: 0.016048062592744827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9387379087977891, f1=0.9426751592356688, best_f1=0.9426751592356688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0361579991877079\n",
            "step: 10, loss: 0.03225671872496605\n",
            "step: 20, loss: 0.027444962412118912\n",
            "step: 30, loss: 0.08749599754810333\n",
            "step: 40, loss: 0.009678586386144161\n",
            "step: 50, loss: 0.09865362197160721\n",
            "step: 60, loss: 0.0243461262434721\n",
            "step: 70, loss: 0.009901993907988071\n",
            "step: 80, loss: 0.021649440750479698\n",
            "step: 90, loss: 0.008754130452871323\n",
            "step: 100, loss: 0.08516976982355118\n",
            "step: 110, loss: 0.023243263363838196\n",
            "step: 120, loss: 0.01179206557571888\n",
            "step: 130, loss: 0.006464681588113308\n",
            "step: 140, loss: 0.024137670174241066\n",
            "step: 150, loss: 0.02248375304043293\n",
            "step: 160, loss: 0.007413546089082956\n",
            "step: 170, loss: 0.1295415163040161\n",
            "step: 180, loss: 0.05063636600971222\n",
            "step: 190, loss: 0.003133955877274275\n",
            "step: 200, loss: 0.06698116660118103\n",
            "step: 210, loss: 0.04477537423372269\n",
            "step: 220, loss: 0.10455337166786194\n",
            "step: 230, loss: 0.10763264447450638\n",
            "step: 240, loss: 0.0034286899026483297\n",
            "step: 250, loss: 0.028674285858869553\n",
            "step: 260, loss: 0.05083981156349182\n",
            "step: 270, loss: 0.005372663494199514\n",
            "step: 280, loss: 0.16593343019485474\n",
            "step: 290, loss: 0.0031755685340613127\n",
            "step: 300, loss: 0.04466870427131653\n",
            "step: 310, loss: 0.01977984793484211\n",
            "step: 320, loss: 0.061392322182655334\n",
            "step: 330, loss: 0.004433652851730585\n",
            "step: 340, loss: 0.042727500200271606\n",
            "step: 350, loss: 0.010430168360471725\n",
            "step: 360, loss: 0.042591679841279984\n",
            "step: 370, loss: 0.033035825937986374\n",
            "step: 380, loss: 0.11584734916687012\n",
            "step: 390, loss: 0.011988498270511627\n",
            "step: 400, loss: 0.023807061836123466\n",
            "step: 410, loss: 0.00391494482755661\n",
            "step: 420, loss: 0.1092425063252449\n",
            "step: 430, loss: 0.04715125635266304\n",
            "step: 440, loss: 0.03739115968346596\n",
            "step: 450, loss: 0.06263525784015656\n",
            "step: 460, loss: 0.04055683687329292\n",
            "step: 470, loss: 0.09662601351737976\n",
            "step: 480, loss: 0.005424105562269688\n",
            "step: 490, loss: 0.004446505568921566\n",
            "step: 500, loss: 0.0037889047525823116\n",
            "step: 510, loss: 0.0052258591167628765\n",
            "step: 520, loss: 0.04991353303194046\n",
            "step: 530, loss: 0.010040398687124252\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9385474860335196, f1=0.9419354838709678, best_f1=0.9426751592356688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005723079200834036\n",
            "step: 10, loss: 0.006814571563154459\n",
            "step: 20, loss: 0.0014205977786332369\n",
            "step: 30, loss: 0.006356928031891584\n",
            "step: 40, loss: 0.016937127336859703\n",
            "step: 50, loss: 0.07905756682157516\n",
            "step: 60, loss: 0.0009415718959644437\n",
            "step: 70, loss: 0.025147365406155586\n",
            "step: 80, loss: 0.0044614337384700775\n",
            "step: 90, loss: 0.054045334458351135\n",
            "step: 100, loss: 0.06338650733232498\n",
            "step: 110, loss: 0.03396662697196007\n",
            "step: 120, loss: 0.00046988832764327526\n",
            "step: 130, loss: 0.0033641948830336332\n",
            "step: 140, loss: 0.010851985774934292\n",
            "step: 150, loss: 0.032418809831142426\n",
            "step: 160, loss: 0.006532840896397829\n",
            "step: 170, loss: 0.0013073206646367908\n",
            "step: 180, loss: 0.0021528161596506834\n",
            "step: 190, loss: 0.04716425761580467\n",
            "step: 200, loss: 0.016883935779333115\n",
            "step: 210, loss: 0.03039388172328472\n",
            "step: 220, loss: 0.012114390730857849\n",
            "step: 230, loss: 0.04679577425122261\n",
            "step: 240, loss: 0.003152471501380205\n",
            "step: 250, loss: 0.03663688898086548\n",
            "step: 260, loss: 0.008227696642279625\n",
            "step: 270, loss: 0.006793361157178879\n",
            "step: 280, loss: 0.10927943140268326\n",
            "step: 290, loss: 0.09157305955886841\n",
            "step: 300, loss: 0.00039416473009623587\n",
            "step: 310, loss: 0.026689549908041954\n",
            "step: 320, loss: 0.004467883612960577\n",
            "step: 330, loss: 0.002528916345909238\n",
            "step: 340, loss: 0.14500118792057037\n",
            "step: 350, loss: 0.016260778531432152\n",
            "step: 360, loss: 0.053769320249557495\n",
            "step: 370, loss: 0.003101135604083538\n",
            "step: 380, loss: 0.024801691994071007\n",
            "step: 390, loss: 0.0007888007094152272\n",
            "step: 400, loss: 0.09362640231847763\n",
            "step: 410, loss: 0.020333178341388702\n",
            "step: 420, loss: 0.003843069775030017\n",
            "step: 430, loss: 0.0346059650182724\n",
            "step: 440, loss: 0.0022273308131843805\n",
            "step: 450, loss: 0.0026484401896595955\n",
            "step: 460, loss: 0.0044701281003654\n",
            "step: 470, loss: 0.014701714739203453\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 480, loss: 0.24645915627479553\n",
            "step: 490, loss: 0.004608319140970707\n",
            "step: 500, loss: 0.025165900588035583\n",
            "step: 510, loss: 0.026782991364598274\n",
            "step: 520, loss: 0.0914570540189743\n",
            "step: 530, loss: 0.018058402463793755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9410125406409661, f1=0.934622467771639, best_f1=0.934622467771639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020834550261497498\n",
            "step: 10, loss: 0.04787953570485115\n",
            "step: 20, loss: 0.013070871122181416\n",
            "step: 30, loss: 0.0011224907357245684\n",
            "step: 40, loss: 0.0056644948199391365\n",
            "step: 50, loss: 0.002371265785768628\n",
            "step: 60, loss: 0.13789907097816467\n",
            "step: 70, loss: 0.00698635820299387\n",
            "step: 80, loss: 0.0011242344044148922\n",
            "step: 90, loss: 0.07658139616250992\n",
            "step: 100, loss: 0.017582625150680542\n",
            "step: 110, loss: 0.0016375145642086864\n",
            "step: 120, loss: 0.0038645442109555006\n",
            "step: 130, loss: 0.0009484371403232217\n",
            "step: 140, loss: 0.043578993529081345\n",
            "step: 150, loss: 0.01558883860707283\n",
            "step: 160, loss: 0.006042839027941227\n",
            "step: 170, loss: 0.030664701014757156\n",
            "step: 180, loss: 0.0023609264753758907\n",
            "step: 190, loss: 0.0016433793352916837\n",
            "step: 200, loss: 0.000805108982603997\n",
            "step: 210, loss: 0.004941342864185572\n",
            "step: 220, loss: 0.143914133310318\n",
            "step: 230, loss: 0.019991092383861542\n",
            "step: 240, loss: 0.0023381374776363373\n",
            "step: 250, loss: 0.0037048368249088526\n",
            "step: 260, loss: 0.0025966581888496876\n",
            "step: 270, loss: 0.00011946802260354161\n",
            "step: 280, loss: 0.001077710185199976\n",
            "step: 290, loss: 0.14553874731063843\n",
            "step: 300, loss: 0.011412191204726696\n",
            "step: 310, loss: 0.002838225569576025\n",
            "step: 320, loss: 0.05450898036360741\n",
            "step: 330, loss: 0.04246392101049423\n",
            "step: 340, loss: 0.007528673857450485\n",
            "step: 350, loss: 0.029150232672691345\n",
            "step: 360, loss: 0.0008545289165340364\n",
            "step: 370, loss: 0.04179603233933449\n",
            "step: 380, loss: 7.417218876071274e-05\n",
            "step: 390, loss: 0.00038696586852893233\n",
            "step: 400, loss: 0.02549278922379017\n",
            "step: 410, loss: 0.01135576143860817\n",
            "step: 420, loss: 0.0012358188396319747\n",
            "step: 430, loss: 0.00297615141607821\n",
            "step: 440, loss: 0.0055243829265236855\n",
            "step: 450, loss: 0.1571044921875\n",
            "step: 460, loss: 0.007279423996806145\n",
            "step: 470, loss: 0.010462114587426186\n",
            "step: 480, loss: 0.0030253021977841854\n",
            "step: 490, loss: 0.0010076418984681368\n",
            "step: 500, loss: 0.03595242649316788\n",
            "step: 510, loss: 0.006472621578723192\n",
            "step: 520, loss: 0.02957121655344963\n",
            "step: 530, loss: 0.005758689250797033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9411764705882353, f1=0.9410150891632373, best_f1=0.9410150891632373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00832322333008051\n",
            "step: 10, loss: 0.0002250179386464879\n",
            "step: 20, loss: 0.11102385073900223\n",
            "step: 30, loss: 0.0011013925541192293\n",
            "step: 40, loss: 0.0007326924824155867\n",
            "step: 50, loss: 0.013825629837810993\n",
            "step: 60, loss: 0.0005042552947998047\n",
            "step: 70, loss: 0.0008959168335422873\n",
            "step: 80, loss: 0.027659688144922256\n",
            "step: 90, loss: 0.002166209276765585\n",
            "step: 100, loss: 0.014434569515287876\n",
            "step: 110, loss: 0.011797169223427773\n",
            "step: 120, loss: 0.028645873069763184\n",
            "step: 130, loss: 0.01226021721959114\n",
            "step: 140, loss: 0.009101735427975655\n",
            "step: 150, loss: 0.022244714200496674\n",
            "step: 160, loss: 0.0018502939492464066\n",
            "step: 170, loss: 0.0008387524867430329\n",
            "step: 180, loss: 0.028505006805062294\n",
            "step: 190, loss: 0.007003356236964464\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 200, loss: 0.0010064449161291122\n",
            "step: 210, loss: 0.005690797232091427\n",
            "step: 220, loss: 0.0024439250119030476\n",
            "step: 230, loss: 0.00036989463842473924\n",
            "step: 240, loss: 0.009634379297494888\n",
            "step: 250, loss: 0.011921407654881477\n",
            "step: 260, loss: 0.004600135143846273\n",
            "step: 270, loss: 0.11845269799232483\n",
            "step: 280, loss: 0.005335036665201187\n",
            "step: 290, loss: 0.0007967686397023499\n",
            "step: 300, loss: 0.002665080362930894\n",
            "step: 310, loss: 0.009424257092177868\n",
            "step: 320, loss: 0.00033285870449617505\n",
            "step: 330, loss: 0.01204647496342659\n",
            "step: 340, loss: 0.04133171588182449\n",
            "step: 350, loss: 0.0053005702793598175\n",
            "step: 360, loss: 0.006976163014769554\n",
            "step: 370, loss: 0.0018751011230051517\n",
            "step: 380, loss: 0.0020251476671546698\n",
            "step: 390, loss: 0.015171248465776443\n",
            "step: 400, loss: 0.0003797896788455546\n",
            "step: 410, loss: 0.0005826774286106229\n",
            "step: 420, loss: 0.012433603405952454\n",
            "step: 430, loss: 0.0008864053525030613\n",
            "step: 440, loss: 0.0009138880996033549\n",
            "step: 450, loss: 0.0016529789427295327\n",
            "step: 460, loss: 0.001130920136347413\n",
            "step: 470, loss: 0.0034428881481289864\n",
            "step: 480, loss: 0.0115025220438838\n",
            "step: 490, loss: 0.01174985896795988\n",
            "step: 500, loss: 0.01838747411966324\n",
            "step: 510, loss: 0.01129350159317255\n",
            "step: 520, loss: 0.01109018549323082\n",
            "step: 530, loss: 0.11421897262334824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9433255269320844, f1=0.9392675011590171, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001342212490271777\n",
            "step: 10, loss: 0.00031746807508170605\n",
            "step: 20, loss: 0.0011354265734553337\n",
            "step: 30, loss: 0.011728874407708645\n",
            "step: 40, loss: 0.005379855632781982\n",
            "step: 50, loss: 0.009842222556471825\n",
            "step: 60, loss: 0.04410979524254799\n",
            "step: 70, loss: 0.00044789540697820485\n",
            "step: 80, loss: 0.0015502457972615957\n",
            "step: 90, loss: 0.0009636356262490153\n",
            "step: 100, loss: 0.00012041611626045778\n",
            "step: 110, loss: 0.0002761341747827828\n",
            "step: 120, loss: 0.0004584275302477181\n",
            "step: 130, loss: 0.01499202474951744\n",
            "step: 140, loss: 0.00014824143727310002\n",
            "step: 150, loss: 0.08500062674283981\n",
            "step: 160, loss: 9.179919288726524e-05\n",
            "step: 170, loss: 0.0004885879461653531\n",
            "step: 180, loss: 0.004475985653698444\n",
            "step: 190, loss: 0.00046471721725538373\n",
            "step: 200, loss: 0.0002741771168075502\n",
            "step: 210, loss: 0.0005286402883939445\n",
            "step: 220, loss: 0.00041993040940724313\n",
            "step: 230, loss: 0.0040346127934753895\n",
            "step: 240, loss: 0.0005395518965087831\n",
            "step: 250, loss: 0.0004765736812260002\n",
            "step: 260, loss: 0.06524863839149475\n",
            "step: 270, loss: 0.0005171767552383244\n",
            "step: 280, loss: 0.0003921081079170108\n",
            "step: 290, loss: 0.0012355224462226033\n",
            "step: 300, loss: 0.003240796271711588\n",
            "step: 310, loss: 0.0007008447428233922\n",
            "step: 320, loss: 0.004242428578436375\n",
            "step: 330, loss: 0.0005130689824000001\n",
            "step: 340, loss: 0.009178945794701576\n",
            "step: 350, loss: 0.05532969906926155\n",
            "step: 360, loss: 0.0031671328470110893\n",
            "step: 370, loss: 0.0018380949040874839\n",
            "step: 380, loss: 0.0005007098661735654\n",
            "step: 390, loss: 0.001222051098011434\n",
            "step: 400, loss: 0.00829986110329628\n",
            "step: 410, loss: 0.01696852408349514\n",
            "step: 420, loss: 0.03321333974599838\n",
            "step: 430, loss: 0.00048268030514009297\n",
            "step: 440, loss: 0.002802795497700572\n",
            "step: 450, loss: 0.0010657947277650237\n",
            "step: 460, loss: 0.0011331530986353755\n",
            "step: 470, loss: 0.005340824835002422\n",
            "step: 480, loss: 0.09535203874111176\n",
            "step: 490, loss: 0.00789873767644167\n",
            "step: 500, loss: 0.06803953647613525\n",
            "step: 510, loss: 0.043735720217227936\n",
            "step: 520, loss: 0.08010351657867432\n",
            "step: 530, loss: 0.0026987381279468536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9400187441424556, f1=0.9331476323119777, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008563774754293263\n",
            "step: 10, loss: 0.0625843033194542\n",
            "step: 20, loss: 0.002793934429064393\n",
            "step: 30, loss: 0.017295174300670624\n",
            "step: 40, loss: 0.0001372547703795135\n",
            "step: 50, loss: 0.005169880110770464\n",
            "step: 60, loss: 0.0013807631330564618\n",
            "step: 70, loss: 0.0002995500690303743\n",
            "step: 80, loss: 0.008668330498039722\n",
            "step: 90, loss: 0.0007276290562003851\n",
            "step: 100, loss: 0.0015268104616552591\n",
            "step: 110, loss: 0.000639788806438446\n",
            "step: 120, loss: 0.03314316272735596\n",
            "step: 130, loss: 0.0015030507929623127\n",
            "step: 140, loss: 0.004256189800798893\n",
            "step: 150, loss: 0.008202278055250645\n",
            "step: 160, loss: 0.028438687324523926\n",
            "step: 170, loss: 0.013502940535545349\n",
            "step: 180, loss: 0.002867850475013256\n",
            "step: 190, loss: 0.0045578982681035995\n",
            "step: 200, loss: 0.0009899245342239738\n",
            "step: 210, loss: 0.0054556201212108135\n",
            "step: 220, loss: 0.0027009134646505117\n",
            "step: 230, loss: 0.0011586463078856468\n",
            "step: 240, loss: 0.0014303330099210143\n",
            "step: 250, loss: 7.496252510463819e-05\n",
            "step: 260, loss: 9.480041626375169e-05\n",
            "step: 270, loss: 0.010689113289117813\n",
            "step: 280, loss: 0.0023824807722121477\n",
            "step: 290, loss: 0.0002955622912850231\n",
            "step: 300, loss: 0.0020285933278501034\n",
            "step: 310, loss: 0.0003316628572065383\n",
            "step: 320, loss: 0.015260877087712288\n",
            "step: 330, loss: 0.00021366179862525314\n",
            "step: 340, loss: 6.502788892248645e-05\n",
            "step: 350, loss: 0.0011512167984619737\n",
            "step: 360, loss: 0.00038860339554958045\n",
            "step: 370, loss: 0.0013624678831547499\n",
            "step: 380, loss: 0.022146038711071014\n",
            "step: 390, loss: 0.15942390263080597\n",
            "step: 400, loss: 0.0010818380396813154\n",
            "step: 410, loss: 0.00041573235648684204\n",
            "step: 420, loss: 0.0018302724929526448\n",
            "step: 430, loss: 0.022652270272374153\n",
            "step: 440, loss: 0.0024638460017740726\n",
            "step: 450, loss: 0.0006740656681358814\n",
            "step: 460, loss: 0.00029693098622374237\n",
            "step: 470, loss: 0.00013094696623738855\n",
            "step: 480, loss: 7.174410711741075e-05\n",
            "step: 490, loss: 0.0038866540417075157\n",
            "step: 500, loss: 0.0012696085032075644\n",
            "step: 510, loss: 0.0002260289911646396\n",
            "step: 520, loss: 0.01862211897969246\n",
            "step: 530, loss: 0.0013195137726143003\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9406307977736549, f1=0.9335776454420522, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015997110167518258\n",
            "step: 10, loss: 0.000868145958520472\n",
            "step: 20, loss: 0.0018190309638157487\n",
            "step: 30, loss: 0.00033038476249203086\n",
            "step: 40, loss: 0.0006976814474910498\n",
            "step: 50, loss: 0.00020595520618371665\n",
            "step: 60, loss: 0.0001481154322391376\n",
            "step: 70, loss: 0.000216293876292184\n",
            "step: 80, loss: 0.023171598091721535\n",
            "step: 90, loss: 6.062351167201996e-05\n",
            "step: 100, loss: 0.00013977254275232553\n",
            "step: 110, loss: 4.5754793973173946e-05\n",
            "step: 120, loss: 0.0010923986556008458\n",
            "step: 130, loss: 0.0338941253721714\n",
            "step: 140, loss: 0.00039232574636116624\n",
            "step: 150, loss: 8.010405872482806e-05\n",
            "step: 160, loss: 0.003921481315046549\n",
            "step: 170, loss: 0.016700655221939087\n",
            "step: 180, loss: 0.0005737433093599975\n",
            "step: 190, loss: 0.0001471071591367945\n",
            "step: 200, loss: 0.0011603519087657332\n",
            "step: 210, loss: 0.0001817496377043426\n",
            "step: 220, loss: 0.0004664068401325494\n",
            "step: 230, loss: 0.0009061549790203571\n",
            "step: 240, loss: 4.928962152916938e-05\n",
            "step: 250, loss: 0.000927539193071425\n",
            "step: 260, loss: 0.0028650495223701\n",
            "step: 270, loss: 0.0009960492607206106\n",
            "step: 280, loss: 0.013495701365172863\n",
            "step: 290, loss: 0.04100016877055168\n",
            "step: 300, loss: 4.7243531298590824e-05\n",
            "step: 310, loss: 0.013239904306828976\n",
            "step: 320, loss: 0.003991695586591959\n",
            "step: 330, loss: 0.006420547142624855\n",
            "step: 340, loss: 0.010385410860180855\n",
            "step: 350, loss: 0.008068466559052467\n",
            "step: 360, loss: 5.410626545199193e-05\n",
            "step: 370, loss: 0.0010716163087636232\n",
            "step: 380, loss: 0.0005719356122426689\n",
            "step: 390, loss: 0.00011323072976665571\n",
            "step: 400, loss: 0.0002403412654530257\n",
            "step: 410, loss: 0.00365914567373693\n",
            "step: 420, loss: 9.129913814831525e-05\n",
            "step: 430, loss: 5.5018477723933756e-05\n",
            "step: 440, loss: 0.0008348796400241554\n",
            "step: 450, loss: 0.00056644924916327\n",
            "step: 460, loss: 0.010445189662277699\n",
            "step: 470, loss: 0.016230162233114243\n",
            "step: 480, loss: 8.706992957741022e-05\n",
            "step: 490, loss: 0.001867602113634348\n",
            "step: 500, loss: 0.0026549948379397392\n",
            "step: 510, loss: 0.004787901416420937\n",
            "step: 520, loss: 0.0001503763924119994\n",
            "step: 530, loss: 0.00013657924137078226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9412317818523741, f1=0.9389277389277391, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.927481495542452e-05\n",
            "step: 10, loss: 9.109747770708054e-05\n",
            "step: 20, loss: 4.1959912778111175e-05\n",
            "step: 30, loss: 2.5822575480560772e-05\n",
            "step: 40, loss: 0.0002267227100674063\n",
            "step: 50, loss: 0.002139677293598652\n",
            "step: 60, loss: 0.00017761881463229656\n",
            "step: 70, loss: 9.024712926475331e-05\n",
            "step: 80, loss: 0.00010889465920627117\n",
            "step: 90, loss: 0.00032467208802700043\n",
            "step: 100, loss: 0.0008203868637792766\n",
            "step: 110, loss: 0.0005806826520711184\n",
            "step: 120, loss: 0.0002882083354052156\n",
            "step: 130, loss: 0.007844218984246254\n",
            "step: 140, loss: 0.0034575669560581446\n",
            "step: 150, loss: 0.0029785365331918\n",
            "step: 160, loss: 0.0009911153465509415\n",
            "step: 170, loss: 0.0002965344174299389\n",
            "step: 180, loss: 0.0007771109230816364\n",
            "step: 190, loss: 0.004488188307732344\n",
            "step: 200, loss: 0.0010071403812617064\n",
            "step: 210, loss: 0.00929993949830532\n",
            "step: 220, loss: 0.013491489924490452\n",
            "step: 230, loss: 0.0012525059282779694\n",
            "step: 240, loss: 6.734677299391478e-05\n",
            "step: 250, loss: 3.0743532988708466e-05\n",
            "step: 260, loss: 0.0013644042192026973\n",
            "step: 270, loss: 0.001009313273243606\n",
            "step: 280, loss: 0.03626009076833725\n",
            "step: 290, loss: 0.00016382870671804994\n",
            "step: 300, loss: 0.0005436898791231215\n",
            "step: 310, loss: 0.0005632793181575835\n",
            "step: 320, loss: 0.0010812385007739067\n",
            "step: 330, loss: 0.0005079521215520799\n",
            "step: 340, loss: 0.0026809812989085913\n",
            "step: 350, loss: 0.0016545187681913376\n",
            "step: 360, loss: 9.436633990844712e-05\n",
            "step: 370, loss: 0.007953505031764507\n",
            "step: 380, loss: 0.003949563018977642\n",
            "step: 390, loss: 5.9560949011938646e-05\n",
            "step: 400, loss: 0.0015630333218723536\n",
            "step: 410, loss: 0.0016870005056262016\n",
            "step: 420, loss: 9.596301970304921e-05\n",
            "step: 430, loss: 0.0001365634088870138\n",
            "step: 440, loss: 0.006304907146841288\n",
            "step: 450, loss: 7.703074516030028e-05\n",
            "step: 460, loss: 0.0005286912783049047\n",
            "step: 470, loss: 9.029858483700082e-05\n",
            "step: 480, loss: 9.106930519919842e-05\n",
            "step: 490, loss: 0.004508693236857653\n",
            "step: 500, loss: 0.0133954593911767\n",
            "step: 510, loss: 0.0003142130735795945\n",
            "step: 520, loss: 0.002024455927312374\n",
            "step: 530, loss: 0.002631843788549304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9406858202038925, f1=0.9368709972552608, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001215469601447694\n",
            "step: 10, loss: 0.029168393462896347\n",
            "step: 20, loss: 0.0007585682906210423\n",
            "step: 30, loss: 0.0016405177302658558\n",
            "step: 40, loss: 0.00027458323165774345\n",
            "step: 50, loss: 0.0031648033764213324\n",
            "step: 60, loss: 0.018830761313438416\n",
            "step: 70, loss: 0.005470042582601309\n",
            "step: 80, loss: 3.20675790135283e-05\n",
            "step: 90, loss: 0.00033465068554505706\n",
            "step: 100, loss: 0.0006467649363912642\n",
            "step: 110, loss: 0.00035670623765327036\n",
            "step: 120, loss: 0.00018244137754663825\n",
            "step: 130, loss: 0.0008016828796826303\n",
            "step: 140, loss: 0.18271946907043457\n",
            "step: 150, loss: 6.781150295864791e-05\n",
            "step: 160, loss: 0.0007932519656606019\n",
            "step: 170, loss: 0.0008174965041689575\n",
            "step: 180, loss: 0.00024337437935173512\n",
            "step: 190, loss: 0.002976955147460103\n",
            "step: 200, loss: 0.0017875289777293801\n",
            "step: 210, loss: 0.0005620368174277246\n",
            "step: 220, loss: 0.003344695083796978\n",
            "step: 230, loss: 0.0006880986620672047\n",
            "step: 240, loss: 0.001100849243812263\n",
            "step: 250, loss: 0.00021837638632860035\n",
            "step: 260, loss: 0.0003866591432597488\n",
            "step: 270, loss: 0.046796105802059174\n",
            "step: 280, loss: 0.00044028557022102177\n",
            "step: 290, loss: 0.0010756237898021936\n",
            "step: 300, loss: 0.00034439796581864357\n",
            "step: 310, loss: 0.00022764176537748426\n",
            "step: 320, loss: 0.004883880261331797\n",
            "step: 330, loss: 7.042507786536589e-05\n",
            "step: 340, loss: 0.0013342826860025525\n",
            "step: 350, loss: 0.0018748160218819976\n",
            "step: 360, loss: 7.980683585628867e-05\n",
            "step: 370, loss: 0.00167552859056741\n",
            "step: 380, loss: 8.489465108141303e-05\n",
            "step: 390, loss: 0.0001303347380599007\n",
            "step: 400, loss: 0.0016080030472949147\n",
            "step: 410, loss: 0.002557202475145459\n",
            "step: 420, loss: 0.0005566520849242806\n",
            "step: 430, loss: 0.001225977553986013\n",
            "step: 440, loss: 0.0035787425003945827\n",
            "step: 450, loss: 0.0068101584911346436\n",
            "step: 460, loss: 0.0009266782435588539\n",
            "step: 470, loss: 5.2533290727296844e-05\n",
            "step: 480, loss: 0.0015076438430696726\n",
            "step: 490, loss: 0.0030888670589774847\n",
            "step: 500, loss: 0.022154338657855988\n",
            "step: 510, loss: 0.0015220558270812035\n",
            "step: 520, loss: 4.823501149076037e-05\n",
            "step: 530, loss: 0.0004016751772724092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.941342092914125, f1=0.9435185185185185, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018651513382792473\n",
            "step: 10, loss: 0.002561309840530157\n",
            "step: 20, loss: 0.002228444442152977\n",
            "step: 30, loss: 0.0020766945090144873\n",
            "step: 40, loss: 0.0007200376130640507\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.06811836361885071\n",
            "step: 60, loss: 0.0015930033987388015\n",
            "step: 70, loss: 0.00025940409977920353\n",
            "step: 80, loss: 0.0030882172286510468\n",
            "step: 90, loss: 0.010181318037211895\n",
            "step: 100, loss: 0.008751255460083485\n",
            "step: 110, loss: 6.350456533255056e-05\n",
            "step: 120, loss: 4.775252455146983e-05\n",
            "step: 130, loss: 0.0007037102477625012\n",
            "step: 140, loss: 3.293683039373718e-05\n",
            "step: 150, loss: 0.00013384883641265333\n",
            "step: 160, loss: 0.00014484536950476468\n",
            "step: 170, loss: 0.00019241841800976545\n",
            "step: 180, loss: 0.00011892079783137888\n",
            "step: 190, loss: 0.0003741414111573249\n",
            "step: 200, loss: 8.839865768095478e-05\n",
            "step: 210, loss: 4.194476787233725e-05\n",
            "step: 220, loss: 0.000989115796983242\n",
            "step: 230, loss: 8.287120726890862e-05\n",
            "step: 240, loss: 0.0006556081352755427\n",
            "step: 250, loss: 0.059531498700380325\n",
            "step: 260, loss: 0.00027602355112321675\n",
            "step: 270, loss: 0.0001556927163619548\n",
            "step: 280, loss: 0.0009453105740249157\n",
            "step: 290, loss: 0.0003642928204499185\n",
            "step: 300, loss: 0.0009771184995770454\n",
            "step: 310, loss: 0.00020755089644808322\n",
            "step: 320, loss: 0.0009433940867893398\n",
            "step: 330, loss: 0.0024071051739156246\n",
            "step: 340, loss: 0.00016813245019875467\n",
            "step: 350, loss: 0.001709298579953611\n",
            "step: 360, loss: 0.0016937711043283343\n",
            "step: 370, loss: 0.0029140703845769167\n",
            "step: 380, loss: 0.06329704076051712\n",
            "step: 390, loss: 0.0008988708723336458\n",
            "step: 400, loss: 0.0014907743316143751\n",
            "step: 410, loss: 0.0726521760225296\n",
            "step: 420, loss: 0.004923081956803799\n",
            "step: 430, loss: 0.003046793630346656\n",
            "step: 440, loss: 0.0009939811425283551\n",
            "step: 450, loss: 0.005933655425906181\n",
            "step: 460, loss: 0.00010019337787525728\n",
            "step: 470, loss: 6.983901403145865e-05\n",
            "step: 480, loss: 0.0012111928081139922\n",
            "step: 490, loss: 0.003750522155314684\n",
            "step: 500, loss: 0.004867436829954386\n",
            "step: 510, loss: 0.0006861689034849405\n",
            "step: 520, loss: 1.5537878425675444e-05\n",
            "step: 530, loss: 0.0013195336796343327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9396350023397285, f1=0.9405255878284925, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003838823875412345\n",
            "step: 10, loss: 7.176691724453121e-05\n",
            "step: 20, loss: 0.0008526674937456846\n",
            "step: 30, loss: 0.0015145298093557358\n",
            "step: 40, loss: 0.00026033478206954896\n",
            "step: 50, loss: 0.004323444329202175\n",
            "step: 60, loss: 2.9018587156315334e-05\n",
            "step: 70, loss: 9.625737584428862e-05\n",
            "step: 80, loss: 0.0012570281978696585\n",
            "step: 90, loss: 0.010454618372023106\n",
            "step: 100, loss: 0.0022058538161218166\n",
            "step: 110, loss: 0.0007901926292106509\n",
            "step: 120, loss: 0.0006491644307971001\n",
            "step: 130, loss: 6.45088657620363e-05\n",
            "step: 140, loss: 0.00021269066201057285\n",
            "step: 150, loss: 0.015918513759970665\n",
            "step: 160, loss: 4.58962022094056e-05\n",
            "step: 170, loss: 0.0017010237788781524\n",
            "step: 180, loss: 5.5436656111851335e-05\n",
            "step: 190, loss: 0.00018924707546830177\n",
            "step: 200, loss: 2.5167057174257934e-05\n",
            "step: 210, loss: 5.871818939340301e-05\n",
            "step: 220, loss: 0.005149298347532749\n",
            "step: 230, loss: 0.00011543519940460101\n",
            "step: 240, loss: 0.00020691142708528787\n",
            "step: 250, loss: 3.309111707494594e-05\n",
            "step: 260, loss: 0.00020381588547024876\n",
            "step: 270, loss: 3.564605503925122e-05\n",
            "step: 280, loss: 0.0001205094886245206\n",
            "step: 290, loss: 0.0003108508826699108\n",
            "step: 300, loss: 0.0006191869615577161\n",
            "step: 310, loss: 0.001213090610690415\n",
            "step: 320, loss: 0.002493141684681177\n",
            "step: 330, loss: 0.0012553863925859332\n",
            "step: 340, loss: 3.387543256394565e-05\n",
            "step: 350, loss: 6.274668703554198e-05\n",
            "step: 360, loss: 0.00013384345220401883\n",
            "step: 370, loss: 0.0003714935446623713\n",
            "step: 380, loss: 0.0007322326418943703\n",
            "step: 390, loss: 0.0019354979740455747\n",
            "step: 400, loss: 0.0007266262546181679\n",
            "step: 410, loss: 0.0007851922418922186\n",
            "step: 420, loss: 2.1117339201737195e-05\n",
            "step: 430, loss: 0.022443123161792755\n",
            "step: 440, loss: 3.0518811399815604e-05\n",
            "step: 450, loss: 6.369245966197923e-05\n",
            "step: 460, loss: 0.0011984427692368627\n",
            "step: 470, loss: 0.0006780810072086751\n",
            "step: 480, loss: 8.46593757160008e-05\n",
            "step: 490, loss: 0.0017250519013032317\n",
            "step: 500, loss: 0.0001926084514707327\n",
            "step: 510, loss: 0.0002704447542782873\n",
            "step: 520, loss: 0.00031163598760031164\n",
            "step: 530, loss: 0.00017574468802195042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9408450704225352, f1=0.937471051412691, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010421063052490354\n",
            "step: 10, loss: 4.4772201363230124e-05\n",
            "step: 20, loss: 2.967572618217673e-05\n",
            "step: 30, loss: 6.058184226276353e-05\n",
            "step: 40, loss: 0.004144955426454544\n",
            "step: 50, loss: 0.040789343416690826\n",
            "step: 60, loss: 1.0978355931001715e-05\n",
            "step: 70, loss: 0.0013647370506078005\n",
            "step: 80, loss: 1.766486093401909e-05\n",
            "step: 90, loss: 0.00036535211256705225\n",
            "step: 100, loss: 7.902994548203424e-05\n",
            "step: 110, loss: 0.0005997378029860556\n",
            "step: 120, loss: 8.052241173572838e-05\n",
            "step: 130, loss: 0.052251484245061874\n",
            "step: 140, loss: 0.000640196492895484\n",
            "step: 150, loss: 0.00021749800362158567\n",
            "step: 160, loss: 0.0006160784396342933\n",
            "step: 170, loss: 0.0008998675039038062\n",
            "step: 180, loss: 0.03009360283613205\n",
            "step: 190, loss: 6.108395609771833e-05\n",
            "step: 200, loss: 7.960617949720472e-05\n",
            "step: 210, loss: 0.0009546884102746844\n",
            "step: 220, loss: 2.8647044018725865e-05\n",
            "step: 230, loss: 0.018918389454483986\n",
            "step: 240, loss: 0.001386389252729714\n",
            "step: 250, loss: 2.2115973479230888e-05\n",
            "step: 260, loss: 0.0007237168028950691\n",
            "step: 270, loss: 0.00016302673611789942\n",
            "step: 280, loss: 1.4595488210034091e-05\n",
            "step: 290, loss: 0.03996060788631439\n",
            "step: 300, loss: 0.00017860262596514076\n",
            "step: 310, loss: 0.0002834591141436249\n",
            "step: 320, loss: 0.0008629246149212122\n",
            "step: 330, loss: 2.724100158957299e-05\n",
            "step: 340, loss: 0.0008674767450429499\n",
            "step: 350, loss: 0.0006546132499352098\n",
            "step: 360, loss: 0.0014091384364292026\n",
            "step: 370, loss: 0.0009226099355146289\n",
            "step: 380, loss: 0.0018964458722621202\n",
            "step: 390, loss: 0.004960492718964815\n",
            "step: 400, loss: 0.00025596650084480643\n",
            "step: 410, loss: 0.04681520164012909\n",
            "step: 420, loss: 0.000247212708927691\n",
            "step: 430, loss: 2.3613363737240434e-05\n",
            "step: 440, loss: 0.02239587903022766\n",
            "step: 450, loss: 0.0003202205407433212\n",
            "step: 460, loss: 0.000408169929869473\n",
            "step: 470, loss: 0.003463394707068801\n",
            "step: 480, loss: 0.00011280316539341584\n",
            "step: 490, loss: 0.00017435076006222516\n",
            "step: 500, loss: 0.004999030847102404\n",
            "step: 510, loss: 0.0031225045677274466\n",
            "step: 520, loss: 0.0009488591458648443\n",
            "step: 530, loss: 0.01908832974731922\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.941340782122905, f1=0.9395604395604396, best_f1=0.9392675011590171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2112088117864914e-05\n",
            "step: 10, loss: 0.0004121359088458121\n",
            "step: 20, loss: 9.569540998199955e-05\n",
            "step: 30, loss: 0.00011042547703254968\n",
            "step: 40, loss: 0.15181785821914673\n",
            "step: 50, loss: 0.00023662220337428153\n",
            "step: 60, loss: 1.7355798263452016e-05\n",
            "step: 70, loss: 0.008270013146102428\n",
            "step: 80, loss: 3.0166747819748707e-05\n",
            "step: 90, loss: 7.827100489521399e-05\n",
            "step: 100, loss: 1.7240277884411626e-05\n",
            "step: 110, loss: 0.0010906566167250276\n",
            "step: 120, loss: 0.0017048262525349855\n",
            "step: 130, loss: 0.1091959998011589\n",
            "step: 140, loss: 0.0021015533711761236\n",
            "step: 150, loss: 0.00011521351552801207\n",
            "step: 160, loss: 7.76361339376308e-05\n",
            "step: 170, loss: 0.0006015924154780805\n",
            "step: 180, loss: 0.00011590989015530795\n",
            "step: 190, loss: 2.935660995717626e-05\n",
            "step: 200, loss: 1.9743278244277462e-05\n",
            "step: 210, loss: 0.000628934009000659\n",
            "step: 220, loss: 4.122447353438474e-05\n",
            "step: 230, loss: 0.00019705195154529065\n",
            "step: 240, loss: 8.335785241797566e-05\n",
            "step: 250, loss: 2.4124225092236884e-05\n",
            "step: 260, loss: 1.4949244359740987e-05\n",
            "step: 270, loss: 0.00023426106781698763\n",
            "step: 280, loss: 2.0361354472697712e-05\n",
            "step: 290, loss: 1.0337615094613284e-05\n",
            "step: 300, loss: 1.1391835869289935e-05\n",
            "step: 310, loss: 0.00040664258995093405\n",
            "step: 320, loss: 0.00013583623513113707\n",
            "step: 330, loss: 0.0003584832011256367\n",
            "step: 340, loss: 5.1624159823404625e-05\n",
            "step: 350, loss: 4.455959060578607e-05\n",
            "step: 360, loss: 0.0012899921275675297\n",
            "step: 370, loss: 0.01417544949799776\n",
            "step: 380, loss: 2.33302271226421e-05\n",
            "step: 390, loss: 0.0001632719358894974\n",
            "step: 400, loss: 8.378003258258104e-05\n",
            "step: 410, loss: 8.31072247819975e-05\n",
            "step: 420, loss: 4.535147309070453e-05\n",
            "step: 430, loss: 7.582529360661283e-05\n",
            "step: 440, loss: 0.05618361011147499\n",
            "step: 450, loss: 0.0005280339391902089\n",
            "step: 460, loss: 0.002630854258313775\n",
            "step: 470, loss: 0.0015149656683206558\n",
            "step: 480, loss: 1.1280097169219516e-05\n",
            "step: 490, loss: 0.0013682199642062187\n",
            "step: 500, loss: 0.0020652993116527796\n",
            "step: 510, loss: 0.0002326446119695902\n",
            "step: 520, loss: 1.5169138350756839e-05\n",
            "step: 530, loss: 0.000658186327200383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9404096834264433, f1=0.9386446886446886, best_f1=0.9392675011590171\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 253.88it/s]\n",
            "load_f1 = 0.9421028253821214\n",
            "real_f1 = 0.9418874941887495\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced02b19-9c95-4b56-b5c4-620488955d01"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5628355145454407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.41379310344827586, f1=0.3373493975903615, best_f1=0.3373493975903615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4936589002609253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2666666666666667, f1=0.2666666666666667, best_f1=0.3373493975903615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5533716678619385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5128205128205129, f1=0.3508771929824562, best_f1=0.3508771929824562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3035537898540497\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6086956521739131, f1=0.47058823529411764, best_f1=0.47058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26104632019996643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6923076923076924, f1=0.5625000000000001, best_f1=0.5625000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.175650492310524\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6666666666666666, f1=0.64, best_f1=0.5625000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05452254042029381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7333333333333334, f1=0.5806451612903226, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008291578851640224\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.742857142857143, f1=0.6153846153846153, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006321432068943977\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6956521739130435, f1=0.5833333333333334, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004147296771407127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7692307692307692, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004414299037307501\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7692307692307692, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003699528519064188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7857142857142857, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003985329996794462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7857142857142857, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020675500854849815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7857142857142857, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003553619608283043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7857142857142857, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 89240.51it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7586206896551724\n",
            "real_f1 = 0.7586206896551724\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 196.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db79759-7e2c-4533-f634-36aac472e62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6287398338317871\n",
            "step: 10, loss: 0.6170055866241455\n",
            "step: 20, loss: 0.3245846927165985\n",
            "step: 30, loss: 0.07589269429445267\n",
            "step: 40, loss: 0.1162479966878891\n",
            "step: 50, loss: 0.011368243023753166\n",
            "step: 60, loss: 0.10020691156387329\n",
            "step: 70, loss: 0.025660181418061256\n",
            "step: 80, loss: 0.07038165628910065\n",
            "step: 90, loss: 0.0632835403084755\n",
            "step: 100, loss: 0.019798623397946358\n",
            "step: 110, loss: 0.04488567262887955\n",
            "step: 120, loss: 0.008560716174542904\n",
            "step: 130, loss: 0.0161663256585598\n",
            "step: 140, loss: 0.00861994456499815\n",
            "step: 150, loss: 0.016185395419597626\n",
            "step: 160, loss: 0.00800022017210722\n",
            "step: 170, loss: 0.0464814156293869\n",
            "step: 180, loss: 0.003243489423766732\n",
            "step: 190, loss: 0.04911535233259201\n",
            "step: 200, loss: 0.014451426453888416\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 210, loss: 0.004651886411011219\n",
            "step: 220, loss: 0.0020774726290255785\n",
            "step: 230, loss: 0.04399741813540459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9865470852017937, f1=0.9841986455981941, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004976768046617508\n",
            "step: 10, loss: 0.0036004697903990746\n",
            "step: 20, loss: 0.21437394618988037\n",
            "step: 30, loss: 0.07785701751708984\n",
            "step: 40, loss: 0.0890127569437027\n",
            "step: 50, loss: 0.0028668339364230633\n",
            "step: 60, loss: 0.0015551658580079675\n",
            "step: 70, loss: 0.1238347515463829\n",
            "step: 80, loss: 0.0029887952841818333\n",
            "step: 90, loss: 0.009945257566869259\n",
            "step: 100, loss: 0.006151179783046246\n",
            "step: 110, loss: 0.046558160334825516\n",
            "step: 120, loss: 0.06838656216859818\n",
            "step: 130, loss: 0.09017083048820496\n",
            "step: 140, loss: 0.03170182555913925\n",
            "step: 150, loss: 0.006148990243673325\n",
            "step: 160, loss: 0.08399397134780884\n",
            "step: 170, loss: 0.0021697520278394222\n",
            "step: 180, loss: 0.0056805978529155254\n",
            "step: 190, loss: 0.017505940049886703\n",
            "step: 200, loss: 0.0010932416189461946\n",
            "step: 210, loss: 0.00083641690434888\n",
            "step: 220, loss: 0.10053575783967972\n",
            "step: 230, loss: 0.005139052867889404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9831649831649831, f1=0.9852774631936579, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0060157207772135735\n",
            "step: 10, loss: 0.0013044532388448715\n",
            "step: 20, loss: 0.003027952741831541\n",
            "step: 30, loss: 0.08999254554510117\n",
            "step: 40, loss: 0.07831455022096634\n",
            "step: 50, loss: 0.013850501738488674\n",
            "step: 60, loss: 0.008035908453166485\n",
            "step: 70, loss: 0.007840143516659737\n",
            "step: 80, loss: 0.0008278308669105172\n",
            "step: 90, loss: 0.013708259910345078\n",
            "step: 100, loss: 0.0019228693563491106\n",
            "step: 110, loss: 0.0007019576733000576\n",
            "step: 120, loss: 0.0032853700686246157\n",
            "step: 130, loss: 0.0003793183423113078\n",
            "step: 140, loss: 0.004102237522602081\n",
            "step: 150, loss: 0.09882917255163193\n",
            "step: 160, loss: 0.0015930233057588339\n",
            "step: 170, loss: 0.009857628494501114\n",
            "step: 180, loss: 0.008142880164086819\n",
            "step: 190, loss: 0.0011559498962014914\n",
            "step: 200, loss: 0.004751351661980152\n",
            "step: 210, loss: 0.003968722186982632\n",
            "step: 220, loss: 0.0010248887119814754\n",
            "step: 230, loss: 0.0004605548456311226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9898989898989898, f1=0.9842342342342343, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027793904882855713\n",
            "step: 10, loss: 0.00023180263815447688\n",
            "step: 20, loss: 0.0005135654355399311\n",
            "step: 30, loss: 0.00017145216406788677\n",
            "step: 40, loss: 0.09357640892267227\n",
            "step: 50, loss: 0.010353020392358303\n",
            "step: 60, loss: 0.007035946473479271\n",
            "step: 70, loss: 0.0011747792595997453\n",
            "step: 80, loss: 0.0009744453709572554\n",
            "step: 90, loss: 0.0017496077343821526\n",
            "step: 100, loss: 0.0002995044633280486\n",
            "step: 110, loss: 0.00043126812670379877\n",
            "step: 120, loss: 0.016616234555840492\n",
            "step: 130, loss: 0.0020567025057971478\n",
            "step: 140, loss: 0.00029266325873322785\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 150, loss: 0.046635936945676804\n",
            "step: 160, loss: 0.00785272940993309\n",
            "step: 170, loss: 0.1170072928071022\n",
            "step: 180, loss: 0.0004029073170386255\n",
            "step: 190, loss: 0.007480394095182419\n",
            "step: 200, loss: 0.002417305950075388\n",
            "step: 210, loss: 0.01250892598181963\n",
            "step: 220, loss: 0.00020811246940866113\n",
            "step: 230, loss: 0.009228890761733055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9853768278965129, f1=0.9807474518686297, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007268866756930947\n",
            "step: 10, loss: 0.0007530837319791317\n",
            "step: 20, loss: 0.0006472731474786997\n",
            "step: 30, loss: 0.0004614882927853614\n",
            "step: 40, loss: 0.0003841333382297307\n",
            "step: 50, loss: 0.0647120401263237\n",
            "step: 60, loss: 0.00017193725216202438\n",
            "step: 70, loss: 0.03653615340590477\n",
            "step: 80, loss: 0.00032528044539503753\n",
            "step: 90, loss: 0.00048192418762482703\n",
            "step: 100, loss: 0.02485293708741665\n",
            "step: 110, loss: 0.0005810587317682803\n",
            "step: 120, loss: 0.00019713971414603293\n",
            "step: 130, loss: 0.0008560754940845072\n",
            "step: 140, loss: 0.000842004141304642\n",
            "step: 150, loss: 0.0004971435409970582\n",
            "step: 160, loss: 0.09385579824447632\n",
            "step: 170, loss: 0.012301388196647167\n",
            "step: 180, loss: 0.0009307120926678181\n",
            "step: 190, loss: 0.004864403512328863\n",
            "step: 200, loss: 0.0002498562098480761\n",
            "step: 210, loss: 0.00023424421669915318\n",
            "step: 220, loss: 0.0002525036979932338\n",
            "step: 230, loss: 0.00019514943414833397\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9887640449438202, f1=0.9831271091113611, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002540623245295137\n",
            "step: 10, loss: 0.00038269179640337825\n",
            "step: 20, loss: 0.00024246214888989925\n",
            "step: 30, loss: 0.0003222591185476631\n",
            "step: 40, loss: 0.000184161719516851\n",
            "step: 50, loss: 0.00012892948871012777\n",
            "step: 60, loss: 0.00011485537106636912\n",
            "step: 70, loss: 0.00012383621651679277\n",
            "step: 80, loss: 0.00047578688827343285\n",
            "step: 90, loss: 0.0004688352928496897\n",
            "step: 100, loss: 0.020366452634334564\n",
            "step: 110, loss: 0.0002126629406120628\n",
            "step: 120, loss: 0.00011176343832630664\n",
            "step: 130, loss: 9.03260734048672e-05\n",
            "step: 140, loss: 7.9751291195862e-05\n",
            "step: 150, loss: 0.09700535237789154\n",
            "step: 160, loss: 0.00028571143047884107\n",
            "step: 170, loss: 0.009959857910871506\n",
            "step: 180, loss: 0.041766855865716934\n",
            "step: 190, loss: 0.0021041659638285637\n",
            "step: 200, loss: 0.00022617937065660954\n",
            "step: 210, loss: 0.00020348653197288513\n",
            "step: 220, loss: 7.771200034767389e-05\n",
            "step: 230, loss: 0.03207036480307579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.984304932735426, f1=0.9797297297297298, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006778785609640181\n",
            "step: 10, loss: 0.0002031362528214231\n",
            "step: 20, loss: 0.00011597051343414932\n",
            "step: 30, loss: 0.0028276594821363688\n",
            "step: 40, loss: 0.00013672430941369385\n",
            "step: 50, loss: 7.39100287319161e-05\n",
            "step: 60, loss: 0.00029758643358945847\n",
            "step: 70, loss: 0.01473754271864891\n",
            "step: 80, loss: 0.00012831110507249832\n",
            "step: 90, loss: 4.2555468098726124e-05\n",
            "step: 100, loss: 5.957140092505142e-05\n",
            "step: 110, loss: 0.0007393266423605382\n",
            "step: 120, loss: 0.00013469260011333972\n",
            "step: 130, loss: 0.00015789766621310264\n",
            "step: 140, loss: 0.00014263093180488795\n",
            "step: 150, loss: 0.0006617932231165469\n",
            "step: 160, loss: 0.053634706884622574\n",
            "step: 170, loss: 0.03884851932525635\n",
            "step: 180, loss: 0.0005268572713248432\n",
            "step: 190, loss: 0.00048138879355974495\n",
            "step: 200, loss: 0.04993297904729843\n",
            "step: 210, loss: 8.958952821558341e-05\n",
            "step: 220, loss: 0.0017694317502900958\n",
            "step: 230, loss: 0.0001784090418368578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9843400447427293, f1=0.9809203142536477, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016301145660690963\n",
            "step: 10, loss: 0.028104795143008232\n",
            "step: 20, loss: 0.00013237232633400708\n",
            "step: 30, loss: 0.00017842378292698413\n",
            "step: 40, loss: 0.00030309276189655066\n",
            "step: 50, loss: 0.0016597185749560595\n",
            "step: 60, loss: 0.0007652775966562331\n",
            "step: 70, loss: 0.018299970775842667\n",
            "step: 80, loss: 0.00016522830992471427\n",
            "step: 90, loss: 6.65901752654463e-05\n",
            "step: 100, loss: 0.001293055946007371\n",
            "step: 110, loss: 0.0005824011168442667\n",
            "step: 120, loss: 0.0022679297253489494\n",
            "step: 130, loss: 0.00011209851072635502\n",
            "step: 140, loss: 6.716712232446298e-05\n",
            "step: 150, loss: 6.490988016594201e-05\n",
            "step: 160, loss: 0.000139689160278067\n",
            "step: 170, loss: 9.4929440820124e-05\n",
            "step: 180, loss: 8.602347952546552e-05\n",
            "step: 190, loss: 0.00012810496264137328\n",
            "step: 200, loss: 0.002335190074518323\n",
            "step: 210, loss: 0.00020737761224154383\n",
            "step: 220, loss: 0.006781162694096565\n",
            "step: 230, loss: 0.001352639519609511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9887640449438202, f1=0.9731543624161074, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005625911289826035\n",
            "step: 10, loss: 0.002048413036391139\n",
            "step: 20, loss: 0.019039738923311234\n",
            "step: 30, loss: 0.0013957041082903743\n",
            "step: 40, loss: 0.011436576955020428\n",
            "step: 50, loss: 0.005139630287885666\n",
            "step: 60, loss: 0.0004189035389572382\n",
            "step: 70, loss: 0.0021480401046574116\n",
            "step: 80, loss: 0.07313874363899231\n",
            "step: 90, loss: 0.0016744454624131322\n",
            "step: 100, loss: 0.0013194566126912832\n",
            "step: 110, loss: 0.002491159364581108\n",
            "step: 120, loss: 0.0005371205043047667\n",
            "step: 130, loss: 0.00023937804508022964\n",
            "step: 140, loss: 0.00031308241887018085\n",
            "step: 150, loss: 0.017006302252411842\n",
            "step: 160, loss: 0.0017808381235226989\n",
            "step: 170, loss: 0.000497817003633827\n",
            "step: 180, loss: 0.11254528164863586\n",
            "step: 190, loss: 0.00042249110992997885\n",
            "step: 200, loss: 0.0013124685501679778\n",
            "step: 210, loss: 0.0005225093918852508\n",
            "step: 220, loss: 0.0005029041203670204\n",
            "step: 230, loss: 0.0010997481876984239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9864253393665158, f1=0.9796839729119639, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027670388226397336\n",
            "step: 10, loss: 0.0003649517602752894\n",
            "step: 20, loss: 0.0012668658746406436\n",
            "step: 30, loss: 0.027336524799466133\n",
            "step: 40, loss: 0.0002416484639979899\n",
            "step: 50, loss: 0.0002682362392079085\n",
            "step: 60, loss: 0.00024955644039437175\n",
            "step: 70, loss: 0.0004589023010339588\n",
            "step: 80, loss: 0.0002726046950556338\n",
            "step: 90, loss: 0.002595443045720458\n",
            "step: 100, loss: 0.0007615590584464371\n",
            "step: 110, loss: 0.0008871562895365059\n",
            "step: 120, loss: 0.0004344666958786547\n",
            "step: 130, loss: 0.0005722732748836279\n",
            "step: 140, loss: 0.02947629615664482\n",
            "step: 150, loss: 0.025189999490976334\n",
            "step: 160, loss: 8.624249312561005e-05\n",
            "step: 170, loss: 0.0005181374726817012\n",
            "step: 180, loss: 0.0029053385369479656\n",
            "step: 190, loss: 0.0009017442935146391\n",
            "step: 200, loss: 0.0001269693748326972\n",
            "step: 210, loss: 0.0017826813273131847\n",
            "step: 220, loss: 0.00010783578909467906\n",
            "step: 230, loss: 0.0014702263288199902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9852440408626559, f1=0.9786276715410572, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013213992351666093\n",
            "step: 10, loss: 0.00012290573795326054\n",
            "step: 20, loss: 0.0440414622426033\n",
            "step: 30, loss: 0.029000360518693924\n",
            "step: 40, loss: 0.0005324496887624264\n",
            "step: 50, loss: 0.006254593376070261\n",
            "step: 60, loss: 0.0003503359039314091\n",
            "step: 70, loss: 0.006486298982053995\n",
            "step: 80, loss: 0.00012735207565128803\n",
            "step: 90, loss: 0.0002431704051559791\n",
            "step: 100, loss: 0.00019366161723155528\n",
            "step: 110, loss: 0.0028442975599318743\n",
            "step: 120, loss: 0.00010441905760671943\n",
            "step: 130, loss: 0.00011996964894933626\n",
            "step: 140, loss: 0.0002332036820007488\n",
            "step: 150, loss: 0.03108302131295204\n",
            "step: 160, loss: 0.00010920778731815517\n",
            "step: 170, loss: 0.018422279506921768\n",
            "step: 180, loss: 0.00017074489733204246\n",
            "step: 190, loss: 0.04522450268268585\n",
            "step: 200, loss: 0.0003096498257946223\n",
            "step: 210, loss: 0.0002040144318016246\n",
            "step: 220, loss: 0.00010233134526060894\n",
            "step: 230, loss: 0.00034873143886215985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9853438556933484, f1=0.9798206278026906, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.95289083220996e-05\n",
            "step: 10, loss: 0.00049233203753829\n",
            "step: 20, loss: 9.797926031751558e-05\n",
            "step: 30, loss: 0.00016467797104269266\n",
            "step: 40, loss: 0.0003930600068997592\n",
            "step: 50, loss: 0.0003104038769379258\n",
            "step: 60, loss: 0.000537690008059144\n",
            "step: 70, loss: 0.0050271134823560715\n",
            "step: 80, loss: 0.0010425058426335454\n",
            "step: 90, loss: 8.31836587167345e-05\n",
            "step: 100, loss: 0.00011904873827006668\n",
            "step: 110, loss: 0.0008104121661745012\n",
            "step: 120, loss: 7.467936666216701e-05\n",
            "step: 130, loss: 7.316585106309503e-05\n",
            "step: 140, loss: 0.00010692621435737237\n",
            "step: 150, loss: 0.00040067333611659706\n",
            "step: 160, loss: 0.0002325134410057217\n",
            "step: 170, loss: 0.00014761646161787212\n",
            "step: 180, loss: 0.0002637183351907879\n",
            "step: 190, loss: 0.00131084187887609\n",
            "step: 200, loss: 4.426861050887965e-05\n",
            "step: 210, loss: 0.00030098308343440294\n",
            "step: 220, loss: 7.451918645529076e-05\n",
            "step: 230, loss: 0.027549976482987404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9841986455981941, f1=0.9798657718120806, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018843045108951628\n",
            "step: 10, loss: 0.00079743517562747\n",
            "step: 20, loss: 0.00012810455518774688\n",
            "step: 30, loss: 8.807966514723375e-05\n",
            "step: 40, loss: 0.0001115100021706894\n",
            "step: 50, loss: 0.00011959655967075378\n",
            "step: 60, loss: 7.170601020334288e-05\n",
            "step: 70, loss: 0.00035616260720416903\n",
            "step: 80, loss: 0.00018921254377346486\n",
            "step: 90, loss: 0.00015147638623602688\n",
            "step: 100, loss: 4.397440352477133e-05\n",
            "step: 110, loss: 0.0028856617864221334\n",
            "step: 120, loss: 0.02672000601887703\n",
            "step: 130, loss: 0.0002210071252193302\n",
            "step: 140, loss: 0.00011574963718885556\n",
            "step: 150, loss: 9.029218927025795e-05\n",
            "step: 160, loss: 6.65154802845791e-05\n",
            "step: 170, loss: 6.615507300011814e-05\n",
            "step: 180, loss: 0.00012994790449738503\n",
            "step: 190, loss: 5.248394882073626e-05\n",
            "step: 200, loss: 0.0002034520439337939\n",
            "step: 210, loss: 5.085672819404863e-05\n",
            "step: 220, loss: 0.00013070525892544538\n",
            "step: 230, loss: 7.189460302470252e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9852440408626559, f1=0.9796380090497738, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.17993059172295e-05\n",
            "step: 10, loss: 0.015811767429113388\n",
            "step: 20, loss: 5.853818584000692e-05\n",
            "step: 30, loss: 5.521334242075682e-05\n",
            "step: 40, loss: 0.024112462997436523\n",
            "step: 50, loss: 9.452247468288988e-05\n",
            "step: 60, loss: 8.230312232626602e-05\n",
            "step: 70, loss: 5.510869232239202e-05\n",
            "step: 80, loss: 6.434617534978315e-05\n",
            "step: 90, loss: 6.507922807941213e-05\n",
            "step: 100, loss: 5.2892246458213776e-05\n",
            "step: 110, loss: 0.005251242779195309\n",
            "step: 120, loss: 4.0309212636202574e-05\n",
            "step: 130, loss: 5.554458766710013e-05\n",
            "step: 140, loss: 7.345463382080197e-05\n",
            "step: 150, loss: 5.0727790949167684e-05\n",
            "step: 160, loss: 0.00024357317306566983\n",
            "step: 170, loss: 9.631291322875768e-05\n",
            "step: 180, loss: 4.197122325422242e-05\n",
            "step: 190, loss: 0.00020417780615389347\n",
            "step: 200, loss: 6.554660649271682e-05\n",
            "step: 210, loss: 6.112117989687249e-05\n",
            "step: 220, loss: 3.856646435451694e-05\n",
            "step: 230, loss: 0.0005535394884645939\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9853438556933484, f1=0.9775784753363228, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.495546429301612e-05\n",
            "step: 10, loss: 3.414861566852778e-05\n",
            "step: 20, loss: 7.955086039146408e-05\n",
            "step: 30, loss: 8.730233093956485e-05\n",
            "step: 40, loss: 9.404934826306999e-05\n",
            "step: 50, loss: 0.03799119591712952\n",
            "step: 60, loss: 6.342565029626712e-05\n",
            "step: 70, loss: 9.075671550817788e-05\n",
            "step: 80, loss: 9.851194772636518e-05\n",
            "step: 90, loss: 0.0001315141562372446\n",
            "step: 100, loss: 8.053216151893139e-05\n",
            "step: 110, loss: 6.432658119592816e-05\n",
            "step: 120, loss: 8.120117854559794e-05\n",
            "step: 130, loss: 4.846740193897858e-05\n",
            "step: 140, loss: 5.542780490941368e-05\n",
            "step: 150, loss: 0.00020255890558473766\n",
            "step: 160, loss: 6.386195309460163e-05\n",
            "step: 170, loss: 3.507982910377905e-05\n",
            "step: 180, loss: 0.00016891348059289157\n",
            "step: 190, loss: 0.00015525927301496267\n",
            "step: 200, loss: 6.165200466057286e-05\n",
            "step: 210, loss: 9.058507566805929e-05\n",
            "step: 220, loss: 0.0002461929398123175\n",
            "step: 230, loss: 4.64630575152114e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9853107344632768, f1=0.9774774774774775, best_f1=0.9842342342342343\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 160.17it/s]\n",
            "load_f1 = 0.9898989898989898\n",
            "real_f1 = 0.9898989898989898\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 183.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29c3fa1-0f3f-4eb2-d977-0ced52bfc118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6264797449111938\n",
            "step: 10, loss: 0.5796636939048767\n",
            "step: 20, loss: 0.5105891227722168\n",
            "step: 30, loss: 0.04149889573454857\n",
            "step: 40, loss: 0.18932755291461945\n",
            "step: 50, loss: 0.25733035802841187\n",
            "step: 60, loss: 0.06434642523527145\n",
            "step: 70, loss: 0.15270158648490906\n",
            "step: 80, loss: 0.02858687750995159\n",
            "step: 90, loss: 0.2736411988735199\n",
            "step: 100, loss: 0.09923329204320908\n",
            "step: 110, loss: 0.15016210079193115\n",
            "step: 120, loss: 0.14926376938819885\n",
            "step: 130, loss: 0.12824930250644684\n",
            "step: 140, loss: 0.08006158471107483\n",
            "step: 150, loss: 0.10085911303758621\n",
            "step: 160, loss: 0.021000036969780922\n",
            "step: 170, loss: 0.20480872690677643\n",
            "step: 180, loss: 0.09711119532585144\n",
            "step: 190, loss: 0.026989564299583435\n",
            "step: 200, loss: 0.16218023002147675\n",
            "step: 210, loss: 0.0882437527179718\n",
            "step: 220, loss: 0.14681333303451538\n",
            "step: 230, loss: 0.16993378102779388\n",
            "step: 240, loss: 0.05940072238445282\n",
            "step: 250, loss: 0.05520491674542427\n",
            "step: 260, loss: 0.1410529613494873\n",
            "step: 270, loss: 0.012315092608332634\n",
            "step: 280, loss: 0.04700390249490738\n",
            "step: 290, loss: 0.04785050079226494\n",
            "step: 300, loss: 0.05958467721939087\n",
            "step: 310, loss: 0.09795168787240982\n",
            "step: 320, loss: 0.09017205983400345\n",
            "step: 330, loss: 0.14253640174865723\n",
            "step: 340, loss: 0.02440459653735161\n",
            "step: 350, loss: 0.04501941055059433\n",
            "step: 360, loss: 0.015924930572509766\n",
            "step: 370, loss: 0.04987553134560585\n",
            "step: 380, loss: 0.028937209397554398\n",
            "step: 390, loss: 0.09460382163524628\n",
            "step: 400, loss: 0.4052278399467468\n",
            "step: 410, loss: 0.0835861936211586\n",
            "step: 420, loss: 0.09184549748897552\n",
            "step: 430, loss: 0.20632974803447723\n",
            "step: 440, loss: 0.05220947787165642\n",
            "step: 450, loss: 0.007643949240446091\n",
            "step: 460, loss: 0.0045568812638521194\n",
            "step: 470, loss: 0.12635540962219238\n",
            "step: 480, loss: 0.03533947467803955\n",
            "step: 490, loss: 0.05013275146484375\n",
            "step: 500, loss: 0.14723052084445953\n",
            "step: 510, loss: 0.12740114331245422\n",
            "step: 520, loss: 0.0183267742395401\n",
            "step: 530, loss: 0.0026379062328487635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9313047487321346, f1=0.928801102434543, best_f1=0.928801102434543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4282241761684418\n",
            "step: 10, loss: 0.14382141828536987\n",
            "step: 20, loss: 0.010002495720982552\n",
            "step: 30, loss: 0.027746351435780525\n",
            "step: 40, loss: 0.08094671368598938\n",
            "step: 50, loss: 0.10785053670406342\n",
            "step: 60, loss: 0.007966406643390656\n",
            "step: 70, loss: 0.006841112859547138\n",
            "step: 80, loss: 0.06009824201464653\n",
            "step: 90, loss: 0.023692838847637177\n",
            "step: 100, loss: 0.010499331168830395\n",
            "step: 110, loss: 0.04325389117002487\n",
            "step: 120, loss: 0.02892235666513443\n",
            "step: 130, loss: 0.08882779628038406\n",
            "step: 140, loss: 0.01987864263355732\n",
            "step: 150, loss: 0.05871433764696121\n",
            "step: 160, loss: 0.036236248910427094\n",
            "step: 170, loss: 0.039043132215738297\n",
            "step: 180, loss: 0.019424809142947197\n",
            "step: 190, loss: 0.06440184265375137\n",
            "step: 200, loss: 0.008500497788190842\n",
            "step: 210, loss: 0.04987800121307373\n",
            "step: 220, loss: 0.06631910055875778\n",
            "step: 230, loss: 0.005671640858054161\n",
            "step: 240, loss: 0.11282604187726974\n",
            "step: 250, loss: 0.13542701303958893\n",
            "step: 260, loss: 0.00848821084946394\n",
            "step: 270, loss: 0.2919505834579468\n",
            "step: 280, loss: 0.04637765511870384\n",
            "step: 290, loss: 0.04090355709195137\n",
            "step: 300, loss: 0.13528423011302948\n",
            "step: 310, loss: 0.015960611402988434\n",
            "step: 320, loss: 0.06502464413642883\n",
            "step: 330, loss: 0.00960633996874094\n",
            "step: 340, loss: 0.009605173021554947\n",
            "step: 350, loss: 0.0028264345601201057\n",
            "step: 360, loss: 0.07856465131044388\n",
            "step: 370, loss: 0.14473913609981537\n",
            "step: 380, loss: 0.08355088531970978\n",
            "step: 390, loss: 0.1286901980638504\n",
            "step: 400, loss: 0.08981747925281525\n",
            "step: 410, loss: 0.046405769884586334\n",
            "step: 420, loss: 0.024850983172655106\n",
            "step: 430, loss: 0.023894062265753746\n",
            "step: 440, loss: 0.17610956728458405\n",
            "step: 450, loss: 0.021763669326901436\n",
            "step: 460, loss: 0.09260276705026627\n",
            "step: 470, loss: 0.05711446329951286\n",
            "step: 480, loss: 0.2606547474861145\n",
            "step: 490, loss: 0.021673955023288727\n",
            "step: 500, loss: 0.2507379353046417\n",
            "step: 510, loss: 0.022866982966661453\n",
            "step: 520, loss: 0.023683272302150726\n",
            "step: 530, loss: 0.05701705068349838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9359513791491351, f1=0.9408476944573824, best_f1=0.9408476944573824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02378014102578163\n",
            "step: 10, loss: 0.046503495424985886\n",
            "step: 20, loss: 0.08083922415971756\n",
            "step: 30, loss: 0.034228891134262085\n",
            "step: 40, loss: 0.02027580700814724\n",
            "step: 50, loss: 0.13076484203338623\n",
            "step: 60, loss: 0.010594965890049934\n",
            "step: 70, loss: 0.0045076641254127026\n",
            "step: 80, loss: 0.001964293885976076\n",
            "step: 90, loss: 0.008689631707966328\n",
            "step: 100, loss: 0.07555422931909561\n",
            "step: 110, loss: 0.010058310814201832\n",
            "step: 120, loss: 0.002464551944285631\n",
            "step: 130, loss: 0.0032214648090302944\n",
            "step: 140, loss: 0.01893952488899231\n",
            "step: 150, loss: 0.009173804894089699\n",
            "step: 160, loss: 0.00424785353243351\n",
            "step: 170, loss: 0.08977067470550537\n",
            "step: 180, loss: 0.0061074523255229\n",
            "step: 190, loss: 0.010221621952950954\n",
            "step: 200, loss: 0.09062105417251587\n",
            "step: 210, loss: 0.04276618734002113\n",
            "step: 220, loss: 0.16883179545402527\n",
            "step: 230, loss: 0.04446353390812874\n",
            "step: 240, loss: 0.0021331205498427153\n",
            "step: 250, loss: 0.07015679776668549\n",
            "step: 260, loss: 0.013573980890214443\n",
            "step: 270, loss: 0.031260911375284195\n",
            "step: 280, loss: 0.14855830371379852\n",
            "step: 290, loss: 0.0040300870314240456\n",
            "step: 300, loss: 0.019531184807419777\n",
            "step: 310, loss: 0.016894502565264702\n",
            "step: 320, loss: 0.013825053349137306\n",
            "step: 330, loss: 0.0005851167370565236\n",
            "step: 340, loss: 0.033035408705472946\n",
            "step: 350, loss: 0.05793813616037369\n",
            "step: 360, loss: 0.06407755613327026\n",
            "step: 370, loss: 0.012588584795594215\n",
            "step: 380, loss: 0.006076066754758358\n",
            "step: 390, loss: 0.02180168405175209\n",
            "step: 400, loss: 0.024262098595499992\n",
            "step: 410, loss: 0.0047386265359818935\n",
            "step: 420, loss: 0.16090525686740875\n",
            "step: 430, loss: 0.03688650205731392\n",
            "step: 440, loss: 0.0004332697717472911\n",
            "step: 450, loss: 0.08810176700353622\n",
            "step: 460, loss: 0.02592308819293976\n",
            "step: 470, loss: 0.014568261802196503\n",
            "step: 480, loss: 0.008333717472851276\n",
            "step: 490, loss: 0.02139556035399437\n",
            "step: 500, loss: 0.04548261687159538\n",
            "step: 510, loss: 0.061884474009275436\n",
            "step: 520, loss: 0.07810647785663605\n",
            "step: 530, loss: 0.028316980227828026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.937528921795465, f1=0.9397590361445785, best_f1=0.9397590361445785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00614946149289608\n",
            "step: 10, loss: 0.0017209402285516262\n",
            "step: 20, loss: 0.00041377736488357186\n",
            "step: 30, loss: 0.0012282916577532887\n",
            "step: 40, loss: 0.002112924586981535\n",
            "step: 50, loss: 0.012344255112111568\n",
            "step: 60, loss: 0.001160252490080893\n",
            "step: 70, loss: 0.0012835027882829309\n",
            "step: 80, loss: 0.0016844646306708455\n",
            "step: 90, loss: 0.031342510133981705\n",
            "step: 100, loss: 0.03053983859717846\n",
            "step: 110, loss: 0.08120924979448318\n",
            "step: 120, loss: 0.0023935867939144373\n",
            "step: 130, loss: 0.0295831561088562\n",
            "step: 140, loss: 0.0009356781374663115\n",
            "step: 150, loss: 0.0035690967924892902\n",
            "step: 160, loss: 0.09106329828500748\n",
            "step: 170, loss: 0.004344150889664888\n",
            "step: 180, loss: 0.001020793686620891\n",
            "step: 190, loss: 0.11776486039161682\n",
            "step: 200, loss: 0.03515485301613808\n",
            "step: 210, loss: 0.04543352499604225\n",
            "step: 220, loss: 0.06910441070795059\n",
            "step: 230, loss: 0.09622053802013397\n",
            "step: 240, loss: 0.006698907818645239\n",
            "step: 250, loss: 0.07311729341745377\n",
            "step: 260, loss: 0.0017759207403287292\n",
            "step: 270, loss: 0.008103259839117527\n",
            "step: 280, loss: 0.014604317024350166\n",
            "step: 290, loss: 0.022977864369750023\n",
            "step: 300, loss: 0.002766540041193366\n",
            "step: 310, loss: 0.008854048326611519\n",
            "step: 320, loss: 0.014939923770725727\n",
            "step: 330, loss: 0.0038089780136942863\n",
            "step: 340, loss: 0.05705561488866806\n",
            "step: 350, loss: 0.10726699978113174\n",
            "step: 360, loss: 0.01814541593194008\n",
            "step: 370, loss: 0.012864870950579643\n",
            "step: 380, loss: 0.008292542770504951\n",
            "step: 390, loss: 0.002932235598564148\n",
            "step: 400, loss: 0.01629350334405899\n",
            "step: 410, loss: 0.008692096918821335\n",
            "step: 420, loss: 0.003908325918018818\n",
            "step: 430, loss: 0.23547011613845825\n",
            "step: 440, loss: 0.007704154122620821\n",
            "step: 450, loss: 0.0035476358607411385\n",
            "step: 460, loss: 0.0010420707985758781\n",
            "step: 470, loss: 0.02036360278725624\n",
            "step: 480, loss: 0.12194480001926422\n",
            "step: 490, loss: 0.001477784593589604\n",
            "step: 500, loss: 0.0015963183250278234\n",
            "step: 510, loss: 0.01896330714225769\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 520, loss: 0.1109549030661583\n",
            "step: 530, loss: 0.02305159904062748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9397260273972603, f1=0.9410689812699863, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01907278597354889\n",
            "step: 10, loss: 0.04253368824720383\n",
            "step: 20, loss: 0.09553717821836472\n",
            "step: 30, loss: 0.003820653771981597\n",
            "step: 40, loss: 0.00491533987224102\n",
            "step: 50, loss: 0.0028913491405546665\n",
            "step: 60, loss: 0.009862327948212624\n",
            "step: 70, loss: 0.004498199559748173\n",
            "step: 80, loss: 0.000882316380739212\n",
            "step: 90, loss: 0.0006767820450477302\n",
            "step: 100, loss: 0.0022152455057948828\n",
            "step: 110, loss: 0.00041573005728423595\n",
            "step: 120, loss: 0.0008791537838988006\n",
            "step: 130, loss: 0.0004181790864095092\n",
            "step: 140, loss: 0.0021528967190533876\n",
            "step: 150, loss: 0.008331084623932838\n",
            "step: 160, loss: 0.0006459890864789486\n",
            "step: 170, loss: 0.009071671403944492\n",
            "step: 180, loss: 0.000791162543464452\n",
            "step: 190, loss: 0.013969296589493752\n",
            "step: 200, loss: 0.0019563643727451563\n",
            "step: 210, loss: 0.0032917477656155825\n",
            "step: 220, loss: 0.006920026149600744\n",
            "step: 230, loss: 0.027726629748940468\n",
            "step: 240, loss: 0.01274082437157631\n",
            "step: 250, loss: 0.0008840272785164416\n",
            "step: 260, loss: 0.0006265446427278221\n",
            "step: 270, loss: 0.0005174357793293893\n",
            "step: 280, loss: 0.0006103711202740669\n",
            "step: 290, loss: 0.1965404450893402\n",
            "step: 300, loss: 0.0041307322680950165\n",
            "step: 310, loss: 0.0007117776549421251\n",
            "step: 320, loss: 0.01594020053744316\n",
            "step: 330, loss: 0.018343940377235413\n",
            "step: 340, loss: 0.003085664939135313\n",
            "step: 350, loss: 0.005248062312602997\n",
            "step: 360, loss: 0.0051747700199484825\n",
            "step: 370, loss: 0.0019047128735110164\n",
            "step: 380, loss: 0.0002966096217278391\n",
            "step: 390, loss: 0.0003929976955987513\n",
            "step: 400, loss: 0.004959827288985252\n",
            "step: 410, loss: 0.0009485890041105449\n",
            "step: 420, loss: 0.001862755510956049\n",
            "step: 430, loss: 0.0029569033067673445\n",
            "step: 440, loss: 0.03864328935742378\n",
            "step: 450, loss: 0.016541199758648872\n",
            "step: 460, loss: 0.000713221961632371\n",
            "step: 470, loss: 0.0014660024316981435\n",
            "step: 480, loss: 0.0016137680504471064\n",
            "step: 490, loss: 0.0003380277776159346\n",
            "step: 500, loss: 0.00046172604197636247\n",
            "step: 510, loss: 0.1424761414527893\n",
            "step: 520, loss: 0.0021376044023782015\n",
            "step: 530, loss: 0.0030798083171248436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9287749287749287, f1=0.9359095193213949, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00214552553370595\n",
            "step: 10, loss: 0.0048523335717618465\n",
            "step: 20, loss: 0.13947059214115143\n",
            "step: 30, loss: 0.006380051840096712\n",
            "step: 40, loss: 0.0005257982993498445\n",
            "step: 50, loss: 0.016026217490434647\n",
            "step: 60, loss: 0.00018302095122635365\n",
            "step: 70, loss: 0.006065624300390482\n",
            "step: 80, loss: 0.002164103789255023\n",
            "step: 90, loss: 0.003387796925380826\n",
            "step: 100, loss: 0.006712055299431086\n",
            "step: 110, loss: 0.012701311148703098\n",
            "step: 120, loss: 0.016804374754428864\n",
            "step: 130, loss: 0.0026263538748025894\n",
            "step: 140, loss: 0.001533582923002541\n",
            "step: 150, loss: 0.04747790843248367\n",
            "step: 160, loss: 0.009344718419015408\n",
            "step: 170, loss: 0.00033521177829243243\n",
            "step: 180, loss: 0.00040663316030986607\n",
            "step: 190, loss: 0.00024688037228770554\n",
            "step: 200, loss: 0.0012907484779134393\n",
            "step: 210, loss: 0.0025356044061481953\n",
            "step: 220, loss: 0.00992236752063036\n",
            "step: 230, loss: 0.004517430905252695\n",
            "step: 240, loss: 0.059908222407102585\n",
            "step: 250, loss: 0.0034580484498292208\n",
            "step: 260, loss: 0.0001600418472662568\n",
            "step: 270, loss: 0.10763762146234512\n",
            "step: 280, loss: 0.0648399218916893\n",
            "step: 290, loss: 0.0014793338486924767\n",
            "step: 300, loss: 0.0039058434776961803\n",
            "step: 310, loss: 0.008669165894389153\n",
            "step: 320, loss: 0.0025271649938076735\n",
            "step: 330, loss: 0.0016444443026557565\n",
            "step: 340, loss: 0.0778818279504776\n",
            "step: 350, loss: 0.003396779065951705\n",
            "step: 360, loss: 0.00896213948726654\n",
            "step: 370, loss: 0.003362886141985655\n",
            "step: 380, loss: 0.00037587888073176146\n",
            "step: 390, loss: 0.004522319417446852\n",
            "step: 400, loss: 0.0022676601074635983\n",
            "step: 410, loss: 0.002865451155230403\n",
            "step: 420, loss: 0.01624351181089878\n",
            "step: 430, loss: 0.0007310957298614085\n",
            "step: 440, loss: 0.0007669473416171968\n",
            "step: 450, loss: 0.0030823221895843744\n",
            "step: 460, loss: 0.002088285516947508\n",
            "step: 470, loss: 0.0024005777668207884\n",
            "step: 480, loss: 0.012545347213745117\n",
            "step: 490, loss: 0.013417490758001804\n",
            "step: 500, loss: 0.0004900619387626648\n",
            "step: 510, loss: 0.0011298370081931353\n",
            "step: 520, loss: 0.0002888762974180281\n",
            "step: 530, loss: 0.012786309234797955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9365225390984362, f1=0.942035600182565, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010592280159471557\n",
            "step: 10, loss: 0.0003942172625102103\n",
            "step: 20, loss: 0.00017595798999536783\n",
            "step: 30, loss: 0.0019488041289150715\n",
            "step: 40, loss: 0.03136494755744934\n",
            "step: 50, loss: 0.0007972888997755945\n",
            "step: 60, loss: 0.0002916343801189214\n",
            "step: 70, loss: 0.011471526697278023\n",
            "step: 80, loss: 0.0033972011879086494\n",
            "step: 90, loss: 0.02118891291320324\n",
            "step: 100, loss: 0.07018017768859863\n",
            "step: 110, loss: 0.0009830074850469828\n",
            "step: 120, loss: 0.0006048679351806641\n",
            "step: 130, loss: 0.0006809807382524014\n",
            "step: 140, loss: 0.002399051794782281\n",
            "step: 150, loss: 0.00953192263841629\n",
            "step: 160, loss: 0.004716468974947929\n",
            "step: 170, loss: 0.007118814159184694\n",
            "step: 180, loss: 0.008843835443258286\n",
            "step: 190, loss: 0.001547761494293809\n",
            "step: 200, loss: 0.00022133854508865625\n",
            "step: 210, loss: 0.000692821922712028\n",
            "step: 220, loss: 0.0002506775490473956\n",
            "step: 230, loss: 0.007374669425189495\n",
            "step: 240, loss: 0.0009982057381421328\n",
            "step: 250, loss: 0.00040318904211744666\n",
            "step: 260, loss: 0.0009453139500692487\n",
            "step: 270, loss: 0.0025809959042817354\n",
            "step: 280, loss: 0.0003245171101298183\n",
            "step: 290, loss: 0.0001663397706579417\n",
            "step: 300, loss: 0.0006600231863558292\n",
            "step: 310, loss: 0.0001073975145118311\n",
            "step: 320, loss: 0.0004972700262442231\n",
            "step: 330, loss: 0.002019620267674327\n",
            "step: 340, loss: 0.02532227151095867\n",
            "step: 350, loss: 0.022606132552027702\n",
            "step: 360, loss: 0.001743991975672543\n",
            "step: 370, loss: 0.002322485903277993\n",
            "step: 380, loss: 0.002574289683252573\n",
            "step: 390, loss: 0.0011184157337993383\n",
            "step: 400, loss: 0.01023595966398716\n",
            "step: 410, loss: 0.005021970719099045\n",
            "step: 420, loss: 0.0004558605724014342\n",
            "step: 430, loss: 9.53617854975164e-05\n",
            "step: 440, loss: 0.00016743148444220424\n",
            "step: 450, loss: 0.00376032292842865\n",
            "step: 460, loss: 0.0001346164644928649\n",
            "step: 480, loss: 0.09236464649438858\n",
            "step: 490, loss: 0.002606153255328536\n",
            "step: 500, loss: 0.00022337239352054894\n",
            "step: 510, loss: 0.0021445313468575478\n",
            "step: 520, loss: 0.02200537733733654\n",
            "step: 530, loss: 0.002923469291999936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9379374708352777, f1=0.9413936317489617, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006544138304889202\n",
            "step: 10, loss: 0.03570239618420601\n",
            "step: 20, loss: 0.011199328117072582\n",
            "step: 30, loss: 0.000630350667051971\n",
            "step: 40, loss: 0.001198118319734931\n",
            "step: 50, loss: 0.0020419645588845015\n",
            "step: 60, loss: 0.0011268719099462032\n",
            "step: 70, loss: 0.00033631210681051016\n",
            "step: 80, loss: 0.001407264149747789\n",
            "step: 90, loss: 0.0005983322625979781\n",
            "step: 100, loss: 0.00018652663857210428\n",
            "step: 110, loss: 0.00015885315951891243\n",
            "step: 120, loss: 0.0002535762614570558\n",
            "step: 130, loss: 0.0003223646490368992\n",
            "step: 140, loss: 0.002309644129127264\n",
            "step: 150, loss: 0.00014131856733001769\n",
            "step: 160, loss: 0.05816630274057388\n",
            "step: 170, loss: 0.0009059247095137835\n",
            "step: 180, loss: 0.0003275318304076791\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.002807553159072995\n",
            "step: 200, loss: 0.001613837550394237\n",
            "step: 210, loss: 0.0001613945933058858\n",
            "step: 220, loss: 0.0027942396700382233\n",
            "step: 230, loss: 0.00028579641366377473\n",
            "step: 240, loss: 8.408905705437064e-05\n",
            "step: 250, loss: 6.832656072219834e-05\n",
            "step: 260, loss: 7.598514639539644e-05\n",
            "step: 270, loss: 0.0005232898984104395\n",
            "step: 280, loss: 0.02706129476428032\n",
            "step: 290, loss: 0.00021219828340690583\n",
            "step: 300, loss: 0.0007233505020849407\n",
            "step: 310, loss: 0.00032776768784970045\n",
            "step: 320, loss: 0.023361265659332275\n",
            "step: 330, loss: 0.0002612602838780731\n",
            "step: 340, loss: 0.0004179870302323252\n",
            "step: 350, loss: 0.00021445209858939052\n",
            "step: 360, loss: 0.00498993881046772\n",
            "step: 370, loss: 0.00075528584420681\n",
            "step: 380, loss: 0.0009459700086154044\n",
            "step: 390, loss: 0.06990984827280045\n",
            "step: 400, loss: 0.028905479237437248\n",
            "step: 410, loss: 0.00022102822549641132\n",
            "step: 420, loss: 0.0009520823950879276\n",
            "step: 430, loss: 0.05106959864497185\n",
            "step: 440, loss: 0.0003161929489579052\n",
            "step: 450, loss: 0.00031809776555746794\n",
            "step: 460, loss: 0.00023858096392359585\n",
            "step: 470, loss: 0.00015901026199571788\n",
            "step: 480, loss: 0.00012786251318175346\n",
            "step: 490, loss: 0.060140907764434814\n",
            "step: 500, loss: 0.004495264496654272\n",
            "step: 510, loss: 0.002480795606970787\n",
            "step: 520, loss: 0.001965508097782731\n",
            "step: 530, loss: 0.0008595475228503346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.931892907468295, f1=0.9392111368909513, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002621621359139681\n",
            "step: 10, loss: 0.00034448778023943305\n",
            "step: 20, loss: 0.0013008207315579057\n",
            "step: 30, loss: 0.0005037090741097927\n",
            "step: 40, loss: 0.0003825408057309687\n",
            "step: 50, loss: 0.00020575021335389465\n",
            "step: 60, loss: 8.228506339946762e-05\n",
            "step: 70, loss: 0.006074386648833752\n",
            "step: 80, loss: 0.0003010049695149064\n",
            "step: 90, loss: 0.0014812470180913806\n",
            "step: 100, loss: 0.0001330364029854536\n",
            "step: 110, loss: 0.00012709795555565506\n",
            "step: 120, loss: 0.00040220029768534005\n",
            "step: 130, loss: 0.0003073179686907679\n",
            "step: 140, loss: 0.003208366222679615\n",
            "step: 150, loss: 0.00012111983960494399\n",
            "step: 160, loss: 0.0011198900174349546\n",
            "step: 170, loss: 0.15147292613983154\n",
            "step: 180, loss: 8.067591988947242e-05\n",
            "step: 190, loss: 0.0014342796057462692\n",
            "step: 200, loss: 0.0001023646182147786\n",
            "step: 210, loss: 8.264795906143263e-05\n",
            "step: 220, loss: 0.0001750914379954338\n",
            "step: 230, loss: 8.742330101085827e-05\n",
            "step: 240, loss: 0.06059933453798294\n",
            "step: 250, loss: 0.0010072910226881504\n",
            "step: 260, loss: 0.015440357849001884\n",
            "step: 270, loss: 8.565836469642818e-05\n",
            "step: 280, loss: 0.010838417336344719\n",
            "step: 290, loss: 0.04183483496308327\n",
            "step: 300, loss: 0.0003360615810379386\n",
            "step: 310, loss: 0.026542847976088524\n",
            "step: 320, loss: 0.000332935422193259\n",
            "step: 330, loss: 0.0001863948709797114\n",
            "step: 340, loss: 7.432234269799665e-05\n",
            "step: 350, loss: 0.0005768080009147525\n",
            "step: 360, loss: 0.00010703269799705595\n",
            "step: 370, loss: 0.03989681601524353\n",
            "step: 380, loss: 0.002762393793091178\n",
            "step: 390, loss: 0.00023019984655547887\n",
            "step: 400, loss: 0.0010311175137758255\n",
            "step: 410, loss: 0.00036334936157800257\n",
            "step: 420, loss: 5.990951831336133e-05\n",
            "step: 430, loss: 0.00075879180803895\n",
            "step: 440, loss: 0.0018122639739885926\n",
            "step: 450, loss: 0.0003798298130277544\n",
            "step: 460, loss: 0.00011830039147753268\n",
            "step: 470, loss: 5.137227344675921e-05\n",
            "step: 480, loss: 0.00015292508760467172\n",
            "step: 490, loss: 0.00047658407129347324\n",
            "step: 500, loss: 0.0020326185040175915\n",
            "step: 510, loss: 0.0004310854710638523\n",
            "step: 520, loss: 0.000301951018627733\n",
            "step: 530, loss: 0.0001993140031117946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9379441023211748, f1=0.9418386491557222, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032473502214998007\n",
            "step: 10, loss: 0.00014485811698250473\n",
            "step: 20, loss: 0.0001415980514138937\n",
            "step: 30, loss: 0.0001185989603982307\n",
            "step: 40, loss: 0.0012327530421316624\n",
            "step: 50, loss: 0.0007192183984443545\n",
            "step: 60, loss: 0.0008512124186381698\n",
            "step: 70, loss: 0.00039599102456122637\n",
            "step: 80, loss: 0.00018608941172715276\n",
            "step: 90, loss: 0.00024975574342533946\n",
            "step: 100, loss: 0.00026034272741526365\n",
            "step: 110, loss: 7.487408583983779e-05\n",
            "step: 120, loss: 0.00022992298181634396\n",
            "step: 130, loss: 0.00021792470943182707\n",
            "step: 140, loss: 0.006843366660177708\n",
            "step: 150, loss: 0.0005162103334441781\n",
            "step: 160, loss: 0.0001935295294970274\n",
            "step: 170, loss: 0.00034934416180476546\n",
            "step: 180, loss: 0.0002976059913635254\n",
            "step: 190, loss: 0.0002785835531540215\n",
            "step: 200, loss: 0.00021653337171301246\n",
            "step: 210, loss: 0.000312607066007331\n",
            "step: 220, loss: 0.0037895238492637873\n",
            "step: 230, loss: 0.0018189733382314444\n",
            "step: 240, loss: 0.0011559005361050367\n",
            "step: 250, loss: 0.001450743991881609\n",
            "step: 260, loss: 0.00027515151305124164\n",
            "step: 270, loss: 9.190602577291429e-05\n",
            "step: 280, loss: 7.284348248504102e-05\n",
            "step: 290, loss: 6.440307333832607e-05\n",
            "step: 300, loss: 3.564564758562483e-05\n",
            "step: 310, loss: 8.963185973698273e-05\n",
            "step: 320, loss: 0.0009282948449254036\n",
            "step: 330, loss: 0.00012139086902607232\n",
            "step: 340, loss: 0.06529013812541962\n",
            "step: 350, loss: 0.0001456641184631735\n",
            "step: 360, loss: 0.0022608910221606493\n",
            "step: 370, loss: 0.0036493302322924137\n",
            "step: 380, loss: 0.00013203790877014399\n",
            "step: 390, loss: 0.0005289770197123289\n",
            "step: 400, loss: 0.00018573971465229988\n",
            "step: 410, loss: 0.0020394327584654093\n",
            "step: 420, loss: 0.0001427287788828835\n",
            "step: 430, loss: 9.759997192304581e-05\n",
            "step: 440, loss: 0.002281413646414876\n",
            "step: 450, loss: 0.00022541036014445126\n",
            "step: 460, loss: 5.874583075637929e-05\n",
            "step: 470, loss: 0.00039489357732236385\n",
            "step: 480, loss: 0.002415654482319951\n",
            "step: 490, loss: 0.00036814436316490173\n",
            "step: 500, loss: 0.0014431847957894206\n",
            "step: 510, loss: 0.00011544768494786695\n",
            "step: 520, loss: 9.207518451148644e-05\n",
            "step: 530, loss: 0.0009262213134206831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9394785847299814, f1=0.9414498141263941, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018910960061475635\n",
            "step: 10, loss: 0.0003156732418574393\n",
            "step: 20, loss: 0.00014389576972462237\n",
            "step: 30, loss: 0.0002286114322487265\n",
            "step: 40, loss: 8.464180427836254e-05\n",
            "step: 50, loss: 0.00039221704355441034\n",
            "step: 60, loss: 0.04141397774219513\n",
            "step: 70, loss: 5.900092583033256e-05\n",
            "step: 80, loss: 0.0003111511468887329\n",
            "step: 90, loss: 0.00020307950035203248\n",
            "step: 100, loss: 0.009157117456197739\n",
            "step: 110, loss: 0.00011330701818224043\n",
            "step: 120, loss: 0.0020323689095675945\n",
            "step: 130, loss: 0.0002132020308636129\n",
            "step: 140, loss: 5.785674147773534e-05\n",
            "step: 150, loss: 4.770676969201304e-05\n",
            "step: 160, loss: 5.8411835198057815e-05\n",
            "step: 170, loss: 0.00011827093840111047\n",
            "step: 180, loss: 6.235302134882659e-05\n",
            "step: 190, loss: 0.0013423513155430555\n",
            "step: 200, loss: 0.011949460953474045\n",
            "step: 210, loss: 0.00010002048657042906\n",
            "step: 220, loss: 0.00117143162060529\n",
            "step: 230, loss: 2.9834320230293088e-05\n",
            "step: 240, loss: 0.00014997024845797569\n",
            "step: 250, loss: 3.911961903213523e-05\n",
            "step: 260, loss: 0.0002173220709664747\n",
            "step: 270, loss: 6.0748217947548255e-05\n",
            "step: 280, loss: 7.697546971030533e-05\n",
            "step: 290, loss: 0.0004456517635844648\n",
            "step: 300, loss: 0.0003244552353862673\n",
            "step: 310, loss: 9.787515591597185e-05\n",
            "step: 320, loss: 0.00020449032308533788\n",
            "step: 330, loss: 0.0004955333424732089\n",
            "step: 340, loss: 0.00017262162873521447\n",
            "step: 350, loss: 0.00020422872330527753\n",
            "step: 360, loss: 5.900464384467341e-05\n",
            "step: 370, loss: 0.00022661659750156105\n",
            "step: 380, loss: 0.0007324364269152284\n",
            "step: 390, loss: 7.481126522179693e-05\n",
            "step: 400, loss: 3.168187686242163e-05\n",
            "step: 410, loss: 0.00010167995060328394\n",
            "step: 420, loss: 0.0001257256808457896\n",
            "step: 430, loss: 5.199280712986365e-05\n",
            "step: 440, loss: 8.173583773896098e-05\n",
            "step: 450, loss: 5.6416360166622326e-05\n",
            "step: 460, loss: 9.309010056313127e-05\n",
            "step: 470, loss: 0.00010676413512555882\n",
            "step: 480, loss: 0.0002705891092773527\n",
            "step: 490, loss: 6.688167195534334e-05\n",
            "step: 500, loss: 0.00014865271805319935\n",
            "step: 510, loss: 0.00013280462007969618\n",
            "step: 520, loss: 7.150095188990235e-05\n",
            "step: 530, loss: 0.0004098078643437475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9278937381404174, f1=0.9312617702448212, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.715287549421191e-05\n",
            "step: 10, loss: 9.434478124603629e-05\n",
            "step: 20, loss: 0.09378344565629959\n",
            "step: 30, loss: 0.001240090117789805\n",
            "step: 40, loss: 0.00022810790687799454\n",
            "step: 50, loss: 0.04052349179983139\n",
            "step: 60, loss: 0.0006504969205707312\n",
            "step: 70, loss: 0.0011054264614358544\n",
            "step: 80, loss: 6.21793296886608e-05\n",
            "step: 90, loss: 8.900385000742972e-05\n",
            "step: 100, loss: 0.0020414297468960285\n",
            "step: 110, loss: 9.400849376106635e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 3.076145731029101e-05\n",
            "step: 130, loss: 0.0011663156328722835\n",
            "step: 140, loss: 0.0015459296992048621\n",
            "step: 150, loss: 5.841030724695884e-05\n",
            "step: 160, loss: 3.74474584532436e-05\n",
            "step: 170, loss: 3.513900810503401e-05\n",
            "step: 180, loss: 5.392428647610359e-05\n",
            "step: 190, loss: 0.00017825327813625336\n",
            "step: 200, loss: 9.552339179208502e-05\n",
            "step: 210, loss: 4.0376635297434404e-05\n",
            "step: 220, loss: 0.00011826519039459527\n",
            "step: 230, loss: 5.377246634452604e-05\n",
            "step: 240, loss: 0.0009292103932239115\n",
            "step: 250, loss: 0.00014564207231160253\n",
            "step: 260, loss: 0.00017955036310013384\n",
            "step: 270, loss: 0.00034404045436531305\n",
            "step: 280, loss: 0.001932558254338801\n",
            "step: 290, loss: 0.003300898941233754\n",
            "step: 300, loss: 0.0006401835125871003\n",
            "step: 310, loss: 9.682479139883071e-05\n",
            "step: 320, loss: 4.1883620724547654e-05\n",
            "step: 330, loss: 0.00249828421510756\n",
            "step: 340, loss: 3.94671515095979e-05\n",
            "step: 350, loss: 0.00038728577783331275\n",
            "step: 360, loss: 0.0005422018002718687\n",
            "step: 370, loss: 0.0007345196790993214\n",
            "step: 380, loss: 0.00014298023597802967\n",
            "step: 390, loss: 4.03165613533929e-05\n",
            "step: 400, loss: 5.6421828048769385e-05\n",
            "step: 410, loss: 0.021121995523571968\n",
            "step: 420, loss: 0.0031543681398034096\n",
            "step: 430, loss: 0.016106072813272476\n",
            "step: 440, loss: 0.0003172872238792479\n",
            "step: 450, loss: 0.0016204708954319358\n",
            "step: 460, loss: 0.00024159395252354443\n",
            "step: 470, loss: 5.733309080824256e-05\n",
            "step: 480, loss: 0.0015614326111972332\n",
            "step: 490, loss: 0.023828906938433647\n",
            "step: 500, loss: 0.000101591031125281\n",
            "step: 510, loss: 0.00015912618255242705\n",
            "step: 520, loss: 5.20667563250754e-05\n",
            "step: 530, loss: 0.00030906108440831304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9373246024321795, f1=0.9408476944573824, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003078039037063718\n",
            "step: 10, loss: 3.407307303859852e-05\n",
            "step: 20, loss: 0.00017728732200339437\n",
            "step: 30, loss: 0.0009323506383225322\n",
            "step: 40, loss: 8.444044942734763e-05\n",
            "step: 50, loss: 0.0008697164594195783\n",
            "step: 60, loss: 8.397047349717468e-05\n",
            "step: 70, loss: 3.005436883540824e-05\n",
            "step: 80, loss: 7.207145972643048e-05\n",
            "step: 90, loss: 0.022530220448970795\n",
            "step: 100, loss: 0.003009763080626726\n",
            "step: 110, loss: 5.350053106667474e-05\n",
            "step: 120, loss: 0.00023589988995809108\n",
            "step: 130, loss: 2.6378123948234133e-05\n",
            "step: 140, loss: 0.00016709214833099395\n",
            "step: 150, loss: 0.0012213286245241761\n",
            "step: 160, loss: 2.4727851268835366e-05\n",
            "step: 170, loss: 0.0001020770869217813\n",
            "step: 180, loss: 0.0043305400758981705\n",
            "step: 190, loss: 7.227167225209996e-05\n",
            "step: 200, loss: 5.4147316404851153e-05\n",
            "step: 210, loss: 0.00020873192988801748\n",
            "step: 220, loss: 0.005083456635475159\n",
            "step: 230, loss: 0.005153352860361338\n",
            "step: 240, loss: 0.039471469819545746\n",
            "step: 250, loss: 6.836559623479843e-05\n",
            "step: 260, loss: 5.5396514653693885e-05\n",
            "step: 270, loss: 9.463949390919879e-05\n",
            "step: 280, loss: 5.116142347105779e-05\n",
            "step: 290, loss: 2.3077423975337297e-05\n",
            "step: 300, loss: 5.28731252416037e-05\n",
            "step: 310, loss: 0.0010482431389391422\n",
            "step: 320, loss: 0.0007026380626484752\n",
            "step: 330, loss: 0.0012632766738533974\n",
            "step: 340, loss: 2.4805816792650148e-05\n",
            "step: 350, loss: 4.843399074161425e-05\n",
            "step: 360, loss: 2.826304807967972e-05\n",
            "step: 370, loss: 0.00015595200238749385\n",
            "step: 380, loss: 0.012285543605685234\n",
            "step: 390, loss: 0.00020630517974495888\n",
            "step: 400, loss: 2.1512874809559435e-05\n",
            "step: 410, loss: 0.00026635723770596087\n",
            "step: 420, loss: 2.582292836450506e-05\n",
            "step: 430, loss: 0.002717449329793453\n",
            "step: 440, loss: 9.85519218374975e-05\n",
            "step: 450, loss: 9.240825602319092e-05\n",
            "step: 460, loss: 0.0005987995536997914\n",
            "step: 470, loss: 2.691622285055928e-05\n",
            "step: 480, loss: 0.010129563510417938\n",
            "step: 490, loss: 2.1747742721345276e-05\n",
            "step: 500, loss: 2.2905449441168457e-05\n",
            "step: 510, loss: 7.289491622941568e-05\n",
            "step: 520, loss: 0.0014766944805160165\n",
            "step: 530, loss: 2.4917353584896773e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9397031539888682, f1=0.9402023919043238, best_f1=0.9410689812699863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010338669380871579\n",
            "step: 10, loss: 2.694545401027426e-05\n",
            "step: 20, loss: 1.785129279596731e-05\n",
            "step: 30, loss: 6.066533751436509e-05\n",
            "step: 40, loss: 6.770540494471788e-05\n",
            "step: 50, loss: 0.0001593145716469735\n",
            "step: 60, loss: 6.450906221289188e-05\n",
            "step: 70, loss: 8.423496183240786e-05\n",
            "step: 80, loss: 0.0053588226437568665\n",
            "step: 90, loss: 2.543929440435022e-05\n",
            "step: 100, loss: 0.000984475016593933\n",
            "step: 110, loss: 7.792904216330498e-05\n",
            "step: 120, loss: 6.933875556569546e-05\n",
            "step: 130, loss: 0.026273835450410843\n",
            "step: 140, loss: 4.8128244088729843e-05\n",
            "step: 150, loss: 0.00010321208537789062\n",
            "step: 160, loss: 9.808482718653977e-05\n",
            "step: 170, loss: 0.000292964861728251\n",
            "step: 180, loss: 3.975301297032274e-05\n",
            "step: 190, loss: 8.363943197764456e-05\n",
            "step: 200, loss: 0.00016214251809287816\n",
            "step: 210, loss: 3.003520760103129e-05\n",
            "step: 220, loss: 3.9170554373413324e-05\n",
            "step: 230, loss: 9.090275125345215e-05\n",
            "step: 240, loss: 4.515424006967805e-05\n",
            "step: 250, loss: 3.202108928235248e-05\n",
            "step: 260, loss: 3.8929068978177384e-05\n",
            "step: 270, loss: 1.8045009710476734e-05\n",
            "step: 280, loss: 2.0566771127050743e-05\n",
            "step: 290, loss: 0.0013858515303581953\n",
            "step: 300, loss: 4.397188240545802e-05\n",
            "step: 310, loss: 0.00010562088573351502\n",
            "step: 320, loss: 8.67858761921525e-05\n",
            "step: 330, loss: 7.161818939493969e-05\n",
            "step: 340, loss: 3.5813183785649016e-05\n",
            "step: 350, loss: 0.00017018176731653512\n",
            "step: 360, loss: 5.9659098042175174e-05\n",
            "step: 370, loss: 3.645984543254599e-05\n",
            "step: 380, loss: 8.986005559563637e-05\n",
            "step: 390, loss: 0.022940412163734436\n",
            "step: 400, loss: 0.0002015558275161311\n",
            "step: 410, loss: 2.3230155420606025e-05\n",
            "step: 420, loss: 2.2570859073312022e-05\n",
            "step: 430, loss: 0.0005448806914500892\n",
            "step: 440, loss: 3.828256740234792e-05\n",
            "step: 450, loss: 4.355188139015809e-05\n",
            "step: 460, loss: 2.545786446717102e-05\n",
            "step: 470, loss: 2.7994121410301886e-05\n",
            "step: 480, loss: 1.819769386202097e-05\n",
            "step: 490, loss: 3.431227742112242e-05\n",
            "step: 500, loss: 1.639469337533228e-05\n",
            "step: 510, loss: 0.00039359761285595596\n",
            "step: 520, loss: 3.0430312108364888e-05\n",
            "step: 530, loss: 4.371789691504091e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9398704902867714, f1=0.9372423270728356, best_f1=0.9372423270728356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3725495339022018e-05\n",
            "step: 10, loss: 2.5211797037627548e-05\n",
            "step: 20, loss: 9.683192183729261e-05\n",
            "step: 30, loss: 4.4411252019926906e-05\n",
            "step: 40, loss: 0.03558916971087456\n",
            "step: 50, loss: 0.00024175795260816813\n",
            "step: 60, loss: 1.1928250387427397e-05\n",
            "step: 70, loss: 0.0008486738079227507\n",
            "step: 80, loss: 2.3293659978662618e-05\n",
            "step: 90, loss: 2.4075985493254848e-05\n",
            "step: 100, loss: 1.9132494344376028e-05\n",
            "step: 110, loss: 3.196854959242046e-05\n",
            "step: 120, loss: 9.033957758219913e-05\n",
            "step: 130, loss: 0.04347885400056839\n",
            "step: 140, loss: 4.221136623527855e-05\n",
            "step: 150, loss: 5.08632801938802e-05\n",
            "step: 160, loss: 2.294344267284032e-05\n",
            "step: 170, loss: 3.1206593121169135e-05\n",
            "step: 180, loss: 1.722912202239968e-05\n",
            "step: 190, loss: 3.62341343134176e-05\n",
            "step: 200, loss: 1.5858322512940504e-05\n",
            "step: 210, loss: 9.645606041885912e-05\n",
            "step: 220, loss: 2.7467971449368633e-05\n",
            "step: 230, loss: 6.133898568805307e-05\n",
            "step: 240, loss: 2.5252578780055046e-05\n",
            "step: 250, loss: 0.00015347017324529588\n",
            "step: 260, loss: 4.7264657041523606e-05\n",
            "step: 270, loss: 3.979129542130977e-05\n",
            "step: 280, loss: 3.071616811212152e-05\n",
            "step: 290, loss: 1.4319820365926716e-05\n",
            "step: 300, loss: 2.4082883101073094e-05\n",
            "step: 310, loss: 3.632597508840263e-05\n",
            "step: 320, loss: 9.934571426128969e-05\n",
            "step: 330, loss: 6.669331924058497e-05\n",
            "step: 340, loss: 2.533440783736296e-05\n",
            "step: 350, loss: 2.2461932530859485e-05\n",
            "step: 360, loss: 0.0025964719243347645\n",
            "step: 370, loss: 1.5347988664871082e-05\n",
            "step: 380, loss: 4.595115751726553e-05\n",
            "step: 390, loss: 0.000650746573228389\n",
            "step: 400, loss: 2.1267231204546988e-05\n",
            "step: 410, loss: 1.8428656403557397e-05\n",
            "step: 420, loss: 3.71324822481256e-05\n",
            "step: 430, loss: 0.0034011222887784243\n",
            "step: 440, loss: 3.238242425140925e-05\n",
            "step: 450, loss: 1.536660965939518e-05\n",
            "step: 460, loss: 1.992991383303888e-05\n",
            "step: 470, loss: 0.011226708069443703\n",
            "step: 480, loss: 1.4006892342877109e-05\n",
            "step: 490, loss: 1.709136995486915e-05\n",
            "step: 500, loss: 2.9607175747514702e-05\n",
            "step: 510, loss: 2.9300808819243684e-05\n",
            "step: 520, loss: 2.2168389477883466e-05\n",
            "step: 530, loss: 3.3960939617827535e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9396035039188566, f1=0.9342465753424657, best_f1=0.9372423270728356\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:30, 189.07it/s]\n",
            "load_f1 = 0.9390414146114472\n",
            "real_f1 = 0.9384902143522833\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3c23a2-eac9-4bc7-d73e-d71c4246a496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5498040914535522\n",
            "step: 10, loss: 0.37779876589775085\n",
            "step: 20, loss: 0.38229817152023315\n",
            "step: 30, loss: 0.3414197266101837\n",
            "step: 40, loss: 0.18779604136943817\n",
            "step: 50, loss: 0.41652584075927734\n",
            "step: 60, loss: 0.25878581404685974\n",
            "step: 70, loss: 0.1763940453529358\n",
            "step: 80, loss: 0.20959551632404327\n",
            "step: 90, loss: 0.35307425260543823\n",
            "step: 100, loss: 0.38716554641723633\n",
            "step: 110, loss: 0.23845350742340088\n",
            "step: 120, loss: 0.16744066774845123\n",
            "step: 130, loss: 0.2685664892196655\n",
            "step: 140, loss: 0.20945361256599426\n",
            "step: 150, loss: 0.145716592669487\n",
            "step: 160, loss: 0.26183387637138367\n",
            "step: 170, loss: 0.28116127848625183\n",
            "step: 180, loss: 0.12734976410865784\n",
            "step: 190, loss: 0.17719824612140656\n",
            "step: 200, loss: 0.313344806432724\n",
            "step: 210, loss: 0.26320314407348633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6591760299625468, f1=0.6483300589390962, best_f1=0.6483300589390962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10982842743396759\n",
            "step: 10, loss: 0.13342329859733582\n",
            "step: 20, loss: 0.14701780676841736\n",
            "step: 30, loss: 0.13193215429782867\n",
            "step: 40, loss: 0.11845387518405914\n",
            "step: 50, loss: 0.181062251329422\n",
            "step: 60, loss: 0.3943815231323242\n",
            "step: 70, loss: 0.10819822549819946\n",
            "step: 80, loss: 0.1971900910139084\n",
            "step: 90, loss: 0.10545416921377182\n",
            "step: 100, loss: 0.01892581395804882\n",
            "step: 110, loss: 0.05569609999656677\n",
            "step: 120, loss: 0.12285459041595459\n",
            "step: 130, loss: 0.014573303051292896\n",
            "step: 140, loss: 0.1706826090812683\n",
            "step: 150, loss: 0.16561293601989746\n",
            "step: 160, loss: 0.21735629439353943\n",
            "step: 170, loss: 0.11218240857124329\n",
            "step: 180, loss: 0.1317686289548874\n",
            "step: 190, loss: 0.13911177217960358\n",
            "step: 200, loss: 0.03660142049193382\n",
            "step: 210, loss: 0.10887462645769119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7011952191235059, f1=0.6680672268907564, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051715247333049774\n",
            "step: 10, loss: 0.16549478471279144\n",
            "step: 20, loss: 0.16910479962825775\n",
            "step: 30, loss: 0.31983110308647156\n",
            "step: 40, loss: 0.10510765761137009\n",
            "step: 50, loss: 0.08895600587129593\n",
            "step: 60, loss: 0.17321735620498657\n",
            "step: 70, loss: 0.04715650901198387\n",
            "step: 80, loss: 0.10729236900806427\n",
            "step: 90, loss: 0.04088016599416733\n",
            "step: 100, loss: 0.17520888149738312\n",
            "step: 110, loss: 0.15838564932346344\n",
            "step: 120, loss: 0.07819822430610657\n",
            "step: 130, loss: 0.135456845164299\n",
            "step: 140, loss: 0.10626200586557388\n",
            "step: 150, loss: 0.24412310123443604\n",
            "step: 160, loss: 0.009628402069211006\n",
            "step: 170, loss: 0.15552474558353424\n",
            "step: 180, loss: 0.09027610719203949\n",
            "step: 190, loss: 0.23999488353729248\n",
            "step: 200, loss: 0.036798544228076935\n",
            "step: 210, loss: 0.11245505511760712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6772908366533865, f1=0.654690618762475, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06713583320379257\n",
            "step: 10, loss: 0.075599804520607\n",
            "step: 20, loss: 0.021679142490029335\n",
            "step: 30, loss: 0.1861591339111328\n",
            "step: 40, loss: 0.024266330525279045\n",
            "step: 50, loss: 0.06476384401321411\n",
            "step: 60, loss: 0.05434846132993698\n",
            "step: 70, loss: 0.2342473566532135\n",
            "step: 80, loss: 0.15543851256370544\n",
            "step: 90, loss: 0.03498319536447525\n",
            "step: 100, loss: 0.2507036328315735\n",
            "step: 110, loss: 0.05115671455860138\n",
            "step: 120, loss: 0.1289844959974289\n",
            "step: 130, loss: 0.20006297528743744\n",
            "step: 140, loss: 0.06593553721904755\n",
            "step: 150, loss: 0.026915282011032104\n",
            "step: 160, loss: 0.023576881736516953\n",
            "step: 170, loss: 0.0850742906332016\n",
            "step: 180, loss: 0.3225179612636566\n",
            "step: 190, loss: 0.04941257834434509\n",
            "step: 200, loss: 0.2638801038265228\n",
            "step: 210, loss: 0.07387332618236542\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6846153846153845, f1=0.6614481409001958, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12477241456508636\n",
            "step: 10, loss: 0.013700373470783234\n",
            "step: 20, loss: 0.02192738465964794\n",
            "step: 30, loss: 0.026220425963401794\n",
            "step: 40, loss: 0.07253073155879974\n",
            "step: 50, loss: 0.022711487486958504\n",
            "step: 60, loss: 0.06492961943149567\n",
            "step: 70, loss: 0.03664252534508705\n",
            "step: 80, loss: 0.037848763167858124\n",
            "step: 90, loss: 0.04787055402994156\n",
            "step: 100, loss: 0.03652197867631912\n",
            "step: 110, loss: 0.23017889261245728\n",
            "step: 120, loss: 0.1306942105293274\n",
            "step: 130, loss: 0.015473827719688416\n",
            "step: 140, loss: 0.011062911711633205\n",
            "step: 150, loss: 0.03377193957567215\n",
            "step: 160, loss: 0.03261227533221245\n",
            "step: 170, loss: 0.049793973565101624\n",
            "step: 180, loss: 0.06183294951915741\n",
            "step: 190, loss: 0.02959929034113884\n",
            "step: 200, loss: 0.061041805893182755\n",
            "step: 210, loss: 0.016397293657064438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6772908366533865, f1=0.6129032258064516, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02899480238556862\n",
            "step: 10, loss: 0.0462799146771431\n",
            "step: 20, loss: 0.041634686291217804\n",
            "step: 30, loss: 0.01650536246597767\n",
            "step: 40, loss: 0.0050338576547801495\n",
            "step: 50, loss: 0.005760878790169954\n",
            "step: 60, loss: 0.09755370020866394\n",
            "step: 70, loss: 0.005023146979510784\n",
            "step: 80, loss: 0.07714322954416275\n",
            "step: 90, loss: 0.1979682594537735\n",
            "step: 100, loss: 0.0052132271230220795\n",
            "step: 110, loss: 0.005486433859914541\n",
            "step: 120, loss: 0.006620378699153662\n",
            "step: 130, loss: 0.02381177991628647\n",
            "step: 140, loss: 0.04324512928724289\n",
            "step: 150, loss: 0.0040918258018791676\n",
            "step: 160, loss: 0.01001745369285345\n",
            "step: 170, loss: 0.14789998531341553\n",
            "step: 180, loss: 0.06096641346812248\n",
            "step: 190, loss: 0.05871545895934105\n",
            "step: 200, loss: 0.003648963291198015\n",
            "step: 210, loss: 0.030315808951854706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6800804828973844, f1=0.6252587991718427, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007322218734771013\n",
            "step: 10, loss: 0.014323972165584564\n",
            "step: 20, loss: 0.0006835148087702692\n",
            "step: 30, loss: 0.03851873055100441\n",
            "step: 40, loss: 0.026293210685253143\n",
            "step: 50, loss: 0.09103643149137497\n",
            "step: 60, loss: 0.0027006445452570915\n",
            "step: 70, loss: 0.007081540767103434\n",
            "step: 80, loss: 0.04168139770627022\n",
            "step: 90, loss: 0.05753052234649658\n",
            "step: 100, loss: 0.001297361683100462\n",
            "step: 110, loss: 0.11308598518371582\n",
            "step: 120, loss: 0.027504513040184975\n",
            "step: 130, loss: 0.010346801951527596\n",
            "step: 140, loss: 0.004820907022804022\n",
            "step: 150, loss: 0.014408527873456478\n",
            "step: 160, loss: 0.0029216280672699213\n",
            "step: 170, loss: 0.0004466455429792404\n",
            "step: 180, loss: 0.005313217639923096\n",
            "step: 190, loss: 0.05610351264476776\n",
            "step: 200, loss: 0.002115415409207344\n",
            "step: 210, loss: 0.06633972376585007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6680584551148224, f1=0.6033755274261604, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035432183649390936\n",
            "step: 10, loss: 0.018533386290073395\n",
            "step: 20, loss: 0.003791989991441369\n",
            "step: 30, loss: 0.013059343211352825\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.19765834510326385\n",
            "step: 50, loss: 0.0027853474020957947\n",
            "step: 60, loss: 0.05391194671392441\n",
            "step: 70, loss: 0.02606750652194023\n",
            "step: 80, loss: 0.027433903887867928\n",
            "step: 90, loss: 0.03242642804980278\n",
            "step: 100, loss: 0.18535497784614563\n",
            "step: 110, loss: 0.07539872080087662\n",
            "step: 120, loss: 0.0015699905343353748\n",
            "step: 130, loss: 0.001093888422474265\n",
            "step: 140, loss: 0.11796796321868896\n",
            "step: 150, loss: 0.0043112123385071754\n",
            "step: 160, loss: 0.006786564365029335\n",
            "step: 170, loss: 0.0022955809254199266\n",
            "step: 180, loss: 0.021662726998329163\n",
            "step: 190, loss: 0.006911387667059898\n",
            "step: 200, loss: 0.001327352598309517\n",
            "step: 210, loss: 0.16484519839286804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6968503937007874, f1=0.6153846153846154, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011962044052779675\n",
            "step: 10, loss: 0.004652380011975765\n",
            "step: 20, loss: 0.005024394486099482\n",
            "step: 30, loss: 0.03373245894908905\n",
            "step: 40, loss: 0.04232005402445793\n",
            "step: 50, loss: 0.09055998176336288\n",
            "step: 60, loss: 0.053642094135284424\n",
            "step: 70, loss: 0.0006138205644674599\n",
            "step: 80, loss: 0.002378253499045968\n",
            "step: 90, loss: 0.00047785439528524876\n",
            "step: 100, loss: 0.0004901181673631072\n",
            "step: 110, loss: 0.0051302118226885796\n",
            "step: 120, loss: 0.002896262798458338\n",
            "step: 130, loss: 0.009339659474790096\n",
            "step: 140, loss: 0.10493410378694534\n",
            "step: 150, loss: 0.05260748788714409\n",
            "step: 160, loss: 0.0021151998080313206\n",
            "step: 170, loss: 0.038595590740442276\n",
            "step: 180, loss: 0.0069406661204993725\n",
            "step: 190, loss: 0.000325158704072237\n",
            "step: 200, loss: 0.019184213131666183\n",
            "step: 210, loss: 0.0017459879163652658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6796116504854368, f1=0.6144814090019569, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03388218581676483\n",
            "step: 10, loss: 0.009261123836040497\n",
            "step: 20, loss: 0.00037815660471096635\n",
            "step: 30, loss: 0.016760891303420067\n",
            "step: 40, loss: 0.008384027518332005\n",
            "step: 50, loss: 0.01934649422764778\n",
            "step: 60, loss: 0.017432112246751785\n",
            "step: 70, loss: 0.031465794891119\n",
            "step: 80, loss: 0.07543405890464783\n",
            "step: 90, loss: 0.08959346264600754\n",
            "step: 100, loss: 0.01589546911418438\n",
            "step: 110, loss: 0.0004918609629385173\n",
            "step: 120, loss: 0.05518413707613945\n",
            "step: 130, loss: 0.0304089467972517\n",
            "step: 140, loss: 0.02626798301935196\n",
            "step: 150, loss: 0.03358534723520279\n",
            "step: 160, loss: 0.0024544766638427973\n",
            "step: 170, loss: 0.2520368695259094\n",
            "step: 180, loss: 0.09644661098718643\n",
            "step: 190, loss: 0.04739939793944359\n",
            "step: 200, loss: 0.008076646365225315\n",
            "step: 210, loss: 0.056075628846883774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6774847870182557, f1=0.6074380165289256, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030870752409100533\n",
            "step: 10, loss: 0.027170099318027496\n",
            "step: 20, loss: 0.043015483766794205\n",
            "step: 30, loss: 0.03936830535531044\n",
            "step: 40, loss: 0.0038599062245339155\n",
            "step: 50, loss: 0.048672426491975784\n",
            "step: 60, loss: 0.0253412164747715\n",
            "step: 70, loss: 0.0004627077141776681\n",
            "step: 80, loss: 0.0449187345802784\n",
            "step: 90, loss: 0.010423469357192516\n",
            "step: 100, loss: 0.025668328627943993\n",
            "step: 110, loss: 0.024129338562488556\n",
            "step: 120, loss: 0.03211308643221855\n",
            "step: 130, loss: 0.0059243240393698215\n",
            "step: 140, loss: 0.0015507632633671165\n",
            "step: 150, loss: 0.0004797623842023313\n",
            "step: 160, loss: 0.0015229217242449522\n",
            "step: 170, loss: 0.012950914911925793\n",
            "step: 180, loss: 0.006541495211422443\n",
            "step: 190, loss: 0.0005177112761884928\n",
            "step: 200, loss: 0.0006364524597302079\n",
            "step: 210, loss: 0.0004995209746994078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6734279918864097, f1=0.6050420168067228, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01787453331053257\n",
            "step: 10, loss: 0.000297430349746719\n",
            "step: 20, loss: 0.014543828554451466\n",
            "step: 30, loss: 0.0006560804322361946\n",
            "step: 40, loss: 0.010154885239899158\n",
            "step: 50, loss: 0.0003584440564736724\n",
            "step: 60, loss: 0.0005361092044040561\n",
            "step: 70, loss: 0.009763163514435291\n",
            "step: 80, loss: 0.08859609812498093\n",
            "step: 90, loss: 0.001710523501969874\n",
            "step: 100, loss: 0.0032056679483503103\n",
            "step: 110, loss: 0.0016232574125751853\n",
            "step: 120, loss: 0.0006485140766017139\n",
            "step: 130, loss: 0.000695940398145467\n",
            "step: 140, loss: 0.0002820062218233943\n",
            "step: 150, loss: 0.012451490387320518\n",
            "step: 160, loss: 0.0007124710828065872\n",
            "step: 170, loss: 0.002036537742242217\n",
            "step: 180, loss: 0.00021134468261152506\n",
            "step: 190, loss: 0.00033743135281838477\n",
            "step: 200, loss: 0.0005207629292272031\n",
            "step: 210, loss: 0.02219071239233017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6553911205073996, f1=0.6004319654427647, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027350934222340584\n",
            "step: 10, loss: 0.00015084302867762744\n",
            "step: 20, loss: 0.009778988547623158\n",
            "step: 30, loss: 0.009768380783498287\n",
            "step: 40, loss: 0.005846688523888588\n",
            "step: 50, loss: 0.012270929291844368\n",
            "step: 60, loss: 0.0002772006264422089\n",
            "step: 70, loss: 0.062045011669397354\n",
            "step: 80, loss: 0.0011698288144543767\n",
            "step: 90, loss: 0.0002144354220945388\n",
            "step: 100, loss: 0.000143104451126419\n",
            "step: 110, loss: 0.0003680475929286331\n",
            "step: 120, loss: 0.0009081756579689682\n",
            "step: 130, loss: 0.0014401610242202878\n",
            "step: 140, loss: 0.0013333751121535897\n",
            "step: 150, loss: 0.00026402398361824453\n",
            "step: 160, loss: 0.013614602386951447\n",
            "step: 170, loss: 0.0002712707791943103\n",
            "step: 180, loss: 0.04009220376610756\n",
            "step: 190, loss: 0.0002727325481828302\n",
            "step: 200, loss: 0.0002865654823835939\n",
            "step: 210, loss: 0.020165981724858284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6595289079229123, f1=0.6167400881057269, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.801443957258016e-05\n",
            "step: 10, loss: 0.021000105887651443\n",
            "step: 20, loss: 0.0002562413574196398\n",
            "step: 30, loss: 0.00011183074093423784\n",
            "step: 40, loss: 0.00024280213983729482\n",
            "step: 50, loss: 0.010557850822806358\n",
            "step: 60, loss: 0.00020792809664271772\n",
            "step: 70, loss: 0.00025280346744693816\n",
            "step: 80, loss: 0.0001090197401936166\n",
            "step: 90, loss: 0.00023681654420215636\n",
            "step: 100, loss: 0.006776096299290657\n",
            "step: 110, loss: 0.0003539700119290501\n",
            "step: 120, loss: 0.03638570010662079\n",
            "step: 130, loss: 0.00038108666194602847\n",
            "step: 140, loss: 0.0059177023358643055\n",
            "step: 150, loss: 0.00020290643442422152\n",
            "step: 160, loss: 0.0033830299507826567\n",
            "step: 170, loss: 0.00022869330132380128\n",
            "step: 180, loss: 0.0013140486553311348\n",
            "step: 190, loss: 0.00027867176686413586\n",
            "step: 200, loss: 0.0007061630021780729\n",
            "step: 210, loss: 0.00022713928774464875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6518847006651884, f1=0.6139954853273137, best_f1=0.6680672268907564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000126939412439242\n",
            "step: 10, loss: 0.0003683440154418349\n",
            "step: 20, loss: 0.0005817097262479365\n",
            "step: 30, loss: 0.02152130752801895\n",
            "step: 40, loss: 0.0007277707918547094\n",
            "step: 50, loss: 9.797293751034886e-05\n",
            "step: 60, loss: 0.001901643117889762\n",
            "step: 70, loss: 0.00031225773273035884\n",
            "step: 80, loss: 9.062259050551802e-05\n",
            "step: 90, loss: 0.0006278220098465681\n",
            "step: 100, loss: 0.00018454385281074792\n",
            "step: 110, loss: 0.00010881990601774305\n",
            "step: 120, loss: 0.0001273860689252615\n",
            "step: 130, loss: 0.00020170514471828938\n",
            "step: 140, loss: 0.00013003252388443798\n",
            "step: 150, loss: 7.917888433439657e-05\n",
            "step: 160, loss: 0.0039586457423865795\n",
            "step: 170, loss: 0.00011863602412631735\n",
            "step: 180, loss: 0.0001021783536998555\n",
            "step: 190, loss: 0.0001640539412619546\n",
            "step: 200, loss: 0.008513140492141247\n",
            "step: 210, loss: 0.0004170868778601289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6578366445916115, f1=0.609865470852018, best_f1=0.6680672268907564\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 282.15it/s]\n",
            "load_f1 = 0.7033398821218074\n",
            "real_f1 = 0.7000000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 184.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a58f41-b42f-4cd5-cc1d-f0a2804eef57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5859736204147339\n",
            "step: 10, loss: 0.3428954482078552\n",
            "step: 20, loss: 0.2946934401988983\n",
            "step: 30, loss: 0.43301472067832947\n",
            "step: 40, loss: 0.41834166646003723\n",
            "step: 50, loss: 0.33564040064811707\n",
            "step: 60, loss: 0.2624850869178772\n",
            "step: 70, loss: 0.25834280252456665\n",
            "step: 80, loss: 0.2958296239376068\n",
            "step: 90, loss: 0.2785518765449524\n",
            "step: 100, loss: 0.3322627544403076\n",
            "step: 110, loss: 0.5067391991615295\n",
            "step: 120, loss: 0.09253686666488647\n",
            "step: 130, loss: 0.11667226999998093\n",
            "step: 140, loss: 0.06084996089339256\n",
            "step: 150, loss: 0.10934719443321228\n",
            "step: 160, loss: 0.08438940346240997\n",
            "step: 170, loss: 0.20361453294754028\n",
            "step: 180, loss: 0.02084360457956791\n",
            "step: 190, loss: 0.23858104646205902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6854219948849106, f1=0.7180851063829788, best_f1=0.7180851063829788\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28745779395103455\n",
            "step: 10, loss: 0.05884002894163132\n",
            "step: 20, loss: 0.08883176743984222\n",
            "step: 30, loss: 0.17984971404075623\n",
            "step: 40, loss: 0.15311773121356964\n",
            "step: 50, loss: 0.019676411524415016\n",
            "step: 60, loss: 0.18516451120376587\n",
            "step: 70, loss: 0.14417676627635956\n",
            "step: 80, loss: 0.16189737617969513\n",
            "step: 90, loss: 0.1483948826789856\n",
            "step: 100, loss: 0.015505686402320862\n",
            "step: 110, loss: 0.1715383231639862\n",
            "step: 120, loss: 0.25803011655807495\n",
            "step: 130, loss: 0.12125241756439209\n",
            "step: 140, loss: 0.08330117166042328\n",
            "step: 150, loss: 0.10313200205564499\n",
            "step: 160, loss: 0.037156011909246445\n",
            "step: 170, loss: 0.15044814348220825\n",
            "step: 180, loss: 0.15463685989379883\n",
            "step: 190, loss: 0.01814812235534191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7922437673130193, f1=0.7734806629834254, best_f1=0.7734806629834254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018573680892586708\n",
            "step: 10, loss: 0.212227463722229\n",
            "step: 20, loss: 0.06114271283149719\n",
            "step: 30, loss: 0.13110890984535217\n",
            "step: 40, loss: 0.12761494517326355\n",
            "step: 50, loss: 0.10991301387548447\n",
            "step: 60, loss: 0.012872488237917423\n",
            "step: 70, loss: 0.0736360028386116\n",
            "step: 80, loss: 0.09504639357328415\n",
            "step: 90, loss: 0.0630415603518486\n",
            "step: 100, loss: 0.0307109784334898\n",
            "step: 110, loss: 0.014094065874814987\n",
            "step: 120, loss: 0.15367792546749115\n",
            "step: 130, loss: 0.013880091719329357\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 140, loss: 0.05014268681406975\n",
            "step: 150, loss: 0.1377774327993393\n",
            "step: 160, loss: 0.026717010885477066\n",
            "step: 170, loss: 0.12461995333433151\n",
            "step: 180, loss: 0.15582408010959625\n",
            "step: 190, loss: 0.11308372765779495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7909604519774011, f1=0.8, best_f1=0.7734806629834254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03597605973482132\n",
            "step: 10, loss: 0.13167685270309448\n",
            "step: 20, loss: 0.038504213094711304\n",
            "step: 30, loss: 0.0111116087064147\n",
            "step: 40, loss: 0.09904304891824722\n",
            "step: 50, loss: 0.00945945642888546\n",
            "step: 60, loss: 0.1637646108865738\n",
            "step: 70, loss: 0.03114270605146885\n",
            "step: 80, loss: 0.07868309319019318\n",
            "step: 90, loss: 0.0094047412276268\n",
            "step: 100, loss: 0.03383363038301468\n",
            "step: 110, loss: 0.0015194148290902376\n",
            "step: 120, loss: 0.008659016340970993\n",
            "step: 130, loss: 0.08792567998170853\n",
            "step: 140, loss: 0.056013260036706924\n",
            "step: 150, loss: 0.008355511352419853\n",
            "step: 160, loss: 0.01768551766872406\n",
            "step: 170, loss: 0.10280552506446838\n",
            "step: 180, loss: 0.01142455730587244\n",
            "step: 190, loss: 0.17399822175502777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7835616438356164, f1=0.7966101694915253, best_f1=0.7734806629834254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19867181777954102\n",
            "step: 10, loss: 0.012175059877336025\n",
            "step: 20, loss: 0.0032583295833319426\n",
            "step: 30, loss: 0.002666308544576168\n",
            "step: 40, loss: 0.18350592255592346\n",
            "step: 50, loss: 0.1428181678056717\n",
            "step: 60, loss: 0.04832838848233223\n",
            "step: 70, loss: 0.004698409233242273\n",
            "step: 80, loss: 0.01396638061851263\n",
            "step: 90, loss: 0.02447126805782318\n",
            "step: 100, loss: 0.0009770295582711697\n",
            "step: 110, loss: 0.0048856413923203945\n",
            "step: 120, loss: 0.013841946609318256\n",
            "step: 130, loss: 0.07971442490816116\n",
            "step: 140, loss: 0.0554366298019886\n",
            "step: 150, loss: 0.10102344304323196\n",
            "step: 160, loss: 0.02849692478775978\n",
            "step: 170, loss: 0.019994059577584267\n",
            "step: 180, loss: 0.009950585663318634\n",
            "step: 190, loss: 0.005170060321688652\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7957559681697614, f1=0.8073878627968338, best_f1=0.8073878627968338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006370760500431061\n",
            "step: 10, loss: 0.005274791270494461\n",
            "step: 20, loss: 0.002459293929859996\n",
            "step: 30, loss: 0.0006202494259923697\n",
            "step: 40, loss: 0.028384145349264145\n",
            "step: 50, loss: 0.004288894589990377\n",
            "step: 60, loss: 0.03805924579501152\n",
            "step: 70, loss: 0.0008120008278638124\n",
            "step: 80, loss: 0.058616120368242264\n",
            "step: 90, loss: 0.009955989196896553\n",
            "step: 100, loss: 0.04781657084822655\n",
            "step: 110, loss: 0.011715746484696865\n",
            "step: 120, loss: 0.0033869221806526184\n",
            "step: 130, loss: 0.002813513157889247\n",
            "step: 140, loss: 0.035764213651418686\n",
            "step: 150, loss: 0.0009845277527347207\n",
            "step: 160, loss: 0.016507573425769806\n",
            "step: 170, loss: 0.010427790693938732\n",
            "step: 180, loss: 0.009960522875189781\n",
            "step: 190, loss: 0.003520769765600562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8021390374331551, f1=0.8021680216802168, best_f1=0.8021680216802168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002225299831479788\n",
            "step: 10, loss: 0.05167379230260849\n",
            "step: 20, loss: 0.04793612286448479\n",
            "step: 30, loss: 0.025715947151184082\n",
            "step: 40, loss: 0.0031139464117586613\n",
            "step: 50, loss: 0.0657677873969078\n",
            "step: 60, loss: 0.003557598451152444\n",
            "step: 70, loss: 0.008377487771213055\n",
            "step: 80, loss: 0.0030536726117134094\n",
            "step: 90, loss: 0.0007288334309123456\n",
            "step: 100, loss: 0.001139584113843739\n",
            "step: 110, loss: 0.0038074906915426254\n",
            "step: 120, loss: 0.0072202133014798164\n",
            "step: 130, loss: 0.02816847339272499\n",
            "step: 140, loss: 0.0026234795805066824\n",
            "step: 150, loss: 0.021073324605822563\n",
            "step: 160, loss: 0.1251450926065445\n",
            "step: 170, loss: 0.028834061697125435\n",
            "step: 180, loss: 0.004939843434840441\n",
            "step: 190, loss: 0.0066359941847622395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7842105263157894, f1=0.7924528301886792, best_f1=0.8021680216802168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00533713772892952\n",
            "step: 10, loss: 0.06272028386592865\n",
            "step: 20, loss: 0.0007554551702924073\n",
            "step: 30, loss: 0.002356771845370531\n",
            "step: 40, loss: 0.0011543519794940948\n",
            "step: 50, loss: 0.0016208733431994915\n",
            "step: 60, loss: 0.00039274521986953914\n",
            "step: 70, loss: 0.04078863188624382\n",
            "step: 80, loss: 9.443463932257146e-05\n",
            "step: 90, loss: 0.012221350334584713\n",
            "step: 100, loss: 0.04724647477269173\n",
            "step: 110, loss: 0.0002531094360165298\n",
            "step: 120, loss: 0.00023701558529864997\n",
            "step: 130, loss: 0.0023038447834551334\n",
            "step: 140, loss: 0.0016952389851212502\n",
            "step: 150, loss: 0.0024232324212789536\n",
            "step: 160, loss: 0.027927376329898834\n",
            "step: 170, loss: 0.020700234919786453\n",
            "step: 180, loss: 0.014240289106965065\n",
            "step: 190, loss: 0.02152181603014469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8010752688172043, f1=0.7967032967032966, best_f1=0.8021680216802168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00402098661288619\n",
            "step: 10, loss: 0.04860740154981613\n",
            "step: 20, loss: 0.0015509355580434203\n",
            "step: 30, loss: 0.0014875751221552491\n",
            "step: 40, loss: 0.020751172676682472\n",
            "step: 50, loss: 0.0006835762760601938\n",
            "step: 60, loss: 0.0003846239997074008\n",
            "step: 70, loss: 0.00034624221734702587\n",
            "step: 80, loss: 0.000678381125908345\n",
            "step: 90, loss: 0.024576453492045403\n",
            "step: 100, loss: 0.014834805391728878\n",
            "step: 110, loss: 0.0006236528279259801\n",
            "step: 120, loss: 0.003241828177124262\n",
            "step: 130, loss: 0.0030643485952168703\n",
            "step: 140, loss: 0.08578767627477646\n",
            "step: 150, loss: 0.007794082164764404\n",
            "step: 160, loss: 0.00046220814692787826\n",
            "step: 170, loss: 0.0007771681412123144\n",
            "step: 180, loss: 0.0016957083716988564\n",
            "step: 190, loss: 0.0005402928218245506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8074866310160429, f1=0.7956403269754768, best_f1=0.7956403269754768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006374032236635685\n",
            "step: 10, loss: 0.0005183526081964374\n",
            "step: 20, loss: 0.018912658095359802\n",
            "step: 30, loss: 0.000138548479299061\n",
            "step: 40, loss: 0.0023765156511217356\n",
            "step: 50, loss: 0.0002606858324725181\n",
            "step: 60, loss: 0.0007253789808601141\n",
            "step: 70, loss: 0.017807183787226677\n",
            "step: 80, loss: 0.0002637276193127036\n",
            "step: 90, loss: 0.00036066595930606127\n",
            "step: 100, loss: 0.00020556421077344567\n",
            "step: 110, loss: 0.02251235954463482\n",
            "step: 120, loss: 0.10617945343255997\n",
            "step: 130, loss: 0.001159132458269596\n",
            "step: 140, loss: 0.0010145248379558325\n",
            "step: 150, loss: 0.005065500736236572\n",
            "step: 160, loss: 0.0026203615125268698\n",
            "step: 170, loss: 0.0003117255982942879\n",
            "step: 180, loss: 0.00020492062321864069\n",
            "step: 190, loss: 0.00046443974133580923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8099173553719008, f1=0.8022922636103152, best_f1=0.8022922636103152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027461853460408747\n",
            "step: 10, loss: 0.0005490813055075705\n",
            "step: 20, loss: 0.019681427627801895\n",
            "step: 30, loss: 0.008612344972789288\n",
            "step: 40, loss: 0.00019716337556019425\n",
            "step: 50, loss: 0.029302814975380898\n",
            "step: 60, loss: 0.00015984497440513223\n",
            "step: 70, loss: 0.00020770668925251812\n",
            "step: 80, loss: 0.00045287629473023117\n",
            "step: 90, loss: 0.0003771288029383868\n",
            "step: 100, loss: 0.0002389194123679772\n",
            "step: 110, loss: 0.00017561452114023268\n",
            "step: 120, loss: 0.0002940602716989815\n",
            "step: 130, loss: 0.00024868943728506565\n",
            "step: 140, loss: 0.00023980015248525888\n",
            "step: 150, loss: 0.00013402572949416935\n",
            "step: 160, loss: 0.0002629086666274816\n",
            "step: 170, loss: 0.0004309725482016802\n",
            "step: 180, loss: 0.000199835718376562\n",
            "step: 190, loss: 0.000520171714015305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8209366391184573, f1=0.7899159663865546, best_f1=0.7899159663865546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013041688362136483\n",
            "step: 10, loss: 0.0010635816724970937\n",
            "step: 20, loss: 0.00014599162386730313\n",
            "step: 30, loss: 0.0022714927326887846\n",
            "step: 40, loss: 0.0012977526057511568\n",
            "step: 50, loss: 0.000675910385325551\n",
            "step: 60, loss: 0.00015921150043141097\n",
            "step: 70, loss: 0.00028983468655496836\n",
            "step: 80, loss: 0.0007728604832664132\n",
            "step: 90, loss: 0.0003312437329441309\n",
            "step: 100, loss: 0.00016875023720785975\n",
            "step: 110, loss: 0.00019528280245140195\n",
            "step: 120, loss: 0.0002402004029136151\n",
            "step: 130, loss: 0.00020311030675657094\n",
            "step: 140, loss: 0.0016669981414452195\n",
            "step: 150, loss: 0.00040724710561335087\n",
            "step: 160, loss: 8.47812116262503e-05\n",
            "step: 170, loss: 0.00035625617601908743\n",
            "step: 180, loss: 8.376971527468413e-05\n",
            "step: 190, loss: 9.884404425974935e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8123249299719888, f1=0.792022792022792, best_f1=0.7899159663865546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007329173968173563\n",
            "step: 10, loss: 0.00015976946451701224\n",
            "step: 20, loss: 0.0004768257786054164\n",
            "step: 30, loss: 0.12744872272014618\n",
            "step: 40, loss: 0.00032068986911326647\n",
            "step: 50, loss: 0.0005077358218841255\n",
            "step: 60, loss: 0.0021129180677235126\n",
            "step: 70, loss: 0.004465129692107439\n",
            "step: 80, loss: 0.0003363546566106379\n",
            "step: 90, loss: 0.0008894117781892419\n",
            "step: 100, loss: 0.0031816211994737387\n",
            "step: 110, loss: 0.00015154611901380122\n",
            "step: 120, loss: 0.003956225235015154\n",
            "step: 130, loss: 0.00044945275294594467\n",
            "step: 140, loss: 0.00018602071213535964\n",
            "step: 150, loss: 0.00014852467575110495\n",
            "step: 160, loss: 0.0002255547879030928\n",
            "step: 170, loss: 0.00019994605099782348\n",
            "step: 180, loss: 0.00013045116793364286\n",
            "step: 190, loss: 0.0027023274451494217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8140161725067385, f1=0.8213333333333334, best_f1=0.7899159663865546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03490752726793289\n",
            "step: 10, loss: 0.0001667511824052781\n",
            "step: 20, loss: 0.00015734988846816123\n",
            "step: 30, loss: 0.0006013228558003902\n",
            "step: 40, loss: 0.00020705924544017762\n",
            "step: 50, loss: 0.00016099157801363617\n",
            "step: 60, loss: 0.0004992168978787959\n",
            "step: 70, loss: 0.00012868507474195212\n",
            "step: 80, loss: 9.169010445475578e-05\n",
            "step: 90, loss: 0.0003658454224932939\n",
            "step: 100, loss: 0.0025857435539364815\n",
            "step: 110, loss: 0.00022971593716647476\n",
            "step: 120, loss: 0.0005168535863049328\n",
            "step: 130, loss: 0.00016175827477127314\n",
            "step: 140, loss: 0.0003660922811832279\n",
            "step: 150, loss: 9.622486686566845e-05\n",
            "step: 160, loss: 0.0002807800192385912\n",
            "step: 170, loss: 0.000169265505974181\n",
            "step: 180, loss: 0.00021740405645687133\n",
            "step: 190, loss: 0.00011435618216637522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8138297872340426, f1=0.8085106382978724, best_f1=0.7899159663865546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019315803656354547\n",
            "step: 10, loss: 8.756924944464117e-05\n",
            "step: 20, loss: 0.0002174217370338738\n",
            "step: 30, loss: 9.26963402889669e-05\n",
            "step: 40, loss: 0.0001130435848608613\n",
            "step: 50, loss: 0.00011768618423957378\n",
            "step: 60, loss: 9.38364610192366e-05\n",
            "step: 70, loss: 0.005617887247353792\n",
            "step: 80, loss: 0.0003678637440316379\n",
            "step: 90, loss: 8.479381358483806e-05\n",
            "step: 100, loss: 0.00020203863095957786\n",
            "step: 110, loss: 0.00018982034816872329\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.0001628799655009061\n",
            "step: 130, loss: 0.010208900086581707\n",
            "step: 140, loss: 0.0014865404227748513\n",
            "step: 150, loss: 0.00015308607544284314\n",
            "step: 160, loss: 0.00014818692579865456\n",
            "step: 170, loss: 0.00013035429583396763\n",
            "step: 180, loss: 0.0034798337146639824\n",
            "step: 190, loss: 0.00012367511226329952\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8121546961325967, f1=0.8055555555555556, best_f1=0.7899159663865546\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 165.45it/s]\n",
            "load_f1 = 0.8186813186813187\n",
            "real_f1 = 0.8209366391184573\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df78992-2cb2-4a75-bae0-663a219acb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.608964741230011\n",
            "step: 10, loss: 0.37176182866096497\n",
            "step: 20, loss: 0.3199981451034546\n",
            "step: 30, loss: 0.4164694845676422\n",
            "step: 40, loss: 0.28772231936454773\n",
            "step: 50, loss: 0.2867978513240814\n",
            "step: 60, loss: 0.22136126458644867\n",
            "step: 70, loss: 0.37581419944763184\n",
            "step: 80, loss: 0.33087587356567383\n",
            "step: 90, loss: 0.278876930475235\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.18068687617778778\n",
            "step: 110, loss: 0.27723193168640137\n",
            "step: 120, loss: 0.22905020415782928\n",
            "step: 130, loss: 0.07581739127635956\n",
            "step: 140, loss: 0.2111777514219284\n",
            "step: 150, loss: 0.25234454870224\n",
            "step: 160, loss: 0.14972996711730957\n",
            "step: 170, loss: 0.17359328269958496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7277227722772277, f1=0.7548076923076923, best_f1=0.7548076923076923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14562007784843445\n",
            "step: 10, loss: 0.1286705583333969\n",
            "step: 20, loss: 0.0883573591709137\n",
            "step: 30, loss: 0.29232487082481384\n",
            "step: 40, loss: 0.06372775882482529\n",
            "step: 50, loss: 0.1989271491765976\n",
            "step: 60, loss: 0.10812437534332275\n",
            "step: 70, loss: 0.1277197003364563\n",
            "step: 80, loss: 0.02560921385884285\n",
            "step: 90, loss: 0.04065953940153122\n",
            "step: 100, loss: 0.11381419003009796\n",
            "step: 110, loss: 0.0788901224732399\n",
            "step: 120, loss: 0.300593763589859\n",
            "step: 130, loss: 0.10390488803386688\n",
            "step: 140, loss: 0.2110726684331894\n",
            "step: 150, loss: 0.14764107763767242\n",
            "step: 160, loss: 0.08441758900880814\n",
            "step: 170, loss: 0.050237830728292465\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.787128712871287, f1=0.7838479809976248, best_f1=0.7838479809976248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0796331912279129\n",
            "step: 10, loss: 0.023827265948057175\n",
            "step: 20, loss: 0.07452094554901123\n",
            "step: 30, loss: 0.24123752117156982\n",
            "step: 40, loss: 0.07801631838083267\n",
            "step: 50, loss: 0.06345465779304504\n",
            "step: 60, loss: 0.10414409637451172\n",
            "step: 70, loss: 0.08429516851902008\n",
            "step: 80, loss: 0.057208701968193054\n",
            "step: 90, loss: 0.15717074275016785\n",
            "step: 100, loss: 0.007543105632066727\n",
            "step: 110, loss: 0.034657176584005356\n",
            "step: 120, loss: 0.0595102459192276\n",
            "step: 130, loss: 0.16340163350105286\n",
            "step: 140, loss: 0.09864652901887894\n",
            "step: 150, loss: 0.1057158038020134\n",
            "step: 160, loss: 0.07387940585613251\n",
            "step: 170, loss: 0.22709529101848602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.759493670886076, f1=0.7707317073170732, best_f1=0.7838479809976248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018930504098534584\n",
            "step: 10, loss: 0.06934985518455505\n",
            "step: 20, loss: 0.02617844007909298\n",
            "step: 30, loss: 0.12421999871730804\n",
            "step: 40, loss: 0.0014624915784224868\n",
            "step: 50, loss: 0.08205651491880417\n",
            "step: 60, loss: 0.137105330824852\n",
            "step: 70, loss: 0.002888160292059183\n",
            "step: 80, loss: 0.10138147324323654\n",
            "step: 90, loss: 0.1536918580532074\n",
            "step: 100, loss: 0.16663600504398346\n",
            "step: 110, loss: 0.09017772972583771\n",
            "step: 120, loss: 0.0063294279389083385\n",
            "step: 130, loss: 0.017151163890957832\n",
            "step: 140, loss: 0.1006346195936203\n",
            "step: 150, loss: 0.11568029224872589\n",
            "step: 160, loss: 0.1979387253522873\n",
            "step: 170, loss: 0.08775162696838379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7727272727272727, f1=0.8057553956834533, best_f1=0.7838479809976248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04783746227622032\n",
            "step: 10, loss: 0.005890728905797005\n",
            "step: 20, loss: 0.01631157286465168\n",
            "step: 30, loss: 0.06524590402841568\n",
            "step: 40, loss: 0.0626714676618576\n",
            "step: 50, loss: 0.012689689174294472\n",
            "step: 60, loss: 0.0017868013819679618\n",
            "step: 70, loss: 0.08367787301540375\n",
            "step: 80, loss: 0.10623511672019958\n",
            "step: 90, loss: 0.17485609650611877\n",
            "step: 100, loss: 0.05752899870276451\n",
            "step: 110, loss: 0.15500618517398834\n",
            "step: 120, loss: 0.0127715440467\n",
            "step: 130, loss: 0.043304286897182465\n",
            "step: 140, loss: 0.02446885034441948\n",
            "step: 150, loss: 0.13533082604408264\n",
            "step: 160, loss: 0.05049283802509308\n",
            "step: 170, loss: 0.01912762224674225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7655502392344498, f1=0.7916666666666666, best_f1=0.7838479809976248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007921732030808926\n",
            "step: 10, loss: 0.04073882848024368\n",
            "step: 20, loss: 0.003280221950262785\n",
            "step: 30, loss: 0.02998259849846363\n",
            "step: 40, loss: 0.00848080962896347\n",
            "step: 50, loss: 0.11534682661294937\n",
            "step: 60, loss: 0.006901330780237913\n",
            "step: 70, loss: 0.05241763964295387\n",
            "step: 80, loss: 0.02519953064620495\n",
            "step: 90, loss: 0.012700876221060753\n",
            "step: 100, loss: 0.0013951101573184133\n",
            "step: 110, loss: 0.004749370738863945\n",
            "step: 120, loss: 0.01476444024592638\n",
            "step: 130, loss: 0.03268996998667717\n",
            "step: 140, loss: 0.03321652486920357\n",
            "step: 150, loss: 0.07916546612977982\n",
            "step: 160, loss: 0.04364090785384178\n",
            "step: 170, loss: 0.00098451878875494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7893462469733656, f1=0.7731481481481483, best_f1=0.7731481481481483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004011881537735462\n",
            "step: 10, loss: 0.0006643960950896144\n",
            "step: 20, loss: 0.004081712104380131\n",
            "step: 30, loss: 0.01852068305015564\n",
            "step: 40, loss: 0.13936731219291687\n",
            "step: 50, loss: 0.034923139959573746\n",
            "step: 60, loss: 0.0017194735119119287\n",
            "step: 70, loss: 0.0005021622055210173\n",
            "step: 80, loss: 0.01133015938103199\n",
            "step: 90, loss: 0.0003994764410890639\n",
            "step: 100, loss: 0.002827031072229147\n",
            "step: 110, loss: 0.004791948478668928\n",
            "step: 120, loss: 0.0039197830483317375\n",
            "step: 130, loss: 0.11319585889577866\n",
            "step: 140, loss: 0.056114841252565384\n",
            "step: 150, loss: 0.01219034381210804\n",
            "step: 160, loss: 0.0003668120480142534\n",
            "step: 170, loss: 0.0655570849776268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7922705314009661, f1=0.7915690866510539, best_f1=0.7915690866510539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009519907762296498\n",
            "step: 10, loss: 0.0015598530881106853\n",
            "step: 20, loss: 0.0002720303018577397\n",
            "step: 30, loss: 0.052945103496313095\n",
            "step: 40, loss: 0.00025824218755587935\n",
            "step: 50, loss: 0.006406262516975403\n",
            "step: 60, loss: 0.0010288541670888662\n",
            "step: 70, loss: 0.0006217499612830579\n",
            "step: 80, loss: 0.008118638768792152\n",
            "step: 90, loss: 0.0005842589307576418\n",
            "step: 100, loss: 0.0517183393239975\n",
            "step: 110, loss: 0.05034938082098961\n",
            "step: 120, loss: 0.0016124387038871646\n",
            "step: 130, loss: 0.00637862179428339\n",
            "step: 140, loss: 0.0008440644014626741\n",
            "step: 150, loss: 0.021130423992872238\n",
            "step: 160, loss: 0.003330261679366231\n",
            "step: 170, loss: 0.007644359953701496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7777777777777778, f1=0.771551724137931, best_f1=0.7915690866510539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005383950192481279\n",
            "step: 10, loss: 0.00498559046536684\n",
            "step: 20, loss: 0.0003322017437312752\n",
            "step: 30, loss: 0.014244929887354374\n",
            "step: 40, loss: 0.006013170350342989\n",
            "step: 50, loss: 0.00029076822102069855\n",
            "step: 60, loss: 0.04947999492287636\n",
            "step: 70, loss: 0.018313266336917877\n",
            "step: 80, loss: 0.00035656432737596333\n",
            "step: 90, loss: 0.03964582458138466\n",
            "step: 100, loss: 0.026357725262641907\n",
            "step: 110, loss: 0.0005103240837343037\n",
            "step: 120, loss: 0.001684972201474011\n",
            "step: 130, loss: 0.08439728617668152\n",
            "step: 140, loss: 0.0006876651314087212\n",
            "step: 150, loss: 0.06488781422376633\n",
            "step: 160, loss: 0.02588062174618244\n",
            "step: 170, loss: 0.006827720906585455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8058252427184465, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.055033762007951736\n",
            "step: 10, loss: 0.0001908992271637544\n",
            "step: 20, loss: 0.043274443596601486\n",
            "step: 30, loss: 0.009050547145307064\n",
            "step: 40, loss: 0.003417401108890772\n",
            "step: 50, loss: 0.06796982139348984\n",
            "step: 60, loss: 0.00025074472068808973\n",
            "step: 70, loss: 0.009357758797705173\n",
            "step: 80, loss: 0.001794614945538342\n",
            "step: 90, loss: 0.008849983103573322\n",
            "step: 100, loss: 0.000156313632032834\n",
            "step: 110, loss: 0.0014616578118875623\n",
            "step: 120, loss: 0.0001701415458228439\n",
            "step: 130, loss: 0.001973760314285755\n",
            "step: 140, loss: 0.24192912876605988\n",
            "step: 150, loss: 0.001482749474234879\n",
            "step: 160, loss: 0.0010487547842785716\n",
            "step: 170, loss: 0.0002758832124527544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7862407862407862, f1=0.7842227378190254, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05938039720058441\n",
            "step: 10, loss: 0.02079075574874878\n",
            "step: 20, loss: 0.00015949048975016922\n",
            "step: 30, loss: 0.00014010693121235818\n",
            "step: 40, loss: 0.0056085046380758286\n",
            "step: 50, loss: 0.000360076199285686\n",
            "step: 60, loss: 0.008629819378256798\n",
            "step: 70, loss: 9.478355059400201e-05\n",
            "step: 80, loss: 0.00010693569493014365\n",
            "step: 90, loss: 0.0001338091678917408\n",
            "step: 100, loss: 0.0005566537147387862\n",
            "step: 110, loss: 0.2556253969669342\n",
            "step: 120, loss: 0.00016643029812257737\n",
            "step: 130, loss: 0.0001386226067552343\n",
            "step: 140, loss: 0.0002039999671978876\n",
            "step: 150, loss: 0.002087774919345975\n",
            "step: 160, loss: 0.005788686219602823\n",
            "step: 170, loss: 0.00045702431816607714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7480106100795756, f1=0.7892156862745098, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.061189230531454086\n",
            "step: 10, loss: 0.0005175832775421441\n",
            "step: 20, loss: 0.00024766632122918963\n",
            "step: 30, loss: 0.00024171810946427286\n",
            "step: 40, loss: 0.001167385489679873\n",
            "step: 50, loss: 0.0001378507586196065\n",
            "step: 60, loss: 0.0030700028873980045\n",
            "step: 70, loss: 0.06890933960676193\n",
            "step: 80, loss: 0.0001368656085105613\n",
            "step: 90, loss: 0.0003460514999460429\n",
            "step: 100, loss: 0.0012906816555187106\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.00019505656382534653\n",
            "step: 120, loss: 0.0012233239831402898\n",
            "step: 130, loss: 0.0069166929461061954\n",
            "step: 140, loss: 0.01622786559164524\n",
            "step: 150, loss: 0.004151287488639355\n",
            "step: 160, loss: 0.00024656258756294847\n",
            "step: 170, loss: 0.00016504191444255412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7851851851851852, f1=0.7972027972027972, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018391720950603485\n",
            "step: 10, loss: 0.0002558293635956943\n",
            "step: 20, loss: 0.0015270088333636522\n",
            "step: 30, loss: 0.00020974595099687576\n",
            "step: 40, loss: 0.00027680417406372726\n",
            "step: 50, loss: 0.00014268673839978874\n",
            "step: 60, loss: 0.020870542153716087\n",
            "step: 70, loss: 0.00027614986174739897\n",
            "step: 80, loss: 0.003796263597905636\n",
            "step: 90, loss: 0.0002120194403687492\n",
            "step: 100, loss: 0.004123339429497719\n",
            "step: 110, loss: 0.00012425618479028344\n",
            "step: 120, loss: 0.030836785212159157\n",
            "step: 130, loss: 0.00018400026601739228\n",
            "step: 140, loss: 0.0006229413556866348\n",
            "step: 150, loss: 0.00036146712955087423\n",
            "step: 160, loss: 0.0033518278505653143\n",
            "step: 170, loss: 0.02959415130317211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.766839378238342, f1=0.784503631961259, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002853111072909087\n",
            "step: 10, loss: 0.00013426702935248613\n",
            "step: 20, loss: 0.00026332237757742405\n",
            "step: 30, loss: 0.00020904367556795478\n",
            "step: 40, loss: 0.000625958142336458\n",
            "step: 50, loss: 0.0002688475651666522\n",
            "step: 60, loss: 0.00012440900900401175\n",
            "step: 70, loss: 0.0003121493791695684\n",
            "step: 80, loss: 0.00035766520886681974\n",
            "step: 90, loss: 0.00011240359890507534\n",
            "step: 100, loss: 0.00011405926488805562\n",
            "step: 110, loss: 0.00018545257626101375\n",
            "step: 120, loss: 0.02293545939028263\n",
            "step: 130, loss: 0.003294400405138731\n",
            "step: 140, loss: 0.00030346852145157754\n",
            "step: 150, loss: 0.00012068993237335235\n",
            "step: 160, loss: 6.799866241635755e-05\n",
            "step: 170, loss: 0.00013449552352540195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7688172043010754, f1=0.7786259541984734, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015947918291203678\n",
            "step: 10, loss: 0.003735373029485345\n",
            "step: 20, loss: 0.010113372467458248\n",
            "step: 30, loss: 0.0003142955538351089\n",
            "step: 40, loss: 0.000754607142880559\n",
            "step: 50, loss: 0.00033520793658681214\n",
            "step: 60, loss: 0.018829345703125\n",
            "step: 70, loss: 0.00012034034443786368\n",
            "step: 80, loss: 0.047195374965667725\n",
            "step: 90, loss: 0.00017117975221481174\n",
            "step: 100, loss: 0.001282449346035719\n",
            "step: 110, loss: 0.00023159824195317924\n",
            "step: 120, loss: 0.00027653115103021264\n",
            "step: 130, loss: 0.005107981152832508\n",
            "step: 140, loss: 0.0001711601798888296\n",
            "step: 150, loss: 0.0012963417684659362\n",
            "step: 160, loss: 0.009221234358847141\n",
            "step: 170, loss: 0.0032733932603150606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7774936061381074, f1=0.7865707434052758, best_f1=0.8\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 270.25it/s]\n",
            "load_f1 = 0.8038740920096852\n",
            "real_f1 = 0.8048780487804877\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 248.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ca5dcb-676f-4153-b4f2-655f0bc8b22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6260179877281189\n",
            "step: 10, loss: 0.6272473335266113\n",
            "step: 20, loss: 0.445242315530777\n",
            "step: 30, loss: 0.170008584856987\n",
            "step: 40, loss: 0.35500437021255493\n",
            "step: 50, loss: 0.07746218889951706\n",
            "step: 60, loss: 0.11993733048439026\n",
            "step: 70, loss: 0.07241245359182358\n",
            "step: 80, loss: 0.06335959583520889\n",
            "step: 90, loss: 0.05211516469717026\n",
            "step: 100, loss: 0.005536961834877729\n",
            "step: 110, loss: 0.28452956676483154\n",
            "step: 120, loss: 0.027704857289791107\n",
            "step: 130, loss: 0.0071223219856619835\n",
            "step: 140, loss: 0.002566908486187458\n",
            "step: 150, loss: 0.016703780740499496\n",
            "step: 160, loss: 0.006554034072905779\n",
            "step: 170, loss: 0.08134578913450241\n",
            "step: 180, loss: 0.10202956944704056\n",
            "step: 190, loss: 0.07179022580385208\n",
            "step: 200, loss: 0.08535026758909225\n",
            "step: 210, loss: 0.005083436146378517\n",
            "step: 220, loss: 0.008666153065860271\n",
            "step: 230, loss: 0.03337440267205238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9668141592920354, f1=0.9678848283499446, best_f1=0.9678848283499446\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007230783347040415\n",
            "step: 10, loss: 0.004978269804269075\n",
            "step: 20, loss: 0.18062807619571686\n",
            "step: 30, loss: 0.17280998826026917\n",
            "step: 40, loss: 0.035905566066503525\n",
            "step: 50, loss: 0.004077078774571419\n",
            "step: 60, loss: 0.016993612051010132\n",
            "step: 70, loss: 0.10806027799844742\n",
            "step: 80, loss: 0.01940910518169403\n",
            "step: 90, loss: 0.011914247646927834\n",
            "step: 100, loss: 0.05414203926920891\n",
            "step: 110, loss: 0.03771830350160599\n",
            "step: 120, loss: 0.18400701880455017\n",
            "step: 130, loss: 0.01298193447291851\n",
            "step: 140, loss: 0.002112565329298377\n",
            "step: 150, loss: 0.04937394708395004\n",
            "step: 160, loss: 0.06402704119682312\n",
            "step: 170, loss: 0.00164191541261971\n",
            "step: 180, loss: 0.010511542670428753\n",
            "step: 190, loss: 0.0072831157594919205\n",
            "step: 200, loss: 0.00651921471580863\n",
            "step: 210, loss: 0.0007690017228014767\n",
            "step: 220, loss: 0.0840553417801857\n",
            "step: 230, loss: 0.009204384870827198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9704545454545453, f1=0.9679633867276889, best_f1=0.9679633867276889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012899014167487621\n",
            "step: 10, loss: 0.006694504991173744\n",
            "step: 20, loss: 0.003770081792026758\n",
            "step: 30, loss: 0.004864119924604893\n",
            "step: 40, loss: 0.021142439916729927\n",
            "step: 50, loss: 0.011000009253621101\n",
            "step: 60, loss: 0.002406420186161995\n",
            "step: 70, loss: 0.005648389924317598\n",
            "step: 80, loss: 0.0007329056970775127\n",
            "step: 90, loss: 0.07157924771308899\n",
            "step: 100, loss: 0.002768379868939519\n",
            "step: 110, loss: 0.006050541531294584\n",
            "step: 120, loss: 0.025850793346762657\n",
            "step: 130, loss: 0.0007437097374349833\n",
            "step: 140, loss: 0.0024998243898153305\n",
            "step: 150, loss: 0.03811869025230408\n",
            "step: 160, loss: 0.02770012617111206\n",
            "step: 170, loss: 0.012889519333839417\n",
            "step: 180, loss: 0.007776603568345308\n",
            "step: 190, loss: 0.012026198208332062\n",
            "step: 200, loss: 0.06482347846031189\n",
            "step: 210, loss: 0.01595143787562847\n",
            "step: 220, loss: 0.04604296386241913\n",
            "step: 230, loss: 0.14179947972297668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.96875, f1=0.9720670391061451, best_f1=0.9679633867276889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005860367673449218\n",
            "step: 10, loss: 0.00163787673227489\n",
            "step: 20, loss: 0.0032016332261264324\n",
            "step: 30, loss: 0.0026261257007718086\n",
            "step: 40, loss: 0.0030815950594842434\n",
            "step: 50, loss: 0.0024590229149907827\n",
            "step: 60, loss: 0.002680039033293724\n",
            "step: 70, loss: 0.0006926798960193992\n",
            "step: 80, loss: 0.002603384433314204\n",
            "step: 90, loss: 0.0018391460180282593\n",
            "step: 100, loss: 0.0481957271695137\n",
            "step: 110, loss: 0.00020756623416673392\n",
            "step: 120, loss: 0.0015934703405946493\n",
            "step: 130, loss: 0.03922612592577934\n",
            "step: 140, loss: 0.001076463726349175\n",
            "step: 150, loss: 0.19978973269462585\n",
            "step: 160, loss: 0.004385118838399649\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.013684453442692757\n",
            "step: 180, loss: 0.000817402615211904\n",
            "step: 190, loss: 0.026035849004983902\n",
            "step: 200, loss: 0.0008642776519991457\n",
            "step: 210, loss: 0.0762990191578865\n",
            "step: 220, loss: 0.0015179385663941503\n",
            "step: 230, loss: 0.009694868698716164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9752252252252253, f1=0.9728506787330317, best_f1=0.9728506787330317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007991170510649681\n",
            "step: 10, loss: 0.001594020170159638\n",
            "step: 20, loss: 0.006718166172504425\n",
            "step: 30, loss: 0.00025230227038264275\n",
            "step: 40, loss: 0.0006233867025002837\n",
            "step: 50, loss: 0.0009749357705004513\n",
            "step: 60, loss: 0.0005166685441508889\n",
            "step: 70, loss: 0.00024538743309676647\n",
            "step: 80, loss: 0.00021962950995657593\n",
            "step: 90, loss: 0.0004161326796747744\n",
            "step: 100, loss: 0.001772893825545907\n",
            "step: 110, loss: 0.019300969317555428\n",
            "step: 120, loss: 0.00031146014225669205\n",
            "step: 130, loss: 0.0007190472679212689\n",
            "step: 140, loss: 0.0008780306670814753\n",
            "step: 150, loss: 0.032319940626621246\n",
            "step: 160, loss: 0.0005585881299339235\n",
            "step: 170, loss: 0.04558049142360687\n",
            "step: 180, loss: 0.006686880718916655\n",
            "step: 190, loss: 0.10468197613954544\n",
            "step: 200, loss: 0.00712910620495677\n",
            "step: 210, loss: 0.006558018736541271\n",
            "step: 220, loss: 0.0010279357666149735\n",
            "step: 230, loss: 0.00040875139529816806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9773242630385486, f1=0.9772727272727272, best_f1=0.9772727272727272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014298806199803948\n",
            "step: 10, loss: 0.02173979952931404\n",
            "step: 20, loss: 0.006778893060982227\n",
            "step: 30, loss: 0.002280393848195672\n",
            "step: 40, loss: 0.00043857592390850186\n",
            "step: 50, loss: 0.0008281274931505322\n",
            "step: 60, loss: 0.00032135151559486985\n",
            "step: 70, loss: 0.011406105011701584\n",
            "step: 80, loss: 0.003572834189981222\n",
            "step: 90, loss: 0.006148310843855143\n",
            "step: 100, loss: 0.008729402907192707\n",
            "step: 110, loss: 0.000654039322398603\n",
            "step: 120, loss: 0.0007131243473850191\n",
            "step: 130, loss: 0.0005553795490413904\n",
            "step: 140, loss: 0.00027569313533604145\n",
            "step: 150, loss: 0.004642079584300518\n",
            "step: 160, loss: 0.0010424255160614848\n",
            "step: 170, loss: 0.00023608254559803754\n",
            "step: 180, loss: 0.000924255873542279\n",
            "step: 190, loss: 0.0011449273442849517\n",
            "step: 200, loss: 0.0001617792440811172\n",
            "step: 210, loss: 0.0002572905214037746\n",
            "step: 220, loss: 0.00020052920444868505\n",
            "step: 230, loss: 0.06125007942318916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9808773903262092, f1=0.9773755656108598, best_f1=0.9773755656108598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005605787155218422\n",
            "step: 10, loss: 0.00011290294060017914\n",
            "step: 20, loss: 0.0002305156522197649\n",
            "step: 30, loss: 0.00010948735143756494\n",
            "step: 40, loss: 0.0001618025853531435\n",
            "step: 50, loss: 0.00010472418216522783\n",
            "step: 60, loss: 0.0010376794962212443\n",
            "step: 70, loss: 0.00016857351874932647\n",
            "step: 80, loss: 0.0008151212241500616\n",
            "step: 90, loss: 0.00019050497212447226\n",
            "step: 100, loss: 0.0002461467229295522\n",
            "step: 110, loss: 0.0003145984373986721\n",
            "step: 120, loss: 8.467448787996545e-05\n",
            "step: 130, loss: 0.00013046774256508797\n",
            "step: 140, loss: 0.00010830452083609998\n",
            "step: 150, loss: 0.00013587603461928666\n",
            "step: 160, loss: 0.0720972865819931\n",
            "step: 170, loss: 0.0008293346618302166\n",
            "step: 180, loss: 0.0005135972751304507\n",
            "step: 190, loss: 0.00034832351957447827\n",
            "step: 200, loss: 0.029582219198346138\n",
            "step: 210, loss: 0.00010522630327614024\n",
            "step: 220, loss: 0.03398378565907478\n",
            "step: 230, loss: 0.00993336457759142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.976271186440678, f1=0.9740698985343857, best_f1=0.9773755656108598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004645257431548089\n",
            "step: 10, loss: 0.0005145782488398254\n",
            "step: 20, loss: 0.0003231401205994189\n",
            "step: 30, loss: 0.00045738863991573453\n",
            "step: 40, loss: 0.0014931682962924242\n",
            "step: 50, loss: 0.00026952908956445754\n",
            "step: 60, loss: 0.00029256631387397647\n",
            "step: 70, loss: 0.00011704386270139366\n",
            "step: 80, loss: 0.00024259774363599718\n",
            "step: 90, loss: 8.26406103442423e-05\n",
            "step: 100, loss: 0.04649874567985535\n",
            "step: 110, loss: 0.0003689236764330417\n",
            "step: 120, loss: 0.0023228360805660486\n",
            "step: 130, loss: 0.00035351485712453723\n",
            "step: 140, loss: 0.00015902916493359953\n",
            "step: 150, loss: 0.0007039786432869732\n",
            "step: 160, loss: 0.0015667147235944867\n",
            "step: 170, loss: 0.0005800462095066905\n",
            "step: 180, loss: 0.021722419187426567\n",
            "step: 190, loss: 0.008524147793650627\n",
            "step: 200, loss: 0.0007490087300539017\n",
            "step: 210, loss: 0.0004636706435121596\n",
            "step: 220, loss: 0.0016452680574730039\n",
            "step: 230, loss: 0.0006974394782446325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9819819819819819, f1=0.9753363228699552, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020017437054775655\n",
            "step: 10, loss: 0.0018234089948236942\n",
            "step: 20, loss: 0.0008050092146731913\n",
            "step: 30, loss: 0.0001232438225997612\n",
            "step: 40, loss: 0.04125375673174858\n",
            "step: 50, loss: 0.0001320892624789849\n",
            "step: 60, loss: 0.00022652457118965685\n",
            "step: 70, loss: 0.00011978963448200375\n",
            "step: 80, loss: 0.00023840602079872042\n",
            "step: 90, loss: 0.0002966919564642012\n",
            "step: 100, loss: 0.0158619973808527\n",
            "step: 110, loss: 8.408509893342853e-05\n",
            "step: 120, loss: 5.540937127079815e-05\n",
            "step: 130, loss: 4.2629668314475566e-05\n",
            "step: 140, loss: 0.0001269933272851631\n",
            "step: 150, loss: 0.0007726395851932466\n",
            "step: 160, loss: 0.0003452706150710583\n",
            "step: 170, loss: 0.0002352118754060939\n",
            "step: 180, loss: 0.0037090876139700413\n",
            "step: 190, loss: 8.861265087034553e-05\n",
            "step: 200, loss: 0.0049045211635529995\n",
            "step: 210, loss: 0.001640578731894493\n",
            "step: 220, loss: 7.952863234095275e-05\n",
            "step: 230, loss: 0.0007458134787157178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9819004524886877, f1=0.9717514124293786, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005764744128100574\n",
            "step: 10, loss: 4.939762220601551e-05\n",
            "step: 20, loss: 0.0002370249858358875\n",
            "step: 30, loss: 9.328324813395739e-05\n",
            "step: 40, loss: 4.616055593942292e-05\n",
            "step: 50, loss: 4.4451095163822174e-05\n",
            "step: 60, loss: 0.001659413450397551\n",
            "step: 70, loss: 6.830145139247179e-05\n",
            "step: 80, loss: 0.00018145734793506563\n",
            "step: 90, loss: 0.00010830048995558172\n",
            "step: 100, loss: 5.2821134886471555e-05\n",
            "step: 110, loss: 0.00012510477972682565\n",
            "step: 120, loss: 6.798082904424518e-05\n",
            "step: 130, loss: 6.0195907281013206e-05\n",
            "step: 140, loss: 0.01267619151622057\n",
            "step: 150, loss: 0.06137249246239662\n",
            "step: 160, loss: 6.66164924041368e-05\n",
            "step: 170, loss: 6.551742262672633e-05\n",
            "step: 180, loss: 0.003973895218223333\n",
            "step: 190, loss: 0.0020095547661185265\n",
            "step: 200, loss: 0.00017487183504272252\n",
            "step: 210, loss: 0.00011377512419130653\n",
            "step: 220, loss: 0.00010993259638780728\n",
            "step: 230, loss: 0.000629861606284976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.977728285077951, f1=0.9678848283499446, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.938086153240874e-05\n",
            "step: 10, loss: 0.000779980793595314\n",
            "step: 20, loss: 0.0014163509476929903\n",
            "step: 30, loss: 0.00021555778221227229\n",
            "step: 40, loss: 0.0002530465426389128\n",
            "step: 50, loss: 0.00023453460016753525\n",
            "step: 60, loss: 0.00017332645074930042\n",
            "step: 70, loss: 9.204285743180662e-05\n",
            "step: 80, loss: 3.866270344587974e-05\n",
            "step: 90, loss: 0.000352701754309237\n",
            "step: 100, loss: 7.791675307089463e-05\n",
            "step: 110, loss: 6.625587411690503e-05\n",
            "step: 120, loss: 4.231974889989942e-05\n",
            "step: 130, loss: 3.334694338263944e-05\n",
            "step: 140, loss: 9.423254959983751e-05\n",
            "step: 150, loss: 0.0003229262074455619\n",
            "step: 160, loss: 6.414132076315582e-05\n",
            "step: 170, loss: 3.715220373123884e-05\n",
            "step: 180, loss: 0.004312175326049328\n",
            "step: 190, loss: 6.282705726334825e-05\n",
            "step: 200, loss: 5.6193552154581994e-05\n",
            "step: 210, loss: 3.869584543281235e-05\n",
            "step: 220, loss: 5.3478368499781936e-05\n",
            "step: 230, loss: 5.681228867615573e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9788182831661093, f1=0.9698324022346367, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.708333239657804e-05\n",
            "step: 10, loss: 3.3273005101364106e-05\n",
            "step: 20, loss: 6.347700400510803e-05\n",
            "step: 30, loss: 0.00010202266275882721\n",
            "step: 40, loss: 0.00024990085512399673\n",
            "step: 50, loss: 0.00012722676910925657\n",
            "step: 60, loss: 9.446468175156042e-05\n",
            "step: 70, loss: 7.279578858288005e-05\n",
            "step: 80, loss: 0.0004214034997858107\n",
            "step: 90, loss: 0.00015342766710091382\n",
            "step: 100, loss: 2.7402280466048978e-05\n",
            "step: 110, loss: 4.612117481883615e-05\n",
            "step: 120, loss: 6.314613710856065e-05\n",
            "step: 130, loss: 4.531907688942738e-05\n",
            "step: 140, loss: 4.09012682212051e-05\n",
            "step: 150, loss: 8.83951288415119e-05\n",
            "step: 160, loss: 0.00015851495845708996\n",
            "step: 170, loss: 5.4200503655010834e-05\n",
            "step: 180, loss: 5.8970665122615173e-05\n",
            "step: 190, loss: 4.664212974603288e-05\n",
            "step: 200, loss: 2.2604461264563724e-05\n",
            "step: 210, loss: 3.656590706668794e-05\n",
            "step: 220, loss: 6.012328958604485e-05\n",
            "step: 230, loss: 0.03690812364220619\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9820224719101124, f1=0.9741282339707535, best_f1=0.9741282339707535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.6679073420818895e-05\n",
            "step: 10, loss: 8.180539589375257e-05\n",
            "step: 20, loss: 0.00011925414582947269\n",
            "step: 30, loss: 0.0001661035930737853\n",
            "step: 40, loss: 3.3801687095547095e-05\n",
            "step: 50, loss: 0.0006034228135831654\n",
            "step: 60, loss: 6.047405622666702e-05\n",
            "step: 70, loss: 3.1707164453109726e-05\n",
            "step: 80, loss: 3.527679655235261e-05\n",
            "step: 90, loss: 6.949828093638644e-05\n",
            "step: 100, loss: 3.374911466380581e-05\n",
            "step: 110, loss: 0.027720982208848\n",
            "step: 120, loss: 0.0001759931183187291\n",
            "step: 130, loss: 5.163591413293034e-05\n",
            "step: 140, loss: 0.00011616632400546223\n",
            "step: 150, loss: 4.011571945738979e-05\n",
            "step: 160, loss: 0.00010366956848884001\n",
            "step: 170, loss: 5.812329618493095e-05\n",
            "step: 180, loss: 3.4182012313976884e-05\n",
            "step: 190, loss: 3.34215656039305e-05\n",
            "step: 200, loss: 3.545189611031674e-05\n",
            "step: 210, loss: 2.1185140212764964e-05\n",
            "step: 220, loss: 4.160456228419207e-05\n",
            "step: 230, loss: 2.83747667708667e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9820224719101124, f1=0.9763779527559054, best_f1=0.9741282339707535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.805911445757374e-05\n",
            "step: 10, loss: 8.03230213932693e-05\n",
            "step: 20, loss: 3.8694324757670984e-05\n",
            "step: 30, loss: 4.296541737858206e-05\n",
            "step: 40, loss: 4.6222055971156806e-05\n",
            "step: 50, loss: 7.290715439012274e-05\n",
            "step: 60, loss: 4.625307337846607e-05\n",
            "step: 70, loss: 4.6998764446470886e-05\n",
            "step: 80, loss: 5.813484312966466e-05\n",
            "step: 90, loss: 8.106313907774165e-05\n",
            "step: 100, loss: 3.078836380154826e-05\n",
            "step: 110, loss: 3.0702554795425385e-05\n",
            "step: 120, loss: 2.208652949775569e-05\n",
            "step: 130, loss: 3.435330290812999e-05\n",
            "step: 140, loss: 5.5412827350664884e-05\n",
            "step: 150, loss: 2.650804162840359e-05\n",
            "step: 160, loss: 0.00026512646581977606\n",
            "step: 170, loss: 2.571791446825955e-05\n",
            "step: 180, loss: 3.4732569474726915e-05\n",
            "step: 190, loss: 6.67126732878387e-05\n",
            "step: 200, loss: 2.8907048545079306e-05\n",
            "step: 210, loss: 3.724763519130647e-05\n",
            "step: 220, loss: 0.0005024602869525552\n",
            "step: 230, loss: 0.002683211350813508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9820627802690582, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2336203983286396e-05\n",
            "step: 10, loss: 1.725521906337235e-05\n",
            "step: 20, loss: 5.835137199028395e-05\n",
            "step: 30, loss: 4.068771522725001e-05\n",
            "step: 40, loss: 3.505358108668588e-05\n",
            "step: 50, loss: 0.019087743014097214\n",
            "step: 60, loss: 4.170200554654002e-05\n",
            "step: 70, loss: 4.8389443691121414e-05\n",
            "step: 80, loss: 3.9266084058908746e-05\n",
            "step: 90, loss: 7.143418042687699e-05\n",
            "step: 100, loss: 4.7877110773697495e-05\n",
            "step: 110, loss: 2.4165330614778213e-05\n",
            "step: 120, loss: 8.132861694321036e-05\n",
            "step: 130, loss: 3.336227382533252e-05\n",
            "step: 140, loss: 2.938030411314685e-05\n",
            "step: 150, loss: 0.0003452726814430207\n",
            "step: 160, loss: 2.1248530174489133e-05\n",
            "step: 170, loss: 3.1659019441576675e-05\n",
            "step: 180, loss: 3.940869646612555e-05\n",
            "step: 190, loss: 3.971316255046986e-05\n",
            "step: 200, loss: 5.121338108438067e-05\n",
            "step: 210, loss: 0.00014873757027089596\n",
            "step: 220, loss: 4.623709537554532e-05\n",
            "step: 230, loss: 3.242350430809893e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9820627802690582, f1=0.9753363228699552, best_f1=0.9752808988764046\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 199.48it/s]\n",
            "load_f1 = 0.9820224719101124\n",
            "real_f1 = 0.9820224719101124\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 247.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae45d34-e545-41be-be04-43c130364034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6353820562362671\n",
            "step: 10, loss: 0.5073322653770447\n",
            "step: 20, loss: 0.5153066515922546\n",
            "step: 30, loss: 0.15039213001728058\n",
            "step: 40, loss: 0.16093450784683228\n",
            "step: 50, loss: 0.12545731663703918\n",
            "step: 60, loss: 0.03644094988703728\n",
            "step: 70, loss: 0.13088616728782654\n",
            "step: 80, loss: 0.06656893342733383\n",
            "step: 90, loss: 0.2688131332397461\n",
            "step: 100, loss: 0.019345665350556374\n",
            "step: 110, loss: 0.09453622251749039\n",
            "step: 120, loss: 0.058634184300899506\n",
            "step: 130, loss: 0.07535288482904434\n",
            "step: 140, loss: 0.19131241738796234\n",
            "step: 150, loss: 0.056428734213113785\n",
            "step: 160, loss: 0.018291281536221504\n",
            "step: 170, loss: 0.1848556399345398\n",
            "step: 180, loss: 0.08468259871006012\n",
            "step: 190, loss: 0.009034608490765095\n",
            "step: 200, loss: 0.11882679909467697\n",
            "step: 210, loss: 0.07449843734502792\n",
            "step: 220, loss: 0.24364151060581207\n",
            "step: 230, loss: 0.14176180958747864\n",
            "step: 240, loss: 0.07434066385030746\n",
            "step: 250, loss: 0.01401505246758461\n",
            "step: 260, loss: 0.07387635111808777\n",
            "step: 270, loss: 0.043988361954689026\n",
            "step: 280, loss: 0.06754173338413239\n",
            "step: 290, loss: 0.04291051998734474\n",
            "step: 300, loss: 0.021390268579125404\n",
            "step: 310, loss: 0.1712079644203186\n",
            "step: 320, loss: 0.08941511064767838\n",
            "step: 330, loss: 0.010247890837490559\n",
            "step: 340, loss: 0.04027310013771057\n",
            "step: 350, loss: 0.11168801039457321\n",
            "step: 360, loss: 0.08799289911985397\n",
            "step: 370, loss: 0.07886168360710144\n",
            "step: 380, loss: 0.008352170698344707\n",
            "step: 390, loss: 0.17562159895896912\n",
            "step: 400, loss: 0.15892194211483002\n",
            "step: 410, loss: 0.06706181168556213\n",
            "step: 420, loss: 0.09830500930547714\n",
            "step: 430, loss: 0.11455438286066055\n",
            "step: 440, loss: 0.03454729914665222\n",
            "step: 450, loss: 0.0060407440178096294\n",
            "step: 460, loss: 0.022377699613571167\n",
            "step: 470, loss: 0.15100014209747314\n",
            "step: 480, loss: 0.08393893390893936\n",
            "step: 490, loss: 0.11040318012237549\n",
            "step: 500, loss: 0.09225517511367798\n",
            "step: 510, loss: 0.08995599299669266\n",
            "step: 520, loss: 0.18596521019935608\n",
            "step: 530, loss: 0.004845513496547937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9426038264115725, f1=0.931098696461825, best_f1=0.931098696461825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1382461041212082\n",
            "step: 10, loss: 0.04282284528017044\n",
            "step: 20, loss: 0.00938963983207941\n",
            "step: 30, loss: 0.00649282755330205\n",
            "step: 40, loss: 0.08714873343706131\n",
            "step: 50, loss: 0.13163097202777863\n",
            "step: 60, loss: 0.0028901593759655952\n",
            "step: 70, loss: 0.01797778159379959\n",
            "step: 80, loss: 0.04919851943850517\n",
            "step: 90, loss: 0.01585349254310131\n",
            "step: 100, loss: 0.010473194532096386\n",
            "step: 110, loss: 0.02514643222093582\n",
            "step: 120, loss: 0.056115422397851944\n",
            "step: 130, loss: 0.052064236253499985\n",
            "step: 140, loss: 0.015111136250197887\n",
            "step: 150, loss: 0.04238656908273697\n",
            "step: 160, loss: 0.03451288491487503\n",
            "step: 170, loss: 0.011808876879513264\n",
            "step: 180, loss: 0.09729896485805511\n",
            "step: 190, loss: 0.0488998144865036\n",
            "step: 200, loss: 0.013832363300025463\n",
            "step: 210, loss: 0.042775120586156845\n",
            "step: 220, loss: 0.02667832002043724\n",
            "step: 230, loss: 0.03642230108380318\n",
            "step: 240, loss: 0.029201103374361992\n",
            "step: 250, loss: 0.014055142179131508\n",
            "step: 260, loss: 0.013570050708949566\n",
            "step: 270, loss: 0.13080130517482758\n",
            "step: 280, loss: 0.041927237063646317\n",
            "step: 290, loss: 0.03934074193239212\n",
            "step: 300, loss: 0.19770373404026031\n",
            "step: 310, loss: 0.008122870698571205\n",
            "step: 320, loss: 0.051686957478523254\n",
            "step: 330, loss: 0.07382664084434509\n",
            "step: 340, loss: 0.011689122766256332\n",
            "step: 350, loss: 0.0037838867865502834\n",
            "step: 360, loss: 0.04693153500556946\n",
            "step: 370, loss: 0.1093669906258583\n",
            "step: 380, loss: 0.04576779901981354\n",
            "step: 390, loss: 0.07448209077119827\n",
            "step: 400, loss: 0.051840029656887054\n",
            "step: 410, loss: 0.27044054865837097\n",
            "step: 420, loss: 0.01757739670574665\n",
            "step: 430, loss: 0.12501665949821472\n",
            "step: 440, loss: 0.17302566766738892\n",
            "step: 450, loss: 0.08718055486679077\n",
            "step: 460, loss: 0.022783761844038963\n",
            "step: 470, loss: 0.09229721873998642\n",
            "step: 480, loss: 0.2050756812095642\n",
            "step: 490, loss: 0.008941804990172386\n",
            "step: 500, loss: 0.2092979997396469\n",
            "step: 510, loss: 0.029386015608906746\n",
            "step: 520, loss: 0.11341958492994308\n",
            "step: 530, loss: 0.010024160146713257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9424083769633508, f1=0.934844192634561, best_f1=0.931098696461825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026973038911819458\n",
            "step: 10, loss: 0.01353347860276699\n",
            "step: 20, loss: 0.07133316993713379\n",
            "step: 30, loss: 0.08935505151748657\n",
            "step: 40, loss: 0.0022523519583046436\n",
            "step: 50, loss: 0.1043737605214119\n",
            "step: 60, loss: 0.01703302189707756\n",
            "step: 70, loss: 0.003382388036698103\n",
            "step: 80, loss: 0.009248235262930393\n",
            "step: 90, loss: 0.007325091864913702\n",
            "step: 100, loss: 0.033457931131124496\n",
            "step: 110, loss: 0.005368509329855442\n",
            "step: 120, loss: 0.0011279876343905926\n",
            "step: 130, loss: 0.0010335531551390886\n",
            "step: 140, loss: 0.04606708139181137\n",
            "step: 150, loss: 0.031628020107746124\n",
            "step: 160, loss: 0.016788102686405182\n",
            "step: 170, loss: 0.03070908412337303\n",
            "step: 180, loss: 0.005069802049547434\n",
            "step: 190, loss: 0.001613510656170547\n",
            "step: 200, loss: 0.06937608122825623\n",
            "step: 210, loss: 0.025055408477783203\n",
            "step: 220, loss: 0.043246544897556305\n",
            "step: 230, loss: 0.09389092028141022\n",
            "step: 240, loss: 0.0030385071877390146\n",
            "step: 250, loss: 0.04488210752606392\n",
            "step: 260, loss: 0.04156753420829773\n",
            "step: 270, loss: 0.004339539911597967\n",
            "step: 280, loss: 0.11728541553020477\n",
            "step: 290, loss: 0.0010181983234360814\n",
            "step: 300, loss: 0.040120091289281845\n",
            "step: 310, loss: 0.0019953777082264423\n",
            "step: 320, loss: 0.034853361546993256\n",
            "step: 330, loss: 0.022413186728954315\n",
            "step: 340, loss: 0.0018643955700099468\n",
            "step: 350, loss: 0.005579223856329918\n",
            "step: 360, loss: 0.01886582002043724\n",
            "step: 370, loss: 0.005924299359321594\n",
            "step: 380, loss: 0.01682928390800953\n",
            "step: 390, loss: 0.025322940200567245\n",
            "step: 400, loss: 0.004004334565252066\n",
            "step: 410, loss: 0.005948617588728666\n",
            "step: 420, loss: 0.16766159236431122\n",
            "step: 430, loss: 0.0207191351801157\n",
            "step: 440, loss: 0.009951671585440636\n",
            "step: 450, loss: 0.05534409359097481\n",
            "step: 460, loss: 0.015997495502233505\n",
            "step: 470, loss: 0.06437166780233383\n",
            "step: 480, loss: 0.0018873566295951605\n",
            "step: 490, loss: 0.009788495488464832\n",
            "step: 500, loss: 0.003755688900128007\n",
            "step: 510, loss: 0.01047028973698616\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 520, loss: 0.07852314412593842\n",
            "step: 530, loss: 0.10372145473957062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9486348912540491, f1=0.9401947148817803, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005274541210383177\n",
            "step: 10, loss: 0.005834496114403009\n",
            "step: 20, loss: 0.0024789157323539257\n",
            "step: 30, loss: 0.0005424150731414557\n",
            "step: 40, loss: 0.002455309731885791\n",
            "step: 50, loss: 0.014696570113301277\n",
            "step: 60, loss: 0.0008020129753276706\n",
            "step: 70, loss: 0.002359675941988826\n",
            "step: 80, loss: 0.0004153094196226448\n",
            "step: 90, loss: 0.0968398004770279\n",
            "step: 100, loss: 0.006376064382493496\n",
            "step: 110, loss: 0.014803450554609299\n",
            "step: 120, loss: 0.00038057309575378895\n",
            "step: 130, loss: 0.0006806051242165267\n",
            "step: 140, loss: 0.0038640943821519613\n",
            "step: 150, loss: 0.12534455955028534\n",
            "step: 160, loss: 0.018504032865166664\n",
            "step: 170, loss: 0.005357031244784594\n",
            "step: 180, loss: 0.020235951989889145\n",
            "step: 190, loss: 0.0358971431851387\n",
            "step: 200, loss: 0.04170818626880646\n",
            "step: 210, loss: 0.01401173323392868\n",
            "step: 220, loss: 0.0008343904628418386\n",
            "step: 230, loss: 0.0027725910767912865\n",
            "step: 240, loss: 0.0070181540213525295\n",
            "step: 250, loss: 0.0018852397333830595\n",
            "step: 260, loss: 0.0006217756308615208\n",
            "step: 270, loss: 0.010626215487718582\n",
            "step: 280, loss: 0.0147246690467\n",
            "step: 290, loss: 0.012536974623799324\n",
            "step: 300, loss: 0.0001705121248960495\n",
            "step: 310, loss: 0.002860068343579769\n",
            "step: 320, loss: 0.05913637951016426\n",
            "step: 330, loss: 0.061753977090120316\n",
            "step: 340, loss: 0.016199981793761253\n",
            "step: 350, loss: 0.0035879190545529127\n",
            "step: 360, loss: 0.014963899739086628\n",
            "step: 370, loss: 0.0020500035025179386\n",
            "step: 380, loss: 0.01039524283260107\n",
            "step: 390, loss: 0.0029197654221206903\n",
            "step: 400, loss: 0.0010237162932753563\n",
            "step: 410, loss: 0.0012848018668591976\n",
            "step: 420, loss: 0.04210466518998146\n",
            "step: 430, loss: 0.0020362737122923136\n",
            "step: 440, loss: 0.0010059298947453499\n",
            "step: 450, loss: 0.003738450352102518\n",
            "step: 460, loss: 0.00017922790721058846\n",
            "step: 470, loss: 0.0002772084844764322\n",
            "step: 480, loss: 0.008122620172798634\n",
            "step: 490, loss: 0.003852258436381817\n",
            "step: 500, loss: 0.00922660157084465\n",
            "step: 510, loss: 0.01657610386610031\n",
            "step: 520, loss: 0.11054232716560364\n",
            "step: 530, loss: 0.01507687009871006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9448529411764707, f1=0.932904411764706, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02690829150378704\n",
            "step: 10, loss: 0.005277940537780523\n",
            "step: 20, loss: 0.0029886991251260042\n",
            "step: 30, loss: 0.0007077534100972116\n",
            "step: 40, loss: 0.007650560699403286\n",
            "step: 50, loss: 0.002951163798570633\n",
            "step: 60, loss: 0.025746984407305717\n",
            "step: 70, loss: 0.0008522020652890205\n",
            "step: 80, loss: 0.00047073618043214083\n",
            "step: 90, loss: 0.08391214907169342\n",
            "step: 100, loss: 0.0016375056002289057\n",
            "step: 110, loss: 0.0008560010464861989\n",
            "step: 120, loss: 0.010891750454902649\n",
            "step: 130, loss: 0.00027434006915427744\n",
            "step: 140, loss: 0.003428451484069228\n",
            "step: 150, loss: 0.05398145690560341\n",
            "step: 160, loss: 0.0005891474429517984\n",
            "step: 170, loss: 0.05842139199376106\n",
            "step: 180, loss: 0.000568204908631742\n",
            "step: 190, loss: 0.01885947398841381\n",
            "step: 200, loss: 0.0005700523033738136\n",
            "step: 210, loss: 0.024888312444090843\n",
            "step: 220, loss: 0.0002916379307862371\n",
            "step: 230, loss: 0.006371904630213976\n",
            "step: 240, loss: 0.0016624287236481905\n",
            "step: 250, loss: 0.0035772621631622314\n",
            "step: 260, loss: 0.00088829844025895\n",
            "step: 270, loss: 0.00013153011968825012\n",
            "step: 280, loss: 0.008024878799915314\n",
            "step: 290, loss: 0.12232472002506256\n",
            "step: 300, loss: 0.0004592206096276641\n",
            "step: 310, loss: 0.0005260832840576768\n",
            "step: 320, loss: 0.17919808626174927\n",
            "step: 330, loss: 0.00777842104434967\n",
            "step: 340, loss: 0.007004708517342806\n",
            "step: 350, loss: 0.0006538442685268819\n",
            "step: 360, loss: 0.0014238671865314245\n",
            "step: 370, loss: 0.0028367529157549143\n",
            "step: 380, loss: 0.00015006140165496618\n",
            "step: 390, loss: 0.00023163309379015118\n",
            "step: 400, loss: 0.004644307307898998\n",
            "step: 410, loss: 0.005987347569316626\n",
            "step: 420, loss: 0.0029949755407869816\n",
            "step: 430, loss: 0.005963711533695459\n",
            "step: 440, loss: 0.08908254653215408\n",
            "step: 450, loss: 0.0002991277433466166\n",
            "step: 460, loss: 0.012435010634362698\n",
            "step: 470, loss: 0.15635254979133606\n",
            "step: 480, loss: 0.0014599986607208848\n",
            "step: 490, loss: 0.00021994076087139547\n",
            "step: 500, loss: 0.002943366067484021\n",
            "step: 510, loss: 0.22917890548706055\n",
            "step: 520, loss: 0.0009126780205406249\n",
            "step: 530, loss: 0.0009423285955563188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9435559736594544, f1=0.9339578454332552, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03293495252728462\n",
            "step: 10, loss: 0.00016668772150296718\n",
            "step: 20, loss: 0.045897312462329865\n",
            "step: 30, loss: 0.0013401622418314219\n",
            "step: 40, loss: 0.007644464261829853\n",
            "step: 50, loss: 0.003346678800880909\n",
            "step: 60, loss: 8.529049955541268e-05\n",
            "step: 70, loss: 0.00016514380695298314\n",
            "step: 80, loss: 0.0052136084996163845\n",
            "step: 90, loss: 0.000201104543521069\n",
            "step: 100, loss: 0.035836197435855865\n",
            "step: 110, loss: 0.00038756339927203953\n",
            "step: 120, loss: 0.0001316186971962452\n",
            "step: 130, loss: 0.00019732116197701544\n",
            "step: 140, loss: 0.004121164791285992\n",
            "step: 150, loss: 0.09646855294704437\n",
            "step: 160, loss: 0.0022789195645600557\n",
            "step: 170, loss: 0.0008673936245031655\n",
            "step: 180, loss: 0.0005508067552000284\n",
            "step: 190, loss: 0.023555072024464607\n",
            "step: 200, loss: 0.00038024623063392937\n",
            "step: 210, loss: 0.0024668346159160137\n",
            "step: 220, loss: 0.0018651463324204087\n",
            "step: 230, loss: 0.01827620342373848\n",
            "step: 240, loss: 0.017849434167146683\n",
            "step: 250, loss: 0.0009262178791686893\n",
            "step: 260, loss: 0.0003236375341657549\n",
            "step: 270, loss: 0.015644496306777\n",
            "step: 280, loss: 0.000316145655233413\n",
            "step: 290, loss: 0.0004789275408256799\n",
            "step: 300, loss: 0.0011101554846391082\n",
            "step: 310, loss: 0.0014864258700981736\n",
            "step: 320, loss: 0.0001013902947306633\n",
            "step: 330, loss: 0.00014678901061415672\n",
            "step: 340, loss: 0.09548979252576828\n",
            "step: 350, loss: 0.020293626934289932\n",
            "step: 360, loss: 0.002118331380188465\n",
            "step: 370, loss: 0.0321071557700634\n",
            "step: 380, loss: 0.0015992731787264347\n",
            "step: 390, loss: 0.027896979823708534\n",
            "step: 400, loss: 0.00024721273803152144\n",
            "step: 410, loss: 0.0011940234107896686\n",
            "step: 420, loss: 0.0035162654239684343\n",
            "step: 430, loss: 0.00021738688519690186\n",
            "step: 440, loss: 4.677666584029794e-05\n",
            "step: 450, loss: 0.0030987542122602463\n",
            "step: 460, loss: 0.00020355143351480365\n",
            "step: 470, loss: 0.006947772111743689\n",
            "step: 480, loss: 0.0008791904547251761\n",
            "step: 490, loss: 0.0065558538772165775\n",
            "step: 500, loss: 0.00021911396470386535\n",
            "step: 510, loss: 0.0013459998881444335\n",
            "step: 520, loss: 0.0020270319655537605\n",
            "step: 530, loss: 0.028129318729043007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9341603053435114, f1=0.9190225203641591, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006018589250743389\n",
            "step: 10, loss: 0.001219673315063119\n",
            "step: 20, loss: 0.0012357874074950814\n",
            "step: 30, loss: 0.0007995617925189435\n",
            "step: 40, loss: 0.0002510058111511171\n",
            "step: 50, loss: 0.0008937724051065743\n",
            "step: 60, loss: 0.002238925313577056\n",
            "step: 70, loss: 0.00361591181717813\n",
            "step: 80, loss: 0.006260947789996862\n",
            "step: 90, loss: 0.001266620820388198\n",
            "step: 100, loss: 8.773045556154102e-05\n",
            "step: 110, loss: 0.00038897997001186013\n",
            "step: 120, loss: 0.00013710049097426236\n",
            "step: 130, loss: 0.011114098131656647\n",
            "step: 140, loss: 4.2537514673313126e-05\n",
            "step: 150, loss: 0.1036568284034729\n",
            "step: 160, loss: 0.0004183700948487967\n",
            "step: 170, loss: 0.04061039909720421\n",
            "step: 180, loss: 0.0007281215512193739\n",
            "step: 190, loss: 0.001299054827541113\n",
            "step: 200, loss: 0.00012522809265647084\n",
            "step: 210, loss: 0.0013548467541113496\n",
            "step: 220, loss: 0.1000390574336052\n",
            "step: 230, loss: 0.00023382318613585085\n",
            "step: 240, loss: 0.000261968991253525\n",
            "step: 250, loss: 0.00405695429071784\n",
            "step: 260, loss: 0.0003027759084943682\n",
            "step: 270, loss: 0.0002757013135123998\n",
            "step: 280, loss: 0.007824975065886974\n",
            "step: 290, loss: 0.0007776169804856181\n",
            "step: 300, loss: 0.0013496385654434562\n",
            "step: 310, loss: 7.480932981707156e-05\n",
            "step: 320, loss: 8.888135198503733e-05\n",
            "step: 330, loss: 0.004776468500494957\n",
            "step: 340, loss: 0.01853233389556408\n",
            "step: 350, loss: 0.0014049278106540442\n",
            "step: 360, loss: 0.0006410877103917301\n",
            "step: 370, loss: 5.010610038880259e-05\n",
            "step: 380, loss: 0.0022375828120857477\n",
            "step: 390, loss: 6.158179166959599e-05\n",
            "step: 400, loss: 0.005376606713980436\n",
            "step: 410, loss: 0.0016596984351053834\n",
            "step: 420, loss: 0.0013332524104043841\n",
            "step: 430, loss: 0.00019065593369305134\n",
            "step: 440, loss: 0.00017855939222499728\n",
            "step: 450, loss: 0.004294349811971188\n",
            "step: 460, loss: 0.001194715267047286\n",
            "step: 470, loss: 0.0009935699636116624\n",
            "step: 480, loss: 0.003625516314059496\n",
            "step: 490, loss: 0.0002545539173297584\n",
            "step: 500, loss: 0.0006960559985600412\n",
            "step: 510, loss: 0.0028226228896528482\n",
            "step: 520, loss: 0.059824489057064056\n",
            "step: 530, loss: 0.003643846372142434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9408424041646947, f1=0.9248826291079812, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012469510547816753\n",
            "step: 10, loss: 0.017039325088262558\n",
            "step: 20, loss: 0.03591729328036308\n",
            "step: 30, loss: 0.0006198594346642494\n",
            "step: 40, loss: 4.391962647787295e-05\n",
            "step: 50, loss: 0.003858183976262808\n",
            "step: 60, loss: 0.0002707894891500473\n",
            "step: 70, loss: 4.259733032085933e-05\n",
            "step: 80, loss: 0.0005706443334929645\n",
            "step: 90, loss: 0.0003449819050729275\n",
            "step: 100, loss: 6.920587475178763e-05\n",
            "step: 110, loss: 0.00010078279592562467\n",
            "step: 120, loss: 0.0008436771458946168\n",
            "step: 130, loss: 0.0001361678441753611\n",
            "step: 140, loss: 0.007891563698649406\n",
            "step: 150, loss: 4.2253912397427484e-05\n",
            "step: 160, loss: 0.001836365438066423\n",
            "step: 170, loss: 0.011963286437094212\n",
            "step: 180, loss: 3.6427954910323024e-05\n",
            "step: 190, loss: 0.001317257178016007\n",
            "step: 200, loss: 0.0003531394468154758\n",
            "step: 210, loss: 0.00031465551001019776\n",
            "step: 220, loss: 0.00028976122848689556\n",
            "step: 230, loss: 0.004298623185604811\n",
            "step: 240, loss: 0.0003271752502769232\n",
            "step: 250, loss: 2.4523018510080874e-05\n",
            "step: 260, loss: 7.289511995622888e-05\n",
            "step: 270, loss: 9.458241402171552e-05\n",
            "step: 280, loss: 0.0031380439177155495\n",
            "step: 290, loss: 0.015512421727180481\n",
            "step: 300, loss: 0.0037424112670123577\n",
            "step: 310, loss: 0.0007136098574846983\n",
            "step: 320, loss: 0.013454732485115528\n",
            "step: 330, loss: 0.0010756156407296658\n",
            "step: 340, loss: 0.0002160482108592987\n",
            "step: 350, loss: 6.554000719916075e-05\n",
            "step: 360, loss: 0.0034772194921970367\n",
            "step: 370, loss: 7.36572255846113e-05\n",
            "step: 380, loss: 0.0002656869764905423\n",
            "step: 390, loss: 0.13230346143245697\n",
            "step: 400, loss: 6.732217298122123e-05\n",
            "step: 410, loss: 0.00021098082652315497\n",
            "step: 420, loss: 0.0011408058926463127\n",
            "step: 430, loss: 0.015530903823673725\n",
            "step: 440, loss: 0.00026790291303768754\n",
            "step: 450, loss: 0.00018966877541970462\n",
            "step: 460, loss: 8.511007763445377e-05\n",
            "step: 470, loss: 0.00011938872921746224\n",
            "step: 480, loss: 0.05506008118391037\n",
            "step: 490, loss: 0.018917711451649666\n",
            "step: 500, loss: 0.13796184957027435\n",
            "step: 510, loss: 0.00018012127839028835\n",
            "step: 520, loss: 0.0017553012585267425\n",
            "step: 530, loss: 0.0005390428123064339\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9422098936662043, f1=0.933456561922366, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0054887086153030396\n",
            "step: 10, loss: 0.0005412689642980695\n",
            "step: 20, loss: 0.0006372471107169986\n",
            "step: 30, loss: 0.0025481784250587225\n",
            "step: 40, loss: 0.0002704368671402335\n",
            "step: 50, loss: 0.0001093065511668101\n",
            "step: 60, loss: 0.00010316498810425401\n",
            "step: 70, loss: 0.03951750323176384\n",
            "step: 80, loss: 0.00032251968514174223\n",
            "step: 90, loss: 0.0025940993800759315\n",
            "step: 100, loss: 5.8648016420193017e-05\n",
            "step: 110, loss: 4.571415047394112e-05\n",
            "step: 120, loss: 0.00021889664640184492\n",
            "step: 130, loss: 0.02426040917634964\n",
            "step: 140, loss: 0.006439877673983574\n",
            "step: 150, loss: 0.004265526309609413\n",
            "step: 160, loss: 0.0018289678264409304\n",
            "step: 170, loss: 0.0029739034362137318\n",
            "step: 180, loss: 0.00012874904496129602\n",
            "step: 190, loss: 0.0002014017227338627\n",
            "step: 200, loss: 0.0001069519275915809\n",
            "step: 210, loss: 5.47153758816421e-05\n",
            "step: 220, loss: 0.006548506207764149\n",
            "step: 230, loss: 0.0005538287223316729\n",
            "step: 240, loss: 0.00023727634106762707\n",
            "step: 250, loss: 0.00026774313300848007\n",
            "step: 260, loss: 0.027461666613817215\n",
            "step: 270, loss: 0.00062328897183761\n",
            "step: 280, loss: 0.0017716671572998166\n",
            "step: 290, loss: 0.015231437981128693\n",
            "step: 300, loss: 0.00010885237134061754\n",
            "step: 310, loss: 0.013233541510999203\n",
            "step: 320, loss: 0.013690668158233166\n",
            "step: 330, loss: 0.0010528757702559233\n",
            "step: 340, loss: 0.00010038320760941133\n",
            "step: 350, loss: 0.003679159563034773\n",
            "step: 360, loss: 2.6493225959711708e-05\n",
            "step: 370, loss: 4.1373667045263574e-05\n",
            "step: 380, loss: 7.623912097187713e-05\n",
            "step: 390, loss: 8.115961099974811e-05\n",
            "step: 400, loss: 0.016181768849492073\n",
            "step: 410, loss: 7.52375417505391e-05\n",
            "step: 420, loss: 4.232585706631653e-05\n",
            "step: 430, loss: 5.6295386457350105e-05\n",
            "step: 440, loss: 0.07160913944244385\n",
            "step: 450, loss: 6.363869033521041e-05\n",
            "step: 460, loss: 0.00013075330934952945\n",
            "step: 470, loss: 2.8940294214407913e-05\n",
            "step: 480, loss: 0.0002064796135528013\n",
            "step: 490, loss: 0.00011401755182305351\n",
            "step: 500, loss: 0.00043488069786690176\n",
            "step: 510, loss: 0.00021001407003495842\n",
            "step: 520, loss: 4.645061198971234e-05\n",
            "step: 530, loss: 8.318448817590252e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9472234970169803, f1=0.9386446886446886, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.449840369285084e-05\n",
            "step: 10, loss: 0.00016442347259726375\n",
            "step: 20, loss: 2.8384971301420592e-05\n",
            "step: 30, loss: 3.296367867733352e-05\n",
            "step: 40, loss: 8.423109102295712e-05\n",
            "step: 50, loss: 5.858329313923605e-05\n",
            "step: 60, loss: 3.86440078727901e-05\n",
            "step: 70, loss: 4.026140959467739e-05\n",
            "step: 80, loss: 2.429202686471399e-05\n",
            "step: 90, loss: 3.660992661025375e-05\n",
            "step: 100, loss: 2.5676939912955277e-05\n",
            "step: 110, loss: 1.67709822562756e-05\n",
            "step: 120, loss: 4.91226019221358e-05\n",
            "step: 130, loss: 2.8880962418043055e-05\n",
            "step: 140, loss: 0.0008548669866286218\n",
            "step: 150, loss: 3.9940394344739616e-05\n",
            "step: 160, loss: 2.8072663553757593e-05\n",
            "step: 170, loss: 0.00038763682823628187\n",
            "step: 180, loss: 7.927114347694442e-05\n",
            "step: 190, loss: 5.096866880194284e-05\n",
            "step: 200, loss: 0.00035672783269546926\n",
            "step: 210, loss: 0.00037670941674150527\n",
            "step: 220, loss: 8.356005855603144e-05\n",
            "step: 230, loss: 0.00034978415351361036\n",
            "step: 240, loss: 8.903434354579076e-05\n",
            "step: 250, loss: 2.2328815248329192e-05\n",
            "step: 260, loss: 0.003391255158931017\n",
            "step: 270, loss: 0.007168347015976906\n",
            "step: 280, loss: 4.0166782127926126e-05\n",
            "step: 290, loss: 2.393063550698571e-05\n",
            "step: 300, loss: 0.0006760630640201271\n",
            "step: 310, loss: 3.909685619873926e-05\n",
            "step: 320, loss: 0.0005643125041387975\n",
            "step: 330, loss: 2.8425791242625564e-05\n",
            "step: 340, loss: 4.9359066906617954e-05\n",
            "step: 350, loss: 5.7686262152856216e-05\n",
            "step: 360, loss: 2.682130980247166e-05\n",
            "step: 370, loss: 0.0019289783667773008\n",
            "step: 380, loss: 7.284917955985293e-05\n",
            "step: 390, loss: 8.142585284076631e-05\n",
            "step: 400, loss: 0.0004800416063517332\n",
            "step: 410, loss: 0.0023058729711920023\n",
            "step: 420, loss: 9.701358794700354e-05\n",
            "step: 430, loss: 1.9520015484886244e-05\n",
            "step: 440, loss: 0.0027873008511960506\n",
            "step: 450, loss: 3.909412043867633e-05\n",
            "step: 460, loss: 2.7234094886807725e-05\n",
            "step: 470, loss: 0.00033011811319738626\n",
            "step: 480, loss: 3.067274155910127e-05\n",
            "step: 490, loss: 0.000648816698230803\n",
            "step: 500, loss: 0.0044991374015808105\n",
            "step: 510, loss: 3.0166127544362098e-05\n",
            "step: 520, loss: 0.00021119943994563073\n",
            "step: 530, loss: 8.578428969485685e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9473684210526316, f1=0.9348025711662074, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010249847400700673\n",
            "step: 10, loss: 4.494798122323118e-05\n",
            "step: 20, loss: 2.619148654048331e-05\n",
            "step: 30, loss: 0.00011854506738018245\n",
            "step: 40, loss: 5.948047328274697e-05\n",
            "step: 50, loss: 9.574840078130364e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.016935577616095543\n",
            "step: 70, loss: 2.947289431176614e-05\n",
            "step: 80, loss: 2.6459427317604423e-05\n",
            "step: 90, loss: 3.725697024492547e-05\n",
            "step: 100, loss: 3.1516105082118884e-05\n",
            "step: 110, loss: 3.7495585274882615e-05\n",
            "step: 120, loss: 3.8400350604206324e-05\n",
            "step: 130, loss: 2.825785122695379e-05\n",
            "step: 140, loss: 0.00011430826270952821\n",
            "step: 150, loss: 1.3489059710991569e-05\n",
            "step: 160, loss: 6.544915959239006e-05\n",
            "step: 170, loss: 3.130220648017712e-05\n",
            "step: 180, loss: 0.00010030931298388168\n",
            "step: 190, loss: 8.411310409428552e-05\n",
            "step: 200, loss: 0.0006753912894055247\n",
            "step: 210, loss: 0.018447691574692726\n",
            "step: 220, loss: 0.0012234754394739866\n",
            "step: 230, loss: 1.3422049050859641e-05\n",
            "step: 240, loss: 0.00022787631314713508\n",
            "step: 250, loss: 1.7798873159335926e-05\n",
            "step: 260, loss: 2.6682824682211503e-05\n",
            "step: 270, loss: 4.521777736954391e-05\n",
            "step: 280, loss: 4.636115409084596e-05\n",
            "step: 290, loss: 0.0007126291748136282\n",
            "step: 300, loss: 8.57700506458059e-05\n",
            "step: 310, loss: 4.9883958126883954e-05\n",
            "step: 320, loss: 0.0002644156338647008\n",
            "step: 330, loss: 0.0004947728593833745\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 340, loss: 0.00026775497826747596\n",
            "step: 350, loss: 5.8861947763944045e-05\n",
            "step: 360, loss: 1.3630636203743052e-05\n",
            "step: 370, loss: 0.0007160392706282437\n",
            "step: 380, loss: 0.00015631093992851675\n",
            "step: 390, loss: 1.6275575035251677e-05\n",
            "step: 400, loss: 0.0018005073070526123\n",
            "step: 410, loss: 5.332216824172065e-05\n",
            "step: 420, loss: 7.520632789237425e-05\n",
            "step: 430, loss: 2.4883811420295388e-05\n",
            "step: 440, loss: 0.20726759731769562\n",
            "step: 450, loss: 0.0006856822874397039\n",
            "step: 460, loss: 0.026437869295477867\n",
            "step: 470, loss: 0.00010378185106674209\n",
            "step: 480, loss: 0.0015688029816374183\n",
            "step: 490, loss: 0.00021011399803683162\n",
            "step: 500, loss: 0.00022380526934284717\n",
            "step: 510, loss: 8.874233026290312e-05\n",
            "step: 520, loss: 7.566285785287619e-05\n",
            "step: 530, loss: 6.233822205103934e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9323809523809523, f1=0.9241706161137442, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012748801964335144\n",
            "step: 10, loss: 0.0003306334256194532\n",
            "step: 20, loss: 0.00015423331933561713\n",
            "step: 30, loss: 0.005069484002888203\n",
            "step: 40, loss: 0.0004074301104992628\n",
            "step: 50, loss: 0.03755149617791176\n",
            "step: 60, loss: 0.0024532934185117483\n",
            "step: 70, loss: 0.00022802315652370453\n",
            "step: 80, loss: 5.5877160775708035e-05\n",
            "step: 90, loss: 3.351490886416286e-05\n",
            "step: 100, loss: 5.580268407356925e-05\n",
            "step: 110, loss: 4.5789696741849184e-05\n",
            "step: 120, loss: 3.7590114516206086e-05\n",
            "step: 130, loss: 0.0007638622191734612\n",
            "step: 140, loss: 5.702471753465943e-05\n",
            "step: 150, loss: 2.4683020455995575e-05\n",
            "step: 160, loss: 4.239724512444809e-05\n",
            "step: 170, loss: 0.00012495627743192017\n",
            "step: 180, loss: 0.00017727568047121167\n",
            "step: 190, loss: 4.177571463515051e-05\n",
            "step: 200, loss: 0.0010929268319159746\n",
            "step: 210, loss: 5.963205694570206e-05\n",
            "step: 220, loss: 0.0001239460543729365\n",
            "step: 230, loss: 1.9844019334414043e-05\n",
            "step: 240, loss: 0.00014038715744391084\n",
            "step: 250, loss: 2.8332908186712302e-05\n",
            "step: 260, loss: 3.917489084415138e-05\n",
            "step: 270, loss: 0.0003986618830822408\n",
            "step: 280, loss: 0.0006863302551209927\n",
            "step: 290, loss: 3.961529000662267e-05\n",
            "step: 300, loss: 0.00018224258383270353\n",
            "step: 310, loss: 3.509996895445511e-05\n",
            "step: 320, loss: 0.01807383820414543\n",
            "step: 330, loss: 3.468065187917091e-05\n",
            "step: 340, loss: 2.7043104637414217e-05\n",
            "step: 350, loss: 0.00013249267067294568\n",
            "step: 360, loss: 0.0001757764839567244\n",
            "step: 370, loss: 0.00030327768763527274\n",
            "step: 380, loss: 0.0004107895656488836\n",
            "step: 390, loss: 0.0011632759124040604\n",
            "step: 400, loss: 2.9968630769872107e-05\n",
            "step: 410, loss: 2.4478009436279535e-05\n",
            "step: 420, loss: 0.0033440415281802416\n",
            "step: 430, loss: 0.0004159002855885774\n",
            "step: 440, loss: 2.2187006834428757e-05\n",
            "step: 450, loss: 0.0005256679141893983\n",
            "step: 460, loss: 2.688414133444894e-05\n",
            "step: 470, loss: 2.118138763762545e-05\n",
            "step: 480, loss: 0.0033593943808227777\n",
            "step: 490, loss: 0.00015152814739849418\n",
            "step: 500, loss: 3.100975663983263e-05\n",
            "step: 510, loss: 0.00012776031508110464\n",
            "step: 520, loss: 1.9464254364720546e-05\n",
            "step: 530, loss: 3.171392745571211e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9440820130475303, f1=0.9346314325452018, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004860812332481146\n",
            "step: 10, loss: 2.57026022154605e-05\n",
            "step: 20, loss: 2.1478934286278673e-05\n",
            "step: 30, loss: 3.690328958327882e-05\n",
            "step: 40, loss: 1.6152629541466013e-05\n",
            "step: 50, loss: 0.0015133762499317527\n",
            "step: 60, loss: 2.0484436390688643e-05\n",
            "step: 70, loss: 3.3895103115355596e-05\n",
            "step: 80, loss: 1.5884352251305245e-05\n",
            "step: 90, loss: 0.00029343675123527646\n",
            "step: 100, loss: 0.00012904488539788872\n",
            "step: 110, loss: 3.529953028191812e-05\n",
            "step: 120, loss: 4.2891395423794165e-05\n",
            "step: 130, loss: 0.007245530374348164\n",
            "step: 140, loss: 6.681526429019868e-05\n",
            "step: 150, loss: 0.0007763320463709533\n",
            "step: 160, loss: 1.799279561964795e-05\n",
            "step: 170, loss: 4.758935756399296e-05\n",
            "step: 180, loss: 1.965046612895094e-05\n",
            "step: 190, loss: 3.278647636761889e-05\n",
            "step: 200, loss: 1.4088885109231342e-05\n",
            "step: 210, loss: 5.199732186156325e-05\n",
            "step: 220, loss: 1.3191067409934476e-05\n",
            "step: 230, loss: 1.6711046555428766e-05\n",
            "step: 240, loss: 2.5327361072413623e-05\n",
            "step: 250, loss: 1.2274712389626075e-05\n",
            "step: 260, loss: 1.273289217351703e-05\n",
            "step: 270, loss: 1.6859952665981837e-05\n",
            "step: 280, loss: 1.4472572729573585e-05\n",
            "step: 290, loss: 1.7474641936132684e-05\n",
            "step: 300, loss: 2.4667875550221652e-05\n",
            "step: 310, loss: 0.0003268296713940799\n",
            "step: 320, loss: 0.002734582172706723\n",
            "step: 330, loss: 0.00013265541929285973\n",
            "step: 340, loss: 0.0007047271938063204\n",
            "step: 350, loss: 1.6893896827241406e-05\n",
            "step: 360, loss: 3.153066791128367e-05\n",
            "step: 370, loss: 2.0287414372432977e-05\n",
            "step: 380, loss: 1.846193481469527e-05\n",
            "step: 390, loss: 0.00015700125368312\n",
            "step: 400, loss: 1.3887690329283942e-05\n",
            "step: 410, loss: 3.9807400753488764e-05\n",
            "step: 420, loss: 4.2745414248201996e-05\n",
            "step: 430, loss: 1.4155933058646042e-05\n",
            "step: 440, loss: 3.9223239582497627e-05\n",
            "step: 450, loss: 2.1508931240532547e-05\n",
            "step: 460, loss: 0.0005592386587522924\n",
            "step: 470, loss: 2.0606801626854576e-05\n",
            "step: 480, loss: 1.3742407645622734e-05\n",
            "step: 490, loss: 1.4461425053013954e-05\n",
            "step: 500, loss: 1.8259677744936198e-05\n",
            "step: 510, loss: 1.2889349818578921e-05\n",
            "step: 520, loss: 0.0006576563464477658\n",
            "step: 530, loss: 1.3343795217224397e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.946146703806871, f1=0.937471051412691, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002313100703759119\n",
            "step: 10, loss: 1.0415838005428668e-05\n",
            "step: 20, loss: 1.138811967393849e-05\n",
            "step: 30, loss: 1.5385239748866297e-05\n",
            "step: 40, loss: 7.112858293112367e-05\n",
            "step: 50, loss: 2.462343945808243e-05\n",
            "step: 60, loss: 5.899511597817764e-05\n",
            "step: 70, loss: 2.214920641563367e-05\n",
            "step: 80, loss: 2.2123087546788156e-05\n",
            "step: 90, loss: 1.3410874089458957e-05\n",
            "step: 100, loss: 0.00013831595424562693\n",
            "step: 110, loss: 1.1976701898674946e-05\n",
            "step: 120, loss: 4.7129447921179235e-05\n",
            "step: 130, loss: 0.015332682058215141\n",
            "step: 140, loss: 0.000361207639798522\n",
            "step: 150, loss: 1.033387343341019e-05\n",
            "step: 160, loss: 0.0016584087861701846\n",
            "step: 170, loss: 2.3833063096390106e-05\n",
            "step: 180, loss: 1.659208101045806e-05\n",
            "step: 190, loss: 1.4994007869972847e-05\n",
            "step: 200, loss: 0.0002659619494806975\n",
            "step: 210, loss: 1.637603600102011e-05\n",
            "step: 220, loss: 1.6495267118443735e-05\n",
            "step: 230, loss: 1.980308115889784e-05\n",
            "step: 240, loss: 1.401432109560119e-05\n",
            "step: 250, loss: 1.6286705431411974e-05\n",
            "step: 260, loss: 2.000786116695963e-05\n",
            "step: 270, loss: 1.173453620140208e-05\n",
            "step: 280, loss: 1.6770696674939245e-05\n",
            "step: 290, loss: 4.055539466207847e-05\n",
            "step: 300, loss: 1.7806582036428154e-05\n",
            "step: 310, loss: 4.587781586451456e-05\n",
            "step: 320, loss: 0.0011446921853348613\n",
            "step: 330, loss: 0.00024714009487070143\n",
            "step: 340, loss: 3.924695556634106e-05\n",
            "step: 350, loss: 2.757657966867555e-05\n",
            "step: 360, loss: 7.288731285370886e-05\n",
            "step: 370, loss: 1.5374080248875543e-05\n",
            "step: 380, loss: 1.571305256220512e-05\n",
            "step: 390, loss: 0.0028396009001880884\n",
            "step: 400, loss: 0.0008237042347900569\n",
            "step: 410, loss: 1.446877922717249e-05\n",
            "step: 420, loss: 1.2408824659360107e-05\n",
            "step: 430, loss: 1.579130912432447e-05\n",
            "step: 440, loss: 0.0006547419470734894\n",
            "step: 450, loss: 3.681727685034275e-05\n",
            "step: 460, loss: 2.38809225265868e-05\n",
            "step: 470, loss: 1.442780103388941e-05\n",
            "step: 480, loss: 2.4011773348320276e-05\n",
            "step: 490, loss: 3.51540838892106e-05\n",
            "step: 500, loss: 1.3358500837057363e-05\n",
            "step: 510, loss: 0.0006918307044543326\n",
            "step: 520, loss: 3.850773282465525e-05\n",
            "step: 530, loss: 0.0008538544061593711\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9430970149253732, f1=0.9360518999073215, best_f1=0.9401947148817803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.4342174836201593e-05\n",
            "step: 10, loss: 1.6077954569482245e-05\n",
            "step: 20, loss: 1.1332217582094017e-05\n",
            "step: 30, loss: 5.695839718100615e-05\n",
            "step: 40, loss: 0.013987665995955467\n",
            "step: 50, loss: 4.106436972506344e-05\n",
            "step: 60, loss: 1.2181543752376456e-05\n",
            "step: 70, loss: 1.681537469266914e-05\n",
            "step: 80, loss: 0.0002659653837326914\n",
            "step: 90, loss: 1.3414629393082578e-05\n",
            "step: 100, loss: 9.609190601622686e-05\n",
            "step: 110, loss: 5.09257115481887e-05\n",
            "step: 120, loss: 1.892366708489135e-05\n",
            "step: 130, loss: 0.07298208028078079\n",
            "step: 140, loss: 0.009320687502622604\n",
            "step: 150, loss: 1.747137139318511e-05\n",
            "step: 160, loss: 1.3071902685624082e-05\n",
            "step: 170, loss: 1.3302837032824755e-05\n",
            "step: 180, loss: 5.558510747505352e-05\n",
            "step: 190, loss: 1.661824899201747e-05\n",
            "step: 200, loss: 1.4137307516648434e-05\n",
            "step: 210, loss: 4.534135587164201e-05\n",
            "step: 220, loss: 1.2822295502701309e-05\n",
            "step: 230, loss: 1.651758429943584e-05\n",
            "step: 240, loss: 1.3407170627033338e-05\n",
            "step: 250, loss: 3.527300577843562e-05\n",
            "step: 260, loss: 1.4811521396040916e-05\n",
            "step: 270, loss: 1.5731537132523954e-05\n",
            "step: 280, loss: 1.3816941645927727e-05\n",
            "step: 290, loss: 1.0047031537396833e-05\n",
            "step: 300, loss: 1.277014052902814e-05\n",
            "step: 310, loss: 2.159854193450883e-05\n",
            "step: 320, loss: 6.468484934885055e-05\n",
            "step: 330, loss: 1.3667930943483952e-05\n",
            "step: 340, loss: 2.9943905246909708e-05\n",
            "step: 350, loss: 9.868223969533574e-06\n",
            "step: 360, loss: 0.0035024345852434635\n",
            "step: 370, loss: 1.3626899090013467e-05\n",
            "step: 380, loss: 1.8238690245198086e-05\n",
            "step: 390, loss: 2.0272429537726566e-05\n",
            "step: 400, loss: 2.8353566449368373e-05\n",
            "step: 410, loss: 1.6267937098746188e-05\n",
            "step: 420, loss: 1.7888301954371855e-05\n",
            "step: 430, loss: 3.1655767088523135e-05\n",
            "step: 440, loss: 1.4766820640943479e-05\n",
            "step: 450, loss: 1.4129746887192596e-05\n",
            "step: 460, loss: 3.532904156600125e-05\n",
            "step: 470, loss: 6.47989145363681e-05\n",
            "step: 480, loss: 1.0233295142825227e-05\n",
            "step: 490, loss: 1.1078918760176748e-05\n",
            "step: 500, loss: 1.4256439499149565e-05\n",
            "step: 510, loss: 1.2159222023910843e-05\n",
            "step: 520, loss: 1.2282160241738893e-05\n",
            "step: 530, loss: 2.9168428227421828e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.94362292051756, f1=0.9358560221504384, best_f1=0.9401947148817803\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 248.56it/s]\n",
            "load_f1 = 0.9481961147086033\n",
            "real_f1 = 0.9456221198156683\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e50787-d896-4c2d-802d-cce9d5cecb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=e3830efc7152fb48af70adb16e5d5ae180f6a29bd00bd6d925968164f4b35bf2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1wk6cdx/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4222fa5-a06b-4554-a186-e6690022588b"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5572822690010071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.38095238095238093, f1=0.4137931034482759, best_f1=0.4137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4956880211830139\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5, f1=0.3880597014925373, best_f1=0.3880597014925373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5130605101585388\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5217391304347826, f1=0.42857142857142855, best_f1=0.42857142857142855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2886781692504883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6111111111111112, f1=0.4761904761904762, best_f1=0.4761904761904762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23767723143100739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6956521739130435, f1=0.56, best_f1=0.56\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2428884506225586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7692307692307692, f1=0.5833333333333334, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022770801559090614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8666666666666666, f1=0.7058823529411764, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011586287058889866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8666666666666666, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002045997651293874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8275862068965518, f1=0.7142857142857143, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0043383436277508736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8275862068965518, f1=0.6666666666666666, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028888797387480736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8275862068965518, f1=0.6153846153846153, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001958603970706463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8387096774193549, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023249892983585596\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8387096774193549, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020695291459560394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8387096774193549, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031346718315035105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8387096774193549, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 102768.35it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8148148148148148\n",
            "real_f1 = 0.8275862068965518\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 245.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355f6ce8-ebbc-47d1-cdb9-d31b9079dabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6193021535873413\n",
            "step: 10, loss: 0.6028382182121277\n",
            "step: 20, loss: 0.336209237575531\n",
            "step: 30, loss: 0.23590384423732758\n",
            "step: 40, loss: 0.05278477072715759\n",
            "step: 50, loss: 0.043239179998636246\n",
            "step: 60, loss: 0.04994328320026398\n",
            "step: 70, loss: 0.011044876649975777\n",
            "step: 80, loss: 0.11049789190292358\n",
            "step: 90, loss: 0.13780522346496582\n",
            "step: 100, loss: 0.10830534249544144\n",
            "step: 110, loss: 0.050908807665109634\n",
            "step: 120, loss: 0.0035712027456611395\n",
            "step: 130, loss: 0.009702678769826889\n",
            "step: 140, loss: 0.0020606571342796087\n",
            "step: 150, loss: 0.022425539791584015\n",
            "step: 160, loss: 0.01586167886853218\n",
            "step: 170, loss: 0.07907328754663467\n",
            "step: 180, loss: 0.004419975448399782\n",
            "step: 190, loss: 0.06707999855279922\n",
            "step: 200, loss: 0.038237232714891434\n",
            "step: 210, loss: 0.002631847048178315\n",
            "step: 220, loss: 0.05569899454712868\n",
            "step: 230, loss: 0.07024960964918137\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.990990990990991, f1=0.9819413092550789, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015683112433180213\n",
            "step: 10, loss: 0.002029633615165949\n",
            "step: 20, loss: 0.093981072306633\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.12385444343090057\n",
            "step: 40, loss: 0.031243249773979187\n",
            "step: 50, loss: 0.0052527617663145065\n",
            "step: 60, loss: 0.0012547285296022892\n",
            "step: 70, loss: 0.1365189105272293\n",
            "step: 80, loss: 0.001963244518265128\n",
            "step: 90, loss: 0.006182491313666105\n",
            "step: 100, loss: 0.02682947739958763\n",
            "step: 110, loss: 0.0056467787362635136\n",
            "step: 120, loss: 0.0012324111303314567\n",
            "step: 130, loss: 0.006114790216088295\n",
            "step: 140, loss: 0.002278511179611087\n",
            "step: 150, loss: 0.013954339548945427\n",
            "step: 160, loss: 0.002238851273432374\n",
            "step: 170, loss: 0.0015297747449949384\n",
            "step: 180, loss: 0.004328247159719467\n",
            "step: 190, loss: 0.009159685112535954\n",
            "step: 200, loss: 0.00183941051363945\n",
            "step: 210, loss: 0.0006176151800900698\n",
            "step: 220, loss: 0.021224912256002426\n",
            "step: 230, loss: 0.01529077347368002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9864864864864865, f1=0.9797297297297298, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004285838454961777\n",
            "step: 10, loss: 0.005729111842811108\n",
            "step: 20, loss: 0.0017521979752928019\n",
            "step: 30, loss: 0.013579221442341805\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.01920853927731514\n",
            "step: 50, loss: 0.014165225438773632\n",
            "step: 60, loss: 0.004203870892524719\n",
            "step: 70, loss: 0.014325299300253391\n",
            "step: 80, loss: 0.004078793339431286\n",
            "step: 90, loss: 0.009199033491313457\n",
            "step: 100, loss: 0.022869784384965897\n",
            "step: 110, loss: 0.0062994323670864105\n",
            "step: 120, loss: 0.011275529861450195\n",
            "step: 130, loss: 0.012566747143864632\n",
            "step: 140, loss: 0.0010743646416813135\n",
            "step: 150, loss: 0.11974519491195679\n",
            "step: 160, loss: 0.021782128140330315\n",
            "step: 170, loss: 0.0041000135242938995\n",
            "step: 180, loss: 0.007230894640088081\n",
            "step: 190, loss: 0.00922324601560831\n",
            "step: 200, loss: 0.0304366797208786\n",
            "step: 210, loss: 0.003625185927376151\n",
            "step: 220, loss: 0.001758921891450882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9921259842519685, f1=0.9865168539325843, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006950552342459559\n",
            "step: 10, loss: 0.0002966487081721425\n",
            "step: 20, loss: 0.0005093633662909269\n",
            "step: 30, loss: 0.07658951729536057\n",
            "step: 40, loss: 0.11487188935279846\n",
            "step: 50, loss: 0.0009556713048368692\n",
            "step: 60, loss: 0.008721468970179558\n",
            "step: 70, loss: 0.0009021328878588974\n",
            "step: 80, loss: 0.009417781606316566\n",
            "step: 90, loss: 0.0029168976470828056\n",
            "step: 100, loss: 0.0038013628218322992\n",
            "step: 110, loss: 0.0006362044950947165\n",
            "step: 120, loss: 0.020959651097655296\n",
            "step: 130, loss: 0.031183717772364616\n",
            "step: 140, loss: 0.0003318852395750582\n",
            "step: 150, loss: 0.06388026475906372\n",
            "step: 160, loss: 0.0074613941833376884\n",
            "step: 170, loss: 0.0015073007671162486\n",
            "step: 180, loss: 0.000294468249194324\n",
            "step: 190, loss: 0.007324710488319397\n",
            "step: 200, loss: 0.0013880339683964849\n",
            "step: 210, loss: 0.005590823944658041\n",
            "step: 220, loss: 0.00012109700764995068\n",
            "step: 230, loss: 0.02708631567656994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9875706214689265, f1=0.9796380090497738, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015034859825391322\n",
            "step: 10, loss: 0.0002445425488986075\n",
            "step: 20, loss: 0.00019873565179295838\n",
            "step: 30, loss: 0.00023491588945034891\n",
            "step: 40, loss: 0.00029727991204708815\n",
            "step: 50, loss: 0.00020754104480147362\n",
            "step: 60, loss: 0.0004003612557426095\n",
            "step: 70, loss: 0.0004014205769635737\n",
            "step: 80, loss: 0.0007996759959496558\n",
            "step: 90, loss: 0.0036445753648877144\n",
            "step: 100, loss: 0.0013334465911611915\n",
            "step: 110, loss: 0.006281055044382811\n",
            "step: 120, loss: 0.00023542242706753314\n",
            "step: 130, loss: 0.0014567594043910503\n",
            "step: 140, loss: 0.0016339225694537163\n",
            "step: 150, loss: 0.00044436650932766497\n",
            "step: 160, loss: 0.0008926105219870806\n",
            "step: 170, loss: 0.011553811840713024\n",
            "step: 180, loss: 0.0016944779781624675\n",
            "step: 190, loss: 0.0044023701921105385\n",
            "step: 200, loss: 0.00017918231606017798\n",
            "step: 210, loss: 0.0001341860624961555\n",
            "step: 220, loss: 0.0008219544542953372\n",
            "step: 230, loss: 0.0003011637891177088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9887387387387387, f1=0.9853768278965129, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010849625105038285\n",
            "step: 10, loss: 0.001299253897741437\n",
            "step: 20, loss: 0.0009464983595535159\n",
            "step: 30, loss: 0.00023087323643267155\n",
            "step: 40, loss: 0.00047442392678931355\n",
            "step: 50, loss: 0.00016603167750872672\n",
            "step: 60, loss: 0.00010870487312786281\n",
            "step: 70, loss: 0.0002949413610622287\n",
            "step: 80, loss: 0.0005968628684058785\n",
            "step: 90, loss: 0.0005675117718055844\n",
            "step: 100, loss: 0.02873929962515831\n",
            "step: 110, loss: 0.0009316733339801431\n",
            "step: 120, loss: 0.0001464621163904667\n",
            "step: 130, loss: 0.0010321519803255796\n",
            "step: 140, loss: 7.97348257037811e-05\n",
            "step: 150, loss: 0.00033921233261935413\n",
            "step: 160, loss: 0.0010025585070252419\n",
            "step: 170, loss: 0.00020696644787676632\n",
            "step: 180, loss: 0.011439564637839794\n",
            "step: 190, loss: 0.0043212128803133965\n",
            "step: 200, loss: 0.0006798197864554822\n",
            "step: 210, loss: 0.00018613536667544395\n",
            "step: 220, loss: 0.00016773266543168575\n",
            "step: 230, loss: 0.04450768977403641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9876265466816648, f1=0.9819819819819819, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005936583038419485\n",
            "step: 10, loss: 7.35617577447556e-05\n",
            "step: 20, loss: 0.0002037652739090845\n",
            "step: 30, loss: 0.0003052728425245732\n",
            "step: 40, loss: 7.630146137671545e-05\n",
            "step: 50, loss: 6.640841456828639e-05\n",
            "step: 60, loss: 7.490513962693512e-05\n",
            "step: 70, loss: 0.023033462464809418\n",
            "step: 80, loss: 8.477432129438967e-05\n",
            "step: 90, loss: 3.912909596692771e-05\n",
            "step: 100, loss: 6.993638817220926e-05\n",
            "step: 110, loss: 0.0030598011799156666\n",
            "step: 120, loss: 5.1580580475274473e-05\n",
            "step: 130, loss: 5.099885311210528e-05\n",
            "step: 140, loss: 0.0001020308627630584\n",
            "step: 150, loss: 0.003471442498266697\n",
            "step: 160, loss: 0.07782800495624542\n",
            "step: 170, loss: 0.07519348710775375\n",
            "step: 180, loss: 0.0020599656272679567\n",
            "step: 190, loss: 0.00026737514417618513\n",
            "step: 200, loss: 0.04411260783672333\n",
            "step: 210, loss: 5.971669452264905e-05\n",
            "step: 220, loss: 0.0005250527174212039\n",
            "step: 230, loss: 0.0066178408451378345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9887387387387387, f1=0.9853768278965129, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.295443694805726e-05\n",
            "step: 10, loss: 0.000138251154567115\n",
            "step: 20, loss: 0.00011075310612795874\n",
            "step: 30, loss: 0.00033596265711821616\n",
            "step: 40, loss: 0.00034542157663963735\n",
            "step: 50, loss: 0.00039062125142663717\n",
            "step: 60, loss: 6.92759858793579e-05\n",
            "step: 70, loss: 0.00020223062892910093\n",
            "step: 80, loss: 0.00010096590995090082\n",
            "step: 90, loss: 6.513838161481544e-05\n",
            "step: 100, loss: 8.785323734628037e-05\n",
            "step: 110, loss: 7.472551078535616e-05\n",
            "step: 120, loss: 0.00011777172039728612\n",
            "step: 130, loss: 0.000189416270586662\n",
            "step: 140, loss: 5.273926944937557e-05\n",
            "step: 150, loss: 0.00010233960347250104\n",
            "step: 160, loss: 0.0002732256252784282\n",
            "step: 170, loss: 9.873783710645512e-05\n",
            "step: 180, loss: 7.968281715875491e-05\n",
            "step: 190, loss: 0.00021255530009511858\n",
            "step: 200, loss: 4.520820584730245e-05\n",
            "step: 210, loss: 5.2669536671601236e-05\n",
            "step: 220, loss: 0.0001690410717856139\n",
            "step: 230, loss: 8.247988444054499e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9887640449438202, f1=0.9820627802690582, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.3398347770562395e-05\n",
            "step: 10, loss: 0.0007343686302192509\n",
            "step: 20, loss: 0.005702455528080463\n",
            "step: 30, loss: 4.5882599806645885e-05\n",
            "step: 40, loss: 0.12318570911884308\n",
            "step: 50, loss: 8.117582910927013e-05\n",
            "step: 60, loss: 7.622037082910538e-05\n",
            "step: 70, loss: 0.00022046083176974207\n",
            "step: 80, loss: 0.03753841668367386\n",
            "step: 90, loss: 0.00014839407231193036\n",
            "step: 100, loss: 0.0022474073339253664\n",
            "step: 110, loss: 5.753882942372002e-05\n",
            "step: 120, loss: 5.5678243370493874e-05\n",
            "step: 130, loss: 8.1930986198131e-05\n",
            "step: 140, loss: 0.0008927091839723289\n",
            "step: 150, loss: 0.0001347194192931056\n",
            "step: 160, loss: 3.781350824283436e-05\n",
            "step: 170, loss: 0.0028679214883595705\n",
            "step: 180, loss: 0.00016156959463842213\n",
            "step: 190, loss: 0.0001348523801425472\n",
            "step: 200, loss: 4.31136904808227e-05\n",
            "step: 210, loss: 0.00022261797857936472\n",
            "step: 220, loss: 4.8954836529446766e-05\n",
            "step: 230, loss: 0.00012912553211208433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.987598647125141, f1=0.9841628959276018, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.59003819944337e-05\n",
            "step: 10, loss: 3.111274054390378e-05\n",
            "step: 20, loss: 0.0003392025828361511\n",
            "step: 30, loss: 0.000495835323818028\n",
            "step: 40, loss: 6.98510484653525e-05\n",
            "step: 50, loss: 0.00024877904797904193\n",
            "step: 60, loss: 0.003470013150945306\n",
            "step: 70, loss: 0.0004503911768551916\n",
            "step: 80, loss: 6.96190691087395e-05\n",
            "step: 90, loss: 0.00026564663858152926\n",
            "step: 100, loss: 5.970222264295444e-05\n",
            "step: 110, loss: 0.0009270209120586514\n",
            "step: 120, loss: 0.00010233160719508305\n",
            "step: 130, loss: 0.003389037912711501\n",
            "step: 140, loss: 0.02334909327328205\n",
            "step: 150, loss: 0.019022643566131592\n",
            "step: 160, loss: 2.206452154496219e-05\n",
            "step: 170, loss: 6.44273022771813e-05\n",
            "step: 180, loss: 0.00020630663493648171\n",
            "step: 190, loss: 8.180316945072263e-05\n",
            "step: 200, loss: 3.016994560312014e-05\n",
            "step: 210, loss: 5.303290527081117e-05\n",
            "step: 220, loss: 3.939342423109338e-05\n",
            "step: 230, loss: 4.462194556253962e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9876265466816648, f1=0.9853438556933484, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.886347935302183e-05\n",
            "step: 10, loss: 6.806658348068595e-05\n",
            "step: 20, loss: 0.00012915610568597913\n",
            "step: 30, loss: 0.017228569835424423\n",
            "step: 40, loss: 6.539496826007962e-05\n",
            "step: 50, loss: 6.660311919404194e-05\n",
            "step: 60, loss: 4.894944140687585e-05\n",
            "step: 70, loss: 8.200171578209847e-05\n",
            "step: 80, loss: 3.8273847167147323e-05\n",
            "step: 90, loss: 3.2550236937822774e-05\n",
            "step: 100, loss: 3.501257378957234e-05\n",
            "step: 110, loss: 7.769319927319884e-05\n",
            "step: 120, loss: 3.358538378961384e-05\n",
            "step: 130, loss: 5.192357275518589e-05\n",
            "step: 140, loss: 3.4073407732648775e-05\n",
            "step: 150, loss: 0.031779538840055466\n",
            "step: 160, loss: 3.121322879451327e-05\n",
            "step: 170, loss: 0.018925338983535767\n",
            "step: 180, loss: 3.9333033782895654e-05\n",
            "step: 190, loss: 3.4374748793197796e-05\n",
            "step: 200, loss: 0.0002912209019996226\n",
            "step: 210, loss: 2.3420347133651376e-05\n",
            "step: 220, loss: 4.200778130325489e-05\n",
            "step: 230, loss: 3.5344110074220225e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9898534385569334, f1=0.9853107344632768, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.5131881304550916e-05\n",
            "step: 10, loss: 2.8583290259120986e-05\n",
            "step: 20, loss: 3.223633757443167e-05\n",
            "step: 30, loss: 7.29974708519876e-05\n",
            "step: 40, loss: 7.860933692427352e-05\n",
            "step: 50, loss: 0.0001546482089906931\n",
            "step: 60, loss: 4.859748514718376e-05\n",
            "step: 70, loss: 7.033445581328124e-05\n",
            "step: 80, loss: 3.4263968700543046e-05\n",
            "step: 90, loss: 3.808020119322464e-05\n",
            "step: 100, loss: 2.510393642296549e-05\n",
            "step: 110, loss: 3.398056287551299e-05\n",
            "step: 120, loss: 2.5782152079045773e-05\n",
            "step: 130, loss: 2.9835078748874366e-05\n",
            "step: 140, loss: 2.9339104003156535e-05\n",
            "step: 150, loss: 8.200985757866874e-05\n",
            "step: 160, loss: 0.0008621863089501858\n",
            "step: 170, loss: 2.5670356990303844e-05\n",
            "step: 180, loss: 2.741714342846535e-05\n",
            "step: 190, loss: 9.481079177930951e-05\n",
            "step: 200, loss: 1.882724063761998e-05\n",
            "step: 210, loss: 3.163782093906775e-05\n",
            "step: 220, loss: 2.829616641975008e-05\n",
            "step: 230, loss: 0.04278123751282692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9875706214689265, f1=0.983050847457627, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.523590203258209e-05\n",
            "step: 10, loss: 7.620630640303716e-05\n",
            "step: 20, loss: 0.00021278296480886638\n",
            "step: 30, loss: 5.021090692025609e-05\n",
            "step: 40, loss: 3.4624590625753626e-05\n",
            "step: 50, loss: 7.300232391571626e-05\n",
            "step: 60, loss: 2.3573136786581017e-05\n",
            "step: 70, loss: 5.562159276450984e-05\n",
            "step: 80, loss: 5.39230095455423e-05\n",
            "step: 90, loss: 0.00013606349239125848\n",
            "step: 100, loss: 2.204573502240237e-05\n",
            "step: 110, loss: 8.566542237531394e-05\n",
            "step: 120, loss: 0.021031830459833145\n",
            "step: 130, loss: 5.7505545555613935e-05\n",
            "step: 140, loss: 3.2677089620847255e-05\n",
            "step: 150, loss: 2.703753489186056e-05\n",
            "step: 160, loss: 3.742436092579737e-05\n",
            "step: 170, loss: 4.475496825762093e-05\n",
            "step: 180, loss: 5.542222788790241e-05\n",
            "step: 190, loss: 2.0607825717888772e-05\n",
            "step: 200, loss: 5.0953538448084146e-05\n",
            "step: 210, loss: 1.971756682905834e-05\n",
            "step: 220, loss: 0.00011241356696700677\n",
            "step: 230, loss: 2.7085485271527432e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9887892376681614, f1=0.9820627802690582, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.630351264087949e-05\n",
            "step: 10, loss: 0.0013675604714080691\n",
            "step: 20, loss: 2.808042336255312e-05\n",
            "step: 30, loss: 3.4059034078381956e-05\n",
            "step: 40, loss: 0.01605290174484253\n",
            "step: 50, loss: 0.0003073741972912103\n",
            "step: 60, loss: 3.456416743574664e-05\n",
            "step: 70, loss: 2.8002174076391384e-05\n",
            "step: 80, loss: 2.8926035156473517e-05\n",
            "step: 90, loss: 3.314926289021969e-05\n",
            "step: 100, loss: 2.439621493977029e-05\n",
            "step: 110, loss: 0.00010708460467867553\n",
            "step: 120, loss: 1.616754161659628e-05\n",
            "step: 130, loss: 2.743234290392138e-05\n",
            "step: 140, loss: 2.9645030735991895e-05\n",
            "step: 150, loss: 3.34020696755033e-05\n",
            "step: 160, loss: 3.3354201150359586e-05\n",
            "step: 170, loss: 2.4004297301871702e-05\n",
            "step: 180, loss: 2.6608755433699116e-05\n",
            "step: 190, loss: 4.91113678435795e-05\n",
            "step: 200, loss: 2.6951160180033185e-05\n",
            "step: 210, loss: 3.224106694688089e-05\n",
            "step: 220, loss: 1.7154714441858232e-05\n",
            "step: 230, loss: 3.39130092470441e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9875424688561721, f1=0.9852104664391355, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.7843865862232633e-05\n",
            "step: 10, loss: 1.9609229639172554e-05\n",
            "step: 20, loss: 2.897422746173106e-05\n",
            "step: 30, loss: 2.580808722996153e-05\n",
            "step: 40, loss: 3.6579498555511236e-05\n",
            "step: 50, loss: 0.024446725845336914\n",
            "step: 60, loss: 3.329087121528573e-05\n",
            "step: 70, loss: 3.2233507226919755e-05\n",
            "step: 80, loss: 3.3362070098519325e-05\n",
            "step: 90, loss: 4.794606866198592e-05\n",
            "step: 100, loss: 2.4679435227881186e-05\n",
            "step: 110, loss: 2.4493070668540895e-05\n",
            "step: 120, loss: 0.00012790843902621418\n",
            "step: 130, loss: 3.187586844433099e-05\n",
            "step: 140, loss: 2.1256091713439673e-05\n",
            "step: 150, loss: 7.092951273079962e-05\n",
            "step: 160, loss: 4.040612111566588e-05\n",
            "step: 170, loss: 1.865222475316841e-05\n",
            "step: 180, loss: 8.235572749981657e-05\n",
            "step: 190, loss: 7.916631147963926e-05\n",
            "step: 200, loss: 2.7923984816879965e-05\n",
            "step: 210, loss: 2.6001882361015305e-05\n",
            "step: 220, loss: 5.301458077155985e-05\n",
            "step: 230, loss: 2.2015979993739165e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9876543209876544, f1=0.9820627802690582, best_f1=0.9865168539325843\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 180.66it/s]\n",
            "load_f1 = 0.990990990990991\n",
            "real_f1 = 0.9887133182844244\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 222.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eff57d-e955-4aa9-ad08-d600c8353cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5927374362945557\n",
            "step: 10, loss: 0.5235815048217773\n",
            "step: 20, loss: 0.4407440423965454\n",
            "step: 30, loss: 0.09949018806219101\n",
            "step: 40, loss: 0.121893972158432\n",
            "step: 50, loss: 0.23051150143146515\n",
            "step: 60, loss: 0.05014362186193466\n",
            "step: 70, loss: 0.07669265568256378\n",
            "step: 80, loss: 0.027580246329307556\n",
            "step: 90, loss: 0.09340274333953857\n",
            "step: 100, loss: 0.09944058954715729\n",
            "step: 110, loss: 0.06582484394311905\n",
            "step: 120, loss: 0.10032361000776291\n",
            "step: 130, loss: 0.14945147931575775\n",
            "step: 140, loss: 0.11879435181617737\n",
            "step: 150, loss: 0.06301072984933853\n",
            "step: 160, loss: 0.021238276734948158\n",
            "step: 170, loss: 0.2021491974592209\n",
            "step: 180, loss: 0.09085869044065475\n",
            "step: 190, loss: 0.02233009971678257\n",
            "step: 200, loss: 0.17015953361988068\n",
            "step: 210, loss: 0.10610375553369522\n",
            "step: 220, loss: 0.17326034605503082\n",
            "step: 230, loss: 0.19702309370040894\n",
            "step: 240, loss: 0.07806769758462906\n",
            "step: 250, loss: 0.03333250805735588\n",
            "step: 260, loss: 0.14067518711090088\n",
            "step: 270, loss: 0.012012235820293427\n",
            "step: 280, loss: 0.029740121215581894\n",
            "step: 290, loss: 0.08727956563234329\n",
            "step: 300, loss: 0.04763399437069893\n",
            "step: 310, loss: 0.12703154981136322\n",
            "step: 320, loss: 0.0779431089758873\n",
            "step: 330, loss: 0.07535506039857864\n",
            "step: 340, loss: 0.04124787449836731\n",
            "step: 350, loss: 0.015237855724990368\n",
            "step: 360, loss: 0.020859267562627792\n",
            "step: 370, loss: 0.09611965715885162\n",
            "step: 380, loss: 0.03686175122857094\n",
            "step: 390, loss: 0.1671753227710724\n",
            "step: 400, loss: 0.3706247806549072\n",
            "step: 410, loss: 0.07084418833255768\n",
            "step: 420, loss: 0.09407181292772293\n",
            "step: 430, loss: 0.1478962004184723\n",
            "step: 440, loss: 0.037125758826732635\n",
            "step: 450, loss: 0.006572895683348179\n",
            "step: 460, loss: 0.005890555679798126\n",
            "step: 470, loss: 0.15760014951229095\n",
            "step: 480, loss: 0.047884728759527206\n",
            "step: 490, loss: 0.04913824424147606\n",
            "step: 500, loss: 0.11473333090543747\n",
            "step: 510, loss: 0.11387835443019867\n",
            "step: 520, loss: 0.11712421476840973\n",
            "step: 530, loss: 0.005335430148988962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9247015610651974, f1=0.9240681086056144, best_f1=0.9240681086056144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1503003090620041\n",
            "step: 10, loss: 0.15869532525539398\n",
            "step: 20, loss: 0.005288717336952686\n",
            "step: 30, loss: 0.012261561118066311\n",
            "step: 40, loss: 0.029279131442308426\n",
            "step: 50, loss: 0.0468541756272316\n",
            "step: 60, loss: 0.019796237349510193\n",
            "step: 70, loss: 0.025467464700341225\n",
            "step: 80, loss: 0.0640711858868599\n",
            "step: 90, loss: 0.01816231943666935\n",
            "step: 100, loss: 0.013695618137717247\n",
            "step: 110, loss: 0.017381122335791588\n",
            "step: 120, loss: 0.048669904470443726\n",
            "step: 130, loss: 0.2167479246854782\n",
            "step: 140, loss: 0.07588928937911987\n",
            "step: 150, loss: 0.15242601931095123\n",
            "step: 160, loss: 0.021035728976130486\n",
            "step: 170, loss: 0.03666367009282112\n",
            "step: 180, loss: 0.014366579242050648\n",
            "step: 190, loss: 0.13624154031276703\n",
            "step: 200, loss: 0.008173773996531963\n",
            "step: 210, loss: 0.051074035465717316\n",
            "step: 220, loss: 0.08525827527046204\n",
            "step: 230, loss: 0.006184106692671776\n",
            "step: 240, loss: 0.03389495238661766\n",
            "step: 250, loss: 0.117338165640831\n",
            "step: 260, loss: 0.004539045970886946\n",
            "step: 270, loss: 0.24687083065509796\n",
            "step: 280, loss: 0.024015087634325027\n",
            "step: 290, loss: 0.014528991654515266\n",
            "step: 300, loss: 0.02504332922399044\n",
            "step: 310, loss: 0.011337400414049625\n",
            "step: 320, loss: 0.07962383329868317\n",
            "step: 330, loss: 0.08244628459215164\n",
            "step: 340, loss: 0.017671354115009308\n",
            "step: 350, loss: 0.0013586889253929257\n",
            "step: 360, loss: 0.02969709411263466\n",
            "step: 370, loss: 0.12740464508533478\n",
            "step: 380, loss: 0.046790193766355515\n",
            "step: 390, loss: 0.05620989575982094\n",
            "step: 400, loss: 0.07242139428853989\n",
            "step: 410, loss: 0.00664899405092001\n",
            "step: 420, loss: 0.008116215467453003\n",
            "step: 430, loss: 0.01835963875055313\n",
            "step: 440, loss: 0.12529420852661133\n",
            "step: 450, loss: 0.04681434482336044\n",
            "step: 460, loss: 0.05065586417913437\n",
            "step: 470, loss: 0.0157249066978693\n",
            "step: 480, loss: 0.34101375937461853\n",
            "step: 490, loss: 0.03809993714094162\n",
            "step: 500, loss: 0.11863058060407639\n",
            "step: 510, loss: 0.020869795233011246\n",
            "step: 520, loss: 0.0291425958275795\n",
            "step: 530, loss: 0.021644845604896545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9369114877589454, f1=0.9338919925512105, best_f1=0.9338919925512105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051865220069885254\n",
            "step: 10, loss: 0.10970281809568405\n",
            "step: 20, loss: 0.06434851884841919\n",
            "step: 30, loss: 0.050348423421382904\n",
            "step: 40, loss: 0.002386000007390976\n",
            "step: 50, loss: 0.05649912729859352\n",
            "step: 60, loss: 0.040598224848508835\n",
            "step: 70, loss: 0.004171755164861679\n",
            "step: 80, loss: 0.0026896465569734573\n",
            "step: 90, loss: 0.007792051415890455\n",
            "step: 100, loss: 0.05166807025671005\n",
            "step: 110, loss: 0.0012191561982035637\n",
            "step: 120, loss: 0.001701138447970152\n",
            "step: 130, loss: 0.0022907631937414408\n",
            "step: 140, loss: 0.01603493094444275\n",
            "step: 150, loss: 0.011742318980395794\n",
            "step: 160, loss: 0.008037303574383259\n",
            "step: 170, loss: 0.05138927698135376\n",
            "step: 180, loss: 0.004740141332149506\n",
            "step: 190, loss: 0.009518489241600037\n",
            "step: 200, loss: 0.040273286402225494\n",
            "step: 210, loss: 0.07689685374498367\n",
            "step: 220, loss: 0.1379166841506958\n",
            "step: 230, loss: 0.057660069316625595\n",
            "step: 240, loss: 0.001299678930081427\n",
            "step: 250, loss: 0.015536455437541008\n",
            "step: 260, loss: 0.010783091187477112\n",
            "step: 270, loss: 0.013265757821500301\n",
            "step: 280, loss: 0.10676033794879913\n",
            "step: 290, loss: 0.002613582881167531\n",
            "step: 300, loss: 0.08800067007541656\n",
            "step: 310, loss: 0.0054809036664664745\n",
            "step: 320, loss: 0.041116904467344284\n",
            "step: 330, loss: 0.0015083298785611987\n",
            "step: 340, loss: 0.004259227775037289\n",
            "step: 350, loss: 0.003532965201884508\n",
            "step: 360, loss: 0.04408503696322441\n",
            "step: 370, loss: 0.013832267373800278\n",
            "step: 380, loss: 0.006014594342559576\n",
            "step: 390, loss: 0.041242565959692\n",
            "step: 400, loss: 0.04192272573709488\n",
            "step: 410, loss: 0.008495515212416649\n",
            "step: 420, loss: 0.23933395743370056\n",
            "step: 430, loss: 0.008403589949011803\n",
            "step: 440, loss: 0.0006523514748550951\n",
            "step: 450, loss: 0.08295772224664688\n",
            "step: 460, loss: 0.023395774886012077\n",
            "step: 470, loss: 0.019942576065659523\n",
            "step: 480, loss: 0.0027252009604126215\n",
            "step: 490, loss: 0.006674472242593765\n",
            "step: 500, loss: 0.03654877468943596\n",
            "step: 510, loss: 0.018710032105445862\n",
            "step: 520, loss: 0.044248707592487335\n",
            "step: 530, loss: 0.05747614800930023\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9411764705882353, f1=0.9372693726937269, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08082915842533112\n",
            "step: 10, loss: 0.0020953554194420576\n",
            "step: 20, loss: 0.0012858306290581822\n",
            "step: 30, loss: 0.0005070571787655354\n",
            "step: 40, loss: 0.0019557196646928787\n",
            "step: 50, loss: 0.001029070233926177\n",
            "step: 60, loss: 0.002362570259720087\n",
            "step: 70, loss: 0.0031074017751961946\n",
            "step: 80, loss: 0.09482541680335999\n",
            "step: 90, loss: 0.049120739102363586\n",
            "step: 100, loss: 0.0037033604457974434\n",
            "step: 110, loss: 0.021227220073342323\n",
            "step: 120, loss: 0.01511509157717228\n",
            "step: 130, loss: 0.030804777517914772\n",
            "step: 140, loss: 0.07983237504959106\n",
            "step: 150, loss: 0.008850577287375927\n",
            "step: 160, loss: 0.05630253255367279\n",
            "step: 170, loss: 0.012318223714828491\n",
            "step: 180, loss: 0.000791952305007726\n",
            "step: 190, loss: 0.004351758398115635\n",
            "step: 200, loss: 0.00967482477426529\n",
            "step: 210, loss: 0.031004520133137703\n",
            "step: 220, loss: 0.003018850227817893\n",
            "step: 230, loss: 0.07969392836093903\n",
            "step: 240, loss: 0.008860599249601364\n",
            "step: 250, loss: 0.013332413509488106\n",
            "step: 260, loss: 0.0042426697909832\n",
            "step: 270, loss: 0.01256580650806427\n",
            "step: 280, loss: 0.0046903532929718494\n",
            "step: 290, loss: 0.029456712305545807\n",
            "step: 300, loss: 0.0002462713164277375\n",
            "step: 310, loss: 0.004628614988178015\n",
            "step: 320, loss: 0.0023629090283066034\n",
            "step: 330, loss: 0.011128289625048637\n",
            "step: 340, loss: 0.001492982148192823\n",
            "step: 350, loss: 0.10419569164514542\n",
            "step: 360, loss: 0.008131015114486217\n",
            "step: 370, loss: 0.03328173980116844\n",
            "step: 380, loss: 0.004445487167686224\n",
            "step: 390, loss: 0.00045850881724618375\n",
            "step: 400, loss: 0.0019900694023817778\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 410, loss: 0.009276294149458408\n",
            "step: 420, loss: 0.15023492276668549\n",
            "step: 430, loss: 0.15992222726345062\n",
            "step: 440, loss: 0.008624912239611149\n",
            "step: 450, loss: 0.0027175003197044134\n",
            "step: 460, loss: 0.00535786896944046\n",
            "step: 470, loss: 0.027508936822414398\n",
            "step: 480, loss: 0.1674058735370636\n",
            "step: 490, loss: 0.0032267477363348007\n",
            "step: 500, loss: 0.0034340249840170145\n",
            "step: 510, loss: 0.009897495619952679\n",
            "step: 520, loss: 0.021593378856778145\n",
            "step: 530, loss: 0.02840033546090126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9374130737134909, f1=0.93646408839779, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011277435347437859\n",
            "step: 10, loss: 0.048425838351249695\n",
            "step: 20, loss: 0.002728687133640051\n",
            "step: 30, loss: 0.015858685597777367\n",
            "step: 40, loss: 0.02764544077217579\n",
            "step: 50, loss: 0.0013592962641268969\n",
            "step: 60, loss: 0.025462456047534943\n",
            "step: 70, loss: 0.0018305593403056264\n",
            "step: 80, loss: 0.005319193936884403\n",
            "step: 90, loss: 0.006404025945812464\n",
            "step: 100, loss: 0.013199505396187305\n",
            "step: 110, loss: 0.002773130312561989\n",
            "step: 120, loss: 0.006726904306560755\n",
            "step: 130, loss: 0.0006713983602821827\n",
            "step: 140, loss: 0.009325813502073288\n",
            "step: 150, loss: 0.0035772393457591534\n",
            "step: 160, loss: 0.000543290632776916\n",
            "step: 170, loss: 0.01895514316856861\n",
            "step: 180, loss: 0.0013714261585846543\n",
            "step: 190, loss: 0.001076143584214151\n",
            "step: 200, loss: 0.00047951401211321354\n",
            "step: 210, loss: 0.0004134147020522505\n",
            "step: 220, loss: 0.0007821741164661944\n",
            "step: 230, loss: 0.007805663626641035\n",
            "step: 240, loss: 0.0019836423452943563\n",
            "step: 250, loss: 0.0017732587875798345\n",
            "step: 260, loss: 0.007078444119542837\n",
            "step: 270, loss: 0.00025938707403838634\n",
            "step: 280, loss: 0.007359933573752642\n",
            "step: 290, loss: 0.10254016518592834\n",
            "step: 300, loss: 0.0031982867512851954\n",
            "step: 310, loss: 0.010183055885136127\n",
            "step: 320, loss: 0.06543801724910736\n",
            "step: 330, loss: 0.009052885696291924\n",
            "step: 340, loss: 0.008481933735311031\n",
            "step: 350, loss: 0.0007028573309071362\n",
            "step: 360, loss: 0.0012421456631273031\n",
            "step: 370, loss: 0.002566548530012369\n",
            "step: 380, loss: 0.0006195184541866183\n",
            "step: 390, loss: 0.00032793483114801347\n",
            "step: 400, loss: 0.004097499884665012\n",
            "step: 410, loss: 0.0018665895331650972\n",
            "step: 420, loss: 0.005454482976347208\n",
            "step: 430, loss: 0.00955807976424694\n",
            "step: 440, loss: 0.019578440114855766\n",
            "step: 450, loss: 0.001958937617018819\n",
            "step: 460, loss: 0.0004572531906887889\n",
            "step: 470, loss: 0.002482407260686159\n",
            "step: 480, loss: 0.10424744337797165\n",
            "step: 490, loss: 0.0026847461704164743\n",
            "step: 500, loss: 0.005619560834020376\n",
            "step: 510, loss: 0.008116436190903187\n",
            "step: 520, loss: 0.01093999668955803\n",
            "step: 530, loss: 0.0022830553352832794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.929170549860205, f1=0.9408476944573824, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00024525861954316497\n",
            "step: 10, loss: 0.0016301436116918921\n",
            "step: 20, loss: 0.08088165521621704\n",
            "step: 30, loss: 0.005074262153357267\n",
            "step: 40, loss: 0.007275882177054882\n",
            "step: 50, loss: 0.018346646800637245\n",
            "step: 60, loss: 0.00043231097515672445\n",
            "step: 70, loss: 0.0006387311150319874\n",
            "step: 80, loss: 0.004465554840862751\n",
            "step: 90, loss: 0.0007930507999844849\n",
            "step: 100, loss: 0.010565503500401974\n",
            "step: 110, loss: 0.0021816850639879704\n",
            "step: 120, loss: 0.007000960875302553\n",
            "step: 130, loss: 0.00030387454899027944\n",
            "step: 140, loss: 0.07574258744716644\n",
            "step: 150, loss: 0.02056661993265152\n",
            "step: 160, loss: 0.004402942024171352\n",
            "step: 170, loss: 0.0020468842703849077\n",
            "step: 180, loss: 0.0009495448903180659\n",
            "step: 190, loss: 0.0013564040418714285\n",
            "step: 200, loss: 0.0013748990604653955\n",
            "step: 210, loss: 0.01725897006690502\n",
            "step: 220, loss: 0.0016213225899264216\n",
            "step: 230, loss: 0.0003027695929631591\n",
            "step: 240, loss: 0.07506008446216583\n",
            "step: 250, loss: 0.0006510980892926455\n",
            "step: 260, loss: 6.404404120985419e-05\n",
            "step: 270, loss: 0.054865773767232895\n",
            "step: 280, loss: 0.045882973819971085\n",
            "step: 290, loss: 0.0069173588417470455\n",
            "step: 300, loss: 0.0004299620632082224\n",
            "step: 310, loss: 0.0002382943785050884\n",
            "step: 320, loss: 0.0002980373101308942\n",
            "step: 330, loss: 8.631907257949933e-05\n",
            "step: 340, loss: 0.012719100341200829\n",
            "step: 350, loss: 0.0005143595626577735\n",
            "step: 360, loss: 0.0039063915610313416\n",
            "step: 370, loss: 0.0018661273643374443\n",
            "step: 380, loss: 0.00266736070625484\n",
            "step: 390, loss: 0.11395284533500671\n",
            "step: 400, loss: 0.005838700570166111\n",
            "step: 410, loss: 0.0007502158405259252\n",
            "step: 420, loss: 0.003911091014742851\n",
            "step: 430, loss: 0.0005800843355245888\n",
            "step: 440, loss: 0.00013372224930208176\n",
            "step: 450, loss: 0.0002514382067602128\n",
            "step: 460, loss: 0.0006865655886940658\n",
            "step: 470, loss: 0.0006567159434780478\n",
            "step: 480, loss: 0.010569276288151741\n",
            "step: 490, loss: 0.010517158545553684\n",
            "step: 500, loss: 0.14381174743175507\n",
            "step: 510, loss: 0.05439369007945061\n",
            "step: 520, loss: 0.0057329945266246796\n",
            "step: 530, loss: 0.008279597386717796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9311475409836065, f1=0.9348230912476723, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000773433712311089\n",
            "step: 10, loss: 0.003562171943485737\n",
            "step: 20, loss: 0.0016769309295341372\n",
            "step: 30, loss: 0.0014724972425028682\n",
            "step: 40, loss: 0.003215551609173417\n",
            "step: 50, loss: 0.008467761799693108\n",
            "step: 60, loss: 0.0007137763896025717\n",
            "step: 70, loss: 0.005451794248074293\n",
            "step: 80, loss: 0.0017968285828828812\n",
            "step: 90, loss: 0.010059304535388947\n",
            "step: 100, loss: 0.009749219752848148\n",
            "step: 110, loss: 0.0012687514536082745\n",
            "step: 120, loss: 0.008857181295752525\n",
            "step: 130, loss: 0.01166630256921053\n",
            "step: 140, loss: 9.7722775535658e-05\n",
            "step: 150, loss: 0.00039049627957865596\n",
            "step: 160, loss: 0.00042781123192980886\n",
            "step: 170, loss: 0.0010249684564769268\n",
            "step: 180, loss: 0.0014440594241023064\n",
            "step: 190, loss: 0.002678381744772196\n",
            "step: 200, loss: 0.00021115860727149993\n",
            "step: 210, loss: 0.0034925946965813637\n",
            "step: 220, loss: 0.00023548926401417702\n",
            "step: 230, loss: 0.03070197068154812\n",
            "step: 240, loss: 0.0011787381954491138\n",
            "step: 250, loss: 0.0014950195327401161\n",
            "step: 260, loss: 0.0001657562970649451\n",
            "step: 270, loss: 0.0002561829751357436\n",
            "step: 280, loss: 0.000185706332558766\n",
            "step: 290, loss: 0.0005668188678100705\n",
            "step: 300, loss: 0.009097833186388016\n",
            "step: 310, loss: 0.0015923258615657687\n",
            "step: 320, loss: 0.008829004131257534\n",
            "step: 330, loss: 0.007600284647196531\n",
            "step: 340, loss: 0.03609834238886833\n",
            "step: 350, loss: 0.0021838396787643433\n",
            "step: 360, loss: 0.0026836737524718046\n",
            "step: 370, loss: 0.029905417934060097\n",
            "step: 380, loss: 0.001477581448853016\n",
            "step: 390, loss: 0.013218735344707966\n",
            "step: 400, loss: 0.014260786585509777\n",
            "step: 410, loss: 0.004537804052233696\n",
            "step: 420, loss: 0.00255571654997766\n",
            "step: 430, loss: 0.0004965770640410483\n",
            "step: 440, loss: 0.0012835139641538262\n",
            "step: 450, loss: 0.0009881958831101656\n",
            "step: 460, loss: 0.00019660871475934982\n",
            "step: 470, loss: 0.006297852378338575\n",
            "step: 480, loss: 0.07533234357833862\n",
            "step: 490, loss: 0.0009439790737815201\n",
            "step: 500, loss: 0.0002379032812314108\n",
            "step: 510, loss: 0.000983702833764255\n",
            "step: 520, loss: 0.042054854333400726\n",
            "step: 530, loss: 0.0034413228277117014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9384328358208955, f1=0.9405204460966543, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06799587607383728\n",
            "step: 10, loss: 0.06703253835439682\n",
            "step: 20, loss: 0.10876984149217606\n",
            "step: 30, loss: 0.00018859707051888108\n",
            "step: 40, loss: 0.00286857015453279\n",
            "step: 50, loss: 0.0030753803439438343\n",
            "step: 60, loss: 0.0006892528035677969\n",
            "step: 70, loss: 0.0003699446970131248\n",
            "step: 80, loss: 0.01659478060901165\n",
            "step: 90, loss: 0.00037507389788515866\n",
            "step: 100, loss: 0.00011599207937251776\n",
            "step: 110, loss: 0.01781618408858776\n",
            "step: 120, loss: 0.029866747558116913\n",
            "step: 130, loss: 9.027989290188998e-05\n",
            "step: 140, loss: 0.0049683088436722755\n",
            "step: 150, loss: 0.002326685469597578\n",
            "step: 160, loss: 0.011545628309249878\n",
            "step: 170, loss: 0.00270992424339056\n",
            "step: 180, loss: 0.00020986229355912656\n",
            "step: 190, loss: 0.004741838667541742\n",
            "step: 200, loss: 0.0014775629388168454\n",
            "step: 210, loss: 0.0003584629448596388\n",
            "step: 220, loss: 0.002300610765814781\n",
            "step: 230, loss: 0.034497227519750595\n",
            "step: 240, loss: 0.0017643460305407643\n",
            "step: 250, loss: 0.0007206203881651163\n",
            "step: 260, loss: 0.00012421926658134907\n",
            "step: 270, loss: 0.008972407318651676\n",
            "step: 280, loss: 0.008146538399159908\n",
            "step: 290, loss: 0.0012867182958871126\n",
            "step: 300, loss: 0.0012046464253216982\n",
            "step: 310, loss: 0.0021315428894013166\n",
            "step: 320, loss: 0.009154465980827808\n",
            "step: 330, loss: 0.00010995581396855414\n",
            "step: 340, loss: 0.00014741953054908663\n",
            "step: 350, loss: 0.00010869866673601791\n",
            "step: 360, loss: 0.00016249343752861023\n",
            "step: 370, loss: 4.237650500726886e-05\n",
            "step: 380, loss: 0.0005208896473050117\n",
            "step: 390, loss: 0.12683922052383423\n",
            "step: 400, loss: 5.883727135369554e-05\n",
            "step: 410, loss: 0.00013389086234383285\n",
            "step: 420, loss: 0.004310526419430971\n",
            "step: 430, loss: 0.04041717201471329\n",
            "step: 440, loss: 0.00023342361964751035\n",
            "step: 450, loss: 0.0022320898715406656\n",
            "step: 460, loss: 0.0014062529662624002\n",
            "step: 470, loss: 4.632192212739028e-05\n",
            "step: 480, loss: 4.295071994420141e-05\n",
            "step: 490, loss: 0.00013429457612801343\n",
            "step: 500, loss: 0.0002922705898527056\n",
            "step: 510, loss: 0.004207635764032602\n",
            "step: 520, loss: 0.0016502132639288902\n",
            "step: 530, loss: 7.392298721242696e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9328984156570364, f1=0.9372384937238494, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011926073580980301\n",
            "step: 10, loss: 0.0002694444847293198\n",
            "step: 20, loss: 0.011160403490066528\n",
            "step: 30, loss: 0.00010812284017447382\n",
            "step: 40, loss: 4.67242207378149e-05\n",
            "step: 50, loss: 2.8017288059345447e-05\n",
            "step: 60, loss: 7.469010597560555e-05\n",
            "step: 70, loss: 3.5529545129975304e-05\n",
            "step: 80, loss: 0.001678480301052332\n",
            "step: 90, loss: 0.003310411935672164\n",
            "step: 100, loss: 3.749705138034187e-05\n",
            "step: 110, loss: 3.8573922211071476e-05\n",
            "step: 120, loss: 0.0003927024081349373\n",
            "step: 130, loss: 0.009734662249684334\n",
            "step: 140, loss: 0.0005683638737536967\n",
            "step: 150, loss: 4.2309540731366724e-05\n",
            "step: 160, loss: 0.0016282713040709496\n",
            "step: 170, loss: 0.018182404339313507\n",
            "step: 180, loss: 3.521380131132901e-05\n",
            "step: 190, loss: 5.467686787596904e-05\n",
            "step: 200, loss: 5.231235263636336e-05\n",
            "step: 210, loss: 5.517040699487552e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 220, loss: 0.0002553851227276027\n",
            "step: 230, loss: 3.445038964855485e-05\n",
            "step: 240, loss: 6.001653309795074e-05\n",
            "step: 250, loss: 0.00016137165948748589\n",
            "step: 260, loss: 0.00014399572683032602\n",
            "step: 270, loss: 3.576425297069363e-05\n",
            "step: 280, loss: 0.00034955187584273517\n",
            "step: 290, loss: 0.04327256605029106\n",
            "step: 300, loss: 0.00011769047705456614\n",
            "step: 310, loss: 0.02648099884390831\n",
            "step: 320, loss: 0.0022472140844911337\n",
            "step: 330, loss: 0.0007391462568193674\n",
            "step: 340, loss: 0.00010697147808969021\n",
            "step: 350, loss: 0.003315637120977044\n",
            "step: 360, loss: 9.466869232710451e-05\n",
            "step: 370, loss: 0.00021010944328736514\n",
            "step: 380, loss: 0.0005821297527290881\n",
            "step: 390, loss: 0.0016664366703480482\n",
            "step: 400, loss: 0.02617499604821205\n",
            "step: 410, loss: 9.941023745341226e-05\n",
            "step: 420, loss: 0.00010539124195929617\n",
            "step: 430, loss: 3.9377489883918315e-05\n",
            "step: 440, loss: 0.0006394766387529671\n",
            "step: 450, loss: 0.0003217713674530387\n",
            "step: 460, loss: 0.0002611148520372808\n",
            "step: 470, loss: 2.5368668502778746e-05\n",
            "step: 480, loss: 4.887972318101674e-05\n",
            "step: 490, loss: 0.00020186349865980446\n",
            "step: 500, loss: 0.002624484710395336\n",
            "step: 510, loss: 0.00021025861497037113\n",
            "step: 520, loss: 5.296014933264814e-05\n",
            "step: 530, loss: 0.000247227173531428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9395477618827872, f1=0.9362292051756008, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.72755010984838e-05\n",
            "step: 10, loss: 5.97715079493355e-05\n",
            "step: 20, loss: 3.91232970287092e-05\n",
            "step: 30, loss: 3.5258326533949e-05\n",
            "step: 40, loss: 0.00027139473240822554\n",
            "step: 50, loss: 0.0008254731656052172\n",
            "step: 60, loss: 7.742566231172532e-05\n",
            "step: 70, loss: 7.259551784954965e-05\n",
            "step: 80, loss: 2.8847942303400487e-05\n",
            "step: 90, loss: 5.1346298278076574e-05\n",
            "step: 100, loss: 2.93955654342426e-05\n",
            "step: 110, loss: 2.662358201632742e-05\n",
            "step: 120, loss: 4.590044045471586e-05\n",
            "step: 130, loss: 0.00010036721505457535\n",
            "step: 140, loss: 0.011874108575284481\n",
            "step: 150, loss: 7.415178697556257e-05\n",
            "step: 160, loss: 0.00012780888937413692\n",
            "step: 170, loss: 0.00013325655891094357\n",
            "step: 180, loss: 8.859784429660067e-05\n",
            "step: 190, loss: 6.968944217078388e-05\n",
            "step: 200, loss: 5.898157542105764e-05\n",
            "step: 210, loss: 0.00010636328806867823\n",
            "step: 220, loss: 0.14742842316627502\n",
            "step: 230, loss: 0.004637870006263256\n",
            "step: 240, loss: 0.0012472757371142507\n",
            "step: 250, loss: 6.655495235463604e-05\n",
            "step: 260, loss: 0.0012959244195371866\n",
            "step: 270, loss: 0.00014233242836780846\n",
            "step: 280, loss: 5.1499482651706785e-05\n",
            "step: 290, loss: 4.0800427086651325e-05\n",
            "step: 300, loss: 3.728066440089606e-05\n",
            "step: 310, loss: 0.00019831545068882406\n",
            "step: 320, loss: 0.0006812619976699352\n",
            "step: 330, loss: 0.001214688876643777\n",
            "step: 340, loss: 0.0001189399408758618\n",
            "step: 350, loss: 0.00048103596782311797\n",
            "step: 360, loss: 0.0001939982030307874\n",
            "step: 370, loss: 0.004417775198817253\n",
            "step: 380, loss: 0.000122765006381087\n",
            "step: 390, loss: 0.00043578058830462396\n",
            "step: 400, loss: 0.0010499906493350863\n",
            "step: 410, loss: 0.00382587150670588\n",
            "step: 420, loss: 0.0019727922044694424\n",
            "step: 430, loss: 0.00035061544622294605\n",
            "step: 440, loss: 0.012526760809123516\n",
            "step: 450, loss: 0.00010241942072752863\n",
            "step: 460, loss: 0.0019655926153063774\n",
            "step: 470, loss: 8.44297610456124e-05\n",
            "step: 480, loss: 4.882699067820795e-05\n",
            "step: 490, loss: 0.00012656931357923895\n",
            "step: 500, loss: 0.0022998214699327946\n",
            "step: 510, loss: 3.321706390124746e-05\n",
            "step: 520, loss: 0.00016481241618748754\n",
            "step: 530, loss: 0.0016329140635207295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9378214118747078, f1=0.936269915651359, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00503041036427021\n",
            "step: 10, loss: 0.00035248565836809576\n",
            "step: 20, loss: 0.0003822758444584906\n",
            "step: 30, loss: 4.65074299427215e-05\n",
            "step: 40, loss: 0.00025793988606892526\n",
            "step: 50, loss: 0.000133704103063792\n",
            "step: 60, loss: 0.0314672626554966\n",
            "step: 70, loss: 0.008620085194706917\n",
            "step: 80, loss: 6.869986827950925e-05\n",
            "step: 90, loss: 0.00032115919748321176\n",
            "step: 100, loss: 0.0004788497753906995\n",
            "step: 110, loss: 0.00011981306306552142\n",
            "step: 120, loss: 7.510127034038305e-05\n",
            "step: 130, loss: 3.2449501304654405e-05\n",
            "step: 140, loss: 0.0006699027144350111\n",
            "step: 150, loss: 0.00010125806875294074\n",
            "step: 160, loss: 0.00011229427764192224\n",
            "step: 170, loss: 5.205456182011403e-05\n",
            "step: 180, loss: 0.00017813453450798988\n",
            "step: 190, loss: 0.0010485276579856873\n",
            "step: 200, loss: 0.002807005774229765\n",
            "step: 210, loss: 0.0012249695137143135\n",
            "step: 220, loss: 0.004519581329077482\n",
            "step: 230, loss: 0.0002856283390428871\n",
            "step: 240, loss: 2.938354191428516e-05\n",
            "step: 250, loss: 2.528996265027672e-05\n",
            "step: 260, loss: 3.3778971555875614e-05\n",
            "step: 270, loss: 8.180732402252033e-05\n",
            "step: 280, loss: 5.865234197699465e-05\n",
            "step: 290, loss: 6.375994416885078e-05\n",
            "step: 300, loss: 6.080327511881478e-05\n",
            "step: 310, loss: 0.00012479216093197465\n",
            "step: 320, loss: 0.00015232579607982188\n",
            "step: 330, loss: 0.0010676145320758224\n",
            "step: 340, loss: 3.286992432549596e-05\n",
            "step: 350, loss: 0.0003992553392890841\n",
            "step: 360, loss: 2.7584435883909464e-05\n",
            "step: 370, loss: 0.0005783025408163667\n",
            "step: 380, loss: 3.174941593897529e-05\n",
            "step: 390, loss: 3.846706385957077e-05\n",
            "step: 400, loss: 3.470237788860686e-05\n",
            "step: 410, loss: 3.226376065867953e-05\n",
            "step: 420, loss: 5.561229045270011e-05\n",
            "step: 430, loss: 2.4482045773766004e-05\n",
            "step: 440, loss: 0.00034240781678818166\n",
            "step: 450, loss: 0.007487325929105282\n",
            "step: 460, loss: 7.956069021020085e-05\n",
            "step: 470, loss: 3.025932892342098e-05\n",
            "step: 480, loss: 0.001854085479862988\n",
            "step: 490, loss: 3.786845991271548e-05\n",
            "step: 500, loss: 0.029485665261745453\n",
            "step: 510, loss: 3.8027817936381325e-05\n",
            "step: 520, loss: 3.8945567212067544e-05\n",
            "step: 530, loss: 5.135711035109125e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9372026641294006, f1=0.932503590234562, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.689529355848208e-05\n",
            "step: 10, loss: 0.00022227730369195342\n",
            "step: 20, loss: 5.7116463722195476e-05\n",
            "step: 30, loss: 0.00038012044387869537\n",
            "step: 40, loss: 2.9216087568784133e-05\n",
            "step: 50, loss: 0.09654591232538223\n",
            "step: 60, loss: 0.0015381363919004798\n",
            "step: 70, loss: 4.310827353037894e-05\n",
            "step: 80, loss: 3.369259138708003e-05\n",
            "step: 90, loss: 0.000107056628621649\n",
            "step: 100, loss: 0.0002546820614952594\n",
            "step: 110, loss: 3.852828376693651e-05\n",
            "step: 120, loss: 1.6733754819142632e-05\n",
            "step: 130, loss: 0.001653663464821875\n",
            "step: 140, loss: 0.00012041413719998673\n",
            "step: 150, loss: 1.7553216821397655e-05\n",
            "step: 160, loss: 1.8898077541962266e-05\n",
            "step: 170, loss: 2.755481909844093e-05\n",
            "step: 180, loss: 5.438872176455334e-05\n",
            "step: 190, loss: 4.00175922550261e-05\n",
            "step: 200, loss: 0.0001432657882105559\n",
            "step: 210, loss: 4.4464755774242803e-05\n",
            "step: 220, loss: 3.716937135322951e-05\n",
            "step: 230, loss: 2.419086013105698e-05\n",
            "step: 240, loss: 0.0005161790177226067\n",
            "step: 250, loss: 1.8525506675359793e-05\n",
            "step: 260, loss: 3.698961154441349e-05\n",
            "step: 270, loss: 3.943780029658228e-05\n",
            "step: 280, loss: 0.0018701516091823578\n",
            "step: 290, loss: 0.00013000896433368325\n",
            "step: 300, loss: 0.0005019104573875666\n",
            "step: 310, loss: 0.0011232278775423765\n",
            "step: 320, loss: 2.0138393665547483e-05\n",
            "step: 330, loss: 0.0003111725382041186\n",
            "step: 340, loss: 0.00019417675503063947\n",
            "step: 350, loss: 0.0034665013663470745\n",
            "step: 360, loss: 0.00013250214396975935\n",
            "step: 370, loss: 4.1395356674911454e-05\n",
            "step: 380, loss: 0.00023805254022590816\n",
            "step: 390, loss: 3.5636883694678545e-05\n",
            "step: 400, loss: 3.158908657496795e-05\n",
            "step: 410, loss: 0.00039585481863468885\n",
            "step: 420, loss: 0.007285858504474163\n",
            "step: 430, loss: 0.023881467059254646\n",
            "step: 440, loss: 2.0756891899509355e-05\n",
            "step: 450, loss: 5.152906305738725e-05\n",
            "step: 460, loss: 1.7605456378078088e-05\n",
            "step: 470, loss: 2.057435267488472e-05\n",
            "step: 480, loss: 6.835618842160329e-05\n",
            "step: 490, loss: 0.0009392238571308553\n",
            "step: 500, loss: 2.6455731131136417e-05\n",
            "step: 510, loss: 8.051779877860099e-05\n",
            "step: 520, loss: 2.388966277067084e-05\n",
            "step: 530, loss: 3.6687579267891124e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9386213408876298, f1=0.9381199811053378, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00440469104796648\n",
            "step: 10, loss: 0.0010675902012735605\n",
            "step: 20, loss: 3.107814700342715e-05\n",
            "step: 30, loss: 3.733986523002386e-05\n",
            "step: 40, loss: 1.97101580852177e-05\n",
            "step: 50, loss: 0.0021939247380942106\n",
            "step: 60, loss: 2.3401387807098217e-05\n",
            "step: 70, loss: 4.306216578697786e-05\n",
            "step: 80, loss: 2.1225401724223047e-05\n",
            "step: 90, loss: 0.02461281791329384\n",
            "step: 100, loss: 9.514429257251322e-05\n",
            "step: 110, loss: 2.9950817406643182e-05\n",
            "step: 120, loss: 2.5476319933659397e-05\n",
            "step: 130, loss: 3.2564406865276396e-05\n",
            "step: 140, loss: 2.3580501874675974e-05\n",
            "step: 150, loss: 2.596045669633895e-05\n",
            "step: 160, loss: 4.7134100896073505e-05\n",
            "step: 170, loss: 2.1185222067288123e-05\n",
            "step: 180, loss: 0.004596621263772249\n",
            "step: 190, loss: 3.905143603333272e-05\n",
            "step: 200, loss: 2.2872596673551016e-05\n",
            "step: 210, loss: 3.164337977068499e-05\n",
            "step: 220, loss: 3.7731919292127714e-05\n",
            "step: 230, loss: 1.8059872672893107e-05\n",
            "step: 240, loss: 0.00018580886535346508\n",
            "step: 250, loss: 6.885027687530965e-05\n",
            "step: 260, loss: 1.4595459106203634e-05\n",
            "step: 270, loss: 1.66889549291227e-05\n",
            "step: 280, loss: 3.978003223892301e-05\n",
            "step: 290, loss: 4.9779282562667504e-05\n",
            "step: 300, loss: 2.2433083358919248e-05\n",
            "step: 310, loss: 0.0011569684138521552\n",
            "step: 320, loss: 0.0004577790678013116\n",
            "step: 330, loss: 6.883658352307975e-05\n",
            "step: 340, loss: 1.7460099115851335e-05\n",
            "step: 350, loss: 5.2553692512447014e-05\n",
            "step: 360, loss: 1.8085840565618128e-05\n",
            "step: 370, loss: 5.7479250244796276e-05\n",
            "step: 380, loss: 0.00013507767289411277\n",
            "step: 390, loss: 0.00027401160332374275\n",
            "step: 400, loss: 0.00016665221482980996\n",
            "step: 410, loss: 0.0010000012116506696\n",
            "step: 420, loss: 2.2142525267554447e-05\n",
            "step: 430, loss: 0.0027553376276046038\n",
            "step: 440, loss: 2.892489283112809e-05\n",
            "step: 450, loss: 3.007252780662384e-05\n",
            "step: 460, loss: 0.0010378542356193066\n",
            "step: 470, loss: 0.00027996173594146967\n",
            "step: 480, loss: 3.787223613471724e-05\n",
            "step: 490, loss: 4.085967520950362e-05\n",
            "step: 500, loss: 4.1406514355912805e-05\n",
            "step: 510, loss: 3.913118052878417e-05\n",
            "step: 520, loss: 0.0019692620262503624\n",
            "step: 530, loss: 1.8901620933320373e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9342789598108747, f1=0.9332702318977756, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.565096838225145e-05\n",
            "step: 10, loss: 1.1164598618051969e-05\n",
            "step: 20, loss: 0.00026174605591222644\n",
            "step: 30, loss: 2.8825168556068093e-05\n",
            "step: 40, loss: 3.27176385326311e-05\n",
            "step: 50, loss: 0.00011363835074007511\n",
            "step: 60, loss: 3.643763193394989e-05\n",
            "step: 70, loss: 2.9479473596438766e-05\n",
            "step: 80, loss: 2.3919357772683725e-05\n",
            "step: 90, loss: 1.7810234567150474e-05\n",
            "step: 100, loss: 4.840381006943062e-05\n",
            "step: 110, loss: 2.029120150837116e-05\n",
            "step: 120, loss: 2.517788379918784e-05\n",
            "step: 130, loss: 0.022155873477458954\n",
            "step: 140, loss: 0.0003215074248146266\n",
            "step: 150, loss: 1.371259531879332e-05\n",
            "step: 160, loss: 0.0002720175834838301\n",
            "step: 170, loss: 0.0014779387274757028\n",
            "step: 180, loss: 5.0830327381845564e-05\n",
            "step: 190, loss: 0.0006724981940351427\n",
            "step: 200, loss: 7.00897944625467e-05\n",
            "step: 210, loss: 7.49796672607772e-05\n",
            "step: 220, loss: 1.639848414924927e-05\n",
            "step: 230, loss: 2.5196712158503942e-05\n",
            "step: 240, loss: 1.4595476386602968e-05\n",
            "step: 250, loss: 1.7635094991419464e-05\n",
            "step: 260, loss: 3.628390913945623e-05\n",
            "step: 270, loss: 1.7772903447621502e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 280, loss: 1.6320194845320657e-05\n",
            "step: 290, loss: 5.4237429139902815e-05\n",
            "step: 300, loss: 1.992245961446315e-05\n",
            "step: 310, loss: 0.0007112135645002127\n",
            "step: 320, loss: 4.472491855267435e-05\n",
            "step: 330, loss: 3.9687663957010955e-05\n",
            "step: 340, loss: 0.00011470106255728751\n",
            "step: 350, loss: 2.206015597039368e-05\n",
            "step: 360, loss: 8.08430922916159e-05\n",
            "step: 370, loss: 2.1352761905291118e-05\n",
            "step: 380, loss: 0.0005731277633458376\n",
            "step: 390, loss: 0.018564000725746155\n",
            "step: 400, loss: 7.32035405235365e-05\n",
            "step: 410, loss: 3.227445631637238e-05\n",
            "step: 420, loss: 4.488021295401268e-05\n",
            "step: 430, loss: 2.2473945136880502e-05\n",
            "step: 440, loss: 4.316014383221045e-05\n",
            "step: 450, loss: 0.00021949289657641202\n",
            "step: 460, loss: 2.5211209504050203e-05\n",
            "step: 470, loss: 1.5098359654075466e-05\n",
            "step: 480, loss: 1.895013701869175e-05\n",
            "step: 490, loss: 2.542346737754997e-05\n",
            "step: 500, loss: 1.1850042938021943e-05\n",
            "step: 510, loss: 5.03391565871425e-05\n",
            "step: 520, loss: 6.34497991995886e-05\n",
            "step: 530, loss: 1.8976092178490944e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9358914365933552, f1=0.9388136384866884, best_f1=0.9372693726937269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2399366571335122e-05\n",
            "step: 10, loss: 0.00010113115422427654\n",
            "step: 20, loss: 3.914320404874161e-05\n",
            "step: 30, loss: 2.7635587684926577e-05\n",
            "step: 40, loss: 0.07881198823451996\n",
            "step: 50, loss: 6.331608892651275e-05\n",
            "step: 60, loss: 1.3720030437980313e-05\n",
            "step: 70, loss: 4.462393917492591e-05\n",
            "step: 80, loss: 3.548246604623273e-05\n",
            "step: 90, loss: 2.445202881062869e-05\n",
            "step: 100, loss: 0.00011924151476705447\n",
            "step: 110, loss: 2.35506086028181e-05\n",
            "step: 120, loss: 0.0001391183614032343\n",
            "step: 130, loss: 0.0780295729637146\n",
            "step: 140, loss: 1.949714896909427e-05\n",
            "step: 150, loss: 9.354241046821699e-05\n",
            "step: 160, loss: 1.850687840487808e-05\n",
            "step: 170, loss: 2.4678649424458854e-05\n",
            "step: 180, loss: 1.0695219316403382e-05\n",
            "step: 190, loss: 2.5975201424444094e-05\n",
            "step: 200, loss: 1.8648348486749455e-05\n",
            "step: 210, loss: 0.001919183530844748\n",
            "step: 220, loss: 1.7813883459893987e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 230, loss: 3.274998380220495e-05\n",
            "step: 240, loss: 2.6573854484013282e-05\n",
            "step: 250, loss: 2.7805797799373977e-05\n",
            "step: 260, loss: 1.740427251206711e-05\n",
            "step: 270, loss: 1.820517354644835e-05\n",
            "step: 280, loss: 1.7162066797027364e-05\n",
            "step: 290, loss: 1.2479586985136848e-05\n",
            "step: 300, loss: 1.682675429037772e-05\n",
            "step: 310, loss: 7.158584048738703e-05\n",
            "step: 320, loss: 1.617471752979327e-05\n",
            "step: 330, loss: 2.460783434798941e-05\n",
            "step: 340, loss: 2.4212080461438745e-05\n",
            "step: 350, loss: 1.439800143998582e-05\n",
            "step: 360, loss: 0.0023999824188649654\n",
            "step: 370, loss: 0.0012463576858863235\n",
            "step: 380, loss: 4.973066461388953e-05\n",
            "step: 390, loss: 6.53021561447531e-05\n",
            "step: 400, loss: 2.1572093828581274e-05\n",
            "step: 410, loss: 1.8093343896907754e-05\n",
            "step: 420, loss: 2.988118285429664e-05\n",
            "step: 430, loss: 6.167616811580956e-05\n",
            "step: 440, loss: 9.291413152823225e-05\n",
            "step: 450, loss: 1.2989889000891708e-05\n",
            "step: 460, loss: 1.9896237063221633e-05\n",
            "step: 470, loss: 0.0038878240156918764\n",
            "step: 480, loss: 1.1443987204984296e-05\n",
            "step: 490, loss: 1.5429930499522015e-05\n",
            "step: 500, loss: 2.0227824279572815e-05\n",
            "step: 510, loss: 1.9869738025590777e-05\n",
            "step: 520, loss: 1.8566219296189956e-05\n",
            "step: 530, loss: 3.054370608879253e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9354686764013189, f1=0.9353468617272298, best_f1=0.9372693726937269\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:25, 226.71it/s]\n",
            "load_f1 = 0.9372114496768237\n",
            "real_f1 = 0.9359742054352833\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 230.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a6e4e8-b920-4d9a-b65a-72bf3a1ecd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5733955502510071\n",
            "step: 10, loss: 0.3713187277317047\n",
            "step: 20, loss: 0.36709970235824585\n",
            "step: 30, loss: 0.3184810280799866\n",
            "step: 40, loss: 0.18824201822280884\n",
            "step: 50, loss: 0.39790093898773193\n",
            "step: 60, loss: 0.18135006725788116\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.1604800820350647\n",
            "step: 80, loss: 0.2055835723876953\n",
            "step: 90, loss: 0.3578401505947113\n",
            "step: 100, loss: 0.4906202554702759\n",
            "step: 110, loss: 0.24414187669754028\n",
            "step: 120, loss: 0.18905718624591827\n",
            "step: 130, loss: 0.19495978951454163\n",
            "step: 140, loss: 0.16580449044704437\n",
            "step: 150, loss: 0.18106231093406677\n",
            "step: 160, loss: 0.30233222246170044\n",
            "step: 170, loss: 0.29064950346946716\n",
            "step: 180, loss: 0.13148073852062225\n",
            "step: 190, loss: 0.2088148444890976\n",
            "step: 200, loss: 0.28313392400741577\n",
            "step: 210, loss: 0.25537511706352234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5850091407678244, f1=0.6077348066298343, best_f1=0.6077348066298343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11504288017749786\n",
            "step: 10, loss: 0.2866349518299103\n",
            "step: 20, loss: 0.274432510137558\n",
            "step: 30, loss: 0.21914899349212646\n",
            "step: 40, loss: 0.17545461654663086\n",
            "step: 50, loss: 0.16655494272708893\n",
            "step: 60, loss: 0.4666720926761627\n",
            "step: 70, loss: 0.1616833359003067\n",
            "step: 80, loss: 0.23162460327148438\n",
            "step: 90, loss: 0.09535705298185349\n",
            "step: 100, loss: 0.01685061678290367\n",
            "step: 110, loss: 0.05549604073166847\n",
            "step: 120, loss: 0.13794544339179993\n",
            "step: 130, loss: 0.01618514396250248\n",
            "step: 140, loss: 0.20250266790390015\n",
            "step: 150, loss: 0.2224917709827423\n",
            "step: 160, loss: 0.171671062707901\n",
            "step: 170, loss: 0.14164316654205322\n",
            "step: 180, loss: 0.18674582242965698\n",
            "step: 190, loss: 0.2580021023750305\n",
            "step: 200, loss: 0.03839583694934845\n",
            "step: 210, loss: 0.11288551986217499\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6775510204081632, f1=0.632967032967033, best_f1=0.632967032967033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05751253664493561\n",
            "step: 10, loss: 0.14634303748607635\n",
            "step: 20, loss: 0.05875386297702789\n",
            "step: 30, loss: 0.23320911824703217\n",
            "step: 40, loss: 0.0607774518430233\n",
            "step: 50, loss: 0.1275242567062378\n",
            "step: 60, loss: 0.15855225920677185\n",
            "step: 70, loss: 0.07789168506860733\n",
            "step: 80, loss: 0.17176412045955658\n",
            "step: 90, loss: 0.036958497017621994\n",
            "step: 100, loss: 0.12660177052021027\n",
            "step: 110, loss: 0.12890490889549255\n",
            "step: 120, loss: 0.12659011781215668\n",
            "step: 130, loss: 0.08473599702119827\n",
            "step: 140, loss: 0.09986744821071625\n",
            "step: 150, loss: 0.22099198400974274\n",
            "step: 160, loss: 0.04067911580204964\n",
            "step: 170, loss: 0.19114315509796143\n",
            "step: 180, loss: 0.11946108192205429\n",
            "step: 190, loss: 0.18460263311862946\n",
            "step: 200, loss: 0.06298654526472092\n",
            "step: 210, loss: 0.1366962492465973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6692456479690523, f1=0.6482213438735178, best_f1=0.632967032967033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051812998950481415\n",
            "step: 10, loss: 0.06404311209917068\n",
            "step: 20, loss: 0.08068463951349258\n",
            "step: 30, loss: 0.1578686684370041\n",
            "step: 40, loss: 0.00781615637242794\n",
            "step: 50, loss: 0.09162185341119766\n",
            "step: 60, loss: 0.09407063573598862\n",
            "step: 70, loss: 0.2025119811296463\n",
            "step: 80, loss: 0.03521097078919411\n",
            "step: 90, loss: 0.010519836097955704\n",
            "step: 100, loss: 0.2267012745141983\n",
            "step: 110, loss: 0.1093812808394432\n",
            "step: 120, loss: 0.14667707681655884\n",
            "step: 130, loss: 0.3019335865974426\n",
            "step: 140, loss: 0.11686531454324722\n",
            "step: 150, loss: 0.10588313639163971\n",
            "step: 160, loss: 0.020076945424079895\n",
            "step: 170, loss: 0.1796530783176422\n",
            "step: 180, loss: 0.2763211131095886\n",
            "step: 190, loss: 0.08688779920339584\n",
            "step: 200, loss: 0.21716831624507904\n",
            "step: 210, loss: 0.08632662147283554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6792452830188679, f1=0.6368932038834951, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09568804502487183\n",
            "step: 10, loss: 0.057533688843250275\n",
            "step: 20, loss: 0.04131271690130234\n",
            "step: 30, loss: 0.045047808438539505\n",
            "step: 40, loss: 0.021451953798532486\n",
            "step: 50, loss: 0.09120377153158188\n",
            "step: 60, loss: 0.04456508904695511\n",
            "step: 70, loss: 0.003989659249782562\n",
            "step: 80, loss: 0.04168357327580452\n",
            "step: 90, loss: 0.042065560817718506\n",
            "step: 100, loss: 0.002157706068828702\n",
            "step: 110, loss: 0.11103322356939316\n",
            "step: 120, loss: 0.03534538298845291\n",
            "step: 130, loss: 0.06885265558958054\n",
            "step: 140, loss: 0.03575030714273453\n",
            "step: 150, loss: 0.03978215157985687\n",
            "step: 160, loss: 0.13861480355262756\n",
            "step: 170, loss: 0.022815169766545296\n",
            "step: 180, loss: 0.012851452454924583\n",
            "step: 190, loss: 0.03566928580403328\n",
            "step: 200, loss: 0.17914804816246033\n",
            "step: 210, loss: 0.018270913511514664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6705882352941176, f1=0.6201232032854209, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027922097593545914\n",
            "step: 10, loss: 0.08535785973072052\n",
            "step: 20, loss: 0.022901857271790504\n",
            "step: 30, loss: 0.026438318192958832\n",
            "step: 40, loss: 0.0015524636255577207\n",
            "step: 50, loss: 0.0030505801551043987\n",
            "step: 60, loss: 0.016909539699554443\n",
            "step: 70, loss: 0.013842637650668621\n",
            "step: 80, loss: 0.11936309188604355\n",
            "step: 90, loss: 0.06591818481683731\n",
            "step: 100, loss: 0.004460717551410198\n",
            "step: 110, loss: 0.005771985277533531\n",
            "step: 120, loss: 0.028642652556300163\n",
            "step: 130, loss: 0.03366711735725403\n",
            "step: 140, loss: 0.02928776480257511\n",
            "step: 150, loss: 0.005719418171793222\n",
            "step: 160, loss: 0.007153604179620743\n",
            "step: 170, loss: 0.15575191378593445\n",
            "step: 180, loss: 0.1191861480474472\n",
            "step: 190, loss: 0.10973567515611649\n",
            "step: 200, loss: 0.009479215368628502\n",
            "step: 210, loss: 0.06116138771176338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6569037656903767, f1=0.6180257510729614, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008278429508209229\n",
            "step: 10, loss: 0.0035256787668913603\n",
            "step: 20, loss: 0.008274038322269917\n",
            "step: 30, loss: 0.017354819923639297\n",
            "step: 40, loss: 0.015768440440297127\n",
            "step: 50, loss: 0.08428175002336502\n",
            "step: 60, loss: 0.006772833876311779\n",
            "step: 70, loss: 0.004278141539543867\n",
            "step: 80, loss: 0.024776089936494827\n",
            "step: 90, loss: 0.032232966274023056\n",
            "step: 100, loss: 0.005152019206434488\n",
            "step: 110, loss: 0.08250272274017334\n",
            "step: 120, loss: 0.03501302748918533\n",
            "step: 130, loss: 0.015573974698781967\n",
            "step: 140, loss: 0.011070461943745613\n",
            "step: 150, loss: 0.0022171377204358578\n",
            "step: 160, loss: 0.006250392179936171\n",
            "step: 170, loss: 0.0009108501835726202\n",
            "step: 180, loss: 0.025411006063222885\n",
            "step: 190, loss: 0.058279991149902344\n",
            "step: 200, loss: 0.01262367982417345\n",
            "step: 210, loss: 0.0026801126077771187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6589147286821705, f1=0.6415841584158416, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002416138770058751\n",
            "step: 10, loss: 0.06002920866012573\n",
            "step: 20, loss: 0.005222855601459742\n",
            "step: 30, loss: 0.05575478821992874\n",
            "step: 40, loss: 0.007342812605202198\n",
            "step: 50, loss: 0.0023240118753165007\n",
            "step: 60, loss: 0.038366034626960754\n",
            "step: 70, loss: 0.03333043307065964\n",
            "step: 80, loss: 0.06297208368778229\n",
            "step: 90, loss: 0.11462381482124329\n",
            "step: 100, loss: 0.004734581336379051\n",
            "step: 110, loss: 0.07998785376548767\n",
            "step: 120, loss: 0.011536234058439732\n",
            "step: 130, loss: 0.0005707366508431733\n",
            "step: 140, loss: 0.048573292791843414\n",
            "step: 150, loss: 0.003182176500558853\n",
            "step: 160, loss: 0.05735837668180466\n",
            "step: 170, loss: 0.001965414034202695\n",
            "step: 180, loss: 0.012330637313425541\n",
            "step: 190, loss: 0.04164749011397362\n",
            "step: 200, loss: 0.012156681157648563\n",
            "step: 210, loss: 0.08181098103523254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6535087719298245, f1=0.6294642857142857, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007094184402376413\n",
            "step: 10, loss: 0.0005852296599186957\n",
            "step: 20, loss: 0.0010013481369242072\n",
            "step: 30, loss: 0.008020948618650436\n",
            "step: 40, loss: 0.03315671160817146\n",
            "step: 50, loss: 0.009265684522688389\n",
            "step: 60, loss: 0.0835811197757721\n",
            "step: 70, loss: 0.008226323872804642\n",
            "step: 80, loss: 0.0016104532405734062\n",
            "step: 90, loss: 0.00016367409261874855\n",
            "step: 100, loss: 0.0016620473470538855\n",
            "step: 110, loss: 0.0036736472975462675\n",
            "step: 120, loss: 0.0011509967735037208\n",
            "step: 130, loss: 0.0034613085445016623\n",
            "step: 140, loss: 0.01771625690162182\n",
            "step: 150, loss: 0.07605358213186264\n",
            "step: 160, loss: 0.0019494141452014446\n",
            "step: 170, loss: 0.0022635518107563257\n",
            "step: 180, loss: 0.015287145040929317\n",
            "step: 190, loss: 0.0006973569979891181\n",
            "step: 200, loss: 0.0756041407585144\n",
            "step: 210, loss: 0.008063253946602345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6567717996289425, f1=0.6420664206642066, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06317079067230225\n",
            "step: 10, loss: 0.00370375020429492\n",
            "step: 20, loss: 0.0003953636914957315\n",
            "step: 30, loss: 0.010971635580062866\n",
            "step: 40, loss: 0.0032379161566495895\n",
            "step: 50, loss: 0.014480970799922943\n",
            "step: 60, loss: 0.017240501940250397\n",
            "step: 70, loss: 0.010967740789055824\n",
            "step: 80, loss: 0.023571643978357315\n",
            "step: 90, loss: 0.01328783668577671\n",
            "step: 100, loss: 0.004692047834396362\n",
            "step: 110, loss: 0.0002649713715072721\n",
            "step: 120, loss: 0.0013323907041922212\n",
            "step: 130, loss: 0.03983087092638016\n",
            "step: 140, loss: 0.047388263046741486\n",
            "step: 150, loss: 0.054285161197185516\n",
            "step: 160, loss: 0.0007919397903606296\n",
            "step: 170, loss: 0.025615248829126358\n",
            "step: 180, loss: 0.008417908102273941\n",
            "step: 190, loss: 0.02920171059668064\n",
            "step: 200, loss: 0.007857983000576496\n",
            "step: 210, loss: 0.03343847393989563\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6758893280632412, f1=0.6719367588932806, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03578601032495499\n",
            "step: 10, loss: 0.007416334003210068\n",
            "step: 20, loss: 0.02028139866888523\n",
            "step: 30, loss: 0.00027067679911851883\n",
            "step: 40, loss: 0.013058636337518692\n",
            "step: 50, loss: 0.026395544409751892\n",
            "step: 60, loss: 0.018059635534882545\n",
            "step: 70, loss: 0.0002707219682633877\n",
            "step: 80, loss: 0.017121776938438416\n",
            "step: 90, loss: 0.037712834775447845\n",
            "step: 100, loss: 0.07526945322751999\n",
            "step: 110, loss: 0.038564931601285934\n",
            "step: 120, loss: 0.006012878380715847\n",
            "step: 130, loss: 0.028216561302542686\n",
            "step: 140, loss: 0.00026020358200185\n",
            "step: 150, loss: 0.00016169201990123838\n",
            "step: 160, loss: 0.015122655779123306\n",
            "step: 170, loss: 0.02366567589342594\n",
            "step: 180, loss: 0.0004538492939900607\n",
            "step: 190, loss: 0.0009096033172681928\n",
            "step: 200, loss: 0.00039001982077024877\n",
            "step: 210, loss: 0.002178475959226489\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6582809224318658, f1=0.6469344608879491, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017050277441740036\n",
            "step: 10, loss: 0.0010644706198945642\n",
            "step: 20, loss: 0.007605219259858131\n",
            "step: 30, loss: 0.000805146002676338\n",
            "step: 40, loss: 0.0009564099018462002\n",
            "step: 50, loss: 0.0019510139245539904\n",
            "step: 60, loss: 0.0037728322204202414\n",
            "step: 70, loss: 0.0007182184490375221\n",
            "step: 80, loss: 0.0019978235941380262\n",
            "step: 90, loss: 0.007131255231797695\n",
            "step: 100, loss: 0.009444587863981724\n",
            "step: 110, loss: 0.0015145433135330677\n",
            "step: 120, loss: 0.021330295130610466\n",
            "step: 130, loss: 0.004181503318250179\n",
            "step: 140, loss: 0.000517792534083128\n",
            "step: 150, loss: 0.024576816707849503\n",
            "step: 160, loss: 0.00010677377576939762\n",
            "step: 170, loss: 0.022890770807862282\n",
            "step: 180, loss: 0.0009321856196038425\n",
            "step: 190, loss: 0.0017019638326019049\n",
            "step: 200, loss: 0.0029246939811855555\n",
            "step: 210, loss: 0.02020195685327053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6556701030927835, f1=0.6502057613168725, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05056071653962135\n",
            "step: 10, loss: 0.00026387142133899033\n",
            "step: 20, loss: 0.0006034334073774517\n",
            "step: 30, loss: 0.022170711308717728\n",
            "step: 40, loss: 0.0031169175636023283\n",
            "step: 50, loss: 0.008511797524988651\n",
            "step: 60, loss: 0.00024624221259728074\n",
            "step: 70, loss: 0.024168912321329117\n",
            "step: 80, loss: 0.004006620030850172\n",
            "step: 90, loss: 0.005466022063046694\n",
            "step: 100, loss: 0.002064575208351016\n",
            "step: 110, loss: 0.00027759766089729965\n",
            "step: 120, loss: 0.00020538957323879004\n",
            "step: 130, loss: 0.00046910601668059826\n",
            "step: 140, loss: 0.009500900283455849\n",
            "step: 150, loss: 0.001036143279634416\n",
            "step: 160, loss: 0.041342347860336304\n",
            "step: 170, loss: 0.0005326003301888704\n",
            "step: 180, loss: 0.03860565274953842\n",
            "step: 190, loss: 0.0009754607453942299\n",
            "step: 200, loss: 0.0007276707328855991\n",
            "step: 210, loss: 0.0008337446488440037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6612576064908721, f1=0.6363636363636364, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.697859059087932e-05\n",
            "step: 10, loss: 0.02944491058588028\n",
            "step: 20, loss: 0.00019368858193047345\n",
            "step: 30, loss: 0.0001204886066261679\n",
            "step: 40, loss: 0.00020602322183549404\n",
            "step: 50, loss: 0.009936240501701832\n",
            "step: 60, loss: 0.00023806760145816952\n",
            "step: 70, loss: 0.00015022599836811423\n",
            "step: 80, loss: 0.00015682470984756947\n",
            "step: 90, loss: 0.001073510735295713\n",
            "step: 100, loss: 0.001714340760372579\n",
            "step: 110, loss: 0.0003871456428896636\n",
            "step: 120, loss: 0.02496790513396263\n",
            "step: 130, loss: 0.001227657776325941\n",
            "step: 140, loss: 0.020823931321501732\n",
            "step: 150, loss: 0.0001768813090166077\n",
            "step: 160, loss: 0.002636507386341691\n",
            "step: 170, loss: 0.0002954751253128052\n",
            "step: 180, loss: 0.003143428824841976\n",
            "step: 190, loss: 0.0010589767480269074\n",
            "step: 200, loss: 0.0002824451948981732\n",
            "step: 210, loss: 0.0014817360788583755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6557377049180328, f1=0.6502057613168725, best_f1=0.6368932038834951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00024162602494470775\n",
            "step: 10, loss: 0.0002615316188894212\n",
            "step: 20, loss: 0.0032558036036789417\n",
            "step: 30, loss: 0.011076287366449833\n",
            "step: 40, loss: 0.0016866764053702354\n",
            "step: 50, loss: 0.00015882377920206636\n",
            "step: 60, loss: 0.016376594081521034\n",
            "step: 70, loss: 0.0002752217696979642\n",
            "step: 80, loss: 0.00038266999763436615\n",
            "step: 90, loss: 0.006625500041991472\n",
            "step: 100, loss: 0.00014942113193683326\n",
            "step: 110, loss: 0.0001317718852078542\n",
            "step: 120, loss: 0.00023473154578823596\n",
            "step: 130, loss: 0.0001515410258434713\n",
            "step: 140, loss: 0.0010980631923303008\n",
            "step: 150, loss: 0.00016650374163873494\n",
            "step: 160, loss: 0.002476288704201579\n",
            "step: 170, loss: 0.00015705816622357816\n",
            "step: 180, loss: 0.00010756661504274234\n",
            "step: 190, loss: 0.00020750498515553772\n",
            "step: 200, loss: 0.004770062398165464\n",
            "step: 210, loss: 0.0003288961888756603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.652542372881356, f1=0.6279569892473118, best_f1=0.6368932038834951\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 402.19it/s]\n",
            "load_f1 = 0.6732283464566929\n",
            "real_f1 = 0.6640625\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 217.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a758ad4-f7a9-49d7-edcf-8a525784d056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5437835454940796\n",
            "step: 10, loss: 0.3844553232192993\n",
            "step: 20, loss: 0.26937153935432434\n",
            "step: 30, loss: 0.43762698769569397\n",
            "step: 40, loss: 0.42785051465034485\n",
            "step: 50, loss: 0.2811547815799713\n",
            "step: 60, loss: 0.25477951765060425\n",
            "step: 70, loss: 0.2907724976539612\n",
            "step: 80, loss: 0.22229814529418945\n",
            "step: 90, loss: 0.2823222577571869\n",
            "step: 100, loss: 0.31684648990631104\n",
            "step: 110, loss: 0.33642131090164185\n",
            "step: 120, loss: 0.07836171239614487\n",
            "step: 130, loss: 0.12292157113552094\n",
            "step: 140, loss: 0.03438432142138481\n",
            "step: 150, loss: 0.21369308233261108\n",
            "step: 160, loss: 0.053324781358242035\n",
            "step: 170, loss: 0.15360894799232483\n",
            "step: 180, loss: 0.09067252278327942\n",
            "step: 190, loss: 0.17922955751419067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6649350649350649, f1=0.6370757180156659, best_f1=0.6370757180156659\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3942844867706299\n",
            "step: 10, loss: 0.09220122545957565\n",
            "step: 20, loss: 0.11537709087133408\n",
            "step: 30, loss: 0.23168909549713135\n",
            "step: 40, loss: 0.1339629590511322\n",
            "step: 50, loss: 0.0691850557923317\n",
            "step: 60, loss: 0.20009203255176544\n",
            "step: 70, loss: 0.1356707513332367\n",
            "step: 80, loss: 0.33403605222702026\n",
            "step: 90, loss: 0.25405627489089966\n",
            "step: 100, loss: 0.048616208136081696\n",
            "step: 110, loss: 0.11080663651227951\n",
            "step: 120, loss: 0.25081250071525574\n",
            "step: 130, loss: 0.18963591754436493\n",
            "step: 140, loss: 0.12101548165082932\n",
            "step: 150, loss: 0.07012316584587097\n",
            "step: 160, loss: 0.023943809792399406\n",
            "step: 170, loss: 0.11104514449834824\n",
            "step: 180, loss: 0.13444848358631134\n",
            "step: 190, loss: 0.033372994512319565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7582417582417582, f1=0.7965616045845273, best_f1=0.7965616045845273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03958401829004288\n",
            "step: 10, loss: 0.12530618906021118\n",
            "step: 20, loss: 0.08066102117300034\n",
            "step: 30, loss: 0.19387242197990417\n",
            "step: 40, loss: 0.041468724608421326\n",
            "step: 50, loss: 0.2180178165435791\n",
            "step: 60, loss: 0.10596393048763275\n",
            "step: 70, loss: 0.056845828890800476\n",
            "step: 80, loss: 0.08724070340394974\n",
            "step: 90, loss: 0.044248588383197784\n",
            "step: 100, loss: 0.023867066949605942\n",
            "step: 110, loss: 0.009223487228155136\n",
            "step: 120, loss: 0.05339958518743515\n",
            "step: 130, loss: 0.01687692478299141\n",
            "step: 140, loss: 0.06442878395318985\n",
            "step: 150, loss: 0.10609906166791916\n",
            "step: 160, loss: 0.04372725263237953\n",
            "step: 170, loss: 0.096563421189785\n",
            "step: 180, loss: 0.05091210827231407\n",
            "step: 190, loss: 0.01951570250093937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7968337730870713, f1=0.8148148148148148, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0158989317715168\n",
            "step: 10, loss: 0.10047780722379684\n",
            "step: 20, loss: 0.03140459209680557\n",
            "step: 30, loss: 0.07541866600513458\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.007883951999247074\n",
            "step: 50, loss: 0.004869521129876375\n",
            "step: 60, loss: 0.20408876240253448\n",
            "step: 70, loss: 0.0075690122321248055\n",
            "step: 80, loss: 0.05566508322954178\n",
            "step: 90, loss: 0.07685093581676483\n",
            "step: 100, loss: 0.04675725847482681\n",
            "step: 110, loss: 0.01881209947168827\n",
            "step: 120, loss: 0.027932122349739075\n",
            "step: 130, loss: 0.16692408919334412\n",
            "step: 140, loss: 0.008922060020267963\n",
            "step: 150, loss: 0.007093910127878189\n",
            "step: 160, loss: 0.017482547089457512\n",
            "step: 170, loss: 0.05079749599099159\n",
            "step: 180, loss: 0.001948383403941989\n",
            "step: 190, loss: 0.060454487800598145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7750677506775067, f1=0.7875354107648725, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.053988706320524216\n",
            "step: 10, loss: 0.04014143720269203\n",
            "step: 20, loss: 0.06892568618059158\n",
            "step: 30, loss: 0.005461543332785368\n",
            "step: 40, loss: 0.042129941284656525\n",
            "step: 50, loss: 0.13478976488113403\n",
            "step: 60, loss: 0.10263290256261826\n",
            "step: 70, loss: 0.0878324881196022\n",
            "step: 80, loss: 0.01659640111029148\n",
            "step: 90, loss: 0.008592662401497364\n",
            "step: 100, loss: 0.0038243953604251146\n",
            "step: 110, loss: 0.014831678941845894\n",
            "step: 120, loss: 0.006494604051113129\n",
            "step: 130, loss: 0.027981525287032127\n",
            "step: 140, loss: 0.14018353819847107\n",
            "step: 150, loss: 0.004110617097467184\n",
            "step: 160, loss: 0.022355817258358\n",
            "step: 170, loss: 0.10853253304958344\n",
            "step: 180, loss: 0.07221774756908417\n",
            "step: 190, loss: 0.0060055917128920555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7845303867403315, f1=0.7855153203342619, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05365404486656189\n",
            "step: 10, loss: 0.03430720791220665\n",
            "step: 20, loss: 0.005762737710028887\n",
            "step: 30, loss: 0.0005801179795525968\n",
            "step: 40, loss: 0.0036779444199055433\n",
            "step: 50, loss: 0.22495193779468536\n",
            "step: 60, loss: 0.017018623650074005\n",
            "step: 70, loss: 0.0011269962415099144\n",
            "step: 80, loss: 0.02007051557302475\n",
            "step: 90, loss: 0.03878423199057579\n",
            "step: 100, loss: 0.00044884695671498775\n",
            "step: 110, loss: 0.08816685527563095\n",
            "step: 120, loss: 0.00119341304525733\n",
            "step: 130, loss: 0.02864951826632023\n",
            "step: 140, loss: 0.04133026301860809\n",
            "step: 150, loss: 0.0023901008535176516\n",
            "step: 160, loss: 0.15437102317810059\n",
            "step: 170, loss: 0.028603605926036835\n",
            "step: 180, loss: 0.00295532145537436\n",
            "step: 190, loss: 0.019596757367253304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7672634271099745, f1=0.7526315789473683, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006387455388903618\n",
            "step: 10, loss: 0.006756922695785761\n",
            "step: 20, loss: 0.04971915856003761\n",
            "step: 30, loss: 0.05230497941374779\n",
            "step: 40, loss: 0.011950608342885971\n",
            "step: 50, loss: 0.016089584678411484\n",
            "step: 60, loss: 0.07197339832782745\n",
            "step: 70, loss: 0.08623605221509933\n",
            "step: 80, loss: 0.004990084562450647\n",
            "step: 90, loss: 0.0028397943824529648\n",
            "step: 100, loss: 0.0011888784356415272\n",
            "step: 110, loss: 0.0029836532194167376\n",
            "step: 120, loss: 0.0005402666283771396\n",
            "step: 130, loss: 0.0009199423948302865\n",
            "step: 140, loss: 0.0019163029501214623\n",
            "step: 150, loss: 0.000985907856374979\n",
            "step: 160, loss: 0.03589736670255661\n",
            "step: 170, loss: 0.000990206142887473\n",
            "step: 180, loss: 0.001256966614164412\n",
            "step: 190, loss: 0.001710801851004362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7632311977715878, f1=0.7705382436260623, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021733902394771576\n",
            "step: 10, loss: 0.0027819545939564705\n",
            "step: 20, loss: 0.004179326351732016\n",
            "step: 30, loss: 0.0017926867585629225\n",
            "step: 40, loss: 0.0006979473982937634\n",
            "step: 50, loss: 0.006577224470674992\n",
            "step: 60, loss: 0.0006441896548494697\n",
            "step: 70, loss: 0.004280067980289459\n",
            "step: 80, loss: 0.0007045323145575821\n",
            "step: 90, loss: 0.0006648930138908327\n",
            "step: 100, loss: 0.0061516291461884975\n",
            "step: 110, loss: 0.0007281571161001921\n",
            "step: 120, loss: 0.007773841265588999\n",
            "step: 130, loss: 0.001286122016608715\n",
            "step: 140, loss: 0.002400102326646447\n",
            "step: 150, loss: 0.019477244466543198\n",
            "step: 160, loss: 0.0027789536397904158\n",
            "step: 170, loss: 0.01121154148131609\n",
            "step: 180, loss: 0.006369614973664284\n",
            "step: 190, loss: 0.005680527072399855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7667560321715818, f1=0.7479224376731302, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000353233510395512\n",
            "step: 10, loss: 0.03298313170671463\n",
            "step: 20, loss: 0.0011445499258115888\n",
            "step: 30, loss: 0.0006213284214027226\n",
            "step: 40, loss: 0.05120371654629707\n",
            "step: 50, loss: 0.004620281048119068\n",
            "step: 60, loss: 0.001408525393344462\n",
            "step: 70, loss: 0.0007710065692663193\n",
            "step: 80, loss: 0.0001935653854161501\n",
            "step: 90, loss: 0.0006373703945428133\n",
            "step: 100, loss: 0.0003621973446570337\n",
            "step: 110, loss: 0.000622714520432055\n",
            "step: 120, loss: 0.0006766529404558241\n",
            "step: 130, loss: 0.0028640765231102705\n",
            "step: 140, loss: 0.12536874413490295\n",
            "step: 150, loss: 0.005497712176293135\n",
            "step: 160, loss: 0.0014754727017134428\n",
            "step: 170, loss: 0.0027364911511540413\n",
            "step: 180, loss: 0.018428193405270576\n",
            "step: 190, loss: 0.0021221430506557226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7519582245430808, f1=0.7945945945945946, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001015259651467204\n",
            "step: 10, loss: 0.0007620420074090362\n",
            "step: 20, loss: 0.005863090045750141\n",
            "step: 30, loss: 0.0004702129226643592\n",
            "step: 40, loss: 0.04392186924815178\n",
            "step: 50, loss: 0.0003172970318701118\n",
            "step: 60, loss: 0.00028329427004791796\n",
            "step: 70, loss: 0.005371208768337965\n",
            "step: 80, loss: 0.0003314089262858033\n",
            "step: 90, loss: 0.0005288630491122603\n",
            "step: 100, loss: 0.0020049854647368193\n",
            "step: 110, loss: 0.0006662340019829571\n",
            "step: 120, loss: 0.1752806007862091\n",
            "step: 130, loss: 0.0020399473141878843\n",
            "step: 140, loss: 0.000620740232989192\n",
            "step: 150, loss: 0.001101350993849337\n",
            "step: 160, loss: 0.001706120208837092\n",
            "step: 170, loss: 0.00025171705055981874\n",
            "step: 180, loss: 0.00048215544666163623\n",
            "step: 190, loss: 0.001666342024691403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7806267806267806, f1=0.7803468208092486, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00044316271669231355\n",
            "step: 10, loss: 0.028754862025380135\n",
            "step: 20, loss: 0.019903061911463737\n",
            "step: 30, loss: 0.003176142228767276\n",
            "step: 40, loss: 0.0004929006099700928\n",
            "step: 50, loss: 0.0005193932447582483\n",
            "step: 60, loss: 0.00036734348395839334\n",
            "step: 70, loss: 0.0006251255981624126\n",
            "step: 80, loss: 0.00107967434450984\n",
            "step: 90, loss: 0.000252315221587196\n",
            "step: 100, loss: 0.0003000661381520331\n",
            "step: 110, loss: 0.0006659863283857703\n",
            "step: 120, loss: 0.000496305525302887\n",
            "step: 130, loss: 0.0004912129370495677\n",
            "step: 140, loss: 0.001094033126719296\n",
            "step: 150, loss: 0.0002177658025175333\n",
            "step: 160, loss: 0.0020066373981535435\n",
            "step: 170, loss: 0.00028534067678265274\n",
            "step: 180, loss: 0.0002634388511069119\n",
            "step: 190, loss: 0.0009897968266159296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7859078590785908, f1=0.7643678160919539, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017910503083840013\n",
            "step: 10, loss: 0.009063749574124813\n",
            "step: 20, loss: 0.0005756790633313358\n",
            "step: 30, loss: 0.0016376140993088484\n",
            "step: 40, loss: 0.00029026553966104984\n",
            "step: 50, loss: 0.0006374728982336819\n",
            "step: 60, loss: 0.000566851522307843\n",
            "step: 70, loss: 0.000402134086471051\n",
            "step: 80, loss: 0.00029407665715552866\n",
            "step: 90, loss: 0.00023148235050030053\n",
            "step: 100, loss: 0.0007419756148010492\n",
            "step: 110, loss: 0.0005004861741326749\n",
            "step: 120, loss: 0.0001506140106357634\n",
            "step: 130, loss: 0.0003998529282398522\n",
            "step: 140, loss: 0.0007876959862187505\n",
            "step: 150, loss: 0.00023448924184776843\n",
            "step: 160, loss: 0.00029677251586690545\n",
            "step: 170, loss: 0.0001924650714499876\n",
            "step: 180, loss: 0.00010121783998329192\n",
            "step: 190, loss: 0.00031601733644492924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7759562841530054, f1=0.7586206896551724, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003000050492119044\n",
            "step: 10, loss: 0.00014102608838584274\n",
            "step: 20, loss: 0.00013910788402426988\n",
            "step: 30, loss: 0.00025860031018964946\n",
            "step: 40, loss: 0.00026746143703348935\n",
            "step: 50, loss: 0.0005478011444211006\n",
            "step: 60, loss: 0.00014699371240567416\n",
            "step: 70, loss: 0.000961954821832478\n",
            "step: 80, loss: 0.016027722507715225\n",
            "step: 90, loss: 0.0006023842142894864\n",
            "step: 100, loss: 0.0012814399087801576\n",
            "step: 110, loss: 0.00046131465933285654\n",
            "step: 120, loss: 0.0005309391999617219\n",
            "step: 130, loss: 0.00037421571323648095\n",
            "step: 140, loss: 0.00015811460616532713\n",
            "step: 150, loss: 0.00023640648578293622\n",
            "step: 160, loss: 0.0002809022553265095\n",
            "step: 170, loss: 0.00044246946345083416\n",
            "step: 180, loss: 0.0001436142629245296\n",
            "step: 190, loss: 0.13257940113544464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.770949720670391, f1=0.7687861271676301, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006447473424486816\n",
            "step: 10, loss: 0.00016885617515072227\n",
            "step: 20, loss: 0.00029007482226006687\n",
            "step: 30, loss: 0.0001027919861371629\n",
            "step: 40, loss: 0.0002821201633196324\n",
            "step: 50, loss: 0.0001832927664509043\n",
            "step: 60, loss: 0.0003382973081897944\n",
            "step: 70, loss: 0.0015541142784059048\n",
            "step: 80, loss: 0.0002497140958439559\n",
            "step: 90, loss: 0.001108113443478942\n",
            "step: 100, loss: 0.0007502463995479047\n",
            "step: 110, loss: 0.002714283764362335\n",
            "step: 120, loss: 0.00012559119204524904\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.004968064371496439\n",
            "step: 140, loss: 0.00018576328875496984\n",
            "step: 150, loss: 0.00015399299445562065\n",
            "step: 160, loss: 0.0012560357572510839\n",
            "step: 170, loss: 0.00019712763605639338\n",
            "step: 180, loss: 0.017986003309488297\n",
            "step: 190, loss: 0.00034907663939520717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.776536312849162, f1=0.7630057803468209, best_f1=0.8148148148148148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007644536439329386\n",
            "step: 10, loss: 0.00011254453420406207\n",
            "step: 20, loss: 0.00017274392303079367\n",
            "step: 30, loss: 0.00016658968525007367\n",
            "step: 40, loss: 0.00039680401096120477\n",
            "step: 50, loss: 0.00012178827455500141\n",
            "step: 60, loss: 0.00012627505930140615\n",
            "step: 70, loss: 0.002890824107453227\n",
            "step: 80, loss: 0.0019557769410312176\n",
            "step: 90, loss: 0.00016221287660300732\n",
            "step: 100, loss: 0.0013081008801236749\n",
            "step: 110, loss: 0.0003273543552495539\n",
            "step: 120, loss: 0.0007456004968844354\n",
            "step: 130, loss: 0.00010131675662705675\n",
            "step: 140, loss: 0.00024198494793381542\n",
            "step: 150, loss: 0.00033015289227478206\n",
            "step: 160, loss: 0.00014005844423081726\n",
            "step: 170, loss: 0.00016398595471400768\n",
            "step: 180, loss: 0.003762507811188698\n",
            "step: 190, loss: 0.00012795772636309266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7688022284122563, f1=0.7643678160919539, best_f1=0.8148148148148148\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:10, 199.86it/s]\n",
            "load_f1 = 0.6426735218508998\n",
            "real_f1 = 0.6361323155216284\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 221.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a26a4e-5350-44cb-e60b-b0887dadd1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6290316581726074\n",
            "step: 10, loss: 0.35872894525527954\n",
            "step: 20, loss: 0.3191245496273041\n",
            "step: 30, loss: 0.37396809458732605\n",
            "step: 40, loss: 0.2700957655906677\n",
            "step: 50, loss: 0.2637379765510559\n",
            "step: 60, loss: 0.2441197782754898\n",
            "step: 70, loss: 0.38381126523017883\n",
            "step: 80, loss: 0.3694934546947479\n",
            "step: 90, loss: 0.24785521626472473\n",
            "step: 100, loss: 0.16236698627471924\n",
            "step: 110, loss: 0.24819211661815643\n",
            "step: 120, loss: 0.21017292141914368\n",
            "step: 130, loss: 0.04938714578747749\n",
            "step: 140, loss: 0.2329823076725006\n",
            "step: 150, loss: 0.2328414022922516\n",
            "step: 160, loss: 0.12636646628379822\n",
            "step: 170, loss: 0.2855156362056732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7444168734491315, f1=0.720763723150358, best_f1=0.720763723150358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16078715026378632\n",
            "step: 10, loss: 0.11963994801044464\n",
            "step: 20, loss: 0.039548128843307495\n",
            "step: 30, loss: 0.2817404866218567\n",
            "step: 40, loss: 0.07223214954137802\n",
            "step: 50, loss: 0.20839014649391174\n",
            "step: 60, loss: 0.08881810307502747\n",
            "step: 70, loss: 0.14993643760681152\n",
            "step: 80, loss: 0.07745501399040222\n",
            "step: 90, loss: 0.16619932651519775\n",
            "step: 100, loss: 0.2040548473596573\n",
            "step: 110, loss: 0.08990336954593658\n",
            "step: 120, loss: 0.26486533880233765\n",
            "step: 130, loss: 0.08762931078672409\n",
            "step: 140, loss: 0.22698119282722473\n",
            "step: 150, loss: 0.1337176263332367\n",
            "step: 160, loss: 0.12335643917322159\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.02154509723186493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.7423167848699763, f1=0.7713625866050808, best_f1=0.720763723150358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17014412581920624\n",
            "step: 10, loss: 0.11567521095275879\n",
            "step: 20, loss: 0.10815800726413727\n",
            "step: 30, loss: 0.15103568136692047\n",
            "step: 40, loss: 0.10569784045219421\n",
            "step: 50, loss: 0.06695225089788437\n",
            "step: 60, loss: 0.11099988222122192\n",
            "step: 70, loss: 0.045565392822027206\n",
            "step: 80, loss: 0.05589798837900162\n",
            "step: 90, loss: 0.06375506520271301\n",
            "step: 100, loss: 0.01848059520125389\n",
            "step: 110, loss: 0.06675703823566437\n",
            "step: 120, loss: 0.05047593638300896\n",
            "step: 130, loss: 0.07389073818922043\n",
            "step: 140, loss: 0.01762515679001808\n",
            "step: 150, loss: 0.19886118173599243\n",
            "step: 160, loss: 0.09674390405416489\n",
            "step: 170, loss: 0.23010855913162231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7981220657276995, f1=0.7671232876712328, best_f1=0.7671232876712328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015740592032670975\n",
            "step: 10, loss: 0.04109233245253563\n",
            "step: 20, loss: 0.005345993675291538\n",
            "step: 30, loss: 0.010445669293403625\n",
            "step: 40, loss: 0.004075797740370035\n",
            "step: 50, loss: 0.07006660103797913\n",
            "step: 60, loss: 0.19749493896961212\n",
            "step: 70, loss: 0.0077652777545154095\n",
            "step: 80, loss: 0.07829029113054276\n",
            "step: 90, loss: 0.03196728974580765\n",
            "step: 100, loss: 0.16735482215881348\n",
            "step: 110, loss: 0.01193452812731266\n",
            "step: 120, loss: 0.00307336263358593\n",
            "step: 130, loss: 0.029677487909793854\n",
            "step: 140, loss: 0.027763033285737038\n",
            "step: 150, loss: 0.028026917949318886\n",
            "step: 160, loss: 0.0733768418431282\n",
            "step: 170, loss: 0.18733245134353638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.788659793814433, f1=0.8029556650246306, best_f1=0.7671232876712328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04392192140221596\n",
            "step: 10, loss: 0.011938345618546009\n",
            "step: 20, loss: 0.03169980272650719\n",
            "step: 30, loss: 0.030956203117966652\n",
            "step: 40, loss: 0.015218770131468773\n",
            "step: 50, loss: 0.003689633449539542\n",
            "step: 60, loss: 0.013410753570497036\n",
            "step: 70, loss: 0.060668133199214935\n",
            "step: 80, loss: 0.1206246018409729\n",
            "step: 90, loss: 0.10077172517776489\n",
            "step: 100, loss: 0.08442405611276627\n",
            "step: 110, loss: 0.014079036191105843\n",
            "step: 120, loss: 0.013885173946619034\n",
            "step: 130, loss: 0.03320605307817459\n",
            "step: 140, loss: 0.01048872247338295\n",
            "step: 150, loss: 0.04576435685157776\n",
            "step: 160, loss: 0.007203089538961649\n",
            "step: 170, loss: 0.005116036161780357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8031088082901555, f1=0.8170426065162907, best_f1=0.8170426065162907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004885065369307995\n",
            "step: 10, loss: 0.011713553220033646\n",
            "step: 20, loss: 0.0051687960512936115\n",
            "step: 30, loss: 0.00220652692951262\n",
            "step: 40, loss: 0.014271738938987255\n",
            "step: 50, loss: 0.15398149192333221\n",
            "step: 60, loss: 0.011783569119870663\n",
            "step: 70, loss: 0.11197741329669952\n",
            "step: 80, loss: 0.04487500712275505\n",
            "step: 90, loss: 0.016486290842294693\n",
            "step: 100, loss: 0.043627649545669556\n",
            "step: 110, loss: 0.009858937934041023\n",
            "step: 120, loss: 0.010284967720508575\n",
            "step: 130, loss: 0.020469853654503822\n",
            "step: 140, loss: 0.016463221982121468\n",
            "step: 150, loss: 0.008153987117111683\n",
            "step: 160, loss: 0.11232244223356247\n",
            "step: 170, loss: 0.002813460072502494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8151898734177214, f1=0.8166259168704155, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010580194648355246\n",
            "step: 10, loss: 0.010819240473210812\n",
            "step: 20, loss: 0.0017087309388443828\n",
            "step: 30, loss: 0.0009312652400694788\n",
            "step: 40, loss: 0.000754331995267421\n",
            "step: 50, loss: 0.09632162749767303\n",
            "step: 60, loss: 0.0043244678527116776\n",
            "step: 70, loss: 0.0013389167143031955\n",
            "step: 80, loss: 0.03206730633974075\n",
            "step: 90, loss: 0.0007998075452633202\n",
            "step: 100, loss: 0.0011031051399186254\n",
            "step: 110, loss: 0.0031388767529278994\n",
            "step: 120, loss: 0.006148249376565218\n",
            "step: 130, loss: 0.1634775698184967\n",
            "step: 140, loss: 0.004378863610327244\n",
            "step: 150, loss: 0.003422877285629511\n",
            "step: 160, loss: 0.0005067166057415307\n",
            "step: 170, loss: 0.020427575334906578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7901907356948228, f1=0.828125, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006983309518545866\n",
            "step: 10, loss: 0.000941068516112864\n",
            "step: 20, loss: 0.0008060664404183626\n",
            "step: 30, loss: 0.013192631304264069\n",
            "step: 40, loss: 0.00019544814131222665\n",
            "step: 50, loss: 0.0018894376698881388\n",
            "step: 60, loss: 0.0014274587156251073\n",
            "step: 70, loss: 0.0007825963548384607\n",
            "step: 80, loss: 0.0007534909527748823\n",
            "step: 90, loss: 0.0015578388702124357\n",
            "step: 100, loss: 0.01920844428241253\n",
            "step: 110, loss: 0.15396399796009064\n",
            "step: 120, loss: 0.00351868849247694\n",
            "step: 130, loss: 0.03536561504006386\n",
            "step: 140, loss: 0.021660195663571358\n",
            "step: 150, loss: 0.13983570039272308\n",
            "step: 160, loss: 0.03841312602162361\n",
            "step: 170, loss: 0.021094782277941704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.798913043478261, f1=0.8030690537084398, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003485708264634013\n",
            "step: 10, loss: 0.16286718845367432\n",
            "step: 20, loss: 0.004540441557765007\n",
            "step: 30, loss: 0.015208626165986061\n",
            "step: 40, loss: 0.0008102252031676471\n",
            "step: 50, loss: 0.00040106114465743303\n",
            "step: 60, loss: 0.048448000103235245\n",
            "step: 70, loss: 0.0007027421379461884\n",
            "step: 80, loss: 0.0010691304923966527\n",
            "step: 90, loss: 0.04240624979138374\n",
            "step: 100, loss: 0.06957107037305832\n",
            "step: 110, loss: 0.02401158958673477\n",
            "step: 120, loss: 0.04810840263962746\n",
            "step: 130, loss: 0.06088840961456299\n",
            "step: 140, loss: 0.0007876462768763304\n",
            "step: 150, loss: 0.0006628797273151577\n",
            "step: 160, loss: 0.013790827244520187\n",
            "step: 170, loss: 0.031497035175561905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8138297872340425, f1=0.8232323232323232, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029871415346860886\n",
            "step: 10, loss: 0.00030450260965153575\n",
            "step: 20, loss: 0.27071648836135864\n",
            "step: 30, loss: 0.001881062868051231\n",
            "step: 40, loss: 0.024654125794768333\n",
            "step: 50, loss: 0.017957258969545364\n",
            "step: 60, loss: 0.00028285308508202434\n",
            "step: 70, loss: 0.011621796526014805\n",
            "step: 80, loss: 0.11074413359165192\n",
            "step: 90, loss: 0.023596780374646187\n",
            "step: 100, loss: 0.0004310820368118584\n",
            "step: 110, loss: 0.0023438818752765656\n",
            "step: 120, loss: 0.0013875637669116259\n",
            "step: 130, loss: 0.00042738436604849994\n",
            "step: 140, loss: 0.0014449593145400286\n",
            "step: 150, loss: 0.0014493942726403475\n",
            "step: 160, loss: 0.0010022212518379092\n",
            "step: 170, loss: 0.0001697512052487582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7945945945945946, f1=0.8010335917312662, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014844266697764397\n",
            "step: 10, loss: 0.04658293351531029\n",
            "step: 20, loss: 0.00046615811879746616\n",
            "step: 30, loss: 0.0003225473337806761\n",
            "step: 40, loss: 0.001066646189428866\n",
            "step: 50, loss: 0.007033152040094137\n",
            "step: 60, loss: 0.011048215441405773\n",
            "step: 70, loss: 0.003610219107940793\n",
            "step: 80, loss: 0.0003096492728218436\n",
            "step: 90, loss: 0.0002967960317619145\n",
            "step: 100, loss: 0.0003588027029763907\n",
            "step: 110, loss: 0.004611192736774683\n",
            "step: 120, loss: 0.00034795087412931025\n",
            "step: 130, loss: 0.00011673946573864669\n",
            "step: 140, loss: 0.003962934948503971\n",
            "step: 150, loss: 0.0024074246175587177\n",
            "step: 160, loss: 0.0029626768082380295\n",
            "step: 170, loss: 0.0002996786788571626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8093994778067886, f1=0.8040201005025125, best_f1=0.8166259168704155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000250005949055776\n",
            "step: 10, loss: 0.002122958656400442\n",
            "step: 20, loss: 0.00027642102213576436\n",
            "step: 30, loss: 0.00042948860209435225\n",
            "step: 40, loss: 0.0018349691526964307\n",
            "step: 50, loss: 0.0001395074650645256\n",
            "step: 60, loss: 0.016254307702183723\n",
            "step: 70, loss: 0.06766146421432495\n",
            "step: 80, loss: 0.00023684214102104306\n",
            "step: 90, loss: 0.0004117623611818999\n",
            "step: 100, loss: 0.0036856785882264376\n",
            "step: 110, loss: 0.0011137353722006083\n",
            "step: 120, loss: 0.001820934470742941\n",
            "step: 130, loss: 0.002327906433492899\n",
            "step: 140, loss: 0.011413048021495342\n",
            "step: 150, loss: 0.0009668369893915951\n",
            "step: 160, loss: 0.00021404106519185007\n",
            "step: 170, loss: 0.00015231767611112446\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8170426065162907, f1=0.8038740920096852, best_f1=0.8038740920096852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016406690701842308\n",
            "step: 10, loss: 0.0003998297033831477\n",
            "step: 20, loss: 0.0018238609191030264\n",
            "step: 30, loss: 0.00013731460785493255\n",
            "step: 40, loss: 0.00723812822252512\n",
            "step: 50, loss: 0.05820313096046448\n",
            "step: 60, loss: 0.003165645757690072\n",
            "step: 70, loss: 0.001497324788942933\n",
            "step: 80, loss: 0.0017128533218055964\n",
            "step: 90, loss: 9.448845230508596e-05\n",
            "step: 100, loss: 0.0003898335271514952\n",
            "step: 110, loss: 0.00018755342171061784\n",
            "step: 120, loss: 0.019377637654542923\n",
            "step: 130, loss: 0.00023273465922102332\n",
            "step: 140, loss: 0.00031505830702371895\n",
            "step: 150, loss: 0.03967755660414696\n",
            "step: 160, loss: 0.0004178386298008263\n",
            "step: 170, loss: 0.0002608496288303286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8091603053435115, f1=0.7970660146699265, best_f1=0.8038740920096852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007485164096578956\n",
            "step: 10, loss: 0.00017710549582261592\n",
            "step: 20, loss: 0.0038143559359014034\n",
            "step: 30, loss: 0.0005349125131033361\n",
            "step: 40, loss: 0.00031345285242423415\n",
            "step: 50, loss: 0.0005075978697277606\n",
            "step: 60, loss: 0.03936197981238365\n",
            "step: 70, loss: 0.0025587778072804213\n",
            "step: 80, loss: 0.001902678282931447\n",
            "step: 90, loss: 0.00023148591571953148\n",
            "step: 100, loss: 0.0001110542580136098\n",
            "step: 110, loss: 0.0002953777147922665\n",
            "step: 120, loss: 0.010796796530485153\n",
            "step: 130, loss: 0.0011267635272815824\n",
            "step: 140, loss: 0.00035521091194823384\n",
            "step: 150, loss: 0.0008016637875698507\n",
            "step: 160, loss: 0.00020691774261649698\n",
            "step: 170, loss: 0.0003336233494337648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8257372654155496, f1=0.8041237113402061, best_f1=0.8041237113402061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000151073036249727\n",
            "step: 10, loss: 0.0004355897253844887\n",
            "step: 20, loss: 0.0012231265427544713\n",
            "step: 30, loss: 0.0003664122777990997\n",
            "step: 40, loss: 0.002154693240299821\n",
            "step: 50, loss: 0.0001431686250725761\n",
            "step: 60, loss: 0.05840041860938072\n",
            "step: 70, loss: 0.00019758324197027832\n",
            "step: 80, loss: 0.005681624170392752\n",
            "step: 90, loss: 0.001925625023432076\n",
            "step: 100, loss: 0.0002999936987180263\n",
            "step: 110, loss: 0.00014378091145772487\n",
            "step: 120, loss: 0.00044696906115859747\n",
            "step: 130, loss: 0.0005188704235479236\n",
            "step: 140, loss: 0.00016306154429912567\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.0003624334931373596\n",
            "step: 160, loss: 0.00040353150689043105\n",
            "step: 170, loss: 0.0008352779550477862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8181818181818182, f1=0.8020565552699228, best_f1=0.8041237113402061\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 253.53it/s]\n",
            "load_f1 = 0.6141732283464566\n",
            "real_f1 = 0.6058252427184466\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 223.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6998ea03-2d90-48c9-a4d5-682bf0946b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.616497278213501\n",
            "step: 10, loss: 0.6252344250679016\n",
            "step: 20, loss: 0.3142161965370178\n",
            "step: 30, loss: 0.0699378252029419\n",
            "step: 40, loss: 0.2210879772901535\n",
            "step: 50, loss: 0.10188774019479752\n",
            "step: 60, loss: 0.08319952338933945\n",
            "step: 70, loss: 0.1264403909444809\n",
            "step: 80, loss: 0.06855887919664383\n",
            "step: 90, loss: 0.07461768388748169\n",
            "step: 100, loss: 0.04406120628118515\n",
            "step: 110, loss: 0.2405397891998291\n",
            "step: 120, loss: 0.010909640230238438\n",
            "step: 130, loss: 0.014701712876558304\n",
            "step: 140, loss: 0.0070767709985375404\n",
            "step: 150, loss: 0.024735987186431885\n",
            "step: 160, loss: 0.0047877393662929535\n",
            "step: 170, loss: 0.12957409024238586\n",
            "step: 180, loss: 0.03602477163076401\n",
            "step: 190, loss: 0.02586657740175724\n",
            "step: 200, loss: 0.05013897642493248\n",
            "step: 210, loss: 0.0024750027805566788\n",
            "step: 220, loss: 0.015070619061589241\n",
            "step: 230, loss: 0.02278526872396469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9726651480637813, f1=0.9681818181818181, best_f1=0.9681818181818181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004510430619120598\n",
            "step: 10, loss: 0.015317348763346672\n",
            "step: 20, loss: 0.16757462918758392\n",
            "step: 30, loss: 0.19390513002872467\n",
            "step: 40, loss: 0.05791383236646652\n",
            "step: 50, loss: 0.0037836392875760794\n",
            "step: 60, loss: 0.008605588227510452\n",
            "step: 70, loss: 0.015738531947135925\n",
            "step: 80, loss: 0.06567082554101944\n",
            "step: 90, loss: 0.006308234762400389\n",
            "step: 100, loss: 0.06326666474342346\n",
            "step: 110, loss: 0.06825077533721924\n",
            "step: 120, loss: 0.1332036852836609\n",
            "step: 130, loss: 0.009749959222972393\n",
            "step: 140, loss: 0.0020955675281584263\n",
            "step: 150, loss: 0.07445012032985687\n",
            "step: 160, loss: 0.017032669857144356\n",
            "step: 170, loss: 0.0012536931317299604\n",
            "step: 180, loss: 0.10903969407081604\n",
            "step: 190, loss: 0.0027832190971821547\n",
            "step: 200, loss: 0.0021617766469717026\n",
            "step: 210, loss: 0.0009970251703634858\n",
            "step: 220, loss: 0.12366972863674164\n",
            "step: 230, loss: 0.0037805535830557346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9686800894854586, f1=0.9719416386083053, best_f1=0.9681818181818181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04039733111858368\n",
            "step: 10, loss: 0.0033835608046501875\n",
            "step: 20, loss: 0.09352539479732513\n",
            "step: 30, loss: 0.02819577045738697\n",
            "step: 40, loss: 0.006846242118626833\n",
            "step: 50, loss: 0.010010654106736183\n",
            "step: 60, loss: 0.006530353799462318\n",
            "step: 70, loss: 0.004337223246693611\n",
            "step: 80, loss: 0.007075682282447815\n",
            "step: 90, loss: 0.0011290351394563913\n",
            "step: 100, loss: 0.0032271738164126873\n",
            "step: 110, loss: 0.0004383292398415506\n",
            "step: 120, loss: 0.021847128868103027\n",
            "step: 130, loss: 0.0004312340170145035\n",
            "step: 140, loss: 0.0021657939068973064\n",
            "step: 150, loss: 0.03187781199812889\n",
            "step: 160, loss: 0.021632585674524307\n",
            "step: 170, loss: 0.0171661339700222\n",
            "step: 180, loss: 0.02531660534441471\n",
            "step: 190, loss: 0.027227791026234627\n",
            "step: 200, loss: 0.08173702657222748\n",
            "step: 210, loss: 0.07389384508132935\n",
            "step: 220, loss: 0.0016526124672964215\n",
            "step: 230, loss: 0.001321239280514419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9772209567198178, f1=0.9738933030646991, best_f1=0.9738933030646991\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001319101866101846\n",
            "step: 10, loss: 0.0005690369871445\n",
            "step: 20, loss: 0.0007884560618549585\n",
            "step: 30, loss: 0.003125527873635292\n",
            "step: 40, loss: 0.01557102520018816\n",
            "step: 50, loss: 0.000994014204479754\n",
            "step: 60, loss: 0.0015302306273952127\n",
            "step: 70, loss: 0.0009970634710043669\n",
            "step: 80, loss: 0.00043958405149169266\n",
            "step: 90, loss: 0.0007720833527855575\n",
            "step: 100, loss: 0.0004906198009848595\n",
            "step: 110, loss: 0.00044458656338974833\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.02645803801715374\n",
            "step: 130, loss: 0.005057545378804207\n",
            "step: 140, loss: 0.002118527889251709\n",
            "step: 150, loss: 0.11834167689085007\n",
            "step: 160, loss: 0.1883639395236969\n",
            "step: 170, loss: 0.05109625309705734\n",
            "step: 180, loss: 0.0027818719390779734\n",
            "step: 190, loss: 0.0031975130550563335\n",
            "step: 200, loss: 0.0018451792420819402\n",
            "step: 210, loss: 0.0004304909089114517\n",
            "step: 220, loss: 0.0005007369327358902\n",
            "step: 230, loss: 0.0009171079727821052\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9774774774774775, f1=0.971815107102593, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003191263822373003\n",
            "step: 10, loss: 0.0003013799723703414\n",
            "step: 20, loss: 0.0003492274845484644\n",
            "step: 30, loss: 0.0002114232920575887\n",
            "step: 40, loss: 0.0010586322750896215\n",
            "step: 50, loss: 0.0003836308023892343\n",
            "step: 60, loss: 0.19351918995380402\n",
            "step: 70, loss: 0.0002933436771854758\n",
            "step: 80, loss: 0.0010437598684802651\n",
            "step: 90, loss: 0.010897588916122913\n",
            "step: 100, loss: 0.00037131065619178116\n",
            "step: 110, loss: 0.0007237619138322771\n",
            "step: 120, loss: 0.0002252409904031083\n",
            "step: 130, loss: 0.0017177363624796271\n",
            "step: 140, loss: 0.0006135287694633007\n",
            "step: 150, loss: 0.0009399050031788647\n",
            "step: 160, loss: 0.0006400919519364834\n",
            "step: 170, loss: 0.002306975657120347\n",
            "step: 180, loss: 0.00029733101837337017\n",
            "step: 190, loss: 0.16226112842559814\n",
            "step: 200, loss: 0.007368250284343958\n",
            "step: 210, loss: 0.004478382878005505\n",
            "step: 220, loss: 0.0010525319958105683\n",
            "step: 230, loss: 0.0011551734060049057\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9821428571428571, f1=0.9732142857142857, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008933818899095058\n",
            "step: 10, loss: 0.0006864150636829436\n",
            "step: 20, loss: 0.0008655193378217518\n",
            "step: 30, loss: 0.0003245485422667116\n",
            "step: 40, loss: 0.0007741273147985339\n",
            "step: 50, loss: 0.012825103476643562\n",
            "step: 60, loss: 0.00046431581722572446\n",
            "step: 70, loss: 0.0003335803048685193\n",
            "step: 80, loss: 0.0012743128463625908\n",
            "step: 90, loss: 0.00021530882804654539\n",
            "step: 100, loss: 0.005798024125397205\n",
            "step: 110, loss: 0.0006787974270991981\n",
            "step: 120, loss: 0.0023038536310195923\n",
            "step: 130, loss: 0.00017720097093842924\n",
            "step: 140, loss: 0.0001451152202207595\n",
            "step: 150, loss: 0.04513045772910118\n",
            "step: 160, loss: 0.003145152935758233\n",
            "step: 170, loss: 0.0005269521498121321\n",
            "step: 180, loss: 0.005129431374371052\n",
            "step: 190, loss: 0.0038283199537545443\n",
            "step: 200, loss: 0.00026480198721401393\n",
            "step: 210, loss: 0.001399607746861875\n",
            "step: 220, loss: 0.00018274415924679488\n",
            "step: 230, loss: 0.06829587370157242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9808773903262092, f1=0.9740112994350283, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011722258932422847\n",
            "step: 10, loss: 0.00016859637980815023\n",
            "step: 20, loss: 0.00144930777605623\n",
            "step: 30, loss: 0.003706286195665598\n",
            "step: 40, loss: 0.00032501743407920003\n",
            "step: 50, loss: 0.00016687267634551972\n",
            "step: 60, loss: 0.00026241259183734655\n",
            "step: 70, loss: 0.007006252650171518\n",
            "step: 80, loss: 0.0025061971973627806\n",
            "step: 90, loss: 0.000301397405564785\n",
            "step: 100, loss: 0.0001957505155587569\n",
            "step: 110, loss: 0.00021266243129502982\n",
            "step: 120, loss: 7.685598393436521e-05\n",
            "step: 130, loss: 0.00015674417954869568\n",
            "step: 140, loss: 0.00019805351621471345\n",
            "step: 150, loss: 0.010134466923773289\n",
            "step: 160, loss: 0.04280505329370499\n",
            "step: 170, loss: 0.15631964802742004\n",
            "step: 180, loss: 0.0010137784993276\n",
            "step: 190, loss: 9.662690717959777e-05\n",
            "step: 200, loss: 0.021647633984684944\n",
            "step: 210, loss: 6.852154183434322e-05\n",
            "step: 220, loss: 0.00018366871518082917\n",
            "step: 230, loss: 0.00021493839449249208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9809203142536477, f1=0.9763779527559054, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022465811343863606\n",
            "step: 10, loss: 0.00044056487968191504\n",
            "step: 20, loss: 0.0001479827769799158\n",
            "step: 30, loss: 7.589644519612193e-05\n",
            "step: 40, loss: 0.0008737425087019801\n",
            "step: 50, loss: 0.0018814406357705593\n",
            "step: 60, loss: 0.0002369158755755052\n",
            "step: 70, loss: 0.0006907846545800567\n",
            "step: 80, loss: 0.000441259442595765\n",
            "step: 90, loss: 6.526923971250653e-05\n",
            "step: 100, loss: 0.00015162251656875014\n",
            "step: 110, loss: 0.00011107041063951328\n",
            "step: 120, loss: 0.031593650579452515\n",
            "step: 130, loss: 4.074484604643658e-05\n",
            "step: 140, loss: 9.39011515583843e-05\n",
            "step: 150, loss: 8.853743929648772e-05\n",
            "step: 160, loss: 0.00014338098117150366\n",
            "step: 170, loss: 0.0005783923552371562\n",
            "step: 180, loss: 0.041662003844976425\n",
            "step: 190, loss: 0.007700107526034117\n",
            "step: 200, loss: 0.0095151886343956\n",
            "step: 210, loss: 0.0003762002452276647\n",
            "step: 220, loss: 0.004284838680177927\n",
            "step: 230, loss: 0.0002794510801322758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9776785714285714, f1=0.9775784753363228, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.159207259770483e-05\n",
            "step: 10, loss: 0.0006697919452562928\n",
            "step: 20, loss: 0.0005241652252152562\n",
            "step: 30, loss: 0.00017725626821629703\n",
            "step: 40, loss: 0.009582070633769035\n",
            "step: 50, loss: 0.00011974424705840647\n",
            "step: 60, loss: 0.0014365913812071085\n",
            "step: 70, loss: 0.00047808015369810164\n",
            "step: 80, loss: 0.0003294751513749361\n",
            "step: 90, loss: 0.0006244330434128642\n",
            "step: 100, loss: 0.0006230267463251948\n",
            "step: 110, loss: 4.6436325646936893e-05\n",
            "step: 120, loss: 7.47495869291015e-05\n",
            "step: 130, loss: 3.509429734549485e-05\n",
            "step: 140, loss: 3.6182162148179486e-05\n",
            "step: 150, loss: 0.00017470523016527295\n",
            "step: 160, loss: 4.364299093140289e-05\n",
            "step: 170, loss: 7.075133180478588e-05\n",
            "step: 180, loss: 0.00010159720113733783\n",
            "step: 190, loss: 8.0605830589775e-05\n",
            "step: 200, loss: 0.00021631340496242046\n",
            "step: 210, loss: 0.00013114366447553039\n",
            "step: 220, loss: 3.72736576537136e-05\n",
            "step: 230, loss: 0.00010830086830537766\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9776286353467561, f1=0.9741863075196409, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.1901308324886486e-05\n",
            "step: 10, loss: 0.00022487908427137882\n",
            "step: 20, loss: 0.011383316479623318\n",
            "step: 30, loss: 7.482731598429382e-05\n",
            "step: 40, loss: 9.109019447350875e-05\n",
            "step: 50, loss: 3.453568569966592e-05\n",
            "step: 60, loss: 4.659902333514765e-05\n",
            "step: 70, loss: 0.00010037265019491315\n",
            "step: 80, loss: 5.4508411267306656e-05\n",
            "step: 90, loss: 0.0001224948646267876\n",
            "step: 100, loss: 5.3863910579821095e-05\n",
            "step: 110, loss: 0.0004390190588310361\n",
            "step: 120, loss: 0.00012321046961005777\n",
            "step: 130, loss: 5.279778633848764e-05\n",
            "step: 140, loss: 0.02699338272213936\n",
            "step: 150, loss: 0.021360836923122406\n",
            "step: 160, loss: 5.322169454302639e-05\n",
            "step: 170, loss: 4.187297963653691e-05\n",
            "step: 180, loss: 0.003444286063313484\n",
            "step: 190, loss: 0.0005306907114572823\n",
            "step: 200, loss: 0.00032889985595829785\n",
            "step: 210, loss: 0.005425171926617622\n",
            "step: 220, loss: 5.099690315546468e-05\n",
            "step: 230, loss: 0.0007309096399694681\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9765886287625419, f1=0.9707865168539327, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.335286889225245e-05\n",
            "step: 10, loss: 0.00013545998081099242\n",
            "step: 20, loss: 0.00023543505812995136\n",
            "step: 30, loss: 0.00022386621276382357\n",
            "step: 40, loss: 0.0001377669395878911\n",
            "step: 50, loss: 0.0016681089764460921\n",
            "step: 60, loss: 0.00037803003215231\n",
            "step: 70, loss: 0.0013717695837840438\n",
            "step: 80, loss: 3.281136741861701e-05\n",
            "step: 90, loss: 6.51927330181934e-05\n",
            "step: 100, loss: 0.00011355282185832039\n",
            "step: 110, loss: 0.0004919549101032317\n",
            "step: 120, loss: 0.0007864421349950135\n",
            "step: 130, loss: 3.5976416256744415e-05\n",
            "step: 140, loss: 5.321337084751576e-05\n",
            "step: 150, loss: 0.0005544815794564784\n",
            "step: 160, loss: 5.3995081543689594e-05\n",
            "step: 170, loss: 0.0032095026690512896\n",
            "step: 180, loss: 7.722511509200558e-05\n",
            "step: 190, loss: 3.3761007216526195e-05\n",
            "step: 200, loss: 0.00027683580992743373\n",
            "step: 210, loss: 2.7894271624973044e-05\n",
            "step: 220, loss: 7.612154877278954e-05\n",
            "step: 230, loss: 3.644328535301611e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9777777777777777, f1=0.9753914988814317, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.233649037312716e-05\n",
            "step: 10, loss: 4.0860631997929886e-05\n",
            "step: 20, loss: 0.00023754237918183208\n",
            "step: 30, loss: 0.00017793518782127649\n",
            "step: 40, loss: 0.02472844533622265\n",
            "step: 50, loss: 0.05937892943620682\n",
            "step: 60, loss: 7.866548548918217e-05\n",
            "step: 70, loss: 0.0006391355418600142\n",
            "step: 80, loss: 0.009506519883871078\n",
            "step: 90, loss: 8.842721581459045e-05\n",
            "step: 100, loss: 5.5447475460823625e-05\n",
            "step: 110, loss: 3.605925303418189e-05\n",
            "step: 120, loss: 5.610809603240341e-05\n",
            "step: 130, loss: 5.852214599144645e-05\n",
            "step: 140, loss: 5.1170234655728564e-05\n",
            "step: 150, loss: 0.00010445321095176041\n",
            "step: 160, loss: 5.483210406964645e-05\n",
            "step: 170, loss: 8.383260137634352e-05\n",
            "step: 180, loss: 0.00012454710667952895\n",
            "step: 190, loss: 4.940825601806864e-05\n",
            "step: 200, loss: 0.0005112512153573334\n",
            "step: 210, loss: 5.542254803003743e-05\n",
            "step: 220, loss: 4.160836397204548e-05\n",
            "step: 230, loss: 0.05190206319093704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9810055865921787, f1=0.9741282339707535, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.917101563885808e-05\n",
            "step: 10, loss: 6.929069058969617e-05\n",
            "step: 20, loss: 0.0001565226848470047\n",
            "step: 30, loss: 0.00019206598517484963\n",
            "step: 40, loss: 3.536274380167015e-05\n",
            "step: 50, loss: 6.682699313387275e-05\n",
            "step: 60, loss: 4.662982246372849e-05\n",
            "step: 70, loss: 2.9856726541765966e-05\n",
            "step: 80, loss: 5.077280002296902e-05\n",
            "step: 90, loss: 0.00352520402520895\n",
            "step: 100, loss: 5.0579281378304586e-05\n",
            "step: 110, loss: 0.014992509968578815\n",
            "step: 120, loss: 4.755592453875579e-05\n",
            "step: 130, loss: 4.404899664223194e-05\n",
            "step: 140, loss: 7.005257793935016e-05\n",
            "step: 150, loss: 3.718083098647185e-05\n",
            "step: 160, loss: 0.0003344917204231024\n",
            "step: 170, loss: 9.539574239170179e-05\n",
            "step: 180, loss: 3.3481683203717694e-05\n",
            "step: 190, loss: 3.249034853070043e-05\n",
            "step: 200, loss: 4.3057614675490186e-05\n",
            "step: 210, loss: 2.2667765733785927e-05\n",
            "step: 220, loss: 0.00012619365588761866\n",
            "step: 230, loss: 2.6501062166062184e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9810055865921787, f1=0.9752252252252253, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001347194192931056\n",
            "step: 10, loss: 5.2417468396015465e-05\n",
            "step: 20, loss: 2.75401325779967e-05\n",
            "step: 30, loss: 4.2604213376762345e-05\n",
            "step: 40, loss: 0.0026712703984230757\n",
            "step: 50, loss: 4.779779555974528e-05\n",
            "step: 60, loss: 3.342537092976272e-05\n",
            "step: 70, loss: 0.00010869180550798774\n",
            "step: 80, loss: 4.6421213482972234e-05\n",
            "step: 90, loss: 0.0004856487503275275\n",
            "step: 100, loss: 3.1749346817377955e-05\n",
            "step: 110, loss: 5.121075810166076e-05\n",
            "step: 120, loss: 1.8838400137610734e-05\n",
            "step: 130, loss: 3.095985812251456e-05\n",
            "step: 140, loss: 3.173843651893549e-05\n",
            "step: 150, loss: 3.649269274319522e-05\n",
            "step: 160, loss: 0.0001916232577059418\n",
            "step: 170, loss: 1.7299984392593615e-05\n",
            "step: 180, loss: 2.861649227270391e-05\n",
            "step: 190, loss: 8.158788841683418e-05\n",
            "step: 200, loss: 2.786385448416695e-05\n",
            "step: 210, loss: 0.0001776122226146981\n",
            "step: 220, loss: 1.5809931937837973e-05\n",
            "step: 230, loss: 0.001070215250365436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9798657718120806, f1=0.9740698985343857, best_f1=0.9732142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2738597181160003e-05\n",
            "step: 10, loss: 1.6759729987825267e-05\n",
            "step: 20, loss: 5.8014786191051826e-05\n",
            "step: 30, loss: 3.4754797525238246e-05\n",
            "step: 40, loss: 4.6586519601987675e-05\n",
            "step: 50, loss: 0.027047252282500267\n",
            "step: 60, loss: 4.225346492603421e-05\n",
            "step: 70, loss: 7.880443445174024e-05\n",
            "step: 80, loss: 0.00012071905803168193\n",
            "step: 90, loss: 8.515013178111985e-05\n",
            "step: 100, loss: 3.162660505040549e-05\n",
            "step: 110, loss: 2.159131872758735e-05\n",
            "step: 120, loss: 3.789192851400003e-05\n",
            "step: 130, loss: 7.104618271114305e-05\n",
            "step: 140, loss: 2.601671076263301e-05\n",
            "step: 150, loss: 6.290211604209617e-05\n",
            "step: 160, loss: 2.1185265723033808e-05\n",
            "step: 170, loss: 2.938325997092761e-05\n",
            "step: 180, loss: 3.064633347094059e-05\n",
            "step: 190, loss: 3.3022664865711704e-05\n",
            "step: 200, loss: 3.340683178976178e-05\n",
            "step: 210, loss: 9.063672041520476e-05\n",
            "step: 220, loss: 3.2343985367333516e-05\n",
            "step: 230, loss: 4.015829472336918e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9798657718120806, f1=0.9740698985343857, best_f1=0.9732142857142857\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 205.64it/s]\n",
            "load_f1 = 0.9807909604519773\n",
            "real_f1 = 0.9785794813979707\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 237.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b79e388-bf2b-47dd-ea23-28eb6ad665e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6277890801429749\n",
            "step: 10, loss: 0.5401137471199036\n",
            "step: 20, loss: 0.4925890564918518\n",
            "step: 30, loss: 0.09358377009630203\n",
            "step: 40, loss: 0.15697036683559418\n",
            "step: 50, loss: 0.11799511313438416\n",
            "step: 60, loss: 0.022120365872979164\n",
            "step: 70, loss: 0.20516636967658997\n",
            "step: 80, loss: 0.05157952755689621\n",
            "step: 90, loss: 0.2309870421886444\n",
            "step: 100, loss: 0.015370539389550686\n",
            "step: 110, loss: 0.09851789474487305\n",
            "step: 120, loss: 0.020424634218215942\n",
            "step: 130, loss: 0.10099302232265472\n",
            "step: 140, loss: 0.1655089408159256\n",
            "step: 150, loss: 0.053060613572597504\n",
            "step: 160, loss: 0.053138889372348785\n",
            "step: 170, loss: 0.2071080505847931\n",
            "step: 180, loss: 0.08301939815282822\n",
            "step: 190, loss: 0.006557975430041552\n",
            "step: 200, loss: 0.1563977599143982\n",
            "step: 210, loss: 0.10844579339027405\n",
            "step: 220, loss: 0.3229817748069763\n",
            "step: 230, loss: 0.13933397829532623\n",
            "step: 240, loss: 0.04135093465447426\n",
            "step: 250, loss: 0.014047699049115181\n",
            "step: 260, loss: 0.054450128227472305\n",
            "step: 270, loss: 0.02808237262070179\n",
            "step: 280, loss: 0.06523087620735168\n",
            "step: 290, loss: 0.050891194492578506\n",
            "step: 300, loss: 0.034790221601724625\n",
            "step: 310, loss: 0.10996970534324646\n",
            "step: 320, loss: 0.10409153252840042\n",
            "step: 330, loss: 0.008260729722678661\n",
            "step: 340, loss: 0.035836976021528244\n",
            "step: 350, loss: 0.0458446629345417\n",
            "step: 360, loss: 0.05983582139015198\n",
            "step: 370, loss: 0.0452202707529068\n",
            "step: 380, loss: 0.005339811556041241\n",
            "step: 390, loss: 0.19936475157737732\n",
            "step: 400, loss: 0.23418304324150085\n",
            "step: 410, loss: 0.05657704547047615\n",
            "step: 420, loss: 0.11794032901525497\n",
            "step: 430, loss: 0.1302942931652069\n",
            "step: 440, loss: 0.02730679325759411\n",
            "step: 450, loss: 0.006465299054980278\n",
            "step: 460, loss: 0.018121512606739998\n",
            "step: 470, loss: 0.13384774327278137\n",
            "step: 480, loss: 0.07885212451219559\n",
            "step: 490, loss: 0.10032012313604355\n",
            "step: 500, loss: 0.045978084206581116\n",
            "step: 510, loss: 0.10449875891208649\n",
            "step: 520, loss: 0.17482031881809235\n",
            "step: 530, loss: 0.0064847455359995365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9400921658986175, f1=0.9385884509624199, best_f1=0.9385884509624199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1211157888174057\n",
            "step: 10, loss: 0.0477655865252018\n",
            "step: 20, loss: 0.003342619864270091\n",
            "step: 30, loss: 0.007796016987413168\n",
            "step: 40, loss: 0.07832899689674377\n",
            "step: 50, loss: 0.07141371071338654\n",
            "step: 60, loss: 0.03334581479430199\n",
            "step: 70, loss: 0.018582817167043686\n",
            "step: 80, loss: 0.02820514515042305\n",
            "step: 90, loss: 0.010513702407479286\n",
            "step: 100, loss: 0.01664401777088642\n",
            "step: 110, loss: 0.004755600355565548\n",
            "step: 120, loss: 0.08182498067617416\n",
            "step: 130, loss: 0.03648056089878082\n",
            "step: 140, loss: 0.013420580886304379\n",
            "step: 150, loss: 0.07560335844755173\n",
            "step: 160, loss: 0.03077135607600212\n",
            "step: 170, loss: 0.022284122183918953\n",
            "step: 180, loss: 0.06717805564403534\n",
            "step: 190, loss: 0.14986728131771088\n",
            "step: 200, loss: 0.007931994274258614\n",
            "step: 210, loss: 0.0427282340824604\n",
            "step: 220, loss: 0.034631840884685516\n",
            "step: 230, loss: 0.011200609616935253\n",
            "step: 240, loss: 0.006820487789809704\n",
            "step: 250, loss: 0.0036434470675885677\n",
            "step: 260, loss: 0.005505657754838467\n",
            "step: 270, loss: 0.19980962574481964\n",
            "step: 280, loss: 0.1843961626291275\n",
            "step: 290, loss: 0.02369779162108898\n",
            "step: 300, loss: 0.15149842202663422\n",
            "step: 310, loss: 0.008917215280234814\n",
            "step: 320, loss: 0.06599016487598419\n",
            "step: 330, loss: 0.015398976393043995\n",
            "step: 340, loss: 0.018164489418268204\n",
            "step: 350, loss: 0.0012905787443742156\n",
            "step: 360, loss: 0.11535002291202545\n",
            "step: 370, loss: 0.07539691776037216\n",
            "step: 380, loss: 0.04594064876437187\n",
            "step: 390, loss: 0.08317992836236954\n",
            "step: 400, loss: 0.01050263550132513\n",
            "step: 410, loss: 0.04844405874609947\n",
            "step: 420, loss: 0.026265397667884827\n",
            "step: 430, loss: 0.0682498961687088\n",
            "step: 440, loss: 0.18595291674137115\n",
            "step: 450, loss: 0.04901513084769249\n",
            "step: 460, loss: 0.06361862272024155\n",
            "step: 470, loss: 0.11881938576698303\n",
            "step: 480, loss: 0.1620926856994629\n",
            "step: 490, loss: 0.016985617578029633\n",
            "step: 500, loss: 0.099538654088974\n",
            "step: 510, loss: 0.00711823720484972\n",
            "step: 520, loss: 0.0744623914361\n",
            "step: 530, loss: 0.004302572458982468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9383402874362541, f1=0.9361702127659576, best_f1=0.9385884509624199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024890506640076637\n",
            "step: 10, loss: 0.030244987457990646\n",
            "step: 20, loss: 0.029570670798420906\n",
            "step: 30, loss: 0.033719804137945175\n",
            "step: 40, loss: 0.14237116277217865\n",
            "step: 50, loss: 0.05380785092711449\n",
            "step: 60, loss: 0.015229018405079842\n",
            "step: 70, loss: 0.003820891957730055\n",
            "step: 80, loss: 0.028413234278559685\n",
            "step: 90, loss: 0.005277583375573158\n",
            "step: 100, loss: 0.07085691392421722\n",
            "step: 110, loss: 0.0009531021933071315\n",
            "step: 120, loss: 0.0030116806738078594\n",
            "step: 130, loss: 0.005403550341725349\n",
            "step: 140, loss: 0.052197568118572235\n",
            "step: 150, loss: 0.01705251820385456\n",
            "step: 160, loss: 0.05670924484729767\n",
            "step: 170, loss: 0.11197502911090851\n",
            "step: 180, loss: 0.030233105644583702\n",
            "step: 190, loss: 0.016196420416235924\n",
            "step: 200, loss: 0.09643767774105072\n",
            "step: 210, loss: 0.012570958584547043\n",
            "step: 220, loss: 0.2870924174785614\n",
            "step: 230, loss: 0.1771148145198822\n",
            "step: 240, loss: 0.005214963108301163\n",
            "step: 250, loss: 0.013010144233703613\n",
            "step: 260, loss: 0.010617159307003021\n",
            "step: 270, loss: 0.008892916142940521\n",
            "step: 280, loss: 0.05905481055378914\n",
            "step: 290, loss: 0.0027998003643006086\n",
            "step: 300, loss: 0.06305889040231705\n",
            "step: 310, loss: 0.008247855119407177\n",
            "step: 320, loss: 0.10121597349643707\n",
            "step: 330, loss: 0.017338305711746216\n",
            "step: 340, loss: 0.00426814891397953\n",
            "step: 350, loss: 0.0048324051313102245\n",
            "step: 360, loss: 0.036986734718084335\n",
            "step: 370, loss: 0.010715476237237453\n",
            "step: 380, loss: 0.007789201103150845\n",
            "step: 390, loss: 0.007047706749290228\n",
            "step: 400, loss: 0.017343733459711075\n",
            "step: 410, loss: 0.012584852054715157\n",
            "step: 420, loss: 0.18326450884342194\n",
            "step: 430, loss: 0.14313212037086487\n",
            "step: 440, loss: 0.009874312207102776\n",
            "step: 450, loss: 0.05186435952782631\n",
            "step: 460, loss: 0.042007628828287125\n",
            "step: 470, loss: 0.10368693619966507\n",
            "step: 480, loss: 0.0020001260563731194\n",
            "step: 490, loss: 0.0027760942466557026\n",
            "step: 500, loss: 0.025789018720388412\n",
            "step: 510, loss: 0.015513168647885323\n",
            "step: 520, loss: 0.040211271494627\n",
            "step: 530, loss: 0.02298365905880928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9442622950819672, f1=0.9337094499294781, best_f1=0.9337094499294781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004029400646686554\n",
            "step: 10, loss: 0.0031598226632922888\n",
            "step: 20, loss: 0.0021823272109031677\n",
            "step: 30, loss: 0.0011236706050112844\n",
            "step: 40, loss: 0.009433098137378693\n",
            "step: 50, loss: 0.0014454106567427516\n",
            "step: 60, loss: 0.0006287130527198315\n",
            "step: 70, loss: 0.00445720087736845\n",
            "step: 80, loss: 0.0065414863638579845\n",
            "step: 90, loss: 0.11655863374471664\n",
            "step: 100, loss: 0.028793998062610626\n",
            "step: 110, loss: 0.027862319722771645\n",
            "step: 120, loss: 0.002509559039026499\n",
            "step: 130, loss: 0.00173733732663095\n",
            "step: 140, loss: 0.0059920852072536945\n",
            "step: 150, loss: 0.008801323361694813\n",
            "step: 160, loss: 0.024781756103038788\n",
            "step: 170, loss: 0.004622488282620907\n",
            "step: 180, loss: 0.006789210252463818\n",
            "step: 190, loss: 0.003649692749604583\n",
            "step: 200, loss: 0.0017819618806242943\n",
            "step: 210, loss: 0.12227856367826462\n",
            "step: 220, loss: 0.003532339120283723\n",
            "step: 230, loss: 0.010673709213733673\n",
            "step: 240, loss: 0.0019374415278434753\n",
            "step: 250, loss: 0.12793157994747162\n",
            "step: 260, loss: 0.0018993888515979052\n",
            "step: 270, loss: 0.005094545427709818\n",
            "step: 280, loss: 0.017458468675613403\n",
            "step: 290, loss: 0.043654683977365494\n",
            "step: 300, loss: 0.0003125408256892115\n",
            "step: 310, loss: 0.0003632127190940082\n",
            "step: 320, loss: 0.0005837606149725616\n",
            "step: 330, loss: 0.0029162445571273565\n",
            "step: 340, loss: 0.03356395289301872\n",
            "step: 350, loss: 0.05105164647102356\n",
            "step: 360, loss: 0.004505367949604988\n",
            "step: 370, loss: 0.0038112029433250427\n",
            "step: 380, loss: 0.015389458276331425\n",
            "step: 390, loss: 0.006079785991460085\n",
            "step: 400, loss: 0.010752893052995205\n",
            "step: 410, loss: 0.0016250339103862643\n",
            "step: 420, loss: 0.0008481679251417518\n",
            "step: 430, loss: 0.09020119905471802\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 440, loss: 0.007390564773231745\n",
            "step: 450, loss: 0.0017554184887558222\n",
            "step: 460, loss: 0.0004815805295947939\n",
            "step: 470, loss: 0.04125542938709259\n",
            "step: 480, loss: 0.04979235678911209\n",
            "step: 490, loss: 0.011660313233733177\n",
            "step: 500, loss: 0.012067980132997036\n",
            "step: 510, loss: 0.01640443131327629\n",
            "step: 520, loss: 0.2337028831243515\n",
            "step: 530, loss: 0.031108904629945755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9368709972552608, f1=0.9402644778841769, best_f1=0.9337094499294781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019901832565665245\n",
            "step: 10, loss: 0.019820135086774826\n",
            "step: 20, loss: 0.01383666880428791\n",
            "step: 30, loss: 0.0016255427617579699\n",
            "step: 40, loss: 0.050771184265613556\n",
            "step: 50, loss: 0.0005433229962363839\n",
            "step: 60, loss: 0.05145745351910591\n",
            "step: 70, loss: 0.0010125036351382732\n",
            "step: 80, loss: 0.005387888755649328\n",
            "step: 90, loss: 0.014230873435735703\n",
            "step: 100, loss: 0.0014576801331713796\n",
            "step: 110, loss: 0.0014366862596943974\n",
            "step: 120, loss: 0.002006460912525654\n",
            "step: 130, loss: 0.000402974896132946\n",
            "step: 140, loss: 0.0012014267267659307\n",
            "step: 150, loss: 0.01397110614925623\n",
            "step: 160, loss: 0.00027145532658323646\n",
            "step: 170, loss: 0.031449928879737854\n",
            "step: 180, loss: 0.00043977409950457513\n",
            "step: 190, loss: 0.026059091091156006\n",
            "step: 200, loss: 0.0005186606431379914\n",
            "step: 210, loss: 0.037274230271577835\n",
            "step: 220, loss: 0.0010889873374253511\n",
            "step: 230, loss: 0.0032902390230447054\n",
            "step: 240, loss: 0.00033385661663487554\n",
            "step: 250, loss: 0.00016519712517037988\n",
            "step: 260, loss: 0.000385638588340953\n",
            "step: 270, loss: 0.0003206878318451345\n",
            "step: 280, loss: 0.08906394988298416\n",
            "step: 290, loss: 0.2223379760980606\n",
            "step: 300, loss: 0.014010470360517502\n",
            "step: 310, loss: 0.0013560400111600757\n",
            "step: 320, loss: 0.06522440910339355\n",
            "step: 330, loss: 0.005517344456166029\n",
            "step: 340, loss: 0.003196856938302517\n",
            "step: 350, loss: 0.009644375182688236\n",
            "step: 360, loss: 0.020292842760682106\n",
            "step: 370, loss: 0.06336291134357452\n",
            "step: 380, loss: 0.019686011597514153\n",
            "step: 390, loss: 0.0002327578840777278\n",
            "step: 400, loss: 0.0015990639803931117\n",
            "step: 410, loss: 0.0014078631065785885\n",
            "step: 420, loss: 0.0012799229007214308\n",
            "step: 430, loss: 0.03158478066325188\n",
            "step: 440, loss: 0.0639154314994812\n",
            "step: 450, loss: 0.0037433423567563295\n",
            "step: 460, loss: 0.0008429995505139232\n",
            "step: 470, loss: 0.09505152702331543\n",
            "step: 480, loss: 0.0032656630501151085\n",
            "step: 490, loss: 0.0008515637018717825\n",
            "step: 500, loss: 0.0015430997591465712\n",
            "step: 510, loss: 0.000693190551828593\n",
            "step: 520, loss: 0.001225095707923174\n",
            "step: 530, loss: 0.002215772634372115\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9397477814105558, f1=0.9375, best_f1=0.9337094499294781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00525762839242816\n",
            "step: 10, loss: 0.001160134095698595\n",
            "step: 20, loss: 0.11241672933101654\n",
            "step: 30, loss: 0.003891445230692625\n",
            "step: 40, loss: 0.0004979026271030307\n",
            "step: 50, loss: 0.014318653382360935\n",
            "step: 60, loss: 0.00010469734843354672\n",
            "step: 70, loss: 0.00020379199122544378\n",
            "step: 80, loss: 0.0008835762273520231\n",
            "step: 90, loss: 0.00029903455288149416\n",
            "step: 100, loss: 0.0009233365999534726\n",
            "step: 110, loss: 0.00011635648115770891\n",
            "step: 120, loss: 0.0005847836728207767\n",
            "step: 130, loss: 0.0004958037752658129\n",
            "step: 140, loss: 0.002679811092093587\n",
            "step: 150, loss: 0.007288631051778793\n",
            "step: 160, loss: 0.001559481373988092\n",
            "step: 170, loss: 0.001334390603005886\n",
            "step: 180, loss: 0.00017965779989026487\n",
            "step: 190, loss: 0.0017504667630419135\n",
            "step: 200, loss: 0.00014619884314015508\n",
            "step: 210, loss: 0.0018052497180178761\n",
            "step: 220, loss: 0.00562336714938283\n",
            "step: 230, loss: 0.0003312480985186994\n",
            "step: 240, loss: 0.03172751143574715\n",
            "step: 250, loss: 0.00017744407523423433\n",
            "step: 260, loss: 7.221769192256033e-05\n",
            "step: 270, loss: 0.036002520471811295\n",
            "step: 280, loss: 0.002923786174505949\n",
            "step: 290, loss: 0.0003294544294476509\n",
            "step: 300, loss: 0.00021441663557197899\n",
            "step: 310, loss: 0.0025570187717676163\n",
            "step: 320, loss: 0.011787804774940014\n",
            "step: 330, loss: 0.0011238261358812451\n",
            "step: 340, loss: 0.0990300104022026\n",
            "step: 350, loss: 0.0013619504170492291\n",
            "step: 360, loss: 0.014731962233781815\n",
            "step: 370, loss: 0.006709308363497257\n",
            "step: 380, loss: 0.00798651110380888\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 390, loss: 0.11483172327280045\n",
            "step: 400, loss: 0.06127770245075226\n",
            "step: 410, loss: 0.0005869162851013243\n",
            "step: 420, loss: 0.0059735761024057865\n",
            "step: 430, loss: 0.0007967376732267439\n",
            "step: 440, loss: 0.00024131541431415826\n",
            "step: 450, loss: 0.0012457005213946104\n",
            "step: 460, loss: 9.803700959309936e-05\n",
            "step: 470, loss: 0.00662261014804244\n",
            "step: 480, loss: 0.0012758359080180526\n",
            "step: 490, loss: 0.008838511072099209\n",
            "step: 500, loss: 0.0012582233175635338\n",
            "step: 510, loss: 0.004023327026516199\n",
            "step: 520, loss: 0.05839981138706207\n",
            "step: 530, loss: 0.004548028577119112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9444444444444445, f1=0.9401473296500922, best_f1=0.9401473296500922\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003279993252363056\n",
            "step: 10, loss: 0.00012095070269424468\n",
            "step: 20, loss: 0.0006629256531596184\n",
            "step: 30, loss: 0.00047764720511622727\n",
            "step: 40, loss: 0.0005574729293584824\n",
            "step: 50, loss: 0.000732859072741121\n",
            "step: 60, loss: 0.0002195872220909223\n",
            "step: 70, loss: 0.0001396564912283793\n",
            "step: 80, loss: 0.0002924291475210339\n",
            "step: 90, loss: 0.005865878891199827\n",
            "step: 100, loss: 7.383839692920446e-05\n",
            "step: 110, loss: 0.00038922313251532614\n",
            "step: 120, loss: 0.0002607672940939665\n",
            "step: 130, loss: 0.08029236644506454\n",
            "step: 140, loss: 7.005201769061387e-05\n",
            "step: 150, loss: 9.01827952475287e-05\n",
            "step: 160, loss: 0.00018371517944615334\n",
            "step: 170, loss: 0.00014928060409147292\n",
            "step: 180, loss: 0.007441405206918716\n",
            "step: 190, loss: 0.0011366576654836535\n",
            "step: 200, loss: 0.00021456957620102912\n",
            "step: 210, loss: 0.00014636847481597215\n",
            "step: 220, loss: 8.735043957130983e-05\n",
            "step: 230, loss: 0.0012024492025375366\n",
            "step: 240, loss: 0.00043785845628008246\n",
            "step: 250, loss: 0.012077820487320423\n",
            "step: 260, loss: 0.00011969144543400034\n",
            "step: 270, loss: 0.0003594333538785577\n",
            "step: 280, loss: 0.0007949810824356973\n",
            "step: 290, loss: 0.00012913662067148834\n",
            "step: 300, loss: 0.002874132012948394\n",
            "step: 310, loss: 0.0002907110610976815\n",
            "step: 320, loss: 5.67601528018713e-05\n",
            "step: 330, loss: 0.0008077375823631883\n",
            "step: 340, loss: 0.0004796165449079126\n",
            "step: 350, loss: 0.001972158206626773\n",
            "step: 360, loss: 0.003688022494316101\n",
            "step: 370, loss: 0.0008453500922769308\n",
            "step: 380, loss: 0.0240325927734375\n",
            "step: 390, loss: 0.00023088915622793138\n",
            "step: 400, loss: 0.0013621748657897115\n",
            "step: 410, loss: 0.003585544414818287\n",
            "step: 420, loss: 0.024348212406039238\n",
            "step: 430, loss: 8.11056888778694e-05\n",
            "step: 440, loss: 0.000106784536910709\n",
            "step: 450, loss: 0.012155337259173393\n",
            "step: 460, loss: 0.00045598275028169155\n",
            "step: 470, loss: 0.00791491661220789\n",
            "step: 480, loss: 0.06854856759309769\n",
            "step: 490, loss: 0.01694074645638466\n",
            "step: 500, loss: 0.0010097639169543982\n",
            "step: 510, loss: 0.0872812271118164\n",
            "step: 520, loss: 0.02485678344964981\n",
            "step: 530, loss: 0.01139755081385374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9454878607420981, f1=0.9372997711670481, best_f1=0.9372997711670481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019285579037386924\n",
            "step: 10, loss: 0.023263471201062202\n",
            "step: 20, loss: 0.0001089574652723968\n",
            "step: 30, loss: 0.008192921057343483\n",
            "step: 40, loss: 4.88999648950994e-05\n",
            "step: 50, loss: 0.006875291001051664\n",
            "step: 60, loss: 0.00043855467811226845\n",
            "step: 70, loss: 0.019032398238778114\n",
            "step: 80, loss: 0.002714860253036022\n",
            "step: 90, loss: 0.0009240591898560524\n",
            "step: 100, loss: 0.00034236389910802245\n",
            "step: 110, loss: 0.0010982933454215527\n",
            "step: 120, loss: 0.006473291199654341\n",
            "step: 130, loss: 0.0004271473444532603\n",
            "step: 140, loss: 0.011484192684292793\n",
            "step: 150, loss: 9.480108565185219e-05\n",
            "step: 160, loss: 0.0022754317615181208\n",
            "step: 170, loss: 0.005010291468352079\n",
            "step: 180, loss: 5.3803629270987585e-05\n",
            "step: 190, loss: 0.003039880422875285\n",
            "step: 200, loss: 0.0013977609341964126\n",
            "step: 210, loss: 0.00010274403030052781\n",
            "step: 220, loss: 0.002912912517786026\n",
            "step: 230, loss: 0.0002829078584909439\n",
            "step: 240, loss: 0.00022734698723070323\n",
            "step: 250, loss: 0.0003643865929916501\n",
            "step: 260, loss: 3.7571913708234206e-05\n",
            "step: 270, loss: 0.0015984972706064582\n",
            "step: 280, loss: 0.10042616724967957\n",
            "step: 290, loss: 0.00033102091401815414\n",
            "step: 300, loss: 0.013247620314359665\n",
            "step: 310, loss: 0.002351634204387665\n",
            "step: 320, loss: 0.0009152052807621658\n",
            "step: 330, loss: 0.008892995305359364\n",
            "step: 340, loss: 9.141231566900387e-05\n",
            "step: 350, loss: 8.995326788863167e-05\n",
            "step: 360, loss: 0.0006843434530310333\n",
            "step: 370, loss: 0.0020595737732946873\n",
            "step: 380, loss: 0.0006976762088015676\n",
            "step: 390, loss: 0.14056101441383362\n",
            "step: 400, loss: 0.0019074215088039637\n",
            "step: 410, loss: 0.00020735952421091497\n",
            "step: 420, loss: 0.0025091145653277636\n",
            "step: 430, loss: 0.002417110837996006\n",
            "step: 440, loss: 0.0016099673230201006\n",
            "step: 450, loss: 0.0025987299159169197\n",
            "step: 460, loss: 0.0004774643457494676\n",
            "step: 470, loss: 0.0003398527333047241\n",
            "step: 480, loss: 5.794157914351672e-05\n",
            "step: 490, loss: 0.0023451042361557484\n",
            "step: 500, loss: 0.0035822875797748566\n",
            "step: 510, loss: 6.551074329763651e-05\n",
            "step: 520, loss: 0.0013762138551101089\n",
            "step: 530, loss: 0.0006283746333792806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9423887587822014, f1=0.9350163627863488, best_f1=0.9372997711670481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002291990676894784\n",
            "step: 10, loss: 0.00023019824584480375\n",
            "step: 20, loss: 0.05825866013765335\n",
            "step: 30, loss: 0.0564480684697628\n",
            "step: 40, loss: 0.0007930222782306373\n",
            "step: 50, loss: 9.817945101531222e-05\n",
            "step: 60, loss: 8.168078784365207e-05\n",
            "step: 70, loss: 0.0001599852548679337\n",
            "step: 80, loss: 0.001297641429118812\n",
            "step: 90, loss: 5.603625322692096e-05\n",
            "step: 100, loss: 4.926931069348939e-05\n",
            "step: 110, loss: 3.688247670652345e-05\n",
            "step: 120, loss: 9.6752199169714e-05\n",
            "step: 130, loss: 0.0622701495885849\n",
            "step: 140, loss: 0.0004494689346756786\n",
            "step: 150, loss: 8.196422277251258e-05\n",
            "step: 160, loss: 0.0006392227951437235\n",
            "step: 170, loss: 0.00757882185280323\n",
            "step: 180, loss: 3.925777491531335e-05\n",
            "step: 190, loss: 4.011426790384576e-05\n",
            "step: 200, loss: 0.0005850880988873541\n",
            "step: 210, loss: 0.00018131794058717787\n",
            "step: 220, loss: 0.004168723709881306\n",
            "step: 230, loss: 0.0001256784307770431\n",
            "step: 240, loss: 5.367711855797097e-05\n",
            "step: 250, loss: 0.0014583999291062355\n",
            "step: 260, loss: 0.010410862974822521\n",
            "step: 270, loss: 0.0001799377496354282\n",
            "step: 280, loss: 0.0005648242076858878\n",
            "step: 290, loss: 0.0900806114077568\n",
            "step: 300, loss: 0.05195058137178421\n",
            "step: 310, loss: 0.0091116176918149\n",
            "step: 320, loss: 0.017636224627494812\n",
            "step: 330, loss: 0.00011476722283987328\n",
            "step: 340, loss: 0.0026528039015829563\n",
            "step: 350, loss: 0.06775032728910446\n",
            "step: 360, loss: 2.020567626459524e-05\n",
            "step: 370, loss: 8.122391591314226e-05\n",
            "step: 380, loss: 0.00048351369332522154\n",
            "step: 390, loss: 4.4762768084183335e-05\n",
            "step: 400, loss: 0.0009736159699968994\n",
            "step: 410, loss: 2.5692410417832434e-05\n",
            "step: 420, loss: 0.00014075826038606465\n",
            "step: 430, loss: 8.401070954278111e-05\n",
            "step: 440, loss: 0.0020448477007448673\n",
            "step: 450, loss: 5.2750332542927936e-05\n",
            "step: 460, loss: 0.0006686695851385593\n",
            "step: 470, loss: 3.584170190151781e-05\n",
            "step: 480, loss: 0.0006502229371108115\n",
            "step: 490, loss: 0.0005528644542209804\n",
            "step: 500, loss: 0.0017267153598368168\n",
            "step: 510, loss: 0.000798159628175199\n",
            "step: 520, loss: 0.004854770842939615\n",
            "step: 530, loss: 0.011541763320565224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9397477814105558, f1=0.9394221808014911, best_f1=0.9372997711670481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001853837282396853\n",
            "step: 10, loss: 6.460327131208032e-05\n",
            "step: 20, loss: 0.0001205945955007337\n",
            "step: 30, loss: 0.00020503650011960417\n",
            "step: 40, loss: 0.0010543528478592634\n",
            "step: 50, loss: 0.011734250001609325\n",
            "step: 60, loss: 0.00027808151207864285\n",
            "step: 70, loss: 0.0002982491278089583\n",
            "step: 80, loss: 4.495868051890284e-05\n",
            "step: 90, loss: 0.00028723233845084906\n",
            "step: 100, loss: 0.001761143677867949\n",
            "step: 110, loss: 2.2388503566617146e-05\n",
            "step: 120, loss: 0.00045091332867741585\n",
            "step: 130, loss: 2.9942815672256984e-05\n",
            "step: 140, loss: 0.0008037713705562055\n",
            "step: 150, loss: 9.732276521390304e-05\n",
            "step: 160, loss: 2.931322160293348e-05\n",
            "step: 170, loss: 0.0006888905190862715\n",
            "step: 180, loss: 0.0001435232552466914\n",
            "step: 190, loss: 3.133227801299654e-05\n",
            "step: 200, loss: 0.0005986959440633655\n",
            "step: 210, loss: 0.08385951071977615\n",
            "step: 220, loss: 0.0026507165748625994\n",
            "step: 230, loss: 0.013507096096873283\n",
            "step: 240, loss: 0.0003071201790589839\n",
            "step: 250, loss: 6.460878648795187e-05\n",
            "step: 260, loss: 0.0006056064157746732\n",
            "step: 270, loss: 0.00027082496671937406\n",
            "step: 280, loss: 3.8551468605874106e-05\n",
            "step: 290, loss: 0.00011827194248326123\n",
            "step: 300, loss: 4.8343110393034294e-05\n",
            "step: 310, loss: 8.970459748525172e-05\n",
            "step: 320, loss: 0.0027176993899047375\n",
            "step: 330, loss: 0.0002189222868764773\n",
            "step: 340, loss: 7.50070612411946e-05\n",
            "step: 350, loss: 7.613956404384226e-05\n",
            "step: 360, loss: 0.003264127066358924\n",
            "step: 370, loss: 0.0035232282243669033\n",
            "step: 380, loss: 0.00020388298435136676\n",
            "step: 390, loss: 0.0002879300736822188\n",
            "step: 400, loss: 0.0003498020814731717\n",
            "step: 410, loss: 0.0031711712945252657\n",
            "step: 420, loss: 0.0003260485827922821\n",
            "step: 430, loss: 0.00047829069080762565\n",
            "step: 440, loss: 0.0003436666738707572\n",
            "step: 450, loss: 2.7000080081052147e-05\n",
            "step: 460, loss: 3.514099444146268e-05\n",
            "step: 470, loss: 2.9332346457522362e-05\n",
            "step: 480, loss: 0.00013640450197272003\n",
            "step: 490, loss: 0.00011297149467281997\n",
            "step: 500, loss: 0.004771411418914795\n",
            "step: 510, loss: 2.344276254007127e-05\n",
            "step: 520, loss: 6.668551941402256e-05\n",
            "step: 530, loss: 0.0012149200774729252\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9438727782974743, f1=0.9388136384866884, best_f1=0.9372997711670481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.806793291820213e-05\n",
            "step: 10, loss: 9.851338109001517e-05\n",
            "step: 20, loss: 8.162757148966193e-05\n",
            "step: 30, loss: 0.00019681004050653428\n",
            "step: 40, loss: 4.046364119858481e-05\n",
            "step: 50, loss: 6.004919487168081e-05\n",
            "step: 60, loss: 0.012370854616165161\n",
            "step: 70, loss: 5.334506568033248e-05\n",
            "step: 80, loss: 2.451531145197805e-05\n",
            "step: 90, loss: 6.979874888202175e-05\n",
            "step: 100, loss: 6.140807090559974e-05\n",
            "step: 110, loss: 5.880527896806598e-05\n",
            "step: 120, loss: 1.9430781321716495e-05\n",
            "step: 130, loss: 1.7139787814812735e-05\n",
            "step: 140, loss: 4.3454652768559754e-05\n",
            "step: 150, loss: 1.5962659745127894e-05\n",
            "step: 160, loss: 2.905945984821301e-05\n",
            "step: 170, loss: 2.2511461793328635e-05\n",
            "step: 180, loss: 9.802373824641109e-05\n",
            "step: 190, loss: 4.523535608313978e-05\n",
            "step: 200, loss: 0.0003779154212679714\n",
            "step: 210, loss: 0.0001482255320297554\n",
            "step: 220, loss: 0.0018002755241468549\n",
            "step: 230, loss: 1.7866135749500245e-05\n",
            "step: 240, loss: 0.0002581153530627489\n",
            "step: 250, loss: 3.356857268954627e-05\n",
            "step: 260, loss: 2.1013858713558875e-05\n",
            "step: 270, loss: 2.660150312294718e-05\n",
            "step: 280, loss: 0.004674956668168306\n",
            "step: 290, loss: 0.001339248032309115\n",
            "step: 300, loss: 5.5139891628641635e-05\n",
            "step: 310, loss: 0.0017545027658343315\n",
            "step: 320, loss: 2.3491089450544678e-05\n",
            "step: 330, loss: 2.311500247742515e-05\n",
            "step: 340, loss: 1.942337621585466e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 350, loss: 3.1716986995888874e-05\n",
            "step: 360, loss: 5.2113711717538536e-05\n",
            "step: 370, loss: 0.0006557742599397898\n",
            "step: 380, loss: 2.0589322957675904e-05\n",
            "step: 390, loss: 5.71264790778514e-05\n",
            "step: 400, loss: 4.5213466364657506e-05\n",
            "step: 410, loss: 7.860890036681667e-05\n",
            "step: 420, loss: 0.0002947856264654547\n",
            "step: 430, loss: 4.155679198447615e-05\n",
            "step: 440, loss: 0.010387399233877659\n",
            "step: 450, loss: 0.00010949755960609764\n",
            "step: 460, loss: 0.010922685265541077\n",
            "step: 470, loss: 0.0002217740984633565\n",
            "step: 480, loss: 0.00013465843221638352\n",
            "step: 490, loss: 8.122319559333846e-05\n",
            "step: 500, loss: 4.9631173169473186e-05\n",
            "step: 510, loss: 1.8596219888422638e-05\n",
            "step: 520, loss: 2.9028977223788388e-05\n",
            "step: 530, loss: 1.6189929738175124e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9447565543071161, f1=0.9340196537201684, best_f1=0.9372997711670481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.837453914457001e-05\n",
            "step: 10, loss: 1.3552477867051493e-05\n",
            "step: 20, loss: 1.9691473426064476e-05\n",
            "step: 30, loss: 0.002082939026877284\n",
            "step: 40, loss: 0.00019966527179349214\n",
            "step: 50, loss: 0.10259127616882324\n",
            "step: 60, loss: 0.0011510838521644473\n",
            "step: 70, loss: 2.4608343665022403e-05\n",
            "step: 80, loss: 2.2202029867912643e-05\n",
            "step: 90, loss: 5.801049337605946e-05\n",
            "step: 100, loss: 0.00040180247742682695\n",
            "step: 110, loss: 8.506998710799962e-05\n",
            "step: 120, loss: 1.4088914213061798e-05\n",
            "step: 130, loss: 0.001394439022988081\n",
            "step: 140, loss: 2.075691008940339e-05\n",
            "step: 150, loss: 0.00015495788829866797\n",
            "step: 160, loss: 1.3731296348851174e-05\n",
            "step: 170, loss: 3.141288834740408e-05\n",
            "step: 180, loss: 2.1922809537500143e-05\n",
            "step: 190, loss: 0.0003965340438298881\n",
            "step: 200, loss: 0.0037228583823889494\n",
            "step: 210, loss: 4.256219108356163e-05\n",
            "step: 220, loss: 2.1826055672136135e-05\n",
            "step: 230, loss: 1.3735017091676127e-05\n",
            "step: 240, loss: 2.67522300418932e-05\n",
            "step: 250, loss: 2.1374658899731003e-05\n",
            "step: 260, loss: 2.0321087504271418e-05\n",
            "step: 270, loss: 4.916092802886851e-05\n",
            "step: 280, loss: 0.001514533767476678\n",
            "step: 290, loss: 4.591490142047405e-05\n",
            "step: 300, loss: 0.0006052168319001794\n",
            "step: 310, loss: 5.630679879686795e-05\n",
            "step: 320, loss: 5.389426223700866e-05\n",
            "step: 330, loss: 0.0016781092854216695\n",
            "step: 340, loss: 1.9039376638829708e-05\n",
            "step: 350, loss: 3.734952042577788e-05\n",
            "step: 360, loss: 0.0001497109333286062\n",
            "step: 370, loss: 1.822387639549561e-05\n",
            "step: 380, loss: 8.013076876522973e-05\n",
            "step: 390, loss: 2.3297430743696168e-05\n",
            "step: 400, loss: 2.093930925184395e-05\n",
            "step: 410, loss: 1.6934931409195997e-05\n",
            "step: 420, loss: 0.005685355514287949\n",
            "step: 430, loss: 0.0011130614439025521\n",
            "step: 440, loss: 1.580251228006091e-05\n",
            "step: 450, loss: 7.64301948947832e-05\n",
            "step: 460, loss: 2.3203932869364507e-05\n",
            "step: 470, loss: 2.094288356602192e-05\n",
            "step: 480, loss: 2.908915121224709e-05\n",
            "step: 490, loss: 2.022801709244959e-05\n",
            "step: 500, loss: 2.0510826288955286e-05\n",
            "step: 510, loss: 0.002382053527981043\n",
            "step: 520, loss: 3.3957625419134274e-05\n",
            "step: 530, loss: 0.0003995473380200565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9463276836158193, f1=0.9315714959886738, best_f1=0.9315714959886738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005869815591722727\n",
            "step: 10, loss: 2.4990844394778833e-05\n",
            "step: 20, loss: 2.235809006378986e-05\n",
            "step: 30, loss: 0.0006342357373796403\n",
            "step: 40, loss: 1.7083886632462963e-05\n",
            "step: 50, loss: 3.477520294836722e-05\n",
            "step: 60, loss: 1.8283119061379693e-05\n",
            "step: 70, loss: 1.7579093764652498e-05\n",
            "step: 80, loss: 2.370597758272197e-05\n",
            "step: 90, loss: 6.125629442976788e-05\n",
            "step: 100, loss: 3.389563789824024e-05\n",
            "step: 110, loss: 1.4733306670677848e-05\n",
            "step: 120, loss: 2.084252264467068e-05\n",
            "step: 130, loss: 1.2431184586603194e-05\n",
            "step: 140, loss: 3.3661224733805284e-05\n",
            "step: 150, loss: 5.0937051128130406e-05\n",
            "step: 160, loss: 1.675980820436962e-05\n",
            "step: 170, loss: 3.177334292558953e-05\n",
            "step: 180, loss: 4.135006747674197e-05\n",
            "step: 190, loss: 0.0003965084324590862\n",
            "step: 200, loss: 2.894335011660587e-05\n",
            "step: 210, loss: 2.435886017337907e-05\n",
            "step: 220, loss: 0.0018674946622923017\n",
            "step: 230, loss: 0.0001558394724270329\n",
            "step: 240, loss: 8.679387246957049e-05\n",
            "step: 250, loss: 2.9644743335666135e-05\n",
            "step: 260, loss: 1.7969949112739414e-05\n",
            "step: 270, loss: 1.9646571672637947e-05\n",
            "step: 280, loss: 3.7697729567298666e-05\n",
            "step: 290, loss: 1.2501939636422321e-05\n",
            "step: 300, loss: 0.00013044085062574595\n",
            "step: 310, loss: 0.0014915333595126867\n",
            "step: 320, loss: 0.003159275045618415\n",
            "step: 330, loss: 0.002558006439357996\n",
            "step: 340, loss: 1.7850983567768708e-05\n",
            "step: 350, loss: 1.8365084542892873e-05\n",
            "step: 360, loss: 1.2602538845385425e-05\n",
            "step: 370, loss: 0.00010826573998201638\n",
            "step: 380, loss: 1.5370334949693643e-05\n",
            "step: 390, loss: 0.00038849280099384487\n",
            "step: 400, loss: 3.0336663257912733e-05\n",
            "step: 410, loss: 0.00014331206330098212\n",
            "step: 420, loss: 1.2401354979374446e-05\n",
            "step: 430, loss: 0.00170781253837049\n",
            "step: 440, loss: 0.00011457499931566417\n",
            "step: 450, loss: 4.826101576327346e-05\n",
            "step: 460, loss: 0.002815593034029007\n",
            "step: 470, loss: 0.018956787884235382\n",
            "step: 480, loss: 4.990110028302297e-05\n",
            "step: 490, loss: 1.609676837688312e-05\n",
            "step: 500, loss: 0.00010701519931899384\n",
            "step: 510, loss: 3.519875099300407e-05\n",
            "step: 520, loss: 0.0006519740563817322\n",
            "step: 530, loss: 7.854896102799103e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.939294117647059, f1=0.9328953542937587, best_f1=0.9315714959886738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002014685422182083\n",
            "step: 10, loss: 1.1447691576904617e-05\n",
            "step: 20, loss: 1.5515526683884673e-05\n",
            "step: 30, loss: 1.592912303749472e-05\n",
            "step: 40, loss: 3.5756220313487574e-05\n",
            "step: 50, loss: 0.00023267797951120883\n",
            "step: 60, loss: 6.140947516541928e-05\n",
            "step: 70, loss: 0.0002075389347737655\n",
            "step: 80, loss: 2.036933983617928e-05\n",
            "step: 90, loss: 1.7061413018382154e-05\n",
            "step: 100, loss: 0.00011994758096989244\n",
            "step: 110, loss: 4.187955710222013e-05\n",
            "step: 120, loss: 2.0439956642803736e-05\n",
            "step: 130, loss: 0.003923320211470127\n",
            "step: 140, loss: 0.00015667347179260105\n",
            "step: 150, loss: 6.868817581562325e-05\n",
            "step: 160, loss: 0.0008593078819103539\n",
            "step: 170, loss: 0.007677081041038036\n",
            "step: 180, loss: 1.3585963642981369e-05\n",
            "step: 190, loss: 5.6955774198286235e-05\n",
            "step: 200, loss: 0.00013679797120857984\n",
            "step: 210, loss: 0.00017739225586410612\n",
            "step: 220, loss: 0.0001511242298875004\n",
            "step: 230, loss: 0.0006850072531960905\n",
            "step: 240, loss: 1.2624812370631844e-05\n",
            "step: 250, loss: 2.4090568331303075e-05\n",
            "step: 260, loss: 9.108879021368921e-05\n",
            "step: 270, loss: 0.00010349391959607601\n",
            "step: 280, loss: 1.3228247553342953e-05\n",
            "step: 290, loss: 0.00040979203185997903\n",
            "step: 300, loss: 4.1012404835782945e-05\n",
            "step: 310, loss: 2.294340265507344e-05\n",
            "step: 320, loss: 0.0015216696774587035\n",
            "step: 330, loss: 3.9108246710384265e-05\n",
            "step: 340, loss: 6.087867222959176e-05\n",
            "step: 350, loss: 0.005849554669111967\n",
            "step: 360, loss: 6.449200009228662e-05\n",
            "step: 370, loss: 3.7240100937196985e-05\n",
            "step: 380, loss: 3.219677091692574e-05\n",
            "step: 390, loss: 0.00380121823400259\n",
            "step: 400, loss: 8.424829138675705e-05\n",
            "step: 410, loss: 2.0473044060054235e-05\n",
            "step: 420, loss: 1.0684042536013294e-05\n",
            "step: 430, loss: 7.326593186007813e-05\n",
            "step: 440, loss: 0.00010686753375921398\n",
            "step: 450, loss: 4.5683918870054185e-05\n",
            "step: 460, loss: 0.0001117471547331661\n",
            "step: 470, loss: 2.4546294298488647e-05\n",
            "step: 480, loss: 1.852149398473557e-05\n",
            "step: 490, loss: 1.4632593774877023e-05\n",
            "step: 500, loss: 1.1011864444299135e-05\n",
            "step: 510, loss: 0.01964733749628067\n",
            "step: 520, loss: 2.717438110266812e-05\n",
            "step: 530, loss: 0.0001826496300054714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9422806194274989, f1=0.9348946135831382, best_f1=0.9315714959886738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.0074876374565065e-05\n",
            "step: 10, loss: 1.984750088013243e-05\n",
            "step: 20, loss: 2.8052474590367638e-05\n",
            "step: 30, loss: 3.5085286071989685e-05\n",
            "step: 40, loss: 0.07477300614118576\n",
            "step: 50, loss: 9.44501516642049e-05\n",
            "step: 60, loss: 1.2013923878839705e-05\n",
            "step: 70, loss: 0.0008461989928036928\n",
            "step: 80, loss: 2.302854227309581e-05\n",
            "step: 90, loss: 0.00022906801314093173\n",
            "step: 100, loss: 2.748381120909471e-05\n",
            "step: 110, loss: 0.001699271728284657\n",
            "step: 120, loss: 2.3977845557965338e-05\n",
            "step: 130, loss: 0.0838041752576828\n",
            "step: 140, loss: 2.554076854721643e-05\n",
            "step: 150, loss: 2.732291613938287e-05\n",
            "step: 160, loss: 2.7038815460400656e-05\n",
            "step: 170, loss: 1.2747771506838035e-05\n",
            "step: 180, loss: 1.0173699592996854e-05\n",
            "step: 190, loss: 2.1076853954582475e-05\n",
            "step: 200, loss: 1.1306139640510082e-05\n",
            "step: 210, loss: 0.000439910392742604\n",
            "step: 220, loss: 5.563720333157107e-05\n",
            "step: 230, loss: 1.6264288206002675e-05\n",
            "step: 240, loss: 0.0008142040460370481\n",
            "step: 250, loss: 3.0753704777453095e-05\n",
            "step: 260, loss: 1.7225405827048235e-05\n",
            "step: 270, loss: 1.563844853080809e-05\n",
            "step: 280, loss: 3.847094558295794e-05\n",
            "step: 290, loss: 1.1198111678822897e-05\n",
            "step: 300, loss: 4.193146014586091e-05\n",
            "step: 310, loss: 0.00018113618716597557\n",
            "step: 320, loss: 1.615987093828153e-05\n",
            "step: 330, loss: 7.738572458038107e-05\n",
            "step: 340, loss: 1.5388848623842932e-05\n",
            "step: 350, loss: 1.2181457350379787e-05\n",
            "step: 360, loss: 0.00209937640465796\n",
            "step: 370, loss: 9.611186214897316e-06\n",
            "step: 380, loss: 4.418215758050792e-05\n",
            "step: 390, loss: 4.372711919131689e-05\n",
            "step: 400, loss: 2.9148712201276794e-05\n",
            "step: 410, loss: 6.757755909347907e-05\n",
            "step: 420, loss: 0.00018785153224598616\n",
            "step: 430, loss: 8.081327541731298e-05\n",
            "step: 440, loss: 2.7405023502069525e-05\n",
            "step: 450, loss: 1.4301104783953633e-05\n",
            "step: 460, loss: 4.416811134433374e-05\n",
            "step: 470, loss: 0.002837070031091571\n",
            "step: 480, loss: 1.0326406481908634e-05\n",
            "step: 490, loss: 1.8748667571344413e-05\n",
            "step: 500, loss: 2.5819976144703105e-05\n",
            "step: 510, loss: 2.156494156224653e-05\n",
            "step: 520, loss: 2.0268289517844096e-05\n",
            "step: 530, loss: 0.0006601383211091161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.942008486562942, f1=0.9337094499294781, best_f1=0.9315714959886738\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 249.09it/s]\n",
            "load_f1 = 0.9388136384866884\n",
            "real_f1 = 0.9372384937238494\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6522c5b8-ad34-4170-acd2-ec6fa8be5063"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5410853028297424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4, f1=0.31034482758620696, best_f1=0.31034482758620696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5241936445236206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.4, f1=0.36666666666666664, best_f1=0.31034482758620696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5383229851722717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5806451612903226, f1=0.358974358974359, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20367753505706787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6666666666666666, f1=0.3783783783783784, best_f1=0.3783783783783784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10856222361326218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7741935483870968, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11065379530191422\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7999999999999999, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04349147900938988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8750000000000001, f1=0.6666666666666667, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008650084026157856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8484848484848484, f1=0.6285714285714286, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020829488057643175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8484848484848484, f1=0.6470588235294117, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004934072960168123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8484848484848484, f1=0.6470588235294117, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00290399300865829\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8484848484848484, f1=0.6666666666666667, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005702310707420111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8484848484848484, f1=0.6470588235294117, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002905486850067973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8484848484848484, f1=0.6666666666666667, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002363965380936861\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8484848484848484, f1=0.6666666666666667, best_f1=0.6666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002391693415120244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8484848484848484, f1=0.6666666666666667, best_f1=0.6666666666666667\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 136120.42it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7000000000000001\n",
            "real_f1 = 0.7000000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 247.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5779530c-25b8-4b6b-8625-23ec4d36eeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6129480600357056\n",
            "step: 10, loss: 0.6164372563362122\n",
            "step: 20, loss: 0.32664555311203003\n",
            "step: 30, loss: 0.1552729457616806\n",
            "step: 40, loss: 0.16906501352787018\n",
            "step: 50, loss: 0.013185825198888779\n",
            "step: 60, loss: 0.05338568612933159\n",
            "step: 70, loss: 0.0037449682131409645\n",
            "step: 80, loss: 0.15201285481452942\n",
            "step: 90, loss: 0.1732524335384369\n",
            "step: 100, loss: 0.03540836647152901\n",
            "step: 110, loss: 0.10136370360851288\n",
            "step: 120, loss: 0.006931545212864876\n",
            "step: 130, loss: 0.04817580804228783\n",
            "step: 140, loss: 0.001923798001371324\n",
            "step: 150, loss: 0.0271937046200037\n",
            "step: 160, loss: 0.029683446511626244\n",
            "step: 170, loss: 0.05306230112910271\n",
            "step: 180, loss: 0.006574323400855064\n",
            "step: 190, loss: 0.010956313461065292\n",
            "step: 200, loss: 0.007669045124202967\n",
            "step: 210, loss: 0.008926581591367722\n",
            "step: 220, loss: 0.005398360546678305\n",
            "step: 230, loss: 0.06152060627937317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9785310734463276, f1=0.9761092150170648, best_f1=0.9761092150170648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011685476638376713\n",
            "step: 10, loss: 0.004369777627289295\n",
            "step: 20, loss: 0.16219699382781982\n",
            "step: 30, loss: 0.08487935364246368\n",
            "step: 40, loss: 0.14753752946853638\n",
            "step: 50, loss: 0.029245689511299133\n",
            "step: 60, loss: 0.010968860238790512\n",
            "step: 70, loss: 0.11326649785041809\n",
            "step: 80, loss: 0.007630335167050362\n",
            "step: 90, loss: 0.03313593566417694\n",
            "step: 100, loss: 0.00923028215765953\n",
            "step: 110, loss: 0.004168281797319651\n",
            "step: 120, loss: 0.0014152684016153216\n",
            "step: 130, loss: 0.002661618171259761\n",
            "step: 140, loss: 0.0016179883386939764\n",
            "step: 150, loss: 0.0019510422134771943\n",
            "step: 160, loss: 0.0034490632824599743\n",
            "step: 170, loss: 0.006373555865138769\n",
            "step: 180, loss: 0.005702564027160406\n",
            "step: 190, loss: 0.0022169414442032576\n",
            "step: 200, loss: 0.0016152202151715755\n",
            "step: 210, loss: 0.0006639313651248813\n",
            "step: 220, loss: 0.16181422770023346\n",
            "step: 230, loss: 0.0077910274267196655\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9820627802690582, f1=0.9876265466816648, best_f1=0.9876265466816648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028785178437829018\n",
            "step: 10, loss: 0.10924531519412994\n",
            "step: 20, loss: 0.0061741722747683525\n",
            "step: 30, loss: 0.0029270555824041367\n",
            "step: 40, loss: 0.03736487030982971\n",
            "step: 50, loss: 0.0032236487604677677\n",
            "step: 60, loss: 0.043346308171749115\n",
            "step: 70, loss: 0.015424869023263454\n",
            "step: 80, loss: 0.0006029540672898293\n",
            "step: 90, loss: 0.11173004657030106\n",
            "step: 100, loss: 0.0005394406034611166\n",
            "step: 110, loss: 0.0004622038686648011\n",
            "step: 120, loss: 0.017711784690618515\n",
            "step: 130, loss: 0.0032103881239891052\n",
            "step: 140, loss: 0.0017654423136264086\n",
            "step: 150, loss: 0.05687715485692024\n",
            "step: 160, loss: 0.01746934838593006\n",
            "step: 170, loss: 0.009114296175539494\n",
            "step: 180, loss: 0.03579580783843994\n",
            "step: 190, loss: 0.005258465651422739\n",
            "step: 200, loss: 0.01607467792928219\n",
            "step: 210, loss: 0.0007479360792785883\n",
            "step: 220, loss: 0.0002392137685092166\n",
            "step: 230, loss: 0.00016578781651332974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9852104664391355, f1=0.9828962371721778, best_f1=0.9828962371721778\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013098052295390517\n",
            "step: 10, loss: 0.00023024261463433504\n",
            "step: 20, loss: 0.0027486865874379873\n",
            "step: 30, loss: 0.0002581513545010239\n",
            "step: 40, loss: 0.010658010840415955\n",
            "step: 50, loss: 0.0002901765110436827\n",
            "step: 60, loss: 0.008167145773768425\n",
            "step: 70, loss: 0.004804644733667374\n",
            "step: 80, loss: 0.0004742276796605438\n",
            "step: 90, loss: 0.004083942621946335\n",
            "step: 100, loss: 0.0170816108584404\n",
            "step: 110, loss: 0.07844898104667664\n",
            "step: 120, loss: 0.04641619324684143\n",
            "step: 130, loss: 0.03801514953374863\n",
            "step: 140, loss: 0.001162277883850038\n",
            "step: 150, loss: 0.04164068400859833\n",
            "step: 160, loss: 0.009668344631791115\n",
            "step: 170, loss: 0.23692940175533295\n",
            "step: 180, loss: 0.0009125762735493481\n",
            "step: 190, loss: 0.007247967645525932\n",
            "step: 200, loss: 0.0019906251691281796\n",
            "step: 210, loss: 0.056968025863170624\n",
            "step: 220, loss: 0.008193924091756344\n",
            "step: 230, loss: 0.006771039683371782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9887387387387387, f1=0.9887387387387387, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006741509307175875\n",
            "step: 10, loss: 0.0009391444618813694\n",
            "step: 20, loss: 0.021723467856645584\n",
            "step: 30, loss: 0.00017584225861355662\n",
            "step: 40, loss: 0.0002421043172944337\n",
            "step: 50, loss: 0.03087044321000576\n",
            "step: 60, loss: 0.0022133439779281616\n",
            "step: 70, loss: 0.0001665465533733368\n",
            "step: 80, loss: 0.000329523638356477\n",
            "step: 90, loss: 0.00260086334310472\n",
            "step: 100, loss: 0.00019178312504664063\n",
            "step: 110, loss: 0.00017763876530807465\n",
            "step: 120, loss: 6.526208744617179e-05\n",
            "step: 130, loss: 0.002609874587506056\n",
            "step: 140, loss: 0.005750559736043215\n",
            "step: 150, loss: 0.0017541333800181746\n",
            "step: 160, loss: 0.06245366856455803\n",
            "step: 170, loss: 0.009733202867209911\n",
            "step: 180, loss: 0.07123371958732605\n",
            "step: 190, loss: 0.002595864934846759\n",
            "step: 200, loss: 0.002962416736409068\n",
            "step: 210, loss: 0.00022136291954666376\n",
            "step: 220, loss: 0.0009557349840179086\n",
            "step: 230, loss: 0.00021773016487713903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9854748603351955, f1=0.974585635359116, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002558814885560423\n",
            "step: 10, loss: 0.0002920545230153948\n",
            "step: 20, loss: 0.0003510198148433119\n",
            "step: 30, loss: 0.00105836964212358\n",
            "step: 40, loss: 0.00024107932404149324\n",
            "step: 50, loss: 0.001251155394129455\n",
            "step: 60, loss: 0.00019963351951446384\n",
            "step: 70, loss: 0.000378980184905231\n",
            "step: 80, loss: 0.0021712209563702345\n",
            "step: 90, loss: 0.00022196049394551665\n",
            "step: 100, loss: 0.03935758024454117\n",
            "step: 110, loss: 0.0009394052322022617\n",
            "step: 120, loss: 0.0006682639941573143\n",
            "step: 130, loss: 0.00049036112613976\n",
            "step: 140, loss: 0.00019835818966384977\n",
            "step: 150, loss: 0.07656747847795486\n",
            "step: 160, loss: 0.004094243515282869\n",
            "step: 170, loss: 0.0020172367803752422\n",
            "step: 180, loss: 0.06355635821819305\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.004403059370815754\n",
            "step: 200, loss: 0.007131568621844053\n",
            "step: 210, loss: 0.001589099527336657\n",
            "step: 220, loss: 0.0002658675657585263\n",
            "step: 230, loss: 0.07553112506866455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9852774631936579, f1=0.9817351598173515, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00248400354757905\n",
            "step: 10, loss: 0.00020578732073772699\n",
            "step: 20, loss: 0.00035608879989013076\n",
            "step: 30, loss: 0.002726082457229495\n",
            "step: 40, loss: 0.0001729861251078546\n",
            "step: 50, loss: 0.00013879344624001533\n",
            "step: 60, loss: 0.0025746419560164213\n",
            "step: 70, loss: 0.017724452540278435\n",
            "step: 80, loss: 0.00014831338194198906\n",
            "step: 90, loss: 0.00012509821681305766\n",
            "step: 100, loss: 0.00029700473533011973\n",
            "step: 110, loss: 0.0006341560510918498\n",
            "step: 120, loss: 0.00016298222180921584\n",
            "step: 130, loss: 0.00013615515490528196\n",
            "step: 140, loss: 8.69927680469118e-05\n",
            "step: 150, loss: 0.0037736683152616024\n",
            "step: 160, loss: 0.0810680240392685\n",
            "step: 170, loss: 0.0030180613975971937\n",
            "step: 180, loss: 0.0005994331440888345\n",
            "step: 190, loss: 8.51019358378835e-05\n",
            "step: 200, loss: 0.06669601798057556\n",
            "step: 210, loss: 7.620656833751127e-05\n",
            "step: 220, loss: 0.0042756604962050915\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 230, loss: 9.83697609626688e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9864864864864865, f1=0.978865406006674, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.666708941338584e-05\n",
            "step: 10, loss: 0.0009034445392899215\n",
            "step: 20, loss: 0.0006242335075512528\n",
            "step: 30, loss: 0.0005041034892201424\n",
            "step: 40, loss: 0.0005693771527148783\n",
            "step: 50, loss: 0.0018503741594031453\n",
            "step: 60, loss: 0.0007822254556231201\n",
            "step: 70, loss: 0.001574943889863789\n",
            "step: 80, loss: 0.00017015752382576466\n",
            "step: 90, loss: 7.517704216297716e-05\n",
            "step: 100, loss: 0.006251271348446608\n",
            "step: 110, loss: 0.0011827743146568537\n",
            "step: 120, loss: 0.0012094549601897597\n",
            "step: 130, loss: 0.0007198106031864882\n",
            "step: 140, loss: 8.196241105906665e-05\n",
            "step: 150, loss: 0.004002324305474758\n",
            "step: 160, loss: 0.0007407280500046909\n",
            "step: 170, loss: 0.0001287563209189102\n",
            "step: 180, loss: 0.0001340379094472155\n",
            "step: 190, loss: 0.0023853082675486803\n",
            "step: 200, loss: 0.00013591739116236567\n",
            "step: 210, loss: 6.572485290234908e-05\n",
            "step: 220, loss: 0.010236102156341076\n",
            "step: 230, loss: 5.3678388212574646e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9864864864864865, f1=0.9842342342342343, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.275326475384645e-05\n",
            "step: 10, loss: 0.00036578814615495503\n",
            "step: 20, loss: 0.012786281295120716\n",
            "step: 30, loss: 0.0014605532633140683\n",
            "step: 40, loss: 0.0011125912424176931\n",
            "step: 50, loss: 6.569232937181368e-05\n",
            "step: 60, loss: 0.00010711626964621246\n",
            "step: 70, loss: 0.0004340771120041609\n",
            "step: 80, loss: 6.337196100503206e-05\n",
            "step: 90, loss: 0.00014434823242481798\n",
            "step: 100, loss: 0.00023327101371251047\n",
            "step: 110, loss: 4.573373371385969e-05\n",
            "step: 120, loss: 3.5020188079215586e-05\n",
            "step: 130, loss: 6.671228038612753e-05\n",
            "step: 140, loss: 3.4211643651360646e-05\n",
            "step: 150, loss: 8.197705028578639e-05\n",
            "step: 160, loss: 3.124322756775655e-05\n",
            "step: 170, loss: 5.8694738982012495e-05\n",
            "step: 180, loss: 0.00038281604065559804\n",
            "step: 190, loss: 0.0002983442391268909\n",
            "step: 200, loss: 0.0006132787675596774\n",
            "step: 210, loss: 6.699893128825352e-05\n",
            "step: 220, loss: 4.4742941099684685e-05\n",
            "step: 230, loss: 0.00036967245978303254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9842696629213483, f1=0.9819413092550789, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012033570965286344\n",
            "step: 10, loss: 0.00014375070168171078\n",
            "step: 20, loss: 5.0422546337358654e-05\n",
            "step: 30, loss: 0.0006393414223566651\n",
            "step: 40, loss: 0.0007961756782606244\n",
            "step: 50, loss: 0.00012939225416630507\n",
            "step: 60, loss: 0.00028480065520852804\n",
            "step: 70, loss: 0.001225327025167644\n",
            "step: 80, loss: 0.00015274618635885417\n",
            "step: 90, loss: 0.0003698074142448604\n",
            "step: 100, loss: 0.00011343155347276479\n",
            "step: 110, loss: 0.001887200283817947\n",
            "step: 120, loss: 0.0017830142751336098\n",
            "step: 130, loss: 0.00013747303455602378\n",
            "step: 140, loss: 0.0360754057765007\n",
            "step: 150, loss: 0.015893181785941124\n",
            "step: 160, loss: 0.00012472733214963228\n",
            "step: 170, loss: 0.0003355394583195448\n",
            "step: 180, loss: 0.00036963974707759917\n",
            "step: 190, loss: 0.01551787555217743\n",
            "step: 200, loss: 8.925216388888657e-05\n",
            "step: 210, loss: 0.00012642827641684562\n",
            "step: 220, loss: 4.116998752579093e-05\n",
            "step: 230, loss: 0.0009538746089674532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9865771812080537, f1=0.9821826280623607, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002950991620309651\n",
            "step: 10, loss: 0.00013800253509543836\n",
            "step: 20, loss: 0.00017268139345105737\n",
            "step: 30, loss: 0.012313229031860828\n",
            "step: 40, loss: 0.00017711146210785955\n",
            "step: 50, loss: 4.444513251655735e-05\n",
            "step: 60, loss: 0.0003000679425895214\n",
            "step: 70, loss: 0.0001643535797484219\n",
            "step: 80, loss: 4.8284080548910424e-05\n",
            "step: 90, loss: 4.077875200891867e-05\n",
            "step: 100, loss: 0.0002599218569230288\n",
            "step: 110, loss: 0.00047791359247639775\n",
            "step: 120, loss: 0.0004761606687679887\n",
            "step: 130, loss: 2.9611239369842224e-05\n",
            "step: 140, loss: 5.9244874137220904e-05\n",
            "step: 150, loss: 0.02222704328596592\n",
            "step: 160, loss: 4.288787749828771e-05\n",
            "step: 170, loss: 0.026512155309319496\n",
            "step: 180, loss: 4.15050562878605e-05\n",
            "step: 190, loss: 6.281135574681684e-05\n",
            "step: 200, loss: 0.00042523175943642855\n",
            "step: 210, loss: 3.8301652239169925e-05\n",
            "step: 220, loss: 0.00019078679906670004\n",
            "step: 230, loss: 8.032062760321423e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9864864864864865, f1=0.9853438556933484, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.440574437263422e-05\n",
            "step: 10, loss: 4.696740143117495e-05\n",
            "step: 20, loss: 5.6666365708224475e-05\n",
            "step: 30, loss: 6.914570258231834e-05\n",
            "step: 40, loss: 0.0008729834808036685\n",
            "step: 50, loss: 0.0001640952395973727\n",
            "step: 60, loss: 9.410137863596901e-05\n",
            "step: 70, loss: 9.393430082127452e-05\n",
            "step: 80, loss: 0.0015896469121798873\n",
            "step: 90, loss: 4.59654038422741e-05\n",
            "step: 100, loss: 6.14917662460357e-05\n",
            "step: 110, loss: 4.883571091340855e-05\n",
            "step: 120, loss: 3.301996912341565e-05\n",
            "step: 130, loss: 2.731331369432155e-05\n",
            "step: 140, loss: 3.1283998396247625e-05\n",
            "step: 150, loss: 0.0009082052856683731\n",
            "step: 160, loss: 2.861332359316293e-05\n",
            "step: 170, loss: 2.595732075860724e-05\n",
            "step: 180, loss: 0.0349557101726532\n",
            "step: 190, loss: 0.000962104182690382\n",
            "step: 200, loss: 2.1975160052534193e-05\n",
            "step: 210, loss: 8.297274325741455e-05\n",
            "step: 220, loss: 0.0011812495067715645\n",
            "step: 230, loss: 0.03092704340815544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9853438556933484, f1=0.9841628959276018, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045971147483214736\n",
            "step: 10, loss: 0.0001880475174402818\n",
            "step: 20, loss: 0.0005617537535727024\n",
            "step: 30, loss: 0.0001562174438731745\n",
            "step: 40, loss: 9.412295185029507e-05\n",
            "step: 50, loss: 4.8878118832362816e-05\n",
            "step: 60, loss: 4.3508527596713975e-05\n",
            "step: 70, loss: 8.446219726465642e-05\n",
            "step: 80, loss: 0.00015663560770917684\n",
            "step: 90, loss: 8.578514825785533e-05\n",
            "step: 100, loss: 2.3722312107565813e-05\n",
            "step: 110, loss: 0.0008140738354995847\n",
            "step: 120, loss: 0.04070204868912697\n",
            "step: 130, loss: 8.269163663499057e-05\n",
            "step: 140, loss: 2.6724706913228147e-05\n",
            "step: 150, loss: 2.6232975869788788e-05\n",
            "step: 160, loss: 0.00015330020687542856\n",
            "step: 170, loss: 0.0003022726741619408\n",
            "step: 180, loss: 3.5503926483215764e-05\n",
            "step: 190, loss: 3.630357241490856e-05\n",
            "step: 200, loss: 8.455938950646669e-05\n",
            "step: 210, loss: 5.461333057610318e-05\n",
            "step: 220, loss: 4.2892421333817765e-05\n",
            "step: 230, loss: 5.6695400417083874e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9854748603351955, f1=0.9810901001112348, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.322686486877501e-05\n",
            "step: 10, loss: 0.013061783276498318\n",
            "step: 20, loss: 8.429739682469517e-05\n",
            "step: 30, loss: 0.0001018457769532688\n",
            "step: 40, loss: 0.01513371430337429\n",
            "step: 50, loss: 4.0468767110724e-05\n",
            "step: 60, loss: 4.293223537388258e-05\n",
            "step: 70, loss: 2.7998461519018747e-05\n",
            "step: 80, loss: 2.3424265236826614e-05\n",
            "step: 90, loss: 2.5655526769696735e-05\n",
            "step: 100, loss: 5.488943861564621e-05\n",
            "step: 110, loss: 0.0001547017745906487\n",
            "step: 120, loss: 1.725537185848225e-05\n",
            "step: 130, loss: 2.617310019559227e-05\n",
            "step: 140, loss: 3.881918382830918e-05\n",
            "step: 150, loss: 3.733609264600091e-05\n",
            "step: 160, loss: 7.958233618410304e-05\n",
            "step: 170, loss: 2.1233858205960132e-05\n",
            "step: 180, loss: 7.145912240957841e-05\n",
            "step: 190, loss: 3.38979261869099e-05\n",
            "step: 200, loss: 2.7387093723518774e-05\n",
            "step: 210, loss: 3.3112879464169964e-05\n",
            "step: 220, loss: 3.67493303201627e-05\n",
            "step: 230, loss: 3.390247002243996e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9854748603351955, f1=0.9821826280623607, best_f1=0.9887387387387387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022572913439944386\n",
            "step: 10, loss: 1.872681787062902e-05\n",
            "step: 20, loss: 3.260559242335148e-05\n",
            "step: 30, loss: 6.913207471370697e-05\n",
            "step: 40, loss: 4.2857682274188846e-05\n",
            "step: 50, loss: 0.03649698942899704\n",
            "step: 60, loss: 3.4508895623730496e-05\n",
            "step: 70, loss: 2.605038389447145e-05\n",
            "step: 80, loss: 0.0002544522285461426\n",
            "step: 90, loss: 4.6257020585471764e-05\n",
            "step: 100, loss: 4.373519914224744e-05\n",
            "step: 110, loss: 2.5510287741781212e-05\n",
            "step: 120, loss: 0.00019948650151491165\n",
            "step: 130, loss: 0.00015087032807059586\n",
            "step: 140, loss: 2.123388730979059e-05\n",
            "step: 150, loss: 5.553360460908152e-05\n",
            "step: 160, loss: 0.0015202779322862625\n",
            "step: 170, loss: 1.9862975022988394e-05\n",
            "step: 180, loss: 0.0004251594073139131\n",
            "step: 190, loss: 0.00020575380767695606\n",
            "step: 200, loss: 0.0002528706390876323\n",
            "step: 210, loss: 5.081156632513739e-05\n",
            "step: 220, loss: 0.11070162057876587\n",
            "step: 230, loss: 2.219121961388737e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9853438556933484, f1=0.9864559819413092, best_f1=0.9887387387387387\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 216.74it/s]\n",
            "load_f1 = 0.9887387387387387\n",
            "real_f1 = 0.9887387387387387\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 270.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9800cdba-261e-4b10-a316-9ff858fde0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.610503613948822\n",
            "step: 10, loss: 0.5133370161056519\n",
            "step: 20, loss: 0.5191926956176758\n",
            "step: 30, loss: 0.15274904668331146\n",
            "step: 40, loss: 0.17225342988967896\n",
            "step: 50, loss: 0.1078958734869957\n",
            "step: 60, loss: 0.04493679478764534\n",
            "step: 70, loss: 0.11217907816171646\n",
            "step: 80, loss: 0.0410870797932148\n",
            "step: 90, loss: 0.1810123175382614\n",
            "step: 100, loss: 0.06142173707485199\n",
            "step: 110, loss: 0.10013003647327423\n",
            "step: 120, loss: 0.160480335354805\n",
            "step: 130, loss: 0.11524185538291931\n",
            "step: 140, loss: 0.09780501574277878\n",
            "step: 150, loss: 0.057915911078453064\n",
            "step: 160, loss: 0.013442346826195717\n",
            "step: 170, loss: 0.1795872300863266\n",
            "step: 180, loss: 0.10308228433132172\n",
            "step: 190, loss: 0.018255198374390602\n",
            "step: 200, loss: 0.15011148154735565\n",
            "step: 210, loss: 0.08137350529432297\n",
            "step: 220, loss: 0.25193122029304504\n",
            "step: 230, loss: 0.17151978611946106\n",
            "step: 240, loss: 0.09350082278251648\n",
            "step: 250, loss: 0.052684981375932693\n",
            "step: 260, loss: 0.12932826578617096\n",
            "step: 270, loss: 0.008883548900485039\n",
            "step: 280, loss: 0.0187818706035614\n",
            "step: 290, loss: 0.12822283804416656\n",
            "step: 300, loss: 0.06919820606708527\n",
            "step: 310, loss: 0.10264074057340622\n",
            "step: 320, loss: 0.10067884624004364\n",
            "step: 330, loss: 0.040871791541576385\n",
            "step: 340, loss: 0.06658387184143066\n",
            "step: 350, loss: 0.01963658072054386\n",
            "step: 360, loss: 0.19140593707561493\n",
            "step: 370, loss: 0.09772227704524994\n",
            "step: 380, loss: 0.015114658512175083\n",
            "step: 390, loss: 0.05633283033967018\n",
            "step: 400, loss: 0.42842739820480347\n",
            "step: 410, loss: 0.12962950766086578\n",
            "step: 420, loss: 0.06588674336671829\n",
            "step: 430, loss: 0.1675780862569809\n",
            "step: 440, loss: 0.016874121502041817\n",
            "step: 450, loss: 0.02654932253062725\n",
            "step: 460, loss: 0.006655648350715637\n",
            "step: 470, loss: 0.2128068506717682\n",
            "step: 480, loss: 0.03238687664270401\n",
            "step: 490, loss: 0.038842372596263885\n",
            "step: 500, loss: 0.11561396718025208\n",
            "step: 510, loss: 0.09984346479177475\n",
            "step: 520, loss: 0.013371351175010204\n",
            "step: 530, loss: 0.0033403998240828514\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.923581809657759, f1=0.9255121042830541, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30005723237991333\n",
            "step: 10, loss: 0.13935601711273193\n",
            "step: 20, loss: 0.007638311013579369\n",
            "step: 30, loss: 0.057275883853435516\n",
            "step: 40, loss: 0.048838842660188675\n",
            "step: 50, loss: 0.09497016668319702\n",
            "step: 60, loss: 0.007511339150369167\n",
            "step: 70, loss: 0.054847799241542816\n",
            "step: 80, loss: 0.08249576389789581\n",
            "step: 90, loss: 0.010583187453448772\n",
            "step: 100, loss: 0.08622851967811584\n",
            "step: 110, loss: 0.0399857759475708\n",
            "step: 120, loss: 0.02181442826986313\n",
            "step: 130, loss: 0.1139468178153038\n",
            "step: 140, loss: 0.02161935158073902\n",
            "step: 150, loss: 0.13438571989536285\n",
            "step: 160, loss: 0.03289637714624405\n",
            "step: 170, loss: 0.06258971989154816\n",
            "step: 180, loss: 0.02812214009463787\n",
            "step: 190, loss: 0.0901850163936615\n",
            "step: 200, loss: 0.0057438816875219345\n",
            "step: 210, loss: 0.07711014896631241\n",
            "step: 220, loss: 0.058500830084085464\n",
            "step: 230, loss: 0.005646541249006987\n",
            "step: 240, loss: 0.07752317190170288\n",
            "step: 250, loss: 0.08817797899246216\n",
            "step: 260, loss: 0.001997928600758314\n",
            "step: 270, loss: 0.2567446529865265\n",
            "step: 280, loss: 0.030375748872756958\n",
            "step: 290, loss: 0.06761899590492249\n",
            "step: 300, loss: 0.11467025429010391\n",
            "step: 310, loss: 0.06977517157793045\n",
            "step: 320, loss: 0.0645204707980156\n",
            "step: 330, loss: 0.016385575756430626\n",
            "step: 340, loss: 0.0193563774228096\n",
            "step: 350, loss: 0.0008464207639917731\n",
            "step: 360, loss: 0.09543373435735703\n",
            "step: 370, loss: 0.09196992963552475\n",
            "step: 380, loss: 0.015363642014563084\n",
            "step: 390, loss: 0.0945584699511528\n",
            "step: 400, loss: 0.12588335573673248\n",
            "step: 410, loss: 0.006919359788298607\n",
            "step: 420, loss: 0.01303092297166586\n",
            "step: 430, loss: 0.03831516206264496\n",
            "step: 440, loss: 0.0880156010389328\n",
            "step: 450, loss: 0.0270888302475214\n",
            "step: 460, loss: 0.09803749620914459\n",
            "step: 470, loss: 0.01323692873120308\n",
            "step: 480, loss: 0.23968788981437683\n",
            "step: 490, loss: 0.02316877245903015\n",
            "step: 500, loss: 0.14338062703609467\n",
            "step: 510, loss: 0.0175817608833313\n",
            "step: 520, loss: 0.04050430655479431\n",
            "step: 530, loss: 0.010715288110077381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9277496548550391, f1=0.9307201458523247, best_f1=0.9307201458523247\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09926071763038635\n",
            "step: 10, loss: 0.09558805823326111\n",
            "step: 20, loss: 0.05913970246911049\n",
            "step: 30, loss: 0.04011042043566704\n",
            "step: 40, loss: 0.009866690263152122\n",
            "step: 50, loss: 0.15280908346176147\n",
            "step: 60, loss: 0.027076341211795807\n",
            "step: 70, loss: 0.020446274429559708\n",
            "step: 80, loss: 0.01637258194386959\n",
            "step: 90, loss: 0.006691658869385719\n",
            "step: 100, loss: 0.07535185664892197\n",
            "step: 110, loss: 0.0017318963073194027\n",
            "step: 120, loss: 0.0027136618737131357\n",
            "step: 130, loss: 0.0020642238669097424\n",
            "step: 140, loss: 0.013946897350251675\n",
            "step: 150, loss: 0.027158623561263084\n",
            "step: 160, loss: 0.010423790663480759\n",
            "step: 170, loss: 0.031957805156707764\n",
            "step: 180, loss: 0.005401573143899441\n",
            "step: 190, loss: 0.014441071078181267\n",
            "step: 200, loss: 0.013609698042273521\n",
            "step: 210, loss: 0.06970623135566711\n",
            "step: 220, loss: 0.14406508207321167\n",
            "step: 230, loss: 0.06476673483848572\n",
            "step: 240, loss: 0.008252859115600586\n",
            "step: 250, loss: 0.04121007025241852\n",
            "step: 260, loss: 0.02120634913444519\n",
            "step: 270, loss: 0.010002708993852139\n",
            "step: 280, loss: 0.11615675687789917\n",
            "step: 290, loss: 0.003306121099740267\n",
            "step: 300, loss: 0.08944425731897354\n",
            "step: 310, loss: 0.01642690598964691\n",
            "step: 320, loss: 0.009373415261507034\n",
            "step: 330, loss: 0.0016840090975165367\n",
            "step: 340, loss: 0.0016109340358525515\n",
            "step: 350, loss: 0.001034233719110489\n",
            "step: 360, loss: 0.028206568211317062\n",
            "step: 370, loss: 0.003963986411690712\n",
            "step: 380, loss: 0.009544451721012592\n",
            "step: 390, loss: 0.23494987189769745\n",
            "step: 400, loss: 0.008544769138097763\n",
            "step: 410, loss: 0.012250839732587337\n",
            "step: 420, loss: 0.06709593534469604\n",
            "step: 430, loss: 0.02223716862499714\n",
            "step: 440, loss: 0.0019543995149433613\n",
            "step: 450, loss: 0.06280330568552017\n",
            "step: 460, loss: 0.050808388739824295\n",
            "step: 470, loss: 0.12918104231357574\n",
            "step: 480, loss: 0.0015998982125893235\n",
            "step: 490, loss: 0.015601628459990025\n",
            "step: 500, loss: 0.007358166854828596\n",
            "step: 510, loss: 0.014703530818223953\n",
            "step: 520, loss: 0.0787973552942276\n",
            "step: 530, loss: 0.09008261561393738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.936500685244404, f1=0.9304666056724611, best_f1=0.9304666056724611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045320719480514526\n",
            "step: 10, loss: 0.003132674377411604\n",
            "step: 20, loss: 0.00586816156283021\n",
            "step: 30, loss: 0.0032469211146235466\n",
            "step: 40, loss: 0.015868308022618294\n",
            "step: 50, loss: 0.001556600327603519\n",
            "step: 60, loss: 0.003574649803340435\n",
            "step: 70, loss: 0.0009568470995873213\n",
            "step: 80, loss: 0.06594090908765793\n",
            "step: 90, loss: 0.025680450722575188\n",
            "step: 100, loss: 0.004249949473887682\n",
            "step: 110, loss: 0.05205415561795235\n",
            "step: 120, loss: 0.0004427181265782565\n",
            "step: 130, loss: 0.04095574468374252\n",
            "step: 140, loss: 0.0014880166854709387\n",
            "step: 150, loss: 0.002998407930135727\n",
            "step: 160, loss: 0.01143688429147005\n",
            "step: 170, loss: 0.011313251219689846\n",
            "step: 180, loss: 0.002677084179595113\n",
            "step: 190, loss: 0.04108526185154915\n",
            "step: 200, loss: 0.03802523389458656\n",
            "step: 210, loss: 0.04740007594227791\n",
            "step: 220, loss: 0.010440180078148842\n",
            "step: 230, loss: 0.030254045501351357\n",
            "step: 240, loss: 0.0025966926477849483\n",
            "step: 250, loss: 0.06651268154382706\n",
            "step: 260, loss: 0.0025235742796212435\n",
            "step: 270, loss: 0.01655648835003376\n",
            "step: 280, loss: 0.07172371447086334\n",
            "step: 290, loss: 0.036969415843486786\n",
            "step: 300, loss: 0.00045692690764553845\n",
            "step: 310, loss: 0.00532285962253809\n",
            "step: 320, loss: 0.004750229883939028\n",
            "step: 330, loss: 0.006767400074750185\n",
            "step: 340, loss: 0.26776331663131714\n",
            "step: 350, loss: 0.1300056427717209\n",
            "step: 360, loss: 0.02561924420297146\n",
            "step: 370, loss: 0.023902643471956253\n",
            "step: 380, loss: 0.003033742308616638\n",
            "step: 390, loss: 0.002269561868160963\n",
            "step: 400, loss: 0.0023461899254471064\n",
            "step: 410, loss: 0.006270963232964277\n",
            "step: 420, loss: 0.00515891844406724\n",
            "step: 430, loss: 0.14563961327075958\n",
            "step: 440, loss: 0.0032901461236178875\n",
            "step: 450, loss: 0.10705357044935226\n",
            "step: 460, loss: 0.011485881172120571\n",
            "step: 470, loss: 0.021233266219496727\n",
            "step: 480, loss: 0.14888519048690796\n",
            "step: 490, loss: 0.002368211979046464\n",
            "step: 500, loss: 0.011604542843997478\n",
            "step: 510, loss: 0.031051374971866608\n",
            "step: 520, loss: 0.08948426693677902\n",
            "step: 530, loss: 0.031555209308862686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.925536774783006, f1=0.9290617848970252, best_f1=0.9304666056724611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1192721351981163\n",
            "step: 10, loss: 0.027895662933588028\n",
            "step: 20, loss: 0.0020004662219434977\n",
            "step: 30, loss: 0.0075484635308384895\n",
            "step: 40, loss: 0.09350413829088211\n",
            "step: 50, loss: 0.007149518933147192\n",
            "step: 60, loss: 0.020780544728040695\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 0.0020284096244722605\n",
            "step: 80, loss: 0.09375129640102386\n",
            "step: 90, loss: 0.07456426322460175\n",
            "step: 100, loss: 0.12845484912395477\n",
            "step: 110, loss: 0.0027245976962149143\n",
            "step: 120, loss: 0.0013673766516149044\n",
            "step: 130, loss: 0.0006373588694259524\n",
            "step: 140, loss: 0.0013454948784783483\n",
            "step: 150, loss: 0.004789868835359812\n",
            "step: 160, loss: 0.001233322429470718\n",
            "step: 170, loss: 0.011790556833148003\n",
            "step: 180, loss: 0.0010827212827280164\n",
            "step: 190, loss: 0.00303015043027699\n",
            "step: 200, loss: 0.0030978675931692123\n",
            "step: 210, loss: 0.004423650447279215\n",
            "step: 220, loss: 0.0039110430516302586\n",
            "step: 230, loss: 0.004150616005063057\n",
            "step: 240, loss: 0.004779924638569355\n",
            "step: 250, loss: 0.010385622270405293\n",
            "step: 260, loss: 0.04883425310254097\n",
            "step: 270, loss: 0.005957243964076042\n",
            "step: 280, loss: 0.006114489398896694\n",
            "step: 290, loss: 0.12846258282661438\n",
            "step: 300, loss: 0.010428817942738533\n",
            "step: 310, loss: 0.012345829047262669\n",
            "step: 320, loss: 0.03673657029867172\n",
            "step: 330, loss: 0.15718936920166016\n",
            "step: 340, loss: 0.0036320341750979424\n",
            "step: 350, loss: 0.010610869154334068\n",
            "step: 360, loss: 0.008687203750014305\n",
            "step: 370, loss: 0.02220001071691513\n",
            "step: 380, loss: 0.0013369549997150898\n",
            "step: 390, loss: 0.0003535302821546793\n",
            "step: 400, loss: 0.010361971333622932\n",
            "step: 410, loss: 0.005340441595762968\n",
            "step: 420, loss: 0.002082232153043151\n",
            "step: 430, loss: 0.0021908003836870193\n",
            "step: 440, loss: 0.029834670946002007\n",
            "step: 450, loss: 0.007847714237868786\n",
            "step: 460, loss: 0.030557438731193542\n",
            "step: 470, loss: 0.00475190719589591\n",
            "step: 480, loss: 0.009318063966929913\n",
            "step: 490, loss: 0.0003755018988158554\n",
            "step: 500, loss: 0.005117273889482021\n",
            "step: 510, loss: 0.06543602049350739\n",
            "step: 520, loss: 0.008771431632339954\n",
            "step: 530, loss: 0.011462584137916565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9340560072267391, f1=0.9327313769751693, best_f1=0.9304666056724611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0041847811080515385\n",
            "step: 10, loss: 0.0005497150705195963\n",
            "step: 20, loss: 0.10806167125701904\n",
            "step: 30, loss: 0.018724050372838974\n",
            "step: 40, loss: 0.02630424313247204\n",
            "step: 50, loss: 0.022926220670342445\n",
            "step: 60, loss: 0.00011009911395376548\n",
            "step: 70, loss: 0.00041143110138364136\n",
            "step: 80, loss: 0.0022365404292941093\n",
            "step: 90, loss: 0.00033686120877973735\n",
            "step: 100, loss: 0.0016404340276494622\n",
            "step: 110, loss: 0.006618231534957886\n",
            "step: 120, loss: 0.0015861217398196459\n",
            "step: 130, loss: 0.1305711716413498\n",
            "step: 140, loss: 0.008639681152999401\n",
            "step: 150, loss: 0.0010859051253646612\n",
            "step: 160, loss: 0.025749368593096733\n",
            "step: 170, loss: 0.038736168295145035\n",
            "step: 180, loss: 0.0011861090315505862\n",
            "step: 190, loss: 0.002648895373567939\n",
            "step: 200, loss: 0.006420797202736139\n",
            "step: 210, loss: 0.001223512808792293\n",
            "step: 220, loss: 0.0016100095817819238\n",
            "step: 230, loss: 0.008886614814400673\n",
            "step: 240, loss: 0.11846088618040085\n",
            "step: 250, loss: 0.013008128851652145\n",
            "step: 260, loss: 0.0023091845214366913\n",
            "step: 270, loss: 0.10046568512916565\n",
            "step: 280, loss: 0.015068857930600643\n",
            "step: 290, loss: 0.0007719467976130545\n",
            "step: 300, loss: 0.010983791202306747\n",
            "step: 310, loss: 0.0006556739681400359\n",
            "step: 320, loss: 0.0006958473240956664\n",
            "step: 330, loss: 0.0015983961056917906\n",
            "step: 340, loss: 0.04329284280538559\n",
            "step: 350, loss: 0.0017910415772348642\n",
            "step: 360, loss: 0.03427240997552872\n",
            "step: 370, loss: 0.002020325278863311\n",
            "step: 380, loss: 0.0043379939161241055\n",
            "step: 390, loss: 0.025003371760249138\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 0.0375661700963974\n",
            "step: 410, loss: 0.03634367510676384\n",
            "step: 420, loss: 0.012891828082501888\n",
            "step: 430, loss: 0.0028566892724484205\n",
            "step: 440, loss: 0.0020300017204135656\n",
            "step: 450, loss: 0.00039626439684070647\n",
            "step: 460, loss: 0.007493241224437952\n",
            "step: 470, loss: 0.0010043180081993341\n",
            "step: 480, loss: 0.008845541626214981\n",
            "step: 490, loss: 0.009296414442360401\n",
            "step: 500, loss: 0.0029008558485656977\n",
            "step: 510, loss: 0.001974373124539852\n",
            "step: 520, loss: 0.006400167476385832\n",
            "step: 530, loss: 0.015292024239897728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9359095193213949, f1=0.9337121212121212, best_f1=0.9304666056724611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003957127337343991\n",
            "step: 10, loss: 0.01699225977063179\n",
            "step: 20, loss: 0.00044140100362710655\n",
            "step: 30, loss: 0.0007311057997867465\n",
            "step: 40, loss: 0.013781661167740822\n",
            "step: 50, loss: 0.004940458573400974\n",
            "step: 60, loss: 0.14030839502811432\n",
            "step: 70, loss: 0.0027019174303859472\n",
            "step: 80, loss: 0.004285012371838093\n",
            "step: 90, loss: 0.008354111574590206\n",
            "step: 100, loss: 0.0001610380713827908\n",
            "step: 110, loss: 0.007576220668852329\n",
            "step: 120, loss: 0.02370925433933735\n",
            "step: 130, loss: 0.004108133260160685\n",
            "step: 140, loss: 0.01956627145409584\n",
            "step: 150, loss: 0.0006248832796700299\n",
            "step: 160, loss: 0.0034840116277337074\n",
            "step: 170, loss: 0.0008582363370805979\n",
            "step: 180, loss: 0.0013437013840302825\n",
            "step: 190, loss: 0.027645187452435493\n",
            "step: 200, loss: 0.0031699896790087223\n",
            "step: 210, loss: 0.0029272078536450863\n",
            "step: 220, loss: 0.0003083152696490288\n",
            "step: 230, loss: 0.1052895113825798\n",
            "step: 240, loss: 0.0009328825399279594\n",
            "step: 250, loss: 0.003836473450064659\n",
            "step: 260, loss: 0.0035534368362277746\n",
            "step: 270, loss: 0.01220837701112032\n",
            "step: 280, loss: 0.0016179250087589025\n",
            "step: 290, loss: 0.017151040956377983\n",
            "step: 300, loss: 0.00871141254901886\n",
            "step: 310, loss: 0.00038617930840700865\n",
            "step: 320, loss: 0.012995151802897453\n",
            "step: 330, loss: 0.0008026838768273592\n",
            "step: 340, loss: 0.03994528576731682\n",
            "step: 350, loss: 0.011931153945624828\n",
            "step: 360, loss: 0.012415806762874126\n",
            "step: 370, loss: 0.0036370926536619663\n",
            "step: 380, loss: 0.002756512491032481\n",
            "step: 390, loss: 0.00020471794414334\n",
            "step: 400, loss: 0.004026662092655897\n",
            "step: 410, loss: 0.0012555239954963326\n",
            "step: 420, loss: 0.0008611969533376396\n",
            "step: 430, loss: 4.8705707740737125e-05\n",
            "step: 440, loss: 0.0001907769328681752\n",
            "step: 450, loss: 0.004034065175801516\n",
            "step: 460, loss: 0.0005537194665521383\n",
            "step: 470, loss: 0.00024370540631935\n",
            "step: 480, loss: 0.18595433235168457\n",
            "step: 490, loss: 0.0002002816036110744\n",
            "step: 500, loss: 0.008412383496761322\n",
            "step: 510, loss: 0.02913450449705124\n",
            "step: 520, loss: 0.043696265667676926\n",
            "step: 530, loss: 0.0817769467830658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9416160672582905, f1=0.9410672853828307, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006806849851273\n",
            "step: 10, loss: 0.02465950883924961\n",
            "step: 20, loss: 0.002414637478068471\n",
            "step: 30, loss: 0.02277946285903454\n",
            "step: 40, loss: 0.0031417780555784702\n",
            "step: 50, loss: 0.008416897617280483\n",
            "step: 60, loss: 0.003408523043617606\n",
            "step: 70, loss: 0.0013970505679026246\n",
            "step: 80, loss: 0.0007706787437200546\n",
            "step: 90, loss: 0.0013544528046622872\n",
            "step: 100, loss: 0.001567809609696269\n",
            "step: 110, loss: 0.005124587099999189\n",
            "step: 120, loss: 0.004316254984587431\n",
            "step: 130, loss: 6.255495827645063e-05\n",
            "step: 140, loss: 0.009484579786658287\n",
            "step: 150, loss: 0.000516602594871074\n",
            "step: 160, loss: 0.008523056283593178\n",
            "step: 170, loss: 0.0019039774779230356\n",
            "step: 180, loss: 0.00015310100570786744\n",
            "step: 190, loss: 0.0037335336674004793\n",
            "step: 200, loss: 0.010211721993982792\n",
            "step: 210, loss: 0.0034543343354016542\n",
            "step: 220, loss: 0.0009067032369785011\n",
            "step: 230, loss: 0.012852865271270275\n",
            "step: 240, loss: 0.0001477680925745517\n",
            "step: 250, loss: 0.00016019956092350185\n",
            "step: 260, loss: 0.00020893197506666183\n",
            "step: 270, loss: 0.01195079181343317\n",
            "step: 280, loss: 0.05105523392558098\n",
            "step: 290, loss: 0.049139413982629776\n",
            "step: 300, loss: 0.0012899310095235705\n",
            "step: 310, loss: 0.004795127548277378\n",
            "step: 320, loss: 0.008787584491074085\n",
            "step: 330, loss: 9.627056715544313e-05\n",
            "step: 340, loss: 0.0001547357824165374\n",
            "step: 350, loss: 0.0002189380320487544\n",
            "step: 360, loss: 0.00015695983893238008\n",
            "step: 370, loss: 0.0002121177240042016\n",
            "step: 380, loss: 0.000940389814786613\n",
            "step: 390, loss: 0.1301889270544052\n",
            "step: 400, loss: 0.0027194342110306025\n",
            "step: 410, loss: 0.00028075711452402174\n",
            "step: 420, loss: 0.00868570152670145\n",
            "step: 430, loss: 0.03164157271385193\n",
            "step: 440, loss: 0.004370164126157761\n",
            "step: 450, loss: 0.0027325875125825405\n",
            "step: 460, loss: 0.00034495574072934687\n",
            "step: 470, loss: 0.00016697148384992033\n",
            "step: 480, loss: 8.107382018351927e-05\n",
            "step: 490, loss: 0.004424704238772392\n",
            "step: 500, loss: 0.0011482387781143188\n",
            "step: 510, loss: 0.0047795879654586315\n",
            "step: 520, loss: 0.001382104237563908\n",
            "step: 530, loss: 0.0002423187397653237\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9367441860465117, f1=0.9327188940092166, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00855409074574709\n",
            "step: 10, loss: 0.0006976196891628206\n",
            "step: 20, loss: 0.015208898112177849\n",
            "step: 30, loss: 0.0029592206701636314\n",
            "step: 40, loss: 0.0014437478967010975\n",
            "step: 50, loss: 6.762891280231997e-05\n",
            "step: 60, loss: 0.00018353559426032007\n",
            "step: 70, loss: 0.0005483317654579878\n",
            "step: 80, loss: 0.004308048635721207\n",
            "step: 90, loss: 0.0008138769771903753\n",
            "step: 100, loss: 8.625458576716483e-05\n",
            "step: 110, loss: 0.00030215445440262556\n",
            "step: 120, loss: 0.009872390888631344\n",
            "step: 130, loss: 0.03553760051727295\n",
            "step: 140, loss: 0.00608940189704299\n",
            "step: 150, loss: 0.00026457267813384533\n",
            "step: 160, loss: 0.0008426682325080037\n",
            "step: 170, loss: 0.01917913369834423\n",
            "step: 180, loss: 0.000282236491329968\n",
            "step: 190, loss: 0.0018841971177607775\n",
            "step: 200, loss: 0.0004307385243009776\n",
            "step: 210, loss: 0.0006785893929190934\n",
            "step: 220, loss: 0.0008367258124053478\n",
            "step: 230, loss: 0.0004115664341952652\n",
            "step: 240, loss: 0.020238002762198448\n",
            "step: 250, loss: 0.00028771156212314963\n",
            "step: 260, loss: 0.01920437254011631\n",
            "step: 270, loss: 5.661423710989766e-05\n",
            "step: 280, loss: 0.0066802348010241985\n",
            "step: 290, loss: 0.03504167124629021\n",
            "step: 300, loss: 0.00015168650134000927\n",
            "step: 310, loss: 0.03102644346654415\n",
            "step: 320, loss: 0.0017627017805352807\n",
            "step: 330, loss: 0.00043056669528596103\n",
            "step: 340, loss: 0.000525047245901078\n",
            "step: 350, loss: 0.012145180255174637\n",
            "step: 360, loss: 0.0010002244962379336\n",
            "step: 370, loss: 0.006222480442374945\n",
            "step: 380, loss: 0.011485186405479908\n",
            "step: 390, loss: 0.00042187259532511234\n",
            "step: 400, loss: 0.007973087020218372\n",
            "step: 410, loss: 0.0018168498063459992\n",
            "step: 420, loss: 0.0004142228572163731\n",
            "step: 430, loss: 0.0002698706812225282\n",
            "step: 440, loss: 0.002719239331781864\n",
            "step: 450, loss: 6.567795207956806e-05\n",
            "step: 460, loss: 8.069151954259723e-05\n",
            "step: 470, loss: 0.00021622792701236904\n",
            "step: 480, loss: 0.00011392317537683994\n",
            "step: 490, loss: 0.010665440000593662\n",
            "step: 500, loss: 0.001448773662559688\n",
            "step: 510, loss: 0.0005994226085022092\n",
            "step: 520, loss: 0.000648830144200474\n",
            "step: 530, loss: 0.00015773599443491548\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9351981351981352, f1=0.9381682938168294, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001373708073515445\n",
            "step: 10, loss: 0.00011393886961741373\n",
            "step: 20, loss: 2.036209662037436e-05\n",
            "step: 30, loss: 2.3714197595836595e-05\n",
            "step: 40, loss: 3.357845343998633e-05\n",
            "step: 50, loss: 0.0009608329855836928\n",
            "step: 60, loss: 0.00034207795397378504\n",
            "step: 70, loss: 0.0013028548564761877\n",
            "step: 80, loss: 1.82350559043698e-05\n",
            "step: 90, loss: 0.0018969714874401689\n",
            "step: 100, loss: 0.00020835809118580073\n",
            "step: 110, loss: 9.34943527681753e-05\n",
            "step: 120, loss: 0.018621142953634262\n",
            "step: 130, loss: 5.459013846120797e-05\n",
            "step: 140, loss: 0.027536986395716667\n",
            "step: 150, loss: 4.973976319888607e-05\n",
            "step: 160, loss: 0.00013711965584661812\n",
            "step: 170, loss: 0.00012379713007248938\n",
            "step: 180, loss: 0.00040189025457948446\n",
            "step: 190, loss: 0.00010431565169710666\n",
            "step: 200, loss: 0.0005989056662656367\n",
            "step: 210, loss: 0.0002645437198225409\n",
            "step: 220, loss: 0.0009662683005444705\n",
            "step: 230, loss: 0.006473179906606674\n",
            "step: 240, loss: 0.0005030918982811272\n",
            "step: 250, loss: 4.5470904296962544e-05\n",
            "step: 260, loss: 0.0012163524515926838\n",
            "step: 270, loss: 0.0007315715774893761\n",
            "step: 280, loss: 0.0002702478086575866\n",
            "step: 290, loss: 0.0034253187477588654\n",
            "step: 300, loss: 2.3833687009755522e-05\n",
            "step: 310, loss: 0.00010210351319983602\n",
            "step: 320, loss: 0.0007069007260724902\n",
            "step: 330, loss: 0.0004676894750446081\n",
            "step: 340, loss: 0.005924759898334742\n",
            "step: 350, loss: 0.001957367639988661\n",
            "step: 360, loss: 6.199204653967172e-05\n",
            "step: 370, loss: 0.004383268300443888\n",
            "step: 380, loss: 0.006035674829035997\n",
            "step: 390, loss: 0.0002039099927060306\n",
            "step: 400, loss: 0.0007931645377539098\n",
            "step: 410, loss: 0.0025876429863274097\n",
            "step: 420, loss: 0.0008610851364210248\n",
            "step: 430, loss: 0.00010382203618064523\n",
            "step: 440, loss: 0.004897471982985735\n",
            "step: 450, loss: 0.0006870211800560355\n",
            "step: 460, loss: 5.86827372899279e-05\n",
            "step: 470, loss: 0.006668186280876398\n",
            "step: 480, loss: 7.346231723204255e-05\n",
            "step: 490, loss: 6.241419760044664e-05\n",
            "step: 500, loss: 0.039254095405340195\n",
            "step: 510, loss: 0.00031355739338323474\n",
            "step: 520, loss: 0.0007532525341957808\n",
            "step: 530, loss: 0.0010493311565369368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9388322520852641, f1=0.9390187987161852, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015469513833522797\n",
            "step: 10, loss: 0.07266353815793991\n",
            "step: 20, loss: 3.4558343031676486e-05\n",
            "step: 30, loss: 0.00012147557572461665\n",
            "step: 40, loss: 0.0012485446641221642\n",
            "step: 50, loss: 0.006260852795094252\n",
            "step: 60, loss: 0.0405375137925148\n",
            "step: 70, loss: 2.0380693968036212e-05\n",
            "step: 80, loss: 0.015124638564884663\n",
            "step: 90, loss: 1.8022685253527015e-05\n",
            "step: 100, loss: 0.0010347057832404971\n",
            "step: 110, loss: 0.0002802898525260389\n",
            "step: 120, loss: 0.00020769699767697603\n",
            "step: 130, loss: 3.2721120078349486e-05\n",
            "step: 140, loss: 0.03850885108113289\n",
            "step: 150, loss: 2.7264555683359504e-05\n",
            "step: 160, loss: 2.6050056476378813e-05\n",
            "step: 170, loss: 7.520960207330063e-05\n",
            "step: 180, loss: 0.0033596705179661512\n",
            "step: 190, loss: 0.003043954726308584\n",
            "step: 200, loss: 0.026553137227892876\n",
            "step: 210, loss: 0.0018347513396292925\n",
            "step: 220, loss: 0.002088319743052125\n",
            "step: 230, loss: 0.00016847610822878778\n",
            "step: 240, loss: 0.0008802128140814602\n",
            "step: 250, loss: 6.198185292305425e-05\n",
            "step: 260, loss: 2.463024247845169e-05\n",
            "step: 270, loss: 0.0007985724951140583\n",
            "step: 280, loss: 0.0002863410336431116\n",
            "step: 290, loss: 0.0007292009540833533\n",
            "step: 300, loss: 0.003626954508945346\n",
            "step: 310, loss: 0.00020912091713398695\n",
            "step: 320, loss: 0.0001427463284926489\n",
            "step: 330, loss: 0.0014626566553488374\n",
            "step: 340, loss: 0.003470734693109989\n",
            "step: 350, loss: 0.004610392730683088\n",
            "step: 360, loss: 0.000103618367575109\n",
            "step: 370, loss: 0.0025114258751273155\n",
            "step: 380, loss: 0.00017417134949937463\n",
            "step: 390, loss: 0.00021213329455349594\n",
            "step: 400, loss: 0.00022041617194190621\n",
            "step: 410, loss: 0.0006245353724807501\n",
            "step: 420, loss: 0.0007297425763681531\n",
            "step: 430, loss: 2.9150785849196836e-05\n",
            "step: 440, loss: 0.0010141184320673347\n",
            "step: 450, loss: 0.10582751035690308\n",
            "step: 460, loss: 0.005009902641177177\n",
            "step: 470, loss: 0.0008133137016557157\n",
            "step: 480, loss: 0.007860038429498672\n",
            "step: 490, loss: 0.0001287082995986566\n",
            "step: 500, loss: 0.0012502563185989857\n",
            "step: 510, loss: 0.0001279124553548172\n",
            "step: 520, loss: 0.00012541604519356042\n",
            "step: 530, loss: 5.89154806220904e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9340866290018832, f1=0.9381682938168294, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003550513181835413\n",
            "step: 10, loss: 4.825737050850876e-05\n",
            "step: 20, loss: 0.00023446526029147208\n",
            "step: 30, loss: 0.0068142954260110855\n",
            "step: 40, loss: 0.00038212293293327093\n",
            "step: 50, loss: 0.06270446628332138\n",
            "step: 60, loss: 0.00800444558262825\n",
            "step: 70, loss: 0.003821160877123475\n",
            "step: 80, loss: 9.748512093210593e-05\n",
            "step: 90, loss: 0.0007067913538776338\n",
            "step: 100, loss: 0.00014993610966484994\n",
            "step: 110, loss: 0.0007222354179248214\n",
            "step: 120, loss: 3.73960574506782e-05\n",
            "step: 130, loss: 0.0015453004743903875\n",
            "step: 140, loss: 1.5545392670901492e-05\n",
            "step: 150, loss: 2.8247115551494062e-05\n",
            "step: 160, loss: 2.2417847503675148e-05\n",
            "step: 170, loss: 0.00011809682473540306\n",
            "step: 180, loss: 8.02850117906928e-05\n",
            "step: 190, loss: 4.341865496826358e-05\n",
            "step: 200, loss: 0.00011973790969932452\n",
            "step: 210, loss: 5.06072465213947e-05\n",
            "step: 220, loss: 0.00011546807218110189\n",
            "step: 230, loss: 0.00014018082583788782\n",
            "step: 240, loss: 0.0026415616739541292\n",
            "step: 250, loss: 0.0002591164957266301\n",
            "step: 260, loss: 0.0002130954817403108\n",
            "step: 270, loss: 0.0005556629039347172\n",
            "step: 280, loss: 0.0015779782552272081\n",
            "step: 290, loss: 0.016281861811876297\n",
            "step: 300, loss: 0.004118530545383692\n",
            "step: 310, loss: 0.00024097852292470634\n",
            "step: 320, loss: 0.0010297310072928667\n",
            "step: 330, loss: 0.0001672244688961655\n",
            "step: 340, loss: 0.0013822235632687807\n",
            "step: 350, loss: 0.0021967850625514984\n",
            "step: 360, loss: 0.048398178070783615\n",
            "step: 370, loss: 0.0004328599898144603\n",
            "step: 380, loss: 0.00018718958017416298\n",
            "step: 390, loss: 0.00023780213086865842\n",
            "step: 400, loss: 0.0005402190727181733\n",
            "step: 410, loss: 0.0005297018215060234\n",
            "step: 420, loss: 0.01205623708665371\n",
            "step: 430, loss: 0.016535021364688873\n",
            "step: 440, loss: 0.0003179094346705824\n",
            "step: 450, loss: 0.00511898985132575\n",
            "step: 460, loss: 8.255388820543885e-05\n",
            "step: 470, loss: 0.001079405308701098\n",
            "step: 480, loss: 0.0005962061113677919\n",
            "step: 490, loss: 0.016712335869669914\n",
            "step: 500, loss: 0.0003174935991410166\n",
            "step: 510, loss: 0.009160139597952366\n",
            "step: 520, loss: 1.78100945049664e-05\n",
            "step: 530, loss: 7.91751008364372e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.937471051412691, f1=0.9344337459880789, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008604431641288102\n",
            "step: 10, loss: 6.24709646217525e-05\n",
            "step: 20, loss: 7.877442112658173e-05\n",
            "step: 30, loss: 3.6106757761444896e-05\n",
            "step: 40, loss: 7.344319601543248e-05\n",
            "step: 50, loss: 0.0005476018413901329\n",
            "step: 60, loss: 0.00021891944925300777\n",
            "step: 70, loss: 0.0001856315357144922\n",
            "step: 80, loss: 0.00011624014587141573\n",
            "step: 90, loss: 0.060647834092378616\n",
            "step: 100, loss: 0.001855025184340775\n",
            "step: 110, loss: 0.0005423896363936365\n",
            "step: 120, loss: 0.0012875188840553164\n",
            "step: 130, loss: 0.0005853504408150911\n",
            "step: 140, loss: 0.0002172253152821213\n",
            "step: 150, loss: 0.000994945876300335\n",
            "step: 160, loss: 0.00021737781935371459\n",
            "step: 170, loss: 0.0002714448783081025\n",
            "step: 180, loss: 6.861505244160071e-05\n",
            "step: 190, loss: 0.00013885294902138412\n",
            "step: 200, loss: 0.00015070191875565797\n",
            "step: 210, loss: 0.00010645046131685376\n",
            "step: 220, loss: 0.0001023536387947388\n",
            "step: 230, loss: 0.0002508679172024131\n",
            "step: 240, loss: 0.00012145499931648374\n",
            "step: 250, loss: 0.00027472045621834695\n",
            "step: 260, loss: 0.00010317134001525119\n",
            "step: 270, loss: 3.760988329304382e-05\n",
            "step: 280, loss: 0.0010872103739529848\n",
            "step: 290, loss: 0.009410357102751732\n",
            "step: 300, loss: 5.2268187573645264e-05\n",
            "step: 310, loss: 0.00245875958353281\n",
            "step: 320, loss: 0.002683911705389619\n",
            "step: 330, loss: 0.06634005159139633\n",
            "step: 340, loss: 0.0026582274585962296\n",
            "step: 350, loss: 0.029537446796894073\n",
            "step: 360, loss: 0.0003236875054426491\n",
            "step: 370, loss: 6.57396376482211e-05\n",
            "step: 380, loss: 0.00017764537187758833\n",
            "step: 390, loss: 0.025557009503245354\n",
            "step: 400, loss: 0.0001311750093009323\n",
            "step: 410, loss: 0.0032046562992036343\n",
            "step: 420, loss: 6.991325062699616e-05\n",
            "step: 430, loss: 0.0009877632837742567\n",
            "step: 440, loss: 0.005377490073442459\n",
            "step: 450, loss: 0.0003016888804268092\n",
            "step: 460, loss: 0.0382474884390831\n",
            "step: 470, loss: 9.925539052346721e-05\n",
            "step: 480, loss: 0.0003720140957739204\n",
            "step: 490, loss: 6.602580106118694e-05\n",
            "step: 500, loss: 0.00032295286655426025\n",
            "step: 510, loss: 0.0002315865276614204\n",
            "step: 520, loss: 0.014219569973647594\n",
            "step: 530, loss: 0.00044569108285941184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9382366808109384, f1=0.9403054141601109, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005018578376621008\n",
            "step: 10, loss: 2.051102273981087e-05\n",
            "step: 20, loss: 8.038866508286446e-05\n",
            "step: 30, loss: 3.616922549554147e-05\n",
            "step: 40, loss: 0.0009477455168962479\n",
            "step: 50, loss: 0.0009504980989731848\n",
            "step: 60, loss: 1.605575562280137e-05\n",
            "step: 70, loss: 2.349770329601597e-05\n",
            "step: 80, loss: 1.4148322406981606e-05\n",
            "step: 90, loss: 0.00026490443269722164\n",
            "step: 100, loss: 0.002858429914340377\n",
            "step: 110, loss: 0.0020121638663113117\n",
            "step: 120, loss: 6.192264845594764e-05\n",
            "step: 130, loss: 0.010488811880350113\n",
            "step: 140, loss: 0.0035993142519146204\n",
            "step: 150, loss: 1.2010238606308121e-05\n",
            "step: 160, loss: 0.0010231437627226114\n",
            "step: 170, loss: 0.0006957991281524301\n",
            "step: 180, loss: 4.594152414938435e-05\n",
            "step: 190, loss: 0.0002226904034614563\n",
            "step: 200, loss: 0.000392561691114679\n",
            "step: 210, loss: 1.773205258359667e-05\n",
            "step: 220, loss: 3.253794420743361e-05\n",
            "step: 230, loss: 0.0003508463269099593\n",
            "step: 240, loss: 0.0011217637220397592\n",
            "step: 250, loss: 0.000640456157270819\n",
            "step: 260, loss: 0.0024902098812162876\n",
            "step: 270, loss: 0.00012897416308987886\n",
            "step: 280, loss: 3.778674727072939e-05\n",
            "step: 290, loss: 0.001383058843202889\n",
            "step: 300, loss: 0.01813006028532982\n",
            "step: 310, loss: 0.00020564829173963517\n",
            "step: 320, loss: 0.0010822630720213056\n",
            "step: 330, loss: 0.0022181656677275896\n",
            "step: 340, loss: 0.0004861502966377884\n",
            "step: 350, loss: 0.001812883885577321\n",
            "step: 360, loss: 0.0006182476645335555\n",
            "step: 370, loss: 0.00017290026880800724\n",
            "step: 380, loss: 0.0007755705737508833\n",
            "step: 390, loss: 0.032598916441202164\n",
            "step: 400, loss: 0.0009209130657836795\n",
            "step: 410, loss: 4.7371107939397916e-05\n",
            "step: 420, loss: 0.0004096115881111473\n",
            "step: 430, loss: 0.11574704945087433\n",
            "step: 440, loss: 0.0005880396929569542\n",
            "step: 450, loss: 9.063348261406645e-05\n",
            "step: 460, loss: 0.0006248411955311894\n",
            "step: 470, loss: 0.00011608241766225547\n",
            "step: 480, loss: 3.4243155823787674e-05\n",
            "step: 490, loss: 2.079001205856912e-05\n",
            "step: 500, loss: 2.9132585041224957e-05\n",
            "step: 510, loss: 0.01812981441617012\n",
            "step: 520, loss: 0.0006239743670448661\n",
            "step: 530, loss: 0.0020317137241363525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9368372521899492, f1=0.9374714742126883, best_f1=0.9410672853828307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3687871362199076e-05\n",
            "step: 10, loss: 9.793722711037844e-06\n",
            "step: 20, loss: 0.0035652273800224066\n",
            "step: 30, loss: 9.757017687661573e-05\n",
            "step: 40, loss: 0.08668258786201477\n",
            "step: 50, loss: 0.001069393940269947\n",
            "step: 60, loss: 3.2595267839496955e-05\n",
            "step: 70, loss: 0.0016747487243264914\n",
            "step: 80, loss: 2.2838317818241194e-05\n",
            "step: 90, loss: 4.5620025048265234e-05\n",
            "step: 100, loss: 0.0011002501705661416\n",
            "step: 110, loss: 0.0004476162139326334\n",
            "step: 120, loss: 0.00044934035395272076\n",
            "step: 130, loss: 0.12642204761505127\n",
            "step: 140, loss: 7.788067159708589e-05\n",
            "step: 150, loss: 3.177365579176694e-05\n",
            "step: 160, loss: 9.272013994632289e-05\n",
            "step: 170, loss: 0.00013800141459796578\n",
            "step: 180, loss: 6.67304775561206e-05\n",
            "step: 190, loss: 0.00039648450911045074\n",
            "step: 200, loss: 0.00015986972721293569\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 210, loss: 0.0031168549321591854\n",
            "step: 220, loss: 2.4074501197901554e-05\n",
            "step: 230, loss: 0.0003840427962131798\n",
            "step: 240, loss: 0.019150052219629288\n",
            "step: 250, loss: 7.884045771788806e-05\n",
            "step: 260, loss: 3.446342088864185e-05\n",
            "step: 270, loss: 1.4863692740618717e-05\n",
            "step: 280, loss: 2.2548380002263002e-05\n",
            "step: 290, loss: 1.5790981706231833e-05\n",
            "step: 300, loss: 2.9810085834469646e-05\n",
            "step: 310, loss: 0.0008435420459136367\n",
            "step: 320, loss: 0.00037344160955399275\n",
            "step: 330, loss: 2.149416832253337e-05\n",
            "step: 340, loss: 5.9247337048873305e-05\n",
            "step: 350, loss: 3.37108394887764e-05\n",
            "step: 360, loss: 0.001640145666897297\n",
            "step: 370, loss: 2.6264480766258202e-05\n",
            "step: 380, loss: 0.0004390160320326686\n",
            "step: 390, loss: 0.00027898509870283306\n",
            "step: 400, loss: 0.00277429330162704\n",
            "step: 410, loss: 7.412367995129898e-05\n",
            "step: 420, loss: 0.00016857474111020565\n",
            "step: 430, loss: 0.00018515651754569262\n",
            "step: 440, loss: 0.00013943700469098985\n",
            "step: 450, loss: 1.2598787179740611e-05\n",
            "step: 460, loss: 0.00015346244617830962\n",
            "step: 470, loss: 0.0015097020659595728\n",
            "step: 480, loss: 1.170104314951459e-05\n",
            "step: 490, loss: 0.00011029809684259817\n",
            "step: 500, loss: 0.0012593235587701201\n",
            "step: 510, loss: 6.335590296657756e-05\n",
            "step: 520, loss: 0.0027368913870304823\n",
            "step: 530, loss: 0.0005124799208715558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9385052034058656, f1=0.9398040130657956, best_f1=0.9410672853828307\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:20, 277.91it/s]\n",
            "load_f1 = 0.9402985074626865\n",
            "real_f1 = 0.9392523364485983\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 269.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6598b32a-e272-4e7d-e8cb-7a6cf8f35c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5639081001281738\n",
            "step: 10, loss: 0.3706364929676056\n",
            "step: 20, loss: 0.382394015789032\n",
            "step: 30, loss: 0.3135828673839569\n",
            "step: 40, loss: 0.16647598147392273\n",
            "step: 50, loss: 0.41640153527259827\n",
            "step: 60, loss: 0.20282821357250214\n",
            "step: 70, loss: 0.14026296138763428\n",
            "step: 80, loss: 0.19829455018043518\n",
            "step: 90, loss: 0.36596640944480896\n",
            "step: 100, loss: 0.48103880882263184\n",
            "step: 110, loss: 0.26298466324806213\n",
            "step: 120, loss: 0.17814761400222778\n",
            "step: 130, loss: 0.20168043673038483\n",
            "step: 140, loss: 0.24380607903003693\n",
            "step: 150, loss: 0.15448272228240967\n",
            "step: 160, loss: 0.2299535870552063\n",
            "step: 170, loss: 0.2775481045246124\n",
            "step: 180, loss: 0.15774378180503845\n",
            "step: 190, loss: 0.18518295884132385\n",
            "step: 200, loss: 0.3123333156108856\n",
            "step: 210, loss: 0.2671462595462799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6552462526766595, f1=0.6361746361746361, best_f1=0.6361746361746361\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11002712696790695\n",
            "step: 10, loss: 0.1739218533039093\n",
            "step: 20, loss: 0.2633654773235321\n",
            "step: 30, loss: 0.1931627094745636\n",
            "step: 40, loss: 0.15869595110416412\n",
            "step: 50, loss: 0.3297938406467438\n",
            "step: 60, loss: 0.4221328794956207\n",
            "step: 70, loss: 0.11352359503507614\n",
            "step: 80, loss: 0.2342834174633026\n",
            "step: 90, loss: 0.13326817750930786\n",
            "step: 100, loss: 0.006262835115194321\n",
            "step: 110, loss: 0.10388631373643875\n",
            "step: 120, loss: 0.1705896258354187\n",
            "step: 130, loss: 0.026996392756700516\n",
            "step: 140, loss: 0.1408982276916504\n",
            "step: 150, loss: 0.15276040136814117\n",
            "step: 160, loss: 0.2145272195339203\n",
            "step: 170, loss: 0.16478994488716125\n",
            "step: 180, loss: 0.22769197821617126\n",
            "step: 190, loss: 0.1844366490840912\n",
            "step: 200, loss: 0.043303146958351135\n",
            "step: 210, loss: 0.09861031919717789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6950959488272921, f1=0.6480186480186481, best_f1=0.6480186480186481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05230163410305977\n",
            "step: 10, loss: 0.20368574559688568\n",
            "step: 20, loss: 0.2941474914550781\n",
            "step: 30, loss: 0.18947824835777283\n",
            "step: 40, loss: 0.10861322283744812\n",
            "step: 50, loss: 0.11614259332418442\n",
            "step: 60, loss: 0.338554322719574\n",
            "step: 70, loss: 0.11414992064237595\n",
            "step: 80, loss: 0.12003540247678757\n",
            "step: 90, loss: 0.09511338919401169\n",
            "step: 100, loss: 0.2380192130804062\n",
            "step: 110, loss: 0.13337327539920807\n",
            "step: 120, loss: 0.09438443183898926\n",
            "step: 130, loss: 0.12461186945438385\n",
            "step: 140, loss: 0.037173885852098465\n",
            "step: 150, loss: 0.3189842998981476\n",
            "step: 160, loss: 0.03764372691512108\n",
            "step: 170, loss: 0.12507127225399017\n",
            "step: 180, loss: 0.12439028918743134\n",
            "step: 190, loss: 0.17144152522087097\n",
            "step: 200, loss: 0.0704815462231636\n",
            "step: 210, loss: 0.13636904954910278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.65625, f1=0.6695842450765865, best_f1=0.6480186480186481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09334680438041687\n",
            "step: 10, loss: 0.10783510655164719\n",
            "step: 20, loss: 0.1789252609014511\n",
            "step: 30, loss: 0.22816236317157745\n",
            "step: 40, loss: 0.06507208198308945\n",
            "step: 50, loss: 0.1773841679096222\n",
            "step: 60, loss: 0.10016200691461563\n",
            "step: 70, loss: 0.09227555245161057\n",
            "step: 80, loss: 0.14868302643299103\n",
            "step: 90, loss: 0.026010330766439438\n",
            "step: 100, loss: 0.2377583235502243\n",
            "step: 110, loss: 0.11033304035663605\n",
            "step: 120, loss: 0.1155468076467514\n",
            "step: 130, loss: 0.3334611654281616\n",
            "step: 140, loss: 0.13075359165668488\n",
            "step: 150, loss: 0.1546981930732727\n",
            "step: 160, loss: 0.10166604071855545\n",
            "step: 170, loss: 0.08882159739732742\n",
            "step: 180, loss: 0.3878730833530426\n",
            "step: 190, loss: 0.09949342161417007\n",
            "step: 200, loss: 0.2679075300693512\n",
            "step: 210, loss: 0.2039969563484192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6798418972332015, f1=0.6614481409001958, best_f1=0.6480186480186481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15658758580684662\n",
            "step: 10, loss: 0.04258876293897629\n",
            "step: 20, loss: 0.37136104702949524\n",
            "step: 30, loss: 0.09687234461307526\n",
            "step: 40, loss: 0.1478796899318695\n",
            "step: 50, loss: 0.06793300807476044\n",
            "step: 60, loss: 0.16306129097938538\n",
            "step: 70, loss: 0.029519176110625267\n",
            "step: 80, loss: 0.01742039993405342\n",
            "step: 90, loss: 0.23550669848918915\n",
            "step: 100, loss: 0.0050387014634907246\n",
            "step: 110, loss: 0.13471031188964844\n",
            "step: 120, loss: 0.10191141068935394\n",
            "step: 130, loss: 0.04291908070445061\n",
            "step: 140, loss: 0.073587566614151\n",
            "step: 150, loss: 0.05974706634879112\n",
            "step: 160, loss: 0.11155737936496735\n",
            "step: 170, loss: 0.05288739129900932\n",
            "step: 180, loss: 0.04849237948656082\n",
            "step: 190, loss: 0.04843819886445999\n",
            "step: 200, loss: 0.10828796774148941\n",
            "step: 210, loss: 0.03147848695516586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.700952380952381, f1=0.6928838951310863, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020266523584723473\n",
            "step: 10, loss: 0.03518495708703995\n",
            "step: 20, loss: 0.12015429139137268\n",
            "step: 30, loss: 0.004726327024400234\n",
            "step: 40, loss: 0.0635327696800232\n",
            "step: 50, loss: 0.006388071924448013\n",
            "step: 60, loss: 0.2754669785499573\n",
            "step: 70, loss: 0.006910889875143766\n",
            "step: 80, loss: 0.10444064438343048\n",
            "step: 90, loss: 0.05822986736893654\n",
            "step: 100, loss: 0.013545789755880833\n",
            "step: 110, loss: 0.005092693492770195\n",
            "step: 120, loss: 0.03712555766105652\n",
            "step: 130, loss: 0.08850507438182831\n",
            "step: 140, loss: 0.044696174561977386\n",
            "step: 150, loss: 0.01594991236925125\n",
            "step: 160, loss: 0.049166589975357056\n",
            "step: 170, loss: 0.12125156819820404\n",
            "step: 180, loss: 0.051094476133584976\n",
            "step: 190, loss: 0.055983155965805054\n",
            "step: 200, loss: 0.0032399925403296947\n",
            "step: 210, loss: 0.04789644852280617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6615087040618955, f1=0.6692160611854684, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08127915114164352\n",
            "step: 10, loss: 0.02892228588461876\n",
            "step: 20, loss: 0.026160161942243576\n",
            "step: 30, loss: 0.052875395864248276\n",
            "step: 40, loss: 0.09714821726083755\n",
            "step: 50, loss: 0.1751328408718109\n",
            "step: 60, loss: 0.04236672446131706\n",
            "step: 70, loss: 0.03112264908850193\n",
            "step: 80, loss: 0.0785362720489502\n",
            "step: 90, loss: 0.19114093482494354\n",
            "step: 100, loss: 0.005820651073008776\n",
            "step: 110, loss: 0.2262616604566574\n",
            "step: 120, loss: 0.052347954362630844\n",
            "step: 130, loss: 0.03088473342359066\n",
            "step: 140, loss: 0.012061181478202343\n",
            "step: 150, loss: 0.033005569130182266\n",
            "step: 160, loss: 0.03208281099796295\n",
            "step: 170, loss: 0.021838150918483734\n",
            "step: 180, loss: 0.013779574073851109\n",
            "step: 190, loss: 0.10213777422904968\n",
            "step: 200, loss: 0.032410044223070145\n",
            "step: 210, loss: 0.024715058505535126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6561886051080551, f1=0.6576402321083173, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07327891886234283\n",
            "step: 10, loss: 0.1188114583492279\n",
            "step: 20, loss: 0.007781947031617165\n",
            "step: 30, loss: 0.057399868965148926\n",
            "step: 40, loss: 0.009379838593304157\n",
            "step: 50, loss: 0.00788422953337431\n",
            "step: 60, loss: 0.0449359267950058\n",
            "step: 70, loss: 0.031647443771362305\n",
            "step: 80, loss: 0.13417614996433258\n",
            "step: 90, loss: 0.006609312724322081\n",
            "step: 100, loss: 0.04573671519756317\n",
            "step: 110, loss: 0.18488344550132751\n",
            "step: 120, loss: 0.015420926734805107\n",
            "step: 130, loss: 0.006493610329926014\n",
            "step: 140, loss: 0.08756720274686813\n",
            "step: 150, loss: 0.047209009528160095\n",
            "step: 160, loss: 0.011738837696611881\n",
            "step: 170, loss: 0.02320799045264721\n",
            "step: 180, loss: 0.13451898097991943\n",
            "step: 190, loss: 0.00331412092782557\n",
            "step: 200, loss: 0.0140226436778903\n",
            "step: 210, loss: 0.13162443041801453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6680244399185336, f1=0.6625514403292181, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014358852989971638\n",
            "step: 10, loss: 0.14726591110229492\n",
            "step: 20, loss: 0.011903375387191772\n",
            "step: 30, loss: 0.10475293546915054\n",
            "step: 40, loss: 0.022059671580791473\n",
            "step: 50, loss: 0.0620155967772007\n",
            "step: 60, loss: 0.15046678483486176\n",
            "step: 70, loss: 0.08131146430969238\n",
            "step: 80, loss: 0.004944295156747103\n",
            "step: 90, loss: 0.030322298407554626\n",
            "step: 100, loss: 0.004029077477753162\n",
            "step: 110, loss: 0.05091540887951851\n",
            "step: 120, loss: 0.00552815617993474\n",
            "step: 130, loss: 0.0031851057428866625\n",
            "step: 140, loss: 0.017895428463816643\n",
            "step: 150, loss: 0.24073921144008636\n",
            "step: 160, loss: 0.0036692458670586348\n",
            "step: 170, loss: 0.006967563182115555\n",
            "step: 180, loss: 0.015753747895359993\n",
            "step: 190, loss: 0.0017208089120686054\n",
            "step: 200, loss: 0.10106059163808823\n",
            "step: 210, loss: 0.010723580606281757\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6610169491525424, f1=0.652542372881356, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14934559166431427\n",
            "step: 10, loss: 0.06599054485559464\n",
            "step: 20, loss: 0.012215256690979004\n",
            "step: 30, loss: 0.170230433344841\n",
            "step: 40, loss: 0.0157984159886837\n",
            "step: 50, loss: 0.0070367781445384026\n",
            "step: 60, loss: 0.16900525987148285\n",
            "step: 70, loss: 0.0426776148378849\n",
            "step: 80, loss: 0.03204770013689995\n",
            "step: 90, loss: 0.029917100444436073\n",
            "step: 100, loss: 0.03567573428153992\n",
            "step: 110, loss: 0.0008049691678024828\n",
            "step: 120, loss: 0.04438384994864464\n",
            "step: 130, loss: 0.03966689109802246\n",
            "step: 140, loss: 0.07368934899568558\n",
            "step: 150, loss: 0.03418935090303421\n",
            "step: 160, loss: 0.007292026653885841\n",
            "step: 170, loss: 0.004553858656436205\n",
            "step: 180, loss: 0.06455941498279572\n",
            "step: 190, loss: 0.017610780894756317\n",
            "step: 200, loss: 0.0086404699832201\n",
            "step: 210, loss: 0.0432850643992424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.659919028340081, f1=0.6468253968253969, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07947655767202377\n",
            "step: 10, loss: 0.1289757788181305\n",
            "step: 20, loss: 0.02240743115544319\n",
            "step: 30, loss: 0.00046357751125469804\n",
            "step: 40, loss: 0.010045495815575123\n",
            "step: 50, loss: 0.021873997524380684\n",
            "step: 60, loss: 0.03468218073248863\n",
            "step: 70, loss: 0.009894885122776031\n",
            "step: 80, loss: 0.06476342678070068\n",
            "step: 90, loss: 0.19406171143054962\n",
            "step: 100, loss: 0.04052308201789856\n",
            "step: 110, loss: 0.08739090710878372\n",
            "step: 120, loss: 0.16696932911872864\n",
            "step: 130, loss: 0.016913942992687225\n",
            "step: 140, loss: 0.024083472788333893\n",
            "step: 150, loss: 0.004037641454488039\n",
            "step: 160, loss: 0.0066148764453828335\n",
            "step: 170, loss: 0.09353519976139069\n",
            "step: 180, loss: 0.00626385910436511\n",
            "step: 190, loss: 0.014782138168811798\n",
            "step: 200, loss: 0.004667400848120451\n",
            "step: 210, loss: 0.0026478280778974295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6626506024096386, f1=0.6459627329192545, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04578777775168419\n",
            "step: 10, loss: 0.002073517767712474\n",
            "step: 20, loss: 0.023375842720270157\n",
            "step: 30, loss: 0.0019997290801256895\n",
            "step: 40, loss: 0.1612979620695114\n",
            "step: 50, loss: 0.011014607734978199\n",
            "step: 60, loss: 0.12424435466527939\n",
            "step: 70, loss: 0.004619946703314781\n",
            "step: 80, loss: 0.08907999843358994\n",
            "step: 90, loss: 0.0572272352874279\n",
            "step: 100, loss: 0.04948955774307251\n",
            "step: 110, loss: 0.004345242399722338\n",
            "step: 120, loss: 0.0029281240422278643\n",
            "step: 130, loss: 0.02715270034968853\n",
            "step: 140, loss: 0.12595431506633759\n",
            "step: 150, loss: 0.008324031718075275\n",
            "step: 160, loss: 0.016678180545568466\n",
            "step: 170, loss: 0.006093323230743408\n",
            "step: 180, loss: 0.0074304016306996346\n",
            "step: 190, loss: 0.14873649179935455\n",
            "step: 200, loss: 0.009079145267605782\n",
            "step: 210, loss: 0.03287637606263161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6625258799171843, f1=0.6538461538461539, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07505571842193604\n",
            "step: 10, loss: 0.001101690111681819\n",
            "step: 20, loss: 0.005462172906845808\n",
            "step: 30, loss: 0.031697310507297516\n",
            "step: 40, loss: 0.0021676376927644014\n",
            "step: 50, loss: 0.03549923375248909\n",
            "step: 60, loss: 0.001678365864790976\n",
            "step: 70, loss: 0.02935512363910675\n",
            "step: 80, loss: 0.01054183579981327\n",
            "step: 90, loss: 0.0007763530593365431\n",
            "step: 100, loss: 0.0014150760835036635\n",
            "step: 110, loss: 0.0016683695139363408\n",
            "step: 120, loss: 0.002945000072941184\n",
            "step: 130, loss: 0.0007892890134826303\n",
            "step: 140, loss: 0.0015825567534193397\n",
            "step: 150, loss: 0.06482871621847153\n",
            "step: 160, loss: 0.005509908311069012\n",
            "step: 170, loss: 0.00866283755749464\n",
            "step: 180, loss: 0.020620007067918777\n",
            "step: 190, loss: 0.0023811645805835724\n",
            "step: 200, loss: 0.0021743513643741608\n",
            "step: 210, loss: 0.0027509895153343678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6613226452905812, f1=0.6625766871165644, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002130323089659214\n",
            "step: 10, loss: 0.036182697862386703\n",
            "step: 20, loss: 0.001076450222171843\n",
            "step: 30, loss: 0.008638857863843441\n",
            "step: 40, loss: 0.002746790647506714\n",
            "step: 50, loss: 0.03465171158313751\n",
            "step: 60, loss: 0.0011976818786934018\n",
            "step: 70, loss: 0.0018691349541768432\n",
            "step: 80, loss: 0.009222117252647877\n",
            "step: 90, loss: 0.011222746223211288\n",
            "step: 100, loss: 0.005782946012914181\n",
            "step: 110, loss: 0.0053054760210216045\n",
            "step: 120, loss: 0.026473769918084145\n",
            "step: 130, loss: 0.002668804721906781\n",
            "step: 140, loss: 0.014806913211941719\n",
            "step: 150, loss: 0.025571554899215698\n",
            "step: 160, loss: 0.00436740554869175\n",
            "step: 170, loss: 0.0038544468116015196\n",
            "step: 180, loss: 0.014150574803352356\n",
            "step: 190, loss: 0.005479025188833475\n",
            "step: 200, loss: 0.11775519698858261\n",
            "step: 210, loss: 0.02434883825480938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6551724137931033, f1=0.6627450980392158, best_f1=0.6928838951310863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004113303031772375\n",
            "step: 10, loss: 0.005507614929229021\n",
            "step: 20, loss: 0.025877168402075768\n",
            "step: 30, loss: 0.012151588685810566\n",
            "step: 40, loss: 0.010476954281330109\n",
            "step: 50, loss: 0.0015212582657113671\n",
            "step: 60, loss: 0.019405236467719078\n",
            "step: 70, loss: 0.015290336683392525\n",
            "step: 80, loss: 0.003998701926320791\n",
            "step: 90, loss: 0.048474639654159546\n",
            "step: 100, loss: 0.019536493346095085\n",
            "step: 110, loss: 0.0007775407866574824\n",
            "step: 120, loss: 0.015269072726368904\n",
            "step: 130, loss: 0.1767289638519287\n",
            "step: 140, loss: 0.0007657125825062394\n",
            "step: 150, loss: 0.004018212202936411\n",
            "step: 160, loss: 0.003159131621941924\n",
            "step: 170, loss: 0.0013760300353169441\n",
            "step: 180, loss: 0.009490610100328922\n",
            "step: 190, loss: 0.057793810963630676\n",
            "step: 200, loss: 0.021589534357190132\n",
            "step: 210, loss: 0.02932339906692505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6603053435114504, f1=0.6666666666666667, best_f1=0.6928838951310863\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 486.40it/s]\n",
            "load_f1 = 0.7074074074074074\n",
            "real_f1 = 0.7024029574861369\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 271.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1293ccba-1d6e-45da-c34d-496ace237b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5808643698692322\n",
            "step: 10, loss: 0.3833308517932892\n",
            "step: 20, loss: 0.2889449894428253\n",
            "step: 30, loss: 0.45252662897109985\n",
            "step: 40, loss: 0.42290154099464417\n",
            "step: 50, loss: 0.294102281332016\n",
            "step: 60, loss: 0.27488815784454346\n",
            "step: 70, loss: 0.2779935300350189\n",
            "step: 80, loss: 0.27258503437042236\n",
            "step: 90, loss: 0.2987004220485687\n",
            "step: 100, loss: 0.33599069714546204\n",
            "step: 110, loss: 0.3892587721347809\n",
            "step: 120, loss: 0.08007869124412537\n",
            "step: 130, loss: 0.100266233086586\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 140, loss: 0.06274550408124924\n",
            "step: 150, loss: 0.16285300254821777\n",
            "step: 160, loss: 0.05863627791404724\n",
            "step: 170, loss: 0.19651685655117035\n",
            "step: 180, loss: 0.03130393847823143\n",
            "step: 190, loss: 0.2646682560443878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6567164179104478, f1=0.6567164179104478, best_f1=0.6567164179104478\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38604557514190674\n",
            "step: 10, loss: 0.05890318378806114\n",
            "step: 20, loss: 0.19541141390800476\n",
            "step: 30, loss: 0.12911032140254974\n",
            "step: 40, loss: 0.11362045258283615\n",
            "step: 50, loss: 0.15441671013832092\n",
            "step: 60, loss: 0.22549065947532654\n",
            "step: 70, loss: 0.19817310571670532\n",
            "step: 80, loss: 0.2890779674053192\n",
            "step: 90, loss: 0.22348469495773315\n",
            "step: 100, loss: 0.033364418894052505\n",
            "step: 110, loss: 0.24971260130405426\n",
            "step: 120, loss: 0.3461412191390991\n",
            "step: 130, loss: 0.08769963681697845\n",
            "step: 140, loss: 0.07003937661647797\n",
            "step: 150, loss: 0.10053519904613495\n",
            "step: 160, loss: 0.019472962245345116\n",
            "step: 170, loss: 0.16000185906887054\n",
            "step: 180, loss: 0.06238052621483803\n",
            "step: 190, loss: 0.05021652579307556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7666666666666667, f1=0.76, best_f1=0.76\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03626517951488495\n",
            "step: 10, loss: 0.1227463111281395\n",
            "step: 20, loss: 0.11305126547813416\n",
            "step: 30, loss: 0.12794917821884155\n",
            "step: 40, loss: 0.07473689317703247\n",
            "step: 50, loss: 0.20875418186187744\n",
            "step: 60, loss: 0.13325655460357666\n",
            "step: 70, loss: 0.0965212881565094\n",
            "step: 80, loss: 0.08073597401380539\n",
            "step: 90, loss: 0.13024844229221344\n",
            "step: 100, loss: 0.02744249813258648\n",
            "step: 110, loss: 0.034133244305849075\n",
            "step: 120, loss: 0.023576829582452774\n",
            "step: 130, loss: 0.013900845311582088\n",
            "step: 140, loss: 0.10356828570365906\n",
            "step: 150, loss: 0.1132173091173172\n",
            "step: 160, loss: 0.02774689719080925\n",
            "step: 170, loss: 0.2348616123199463\n",
            "step: 180, loss: 0.028184793889522552\n",
            "step: 190, loss: 0.13684596121311188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7534626038781163, f1=0.7780821917808218, best_f1=0.76\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22022943198680878\n",
            "step: 10, loss: 0.12009681761264801\n",
            "step: 20, loss: 0.032256562262773514\n",
            "step: 30, loss: 0.017099546268582344\n",
            "step: 40, loss: 0.006182825658470392\n",
            "step: 50, loss: 0.016689110547304153\n",
            "step: 60, loss: 0.04004666581749916\n",
            "step: 70, loss: 0.05890636518597603\n",
            "step: 80, loss: 0.26030564308166504\n",
            "step: 90, loss: 0.020649824291467667\n",
            "step: 100, loss: 0.12994781136512756\n",
            "step: 110, loss: 0.02219228632748127\n",
            "step: 120, loss: 0.0805712640285492\n",
            "step: 130, loss: 0.23326584696769714\n",
            "step: 140, loss: 0.05289953947067261\n",
            "step: 150, loss: 0.01699436642229557\n",
            "step: 160, loss: 0.01826256513595581\n",
            "step: 170, loss: 0.07387124001979828\n",
            "step: 180, loss: 0.13498343527317047\n",
            "step: 190, loss: 0.25135642290115356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.784, f1=0.7968337730870713, best_f1=0.7968337730870713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07199261337518692\n",
            "step: 10, loss: 0.03615245595574379\n",
            "step: 20, loss: 0.042025916278362274\n",
            "step: 30, loss: 0.0020366597454994917\n",
            "step: 40, loss: 0.051576629281044006\n",
            "step: 50, loss: 0.040959425270557404\n",
            "step: 60, loss: 0.17356516420841217\n",
            "step: 70, loss: 0.06678397953510284\n",
            "step: 80, loss: 0.020843422040343285\n",
            "step: 90, loss: 0.034193769097328186\n",
            "step: 100, loss: 0.003608266357332468\n",
            "step: 110, loss: 0.12463663518428802\n",
            "step: 120, loss: 0.007075462024658918\n",
            "step: 130, loss: 0.13474750518798828\n",
            "step: 140, loss: 0.06759360432624817\n",
            "step: 150, loss: 0.08635873347520828\n",
            "step: 160, loss: 0.010990628972649574\n",
            "step: 170, loss: 0.003955975640565157\n",
            "step: 180, loss: 0.05155595391988754\n",
            "step: 190, loss: 0.0023023756220936775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7885117493472584, f1=0.7736842105263158, best_f1=0.7736842105263158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00565031124278903\n",
            "step: 10, loss: 0.025304565206170082\n",
            "step: 20, loss: 0.0060349199920892715\n",
            "step: 30, loss: 0.0005828206776641309\n",
            "step: 40, loss: 0.0032726977951824665\n",
            "step: 50, loss: 0.0026650603394955397\n",
            "step: 60, loss: 0.04455474764108658\n",
            "step: 70, loss: 0.0033291042782366276\n",
            "step: 80, loss: 0.012064323760569096\n",
            "step: 90, loss: 0.030429599806666374\n",
            "step: 100, loss: 0.0016802353784441948\n",
            "step: 110, loss: 0.07801579684019089\n",
            "step: 120, loss: 0.0015475329710170627\n",
            "step: 130, loss: 0.025006385520100594\n",
            "step: 140, loss: 0.04894730821251869\n",
            "step: 150, loss: 0.003080027876421809\n",
            "step: 160, loss: 0.010074947960674763\n",
            "step: 170, loss: 0.010611167177557945\n",
            "step: 180, loss: 0.006330089643597603\n",
            "step: 190, loss: 0.026034070178866386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.762836185819071, f1=0.775, best_f1=0.7736842105263158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009224440902471542\n",
            "step: 10, loss: 0.0035818524193018675\n",
            "step: 20, loss: 0.0322389081120491\n",
            "step: 30, loss: 0.014417641796171665\n",
            "step: 40, loss: 0.0010670052142813802\n",
            "step: 50, loss: 0.08450786024332047\n",
            "step: 60, loss: 0.018102722242474556\n",
            "step: 70, loss: 0.0037978412583470345\n",
            "step: 80, loss: 0.005778635386377573\n",
            "step: 90, loss: 0.04424852877855301\n",
            "step: 100, loss: 0.0021593377459794283\n",
            "step: 110, loss: 0.007636784575879574\n",
            "step: 120, loss: 0.006326606497168541\n",
            "step: 130, loss: 0.0013765250332653522\n",
            "step: 140, loss: 0.00239452812820673\n",
            "step: 150, loss: 0.0018472628435119987\n",
            "step: 160, loss: 0.15375296771526337\n",
            "step: 170, loss: 0.0016377074643969536\n",
            "step: 180, loss: 0.003460288979113102\n",
            "step: 190, loss: 0.0009522258187644184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7851458885941645, f1=0.7885117493472584, best_f1=0.7736842105263158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015387564897537231\n",
            "step: 10, loss: 0.0050312732346355915\n",
            "step: 20, loss: 0.0006838878034614027\n",
            "step: 30, loss: 0.0006784333381801844\n",
            "step: 40, loss: 0.0027373579796403646\n",
            "step: 50, loss: 0.004175085108727217\n",
            "step: 60, loss: 0.13474026322364807\n",
            "step: 70, loss: 0.0017281086184084415\n",
            "step: 80, loss: 0.0005657464498654008\n",
            "step: 90, loss: 0.0007813929696567357\n",
            "step: 100, loss: 0.006861054338514805\n",
            "step: 110, loss: 0.0008038937812671065\n",
            "step: 120, loss: 0.007201808970421553\n",
            "step: 130, loss: 0.0005808774149045348\n",
            "step: 140, loss: 0.10900091379880905\n",
            "step: 150, loss: 0.03977581486105919\n",
            "step: 160, loss: 0.001079052104614675\n",
            "step: 170, loss: 0.004752477165311575\n",
            "step: 180, loss: 0.035650912672281265\n",
            "step: 190, loss: 0.002484091091901064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.768079800498753, f1=0.7733990147783251, best_f1=0.7736842105263158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020769464317709208\n",
            "step: 10, loss: 0.029327569529414177\n",
            "step: 20, loss: 0.003985781222581863\n",
            "step: 30, loss: 0.0014393434394150972\n",
            "step: 40, loss: 0.07994192093610764\n",
            "step: 50, loss: 0.00845097005367279\n",
            "step: 60, loss: 0.0010559562360867858\n",
            "step: 70, loss: 0.0030165277421474457\n",
            "step: 80, loss: 0.002075537107884884\n",
            "step: 90, loss: 0.07865490019321442\n",
            "step: 100, loss: 0.04944869130849838\n",
            "step: 110, loss: 0.0074824681505560875\n",
            "step: 120, loss: 0.005063815973699093\n",
            "step: 130, loss: 0.013468514196574688\n",
            "step: 140, loss: 0.1517782062292099\n",
            "step: 150, loss: 0.003869524458423257\n",
            "step: 160, loss: 0.0014721043407917023\n",
            "step: 170, loss: 0.007010593079030514\n",
            "step: 180, loss: 0.0045075127854943275\n",
            "step: 190, loss: 0.0015861260471865535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7936507936507936, f1=0.7864583333333334, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016767893685027957\n",
            "step: 10, loss: 0.003372940933331847\n",
            "step: 20, loss: 0.012977608479559422\n",
            "step: 30, loss: 0.00032452773302793503\n",
            "step: 40, loss: 0.009503564797341824\n",
            "step: 50, loss: 0.0015789043391123414\n",
            "step: 60, loss: 0.0027158865705132484\n",
            "step: 70, loss: 0.003501972882077098\n",
            "step: 80, loss: 0.0009587202803231776\n",
            "step: 90, loss: 0.0007343521574512124\n",
            "step: 100, loss: 0.000994980102404952\n",
            "step: 110, loss: 0.005578856915235519\n",
            "step: 120, loss: 0.03186045587062836\n",
            "step: 130, loss: 0.10078322887420654\n",
            "step: 140, loss: 0.0017857248894870281\n",
            "step: 150, loss: 0.0016050926642492414\n",
            "step: 160, loss: 0.0016366984928026795\n",
            "step: 170, loss: 0.0008346604881808162\n",
            "step: 180, loss: 0.0005848421133123338\n",
            "step: 190, loss: 0.002479920629411936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7867036011080332, f1=0.7407407407407407, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002710640837904066\n",
            "step: 10, loss: 0.0005270740366540849\n",
            "step: 20, loss: 0.019609250128269196\n",
            "step: 30, loss: 0.019237304106354713\n",
            "step: 40, loss: 0.00023751991102471948\n",
            "step: 50, loss: 0.009836296550929546\n",
            "step: 60, loss: 0.0004634743381757289\n",
            "step: 70, loss: 0.0004581943212542683\n",
            "step: 80, loss: 0.0002917862148024142\n",
            "step: 90, loss: 0.00033760222140699625\n",
            "step: 100, loss: 0.0003907439822796732\n",
            "step: 110, loss: 0.004119617398828268\n",
            "step: 120, loss: 0.017667537555098534\n",
            "step: 130, loss: 0.0003508335503283888\n",
            "step: 140, loss: 0.014729891903698444\n",
            "step: 150, loss: 0.001484394771978259\n",
            "step: 160, loss: 0.0018379585817456245\n",
            "step: 170, loss: 0.000598392856772989\n",
            "step: 180, loss: 0.011469267308712006\n",
            "step: 190, loss: 0.012110378593206406\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7857142857142857, f1=0.7848101265822786, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008478126488626003\n",
            "step: 10, loss: 0.010848529636859894\n",
            "step: 20, loss: 0.045984067022800446\n",
            "step: 30, loss: 0.0026347481179982424\n",
            "step: 40, loss: 0.02109253592789173\n",
            "step: 50, loss: 0.01091738324612379\n",
            "step: 60, loss: 0.0006727995933033526\n",
            "step: 70, loss: 0.003622446209192276\n",
            "step: 80, loss: 0.0008617838029749691\n",
            "step: 90, loss: 0.0032436701003462076\n",
            "step: 100, loss: 0.0028580334037542343\n",
            "step: 110, loss: 0.0042795962654054165\n",
            "step: 120, loss: 0.0003763757413253188\n",
            "step: 130, loss: 0.00133475661277771\n",
            "step: 140, loss: 0.019516512751579285\n",
            "step: 150, loss: 0.0006411655340343714\n",
            "step: 160, loss: 0.0004470777930691838\n",
            "step: 170, loss: 0.001445556408725679\n",
            "step: 180, loss: 0.0004504169919528067\n",
            "step: 190, loss: 0.0024635076988488436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7826086956521741, f1=0.7857142857142857, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007720035500824451\n",
            "step: 10, loss: 0.0014550051419064403\n",
            "step: 20, loss: 0.0015467716148123145\n",
            "step: 30, loss: 0.021450646221637726\n",
            "step: 40, loss: 0.0008209163788706064\n",
            "step: 50, loss: 0.06689487397670746\n",
            "step: 60, loss: 0.0008625455666333437\n",
            "step: 70, loss: 0.0062404656782746315\n",
            "step: 80, loss: 0.00571681372821331\n",
            "step: 90, loss: 0.002495924709364772\n",
            "step: 100, loss: 0.006461888551712036\n",
            "step: 110, loss: 0.00028641981771215796\n",
            "step: 120, loss: 0.008938880637288094\n",
            "step: 130, loss: 0.000406875042244792\n",
            "step: 140, loss: 0.0006114545976743102\n",
            "step: 150, loss: 0.0008101603016257286\n",
            "step: 160, loss: 0.0016926416428759694\n",
            "step: 170, loss: 0.0005127215408720076\n",
            "step: 180, loss: 0.0008989848429337144\n",
            "step: 190, loss: 0.002763367723673582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8031496062992125, f1=0.7904509283819628, best_f1=0.7904509283819628\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008540148846805096\n",
            "step: 10, loss: 0.0004362949694041163\n",
            "step: 20, loss: 0.0013684326549991965\n",
            "step: 30, loss: 0.001888272468931973\n",
            "step: 40, loss: 0.002644808031618595\n",
            "step: 50, loss: 0.0008776768809184432\n",
            "step: 60, loss: 0.0035916611086577177\n",
            "step: 70, loss: 0.00023623455490451306\n",
            "step: 80, loss: 0.0003540850302670151\n",
            "step: 90, loss: 0.00856061838567257\n",
            "step: 100, loss: 0.015182011760771275\n",
            "step: 110, loss: 0.001729561248794198\n",
            "step: 120, loss: 0.001203562831506133\n",
            "step: 130, loss: 0.018960997462272644\n",
            "step: 140, loss: 0.007048031780868769\n",
            "step: 150, loss: 0.003057815134525299\n",
            "step: 160, loss: 0.008880850858986378\n",
            "step: 170, loss: 0.006989878136664629\n",
            "step: 180, loss: 0.0010998252546414733\n",
            "step: 190, loss: 0.00032462168019264936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7893333333333333, f1=0.7804878048780487, best_f1=0.7904509283819628\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004858469124883413\n",
            "step: 10, loss: 0.0051247538067400455\n",
            "step: 20, loss: 0.03386039286851883\n",
            "step: 30, loss: 0.0004538467328529805\n",
            "step: 40, loss: 0.0003224034735467285\n",
            "step: 50, loss: 0.025390267372131348\n",
            "step: 60, loss: 0.049020662903785706\n",
            "step: 70, loss: 0.007104829885065556\n",
            "step: 80, loss: 0.00030117720598354936\n",
            "step: 90, loss: 0.001141810673289001\n",
            "step: 100, loss: 0.001017016707919538\n",
            "step: 110, loss: 0.0007602543337270617\n",
            "step: 120, loss: 0.0005249206442385912\n",
            "step: 130, loss: 0.0001783241459634155\n",
            "step: 140, loss: 0.0029609163757413626\n",
            "step: 150, loss: 0.0003736735670827329\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.00024624139769002795\n",
            "step: 170, loss: 0.0002522616705391556\n",
            "step: 180, loss: 0.0026494867634028196\n",
            "step: 190, loss: 0.0001513738534413278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7937336814621411, f1=0.7937336814621411, best_f1=0.7904509283819628\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 234.99it/s]\n",
            "load_f1 = 0.7037974683544304\n",
            "real_f1 = 0.6947890818858561\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 270.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13e72f1-1b00-4a55-f5e2-41a87b96bb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 392kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 257kB/s] \n",
            "Downloading: 100% 440M/440M [00:06<00:00, 70.4MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.640425443649292\n",
            "step: 10, loss: 0.36464306712150574\n",
            "step: 20, loss: 0.29676830768585205\n",
            "step: 30, loss: 0.4180218279361725\n",
            "step: 40, loss: 0.28286653757095337\n",
            "step: 50, loss: 0.2592591643333435\n",
            "step: 60, loss: 0.2437029927968979\n",
            "step: 70, loss: 0.374694287776947\n",
            "step: 80, loss: 0.408625066280365\n",
            "step: 90, loss: 0.2608788013458252\n",
            "step: 100, loss: 0.27509233355522156\n",
            "step: 110, loss: 0.31629085540771484\n",
            "step: 120, loss: 0.20698101818561554\n",
            "step: 130, loss: 0.06288129836320877\n",
            "step: 140, loss: 0.2010435163974762\n",
            "step: 150, loss: 0.22700345516204834\n",
            "step: 160, loss: 0.14270886778831482\n",
            "step: 170, loss: 0.24822698533535004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7111111111111111, f1=0.6847290640394088, best_f1=0.6847290640394088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08808047324419022\n",
            "step: 10, loss: 0.3216687738895416\n",
            "step: 20, loss: 0.0553598552942276\n",
            "step: 30, loss: 0.24580347537994385\n",
            "step: 40, loss: 0.06005960330367088\n",
            "step: 50, loss: 0.2498127520084381\n",
            "step: 60, loss: 0.16365812718868256\n",
            "step: 70, loss: 0.09844357520341873\n",
            "step: 80, loss: 0.0950663834810257\n",
            "step: 90, loss: 0.11449076980352402\n",
            "step: 100, loss: 0.1245528981089592\n",
            "step: 110, loss: 0.07515353709459305\n",
            "step: 120, loss: 0.2598631978034973\n",
            "step: 130, loss: 0.10804972052574158\n",
            "step: 140, loss: 0.24781234562397003\n",
            "step: 150, loss: 0.15086403489112854\n",
            "step: 160, loss: 0.17991574108600616\n",
            "step: 170, loss: 0.03237931802868843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7606263982102909, f1=0.7578475336322871, best_f1=0.7578475336322871\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27123069763183594\n",
            "step: 10, loss: 0.10132894665002823\n",
            "step: 20, loss: 0.016253415495157242\n",
            "step: 30, loss: 0.19009913504123688\n",
            "step: 40, loss: 0.12203672528266907\n",
            "step: 50, loss: 0.016782479360699654\n",
            "step: 60, loss: 0.1022065058350563\n",
            "step: 70, loss: 0.05857080593705177\n",
            "step: 80, loss: 0.0359523631632328\n",
            "step: 90, loss: 0.20311109721660614\n",
            "step: 100, loss: 0.06973825395107269\n",
            "step: 110, loss: 0.1035359725356102\n",
            "step: 120, loss: 0.0359170027077198\n",
            "step: 130, loss: 0.12423214316368103\n",
            "step: 140, loss: 0.09660860151052475\n",
            "step: 150, loss: 0.19522714614868164\n",
            "step: 160, loss: 0.08833807706832886\n",
            "step: 170, loss: 0.2872662842273712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.801007556675063, f1=0.7849999999999999, best_f1=0.7849999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02112256921827793\n",
            "step: 10, loss: 0.03379717841744423\n",
            "step: 20, loss: 0.0062301550060510635\n",
            "step: 30, loss: 0.16711221635341644\n",
            "step: 40, loss: 0.003283505793660879\n",
            "step: 50, loss: 0.14456653594970703\n",
            "step: 60, loss: 0.19052892923355103\n",
            "step: 70, loss: 0.005809340626001358\n",
            "step: 80, loss: 0.09004426747560501\n",
            "step: 90, loss: 0.08952800184488297\n",
            "step: 100, loss: 0.1750626266002655\n",
            "step: 110, loss: 0.011721877381205559\n",
            "step: 120, loss: 0.005194612313061953\n",
            "step: 130, loss: 0.08554442226886749\n",
            "step: 140, loss: 0.02241520583629608\n",
            "step: 150, loss: 0.07437149435281754\n",
            "step: 160, loss: 0.0740833580493927\n",
            "step: 170, loss: 0.17260578274726868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8080808080808081, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1205861046910286\n",
            "step: 10, loss: 0.0423269085586071\n",
            "step: 20, loss: 0.08176124840974808\n",
            "step: 30, loss: 0.05858882516622543\n",
            "step: 40, loss: 0.01377347856760025\n",
            "step: 50, loss: 0.031422555446624756\n",
            "step: 60, loss: 0.0675254836678505\n",
            "step: 70, loss: 0.32202979922294617\n",
            "step: 80, loss: 0.06953074038028717\n",
            "step: 90, loss: 0.09502141177654266\n",
            "step: 100, loss: 0.09355907887220383\n",
            "step: 110, loss: 0.032473914325237274\n",
            "step: 120, loss: 0.028169983997941017\n",
            "step: 130, loss: 0.08470728993415833\n",
            "step: 140, loss: 0.014817692339420319\n",
            "step: 150, loss: 0.05729113146662712\n",
            "step: 160, loss: 0.038990821689367294\n",
            "step: 170, loss: 0.01716211996972561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8020050125313284, f1=0.800982800982801, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008861282840371132\n",
            "step: 10, loss: 0.007331062573939562\n",
            "step: 20, loss: 0.005659463815391064\n",
            "step: 30, loss: 0.04922684282064438\n",
            "step: 40, loss: 0.09749966859817505\n",
            "step: 50, loss: 0.16212911903858185\n",
            "step: 60, loss: 0.05973849818110466\n",
            "step: 70, loss: 0.1184108704328537\n",
            "step: 80, loss: 0.09582261741161346\n",
            "step: 90, loss: 0.0353001169860363\n",
            "step: 100, loss: 0.03588356822729111\n",
            "step: 110, loss: 0.008631155826151371\n",
            "step: 120, loss: 0.041376084089279175\n",
            "step: 130, loss: 0.014980344101786613\n",
            "step: 140, loss: 0.10610251128673553\n",
            "step: 150, loss: 0.0699765682220459\n",
            "step: 160, loss: 0.1659957766532898\n",
            "step: 170, loss: 0.10663378238677979\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8058968058968059, f1=0.788177339901478, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0158421378582716\n",
            "step: 10, loss: 0.04253043234348297\n",
            "step: 20, loss: 0.0046122511848807335\n",
            "step: 30, loss: 0.006156083662062883\n",
            "step: 40, loss: 0.010134400799870491\n",
            "step: 50, loss: 0.008112738840281963\n",
            "step: 60, loss: 0.017370743677020073\n",
            "step: 70, loss: 0.0012245996622368693\n",
            "step: 80, loss: 0.0473036915063858\n",
            "step: 90, loss: 0.002301885513588786\n",
            "step: 100, loss: 0.000728354905731976\n",
            "step: 110, loss: 0.046345315873622894\n",
            "step: 120, loss: 0.014037612825632095\n",
            "step: 130, loss: 0.15387707948684692\n",
            "step: 140, loss: 0.0023108827881515026\n",
            "step: 150, loss: 0.013552024960517883\n",
            "step: 160, loss: 0.003927096724510193\n",
            "step: 170, loss: 0.02753981202840805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8, f1=0.8146341463414632, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008541785180568695\n",
            "step: 10, loss: 0.00887291133403778\n",
            "step: 20, loss: 0.0015618398319929838\n",
            "step: 30, loss: 0.03615884110331535\n",
            "step: 40, loss: 0.0005420444067567587\n",
            "step: 50, loss: 0.0018827990861609578\n",
            "step: 60, loss: 0.009001229889690876\n",
            "step: 70, loss: 0.09498290717601776\n",
            "step: 80, loss: 0.013350420631468296\n",
            "step: 90, loss: 0.005747393239289522\n",
            "step: 100, loss: 0.06788115203380585\n",
            "step: 110, loss: 0.11744029819965363\n",
            "step: 120, loss: 0.01508792769163847\n",
            "step: 130, loss: 0.013921888545155525\n",
            "step: 140, loss: 0.0012934254482388496\n",
            "step: 150, loss: 0.04722415283322334\n",
            "step: 160, loss: 0.02451995760202408\n",
            "step: 170, loss: 0.016245892271399498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.782608695652174, f1=0.7872860635696822, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024195564910769463\n",
            "step: 10, loss: 0.0020172998774796724\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.12025199830532074\n",
            "step: 30, loss: 0.10921931266784668\n",
            "step: 40, loss: 0.008775515481829643\n",
            "step: 50, loss: 0.0023021406959742308\n",
            "step: 60, loss: 0.03245590627193451\n",
            "step: 70, loss: 0.0024480638094246387\n",
            "step: 80, loss: 0.0013553125318139791\n",
            "step: 90, loss: 0.022250743582844734\n",
            "step: 100, loss: 0.01817184127867222\n",
            "step: 110, loss: 0.00968507956713438\n",
            "step: 120, loss: 0.00925581157207489\n",
            "step: 130, loss: 0.06531253457069397\n",
            "step: 140, loss: 0.056704673916101456\n",
            "step: 150, loss: 0.004735163412988186\n",
            "step: 160, loss: 0.10566037893295288\n",
            "step: 170, loss: 0.0040480708703398705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8188585607940446, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.032431457191705704\n",
            "step: 10, loss: 0.00041049355058930814\n",
            "step: 20, loss: 0.09370116889476776\n",
            "step: 30, loss: 0.015316246077418327\n",
            "step: 40, loss: 0.06962040811777115\n",
            "step: 50, loss: 0.06360830366611481\n",
            "step: 60, loss: 0.0016736435936763883\n",
            "step: 70, loss: 0.015986373648047447\n",
            "step: 80, loss: 0.0006670069415122271\n",
            "step: 90, loss: 0.005609242711216211\n",
            "step: 100, loss: 0.0004966098349541426\n",
            "step: 110, loss: 0.028662962839007378\n",
            "step: 120, loss: 0.044800370931625366\n",
            "step: 130, loss: 0.0004997575888410211\n",
            "step: 140, loss: 0.012247418984770775\n",
            "step: 150, loss: 0.0014069024473428726\n",
            "step: 160, loss: 0.0100680161267519\n",
            "step: 170, loss: 0.0017963291611522436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.78743961352657, f1=0.7926267281105991, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0170064028352499\n",
            "step: 10, loss: 0.013949775137007236\n",
            "step: 20, loss: 0.0022287415340542793\n",
            "step: 30, loss: 0.007917969487607479\n",
            "step: 40, loss: 0.050529927015304565\n",
            "step: 50, loss: 0.0018338914960622787\n",
            "step: 60, loss: 0.007723451592028141\n",
            "step: 70, loss: 0.0012547045480459929\n",
            "step: 80, loss: 0.0009709944133646786\n",
            "step: 90, loss: 0.0010811520041897893\n",
            "step: 100, loss: 0.004459843970835209\n",
            "step: 110, loss: 0.027343224734067917\n",
            "step: 120, loss: 0.0006666685803793371\n",
            "step: 130, loss: 0.0002838062064256519\n",
            "step: 140, loss: 0.006192624103277922\n",
            "step: 150, loss: 0.01352771557867527\n",
            "step: 160, loss: 0.00465787947177887\n",
            "step: 170, loss: 0.0009307475993409753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7978142076502732, f1=0.7989276139410187, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005731210112571716\n",
            "step: 10, loss: 0.0016081786016002297\n",
            "step: 20, loss: 0.22761811316013336\n",
            "step: 30, loss: 0.002518133958801627\n",
            "step: 40, loss: 0.001738753286190331\n",
            "step: 50, loss: 0.0017126458697021008\n",
            "step: 60, loss: 0.12661947309970856\n",
            "step: 70, loss: 0.07441601902246475\n",
            "step: 80, loss: 0.0016514264279976487\n",
            "step: 90, loss: 0.0024034332018345594\n",
            "step: 100, loss: 0.002013436984270811\n",
            "step: 110, loss: 0.002415430499240756\n",
            "step: 120, loss: 0.0693492591381073\n",
            "step: 130, loss: 0.017883604392409325\n",
            "step: 140, loss: 0.030673304572701454\n",
            "step: 150, loss: 0.009888364002108574\n",
            "step: 160, loss: 0.003361500333994627\n",
            "step: 170, loss: 0.0007085426477715373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7884615384615384, f1=0.7852193995381063, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020361769944429398\n",
            "step: 10, loss: 0.0005622526514343917\n",
            "step: 20, loss: 0.0008062487468123436\n",
            "step: 30, loss: 0.0008197668939828873\n",
            "step: 40, loss: 0.0033037762623280287\n",
            "step: 50, loss: 0.001102752285078168\n",
            "step: 60, loss: 0.034499961882829666\n",
            "step: 70, loss: 0.003960344474762678\n",
            "step: 80, loss: 0.0037885443307459354\n",
            "step: 90, loss: 0.0004663288127630949\n",
            "step: 100, loss: 0.004847235046327114\n",
            "step: 110, loss: 0.001187362358905375\n",
            "step: 120, loss: 0.04038245603442192\n",
            "step: 130, loss: 0.006295311730355024\n",
            "step: 140, loss: 0.0019866900984197855\n",
            "step: 150, loss: 0.0016934947343543172\n",
            "step: 160, loss: 0.0012557642767205834\n",
            "step: 170, loss: 0.0026699139270931482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.793733681462141, f1=0.8080808080808081, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009152035927399993\n",
            "step: 10, loss: 0.0004187474260106683\n",
            "step: 20, loss: 0.002668824279680848\n",
            "step: 30, loss: 0.0008252033730968833\n",
            "step: 40, loss: 0.0005627803038805723\n",
            "step: 50, loss: 0.0015249267453327775\n",
            "step: 60, loss: 0.0014917092630639672\n",
            "step: 70, loss: 0.0010026327800005674\n",
            "step: 80, loss: 0.004271848127245903\n",
            "step: 90, loss: 0.0004266278410796076\n",
            "step: 100, loss: 0.0006115606520324945\n",
            "step: 110, loss: 0.012752528302371502\n",
            "step: 120, loss: 0.016350045800209045\n",
            "step: 130, loss: 0.0008206022321246564\n",
            "step: 140, loss: 0.0011902559781447053\n",
            "step: 150, loss: 0.0031701638363301754\n",
            "step: 160, loss: 0.0007176901563070714\n",
            "step: 170, loss: 0.0004714620881713927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7969151670951157, f1=0.810126582278481, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002587710914667696\n",
            "step: 10, loss: 0.003472908865660429\n",
            "step: 20, loss: 0.23894155025482178\n",
            "step: 30, loss: 0.0003236150078009814\n",
            "step: 40, loss: 0.00435674050822854\n",
            "step: 50, loss: 0.004451464395970106\n",
            "step: 60, loss: 0.014395480044186115\n",
            "step: 70, loss: 0.0003350242041051388\n",
            "step: 80, loss: 0.01665468141436577\n",
            "step: 90, loss: 0.0008791770087555051\n",
            "step: 100, loss: 0.001054757391102612\n",
            "step: 110, loss: 0.0002822311071213335\n",
            "step: 120, loss: 0.0024151380639523268\n",
            "step: 130, loss: 0.0010921036591753364\n",
            "step: 140, loss: 0.00040665658889338374\n",
            "step: 150, loss: 0.000274669990176335\n",
            "step: 160, loss: 0.0022677334491163492\n",
            "step: 170, loss: 0.016369590535759926\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7926509186351706, f1=0.8071979434447302, best_f1=0.8\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 220.46it/s]\n",
            "load_f1 = 0.3802521008403361\n",
            "real_f1 = 0.34908389585342336\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 177.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8375d66c-fec2-4c7d-fc50-b922e63afa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6053374409675598\n",
            "step: 10, loss: 0.6146105527877808\n",
            "step: 20, loss: 0.3437322974205017\n",
            "step: 30, loss: 0.06551893800497055\n",
            "step: 40, loss: 0.3016244173049927\n",
            "step: 50, loss: 0.06052976846694946\n",
            "step: 60, loss: 0.029285192489624023\n",
            "step: 70, loss: 0.17067338526248932\n",
            "step: 80, loss: 0.03746478632092476\n",
            "step: 90, loss: 0.15609967708587646\n",
            "step: 100, loss: 0.006284716073423624\n",
            "step: 110, loss: 0.17186759412288666\n",
            "step: 120, loss: 0.015173711813986301\n",
            "step: 130, loss: 0.011768899857997894\n",
            "step: 140, loss: 0.004275006707757711\n",
            "step: 150, loss: 0.01676623336970806\n",
            "step: 160, loss: 0.00623351102694869\n",
            "step: 170, loss: 0.13712593913078308\n",
            "step: 180, loss: 0.040428876876831055\n",
            "step: 190, loss: 0.08946597576141357\n",
            "step: 200, loss: 0.05558205023407936\n",
            "step: 210, loss: 0.00956965796649456\n",
            "step: 220, loss: 0.030332257971167564\n",
            "step: 230, loss: 0.020782168954610825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.967741935483871, f1=0.968609865470852, best_f1=0.968609865470852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011871227994561195\n",
            "step: 10, loss: 0.00568348215892911\n",
            "step: 20, loss: 0.17922765016555786\n",
            "step: 30, loss: 0.1330866515636444\n",
            "step: 40, loss: 0.08145634829998016\n",
            "step: 50, loss: 0.012072310782968998\n",
            "step: 60, loss: 0.006608330179005861\n",
            "step: 70, loss: 0.0018582274205982685\n",
            "step: 80, loss: 0.007440540008246899\n",
            "step: 90, loss: 0.01149833481758833\n",
            "step: 100, loss: 0.012991479597985744\n",
            "step: 110, loss: 0.07665300369262695\n",
            "step: 120, loss: 0.13446640968322754\n",
            "step: 130, loss: 0.012161615304648876\n",
            "step: 140, loss: 0.0035687200725078583\n",
            "step: 150, loss: 0.012224326841533184\n",
            "step: 160, loss: 0.02657163515686989\n",
            "step: 170, loss: 0.02839609608054161\n",
            "step: 180, loss: 0.119230255484581\n",
            "step: 190, loss: 0.005122151225805283\n",
            "step: 200, loss: 0.002547828247770667\n",
            "step: 210, loss: 0.0011976842069998384\n",
            "step: 220, loss: 0.1263139545917511\n",
            "step: 230, loss: 0.007551257498562336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9765363128491621, f1=0.9765363128491621, best_f1=0.9765363128491621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003866876708343625\n",
            "step: 10, loss: 0.014970979653298855\n",
            "step: 20, loss: 0.014891916885972023\n",
            "step: 30, loss: 0.11962025612592697\n",
            "step: 40, loss: 0.022962123155593872\n",
            "step: 50, loss: 0.01839856244623661\n",
            "step: 60, loss: 0.0021848194301128387\n",
            "step: 70, loss: 0.027046535164117813\n",
            "step: 80, loss: 0.0016367124626412988\n",
            "step: 90, loss: 0.021430447697639465\n",
            "step: 100, loss: 0.0006654404569417238\n",
            "step: 110, loss: 0.011439284309744835\n",
            "step: 120, loss: 0.03433583676815033\n",
            "step: 130, loss: 0.0035791632253676653\n",
            "step: 140, loss: 0.009579215198755264\n",
            "step: 150, loss: 0.013620460405945778\n",
            "step: 160, loss: 0.01998831517994404\n",
            "step: 170, loss: 0.011213050223886967\n",
            "step: 180, loss: 0.013181978836655617\n",
            "step: 190, loss: 0.003414721693843603\n",
            "step: 200, loss: 0.04494541510939598\n",
            "step: 210, loss: 0.01058848574757576\n",
            "step: 220, loss: 0.0017989532789215446\n",
            "step: 230, loss: 0.022546684369444847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9766925638179801, f1=0.9733924611973392, best_f1=0.9733924611973392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006034061079844832\n",
            "step: 10, loss: 0.00416185799986124\n",
            "step: 20, loss: 0.0015676859766244888\n",
            "step: 30, loss: 0.01713458076119423\n",
            "step: 40, loss: 0.004852945450693369\n",
            "step: 50, loss: 0.001396430772729218\n",
            "step: 60, loss: 0.0008293335558846593\n",
            "step: 70, loss: 0.007533080410212278\n",
            "step: 80, loss: 0.0005418925429694355\n",
            "step: 90, loss: 0.00337335211224854\n",
            "step: 100, loss: 0.0019968892447650433\n",
            "step: 110, loss: 0.0006035075639374554\n",
            "step: 120, loss: 0.008701002225279808\n",
            "step: 130, loss: 0.0016592523315921426\n",
            "step: 140, loss: 0.04237382113933563\n",
            "step: 150, loss: 0.36763161420822144\n",
            "step: 160, loss: 0.0009474006947129965\n",
            "step: 170, loss: 0.02496352232992649\n",
            "step: 180, loss: 0.0006357650272548199\n",
            "step: 190, loss: 0.002516058273613453\n",
            "step: 200, loss: 0.002100803656503558\n",
            "step: 210, loss: 0.019273988902568817\n",
            "step: 220, loss: 0.0008612936362624168\n",
            "step: 230, loss: 0.00188444706145674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9796839729119639, f1=0.9773242630385486, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018891363870352507\n",
            "step: 10, loss: 0.000555063015781343\n",
            "step: 20, loss: 0.0012408395996317267\n",
            "step: 30, loss: 0.0003289613814558834\n",
            "step: 40, loss: 0.0006320315878838301\n",
            "step: 50, loss: 0.0005481831612996757\n",
            "step: 60, loss: 0.11956105381250381\n",
            "step: 70, loss: 0.0011801904765889049\n",
            "step: 80, loss: 0.0008840853697620332\n",
            "step: 90, loss: 0.0015569409588351846\n",
            "step: 100, loss: 0.0007523809326812625\n",
            "step: 110, loss: 0.016943734139204025\n",
            "step: 120, loss: 0.00027209732797928154\n",
            "step: 130, loss: 0.030854947865009308\n",
            "step: 140, loss: 0.04638386145234108\n",
            "step: 150, loss: 0.02561149001121521\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.037036821246147156\n",
            "step: 170, loss: 0.01785544864833355\n",
            "step: 180, loss: 0.0010926563991233706\n",
            "step: 190, loss: 0.08224419504404068\n",
            "step: 200, loss: 0.02433001436293125\n",
            "step: 210, loss: 0.009103837423026562\n",
            "step: 220, loss: 0.00219330913387239\n",
            "step: 230, loss: 0.0016079054912552238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9798657718120806, f1=0.9776785714285714, best_f1=0.9776785714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03200594335794449\n",
            "step: 10, loss: 0.001191290677525103\n",
            "step: 20, loss: 0.0021242534276098013\n",
            "step: 30, loss: 0.000940259953495115\n",
            "step: 40, loss: 0.00023548306489828974\n",
            "step: 50, loss: 0.0015306960558518767\n",
            "step: 60, loss: 0.00015269404684659094\n",
            "step: 70, loss: 0.0011224447516724467\n",
            "step: 80, loss: 0.0005380100919865072\n",
            "step: 90, loss: 0.003851603949442506\n",
            "step: 100, loss: 0.0005615978152491152\n",
            "step: 110, loss: 0.0002559673448558897\n",
            "step: 120, loss: 0.002718281000852585\n",
            "step: 130, loss: 0.00020719628082588315\n",
            "step: 140, loss: 0.00023529892496298999\n",
            "step: 150, loss: 0.002607227535918355\n",
            "step: 160, loss: 0.0001462198415538296\n",
            "step: 170, loss: 0.00024681168724782765\n",
            "step: 180, loss: 0.010699381120502949\n",
            "step: 190, loss: 0.15328897535800934\n",
            "step: 200, loss: 0.014351720921695232\n",
            "step: 210, loss: 0.01000911183655262\n",
            "step: 220, loss: 0.0032006765250116587\n",
            "step: 230, loss: 0.04344252869486809\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9798657718120806, f1=0.9798657718120806, best_f1=0.9776785714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022511251736432314\n",
            "step: 10, loss: 0.00016610685270279646\n",
            "step: 20, loss: 0.006873412057757378\n",
            "step: 30, loss: 0.001595278619788587\n",
            "step: 40, loss: 0.005675031803548336\n",
            "step: 50, loss: 0.003424168098717928\n",
            "step: 60, loss: 0.025923823937773705\n",
            "step: 70, loss: 0.004431607201695442\n",
            "step: 80, loss: 0.0011095762019976974\n",
            "step: 90, loss: 0.0015393947251141071\n",
            "step: 100, loss: 0.0001528822467662394\n",
            "step: 110, loss: 0.0009376582456752658\n",
            "step: 120, loss: 0.00016338209388777614\n",
            "step: 130, loss: 0.025237444788217545\n",
            "step: 140, loss: 0.0008682177867740393\n",
            "step: 150, loss: 0.00027117773424834013\n",
            "step: 160, loss: 0.07134511321783066\n",
            "step: 170, loss: 0.0008927475428208709\n",
            "step: 180, loss: 0.0005150139913894236\n",
            "step: 190, loss: 0.00016507676627952605\n",
            "step: 200, loss: 0.050537899136543274\n",
            "step: 210, loss: 0.00010159335943171754\n",
            "step: 220, loss: 0.0008390299044549465\n",
            "step: 230, loss: 0.000625244399998337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9798206278026906, f1=0.9743589743589743, best_f1=0.9776785714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00048057347885333\n",
            "step: 10, loss: 0.004627116024494171\n",
            "step: 20, loss: 0.00033056619577109814\n",
            "step: 30, loss: 0.00014665364869870245\n",
            "step: 40, loss: 0.006921771448105574\n",
            "step: 50, loss: 0.0001929294812725857\n",
            "step: 60, loss: 0.0009534381679259241\n",
            "step: 70, loss: 0.0004208840546198189\n",
            "step: 80, loss: 0.00018153058772441\n",
            "step: 90, loss: 0.0001419267791789025\n",
            "step: 100, loss: 0.00035176915116608143\n",
            "step: 110, loss: 0.00011942952551180497\n",
            "step: 120, loss: 0.028408532962203026\n",
            "step: 130, loss: 0.001482302090153098\n",
            "step: 140, loss: 7.713901868555695e-05\n",
            "step: 150, loss: 6.606936949538067e-05\n",
            "step: 160, loss: 0.00046979356557130814\n",
            "step: 170, loss: 0.00037906557554379106\n",
            "step: 180, loss: 0.022908810526132584\n",
            "step: 190, loss: 0.0006748242885805666\n",
            "step: 200, loss: 0.009833923541009426\n",
            "step: 210, loss: 0.00021246739197522402\n",
            "step: 220, loss: 0.0004866274248342961\n",
            "step: 230, loss: 0.00022395298583433032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9808773903262092, f1=0.9787234042553192, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001317952701356262\n",
            "step: 10, loss: 0.0027439480181783438\n",
            "step: 20, loss: 0.00029709652881138027\n",
            "step: 30, loss: 0.0006273390026763082\n",
            "step: 40, loss: 0.029013026505708694\n",
            "step: 50, loss: 5.717031308449805e-05\n",
            "step: 60, loss: 0.00017639902944210917\n",
            "step: 70, loss: 0.00044875775347463787\n",
            "step: 80, loss: 0.0005672781262546778\n",
            "step: 90, loss: 0.003090356942266226\n",
            "step: 100, loss: 0.004367989953607321\n",
            "step: 110, loss: 0.00011298301978968084\n",
            "step: 120, loss: 0.00010970656148856506\n",
            "step: 130, loss: 0.000726629514247179\n",
            "step: 140, loss: 0.00014491809997707605\n",
            "step: 150, loss: 0.0002540128771215677\n",
            "step: 160, loss: 0.000108841632027179\n",
            "step: 170, loss: 0.0015202303184196353\n",
            "step: 180, loss: 0.00013743787712883204\n",
            "step: 190, loss: 0.00013043885701335967\n",
            "step: 200, loss: 0.007790122646838427\n",
            "step: 210, loss: 0.0005488385213539004\n",
            "step: 220, loss: 0.00013909893459640443\n",
            "step: 230, loss: 0.0033089173957705498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9786276715410572, f1=0.976324689966178, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006625414825975895\n",
            "step: 10, loss: 0.0012624228838831186\n",
            "step: 20, loss: 0.03940702974796295\n",
            "step: 30, loss: 0.00022846914362162352\n",
            "step: 40, loss: 0.00013556575868278742\n",
            "step: 50, loss: 0.0008999088895507157\n",
            "step: 60, loss: 0.00035904342075809836\n",
            "step: 70, loss: 0.0017998106777668\n",
            "step: 80, loss: 0.005591122899204493\n",
            "step: 90, loss: 0.0018145543290302157\n",
            "step: 100, loss: 0.00031992499134503305\n",
            "step: 110, loss: 0.0009772366611286998\n",
            "step: 120, loss: 0.0016109684947878122\n",
            "step: 130, loss: 0.007173743098974228\n",
            "step: 140, loss: 0.032633889466524124\n",
            "step: 150, loss: 0.009106796234846115\n",
            "step: 160, loss: 0.00016300316201522946\n",
            "step: 170, loss: 8.108287147479132e-05\n",
            "step: 180, loss: 0.00022230346803553402\n",
            "step: 190, loss: 0.0003741636755876243\n",
            "step: 200, loss: 0.0001261454017367214\n",
            "step: 210, loss: 0.0001147369621321559\n",
            "step: 220, loss: 7.359294249908999e-05\n",
            "step: 230, loss: 0.0013565205736085773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9777777777777777, f1=0.9733924611973392, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017685715574771166\n",
            "step: 10, loss: 0.0001621116098249331\n",
            "step: 20, loss: 0.0002839474764186889\n",
            "step: 30, loss: 0.0007934669847600162\n",
            "step: 40, loss: 0.0001500405342085287\n",
            "step: 50, loss: 0.00011999505659332499\n",
            "step: 60, loss: 0.0009250350994989276\n",
            "step: 70, loss: 0.00018025122699327767\n",
            "step: 80, loss: 4.6090473915683106e-05\n",
            "step: 90, loss: 0.0002489350736141205\n",
            "step: 100, loss: 0.00019138514471706003\n",
            "step: 110, loss: 0.0002735552843660116\n",
            "step: 120, loss: 9.958171722246334e-05\n",
            "step: 130, loss: 6.90538072376512e-05\n",
            "step: 140, loss: 0.000734457396902144\n",
            "step: 150, loss: 0.005221018102020025\n",
            "step: 160, loss: 5.4915253713261336e-05\n",
            "step: 170, loss: 0.000211091639357619\n",
            "step: 180, loss: 0.07805318385362625\n",
            "step: 190, loss: 0.0002023393171839416\n",
            "step: 200, loss: 0.0017870916053652763\n",
            "step: 210, loss: 5.285918086883612e-05\n",
            "step: 220, loss: 6.30623399047181e-05\n",
            "step: 230, loss: 7.411904516629875e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.978675645342312, f1=0.9774774774774775, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.6306118494831026e-05\n",
            "step: 10, loss: 5.025501741329208e-05\n",
            "step: 20, loss: 9.228996350429952e-05\n",
            "step: 30, loss: 0.00012082609464414418\n",
            "step: 40, loss: 0.005786811001598835\n",
            "step: 50, loss: 0.003277644282206893\n",
            "step: 60, loss: 0.0003116519656032324\n",
            "step: 70, loss: 0.001058348105289042\n",
            "step: 80, loss: 0.0001557936193421483\n",
            "step: 90, loss: 8.535351662430912e-05\n",
            "step: 100, loss: 0.0010528855491429567\n",
            "step: 110, loss: 5.038883682573214e-05\n",
            "step: 120, loss: 3.779927646974102e-05\n",
            "step: 130, loss: 4.476132744457573e-05\n",
            "step: 140, loss: 9.518602746538818e-05\n",
            "step: 150, loss: 6.496651622001082e-05\n",
            "step: 160, loss: 4.297709165257402e-05\n",
            "step: 170, loss: 0.00028169102733954787\n",
            "step: 180, loss: 0.003998226020485163\n",
            "step: 190, loss: 0.00042549826321192086\n",
            "step: 200, loss: 4.46634876425378e-05\n",
            "step: 210, loss: 0.0001625636505195871\n",
            "step: 220, loss: 5.313827932695858e-05\n",
            "step: 230, loss: 0.02173270471394062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9797752808988766, f1=0.9785794813979707, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018762370746117085\n",
            "step: 10, loss: 9.237712220055982e-05\n",
            "step: 20, loss: 0.0010723391314968467\n",
            "step: 30, loss: 9.695962944533676e-05\n",
            "step: 40, loss: 0.00024374463828280568\n",
            "step: 50, loss: 7.19161907909438e-05\n",
            "step: 60, loss: 0.0003230817092116922\n",
            "step: 70, loss: 3.773512435145676e-05\n",
            "step: 80, loss: 4.6860692236805335e-05\n",
            "step: 90, loss: 0.00201267353259027\n",
            "step: 100, loss: 6.134852446848527e-05\n",
            "step: 110, loss: 0.004229644313454628\n",
            "step: 120, loss: 0.0002513997023925185\n",
            "step: 130, loss: 4.056663965457119e-05\n",
            "step: 140, loss: 9.668890561442822e-05\n",
            "step: 150, loss: 3.677840868476778e-05\n",
            "step: 160, loss: 0.0003992781275883317\n",
            "step: 170, loss: 7.682353316340595e-05\n",
            "step: 180, loss: 8.369241550099105e-05\n",
            "step: 190, loss: 4.7508045099675655e-05\n",
            "step: 200, loss: 9.644261444918811e-05\n",
            "step: 210, loss: 3.062452015001327e-05\n",
            "step: 220, loss: 0.026412123814225197\n",
            "step: 230, loss: 4.959933357895352e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9755011135857461, f1=0.9744160177975528, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.048075425089337e-05\n",
            "step: 10, loss: 0.002482314594089985\n",
            "step: 20, loss: 7.465500675607473e-05\n",
            "step: 30, loss: 0.0001279564603464678\n",
            "step: 40, loss: 0.00022018910385668278\n",
            "step: 50, loss: 0.00017974077491089702\n",
            "step: 60, loss: 4.6427205234067515e-05\n",
            "step: 70, loss: 8.116142271319404e-05\n",
            "step: 80, loss: 5.041018084739335e-05\n",
            "step: 90, loss: 7.551945600425825e-05\n",
            "step: 100, loss: 4.4707503548124805e-05\n",
            "step: 110, loss: 0.08031229674816132\n",
            "step: 120, loss: 2.8423308322089724e-05\n",
            "step: 130, loss: 0.0002219823800260201\n",
            "step: 140, loss: 9.438308916287497e-05\n",
            "step: 150, loss: 6.168008258100599e-05\n",
            "step: 160, loss: 0.00010427983943372965\n",
            "step: 170, loss: 6.728208245476708e-05\n",
            "step: 180, loss: 0.00017384446982759982\n",
            "step: 190, loss: 3.6525099858408794e-05\n",
            "step: 200, loss: 4.969826841261238e-05\n",
            "step: 210, loss: 0.00011323421495035291\n",
            "step: 220, loss: 2.7804549972643144e-05\n",
            "step: 230, loss: 0.011679681949317455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9820224719101124, f1=0.9807909604519773, best_f1=0.9807909604519773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002238804445369169\n",
            "step: 10, loss: 2.2038391762180254e-05\n",
            "step: 20, loss: 0.0001431135315215215\n",
            "step: 30, loss: 0.0001836303563322872\n",
            "step: 40, loss: 6.0997634136583656e-05\n",
            "step: 50, loss: 0.01731690764427185\n",
            "step: 60, loss: 4.9623129598330706e-05\n",
            "step: 70, loss: 6.15198805462569e-05\n",
            "step: 80, loss: 0.0006722413818351924\n",
            "step: 90, loss: 7.77523237047717e-05\n",
            "step: 100, loss: 7.907232065917924e-05\n",
            "step: 110, loss: 3.225229011150077e-05\n",
            "step: 120, loss: 0.004234688822180033\n",
            "step: 130, loss: 0.0008856051717884839\n",
            "step: 140, loss: 5.011706525692716e-05\n",
            "step: 150, loss: 0.0007465205271728337\n",
            "step: 160, loss: 3.8156322261784226e-05\n",
            "step: 170, loss: 0.0005988907068967819\n",
            "step: 180, loss: 0.0005488095339387655\n",
            "step: 190, loss: 0.0003028685168828815\n",
            "step: 200, loss: 6.590294651687145e-05\n",
            "step: 210, loss: 0.00024404619762208313\n",
            "step: 220, loss: 0.0009850768838077784\n",
            "step: 230, loss: 0.0002341828658245504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9820224719101124, f1=0.9796839729119639, best_f1=0.9807909604519773\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 157.08it/s]\n",
            "load_f1 = 0.9799107142857142\n",
            "real_f1 = 0.9788182831661093\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 176.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541dfb9e-36ec-4b9d-dc1c-1ac3653f243f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6150348782539368\n",
            "step: 10, loss: 0.5206366181373596\n",
            "step: 20, loss: 0.48278915882110596\n",
            "step: 30, loss: 0.07277379930019379\n",
            "step: 40, loss: 0.17124685645103455\n",
            "step: 50, loss: 0.15867504477500916\n",
            "step: 60, loss: 0.07070029526948929\n",
            "step: 70, loss: 0.13578470051288605\n",
            "step: 80, loss: 0.04373857378959656\n",
            "step: 90, loss: 0.17275378108024597\n",
            "step: 100, loss: 0.018346421420574188\n",
            "step: 110, loss: 0.06923577934503555\n",
            "step: 120, loss: 0.06227966770529747\n",
            "step: 130, loss: 0.10157280415296555\n",
            "step: 140, loss: 0.23226097226142883\n",
            "step: 150, loss: 0.06250551342964172\n",
            "step: 160, loss: 0.07753836363554001\n",
            "step: 170, loss: 0.2215837687253952\n",
            "step: 180, loss: 0.11440674215555191\n",
            "step: 190, loss: 0.006456861272454262\n",
            "step: 200, loss: 0.13119909167289734\n",
            "step: 210, loss: 0.14040625095367432\n",
            "step: 220, loss: 0.267939954996109\n",
            "step: 230, loss: 0.13315202295780182\n",
            "step: 240, loss: 0.0358952060341835\n",
            "step: 250, loss: 0.01602666825056076\n",
            "step: 260, loss: 0.06121726334095001\n",
            "step: 270, loss: 0.0075569357722997665\n",
            "step: 280, loss: 0.018045956268906593\n",
            "step: 290, loss: 0.03198055550456047\n",
            "step: 300, loss: 0.03922339528799057\n",
            "step: 310, loss: 0.1596914678812027\n",
            "step: 320, loss: 0.0988055169582367\n",
            "step: 330, loss: 0.013238940387964249\n",
            "step: 340, loss: 0.058562152087688446\n",
            "step: 350, loss: 0.14983732998371124\n",
            "step: 360, loss: 0.06898798048496246\n",
            "step: 370, loss: 0.14378494024276733\n",
            "step: 380, loss: 0.015416220761835575\n",
            "step: 390, loss: 0.09535543620586395\n",
            "step: 400, loss: 0.3096233904361725\n",
            "step: 410, loss: 0.07562684267759323\n",
            "step: 420, loss: 0.12233545631170273\n",
            "step: 430, loss: 0.17900370061397552\n",
            "step: 440, loss: 0.03721672669053078\n",
            "step: 450, loss: 0.03137250617146492\n",
            "step: 460, loss: 0.013516669161617756\n",
            "step: 470, loss: 0.11236084997653961\n",
            "step: 480, loss: 0.064555823802948\n",
            "step: 490, loss: 0.12924006581306458\n",
            "step: 500, loss: 0.08830587565898895\n",
            "step: 510, loss: 0.08685401827096939\n",
            "step: 520, loss: 0.13375772535800934\n",
            "step: 530, loss: 0.003251172136515379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9373814041745729, f1=0.923728813559322, best_f1=0.923728813559322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17885394394397736\n",
            "step: 10, loss: 0.09006167948246002\n",
            "step: 20, loss: 0.002055129501968622\n",
            "step: 30, loss: 0.01108899898827076\n",
            "step: 40, loss: 0.08665700256824493\n",
            "step: 50, loss: 0.07010233402252197\n",
            "step: 60, loss: 0.004683926701545715\n",
            "step: 70, loss: 0.037948474287986755\n",
            "step: 80, loss: 0.059951771050691605\n",
            "step: 90, loss: 0.005290847271680832\n",
            "step: 100, loss: 0.020768065005540848\n",
            "step: 110, loss: 0.020067265257239342\n",
            "step: 120, loss: 0.047447897493839264\n",
            "step: 130, loss: 0.08082674443721771\n",
            "step: 140, loss: 0.04382339492440224\n",
            "step: 150, loss: 0.10411521047353745\n",
            "step: 160, loss: 0.02605748362839222\n",
            "step: 170, loss: 0.024309149011969566\n",
            "step: 180, loss: 0.0384509414434433\n",
            "step: 190, loss: 0.03720224276185036\n",
            "step: 200, loss: 0.02417105622589588\n",
            "step: 210, loss: 0.044478923082351685\n",
            "step: 220, loss: 0.1530485451221466\n",
            "step: 230, loss: 0.018456192687153816\n",
            "step: 240, loss: 0.015330868773162365\n",
            "step: 250, loss: 0.04279126599431038\n",
            "step: 260, loss: 0.001731391530483961\n",
            "step: 270, loss: 0.08405232429504395\n",
            "step: 280, loss: 0.12934353947639465\n",
            "step: 290, loss: 0.0341171957552433\n",
            "step: 300, loss: 0.15962597727775574\n",
            "step: 310, loss: 0.02524491399526596\n",
            "step: 320, loss: 0.0699392557144165\n",
            "step: 330, loss: 0.04980256408452988\n",
            "step: 340, loss: 0.024112101644277573\n",
            "step: 350, loss: 0.0433976985514164\n",
            "step: 360, loss: 0.1324910670518875\n",
            "step: 370, loss: 0.09752602875232697\n",
            "step: 380, loss: 0.04387419670820236\n",
            "step: 390, loss: 0.09325682371854782\n",
            "step: 400, loss: 0.026214949786663055\n",
            "step: 410, loss: 0.050680384039878845\n",
            "step: 420, loss: 0.027631709352135658\n",
            "step: 430, loss: 0.1596432775259018\n",
            "step: 440, loss: 0.18267911672592163\n",
            "step: 450, loss: 0.15110434591770172\n",
            "step: 460, loss: 0.09480318427085876\n",
            "step: 470, loss: 0.15242557227611542\n",
            "step: 480, loss: 0.21079428493976593\n",
            "step: 490, loss: 0.010462416335940361\n",
            "step: 500, loss: 0.17797569930553436\n",
            "step: 510, loss: 0.024354543536901474\n",
            "step: 520, loss: 0.06417561322450638\n",
            "step: 530, loss: 0.009429355151951313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9395784543325526, f1=0.9400278940027894, best_f1=0.9400278940027894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06828830391168594\n",
            "step: 10, loss: 0.03291686624288559\n",
            "step: 20, loss: 0.07581434398889542\n",
            "step: 30, loss: 0.031882110983133316\n",
            "step: 40, loss: 0.0025296853855252266\n",
            "step: 50, loss: 0.02446777932345867\n",
            "step: 60, loss: 0.01683906652033329\n",
            "step: 70, loss: 0.0030693449079990387\n",
            "step: 80, loss: 0.0012820372357964516\n",
            "step: 90, loss: 0.0057069906033575535\n",
            "step: 100, loss: 0.05788665637373924\n",
            "step: 110, loss: 0.002471276791766286\n",
            "step: 120, loss: 0.0038119510281831026\n",
            "step: 130, loss: 0.0025567207485437393\n",
            "step: 140, loss: 0.007538913749158382\n",
            "step: 150, loss: 0.02123689092695713\n",
            "step: 160, loss: 0.006776909809559584\n",
            "step: 170, loss: 0.06090610846877098\n",
            "step: 180, loss: 0.01838962733745575\n",
            "step: 190, loss: 0.01978609524667263\n",
            "step: 200, loss: 0.11097750812768936\n",
            "step: 210, loss: 0.06614980846643448\n",
            "step: 220, loss: 0.09236236661672592\n",
            "step: 230, loss: 0.10103059560060501\n",
            "step: 240, loss: 0.022818055003881454\n",
            "step: 250, loss: 0.030988072976469994\n",
            "step: 260, loss: 0.04642627760767937\n",
            "step: 270, loss: 0.006342155393213034\n",
            "step: 280, loss: 0.1242409348487854\n",
            "step: 290, loss: 0.0023400522768497467\n",
            "step: 300, loss: 0.11305239051580429\n",
            "step: 310, loss: 0.013608863577246666\n",
            "step: 320, loss: 0.04930813983082771\n",
            "step: 330, loss: 0.01497943140566349\n",
            "step: 340, loss: 0.004978009033948183\n",
            "step: 350, loss: 0.004659594502300024\n",
            "step: 360, loss: 0.05424989014863968\n",
            "step: 370, loss: 0.017291173338890076\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 380, loss: 0.02168954722583294\n",
            "step: 390, loss: 0.03016982600092888\n",
            "step: 400, loss: 0.03176688402891159\n",
            "step: 410, loss: 0.06321023404598236\n",
            "step: 420, loss: 0.1742304414510727\n",
            "step: 430, loss: 0.05964620038866997\n",
            "step: 440, loss: 0.009445501491427422\n",
            "step: 450, loss: 0.088624007999897\n",
            "step: 460, loss: 0.02238072268664837\n",
            "step: 470, loss: 0.09250563383102417\n",
            "step: 480, loss: 0.002067337743937969\n",
            "step: 490, loss: 0.015816446393728256\n",
            "step: 500, loss: 0.017567899078130722\n",
            "step: 510, loss: 0.004156130366027355\n",
            "step: 520, loss: 0.0479472279548645\n",
            "step: 530, loss: 0.09553609788417816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9407407407407408, f1=0.9359151682803135, best_f1=0.9359151682803135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00604357710108161\n",
            "step: 10, loss: 0.002214307663962245\n",
            "step: 20, loss: 0.0042458223178982735\n",
            "step: 30, loss: 0.004226076416671276\n",
            "step: 40, loss: 0.013698350638151169\n",
            "step: 50, loss: 0.09891433268785477\n",
            "step: 60, loss: 0.004502678755670786\n",
            "step: 70, loss: 0.0031203366816043854\n",
            "step: 80, loss: 0.0033526441548019648\n",
            "step: 90, loss: 0.1522616297006607\n",
            "step: 100, loss: 0.004335357807576656\n",
            "step: 110, loss: 0.04326111078262329\n",
            "step: 120, loss: 0.000583791930694133\n",
            "step: 130, loss: 0.004835933912545443\n",
            "step: 140, loss: 0.036969155073165894\n",
            "step: 150, loss: 0.0137075399979949\n",
            "step: 160, loss: 0.036141037940979004\n",
            "step: 170, loss: 0.02712746150791645\n",
            "step: 180, loss: 0.0051580555737018585\n",
            "step: 190, loss: 0.04810945689678192\n",
            "step: 200, loss: 0.007129527162760496\n",
            "step: 210, loss: 0.032543737441301346\n",
            "step: 220, loss: 0.00234658969566226\n",
            "step: 230, loss: 0.02642190083861351\n",
            "step: 240, loss: 0.0023513655178248882\n",
            "step: 250, loss: 0.0588722825050354\n",
            "step: 260, loss: 0.0011335205053910613\n",
            "step: 270, loss: 0.01282578520476818\n",
            "step: 280, loss: 0.11356154084205627\n",
            "step: 290, loss: 0.010836239904165268\n",
            "step: 300, loss: 0.00044404418440535665\n",
            "step: 310, loss: 0.0017895374912768602\n",
            "step: 320, loss: 0.019683847203850746\n",
            "step: 330, loss: 0.06797076016664505\n",
            "step: 340, loss: 0.043610986322164536\n",
            "step: 350, loss: 0.003694654442369938\n",
            "step: 360, loss: 0.016065839678049088\n",
            "step: 370, loss: 0.0028739480767399073\n",
            "step: 380, loss: 0.007113530766218901\n",
            "step: 390, loss: 0.21775001287460327\n",
            "step: 400, loss: 0.0482780747115612\n",
            "step: 410, loss: 0.017064658924937248\n",
            "step: 420, loss: 0.014170306734740734\n",
            "step: 430, loss: 0.19997180998325348\n",
            "step: 440, loss: 0.024986904114484787\n",
            "step: 450, loss: 0.02549252286553383\n",
            "step: 460, loss: 0.002087745815515518\n",
            "step: 470, loss: 0.00481510441750288\n",
            "step: 480, loss: 0.31020793318748474\n",
            "step: 490, loss: 0.001293483772315085\n",
            "step: 500, loss: 0.02378063090145588\n",
            "step: 510, loss: 0.034435007721185684\n",
            "step: 520, loss: 0.07554813474416733\n",
            "step: 530, loss: 0.049673520028591156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9339622641509434, f1=0.9346497414198403, best_f1=0.9359151682803135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011382199823856354\n",
            "step: 10, loss: 0.05006687343120575\n",
            "step: 20, loss: 0.006642232649028301\n",
            "step: 30, loss: 0.001271706074476242\n",
            "step: 40, loss: 0.07305908203125\n",
            "step: 50, loss: 0.002935876837000251\n",
            "step: 60, loss: 0.051426034420728683\n",
            "step: 70, loss: 0.008878150954842567\n",
            "step: 80, loss: 0.0010461716447025537\n",
            "step: 90, loss: 0.12265418469905853\n",
            "step: 100, loss: 0.05430974066257477\n",
            "step: 110, loss: 0.0024713946040719748\n",
            "step: 120, loss: 0.005889062769711018\n",
            "step: 130, loss: 0.0004623477580025792\n",
            "step: 140, loss: 0.002320617903023958\n",
            "step: 150, loss: 0.007864142768085003\n",
            "step: 160, loss: 0.0011650524102151394\n",
            "step: 170, loss: 0.0396043062210083\n",
            "step: 180, loss: 0.000563640845939517\n",
            "step: 190, loss: 0.0008471523760817945\n",
            "step: 200, loss: 0.0031921640038490295\n",
            "step: 210, loss: 0.011684365570545197\n",
            "step: 220, loss: 0.03191141411662102\n",
            "step: 230, loss: 0.004298707935959101\n",
            "step: 240, loss: 0.0021569090895354748\n",
            "step: 250, loss: 0.0006512570544146001\n",
            "step: 260, loss: 0.0010391067480668426\n",
            "step: 270, loss: 0.002015798119828105\n",
            "step: 280, loss: 0.003126789117231965\n",
            "step: 290, loss: 0.13984578847885132\n",
            "step: 300, loss: 0.0022814415860921144\n",
            "step: 310, loss: 0.01974361203610897\n",
            "step: 320, loss: 0.05561952665448189\n",
            "step: 330, loss: 0.1455567479133606\n",
            "step: 340, loss: 0.0039002017583698034\n",
            "step: 350, loss: 0.0237486120313406\n",
            "step: 360, loss: 0.021729251369833946\n",
            "step: 370, loss: 0.021791541948914528\n",
            "step: 380, loss: 0.001008222228847444\n",
            "step: 390, loss: 0.0008840538212098181\n",
            "step: 400, loss: 0.02558024227619171\n",
            "step: 410, loss: 0.01359829306602478\n",
            "step: 420, loss: 0.003172502387315035\n",
            "step: 430, loss: 0.01263430155813694\n",
            "step: 440, loss: 0.0047660525888204575\n",
            "step: 450, loss: 0.002898584585636854\n",
            "step: 460, loss: 0.006732875481247902\n",
            "step: 470, loss: 0.023532750084996223\n",
            "step: 480, loss: 0.02320772409439087\n",
            "step: 490, loss: 0.007326840423047543\n",
            "step: 500, loss: 0.01498725451529026\n",
            "step: 510, loss: 0.05225867033004761\n",
            "step: 520, loss: 0.0013823778135702014\n",
            "step: 530, loss: 0.007091691717505455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9457436856875585, f1=0.9359925788497218, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006383204599842429\n",
            "step: 10, loss: 0.11585376411676407\n",
            "step: 20, loss: 0.09818246215581894\n",
            "step: 30, loss: 0.0012376690283417702\n",
            "step: 40, loss: 0.05335715785622597\n",
            "step: 50, loss: 0.049832358956336975\n",
            "step: 60, loss: 0.00066517194500193\n",
            "step: 70, loss: 0.019151596352458\n",
            "step: 80, loss: 0.020686618983745575\n",
            "step: 90, loss: 0.0017297938466072083\n",
            "step: 100, loss: 0.0392460860311985\n",
            "step: 110, loss: 0.001011246582493186\n",
            "step: 120, loss: 0.05582968890666962\n",
            "step: 130, loss: 0.014489898458123207\n",
            "step: 140, loss: 0.11517199128866196\n",
            "step: 150, loss: 0.046438783407211304\n",
            "step: 160, loss: 0.009110899642109871\n",
            "step: 170, loss: 0.01398070715367794\n",
            "step: 180, loss: 0.010115315206348896\n",
            "step: 190, loss: 0.012744569219648838\n",
            "step: 200, loss: 0.001188674010336399\n",
            "step: 210, loss: 0.005708725657314062\n",
            "step: 220, loss: 0.006513334345072508\n",
            "step: 230, loss: 0.013541786931455135\n",
            "step: 240, loss: 0.020696915686130524\n",
            "step: 250, loss: 0.04767182469367981\n",
            "step: 260, loss: 0.00501430407166481\n",
            "step: 270, loss: 0.1792074590921402\n",
            "step: 280, loss: 0.002571677789092064\n",
            "step: 290, loss: 0.0012462390586733818\n",
            "step: 300, loss: 0.0007288536289706826\n",
            "step: 310, loss: 0.007639193907380104\n",
            "step: 320, loss: 0.015627827495336533\n",
            "step: 330, loss: 0.0013173766201362014\n",
            "step: 340, loss: 0.06860174238681793\n",
            "step: 350, loss: 0.0947456881403923\n",
            "step: 360, loss: 0.011062326841056347\n",
            "step: 370, loss: 0.04374167323112488\n",
            "step: 380, loss: 0.0007065036916173995\n",
            "step: 390, loss: 0.03141279146075249\n",
            "step: 400, loss: 0.022591758519411087\n",
            "step: 410, loss: 0.006155950017273426\n",
            "step: 420, loss: 0.007514109369367361\n",
            "step: 430, loss: 0.0006098462617956102\n",
            "step: 440, loss: 0.0020040192175656557\n",
            "step: 450, loss: 0.00038858948391862214\n",
            "step: 460, loss: 0.0002905599831137806\n",
            "step: 470, loss: 0.004299124702811241\n",
            "step: 480, loss: 0.021531052887439728\n",
            "step: 490, loss: 0.003885622601956129\n",
            "step: 500, loss: 0.00030549473012797534\n",
            "step: 510, loss: 0.0024361403193324804\n",
            "step: 520, loss: 0.023606954142451286\n",
            "step: 530, loss: 0.012963388115167618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9397818871503082, f1=0.9375879868606289, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004219501279294491\n",
            "step: 10, loss: 0.001273136236704886\n",
            "step: 20, loss: 0.001811167923733592\n",
            "step: 30, loss: 0.0008007274009287357\n",
            "step: 40, loss: 0.004720739088952541\n",
            "step: 50, loss: 0.0015512743266299367\n",
            "step: 60, loss: 0.004217989277094603\n",
            "step: 70, loss: 0.00044354982674121857\n",
            "step: 80, loss: 0.0005780956125818193\n",
            "step: 90, loss: 0.0033153907861560583\n",
            "step: 100, loss: 0.00014053694030735642\n",
            "step: 110, loss: 0.00039300869684666395\n",
            "step: 120, loss: 0.0011006977874785662\n",
            "step: 130, loss: 0.0035843781661242247\n",
            "step: 140, loss: 0.00012771302135661244\n",
            "step: 150, loss: 0.052656494081020355\n",
            "step: 160, loss: 0.00027322571258991957\n",
            "step: 170, loss: 0.002894368954002857\n",
            "step: 180, loss: 0.0017141061834990978\n",
            "step: 190, loss: 0.031644534319639206\n",
            "step: 200, loss: 0.0011271388502791524\n",
            "step: 210, loss: 0.07764820754528046\n",
            "step: 220, loss: 0.018335405737161636\n",
            "step: 230, loss: 0.055127907544374466\n",
            "step: 240, loss: 0.004158723168075085\n",
            "step: 250, loss: 0.0013559100916609168\n",
            "step: 260, loss: 0.015057170763611794\n",
            "step: 270, loss: 0.00687941862270236\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 280, loss: 0.05579160898923874\n",
            "step: 290, loss: 0.028002111241221428\n",
            "step: 300, loss: 0.02532707154750824\n",
            "step: 310, loss: 0.0003984751820098609\n",
            "step: 320, loss: 0.010143905878067017\n",
            "step: 330, loss: 0.013335294090211391\n",
            "step: 340, loss: 0.01728932186961174\n",
            "step: 350, loss: 0.0014264810597524047\n",
            "step: 360, loss: 0.001464550499804318\n",
            "step: 370, loss: 0.011670734733343124\n",
            "step: 380, loss: 0.0033590379171073437\n",
            "step: 390, loss: 0.0009072404354810715\n",
            "step: 400, loss: 0.02339993417263031\n",
            "step: 410, loss: 0.003724698442965746\n",
            "step: 420, loss: 0.0005013247136957943\n",
            "step: 430, loss: 0.0003259599325247109\n",
            "step: 440, loss: 0.012821312993764877\n",
            "step: 450, loss: 0.004103196784853935\n",
            "step: 460, loss: 0.000967846077401191\n",
            "step: 470, loss: 0.010889170691370964\n",
            "step: 480, loss: 0.0144340293481946\n",
            "step: 490, loss: 0.01198363397270441\n",
            "step: 500, loss: 0.00707450695335865\n",
            "step: 510, loss: 0.002570879878476262\n",
            "step: 520, loss: 0.0387573204934597\n",
            "step: 530, loss: 0.0069950358010828495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9365303244005642, f1=0.931711880261927, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006937935249879956\n",
            "step: 10, loss: 0.031477153301239014\n",
            "step: 20, loss: 0.029828641563653946\n",
            "step: 30, loss: 0.0006975956493988633\n",
            "step: 40, loss: 0.0008351602009497583\n",
            "step: 50, loss: 0.02976912073791027\n",
            "step: 60, loss: 0.0017823069356381893\n",
            "step: 70, loss: 0.00031749840127304196\n",
            "step: 80, loss: 0.0035155764780938625\n",
            "step: 90, loss: 0.0010889554396271706\n",
            "step: 100, loss: 0.00013484244118444622\n",
            "step: 110, loss: 0.0008123924490064383\n",
            "step: 120, loss: 0.014357823878526688\n",
            "step: 130, loss: 0.00033560776500962675\n",
            "step: 140, loss: 0.010839970782399178\n",
            "step: 150, loss: 0.001649737823754549\n",
            "step: 160, loss: 0.013676850125193596\n",
            "step: 170, loss: 0.053572818636894226\n",
            "step: 180, loss: 0.0005935812368988991\n",
            "step: 190, loss: 0.0011429981095716357\n",
            "step: 200, loss: 0.0009030669461935759\n",
            "step: 210, loss: 0.0005445638671517372\n",
            "step: 220, loss: 0.004178446251899004\n",
            "step: 230, loss: 0.007734726183116436\n",
            "step: 240, loss: 0.0007306472980417311\n",
            "step: 250, loss: 0.0034640044905245304\n",
            "step: 260, loss: 0.0020370702259242535\n",
            "step: 270, loss: 0.0019731088541448116\n",
            "step: 280, loss: 0.02344425395131111\n",
            "step: 290, loss: 0.0006550024263560772\n",
            "step: 300, loss: 9.931220847647637e-05\n",
            "step: 310, loss: 0.0012637253385037184\n",
            "step: 320, loss: 0.13476023077964783\n",
            "step: 330, loss: 0.03248624876141548\n",
            "step: 340, loss: 0.0012739866506308317\n",
            "step: 350, loss: 0.0004278603591956198\n",
            "step: 360, loss: 0.0023924619890749454\n",
            "step: 370, loss: 0.002226293319836259\n",
            "step: 380, loss: 0.010675369761884212\n",
            "step: 390, loss: 0.16335710883140564\n",
            "step: 400, loss: 0.00039310083957388997\n",
            "step: 410, loss: 0.01807038113474846\n",
            "step: 420, loss: 0.0004374432028271258\n",
            "step: 430, loss: 0.05380164086818695\n",
            "step: 440, loss: 0.008852786384522915\n",
            "step: 450, loss: 0.01021381001919508\n",
            "step: 460, loss: 0.019025661051273346\n",
            "step: 470, loss: 0.0037254232447594404\n",
            "step: 480, loss: 0.0023088669404387474\n",
            "step: 490, loss: 0.007182630244642496\n",
            "step: 500, loss: 0.008669446222484112\n",
            "step: 510, loss: 0.0018489706562831998\n",
            "step: 520, loss: 0.001968284137547016\n",
            "step: 530, loss: 0.00036013827775605023\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9406739439962031, f1=0.9342723004694835, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008786995895206928\n",
            "step: 10, loss: 0.020132049918174744\n",
            "step: 20, loss: 0.0014654778642579913\n",
            "step: 30, loss: 0.0004445772501640022\n",
            "step: 40, loss: 0.000136433620355092\n",
            "step: 50, loss: 0.00018393329810351133\n",
            "step: 60, loss: 0.00013574433978646994\n",
            "step: 70, loss: 0.0006941811880096793\n",
            "step: 80, loss: 0.0021754156332463026\n",
            "step: 90, loss: 4.869530312134884e-05\n",
            "step: 100, loss: 5.348421836970374e-05\n",
            "step: 110, loss: 7.215911318780854e-05\n",
            "step: 120, loss: 0.00016827546642161906\n",
            "step: 130, loss: 0.025731569156050682\n",
            "step: 140, loss: 0.0002926014130935073\n",
            "step: 150, loss: 0.012133744545280933\n",
            "step: 160, loss: 0.0008648633374832571\n",
            "step: 170, loss: 0.01297598984092474\n",
            "step: 180, loss: 6.0171088989591226e-05\n",
            "step: 190, loss: 0.0023226027842611074\n",
            "step: 200, loss: 0.00027840479742735624\n",
            "step: 210, loss: 0.00013369342195801437\n",
            "step: 220, loss: 0.008224394172430038\n",
            "step: 230, loss: 0.00028201579698361456\n",
            "step: 240, loss: 0.03979043662548065\n",
            "step: 250, loss: 0.006759935058653355\n",
            "step: 260, loss: 0.025730084627866745\n",
            "step: 270, loss: 0.000246457289904356\n",
            "step: 280, loss: 0.0002729807165451348\n",
            "step: 290, loss: 0.03625621274113655\n",
            "step: 300, loss: 4.8981470172293484e-05\n",
            "step: 310, loss: 0.038799501955509186\n",
            "step: 320, loss: 0.0028543812222778797\n",
            "step: 330, loss: 0.0008320918423123658\n",
            "step: 340, loss: 0.0010375569108873606\n",
            "step: 350, loss: 0.00040199249633587897\n",
            "step: 360, loss: 4.916394391329959e-05\n",
            "step: 370, loss: 0.0002298564213560894\n",
            "step: 380, loss: 0.0003390866913832724\n",
            "step: 390, loss: 0.0001480868668295443\n",
            "step: 400, loss: 0.0034184884279966354\n",
            "step: 410, loss: 4.247441029292531e-05\n",
            "step: 420, loss: 5.985297684674151e-05\n",
            "step: 430, loss: 6.060775922378525e-05\n",
            "step: 440, loss: 0.000919706595595926\n",
            "step: 450, loss: 8.816979971015826e-05\n",
            "step: 460, loss: 0.00022546440595760942\n",
            "step: 470, loss: 0.00013494091399479657\n",
            "step: 480, loss: 9.230073919752613e-05\n",
            "step: 490, loss: 0.01709914207458496\n",
            "step: 500, loss: 0.000385192601243034\n",
            "step: 510, loss: 0.0653984397649765\n",
            "step: 520, loss: 0.00060028035659343\n",
            "step: 530, loss: 9.701849921839312e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9427393495190106, f1=0.9389799635701275, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022585274651646614\n",
            "step: 10, loss: 0.00032376727904193103\n",
            "step: 20, loss: 6.324033893179148e-05\n",
            "step: 30, loss: 4.406326115713455e-05\n",
            "step: 40, loss: 7.030121196294203e-05\n",
            "step: 50, loss: 5.155104008736089e-05\n",
            "step: 60, loss: 5.427636642707512e-05\n",
            "step: 70, loss: 3.8659924030071124e-05\n",
            "step: 80, loss: 6.698180368402973e-05\n",
            "step: 90, loss: 0.0009718263754621148\n",
            "step: 100, loss: 0.00015560282918158919\n",
            "step: 110, loss: 7.6854950748384e-05\n",
            "step: 120, loss: 8.065402653301135e-05\n",
            "step: 130, loss: 0.00013743086310569197\n",
            "step: 140, loss: 0.007733043283224106\n",
            "step: 150, loss: 0.00014475495845545083\n",
            "step: 160, loss: 0.00013897943426854908\n",
            "step: 170, loss: 0.0003192019066773355\n",
            "step: 180, loss: 0.00012911645171698183\n",
            "step: 190, loss: 0.0008266870281659067\n",
            "step: 200, loss: 0.005285585764795542\n",
            "step: 210, loss: 8.890834578778595e-05\n",
            "step: 220, loss: 0.018839213997125626\n",
            "step: 230, loss: 0.02715611644089222\n",
            "step: 240, loss: 0.002838114043697715\n",
            "step: 250, loss: 0.0001079709836631082\n",
            "step: 260, loss: 0.0011162498267367482\n",
            "step: 270, loss: 0.0007734713726677\n",
            "step: 280, loss: 0.0002994741953443736\n",
            "step: 290, loss: 0.00015942224126774818\n",
            "step: 300, loss: 0.00010901762288995087\n",
            "step: 310, loss: 0.00010180046956520528\n",
            "step: 320, loss: 0.0036385012790560722\n",
            "step: 330, loss: 0.0001200381011585705\n",
            "step: 340, loss: 0.00014086304872762412\n",
            "step: 350, loss: 0.0001231056812684983\n",
            "step: 360, loss: 9.070779924513772e-05\n",
            "step: 370, loss: 0.0005182638997212052\n",
            "step: 380, loss: 0.09215399622917175\n",
            "step: 390, loss: 0.002175690606236458\n",
            "step: 400, loss: 0.00014826755796093494\n",
            "step: 410, loss: 0.0018106091301888227\n",
            "step: 420, loss: 0.00015074523980729282\n",
            "step: 430, loss: 5.3214316722005606e-05\n",
            "step: 440, loss: 0.014295227825641632\n",
            "step: 450, loss: 0.0003152958524879068\n",
            "step: 460, loss: 0.0002495154913049191\n",
            "step: 470, loss: 0.000184512187843211\n",
            "step: 480, loss: 5.438181688077748e-05\n",
            "step: 490, loss: 0.0019830241799354553\n",
            "step: 500, loss: 0.029167354106903076\n",
            "step: 510, loss: 7.215495134005323e-05\n",
            "step: 520, loss: 0.0003657339548226446\n",
            "step: 530, loss: 0.00010138138168258592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9393237610004631, f1=0.9389628269848553, best_f1=0.9359925788497218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003738554660230875\n",
            "step: 10, loss: 0.004321986343711615\n",
            "step: 20, loss: 0.0002916712546721101\n",
            "step: 30, loss: 0.0018066863995045424\n",
            "step: 40, loss: 0.00019839065498672426\n",
            "step: 50, loss: 0.004514507949352264\n",
            "step: 60, loss: 0.029970772564411163\n",
            "step: 70, loss: 0.0001925492542795837\n",
            "step: 80, loss: 0.00017549630138091743\n",
            "step: 90, loss: 9.412829240318388e-05\n",
            "step: 100, loss: 0.0013356072595342994\n",
            "step: 110, loss: 0.0003927160578314215\n",
            "step: 120, loss: 0.0036432866472750902\n",
            "step: 130, loss: 0.06945016235113144\n",
            "step: 140, loss: 0.055206187069416046\n",
            "step: 150, loss: 5.2727631555171683e-05\n",
            "step: 160, loss: 0.009301156736910343\n",
            "step: 170, loss: 0.00013022693747188896\n",
            "step: 180, loss: 0.0005150632932782173\n",
            "step: 190, loss: 0.0026781365741044283\n",
            "step: 200, loss: 0.017364302650094032\n",
            "step: 210, loss: 0.001790617941878736\n",
            "step: 220, loss: 0.0032360099721699953\n",
            "step: 230, loss: 8.728288230486214e-05\n",
            "step: 240, loss: 0.0017224799375981092\n",
            "step: 250, loss: 6.954620039323345e-05\n",
            "step: 260, loss: 9.007785411085933e-05\n",
            "step: 270, loss: 0.00045660670730285347\n",
            "step: 280, loss: 0.00011974170047324151\n",
            "step: 290, loss: 0.006428384222090244\n",
            "step: 300, loss: 0.0030170606914907694\n",
            "step: 310, loss: 0.0005044112913310528\n",
            "step: 320, loss: 0.00013492909783963114\n",
            "step: 330, loss: 0.00032660961733199656\n",
            "step: 340, loss: 0.0016350257210433483\n",
            "step: 350, loss: 0.0001292672095587477\n",
            "step: 360, loss: 0.00034143199445679784\n",
            "step: 370, loss: 0.0011505607981234789\n",
            "step: 380, loss: 0.0011775143211707473\n",
            "step: 390, loss: 0.0002449984021950513\n",
            "step: 400, loss: 0.0009791252668946981\n",
            "step: 410, loss: 0.00039088877383619547\n",
            "step: 420, loss: 0.00024276258773170412\n",
            "step: 430, loss: 0.00011534782242961228\n",
            "step: 440, loss: 0.0013998298672959208\n",
            "step: 450, loss: 8.274587889900431e-05\n",
            "step: 460, loss: 0.061583761125802994\n",
            "step: 470, loss: 0.00015932788664940745\n",
            "step: 480, loss: 0.0004668417095672339\n",
            "step: 490, loss: 0.0010541933588683605\n",
            "step: 500, loss: 0.0005504566361196339\n",
            "step: 510, loss: 0.00037027691723778844\n",
            "step: 520, loss: 0.0026772057171911\n",
            "step: 530, loss: 0.00418095663189888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9477611940298507, f1=0.9380776340110906, best_f1=0.9380776340110906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.453770326217636e-05\n",
            "step: 10, loss: 0.00012236688053235412\n",
            "step: 20, loss: 0.00016652837803121656\n",
            "step: 30, loss: 0.011022093705832958\n",
            "step: 40, loss: 0.0003993965801782906\n",
            "step: 50, loss: 0.08142989873886108\n",
            "step: 60, loss: 0.0005438150255940855\n",
            "step: 70, loss: 0.00637125875800848\n",
            "step: 80, loss: 9.794380457606167e-05\n",
            "step: 90, loss: 9.264433174394071e-05\n",
            "step: 100, loss: 0.0002918443060480058\n",
            "step: 110, loss: 0.1480797678232193\n",
            "step: 120, loss: 0.00017371820285916328\n",
            "step: 130, loss: 0.0007186503498815\n",
            "step: 140, loss: 6.072191899875179e-05\n",
            "step: 150, loss: 0.0004061404906678945\n",
            "step: 160, loss: 0.0004127583815716207\n",
            "step: 170, loss: 0.00013294689415488392\n",
            "step: 180, loss: 0.00012989871902391315\n",
            "step: 190, loss: 0.0006279131630435586\n",
            "step: 200, loss: 0.0003340313851367682\n",
            "step: 210, loss: 0.0005443129339255393\n",
            "step: 220, loss: 0.0015346510335803032\n",
            "step: 230, loss: 0.00017733841377776116\n",
            "step: 240, loss: 6.59493452985771e-05\n",
            "step: 250, loss: 0.0001163663255283609\n",
            "step: 260, loss: 0.00026490239542908967\n",
            "step: 270, loss: 0.00031336164101958275\n",
            "step: 280, loss: 0.0005507071036845446\n",
            "step: 290, loss: 0.0018084943294525146\n",
            "step: 300, loss: 0.0014901263639330864\n",
            "step: 310, loss: 5.941397830611095e-05\n",
            "step: 320, loss: 6.401299469871446e-05\n",
            "step: 330, loss: 0.002678663469851017\n",
            "step: 340, loss: 6.406213651644066e-05\n",
            "step: 350, loss: 0.001228705164976418\n",
            "step: 360, loss: 0.0002820385852828622\n",
            "step: 370, loss: 0.0004377166333142668\n",
            "step: 380, loss: 8.647479262435809e-05\n",
            "step: 390, loss: 0.0006518405862152576\n",
            "step: 400, loss: 0.0001432498829672113\n",
            "step: 410, loss: 0.001083780312910676\n",
            "step: 420, loss: 0.02411786839365959\n",
            "step: 430, loss: 0.012096522375941277\n",
            "step: 440, loss: 0.0003108052769675851\n",
            "step: 450, loss: 0.00028650645981542766\n",
            "step: 460, loss: 0.0001593972701812163\n",
            "step: 470, loss: 2.037294689216651e-05\n",
            "step: 480, loss: 0.0011377355549484491\n",
            "step: 490, loss: 0.0001896476314868778\n",
            "step: 500, loss: 7.435956649715081e-05\n",
            "step: 510, loss: 0.00628388486802578\n",
            "step: 520, loss: 0.00010106650006491691\n",
            "step: 530, loss: 0.00029024173272773623\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9431870669745959, f1=0.9333333333333335, best_f1=0.9380776340110906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005814675241708755\n",
            "step: 10, loss: 0.00036905892193317413\n",
            "step: 20, loss: 0.0008026760770007968\n",
            "step: 30, loss: 0.0011330704437568784\n",
            "step: 40, loss: 0.0006517891306430101\n",
            "step: 50, loss: 0.00023034974583424628\n",
            "step: 60, loss: 0.00012744309788104147\n",
            "step: 70, loss: 0.00017298573220614344\n",
            "step: 80, loss: 9.844851592788473e-05\n",
            "step: 90, loss: 0.010197525843977928\n",
            "step: 100, loss: 0.005607420112937689\n",
            "step: 110, loss: 4.808736048289575e-05\n",
            "step: 120, loss: 0.001454073702916503\n",
            "step: 130, loss: 6.620302883675322e-05\n",
            "step: 140, loss: 0.00018889704369939864\n",
            "step: 150, loss: 0.00025524970260448754\n",
            "step: 160, loss: 0.00012869002239312977\n",
            "step: 170, loss: 0.00037919500027783215\n",
            "step: 180, loss: 0.00027302614762447774\n",
            "step: 190, loss: 0.00019936045282520354\n",
            "step: 200, loss: 3.3839263778645545e-05\n",
            "step: 210, loss: 8.17909458419308e-05\n",
            "step: 220, loss: 0.0001929427671711892\n",
            "step: 230, loss: 0.00019801278540398926\n",
            "step: 240, loss: 6.043038592906669e-05\n",
            "step: 250, loss: 0.0001084955656551756\n",
            "step: 260, loss: 0.0018063883762806654\n",
            "step: 270, loss: 0.0005763554945588112\n",
            "step: 280, loss: 0.0002293313154950738\n",
            "step: 290, loss: 8.277204324258491e-05\n",
            "step: 300, loss: 5.1575712859630585e-05\n",
            "step: 310, loss: 0.006137029267847538\n",
            "step: 320, loss: 0.005048680119216442\n",
            "step: 330, loss: 0.0032197751570492983\n",
            "step: 340, loss: 0.0005411386955529451\n",
            "step: 350, loss: 6.153702997835353e-05\n",
            "step: 360, loss: 0.00010895793093368411\n",
            "step: 370, loss: 8.180086297215894e-05\n",
            "step: 380, loss: 5.396192500484176e-05\n",
            "step: 390, loss: 0.02594747208058834\n",
            "step: 400, loss: 4.7023771912790835e-05\n",
            "step: 410, loss: 0.0008072133641690016\n",
            "step: 420, loss: 0.0001442684733774513\n",
            "step: 430, loss: 0.00656329607591033\n",
            "step: 440, loss: 0.0004780360031872988\n",
            "step: 450, loss: 0.0006876856787130237\n",
            "step: 460, loss: 0.0027160095050930977\n",
            "step: 470, loss: 3.5273558751214296e-05\n",
            "step: 480, loss: 0.0005456576473079622\n",
            "step: 490, loss: 0.00034869168302975595\n",
            "step: 500, loss: 7.057649054331705e-05\n",
            "step: 510, loss: 0.010874743573367596\n",
            "step: 520, loss: 0.00834185816347599\n",
            "step: 530, loss: 5.439219967229292e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9410672853828307, f1=0.9390018484288354, best_f1=0.9380776340110906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0050865779630839825\n",
            "step: 10, loss: 3.0136679924908094e-05\n",
            "step: 20, loss: 0.0008893028134480119\n",
            "step: 30, loss: 6.032305827829987e-05\n",
            "step: 40, loss: 5.987221811665222e-05\n",
            "step: 50, loss: 0.00018897277186624706\n",
            "step: 60, loss: 8.791683649178594e-05\n",
            "step: 70, loss: 0.0004918812774121761\n",
            "step: 80, loss: 1.4960578482714482e-05\n",
            "step: 90, loss: 0.00017863769608084112\n",
            "step: 100, loss: 0.000552767189219594\n",
            "step: 110, loss: 2.833755934261717e-05\n",
            "step: 120, loss: 8.957517275121063e-05\n",
            "step: 130, loss: 0.023536816239356995\n",
            "step: 140, loss: 0.00011725871445378289\n",
            "step: 150, loss: 3.87088002753444e-05\n",
            "step: 160, loss: 0.0002962546132039279\n",
            "step: 170, loss: 0.000803741451818496\n",
            "step: 180, loss: 6.107084482209757e-05\n",
            "step: 190, loss: 9.0221525169909e-05\n",
            "step: 200, loss: 0.005235536955296993\n",
            "step: 210, loss: 0.00014519825344905257\n",
            "step: 220, loss: 4.9346483137924224e-05\n",
            "step: 230, loss: 0.0009703427786007524\n",
            "step: 240, loss: 7.330363587243482e-05\n",
            "step: 250, loss: 4.398182136355899e-05\n",
            "step: 260, loss: 6.882542948005721e-05\n",
            "step: 270, loss: 0.00017205052427016199\n",
            "step: 280, loss: 3.622995791374706e-05\n",
            "step: 290, loss: 0.0003431052027735859\n",
            "step: 300, loss: 0.0002574655518401414\n",
            "step: 310, loss: 0.0018536972347646952\n",
            "step: 320, loss: 0.00040106690721586347\n",
            "step: 330, loss: 0.0007808355148881674\n",
            "step: 340, loss: 0.00019898921891581267\n",
            "step: 350, loss: 0.00016335876716766506\n",
            "step: 360, loss: 0.0037229477893561125\n",
            "step: 370, loss: 0.00011671011452563107\n",
            "step: 380, loss: 8.591293590143323e-05\n",
            "step: 390, loss: 0.03976588323712349\n",
            "step: 400, loss: 0.0006920885061845183\n",
            "step: 410, loss: 6.129220128059387e-05\n",
            "step: 420, loss: 0.00011306087981211022\n",
            "step: 430, loss: 6.363329157466069e-05\n",
            "step: 440, loss: 0.0009251610608771443\n",
            "step: 450, loss: 0.0006124043720774353\n",
            "step: 460, loss: 0.00015381738194264472\n",
            "step: 470, loss: 5.0654434744501486e-05\n",
            "step: 480, loss: 9.436831169296056e-05\n",
            "step: 490, loss: 2.874719211831689e-05\n",
            "step: 500, loss: 2.5104169253609143e-05\n",
            "step: 510, loss: 0.002809263067319989\n",
            "step: 520, loss: 7.716357504250482e-05\n",
            "step: 530, loss: 0.00023607403272762895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9417698303530491, f1=0.9341208541572013, best_f1=0.9380776340110906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.7023055483587086e-05\n",
            "step: 10, loss: 5.3272808145266026e-05\n",
            "step: 20, loss: 0.00014014246698934585\n",
            "step: 30, loss: 0.001279131742194295\n",
            "step: 40, loss: 0.007557334378361702\n",
            "step: 50, loss: 0.007906511425971985\n",
            "step: 60, loss: 0.0008945745066739619\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 0.0015072879614308476\n",
            "step: 80, loss: 3.897725764545612e-05\n",
            "step: 90, loss: 2.4053453671513125e-05\n",
            "step: 100, loss: 0.00023893041361588985\n",
            "step: 110, loss: 0.01378072239458561\n",
            "step: 120, loss: 0.0019969302229583263\n",
            "step: 130, loss: 0.01664220541715622\n",
            "step: 140, loss: 9.157138265436515e-05\n",
            "step: 150, loss: 0.00018153111159335822\n",
            "step: 160, loss: 2.9546741643571295e-05\n",
            "step: 170, loss: 0.00012220474309287965\n",
            "step: 180, loss: 0.00020718465384561568\n",
            "step: 190, loss: 0.001829706714488566\n",
            "step: 200, loss: 9.344097634311765e-05\n",
            "step: 210, loss: 0.0003077231231145561\n",
            "step: 220, loss: 0.004892007913440466\n",
            "step: 230, loss: 0.0016997973434627056\n",
            "step: 240, loss: 5.9603331465041265e-05\n",
            "step: 250, loss: 0.00046658708015456796\n",
            "step: 260, loss: 0.000476044078823179\n",
            "step: 270, loss: 8.697411249158904e-05\n",
            "step: 280, loss: 3.351403938722797e-05\n",
            "step: 290, loss: 1.5996225556591526e-05\n",
            "step: 300, loss: 2.3744392819935456e-05\n",
            "step: 310, loss: 8.8208653323818e-05\n",
            "step: 320, loss: 0.00024937361013144255\n",
            "step: 330, loss: 0.00024276829208247364\n",
            "step: 340, loss: 2.609845068946015e-05\n",
            "step: 350, loss: 2.742075412243139e-05\n",
            "step: 360, loss: 0.0017568571493029594\n",
            "step: 370, loss: 0.0020285549107939005\n",
            "step: 380, loss: 0.0002566813491284847\n",
            "step: 390, loss: 0.0037600856740027666\n",
            "step: 400, loss: 0.00014353728329297155\n",
            "step: 410, loss: 5.204559056437574e-05\n",
            "step: 420, loss: 3.787069726968184e-05\n",
            "step: 430, loss: 0.08568541705608368\n",
            "step: 440, loss: 0.0013663897989317775\n",
            "step: 450, loss: 1.8629807527759112e-05\n",
            "step: 460, loss: 3.643468880909495e-05\n",
            "step: 470, loss: 0.00027149487868882716\n",
            "step: 480, loss: 2.3059015802573413e-05\n",
            "step: 490, loss: 4.784450356964953e-05\n",
            "step: 500, loss: 8.552693907404318e-05\n",
            "step: 510, loss: 0.002072358736768365\n",
            "step: 520, loss: 2.6019930373877287e-05\n",
            "step: 530, loss: 0.000320094870403409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9409090909090909, f1=0.9332129963898917, best_f1=0.9380776340110906\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:31, 181.55it/s]\n",
            "load_f1 = 0.9446254071661238\n",
            "real_f1 = 0.9433611884865365\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 177.20it/s]\n"
          ]
        }
      ]
    }
  ]
}