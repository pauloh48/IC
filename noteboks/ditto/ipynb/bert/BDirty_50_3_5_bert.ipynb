{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDirty_50_3_5_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418d5bd8-5562-4065-d119-6b81836e3612"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 20.94 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "Checking out files: 100% (128/128), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 57.7 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 19.83 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.9 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 25.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 57.9 MB/s \n",
            "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449920 sha256=7b4e54358594e7a4c62d1a89aef40050aa40f3263c38bd2dd31a6ccad10f094b\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=47260f3b02135aeacc5076f53c8c05630e456e5269eb32995837f24b3be44533\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb5f35b-bcc6-419e-cd70-20149fe10795"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10085, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 10085 (delta 98), reused 131 (delta 59), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10085/10085), 14.95 MiB | 21.90 MiB/s, done.\n",
            "Resolving deltas: 100% (6902/6902), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-vvkbei0i\n",
            "Created temporary directory: /tmp/pip-req-tracker-c2evqbtv\n",
            "Initialized build tracking at /tmp/pip-req-tracker-c2evqbtv\n",
            "Created build tracker: /tmp/pip-req-tracker-c2evqbtv\n",
            "Entered build tracker: /tmp/pip-req-tracker-c2evqbtv\n",
            "Created temporary directory: /tmp/pip-install-3mtbnc68\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-n2r7qhrn\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-c2evqbtv'\n",
            "    Running setup.py (path:/tmp/pip-req-build-n2r7qhrn/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-8a62i9vr\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8a62i9vr/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-n2r7qhrn has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-c2evqbtv'\n",
            "Created temporary directory: /tmp/pip-unpack-_5sok9gg\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-8lq2_k3d\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-8lq2_k3d\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-n2r7qhrn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-n2r7qhrn/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-8lq2_k3d\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-8lq2_k3d/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=34209e53ca7fb8d25b87901d4419c1ebd795b4f8194c5f90209f35e31ba9f7e2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vvkbei0i/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-c2evqbtv'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625fa001-6400-4c66-a183-d27d9d9b8838"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.47-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 40.5 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.47\n",
            "  Downloading botocore-1.27.47-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.47->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.47->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.47 botocore-1.27.47 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e49c389-9f39-48a4-b673-8390903a827d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "f7bb32f4-c50e-4162-8167-9dcfaf4b9770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 982, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 982 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (982/982), 251.89 MiB | 28.28 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1273/1273), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab40bbce-0fab-42b5-cc54-0cfe19e51f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/BDirty_50_3_5/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b51176-84f1-4d2c-fccf-d78cde6420ed"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 429kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.11MB/s]\n",
            "Downloading: 100% 440M/440M [00:09<00:00, 48.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5753392577171326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3170731707317073, f1=0.37681159420289856, best_f1=0.37681159420289856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.496731698513031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.41666666666666663, f1=0.3333333333333333, best_f1=0.3333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5465734004974365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5128205128205129, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30273374915122986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5641025641025641, f1=0.4571428571428571, best_f1=0.4571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3335566818714142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5853658536585367, f1=0.4444444444444444, best_f1=0.4444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2905455529689789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7058823529411764, f1=0.45161290322580644, best_f1=0.45161290322580644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22130781412124634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.45454545454545453, f1=0.4799999999999999, best_f1=0.45161290322580644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08355729281902313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.75, f1=0.4827586206896552, best_f1=0.4827586206896552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031443554908037186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7096774193548386, f1=0.5517241379310344, best_f1=0.4827586206896552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.037591997534036636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7878787878787878, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014551480300724506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7692307692307692, f1=0.6666666666666666, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023498041555285454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7878787878787878, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04131972789764404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8125000000000001, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015853818506002426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8387096774193549, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009759517386555672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8387096774193549, f1=0.5333333333333333, best_f1=0.5333333333333333\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 107606.90it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7333333333333334\n",
            "real_f1 = 0.6875000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:15, 285.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07702671-67e1-46f6-e89c-c26ce658832e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6013573408126831\n",
            "step: 10, loss: 0.6070559620857239\n",
            "step: 20, loss: 0.3360409736633301\n",
            "step: 30, loss: 0.17598143219947815\n",
            "step: 40, loss: 0.27434656023979187\n",
            "step: 50, loss: 0.026675183326005936\n",
            "step: 60, loss: 0.06487015634775162\n",
            "step: 70, loss: 0.08455372601747513\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.08345964550971985\n",
            "step: 90, loss: 0.0673287957906723\n",
            "step: 100, loss: 0.012675479985773563\n",
            "step: 110, loss: 0.033842090517282486\n",
            "step: 120, loss: 0.006999011617153883\n",
            "step: 130, loss: 0.009491421282291412\n",
            "step: 140, loss: 0.0035409170668572187\n",
            "step: 150, loss: 0.0402308814227581\n",
            "step: 160, loss: 0.011047862470149994\n",
            "step: 170, loss: 0.0774821862578392\n",
            "step: 180, loss: 0.1287127584218979\n",
            "step: 190, loss: 0.022490086033940315\n",
            "step: 200, loss: 0.013345815241336823\n",
            "step: 210, loss: 0.005647131707519293\n",
            "step: 220, loss: 0.011497896164655685\n",
            "step: 230, loss: 0.07803533226251602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9797297297297298, f1=0.9764309764309763, best_f1=0.9764309764309763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008022038266062737\n",
            "step: 10, loss: 0.003938931040465832\n",
            "step: 20, loss: 0.07578703761100769\n",
            "step: 30, loss: 0.19600339233875275\n",
            "step: 40, loss: 0.010931318625807762\n",
            "step: 50, loss: 0.011455580592155457\n",
            "step: 60, loss: 0.003729761578142643\n",
            "step: 70, loss: 0.16403187811374664\n",
            "step: 80, loss: 0.0024412444327026606\n",
            "step: 90, loss: 0.04198402911424637\n",
            "step: 100, loss: 0.01045041810721159\n",
            "step: 110, loss: 0.05084560811519623\n",
            "step: 120, loss: 0.008762889541685581\n",
            "step: 130, loss: 0.01731971651315689\n",
            "step: 140, loss: 0.0020206684712320566\n",
            "step: 150, loss: 0.04161320626735687\n",
            "step: 160, loss: 0.08423975855112076\n",
            "step: 170, loss: 0.006476460490375757\n",
            "step: 180, loss: 0.009269333444535732\n",
            "step: 190, loss: 0.006837426219135523\n",
            "step: 200, loss: 0.00602988013997674\n",
            "step: 210, loss: 0.0026552702765911818\n",
            "step: 220, loss: 0.09536083042621613\n",
            "step: 230, loss: 0.013209945522248745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9853768278965129, f1=0.9820224719101124, best_f1=0.9820224719101124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030039316043257713\n",
            "step: 10, loss: 0.015411989763379097\n",
            "step: 20, loss: 0.0005327625549398363\n",
            "step: 30, loss: 0.07177217304706573\n",
            "step: 40, loss: 0.19674301147460938\n",
            "step: 50, loss: 0.011488345451653004\n",
            "step: 60, loss: 0.053095266222953796\n",
            "step: 70, loss: 0.009720377624034882\n",
            "step: 80, loss: 0.0011898879893124104\n",
            "step: 90, loss: 0.027171041816473007\n",
            "step: 100, loss: 0.0041281781159341335\n",
            "step: 110, loss: 0.013434148393571377\n",
            "step: 120, loss: 0.021597618237137794\n",
            "step: 130, loss: 0.001095907879061997\n",
            "step: 140, loss: 0.023736651986837387\n",
            "step: 150, loss: 0.0023764583747833967\n",
            "step: 160, loss: 0.007658728398382664\n",
            "step: 170, loss: 0.003205052576959133\n",
            "step: 180, loss: 0.004963978659361601\n",
            "step: 190, loss: 0.0026105064898729324\n",
            "step: 200, loss: 0.007440219633281231\n",
            "step: 210, loss: 0.0007694665109738708\n",
            "step: 220, loss: 0.0007738024578429759\n",
            "step: 230, loss: 0.00028531765565276146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9853438556933484, f1=0.9721913236929923, best_f1=0.9820224719101124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021554611157625914\n",
            "step: 10, loss: 0.0010465357918292284\n",
            "step: 20, loss: 0.0007259598933160305\n",
            "step: 30, loss: 0.0007204951252788305\n",
            "step: 40, loss: 0.002488137222826481\n",
            "step: 50, loss: 0.0006951417890377343\n",
            "step: 60, loss: 0.004837350454181433\n",
            "step: 70, loss: 0.006144417449831963\n",
            "step: 80, loss: 0.0032744244672358036\n",
            "step: 90, loss: 0.021200409159064293\n",
            "step: 100, loss: 0.0006606568349525332\n",
            "step: 110, loss: 0.0006986315129324794\n",
            "step: 120, loss: 0.006224417593330145\n",
            "step: 130, loss: 0.018937446177005768\n",
            "step: 140, loss: 0.0024467853363603354\n",
            "step: 150, loss: 0.1960906833410263\n",
            "step: 160, loss: 0.0010241226991638541\n",
            "step: 170, loss: 0.11325991898775101\n",
            "step: 180, loss: 0.0003567279491107911\n",
            "step: 190, loss: 0.001407412812113762\n",
            "step: 200, loss: 0.007644502446055412\n",
            "step: 210, loss: 0.09869041293859482\n",
            "step: 220, loss: 0.0003465230984147638\n",
            "step: 230, loss: 0.017895342782139778\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9864253393665158, f1=0.978675645342312, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024687605910003185\n",
            "step: 10, loss: 0.012674933299422264\n",
            "step: 20, loss: 0.00432887626811862\n",
            "step: 30, loss: 0.005247761495411396\n",
            "step: 40, loss: 0.00037246374995447695\n",
            "step: 50, loss: 0.0003921421302948147\n",
            "step: 60, loss: 0.01886470802128315\n",
            "step: 70, loss: 0.0013022724306210876\n",
            "step: 80, loss: 0.0003783321881201118\n",
            "step: 90, loss: 0.0005042756674811244\n",
            "step: 100, loss: 0.0008195051923394203\n",
            "step: 110, loss: 0.00044350652024149895\n",
            "step: 120, loss: 0.00011885316052939743\n",
            "step: 130, loss: 0.006516129709780216\n",
            "step: 140, loss: 0.0005343113443814218\n",
            "step: 150, loss: 0.00033529510255903006\n",
            "step: 160, loss: 0.03897297382354736\n",
            "step: 170, loss: 0.08714138716459274\n",
            "step: 180, loss: 0.018664497882127762\n",
            "step: 190, loss: 0.006379565689712763\n",
            "step: 200, loss: 0.0022215035278350115\n",
            "step: 210, loss: 0.000739425653591752\n",
            "step: 220, loss: 0.00039744647801853716\n",
            "step: 230, loss: 0.00033710492425598204\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9795454545454545, f1=0.9775280898876404, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038718702853657305\n",
            "step: 10, loss: 0.0002513524377718568\n",
            "step: 20, loss: 0.00596547219902277\n",
            "step: 30, loss: 0.0002999426214955747\n",
            "step: 40, loss: 0.006828164216130972\n",
            "step: 50, loss: 0.0036844939459115267\n",
            "step: 60, loss: 0.03384861722588539\n",
            "step: 70, loss: 0.009600004181265831\n",
            "step: 80, loss: 0.0008418120560236275\n",
            "step: 90, loss: 0.0005064926808699965\n",
            "step: 100, loss: 0.001678977394476533\n",
            "step: 110, loss: 0.00019402675388846546\n",
            "step: 120, loss: 0.00039947914774529636\n",
            "step: 130, loss: 0.000692762085236609\n",
            "step: 140, loss: 0.0001351213431917131\n",
            "step: 150, loss: 0.0002154184621758759\n",
            "step: 160, loss: 0.0020444272086024284\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.05686277896165848\n",
            "step: 180, loss: 0.00512382248416543\n",
            "step: 190, loss: 0.0831354409456253\n",
            "step: 200, loss: 0.00013412110274657607\n",
            "step: 210, loss: 0.00058198586339131\n",
            "step: 220, loss: 0.0008083231514319777\n",
            "step: 230, loss: 0.10388009250164032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9853438556933484, f1=0.9843400447427293, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016903647920116782\n",
            "step: 10, loss: 0.00032057930366136134\n",
            "step: 20, loss: 0.002689382992684841\n",
            "step: 30, loss: 0.0005830821464769542\n",
            "step: 40, loss: 0.00020939725800417364\n",
            "step: 50, loss: 0.00040131848072633147\n",
            "step: 60, loss: 0.038178540766239166\n",
            "step: 70, loss: 0.0008863491821102798\n",
            "step: 80, loss: 0.0009946455247700214\n",
            "step: 90, loss: 0.0004078477213624865\n",
            "step: 100, loss: 0.001387831405736506\n",
            "step: 110, loss: 0.0006794329383410513\n",
            "step: 120, loss: 0.0007332301465794444\n",
            "step: 130, loss: 0.000375247560441494\n",
            "step: 140, loss: 0.0003962013288401067\n",
            "step: 150, loss: 0.00028758178814314306\n",
            "step: 160, loss: 0.012042982503771782\n",
            "step: 170, loss: 0.009263628162443638\n",
            "step: 180, loss: 0.0005502121639437973\n",
            "step: 190, loss: 0.00019766493642237037\n",
            "step: 200, loss: 0.009019981138408184\n",
            "step: 210, loss: 6.882608431624249e-05\n",
            "step: 220, loss: 0.00034216701169498265\n",
            "step: 230, loss: 0.0001804190978873521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9875706214689265, f1=0.9776286353467561, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001070320577127859\n",
            "step: 10, loss: 0.0005086367018520832\n",
            "step: 20, loss: 0.0002758812333922833\n",
            "step: 30, loss: 0.0003331276820972562\n",
            "step: 40, loss: 0.0005960497073829174\n",
            "step: 50, loss: 0.00011021069076377898\n",
            "step: 60, loss: 7.490520511055365e-05\n",
            "step: 70, loss: 0.00010600591485854238\n",
            "step: 80, loss: 0.00030932563822716475\n",
            "step: 90, loss: 6.276799831539392e-05\n",
            "step: 100, loss: 0.00019276607781648636\n",
            "step: 110, loss: 9.94531437754631e-05\n",
            "step: 120, loss: 0.0008630756055936217\n",
            "step: 130, loss: 6.739694799762219e-05\n",
            "step: 140, loss: 5.6998844229383394e-05\n",
            "step: 150, loss: 0.0001310209627263248\n",
            "step: 160, loss: 0.000862397369928658\n",
            "step: 170, loss: 0.00020116080122534186\n",
            "step: 180, loss: 0.0005355095490813255\n",
            "step: 190, loss: 0.00010895109153352678\n",
            "step: 200, loss: 0.0001543721737107262\n",
            "step: 210, loss: 0.0002128013875335455\n",
            "step: 220, loss: 0.0006404372397810221\n",
            "step: 230, loss: 0.00020229702931828797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9842342342342343, f1=0.9755555555555556, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017261509492527694\n",
            "step: 10, loss: 0.00047462049406021833\n",
            "step: 20, loss: 0.0005435242783278227\n",
            "step: 30, loss: 6.594535807380453e-05\n",
            "step: 40, loss: 0.032712288200855255\n",
            "step: 50, loss: 0.000188158723176457\n",
            "step: 60, loss: 0.0001001627606456168\n",
            "step: 70, loss: 0.00010224637662759051\n",
            "step: 80, loss: 0.02448841743171215\n",
            "step: 90, loss: 0.004334176890552044\n",
            "step: 100, loss: 0.000435000954894349\n",
            "step: 110, loss: 5.171049997443333e-05\n",
            "step: 120, loss: 6.940917228348553e-05\n",
            "step: 130, loss: 7.11390093783848e-05\n",
            "step: 140, loss: 4.8936035454971716e-05\n",
            "step: 150, loss: 0.002370311878621578\n",
            "step: 160, loss: 0.0006379553815349936\n",
            "step: 170, loss: 0.00015498597349505872\n",
            "step: 180, loss: 0.12832209467887878\n",
            "step: 190, loss: 0.00646374374628067\n",
            "step: 200, loss: 0.0005295358132570982\n",
            "step: 210, loss: 0.0005585363251157105\n",
            "step: 220, loss: 0.00012665135727729648\n",
            "step: 230, loss: 0.00012818945106118917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9830124575311437, f1=0.9809203142536477, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013583940744865686\n",
            "step: 10, loss: 5.4440577514469624e-05\n",
            "step: 20, loss: 0.00012671096192207187\n",
            "step: 30, loss: 0.0011073241475969553\n",
            "step: 40, loss: 5.93911754549481e-05\n",
            "step: 50, loss: 0.011771738529205322\n",
            "step: 60, loss: 0.0005578835844062269\n",
            "step: 70, loss: 8.452167821815237e-05\n",
            "step: 80, loss: 9.731640602694824e-05\n",
            "step: 90, loss: 0.00015772362530697137\n",
            "step: 100, loss: 9.106705692829564e-05\n",
            "step: 110, loss: 0.0001118216387112625\n",
            "step: 120, loss: 0.0012280871160328388\n",
            "step: 130, loss: 5.7776029279921204e-05\n",
            "step: 140, loss: 0.00011002847168128937\n",
            "step: 150, loss: 0.00014525694132316858\n",
            "step: 160, loss: 2.611711352074053e-05\n",
            "step: 170, loss: 3.744444256881252e-05\n",
            "step: 180, loss: 0.00011745860683731735\n",
            "step: 190, loss: 0.00013000897888559848\n",
            "step: 200, loss: 4.6838449634378776e-05\n",
            "step: 210, loss: 0.0007462245994247496\n",
            "step: 220, loss: 0.017785081639885902\n",
            "step: 230, loss: 0.00047537742648273706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.983050847457627, f1=0.9820627802690582, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000121133285574615\n",
            "step: 10, loss: 0.0001619068207219243\n",
            "step: 20, loss: 0.0001906638644868508\n",
            "step: 30, loss: 0.0001535180490463972\n",
            "step: 40, loss: 0.004485145211219788\n",
            "step: 50, loss: 0.00010638878302415833\n",
            "step: 60, loss: 0.0001314105320489034\n",
            "step: 70, loss: 0.00044077468919567764\n",
            "step: 80, loss: 4.915962563245557e-05\n",
            "step: 90, loss: 0.00013561194646172225\n",
            "step: 100, loss: 0.00014885970449540764\n",
            "step: 110, loss: 5.990053978166543e-05\n",
            "step: 120, loss: 8.93393371370621e-05\n",
            "step: 130, loss: 0.0003833512600976974\n",
            "step: 140, loss: 0.00012178211909485981\n",
            "step: 150, loss: 0.002321881242096424\n",
            "step: 160, loss: 4.21497825300321e-05\n",
            "step: 170, loss: 7.341026503127068e-05\n",
            "step: 180, loss: 9.771653276402503e-05\n",
            "step: 190, loss: 5.47205563634634e-05\n",
            "step: 200, loss: 8.378973143408075e-05\n",
            "step: 210, loss: 0.00010053018922917545\n",
            "step: 220, loss: 5.1182640163460746e-05\n",
            "step: 230, loss: 0.00016221955593209714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9853107344632768, f1=0.9787234042553192, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.640360566554591e-05\n",
            "step: 10, loss: 0.000158798458869569\n",
            "step: 20, loss: 7.261527935042977e-05\n",
            "step: 30, loss: 0.0029541479889303446\n",
            "step: 40, loss: 6.601745553780347e-05\n",
            "step: 50, loss: 7.890408596722409e-05\n",
            "step: 60, loss: 0.00010680993727874011\n",
            "step: 70, loss: 0.0002971779613289982\n",
            "step: 80, loss: 0.0002940516860689968\n",
            "step: 90, loss: 4.9379854317521676e-05\n",
            "step: 100, loss: 5.428829535958357e-05\n",
            "step: 110, loss: 0.00010257303802063689\n",
            "step: 120, loss: 0.0001378476881654933\n",
            "step: 130, loss: 4.744710531667806e-05\n",
            "step: 140, loss: 8.728313696337864e-05\n",
            "step: 150, loss: 0.00021239681518636644\n",
            "step: 160, loss: 4.885893577011302e-05\n",
            "step: 170, loss: 9.024206519825384e-05\n",
            "step: 180, loss: 0.00010667569586075842\n",
            "step: 190, loss: 6.770806066924706e-05\n",
            "step: 200, loss: 5.520151535165496e-05\n",
            "step: 210, loss: 9.356454393127933e-05\n",
            "step: 220, loss: 6.662544910795987e-05\n",
            "step: 230, loss: 0.045218296349048615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9853768278965129, f1=0.9777777777777777, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003606655518524349\n",
            "step: 10, loss: 0.0011080448748543859\n",
            "step: 20, loss: 0.0004454113950487226\n",
            "step: 30, loss: 8.668890222907066e-05\n",
            "step: 40, loss: 0.0007098490023054183\n",
            "step: 50, loss: 5.148863419890404e-05\n",
            "step: 60, loss: 5.986414180370048e-05\n",
            "step: 70, loss: 0.001369922305457294\n",
            "step: 80, loss: 5.07326731167268e-05\n",
            "step: 90, loss: 0.09111092984676361\n",
            "step: 100, loss: 2.7812224288936704e-05\n",
            "step: 110, loss: 0.0003105713112745434\n",
            "step: 120, loss: 0.0002807806304190308\n",
            "step: 130, loss: 0.0019387674983590841\n",
            "step: 140, loss: 3.849569839076139e-05\n",
            "step: 150, loss: 6.316399230854586e-05\n",
            "step: 160, loss: 3.4305048757232726e-05\n",
            "step: 170, loss: 0.0001381261390633881\n",
            "step: 180, loss: 6.75842966302298e-05\n",
            "step: 190, loss: 3.495247074170038e-05\n",
            "step: 200, loss: 0.00012198969488963485\n",
            "step: 210, loss: 2.8329912311164662e-05\n",
            "step: 220, loss: 0.0002327479305677116\n",
            "step: 230, loss: 3.962752452935092e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9841269841269841, f1=0.9831271091113611, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.0682217988651246e-05\n",
            "step: 10, loss: 7.019712938927114e-05\n",
            "step: 20, loss: 8.032324694795534e-05\n",
            "step: 30, loss: 5.743187284679152e-05\n",
            "step: 40, loss: 5.324011362972669e-05\n",
            "step: 50, loss: 4.333762262831442e-05\n",
            "step: 60, loss: 3.780248880502768e-05\n",
            "step: 70, loss: 6.109176320023835e-05\n",
            "step: 80, loss: 0.0001799958699848503\n",
            "step: 90, loss: 6.49614303256385e-05\n",
            "step: 100, loss: 5.043841156293638e-05\n",
            "step: 110, loss: 3.473660399322398e-05\n",
            "step: 120, loss: 5.838511424371973e-05\n",
            "step: 130, loss: 6.552097329404205e-05\n",
            "step: 140, loss: 8.031503239180893e-05\n",
            "step: 150, loss: 6.388021574821323e-05\n",
            "step: 160, loss: 0.0008827882120385766\n",
            "step: 170, loss: 3.395454405108467e-05\n",
            "step: 180, loss: 4.47346901637502e-05\n",
            "step: 190, loss: 5.025597783969715e-05\n",
            "step: 200, loss: 4.732410525321029e-05\n",
            "step: 210, loss: 5.788373528048396e-05\n",
            "step: 220, loss: 5.7231998653151095e-05\n",
            "step: 230, loss: 5.851074820384383e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9853768278965129, f1=0.9799107142857142, best_f1=0.9776286353467561\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.1444989619776607e-05\n",
            "step: 10, loss: 2.4441162167931907e-05\n",
            "step: 20, loss: 0.0001459898048778996\n",
            "step: 30, loss: 4.6522683987859637e-05\n",
            "step: 40, loss: 0.0007660037372261286\n",
            "step: 50, loss: 0.010284139774739742\n",
            "step: 60, loss: 4.3758656829595566e-05\n",
            "step: 70, loss: 4.199723116471432e-05\n",
            "step: 80, loss: 0.00013999160728417337\n",
            "step: 90, loss: 0.003579790471121669\n",
            "step: 100, loss: 0.0005790345603600144\n",
            "step: 110, loss: 6.433464295696467e-05\n",
            "step: 120, loss: 6.402860162779689e-05\n",
            "step: 130, loss: 0.0001414270664099604\n",
            "step: 140, loss: 3.345579534652643e-05\n",
            "step: 150, loss: 0.0001055607499438338\n",
            "step: 160, loss: 3.618956543505192e-05\n",
            "step: 170, loss: 7.966989505803213e-05\n",
            "step: 180, loss: 8.642025204608217e-05\n",
            "step: 190, loss: 9.063282777788118e-05\n",
            "step: 200, loss: 0.00010273526277160272\n",
            "step: 210, loss: 4.5516484533436596e-05\n",
            "step: 220, loss: 0.000106295759906061\n",
            "step: 230, loss: 5.0993243348784745e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9841986455981941, f1=0.980963045912654, best_f1=0.9776286353467561\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 219.66it/s]\n",
            "load_f1 = 0.9864559819413092\n",
            "real_f1 = 0.9875706214689265\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 267.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90055b08-d13e-425c-b877-4c64073b3f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 301kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 254kB/s] \n",
            "Downloading: 100% 440M/440M [00:07<00:00, 55.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6292595267295837\n",
            "step: 10, loss: 0.5241456627845764\n",
            "step: 20, loss: 0.5412311553955078\n",
            "step: 30, loss: 0.2758166491985321\n",
            "step: 40, loss: 0.21162381768226624\n",
            "step: 50, loss: 0.2294643521308899\n",
            "step: 60, loss: 0.07686296105384827\n",
            "step: 70, loss: 0.1794697493314743\n",
            "step: 80, loss: 0.06249404698610306\n",
            "step: 90, loss: 0.23383788764476776\n",
            "step: 100, loss: 0.03747272118926048\n",
            "step: 110, loss: 0.16519811749458313\n",
            "step: 120, loss: 0.1257106512784958\n",
            "step: 130, loss: 0.03817928954958916\n",
            "step: 140, loss: 0.07654178887605667\n",
            "step: 150, loss: 0.1262238621711731\n",
            "step: 160, loss: 0.055399730801582336\n",
            "step: 170, loss: 0.2391255497932434\n",
            "step: 180, loss: 0.11300566047430038\n",
            "step: 190, loss: 0.018327001482248306\n",
            "step: 200, loss: 0.14910788834095\n",
            "step: 210, loss: 0.06383755803108215\n",
            "step: 220, loss: 0.2640988230705261\n",
            "step: 230, loss: 0.2049870640039444\n",
            "step: 240, loss: 0.08612660318613052\n",
            "step: 250, loss: 0.04586506634950638\n",
            "step: 260, loss: 0.15388384461402893\n",
            "step: 270, loss: 0.012347766198217869\n",
            "step: 280, loss: 0.06911744922399521\n",
            "step: 290, loss: 0.23481285572052002\n",
            "step: 300, loss: 0.1338557004928589\n",
            "step: 310, loss: 0.18356655538082123\n",
            "step: 320, loss: 0.06826626509428024\n",
            "step: 330, loss: 0.052743468433618546\n",
            "step: 340, loss: 0.0949038565158844\n",
            "step: 350, loss: 0.06264176219701767\n",
            "step: 360, loss: 0.11687801033258438\n",
            "step: 370, loss: 0.128071129322052\n",
            "step: 380, loss: 0.02153776027262211\n",
            "step: 390, loss: 0.1777782291173935\n",
            "step: 400, loss: 0.28476157784461975\n",
            "step: 410, loss: 0.07342232018709183\n",
            "step: 420, loss: 0.09527470171451569\n",
            "step: 430, loss: 0.2101748138666153\n",
            "step: 440, loss: 0.048064280301332474\n",
            "step: 450, loss: 0.02241206169128418\n",
            "step: 460, loss: 0.08090080320835114\n",
            "step: 470, loss: 0.11579643189907074\n",
            "step: 480, loss: 0.12319420278072357\n",
            "step: 490, loss: 0.08605089783668518\n",
            "step: 500, loss: 0.09878551214933395\n",
            "step: 510, loss: 0.031415410339832306\n",
            "step: 520, loss: 0.03261495381593704\n",
            "step: 530, loss: 0.006239640060812235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9177126917712692, f1=0.9094287041337669, best_f1=0.9094287041337669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1855548769235611\n",
            "step: 10, loss: 0.09075064212083817\n",
            "step: 20, loss: 0.05159248411655426\n",
            "step: 30, loss: 0.015181455761194229\n",
            "step: 40, loss: 0.06025256961584091\n",
            "step: 50, loss: 0.12272150069475174\n",
            "step: 60, loss: 0.022403771057724953\n",
            "step: 70, loss: 0.04784879833459854\n",
            "step: 80, loss: 0.039197538048028946\n",
            "step: 90, loss: 0.020221777260303497\n",
            "step: 100, loss: 0.018827252089977264\n",
            "step: 110, loss: 0.07474356144666672\n",
            "step: 120, loss: 0.08009224385023117\n",
            "step: 130, loss: 0.11337782442569733\n",
            "step: 140, loss: 0.06185245141386986\n",
            "step: 150, loss: 0.0706922858953476\n",
            "step: 160, loss: 0.01749420538544655\n",
            "step: 170, loss: 0.02778957411646843\n",
            "step: 180, loss: 0.04799700155854225\n",
            "step: 190, loss: 0.06773550808429718\n",
            "step: 200, loss: 0.025821393355727196\n",
            "step: 210, loss: 0.06775090843439102\n",
            "step: 220, loss: 0.05011194571852684\n",
            "step: 230, loss: 0.0023293967824429274\n",
            "step: 240, loss: 0.1620742231607437\n",
            "step: 250, loss: 0.038859229534864426\n",
            "step: 260, loss: 0.006959986872971058\n",
            "step: 270, loss: 0.2919214963912964\n",
            "step: 280, loss: 0.010530252940952778\n",
            "step: 290, loss: 0.06518350541591644\n",
            "step: 300, loss: 0.17350277304649353\n",
            "step: 310, loss: 0.018939483910799026\n",
            "step: 320, loss: 0.08419772237539291\n",
            "step: 330, loss: 0.11764539033174515\n",
            "step: 340, loss: 0.023456400260329247\n",
            "step: 350, loss: 0.003406904172152281\n",
            "step: 360, loss: 0.07894114404916763\n",
            "step: 370, loss: 0.1006658673286438\n",
            "step: 380, loss: 0.04969139024615288\n",
            "step: 390, loss: 0.06612755358219147\n",
            "step: 400, loss: 0.1343858391046524\n",
            "step: 410, loss: 0.04157615080475807\n",
            "step: 420, loss: 0.05197615921497345\n",
            "step: 430, loss: 0.011977686546742916\n",
            "step: 440, loss: 0.023150956258177757\n",
            "step: 450, loss: 0.005144699010998011\n",
            "step: 460, loss: 0.061785031110048294\n",
            "step: 470, loss: 0.0518442802131176\n",
            "step: 480, loss: 0.3840582072734833\n",
            "step: 490, loss: 0.016895651817321777\n",
            "step: 500, loss: 0.38235536217689514\n",
            "step: 510, loss: 0.02842988818883896\n",
            "step: 520, loss: 0.06211164593696594\n",
            "step: 530, loss: 0.07814369350671768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9172445677300047, f1=0.9143914854234151, best_f1=0.9094287041337669\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20274955034255981\n",
            "step: 10, loss: 0.11166493594646454\n",
            "step: 20, loss: 0.06103145331144333\n",
            "step: 30, loss: 0.20120938122272491\n",
            "step: 40, loss: 0.023757854476571083\n",
            "step: 50, loss: 0.06861714273691177\n",
            "step: 60, loss: 0.010245643556118011\n",
            "step: 70, loss: 0.0036424368154257536\n",
            "step: 80, loss: 0.00039130469667725265\n",
            "step: 90, loss: 0.0031815001275390387\n",
            "step: 100, loss: 0.0479995422065258\n",
            "step: 110, loss: 0.010031557641923428\n",
            "step: 120, loss: 0.007067587226629257\n",
            "step: 130, loss: 0.0019744131714105606\n",
            "step: 140, loss: 0.014156432822346687\n",
            "step: 150, loss: 0.022721366956830025\n",
            "step: 160, loss: 0.007276264484971762\n",
            "step: 170, loss: 0.050316378474235535\n",
            "step: 180, loss: 0.04257262870669365\n",
            "step: 190, loss: 0.0432916060090065\n",
            "step: 200, loss: 0.11889093369245529\n",
            "step: 210, loss: 0.027320140972733498\n",
            "step: 220, loss: 0.16965264081954956\n",
            "step: 230, loss: 0.16168726980686188\n",
            "step: 240, loss: 0.016871383413672447\n",
            "step: 250, loss: 0.017369598150253296\n",
            "step: 260, loss: 0.022336456924676895\n",
            "step: 270, loss: 0.01240269560366869\n",
            "step: 280, loss: 0.09822557866573334\n",
            "step: 290, loss: 0.009159758687019348\n",
            "step: 300, loss: 0.03079536370933056\n",
            "step: 310, loss: 0.004888387396931648\n",
            "step: 320, loss: 0.033161673694849014\n",
            "step: 330, loss: 0.0014068839373067021\n",
            "step: 340, loss: 0.039964620023965836\n",
            "step: 350, loss: 0.08842281997203827\n",
            "step: 360, loss: 0.07358375936746597\n",
            "step: 370, loss: 0.028494877740740776\n",
            "step: 380, loss: 0.03684544563293457\n",
            "step: 390, loss: 0.008694295771420002\n",
            "step: 400, loss: 0.008539127185940742\n",
            "step: 410, loss: 0.020855195820331573\n",
            "step: 420, loss: 0.1981791853904724\n",
            "step: 430, loss: 0.005431808531284332\n",
            "step: 440, loss: 0.005484815686941147\n",
            "step: 450, loss: 0.08768568187952042\n",
            "step: 460, loss: 0.05184788256883621\n",
            "step: 470, loss: 0.07201480120420456\n",
            "step: 480, loss: 0.017078375443816185\n",
            "step: 490, loss: 0.005058600567281246\n",
            "step: 500, loss: 0.008187989704310894\n",
            "step: 510, loss: 0.0064084772020578384\n",
            "step: 520, loss: 0.017921103164553642\n",
            "step: 530, loss: 0.07256343960762024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9257884972170687, f1=0.9136490250696379, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012251448817551136\n",
            "step: 10, loss: 0.028203625231981277\n",
            "step: 20, loss: 0.0011847828282043338\n",
            "step: 30, loss: 0.010636034421622753\n",
            "step: 40, loss: 0.001994851278141141\n",
            "step: 50, loss: 0.06524886935949326\n",
            "step: 60, loss: 0.0004916097968816757\n",
            "step: 70, loss: 0.0017007613787427545\n",
            "step: 80, loss: 0.0009920833399519324\n",
            "step: 90, loss: 0.034088943153619766\n",
            "step: 100, loss: 0.008503897115588188\n",
            "step: 110, loss: 0.004090732894837856\n",
            "step: 120, loss: 0.00027571909595280886\n",
            "step: 130, loss: 0.00923825427889824\n",
            "step: 140, loss: 0.005794602911919355\n",
            "step: 150, loss: 0.08688487857580185\n",
            "step: 160, loss: 0.04171200469136238\n",
            "step: 170, loss: 0.01058843731880188\n",
            "step: 180, loss: 0.010342597961425781\n",
            "step: 190, loss: 0.026668336242437363\n",
            "step: 200, loss: 0.008357802405953407\n",
            "step: 210, loss: 0.07980653643608093\n",
            "step: 220, loss: 0.005970470607280731\n",
            "step: 230, loss: 0.22685520350933075\n",
            "step: 240, loss: 0.007239742670208216\n",
            "step: 250, loss: 0.2430586963891983\n",
            "step: 260, loss: 0.003947349265217781\n",
            "step: 270, loss: 0.00822496972978115\n",
            "step: 280, loss: 0.06469642370939255\n",
            "step: 290, loss: 0.05334723740816116\n",
            "step: 300, loss: 0.005584970116615295\n",
            "step: 310, loss: 0.03837605193257332\n",
            "step: 320, loss: 0.016373474150896072\n",
            "step: 330, loss: 0.005537836346775293\n",
            "step: 340, loss: 0.028610721230506897\n",
            "step: 350, loss: 0.009495988488197327\n",
            "step: 360, loss: 0.004364070016890764\n",
            "step: 370, loss: 0.015741605311632156\n",
            "step: 380, loss: 0.0005309724365361035\n",
            "step: 390, loss: 0.0013983292737975717\n",
            "step: 400, loss: 0.025310322642326355\n",
            "step: 410, loss: 0.02022848092019558\n",
            "step: 420, loss: 0.004988345317542553\n",
            "step: 430, loss: 0.0650845617055893\n",
            "step: 440, loss: 0.012890959158539772\n",
            "step: 450, loss: 0.008044439367949963\n",
            "step: 460, loss: 0.00279228831641376\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 470, loss: 0.026406843215227127\n",
            "step: 480, loss: 0.11307043582201004\n",
            "step: 490, loss: 0.06795819103717804\n",
            "step: 500, loss: 0.027499500662088394\n",
            "step: 510, loss: 0.07388601452112198\n",
            "step: 520, loss: 0.18033774197101593\n",
            "step: 530, loss: 0.006432600785046816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9184430027803522, f1=0.9111111111111111, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011259259656071663\n",
            "step: 10, loss: 0.014430507086217403\n",
            "step: 20, loss: 0.009627269580960274\n",
            "step: 30, loss: 0.00036947146872989833\n",
            "step: 40, loss: 0.0020079908426851034\n",
            "step: 50, loss: 0.018229100853204727\n",
            "step: 60, loss: 0.12347866594791412\n",
            "step: 70, loss: 0.019194088876247406\n",
            "step: 80, loss: 0.000709387066308409\n",
            "step: 90, loss: 0.01730933226644993\n",
            "step: 100, loss: 0.0124126635491848\n",
            "step: 110, loss: 0.0015604222426190972\n",
            "step: 120, loss: 0.0007556519703939557\n",
            "step: 130, loss: 0.0006119221216067672\n",
            "step: 140, loss: 0.0248943530023098\n",
            "step: 150, loss: 0.0019758413545787334\n",
            "step: 160, loss: 0.008183316327631474\n",
            "step: 170, loss: 0.016103321686387062\n",
            "step: 180, loss: 0.0011633114190772176\n",
            "step: 190, loss: 0.012099655345082283\n",
            "step: 200, loss: 0.0008043827838264406\n",
            "step: 210, loss: 0.0009214807651005685\n",
            "step: 220, loss: 0.03143446147441864\n",
            "step: 230, loss: 0.01515775267034769\n",
            "step: 240, loss: 0.13566987216472626\n",
            "step: 250, loss: 0.0030285411048680544\n",
            "step: 260, loss: 0.005179464817047119\n",
            "step: 270, loss: 0.04240249842405319\n",
            "step: 280, loss: 0.03282366320490837\n",
            "step: 290, loss: 0.019243117421865463\n",
            "step: 300, loss: 0.00538916140794754\n",
            "step: 310, loss: 0.0005949695478193462\n",
            "step: 320, loss: 0.00967409834265709\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 330, loss: 0.004135778173804283\n",
            "step: 340, loss: 0.0054864464327692986\n",
            "step: 350, loss: 0.009542322717607021\n",
            "step: 360, loss: 0.009919105097651482\n",
            "step: 370, loss: 0.020972643047571182\n",
            "step: 380, loss: 0.011542077176272869\n",
            "step: 390, loss: 0.0002815184707287699\n",
            "step: 400, loss: 0.0013202282134443521\n",
            "step: 410, loss: 0.0006294830818660557\n",
            "step: 420, loss: 0.07810048013925552\n",
            "step: 430, loss: 0.0009386423625983298\n",
            "step: 440, loss: 0.0049060313031077385\n",
            "step: 450, loss: 0.006968605797737837\n",
            "step: 460, loss: 0.13293731212615967\n",
            "step: 470, loss: 0.029359495267271996\n",
            "step: 480, loss: 0.021711237728595734\n",
            "step: 490, loss: 0.0037637706845998764\n",
            "step: 500, loss: 0.036442384123802185\n",
            "step: 510, loss: 0.06823689490556717\n",
            "step: 520, loss: 0.006009566597640514\n",
            "step: 530, loss: 0.03209169954061508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9219924812030075, f1=0.9141248240262787, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038415638264268637\n",
            "step: 10, loss: 0.0004506949335336685\n",
            "step: 20, loss: 0.13600262999534607\n",
            "step: 30, loss: 0.0038290026132017374\n",
            "step: 40, loss: 0.010466529987752438\n",
            "step: 50, loss: 0.0032597950194031\n",
            "step: 60, loss: 0.0009400260751135647\n",
            "step: 70, loss: 0.0006095021381042898\n",
            "step: 80, loss: 0.0027677295729517937\n",
            "step: 90, loss: 0.00021338473015930504\n",
            "step: 100, loss: 0.0034165570978075266\n",
            "step: 110, loss: 0.0016427893424406648\n",
            "step: 120, loss: 0.004648773465305567\n",
            "step: 130, loss: 0.0014181461883708835\n",
            "step: 140, loss: 0.01833917945623398\n",
            "step: 150, loss: 0.0036176894791424274\n",
            "step: 160, loss: 0.005746215116232634\n",
            "step: 170, loss: 0.0034601411316543818\n",
            "step: 180, loss: 0.003710476215928793\n",
            "step: 190, loss: 0.03165451064705849\n",
            "step: 200, loss: 0.0011129735503345728\n",
            "step: 210, loss: 0.041695088148117065\n",
            "step: 220, loss: 0.0015054827090352774\n",
            "step: 230, loss: 0.000605646229814738\n",
            "step: 240, loss: 0.0014108173782005906\n",
            "step: 250, loss: 0.0004438794276211411\n",
            "step: 260, loss: 0.00016106519615277648\n",
            "step: 270, loss: 0.010388505645096302\n",
            "step: 280, loss: 0.0008250685059465468\n",
            "step: 290, loss: 0.0019006183138117194\n",
            "step: 300, loss: 0.0007457399624399841\n",
            "step: 310, loss: 0.0004645981243811548\n",
            "step: 320, loss: 0.00014628680946771055\n",
            "step: 330, loss: 0.00017625506734475493\n",
            "step: 340, loss: 0.3354082405567169\n",
            "step: 350, loss: 0.08591793477535248\n",
            "step: 360, loss: 0.02176196686923504\n",
            "step: 370, loss: 0.008821824565529823\n",
            "step: 380, loss: 0.0004673159564845264\n",
            "step: 390, loss: 0.0030526253394782543\n",
            "step: 400, loss: 0.0019930130802094936\n",
            "step: 410, loss: 0.007937571033835411\n",
            "step: 420, loss: 0.0012885527685284615\n",
            "step: 430, loss: 0.12044724076986313\n",
            "step: 440, loss: 0.007809057831764221\n",
            "step: 450, loss: 0.0015282963868230581\n",
            "step: 460, loss: 0.0004584007547236979\n",
            "step: 470, loss: 0.033008839935064316\n",
            "step: 480, loss: 0.0043427306227386\n",
            "step: 490, loss: 0.003530388930812478\n",
            "step: 500, loss: 0.0006111947586759925\n",
            "step: 510, loss: 0.0019191757310181856\n",
            "step: 520, loss: 0.006488231010735035\n",
            "step: 530, loss: 0.005743095185607672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9204119850187266, f1=0.9095149253731343, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014867506979499012\n",
            "step: 10, loss: 0.03390058875083923\n",
            "step: 20, loss: 0.006252777762711048\n",
            "step: 30, loss: 0.009573184885084629\n",
            "step: 40, loss: 0.016746852546930313\n",
            "step: 50, loss: 0.0015157908201217651\n",
            "step: 60, loss: 0.0003732129989657551\n",
            "step: 70, loss: 0.00028929111431352794\n",
            "step: 80, loss: 0.0024271158035844564\n",
            "step: 90, loss: 0.00031300634145736694\n",
            "step: 100, loss: 0.0001271791843464598\n",
            "step: 110, loss: 0.00020616964320652187\n",
            "step: 120, loss: 0.003692885860800743\n",
            "step: 130, loss: 0.0012083754409104586\n",
            "step: 140, loss: 0.00033761499798856676\n",
            "step: 150, loss: 0.017765067517757416\n",
            "step: 160, loss: 0.009878742508590221\n",
            "step: 170, loss: 0.00106394337490201\n",
            "step: 180, loss: 0.0003376344102434814\n",
            "step: 190, loss: 0.0019940368365496397\n",
            "step: 200, loss: 0.00040119694313034415\n",
            "step: 210, loss: 0.008541190065443516\n",
            "step: 220, loss: 0.00022601558885071427\n",
            "step: 230, loss: 0.0011295683216303587\n",
            "step: 240, loss: 0.0008802691008895636\n",
            "step: 250, loss: 0.025172779336571693\n",
            "step: 260, loss: 0.0009747175499796867\n",
            "step: 270, loss: 0.00013279034465085715\n",
            "step: 280, loss: 0.0005889395833946764\n",
            "step: 290, loss: 0.0001025043020490557\n",
            "step: 300, loss: 0.25071442127227783\n",
            "step: 310, loss: 0.00020165419846307486\n",
            "step: 320, loss: 0.02247069776058197\n",
            "step: 330, loss: 0.0002877497172448784\n",
            "step: 340, loss: 0.0003545338404364884\n",
            "step: 350, loss: 0.004553951323032379\n",
            "step: 360, loss: 0.0016085507813841105\n",
            "step: 370, loss: 0.0023332107812166214\n",
            "step: 380, loss: 0.018730411306023598\n",
            "step: 390, loss: 0.0014841896481812\n",
            "step: 400, loss: 0.0006484621553681791\n",
            "step: 410, loss: 0.00046631123404949903\n",
            "step: 420, loss: 0.0013130201259627938\n",
            "step: 430, loss: 0.00032575157820247114\n",
            "step: 440, loss: 0.0016825892962515354\n",
            "step: 450, loss: 0.001059907954186201\n",
            "step: 460, loss: 0.001159211155027151\n",
            "step: 470, loss: 0.003919443115592003\n",
            "step: 480, loss: 0.06684839725494385\n",
            "step: 490, loss: 0.0015378267271444201\n",
            "step: 500, loss: 0.021573642268776894\n",
            "step: 510, loss: 0.001519518904387951\n",
            "step: 520, loss: 0.010705350898206234\n",
            "step: 530, loss: 0.0017991483910009265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9246511627906977, f1=0.9090909090909092, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016714130470063537\n",
            "step: 10, loss: 0.0006173359579406679\n",
            "step: 20, loss: 0.0006469584186561406\n",
            "step: 30, loss: 0.00013029453111812472\n",
            "step: 40, loss: 0.0004040873027406633\n",
            "step: 50, loss: 0.0015879437560215592\n",
            "step: 60, loss: 0.00017573987133800983\n",
            "step: 70, loss: 0.00012610381236299872\n",
            "step: 80, loss: 0.0009466487099416554\n",
            "step: 90, loss: 0.0015323205152526498\n",
            "step: 100, loss: 0.00029330889810808003\n",
            "step: 110, loss: 0.0004958404460921884\n",
            "step: 120, loss: 0.0020177369005978107\n",
            "step: 130, loss: 0.0018410453340038657\n",
            "step: 140, loss: 0.002190760802477598\n",
            "step: 150, loss: 0.00042789540020748973\n",
            "step: 160, loss: 0.14524757862091064\n",
            "step: 170, loss: 0.003348702099174261\n",
            "step: 180, loss: 0.010691553354263306\n",
            "step: 190, loss: 0.005035657435655594\n",
            "step: 200, loss: 0.002593335695564747\n",
            "step: 210, loss: 0.005002851597964764\n",
            "step: 220, loss: 0.003439251333475113\n",
            "step: 230, loss: 0.0005078867543488741\n",
            "step: 240, loss: 0.00035655396641232073\n",
            "step: 250, loss: 0.00020209782815072685\n",
            "step: 260, loss: 5.099350164528005e-05\n",
            "step: 270, loss: 0.0010871805716305971\n",
            "step: 280, loss: 0.12490841001272202\n",
            "step: 290, loss: 0.0008265124633908272\n",
            "step: 300, loss: 0.003196248086169362\n",
            "step: 310, loss: 0.041697002947330475\n",
            "step: 320, loss: 0.003212974639609456\n",
            "step: 330, loss: 0.0003239848301745951\n",
            "step: 340, loss: 0.0007042799261398613\n",
            "step: 350, loss: 0.00020958912500645965\n",
            "step: 360, loss: 0.0002984145248774439\n",
            "step: 370, loss: 0.00010592663602437824\n",
            "step: 380, loss: 0.007140308618545532\n",
            "step: 390, loss: 0.05474354699254036\n",
            "step: 400, loss: 0.013803962618112564\n",
            "step: 410, loss: 0.0004100946243852377\n",
            "step: 420, loss: 0.0004991780151613057\n",
            "step: 430, loss: 0.00010490623390069231\n",
            "step: 440, loss: 0.0018744246335700154\n",
            "step: 450, loss: 8.601044828537852e-05\n",
            "step: 460, loss: 0.00010475435556145385\n",
            "step: 470, loss: 0.00024303716782014817\n",
            "step: 480, loss: 5.097725443192758e-05\n",
            "step: 490, loss: 0.0004989289445802569\n",
            "step: 500, loss: 0.00010599527740851045\n",
            "step: 510, loss: 0.00020238345314282924\n",
            "step: 520, loss: 0.0003316465299576521\n",
            "step: 530, loss: 0.00018537462165113539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.912739150723285, f1=0.9108635097493036, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00042649658280424774\n",
            "step: 10, loss: 7.33766210032627e-05\n",
            "step: 20, loss: 0.0004519375797826797\n",
            "step: 30, loss: 0.002593549434095621\n",
            "step: 40, loss: 0.0001229705521836877\n",
            "step: 50, loss: 8.718219760339707e-05\n",
            "step: 60, loss: 4.5271634007804096e-05\n",
            "step: 70, loss: 0.014197969809174538\n",
            "step: 80, loss: 0.0002422226534690708\n",
            "step: 90, loss: 7.04526828485541e-05\n",
            "step: 100, loss: 0.00010178719094255939\n",
            "step: 110, loss: 0.00015509602962993085\n",
            "step: 120, loss: 0.0008270168327726424\n",
            "step: 130, loss: 0.041889533400535583\n",
            "step: 140, loss: 0.03189496323466301\n",
            "step: 150, loss: 0.0004198001406621188\n",
            "step: 160, loss: 0.001884213648736477\n",
            "step: 170, loss: 0.009088393300771713\n",
            "step: 180, loss: 0.000565598311368376\n",
            "step: 190, loss: 0.00014845664554741234\n",
            "step: 200, loss: 0.000304271699860692\n",
            "step: 210, loss: 0.00032206744072027504\n",
            "step: 220, loss: 0.0005223358166404068\n",
            "step: 230, loss: 8.815452747512609e-05\n",
            "step: 240, loss: 0.0028557749465107918\n",
            "step: 250, loss: 0.0008612078963778913\n",
            "step: 260, loss: 0.003977011889219284\n",
            "step: 270, loss: 3.525875217746943e-05\n",
            "step: 280, loss: 9.610191045794636e-05\n",
            "step: 290, loss: 9.223592496709898e-05\n",
            "step: 300, loss: 3.5756831493927166e-05\n",
            "step: 310, loss: 0.0004322721215430647\n",
            "step: 320, loss: 0.0003398796543478966\n",
            "step: 330, loss: 9.862925071502104e-05\n",
            "step: 340, loss: 3.352611747686751e-05\n",
            "step: 350, loss: 0.005008072126656771\n",
            "step: 360, loss: 4.9403886805521324e-05\n",
            "step: 370, loss: 0.003438948653638363\n",
            "step: 380, loss: 8.660833555040881e-05\n",
            "step: 390, loss: 0.0002300425840076059\n",
            "step: 400, loss: 0.031503550708293915\n",
            "step: 410, loss: 0.00016708651673980057\n",
            "step: 420, loss: 2.9514252673834562e-05\n",
            "step: 430, loss: 4.9157097237184644e-05\n",
            "step: 440, loss: 0.0006007355987094343\n",
            "step: 450, loss: 2.868788033083547e-05\n",
            "step: 460, loss: 3.083308911300264e-05\n",
            "step: 470, loss: 3.128967364318669e-05\n",
            "step: 480, loss: 0.0004645706794690341\n",
            "step: 490, loss: 0.00021663862571585923\n",
            "step: 500, loss: 0.00022310539497993886\n",
            "step: 510, loss: 6.935677083674818e-05\n",
            "step: 520, loss: 6.534886779263616e-05\n",
            "step: 530, loss: 7.573691254947335e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9205453690644099, f1=0.9158530915853093, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002067928435280919\n",
            "step: 10, loss: 0.04984280467033386\n",
            "step: 20, loss: 3.3248212275793776e-05\n",
            "step: 30, loss: 0.011095286346971989\n",
            "step: 40, loss: 0.014656639657914639\n",
            "step: 50, loss: 0.010232078842818737\n",
            "step: 60, loss: 4.96519060106948e-05\n",
            "step: 70, loss: 5.754476660513319e-05\n",
            "step: 80, loss: 0.0010301638394594193\n",
            "step: 90, loss: 8.399381476920098e-05\n",
            "step: 100, loss: 3.756740625249222e-05\n",
            "step: 110, loss: 8.784302190179005e-05\n",
            "step: 120, loss: 9.745029819896445e-05\n",
            "step: 130, loss: 3.087371078436263e-05\n",
            "step: 140, loss: 0.00021034265228081495\n",
            "step: 150, loss: 2.7201193006476387e-05\n",
            "step: 160, loss: 5.8042372984346e-05\n",
            "step: 170, loss: 0.00031023877090774477\n",
            "step: 180, loss: 0.001350087346509099\n",
            "step: 190, loss: 0.00018661897047422826\n",
            "step: 200, loss: 0.0021551884710788727\n",
            "step: 210, loss: 0.0004460710915736854\n",
            "step: 220, loss: 0.003167755901813507\n",
            "step: 230, loss: 0.00020122916612308472\n",
            "step: 240, loss: 6.155677692731842e-05\n",
            "step: 250, loss: 7.69945327192545e-05\n",
            "step: 260, loss: 0.00021212564024608582\n",
            "step: 270, loss: 9.673328895587474e-05\n",
            "step: 280, loss: 0.00214194948785007\n",
            "step: 290, loss: 2.5591671146685258e-05\n",
            "step: 300, loss: 2.4649449187563732e-05\n",
            "step: 310, loss: 2.2537553377333097e-05\n",
            "step: 320, loss: 0.00023045972920954227\n",
            "step: 330, loss: 8.095172233879566e-05\n",
            "step: 340, loss: 0.00019244976283516735\n",
            "step: 350, loss: 0.0014606531476601958\n",
            "step: 360, loss: 0.00012108897499274462\n",
            "step: 370, loss: 0.002158563816919923\n",
            "step: 380, loss: 0.00011504756548674777\n",
            "step: 390, loss: 0.0027866056188941\n",
            "step: 400, loss: 5.597628478426486e-05\n",
            "step: 410, loss: 0.00033000303665176034\n",
            "step: 420, loss: 0.0012476493138819933\n",
            "step: 430, loss: 0.00024324825790245086\n",
            "step: 440, loss: 0.006242386996746063\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 450, loss: 0.0010609725723043084\n",
            "step: 460, loss: 4.67174977529794e-05\n",
            "step: 470, loss: 0.002597033977508545\n",
            "step: 480, loss: 0.00034503822098486125\n",
            "step: 490, loss: 0.0009580937912687659\n",
            "step: 500, loss: 0.00223310268484056\n",
            "step: 510, loss: 4.669924601330422e-05\n",
            "step: 520, loss: 6.40804209979251e-05\n",
            "step: 530, loss: 0.0026158581022173166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9180327868852459, f1=0.9149232914923291, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018357131630182266\n",
            "step: 10, loss: 0.0003927650104742497\n",
            "step: 20, loss: 0.005908928345888853\n",
            "step: 30, loss: 0.0003872393863275647\n",
            "step: 40, loss: 0.00010647383896866813\n",
            "step: 50, loss: 0.0006494352710433304\n",
            "step: 60, loss: 0.00036462536081671715\n",
            "step: 70, loss: 0.0005380547372624278\n",
            "step: 80, loss: 0.00021875265520066023\n",
            "step: 90, loss: 0.00011829185677925125\n",
            "step: 100, loss: 0.0001397558080498129\n",
            "step: 110, loss: 0.00011854553304146975\n",
            "step: 120, loss: 0.00034669210435822606\n",
            "step: 130, loss: 0.00013089634012430906\n",
            "step: 140, loss: 0.00016908383986447006\n",
            "step: 150, loss: 1.8771475879475474e-05\n",
            "step: 160, loss: 0.00017446895071770996\n",
            "step: 170, loss: 0.0005102959694340825\n",
            "step: 180, loss: 0.00018538373115006834\n",
            "step: 190, loss: 0.0008197808638215065\n",
            "step: 200, loss: 0.001298550982028246\n",
            "step: 210, loss: 2.7849640900967643e-05\n",
            "step: 220, loss: 0.004357457160949707\n",
            "step: 230, loss: 0.0006578196189366281\n",
            "step: 240, loss: 0.00013285773457027972\n",
            "step: 250, loss: 2.8113303415011615e-05\n",
            "step: 260, loss: 3.182974978699349e-05\n",
            "step: 270, loss: 0.00019442036864347756\n",
            "step: 280, loss: 0.00034900737227872014\n",
            "step: 290, loss: 8.60176223795861e-05\n",
            "step: 300, loss: 4.790541424881667e-05\n",
            "step: 310, loss: 3.603275035857223e-05\n",
            "step: 320, loss: 2.101023892464582e-05\n",
            "step: 330, loss: 6.3285646319855e-05\n",
            "step: 340, loss: 6.103983469074592e-05\n",
            "step: 350, loss: 1.7374522940372117e-05\n",
            "step: 360, loss: 2.116273572028149e-05\n",
            "step: 370, loss: 0.00016381131717935205\n",
            "step: 380, loss: 2.1945252228761092e-05\n",
            "step: 390, loss: 1.6446972949779592e-05\n",
            "step: 400, loss: 5.2028564823558554e-05\n",
            "step: 410, loss: 6.946125358808786e-05\n",
            "step: 420, loss: 3.2110845495481044e-05\n",
            "step: 430, loss: 2.1859426851733588e-05\n",
            "step: 440, loss: 5.640901508741081e-05\n",
            "step: 450, loss: 0.0007005927618592978\n",
            "step: 460, loss: 0.0001601131516508758\n",
            "step: 470, loss: 2.4392646082560532e-05\n",
            "step: 480, loss: 4.320354855735786e-05\n",
            "step: 490, loss: 3.08097769448068e-05\n",
            "step: 500, loss: 5.79151492274832e-05\n",
            "step: 510, loss: 2.2820451704319566e-05\n",
            "step: 520, loss: 0.008516148664057255\n",
            "step: 530, loss: 2.1285726688802242e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9118483412322276, f1=0.9115582592419279, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.545033385104034e-05\n",
            "step: 10, loss: 2.8690439648926258e-05\n",
            "step: 20, loss: 2.801276059472002e-05\n",
            "step: 30, loss: 4.200956391287036e-05\n",
            "step: 40, loss: 0.0012879689456894994\n",
            "step: 50, loss: 0.0002518908877391368\n",
            "step: 60, loss: 0.000456731126178056\n",
            "step: 70, loss: 2.2150208678795025e-05\n",
            "step: 80, loss: 1.880868512671441e-05\n",
            "step: 90, loss: 2.332329677301459e-05\n",
            "step: 100, loss: 0.00014460274542216212\n",
            "step: 110, loss: 3.2362451747758314e-05\n",
            "step: 120, loss: 2.4407176169916056e-05\n",
            "step: 130, loss: 8.596352563472465e-05\n",
            "step: 140, loss: 3.6171870306134224e-05\n",
            "step: 150, loss: 1.383558992529288e-05\n",
            "step: 160, loss: 0.0001231275382451713\n",
            "step: 170, loss: 3.977967207902111e-05\n",
            "step: 180, loss: 0.000998157076537609\n",
            "step: 190, loss: 0.00012724175758194178\n",
            "step: 200, loss: 3.07871559925843e-05\n",
            "step: 210, loss: 0.0004316982231102884\n",
            "step: 220, loss: 3.801032289629802e-05\n",
            "step: 230, loss: 1.8212564100394957e-05\n",
            "step: 240, loss: 0.0005341109936125576\n",
            "step: 250, loss: 6.949709495529532e-05\n",
            "step: 260, loss: 0.00013072283763904124\n",
            "step: 270, loss: 0.00026644623721949756\n",
            "step: 280, loss: 0.0001997221406782046\n",
            "step: 290, loss: 8.08191834948957e-05\n",
            "step: 300, loss: 0.00018782315601129085\n",
            "step: 310, loss: 4.472443833947182e-05\n",
            "step: 320, loss: 2.6242370950058103e-05\n",
            "step: 330, loss: 4.1104183765128255e-05\n",
            "step: 340, loss: 5.48840471310541e-05\n",
            "step: 350, loss: 2.8515974918263964e-05\n",
            "step: 360, loss: 5.538127152249217e-05\n",
            "step: 370, loss: 1.9106513718725182e-05\n",
            "step: 380, loss: 3.700470915646292e-05\n",
            "step: 390, loss: 2.0060246242792346e-05\n",
            "step: 400, loss: 0.00021557418222073466\n",
            "step: 410, loss: 0.000714583380613476\n",
            "step: 420, loss: 0.00010434497380629182\n",
            "step: 430, loss: 0.00016715130186639726\n",
            "step: 440, loss: 1.8078497305396013e-05\n",
            "step: 450, loss: 6.633823795709759e-05\n",
            "step: 460, loss: 1.4636461855843663e-05\n",
            "step: 470, loss: 2.5621029635658488e-05\n",
            "step: 480, loss: 5.3854364523431286e-05\n",
            "step: 490, loss: 4.1352850530529395e-05\n",
            "step: 500, loss: 1.5090975466591772e-05\n",
            "step: 510, loss: 0.000527373340446502\n",
            "step: 520, loss: 1.9303955923533067e-05\n",
            "step: 530, loss: 2.469740684318822e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9167855444602948, f1=0.9092618711800659, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022030407562851906\n",
            "step: 10, loss: 1.511703976575518e-05\n",
            "step: 20, loss: 1.09783477455494e-05\n",
            "step: 30, loss: 3.669544094009325e-05\n",
            "step: 40, loss: 1.3030941772740334e-05\n",
            "step: 50, loss: 2.9289298254298046e-05\n",
            "step: 60, loss: 0.000136022106744349\n",
            "step: 70, loss: 2.1579739041044377e-05\n",
            "step: 80, loss: 0.00010182994446950033\n",
            "step: 90, loss: 1.990704913623631e-05\n",
            "step: 100, loss: 2.479413706168998e-05\n",
            "step: 110, loss: 1.2922911082569044e-05\n",
            "step: 120, loss: 0.00024913097149692476\n",
            "step: 130, loss: 4.441468990989961e-05\n",
            "step: 140, loss: 0.00016415906429756433\n",
            "step: 150, loss: 1.8849503248929977e-05\n",
            "step: 160, loss: 1.4368285519594792e-05\n",
            "step: 170, loss: 0.0006751790642738342\n",
            "step: 180, loss: 7.090876169968396e-05\n",
            "step: 190, loss: 4.797809378942475e-05\n",
            "step: 200, loss: 0.0002177448768634349\n",
            "step: 210, loss: 5.424793198471889e-05\n",
            "step: 220, loss: 1.6625695934635587e-05\n",
            "step: 230, loss: 1.702424015093129e-05\n",
            "step: 240, loss: 0.005583968944847584\n",
            "step: 250, loss: 3.609544364735484e-05\n",
            "step: 260, loss: 1.2028862329316325e-05\n",
            "step: 270, loss: 0.00017362984362989664\n",
            "step: 280, loss: 2.9950295356684364e-05\n",
            "step: 290, loss: 2.1147236111573875e-05\n",
            "step: 300, loss: 1.887191683636047e-05\n",
            "step: 310, loss: 7.13862682459876e-05\n",
            "step: 320, loss: 0.000194824009668082\n",
            "step: 330, loss: 4.6030330850044265e-05\n",
            "step: 340, loss: 1.6226989828282967e-05\n",
            "step: 350, loss: 0.008098144084215164\n",
            "step: 360, loss: 1.5884388631093316e-05\n",
            "step: 370, loss: 2.7401361876400188e-05\n",
            "step: 380, loss: 1.497178618592443e-05\n",
            "step: 390, loss: 0.0006920714513398707\n",
            "step: 400, loss: 0.013057461008429527\n",
            "step: 410, loss: 6.0428486904129386e-05\n",
            "step: 420, loss: 1.5504323528148234e-05\n",
            "step: 430, loss: 9.669783321442083e-05\n",
            "step: 440, loss: 0.0006827648030593991\n",
            "step: 450, loss: 0.00029797927709296346\n",
            "step: 460, loss: 8.320822234963998e-05\n",
            "step: 470, loss: 3.242158345528878e-05\n",
            "step: 480, loss: 4.39739778812509e-05\n",
            "step: 490, loss: 0.0009211646392941475\n",
            "step: 500, loss: 4.1045139369089156e-05\n",
            "step: 510, loss: 0.1289994865655899\n",
            "step: 520, loss: 3.210422437405214e-05\n",
            "step: 530, loss: 4.279016866348684e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9050170482221139, f1=0.8794117647058824, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.768699257809203e-05\n",
            "step: 10, loss: 9.24611413211096e-06\n",
            "step: 20, loss: 7.038878538878635e-05\n",
            "step: 30, loss: 3.497257421258837e-05\n",
            "step: 40, loss: 1.896874346130062e-05\n",
            "step: 50, loss: 0.030894627794623375\n",
            "step: 60, loss: 6.0144469898659736e-05\n",
            "step: 70, loss: 9.506958303973079e-05\n",
            "step: 80, loss: 0.0001722960005281493\n",
            "step: 90, loss: 9.551404946250841e-05\n",
            "step: 100, loss: 0.009285589680075645\n",
            "step: 110, loss: 4.2557090637274086e-05\n",
            "step: 120, loss: 2.8157475753687322e-05\n",
            "step: 130, loss: 3.1239505915436894e-05\n",
            "step: 140, loss: 7.571840978926048e-05\n",
            "step: 150, loss: 1.4867076970404014e-05\n",
            "step: 160, loss: 4.0718285163166e-05\n",
            "step: 170, loss: 0.0008979073609225452\n",
            "step: 180, loss: 4.957486453349702e-05\n",
            "step: 190, loss: 1.8372287740930915e-05\n",
            "step: 200, loss: 2.511756247258745e-05\n",
            "step: 210, loss: 1.4137289326754399e-05\n",
            "step: 220, loss: 4.5621698518516496e-05\n",
            "step: 230, loss: 6.634694000240415e-05\n",
            "step: 240, loss: 1.3351207599043846e-05\n",
            "step: 250, loss: 2.1855710656382143e-05\n",
            "step: 260, loss: 0.00019112277368549258\n",
            "step: 270, loss: 0.00023554319341201335\n",
            "step: 280, loss: 1.091501417249674e-05\n",
            "step: 290, loss: 3.907713835360482e-05\n",
            "step: 300, loss: 4.811911639990285e-05\n",
            "step: 310, loss: 7.286226173164323e-05\n",
            "step: 320, loss: 2.7356993086868897e-05\n",
            "step: 330, loss: 3.6551344237523153e-05\n",
            "step: 340, loss: 0.00015740326489321887\n",
            "step: 350, loss: 2.534527811803855e-05\n",
            "step: 360, loss: 4.094117321074009e-05\n",
            "step: 370, loss: 1.0110375114891212e-05\n",
            "step: 380, loss: 6.502964242827147e-05\n",
            "step: 390, loss: 0.0003956523723900318\n",
            "step: 400, loss: 3.240141086280346e-05\n",
            "step: 410, loss: 7.005450606811792e-05\n",
            "step: 420, loss: 1.4450117305386811e-05\n",
            "step: 430, loss: 1.448373313905904e-05\n",
            "step: 440, loss: 3.084411218878813e-05\n",
            "step: 450, loss: 5.0080696382792667e-05\n",
            "step: 460, loss: 1.8145412468584254e-05\n",
            "step: 470, loss: 1.751956733642146e-05\n",
            "step: 480, loss: 1.2855805834988132e-05\n",
            "step: 490, loss: 3.344158176332712e-05\n",
            "step: 500, loss: 1.2501946912379935e-05\n",
            "step: 510, loss: 0.00012879702262580395\n",
            "step: 520, loss: 0.00023554485233034939\n",
            "step: 530, loss: 0.000615019875112921\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9203624225083452, f1=0.9053732762719924, best_f1=0.9136490250696379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011675584391923621\n",
            "step: 10, loss: 6.656504410784692e-05\n",
            "step: 20, loss: 1.2516798960859887e-05\n",
            "step: 30, loss: 0.0019949893467128277\n",
            "step: 40, loss: 0.06550329923629761\n",
            "step: 50, loss: 6.963990745134652e-05\n",
            "step: 60, loss: 1.036740741255926e-05\n",
            "step: 70, loss: 0.0005621804157271981\n",
            "step: 80, loss: 6.962265615584329e-05\n",
            "step: 90, loss: 1.518408225820167e-05\n",
            "step: 100, loss: 1.1432798601163086e-05\n",
            "step: 110, loss: 1.9243736460339278e-05\n",
            "step: 120, loss: 3.0077004339545965e-05\n",
            "step: 130, loss: 0.005312950350344181\n",
            "step: 140, loss: 2.787277480820194e-05\n",
            "step: 150, loss: 1.5306995919672772e-05\n",
            "step: 160, loss: 1.3377278264670167e-05\n",
            "step: 170, loss: 0.00490127969533205\n",
            "step: 180, loss: 1.4025443306309171e-05\n",
            "step: 190, loss: 1.3261887033877429e-05\n",
            "step: 200, loss: 1.6688562027411535e-05\n",
            "step: 210, loss: 6.225573451956734e-05\n",
            "step: 220, loss: 1.3708761798625346e-05\n",
            "step: 230, loss: 2.414016125840135e-05\n",
            "step: 240, loss: 0.0002544661983847618\n",
            "step: 250, loss: 2.2141435692901723e-05\n",
            "step: 260, loss: 1.4457610632234719e-05\n",
            "step: 270, loss: 1.204747604788281e-05\n",
            "step: 280, loss: 1.301975680689793e-05\n",
            "step: 290, loss: 1.0576027307251934e-05\n",
            "step: 300, loss: 1.1440251910244115e-05\n",
            "step: 310, loss: 2.5888723030220717e-05\n",
            "step: 320, loss: 1.1097522474301513e-05\n",
            "step: 330, loss: 4.899277337244712e-05\n",
            "step: 340, loss: 1.1276320037723053e-05\n",
            "step: 350, loss: 2.2834061383036897e-05\n",
            "step: 360, loss: 3.8504636904690415e-05\n",
            "step: 370, loss: 1.043817974277772e-05\n",
            "step: 380, loss: 0.00013605623098555952\n",
            "step: 390, loss: 0.0001284056343138218\n",
            "step: 400, loss: 2.4805040084174834e-05\n",
            "step: 410, loss: 2.1672547518392093e-05\n",
            "step: 420, loss: 2.2961377908359282e-05\n",
            "step: 430, loss: 2.357243101869244e-05\n",
            "step: 440, loss: 0.0002953716611955315\n",
            "step: 450, loss: 9.559031241224147e-06\n",
            "step: 460, loss: 0.00022805640765000135\n",
            "step: 470, loss: 0.004405771382153034\n",
            "step: 480, loss: 9.044957550941035e-06\n",
            "step: 490, loss: 1.9973875168943778e-05\n",
            "step: 500, loss: 1.2937761312059592e-05\n",
            "step: 510, loss: 1.64578013936989e-05\n",
            "step: 520, loss: 2.2536398319061846e-05\n",
            "step: 530, loss: 2.6427716875332408e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9179559306141585, f1=0.911970190964136, best_f1=0.9136490250696379\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 192.79it/s]\n",
            "load_f1 = 0.9218106995884773\n",
            "real_f1 = 0.9189435336976319\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f5e6f4-a444-4c4c-b2a1-8b44edf39152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5611923336982727\n",
            "step: 10, loss: 0.38644158840179443\n",
            "step: 20, loss: 0.37713268399238586\n",
            "step: 30, loss: 0.3179284632205963\n",
            "step: 40, loss: 0.17053601145744324\n",
            "step: 50, loss: 0.44946733117103577\n",
            "step: 60, loss: 0.3093242049217224\n",
            "step: 70, loss: 0.2078080177307129\n",
            "step: 80, loss: 0.20134606957435608\n",
            "step: 90, loss: 0.4359448552131653\n",
            "step: 100, loss: 0.49429771304130554\n",
            "step: 110, loss: 0.2555061876773834\n",
            "step: 120, loss: 0.22763706743717194\n",
            "step: 130, loss: 0.27882811427116394\n",
            "step: 140, loss: 0.27412712574005127\n",
            "step: 150, loss: 0.3425874412059784\n",
            "step: 160, loss: 0.29824167490005493\n",
            "step: 170, loss: 0.2915915548801422\n",
            "step: 180, loss: 0.11215002834796906\n",
            "step: 190, loss: 0.22859713435173035\n",
            "step: 200, loss: 0.27091020345687866\n",
            "step: 210, loss: 0.26401573419570923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5330490405117271, f1=0.5341880341880342, best_f1=0.5341880341880342\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16162054240703583\n",
            "step: 10, loss: 0.24444811046123505\n",
            "step: 20, loss: 0.22366714477539062\n",
            "step: 30, loss: 0.319555401802063\n",
            "step: 40, loss: 0.15789036452770233\n",
            "step: 50, loss: 0.16771237552165985\n",
            "step: 60, loss: 0.3360796868801117\n",
            "step: 70, loss: 0.15536245703697205\n",
            "step: 80, loss: 0.12254005670547485\n",
            "step: 90, loss: 0.14541083574295044\n",
            "step: 100, loss: 0.03212235867977142\n",
            "step: 110, loss: 0.11083339154720306\n",
            "step: 120, loss: 0.12345631420612335\n",
            "step: 130, loss: 0.04446383938193321\n",
            "step: 140, loss: 0.23097017407417297\n",
            "step: 150, loss: 0.2991699278354645\n",
            "step: 160, loss: 0.2259792983531952\n",
            "step: 170, loss: 0.11210542917251587\n",
            "step: 180, loss: 0.21717596054077148\n",
            "step: 190, loss: 0.20722271502017975\n",
            "step: 200, loss: 0.0963422879576683\n",
            "step: 210, loss: 0.1976414918899536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5647969052224371, f1=0.540650406504065, best_f1=0.540650406504065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0786239355802536\n",
            "step: 10, loss: 0.25027549266815186\n",
            "step: 20, loss: 0.1248626783490181\n",
            "step: 30, loss: 0.21035796403884888\n",
            "step: 40, loss: 0.0760589987039566\n",
            "step: 50, loss: 0.12855404615402222\n",
            "step: 60, loss: 0.22699202597141266\n",
            "step: 70, loss: 0.1573581099510193\n",
            "step: 80, loss: 0.17279882729053497\n",
            "step: 90, loss: 0.060598116368055344\n",
            "step: 100, loss: 0.23000800609588623\n",
            "step: 110, loss: 0.11647797375917435\n",
            "step: 120, loss: 0.14938409626483917\n",
            "step: 130, loss: 0.12324649840593338\n",
            "step: 140, loss: 0.14276239275932312\n",
            "step: 150, loss: 0.25406700372695923\n",
            "step: 160, loss: 0.033351533114910126\n",
            "step: 170, loss: 0.11895205825567245\n",
            "step: 180, loss: 0.13711927831172943\n",
            "step: 190, loss: 0.24073848128318787\n",
            "step: 200, loss: 0.050112321972846985\n",
            "step: 210, loss: 0.13689330220222473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5778688524590164, f1=0.5656565656565656, best_f1=0.5656565656565656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3278016746044159\n",
            "step: 10, loss: 0.16932815313339233\n",
            "step: 20, loss: 0.10403262823820114\n",
            "step: 30, loss: 0.26789772510528564\n",
            "step: 40, loss: 0.044133733958005905\n",
            "step: 50, loss: 0.17485059797763824\n",
            "step: 60, loss: 0.23218899965286255\n",
            "step: 70, loss: 0.22304223477840424\n",
            "step: 80, loss: 0.1948365420103073\n",
            "step: 90, loss: 0.07992059737443924\n",
            "step: 100, loss: 0.40223580598831177\n",
            "step: 110, loss: 0.1328185498714447\n",
            "step: 120, loss: 0.04681597277522087\n",
            "step: 130, loss: 0.19121988117694855\n",
            "step: 140, loss: 0.21643094718456268\n",
            "step: 150, loss: 0.05978627875447273\n",
            "step: 160, loss: 0.03636246547102928\n",
            "step: 170, loss: 0.06734732538461685\n",
            "step: 180, loss: 0.3515089154243469\n",
            "step: 190, loss: 0.10944835096597672\n",
            "step: 200, loss: 0.38125428557395935\n",
            "step: 210, loss: 0.0996648520231247\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6061776061776061, f1=0.5568627450980392, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16391387581825256\n",
            "step: 10, loss: 0.034398213028907776\n",
            "step: 20, loss: 0.34440550208091736\n",
            "step: 30, loss: 0.06831744313240051\n",
            "step: 40, loss: 0.1328437179327011\n",
            "step: 50, loss: 0.13717681169509888\n",
            "step: 60, loss: 0.18958395719528198\n",
            "step: 70, loss: 0.24803631007671356\n",
            "step: 80, loss: 0.07886286079883575\n",
            "step: 90, loss: 0.1498102992773056\n",
            "step: 100, loss: 0.04501115158200264\n",
            "step: 110, loss: 0.19307257235050201\n",
            "step: 120, loss: 0.2090694010257721\n",
            "step: 130, loss: 0.11748191714286804\n",
            "step: 140, loss: 0.09425688534975052\n",
            "step: 150, loss: 0.05148768052458763\n",
            "step: 160, loss: 0.12288381159305573\n",
            "step: 170, loss: 0.07285480946302414\n",
            "step: 180, loss: 0.06214740872383118\n",
            "step: 190, loss: 0.08364133536815643\n",
            "step: 200, loss: 0.05173291265964508\n",
            "step: 210, loss: 0.11025509238243103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5708812260536399, f1=0.5557729941291586, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.046704329550266266\n",
            "step: 10, loss: 0.1789127141237259\n",
            "step: 20, loss: 0.05912996456027031\n",
            "step: 30, loss: 0.011254316195845604\n",
            "step: 40, loss: 0.09306842088699341\n",
            "step: 50, loss: 0.01864558644592762\n",
            "step: 60, loss: 0.18297481536865234\n",
            "step: 70, loss: 0.011721691116690636\n",
            "step: 80, loss: 0.04639827460050583\n",
            "step: 90, loss: 0.23857198655605316\n",
            "step: 100, loss: 0.005800997838377953\n",
            "step: 110, loss: 0.0953008309006691\n",
            "step: 120, loss: 0.035374678671360016\n",
            "step: 130, loss: 0.0927596166729927\n",
            "step: 140, loss: 0.0954502522945404\n",
            "step: 150, loss: 0.026092084124684334\n",
            "step: 160, loss: 0.02586241438984871\n",
            "step: 170, loss: 0.1270386278629303\n",
            "step: 180, loss: 0.12866972386837006\n",
            "step: 190, loss: 0.07058203220367432\n",
            "step: 200, loss: 0.01598399318754673\n",
            "step: 210, loss: 0.06956134736537933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.556420233463035, f1=0.569672131147541, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0409090481698513\n",
            "step: 10, loss: 0.10740915685892105\n",
            "step: 20, loss: 0.009524459950625896\n",
            "step: 30, loss: 0.0331883504986763\n",
            "step: 40, loss: 0.08799392729997635\n",
            "step: 50, loss: 0.10713519901037216\n",
            "step: 60, loss: 0.027621876448392868\n",
            "step: 70, loss: 0.04417441412806511\n",
            "step: 80, loss: 0.034628208726644516\n",
            "step: 90, loss: 0.16476179659366608\n",
            "step: 100, loss: 0.00431300513446331\n",
            "step: 110, loss: 0.036298610270023346\n",
            "step: 120, loss: 0.10140202194452286\n",
            "step: 130, loss: 0.14839857816696167\n",
            "step: 140, loss: 0.01828221045434475\n",
            "step: 150, loss: 0.06589596718549728\n",
            "step: 160, loss: 0.029395654797554016\n",
            "step: 170, loss: 0.02244126982986927\n",
            "step: 180, loss: 0.142632856965065\n",
            "step: 190, loss: 0.0638015866279602\n",
            "step: 200, loss: 0.032267976552248\n",
            "step: 210, loss: 0.10985866189002991\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5291005291005292, f1=0.5237258347978909, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012907147407531738\n",
            "step: 10, loss: 0.01829376071691513\n",
            "step: 20, loss: 0.01357145607471466\n",
            "step: 30, loss: 0.21957461535930634\n",
            "step: 40, loss: 0.02363421395421028\n",
            "step: 50, loss: 0.024418950080871582\n",
            "step: 60, loss: 0.009044647216796875\n",
            "step: 70, loss: 0.19680695235729218\n",
            "step: 80, loss: 0.06637648493051529\n",
            "step: 90, loss: 0.1169770136475563\n",
            "step: 100, loss: 0.028216486796736717\n",
            "step: 110, loss: 0.05877493694424629\n",
            "step: 120, loss: 0.001754534081555903\n",
            "step: 130, loss: 0.03900238871574402\n",
            "step: 140, loss: 0.0484958253800869\n",
            "step: 150, loss: 0.07927308976650238\n",
            "step: 160, loss: 0.03095635212957859\n",
            "step: 170, loss: 0.01599019020795822\n",
            "step: 180, loss: 0.08596393465995789\n",
            "step: 190, loss: 0.09014039486646652\n",
            "step: 200, loss: 0.002551599871367216\n",
            "step: 210, loss: 0.10988692194223404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5546558704453441, f1=0.5543710021321961, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.042269814759492874\n",
            "step: 10, loss: 0.05633142217993736\n",
            "step: 20, loss: 0.0028205791022628546\n",
            "step: 30, loss: 0.021612150594592094\n",
            "step: 40, loss: 0.054686516523361206\n",
            "step: 50, loss: 0.06672314554452896\n",
            "step: 60, loss: 0.10029994696378708\n",
            "step: 70, loss: 0.06999605894088745\n",
            "step: 80, loss: 0.039950739592313766\n",
            "step: 90, loss: 0.0023049444425851107\n",
            "step: 100, loss: 0.03955827280879021\n",
            "step: 110, loss: 0.020106468349695206\n",
            "step: 120, loss: 0.0011533934157341719\n",
            "step: 130, loss: 0.08392435312271118\n",
            "step: 140, loss: 0.15580138564109802\n",
            "step: 150, loss: 0.19150902330875397\n",
            "step: 160, loss: 0.010594744235277176\n",
            "step: 170, loss: 0.1688295602798462\n",
            "step: 180, loss: 0.09139091521501541\n",
            "step: 190, loss: 0.010592738166451454\n",
            "step: 200, loss: 0.022085247561335564\n",
            "step: 210, loss: 0.007988912984728813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5361552028218695, f1=0.5261261261261261, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008069775998592377\n",
            "step: 10, loss: 0.1595674455165863\n",
            "step: 20, loss: 0.005499511957168579\n",
            "step: 30, loss: 0.09049864113330841\n",
            "step: 40, loss: 0.004341701976954937\n",
            "step: 50, loss: 0.010579517111182213\n",
            "step: 60, loss: 0.04607633873820305\n",
            "step: 70, loss: 0.04097166657447815\n",
            "step: 80, loss: 0.06995012611150742\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 90, loss: 0.03178061917424202\n",
            "step: 100, loss: 0.12314648926258087\n",
            "step: 110, loss: 0.0015348304295912385\n",
            "step: 120, loss: 0.0051179141737520695\n",
            "step: 130, loss: 0.21108034253120422\n",
            "step: 140, loss: 0.0036767160054296255\n",
            "step: 150, loss: 0.05806631222367287\n",
            "step: 160, loss: 0.047275856137275696\n",
            "step: 170, loss: 0.005568955559283495\n",
            "step: 180, loss: 0.1410343199968338\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.18182332813739777\n",
            "step: 200, loss: 0.06139504164457321\n",
            "step: 210, loss: 0.1595248132944107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5150375939849624, f1=0.5454545454545454, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0453801229596138\n",
            "step: 10, loss: 0.06895886361598969\n",
            "step: 20, loss: 0.06751503795385361\n",
            "step: 30, loss: 0.0040206327103078365\n",
            "step: 40, loss: 0.0077078803442418575\n",
            "step: 50, loss: 0.0010736518306657672\n",
            "step: 60, loss: 0.1951902210712433\n",
            "step: 70, loss: 0.006906111259013414\n",
            "step: 80, loss: 0.3245992362499237\n",
            "step: 90, loss: 0.012729942798614502\n",
            "step: 100, loss: 0.2924582064151764\n",
            "step: 110, loss: 0.012566099874675274\n",
            "step: 120, loss: 0.15929563343524933\n",
            "step: 130, loss: 0.016764821484684944\n",
            "step: 140, loss: 0.0016344841569662094\n",
            "step: 150, loss: 0.0018078511347994208\n",
            "step: 160, loss: 0.011700207367539406\n",
            "step: 170, loss: 0.0247363168746233\n",
            "step: 180, loss: 0.0037898330483585596\n",
            "step: 190, loss: 0.01339690014719963\n",
            "step: 200, loss: 0.015279043465852737\n",
            "step: 210, loss: 0.011564263142645359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5530303030303031, f1=0.5490196078431372, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003863443387672305\n",
            "step: 10, loss: 0.04062485694885254\n",
            "step: 20, loss: 0.02173774503171444\n",
            "step: 30, loss: 0.02984962984919548\n",
            "step: 40, loss: 0.010440164245665073\n",
            "step: 50, loss: 0.0007443698705174029\n",
            "step: 60, loss: 0.044044654816389084\n",
            "step: 70, loss: 0.046471595764160156\n",
            "step: 80, loss: 0.1461157351732254\n",
            "step: 90, loss: 0.003963339142501354\n",
            "step: 100, loss: 0.0011230690870434046\n",
            "step: 110, loss: 0.023044582456350327\n",
            "step: 120, loss: 0.09611707180738449\n",
            "step: 130, loss: 0.011642190627753735\n",
            "step: 140, loss: 0.001475564669817686\n",
            "step: 150, loss: 0.004164684098213911\n",
            "step: 160, loss: 0.030748741701245308\n",
            "step: 170, loss: 0.0014086613664403558\n",
            "step: 180, loss: 0.09460410475730896\n",
            "step: 190, loss: 0.00832638144493103\n",
            "step: 200, loss: 0.004148244857788086\n",
            "step: 210, loss: 0.01752704195678234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.517453798767967, f1=0.5299145299145299, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005693774204701185\n",
            "step: 10, loss: 0.0005724240909330547\n",
            "step: 20, loss: 0.1948850005865097\n",
            "step: 30, loss: 0.0848473459482193\n",
            "step: 40, loss: 0.026782728731632233\n",
            "step: 50, loss: 0.17179670929908752\n",
            "step: 60, loss: 0.005470363423228264\n",
            "step: 70, loss: 0.010140486061573029\n",
            "step: 80, loss: 0.028449513018131256\n",
            "step: 90, loss: 0.0018416172824800014\n",
            "step: 100, loss: 0.003859027987346053\n",
            "step: 110, loss: 0.017686694860458374\n",
            "step: 120, loss: 0.10191787779331207\n",
            "step: 130, loss: 0.0012477375566959381\n",
            "step: 140, loss: 0.004312962293624878\n",
            "step: 150, loss: 0.014039658941328526\n",
            "step: 160, loss: 0.0010092156007885933\n",
            "step: 170, loss: 0.0017462980467826128\n",
            "step: 180, loss: 0.04151666536927223\n",
            "step: 190, loss: 0.0007352022221311927\n",
            "step: 200, loss: 0.01745576225221157\n",
            "step: 210, loss: 0.0253006462007761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5233644859813084, f1=0.5330739299610895, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016086819814518094\n",
            "step: 10, loss: 0.0441623218357563\n",
            "step: 20, loss: 0.1975434273481369\n",
            "step: 30, loss: 0.03178960457444191\n",
            "step: 40, loss: 0.0011209631338715553\n",
            "step: 50, loss: 0.004676549695432186\n",
            "step: 60, loss: 0.006649536546319723\n",
            "step: 70, loss: 0.00691120745614171\n",
            "step: 80, loss: 0.002532718935981393\n",
            "step: 90, loss: 0.1208195760846138\n",
            "step: 100, loss: 0.009966066107153893\n",
            "step: 110, loss: 0.017402222380042076\n",
            "step: 120, loss: 0.02383848838508129\n",
            "step: 130, loss: 0.003400550689548254\n",
            "step: 140, loss: 0.04728928208351135\n",
            "step: 150, loss: 0.0031866838689893484\n",
            "step: 160, loss: 0.008638273924589157\n",
            "step: 170, loss: 0.000620245176833123\n",
            "step: 180, loss: 0.018532585352659225\n",
            "step: 190, loss: 0.00390249234624207\n",
            "step: 200, loss: 0.0112845990806818\n",
            "step: 210, loss: 0.0021099913865327835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5325443786982248, f1=0.5405405405405405, best_f1=0.5568627450980392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024794195778667927\n",
            "step: 10, loss: 0.0013929991982877254\n",
            "step: 20, loss: 0.0047808559611439705\n",
            "step: 30, loss: 0.008889980614185333\n",
            "step: 40, loss: 0.0006930950912646949\n",
            "step: 50, loss: 0.00451296241953969\n",
            "step: 60, loss: 0.03798713907599449\n",
            "step: 70, loss: 0.001218621851876378\n",
            "step: 80, loss: 0.001083742012269795\n",
            "step: 90, loss: 0.0026793323922902346\n",
            "step: 100, loss: 0.0018487529596313834\n",
            "step: 110, loss: 0.04213309660553932\n",
            "step: 120, loss: 0.003612432163208723\n",
            "step: 130, loss: 0.0023345653899013996\n",
            "step: 140, loss: 0.0005168608040548861\n",
            "step: 150, loss: 0.0007911178399808705\n",
            "step: 160, loss: 0.012008327059447765\n",
            "step: 170, loss: 0.011058385483920574\n",
            "step: 180, loss: 0.0008063867571763694\n",
            "step: 190, loss: 0.0007665580487810075\n",
            "step: 200, loss: 0.003915850538760424\n",
            "step: 210, loss: 0.01838054321706295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5288640595903166, f1=0.535645472061657, best_f1=0.5568627450980392\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 450.24it/s]\n",
            "load_f1 = 0.6020202020202021\n",
            "real_f1 = 0.6053169734151329\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 274.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73fdbb3-5c23-4983-defe-97df5cb59aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5789780616760254\n",
            "step: 10, loss: 0.36953702569007874\n",
            "step: 20, loss: 0.28820955753326416\n",
            "step: 30, loss: 0.41517430543899536\n",
            "step: 40, loss: 0.42503538727760315\n",
            "step: 50, loss: 0.2819135785102844\n",
            "step: 60, loss: 0.286545068025589\n",
            "step: 70, loss: 0.263368159532547\n",
            "step: 80, loss: 0.24559541046619415\n",
            "step: 90, loss: 0.2796720564365387\n",
            "step: 100, loss: 0.3162561357021332\n",
            "step: 110, loss: 0.5183542966842651\n",
            "step: 120, loss: 0.20138925313949585\n",
            "step: 130, loss: 0.14013135433197021\n",
            "step: 140, loss: 0.13970859348773956\n",
            "step: 150, loss: 0.1947200745344162\n",
            "step: 160, loss: 0.11561141908168793\n",
            "step: 170, loss: 0.2195371389389038\n",
            "step: 180, loss: 0.04553963989019394\n",
            "step: 190, loss: 0.1884872317314148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.610079575596817, f1=0.6351706036745407, best_f1=0.6351706036745407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22974978387355804\n",
            "step: 10, loss: 0.0707065537571907\n",
            "step: 20, loss: 0.25796306133270264\n",
            "step: 30, loss: 0.1384650021791458\n",
            "step: 40, loss: 0.19323372840881348\n",
            "step: 50, loss: 0.09259499609470367\n",
            "step: 60, loss: 0.4000706970691681\n",
            "step: 70, loss: 0.19665755331516266\n",
            "step: 80, loss: 0.1810174137353897\n",
            "step: 90, loss: 0.243983656167984\n",
            "step: 100, loss: 0.03509033843874931\n",
            "step: 110, loss: 0.10647857189178467\n",
            "step: 120, loss: 0.22845697402954102\n",
            "step: 130, loss: 0.10397869348526001\n",
            "step: 140, loss: 0.04133663699030876\n",
            "step: 150, loss: 0.12040731310844421\n",
            "step: 160, loss: 0.05393365025520325\n",
            "step: 170, loss: 0.19596907496452332\n",
            "step: 180, loss: 0.14984510838985443\n",
            "step: 190, loss: 0.04124350845813751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.721763085399449, f1=0.7675675675675676, best_f1=0.7675675675675676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0605936236679554\n",
            "step: 10, loss: 0.2880299389362335\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.13364887237548828\n",
            "step: 30, loss: 0.08962225168943405\n",
            "step: 40, loss: 0.057532429695129395\n",
            "step: 50, loss: 0.15332122147083282\n",
            "step: 60, loss: 0.03083973191678524\n",
            "step: 70, loss: 0.10247563570737839\n",
            "step: 80, loss: 0.0625186339020729\n",
            "step: 90, loss: 0.03790021315217018\n",
            "step: 100, loss: 0.15905842185020447\n",
            "step: 110, loss: 0.10412997007369995\n",
            "step: 120, loss: 0.1852577030658722\n",
            "step: 130, loss: 0.01340932585299015\n",
            "step: 140, loss: 0.04333516210317612\n",
            "step: 150, loss: 0.13104046881198883\n",
            "step: 160, loss: 0.043685756623744965\n",
            "step: 170, loss: 0.12182677537202835\n",
            "step: 180, loss: 0.16411714255809784\n",
            "step: 190, loss: 0.1302795708179474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7403314917127072, f1=0.7639257294429709, best_f1=0.7639257294429709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02032320760190487\n",
            "step: 10, loss: 0.12911422550678253\n",
            "step: 20, loss: 0.04686703905463219\n",
            "step: 30, loss: 0.009608601219952106\n",
            "step: 40, loss: 0.027042513713240623\n",
            "step: 50, loss: 0.044224295765161514\n",
            "step: 60, loss: 0.04998878389596939\n",
            "step: 70, loss: 0.11743656545877457\n",
            "step: 80, loss: 0.25848880410194397\n",
            "step: 90, loss: 0.043066903948783875\n",
            "step: 100, loss: 0.09558844566345215\n",
            "step: 110, loss: 0.0035479555372148752\n",
            "step: 120, loss: 0.059157490730285645\n",
            "step: 130, loss: 0.228201225399971\n",
            "step: 140, loss: 0.008473467081785202\n",
            "step: 150, loss: 0.10354060679674149\n",
            "step: 160, loss: 0.03795556724071503\n",
            "step: 170, loss: 0.1544751226902008\n",
            "step: 180, loss: 0.030785009264945984\n",
            "step: 190, loss: 0.2095428705215454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7507002801120448, f1=0.7534626038781163, best_f1=0.7534626038781163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09454938769340515\n",
            "step: 10, loss: 0.01559077762067318\n",
            "step: 20, loss: 0.19055798649787903\n",
            "step: 30, loss: 0.0050519309006631374\n",
            "step: 40, loss: 0.08499568700790405\n",
            "step: 50, loss: 0.1076471135020256\n",
            "step: 60, loss: 0.11141052842140198\n",
            "step: 70, loss: 0.09650737792253494\n",
            "step: 80, loss: 0.045030198991298676\n",
            "step: 90, loss: 0.04471242427825928\n",
            "step: 100, loss: 0.004418585449457169\n",
            "step: 110, loss: 0.004532123450189829\n",
            "step: 120, loss: 0.008919503539800644\n",
            "step: 130, loss: 0.09201938658952713\n",
            "step: 140, loss: 0.0058862315490841866\n",
            "step: 150, loss: 0.032572951167821884\n",
            "step: 160, loss: 0.01960659958422184\n",
            "step: 170, loss: 0.0027388008311390877\n",
            "step: 180, loss: 0.09577653557062149\n",
            "step: 190, loss: 0.2008853703737259\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7507163323782234, f1=0.7885714285714286, best_f1=0.7885714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009264182299375534\n",
            "step: 10, loss: 0.04268626496195793\n",
            "step: 20, loss: 0.07872835546731949\n",
            "step: 30, loss: 0.00243653217330575\n",
            "step: 40, loss: 0.0165623277425766\n",
            "step: 50, loss: 0.3007354438304901\n",
            "step: 60, loss: 0.0690053254365921\n",
            "step: 70, loss: 0.011880913749337196\n",
            "step: 80, loss: 0.020938649773597717\n",
            "step: 90, loss: 0.006715728435665369\n",
            "step: 100, loss: 0.0008377659250982106\n",
            "step: 110, loss: 0.08632192015647888\n",
            "step: 120, loss: 0.10420991480350494\n",
            "step: 130, loss: 0.0207042433321476\n",
            "step: 140, loss: 0.1027919352054596\n",
            "step: 150, loss: 0.022086096927523613\n",
            "step: 160, loss: 0.149509996175766\n",
            "step: 170, loss: 0.1432170271873474\n",
            "step: 180, loss: 0.07654202729463577\n",
            "step: 190, loss: 0.02561904862523079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.769230769230769, f1=0.7862796833773087, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004230233374983072\n",
            "step: 10, loss: 0.013088857755064964\n",
            "step: 20, loss: 0.10547413676977158\n",
            "step: 30, loss: 0.03436967357993126\n",
            "step: 40, loss: 0.12162870913743973\n",
            "step: 50, loss: 0.014881747774779797\n",
            "step: 60, loss: 0.015987563878297806\n",
            "step: 70, loss: 0.01066247746348381\n",
            "step: 80, loss: 0.03012465313076973\n",
            "step: 90, loss: 0.026821885257959366\n",
            "step: 100, loss: 0.0030695144087076187\n",
            "step: 110, loss: 0.02125442959368229\n",
            "step: 120, loss: 0.006660813000053167\n",
            "step: 130, loss: 0.051666468381881714\n",
            "step: 140, loss: 0.0053292918018996716\n",
            "step: 150, loss: 0.19344674050807953\n",
            "step: 160, loss: 0.09903251379728317\n",
            "step: 170, loss: 0.08410864323377609\n",
            "step: 180, loss: 0.0886203944683075\n",
            "step: 190, loss: 0.0043763150461018085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.736, f1=0.7493403693931397, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03033001720905304\n",
            "step: 10, loss: 0.13310778141021729\n",
            "step: 20, loss: 0.002234599320217967\n",
            "step: 30, loss: 0.0053597684018313885\n",
            "step: 40, loss: 0.0024021512363106012\n",
            "step: 50, loss: 0.018101884052157402\n",
            "step: 60, loss: 0.0007721498841419816\n",
            "step: 70, loss: 0.015487303026020527\n",
            "step: 80, loss: 0.0005952980718575418\n",
            "step: 90, loss: 0.009936174377799034\n",
            "step: 100, loss: 0.005480511579662561\n",
            "step: 110, loss: 0.0011917094234377146\n",
            "step: 120, loss: 0.028845101594924927\n",
            "step: 130, loss: 0.04500829055905342\n",
            "step: 140, loss: 0.008952879346907139\n",
            "step: 150, loss: 0.02405468188226223\n",
            "step: 160, loss: 0.0015420994022861123\n",
            "step: 170, loss: 0.014949850738048553\n",
            "step: 180, loss: 0.10136955976486206\n",
            "step: 190, loss: 0.03197486326098442\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7624309392265193, f1=0.7870619946091645, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026424131356179714\n",
            "step: 10, loss: 0.004862011875957251\n",
            "step: 20, loss: 0.003341736737638712\n",
            "step: 30, loss: 0.05775933340191841\n",
            "step: 40, loss: 0.002908717840909958\n",
            "step: 50, loss: 0.00037783378502354026\n",
            "step: 60, loss: 0.0677393227815628\n",
            "step: 70, loss: 0.0025871770922094584\n",
            "step: 80, loss: 0.000566869683098048\n",
            "step: 90, loss: 0.04441853612661362\n",
            "step: 100, loss: 0.01001770794391632\n",
            "step: 110, loss: 0.38785648345947266\n",
            "step: 120, loss: 0.006342542823404074\n",
            "step: 130, loss: 0.005050832871347666\n",
            "step: 140, loss: 0.03435634821653366\n",
            "step: 150, loss: 0.0013472180580720305\n",
            "step: 160, loss: 0.0007398475427180529\n",
            "step: 170, loss: 0.008516873233020306\n",
            "step: 180, loss: 0.002999192336574197\n",
            "step: 190, loss: 0.0007945880061015487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7473684210526317, f1=0.7554347826086956, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06827664375305176\n",
            "step: 10, loss: 0.012598179280757904\n",
            "step: 20, loss: 0.0011437606299296021\n",
            "step: 30, loss: 0.00026153732324019074\n",
            "step: 40, loss: 0.015516798011958599\n",
            "step: 50, loss: 0.06546138226985931\n",
            "step: 60, loss: 0.0021441469434648752\n",
            "step: 70, loss: 0.010609024204313755\n",
            "step: 80, loss: 0.0027603176422417164\n",
            "step: 90, loss: 0.006664047483354807\n",
            "step: 100, loss: 0.004021292086690664\n",
            "step: 110, loss: 0.003580769058316946\n",
            "step: 120, loss: 0.05678167939186096\n",
            "step: 130, loss: 0.000510143639985472\n",
            "step: 140, loss: 0.019508328288793564\n",
            "step: 150, loss: 0.12157556414604187\n",
            "step: 160, loss: 0.1409148871898651\n",
            "step: 170, loss: 0.003478032536804676\n",
            "step: 180, loss: 0.0028793474193662405\n",
            "step: 190, loss: 0.000399329059291631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7626666666666667, f1=0.7626666666666667, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007972357561811805\n",
            "step: 10, loss: 0.0011769896373152733\n",
            "step: 20, loss: 0.02034098654985428\n",
            "step: 30, loss: 0.00364372949115932\n",
            "step: 40, loss: 0.003186040325090289\n",
            "step: 50, loss: 0.009168673306703568\n",
            "step: 60, loss: 0.004583810921758413\n",
            "step: 70, loss: 0.004777212161570787\n",
            "step: 80, loss: 0.0021238457411527634\n",
            "step: 90, loss: 0.014247671701014042\n",
            "step: 100, loss: 0.0009058595751412213\n",
            "step: 110, loss: 0.00044093371252529323\n",
            "step: 120, loss: 0.0016728575574234128\n",
            "step: 130, loss: 0.0025869428645819426\n",
            "step: 140, loss: 0.00088430882897228\n",
            "step: 150, loss: 0.0002982032601721585\n",
            "step: 160, loss: 0.0013027189997956157\n",
            "step: 170, loss: 0.001106219831854105\n",
            "step: 180, loss: 0.0038884689565747976\n",
            "step: 190, loss: 0.0013050114503130317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7627118644067796, f1=0.768361581920904, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039595930138602853\n",
            "step: 10, loss: 0.0012279037619009614\n",
            "step: 20, loss: 0.0006379805272445083\n",
            "step: 30, loss: 0.00043412367813289165\n",
            "step: 40, loss: 0.000470335507998243\n",
            "step: 50, loss: 0.019260572269558907\n",
            "step: 60, loss: 0.004150095861405134\n",
            "step: 70, loss: 0.0005889189778827131\n",
            "step: 80, loss: 0.0008083109278231859\n",
            "step: 90, loss: 0.001044582691974938\n",
            "step: 100, loss: 0.0004043552035000175\n",
            "step: 110, loss: 0.0003799877013079822\n",
            "step: 120, loss: 0.0031926052179187536\n",
            "step: 130, loss: 0.005202308762818575\n",
            "step: 140, loss: 0.0007691237842664123\n",
            "step: 150, loss: 0.008272187784314156\n",
            "step: 160, loss: 0.0002163319877581671\n",
            "step: 170, loss: 0.03227011859416962\n",
            "step: 180, loss: 0.00022378721041604877\n",
            "step: 190, loss: 0.00018443958833813667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7658402203856748, f1=0.7734806629834254, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006789189646951854\n",
            "step: 10, loss: 0.003954119980335236\n",
            "step: 20, loss: 0.002147700171917677\n",
            "step: 30, loss: 0.0013480427442118526\n",
            "step: 40, loss: 0.000363241444574669\n",
            "step: 50, loss: 0.08221419155597687\n",
            "step: 60, loss: 0.0005577327101491392\n",
            "step: 70, loss: 0.017629366368055344\n",
            "step: 80, loss: 0.0020790433045476675\n",
            "step: 90, loss: 0.0012982345651835203\n",
            "step: 100, loss: 0.0009461325826123357\n",
            "step: 110, loss: 0.003206223715096712\n",
            "step: 120, loss: 0.006475933827459812\n",
            "step: 130, loss: 0.0006770467734895647\n",
            "step: 140, loss: 0.001614292268641293\n",
            "step: 150, loss: 0.0004752812092192471\n",
            "step: 160, loss: 0.033175114542245865\n",
            "step: 170, loss: 0.0005497073871083558\n",
            "step: 180, loss: 0.00082878622924909\n",
            "step: 190, loss: 0.0027709228452295065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7559055118110236, f1=0.7506561679790026, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008938903920352459\n",
            "step: 10, loss: 0.0005103815929032862\n",
            "step: 20, loss: 0.0016191533068194985\n",
            "step: 30, loss: 0.0004567121504805982\n",
            "step: 40, loss: 0.0017384056700393558\n",
            "step: 50, loss: 0.0004946588887833059\n",
            "step: 60, loss: 0.0031859593000262976\n",
            "step: 70, loss: 0.00036403234116733074\n",
            "step: 80, loss: 0.0005197254940867424\n",
            "step: 90, loss: 0.005657298490405083\n",
            "step: 100, loss: 0.00582754286006093\n",
            "step: 110, loss: 0.0016514165326952934\n",
            "step: 120, loss: 0.002087377244606614\n",
            "step: 130, loss: 0.009376759640872478\n",
            "step: 140, loss: 0.0004636913363356143\n",
            "step: 150, loss: 0.0008854361949488521\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.009611563757061958\n",
            "step: 170, loss: 0.0005688744713552296\n",
            "step: 180, loss: 0.002132711000740528\n",
            "step: 190, loss: 0.0022203787229955196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7578947368421052, f1=0.7611548556430445, best_f1=0.7862796833773087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0046588340774178505\n",
            "step: 10, loss: 0.0011993157677352428\n",
            "step: 20, loss: 0.0009119341848418117\n",
            "step: 30, loss: 0.0003423503949306905\n",
            "step: 40, loss: 0.0003679059154819697\n",
            "step: 50, loss: 0.002914020325988531\n",
            "step: 60, loss: 0.004946446977555752\n",
            "step: 70, loss: 0.0022599538788199425\n",
            "step: 80, loss: 0.0005317020695656538\n",
            "step: 90, loss: 0.0005011288449168205\n",
            "step: 100, loss: 0.0003811796777881682\n",
            "step: 110, loss: 0.0007749670767225325\n",
            "step: 120, loss: 0.0006490892847068608\n",
            "step: 130, loss: 0.006494549568742514\n",
            "step: 140, loss: 0.015641408041119576\n",
            "step: 150, loss: 0.0012665787944570184\n",
            "step: 160, loss: 0.00023672683164477348\n",
            "step: 170, loss: 0.0006722027901560068\n",
            "step: 180, loss: 0.0040395366959273815\n",
            "step: 190, loss: 0.0008492750930599868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7606382978723404, f1=0.7708894878706198, best_f1=0.7862796833773087\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 233.27it/s]\n",
            "load_f1 = 0.7087912087912088\n",
            "real_f1 = 0.6991869918699186\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 267.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d5eea9-3d5d-4ac0-ee3c-823ff99b4d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6349101662635803\n",
            "step: 10, loss: 0.3733975887298584\n",
            "step: 20, loss: 0.30188921093940735\n",
            "step: 30, loss: 0.3882068991661072\n",
            "step: 40, loss: 0.29009875655174255\n",
            "step: 50, loss: 0.27378326654434204\n",
            "step: 60, loss: 0.23128829896450043\n",
            "step: 70, loss: 0.36301106214523315\n",
            "step: 80, loss: 0.3668130934238434\n",
            "step: 90, loss: 0.22979453206062317\n",
            "step: 100, loss: 0.2466941922903061\n",
            "step: 110, loss: 0.29458439350128174\n",
            "step: 120, loss: 0.227329820394516\n",
            "step: 130, loss: 0.10973504185676575\n",
            "step: 140, loss: 0.21953964233398438\n",
            "step: 150, loss: 0.2546914517879486\n",
            "step: 160, loss: 0.1104833260178566\n",
            "step: 170, loss: 0.2765965759754181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6901408450704225, f1=0.6711409395973154, best_f1=0.6711409395973154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1234247237443924\n",
            "step: 10, loss: 0.13223837316036224\n",
            "step: 20, loss: 0.10575111210346222\n",
            "step: 30, loss: 0.19276778399944305\n",
            "step: 40, loss: 0.06249071657657623\n",
            "step: 50, loss: 0.10581756383180618\n",
            "step: 60, loss: 0.052404992282390594\n",
            "step: 70, loss: 0.07709302008152008\n",
            "step: 80, loss: 0.10079770535230637\n",
            "step: 90, loss: 0.07725309580564499\n",
            "step: 100, loss: 0.1284639835357666\n",
            "step: 110, loss: 0.02664780057966709\n",
            "step: 120, loss: 0.10446484386920929\n",
            "step: 130, loss: 0.041113000363111496\n",
            "step: 140, loss: 0.21099825203418732\n",
            "step: 150, loss: 0.228548064827919\n",
            "step: 160, loss: 0.12087885290384293\n",
            "step: 170, loss: 0.07453717291355133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7418546365914787, f1=0.7294117647058824, best_f1=0.7294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07637009024620056\n",
            "step: 10, loss: 0.07901676744222641\n",
            "step: 20, loss: 0.029255203902721405\n",
            "step: 30, loss: 0.12040424346923828\n",
            "step: 40, loss: 0.10294388979673386\n",
            "step: 50, loss: 0.0666002482175827\n",
            "step: 60, loss: 0.1378566175699234\n",
            "step: 70, loss: 0.09858869016170502\n",
            "step: 80, loss: 0.05605373904109001\n",
            "step: 90, loss: 0.2427312284708023\n",
            "step: 100, loss: 0.0050373622216284275\n",
            "step: 110, loss: 0.12021249532699585\n",
            "step: 120, loss: 0.06684296578168869\n",
            "step: 130, loss: 0.219973623752594\n",
            "step: 140, loss: 0.11188613623380661\n",
            "step: 150, loss: 0.04779529199004173\n",
            "step: 160, loss: 0.04561798647046089\n",
            "step: 170, loss: 0.1459435224533081\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7548209366391185, f1=0.7835051546391752, best_f1=0.7835051546391752\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04822376370429993\n",
            "step: 10, loss: 0.14961890876293182\n",
            "step: 20, loss: 0.029539896175265312\n",
            "step: 30, loss: 0.025713521987199783\n",
            "step: 40, loss: 0.015125536359846592\n",
            "step: 50, loss: 0.04051748663187027\n",
            "step: 60, loss: 0.08367199450731277\n",
            "step: 70, loss: 0.01356862485408783\n",
            "step: 80, loss: 0.03358723968267441\n",
            "step: 90, loss: 0.06259535998106003\n",
            "step: 100, loss: 0.1508936882019043\n",
            "step: 110, loss: 0.09677248448133469\n",
            "step: 120, loss: 0.04735105112195015\n",
            "step: 130, loss: 0.03541755676269531\n",
            "step: 140, loss: 0.022476376965641975\n",
            "step: 150, loss: 0.06493458151817322\n",
            "step: 160, loss: 0.12844866514205933\n",
            "step: 170, loss: 0.11450714617967606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7598039215686274, f1=0.7621247113163971, best_f1=0.7621247113163971\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07061909884214401\n",
            "step: 10, loss: 0.12285052239894867\n",
            "step: 20, loss: 0.054036419838666916\n",
            "step: 30, loss: 0.15734833478927612\n",
            "step: 40, loss: 0.016908753663301468\n",
            "step: 50, loss: 0.03225517272949219\n",
            "step: 60, loss: 0.045238226652145386\n",
            "step: 70, loss: 0.14266014099121094\n",
            "step: 80, loss: 0.06083041429519653\n",
            "step: 90, loss: 0.10829967260360718\n",
            "step: 100, loss: 0.01955145224928856\n",
            "step: 110, loss: 0.07082239538431168\n",
            "step: 120, loss: 0.01261519268155098\n",
            "step: 130, loss: 0.04284605011343956\n",
            "step: 140, loss: 0.07522385567426682\n",
            "step: 150, loss: 0.08277721703052521\n",
            "step: 160, loss: 0.1278148591518402\n",
            "step: 170, loss: 0.00693058967590332\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.797979797979798, f1=0.8083140877598153, best_f1=0.8083140877598153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009156487882137299\n",
            "step: 10, loss: 0.01996326632797718\n",
            "step: 20, loss: 0.010390842333436012\n",
            "step: 30, loss: 0.013773275539278984\n",
            "step: 40, loss: 0.011284854263067245\n",
            "step: 50, loss: 0.14380864799022675\n",
            "step: 60, loss: 0.1610913872718811\n",
            "step: 70, loss: 0.0770256295800209\n",
            "step: 80, loss: 0.06491522490978241\n",
            "step: 90, loss: 0.04497101902961731\n",
            "step: 100, loss: 0.008048031479120255\n",
            "step: 110, loss: 0.0017207748023793101\n",
            "step: 120, loss: 0.007945179007947445\n",
            "step: 130, loss: 0.0077005913481116295\n",
            "step: 140, loss: 0.014789191074669361\n",
            "step: 150, loss: 0.029643170535564423\n",
            "step: 160, loss: 0.17914210259914398\n",
            "step: 170, loss: 0.030829327180981636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.78, f1=0.771689497716895, best_f1=0.8083140877598153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009871645830571651\n",
            "step: 10, loss: 0.0248886551707983\n",
            "step: 20, loss: 0.00137730548158288\n",
            "step: 30, loss: 0.0019323753658682108\n",
            "step: 40, loss: 0.004837095271795988\n",
            "step: 50, loss: 0.07047946751117706\n",
            "step: 60, loss: 0.002754013519734144\n",
            "step: 70, loss: 0.0009695951011963189\n",
            "step: 80, loss: 0.008941047824919224\n",
            "step: 90, loss: 0.006054500583559275\n",
            "step: 100, loss: 0.006938259117305279\n",
            "step: 110, loss: 0.011805917136371136\n",
            "step: 120, loss: 0.004820120055228472\n",
            "step: 130, loss: 0.1270166039466858\n",
            "step: 140, loss: 0.013089381158351898\n",
            "step: 150, loss: 0.0017387360567227006\n",
            "step: 160, loss: 0.004260243382304907\n",
            "step: 170, loss: 0.03943953290581703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7906976744186047, f1=0.7924528301886793, best_f1=0.8083140877598153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021588226780295372\n",
            "step: 10, loss: 0.0010783844627439976\n",
            "step: 20, loss: 0.09193672984838486\n",
            "step: 30, loss: 0.027894018217921257\n",
            "step: 40, loss: 0.000910491740796715\n",
            "step: 50, loss: 0.0030819368548691273\n",
            "step: 60, loss: 0.004133069422096014\n",
            "step: 70, loss: 0.0010539793875068426\n",
            "step: 80, loss: 0.005567938555032015\n",
            "step: 90, loss: 0.007092003244906664\n",
            "step: 100, loss: 0.0705108642578125\n",
            "step: 110, loss: 0.013854943215847015\n",
            "step: 120, loss: 0.021193310618400574\n",
            "step: 130, loss: 0.009182050824165344\n",
            "step: 140, loss: 0.001872413675300777\n",
            "step: 150, loss: 0.05048566684126854\n",
            "step: 160, loss: 0.021060774102807045\n",
            "step: 170, loss: 0.004649720620363951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7844155844155845, f1=0.7895981087470448, best_f1=0.8083140877598153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014360406203195453\n",
            "step: 10, loss: 0.023530248552560806\n",
            "step: 20, loss: 0.002449322259053588\n",
            "step: 30, loss: 0.010052538476884365\n",
            "step: 40, loss: 0.00793358776718378\n",
            "step: 50, loss: 0.12445204704999924\n",
            "step: 60, loss: 0.05930840224027634\n",
            "step: 70, loss: 0.09367312490940094\n",
            "step: 80, loss: 0.03275679796934128\n",
            "step: 90, loss: 0.030413281172513962\n",
            "step: 100, loss: 0.008737563155591488\n",
            "step: 110, loss: 0.0021033291704952717\n",
            "step: 120, loss: 0.04805691912770271\n",
            "step: 130, loss: 0.14062778651714325\n",
            "step: 140, loss: 0.05504371225833893\n",
            "step: 150, loss: 0.012118052691221237\n",
            "step: 160, loss: 0.022405270487070084\n",
            "step: 170, loss: 0.0024788351729512215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8098765432098766, f1=0.7857142857142857, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06346803903579712\n",
            "step: 10, loss: 0.001476424396969378\n",
            "step: 20, loss: 0.0025778443086892366\n",
            "step: 30, loss: 0.11623257398605347\n",
            "step: 40, loss: 0.0007492968579754233\n",
            "step: 50, loss: 0.06382530182600021\n",
            "step: 60, loss: 0.0004490888677537441\n",
            "step: 70, loss: 0.02231474034488201\n",
            "step: 80, loss: 0.0006740691023878753\n",
            "step: 90, loss: 0.01600271463394165\n",
            "step: 100, loss: 0.0008880191599018872\n",
            "step: 110, loss: 0.0014237186405807734\n",
            "step: 120, loss: 0.004763900768011808\n",
            "step: 130, loss: 0.0034678231459110975\n",
            "step: 140, loss: 0.12036086618900299\n",
            "step: 150, loss: 0.08551856130361557\n",
            "step: 160, loss: 0.004897233098745346\n",
            "step: 170, loss: 0.001955605112016201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7855421686746988, f1=0.7818181818181817, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03578028455376625\n",
            "step: 10, loss: 0.005399651359766722\n",
            "step: 20, loss: 0.006619045976549387\n",
            "step: 30, loss: 0.0003066071658395231\n",
            "step: 40, loss: 0.0003075927961617708\n",
            "step: 50, loss: 0.0039403242990374565\n",
            "step: 60, loss: 0.007358692120760679\n",
            "step: 70, loss: 0.00041425187373533845\n",
            "step: 80, loss: 0.0004068543785251677\n",
            "step: 90, loss: 0.0007920240750536323\n",
            "step: 100, loss: 0.0031745859887450933\n",
            "step: 110, loss: 0.024564528837800026\n",
            "step: 120, loss: 0.0014736650045961142\n",
            "step: 130, loss: 0.0012556470464915037\n",
            "step: 140, loss: 0.030973970890045166\n",
            "step: 150, loss: 0.01329475175589323\n",
            "step: 160, loss: 0.00422065332531929\n",
            "step: 170, loss: 0.0033753577154129744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7729468599033817, f1=0.7755102040816327, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032984495628625154\n",
            "step: 10, loss: 0.034098319709300995\n",
            "step: 20, loss: 0.0015154839493334293\n",
            "step: 30, loss: 0.0011925561120733619\n",
            "step: 40, loss: 0.0006289267330430448\n",
            "step: 50, loss: 0.000255343911703676\n",
            "step: 60, loss: 0.0005149513599462807\n",
            "step: 70, loss: 0.11658070981502533\n",
            "step: 80, loss: 0.0002257572195958346\n",
            "step: 90, loss: 0.015386001206934452\n",
            "step: 100, loss: 0.0007323794416151941\n",
            "step: 110, loss: 0.0017538216197863221\n",
            "step: 120, loss: 0.12785407900810242\n",
            "step: 130, loss: 0.005652558524161577\n",
            "step: 140, loss: 0.009663957171142101\n",
            "step: 150, loss: 0.00046781578566879034\n",
            "step: 160, loss: 0.00040187378181144595\n",
            "step: 170, loss: 0.0004706756444647908\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7611548556430446, f1=0.7823960880195598, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004036376252770424\n",
            "step: 10, loss: 0.004211035557091236\n",
            "step: 20, loss: 0.002108511980623007\n",
            "step: 30, loss: 0.00032747170189395547\n",
            "step: 40, loss: 0.0010761187877506018\n",
            "step: 50, loss: 0.0019830227829515934\n",
            "step: 60, loss: 0.004830663092434406\n",
            "step: 70, loss: 0.0002833471226040274\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 80, loss: 0.000300778221571818\n",
            "step: 90, loss: 0.00016268593026325107\n",
            "step: 100, loss: 0.0001577728835400194\n",
            "step: 110, loss: 9.878176206257194e-05\n",
            "step: 120, loss: 0.013093595393002033\n",
            "step: 130, loss: 0.00014794376329518855\n",
            "step: 140, loss: 0.0002403700927970931\n",
            "step: 150, loss: 0.0011167738121002913\n",
            "step: 160, loss: 9.580074402038008e-05\n",
            "step: 170, loss: 0.004791234619915485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7604166666666665, f1=0.7893462469733656, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018510087102185935\n",
            "step: 10, loss: 0.0008286808733828366\n",
            "step: 20, loss: 0.001182180130854249\n",
            "step: 30, loss: 0.00030198594322428107\n",
            "step: 40, loss: 0.0001067103075911291\n",
            "step: 50, loss: 0.0023117815144360065\n",
            "step: 60, loss: 8.027419244172052e-05\n",
            "step: 70, loss: 0.001758192665874958\n",
            "step: 80, loss: 0.0005280922632664442\n",
            "step: 90, loss: 0.0005710338009521365\n",
            "step: 100, loss: 0.01911052130162716\n",
            "step: 110, loss: 0.00016369775403290987\n",
            "step: 120, loss: 0.004870882257819176\n",
            "step: 130, loss: 0.0004528301942627877\n",
            "step: 140, loss: 0.010790103115141392\n",
            "step: 150, loss: 7.777903374517336e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.0010944611858576536\n",
            "step: 170, loss: 0.0002214818960055709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7663043478260869, f1=0.7841191066997518, best_f1=0.7857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022597654606215656\n",
            "step: 10, loss: 0.002675937954336405\n",
            "step: 20, loss: 0.006091064307838678\n",
            "step: 30, loss: 0.000734248198568821\n",
            "step: 40, loss: 0.00012637648615054786\n",
            "step: 50, loss: 0.06124657019972801\n",
            "step: 60, loss: 0.010591704398393631\n",
            "step: 70, loss: 0.0007568770088255405\n",
            "step: 80, loss: 0.009095556102693081\n",
            "step: 90, loss: 0.000649642723146826\n",
            "step: 100, loss: 0.002303843852132559\n",
            "step: 110, loss: 0.0014009709702804685\n",
            "step: 120, loss: 0.0010100622894242406\n",
            "step: 130, loss: 0.0013971825828775764\n",
            "step: 140, loss: 0.0013042896753177047\n",
            "step: 150, loss: 0.00036785233533009887\n",
            "step: 160, loss: 0.0002768410195130855\n",
            "step: 170, loss: 0.00013021219638176262\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7608695652173912, f1=0.7819548872180452, best_f1=0.7857142857142857\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 295.91it/s]\n",
            "load_f1 = 0.4317180616740088\n",
            "real_f1 = 0.4112676056338029\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 271.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c27a9d2-8cc0-43f1-b27d-8674ea086c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 498kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.21MB/s]\n",
            "Downloading: 100% 440M/440M [00:43<00:00, 10.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.632515013217926\n",
            "step: 10, loss: 0.642586886882782\n",
            "step: 20, loss: 0.48570528626441956\n",
            "step: 30, loss: 0.3409290909767151\n",
            "step: 40, loss: 0.11028208583593369\n",
            "step: 50, loss: 0.2045281082391739\n",
            "step: 60, loss: 0.07137104868888855\n",
            "step: 70, loss: 0.0794663280248642\n",
            "step: 80, loss: 0.06400313228368759\n",
            "step: 90, loss: 0.14387458562850952\n",
            "step: 100, loss: 0.010596612468361855\n",
            "step: 110, loss: 0.19680488109588623\n",
            "step: 120, loss: 0.014248291961848736\n",
            "step: 130, loss: 0.15658271312713623\n",
            "step: 140, loss: 0.00864881370216608\n",
            "step: 150, loss: 0.09316825866699219\n",
            "step: 160, loss: 0.05390497297048569\n",
            "step: 170, loss: 0.08733851462602615\n",
            "step: 180, loss: 0.04188191890716553\n",
            "step: 190, loss: 0.05350013077259064\n",
            "step: 200, loss: 0.11071658134460449\n",
            "step: 210, loss: 0.007079010829329491\n",
            "step: 220, loss: 0.019978079944849014\n",
            "step: 230, loss: 0.16389219462871552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9573033707865168, f1=0.9508571428571428, best_f1=0.9508571428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008707171306014061\n",
            "step: 10, loss: 0.004445025231689215\n",
            "step: 20, loss: 0.2732962667942047\n",
            "step: 30, loss: 0.13314373791217804\n",
            "step: 40, loss: 0.14821404218673706\n",
            "step: 50, loss: 0.003306318772956729\n",
            "step: 60, loss: 0.0044740731827914715\n",
            "step: 70, loss: 0.12940438091754913\n",
            "step: 80, loss: 0.005742373410612345\n",
            "step: 90, loss: 0.04643292352557182\n",
            "step: 100, loss: 0.03883311524987221\n",
            "step: 110, loss: 0.06004185974597931\n",
            "step: 120, loss: 0.048261791467666626\n",
            "step: 130, loss: 0.034429486840963364\n",
            "step: 140, loss: 0.001882216427475214\n",
            "step: 150, loss: 0.011766569688916206\n",
            "step: 160, loss: 0.040379010140895844\n",
            "step: 170, loss: 0.0349145270884037\n",
            "step: 180, loss: 0.05707649886608124\n",
            "step: 190, loss: 0.05429858714342117\n",
            "step: 200, loss: 0.0033750608563423157\n",
            "step: 210, loss: 0.01441223919391632\n",
            "step: 220, loss: 0.06120113655924797\n",
            "step: 230, loss: 0.014730937778949738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9612403100775194, f1=0.9620535714285715, best_f1=0.9620535714285715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0061707934364676476\n",
            "step: 10, loss: 0.008716066367924213\n",
            "step: 20, loss: 0.10662104189395905\n",
            "step: 30, loss: 0.011383439414203167\n",
            "step: 40, loss: 0.040450386703014374\n",
            "step: 50, loss: 0.03347443789243698\n",
            "step: 60, loss: 0.011155272834002972\n",
            "step: 70, loss: 0.00203660037368536\n",
            "step: 80, loss: 0.002772317035123706\n",
            "step: 90, loss: 0.03283381462097168\n",
            "step: 100, loss: 0.0007043723599053919\n",
            "step: 110, loss: 0.001758666941896081\n",
            "step: 120, loss: 0.006520633120089769\n",
            "step: 130, loss: 0.001816964359022677\n",
            "step: 140, loss: 0.008660449646413326\n",
            "step: 150, loss: 0.010924352332949638\n",
            "step: 160, loss: 0.09634283185005188\n",
            "step: 170, loss: 0.005196617916226387\n",
            "step: 180, loss: 0.008211025968194008\n",
            "step: 190, loss: 0.013715066947042942\n",
            "step: 200, loss: 0.15246018767356873\n",
            "step: 210, loss: 0.019636817276477814\n",
            "step: 220, loss: 0.03220367431640625\n",
            "step: 230, loss: 0.0005716328741982579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9688888888888889, f1=0.9696969696969697, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003485091496258974\n",
            "step: 10, loss: 0.0032758228480815887\n",
            "step: 20, loss: 0.003009666921570897\n",
            "step: 30, loss: 0.000701277342159301\n",
            "step: 40, loss: 0.0012509621446952224\n",
            "step: 50, loss: 0.00024041836149990559\n",
            "step: 60, loss: 0.0007630693144164979\n",
            "step: 70, loss: 0.05427068844437599\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.0007083013188093901\n",
            "step: 90, loss: 0.0038908382412046194\n",
            "step: 100, loss: 0.004336394835263491\n",
            "step: 110, loss: 0.0012652447912842035\n",
            "step: 120, loss: 0.030641455203294754\n",
            "step: 130, loss: 0.00178443337790668\n",
            "step: 140, loss: 0.0021300893276929855\n",
            "step: 150, loss: 0.11849034577608109\n",
            "step: 160, loss: 0.0030662785284221172\n",
            "step: 170, loss: 0.002556981984525919\n",
            "step: 180, loss: 0.0009786596056073904\n",
            "step: 190, loss: 0.0036179644521325827\n",
            "step: 200, loss: 0.03304629027843475\n",
            "step: 210, loss: 0.008109665475785732\n",
            "step: 220, loss: 0.0003810659109149128\n",
            "step: 230, loss: 0.030986882746219635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9688195991091313, f1=0.971815107102593, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0313764363527298\n",
            "step: 10, loss: 0.00110853579826653\n",
            "step: 20, loss: 0.03255898505449295\n",
            "step: 30, loss: 0.0008322803187184036\n",
            "step: 40, loss: 0.0006541418260894716\n",
            "step: 50, loss: 0.0004396237200126052\n",
            "step: 60, loss: 0.0013294850941747427\n",
            "step: 70, loss: 0.0005191076779738069\n",
            "step: 80, loss: 0.0003482549509499222\n",
            "step: 90, loss: 0.000933431088924408\n",
            "step: 100, loss: 0.00018260625074617565\n",
            "step: 110, loss: 0.0002487288147676736\n",
            "step: 120, loss: 0.00017778777692001313\n",
            "step: 130, loss: 0.00755632109940052\n",
            "step: 140, loss: 0.02235453389585018\n",
            "step: 150, loss: 0.008052773773670197\n",
            "step: 160, loss: 0.004594599828124046\n",
            "step: 170, loss: 0.0128934346139431\n",
            "step: 180, loss: 0.011025452986359596\n",
            "step: 190, loss: 0.07651176303625107\n",
            "step: 200, loss: 0.0014585979515686631\n",
            "step: 210, loss: 0.0004641262348741293\n",
            "step: 220, loss: 0.00484930444508791\n",
            "step: 230, loss: 0.0011000155936926603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9663677130044843, f1=0.9628796400449944, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003897473681718111\n",
            "step: 10, loss: 0.007727705407887697\n",
            "step: 20, loss: 0.002828221768140793\n",
            "step: 30, loss: 0.002845784416422248\n",
            "step: 40, loss: 0.0003880493459291756\n",
            "step: 50, loss: 0.0006998783210292459\n",
            "step: 60, loss: 0.0003698954242281616\n",
            "step: 70, loss: 0.0003917161957360804\n",
            "step: 80, loss: 0.0002928819158114493\n",
            "step: 90, loss: 0.000907960522454232\n",
            "step: 100, loss: 0.03363803029060364\n",
            "step: 110, loss: 0.008714649826288223\n",
            "step: 120, loss: 0.0006857209373265505\n",
            "step: 130, loss: 0.00036949425702914596\n",
            "step: 140, loss: 0.0003002323501277715\n",
            "step: 150, loss: 0.030111163854599\n",
            "step: 160, loss: 0.001437175553292036\n",
            "step: 170, loss: 0.004394026938825846\n",
            "step: 180, loss: 0.00626629451289773\n",
            "step: 190, loss: 0.0015360231045633554\n",
            "step: 200, loss: 0.00023735317518003285\n",
            "step: 210, loss: 0.0002849874726962298\n",
            "step: 220, loss: 0.0002037037193076685\n",
            "step: 230, loss: 0.06575588136911392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.966740576496674, f1=0.967525195968645, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008191352826543152\n",
            "step: 10, loss: 0.0004027953837066889\n",
            "step: 20, loss: 0.0001653027575230226\n",
            "step: 30, loss: 0.0010259858099743724\n",
            "step: 40, loss: 0.0005585222388617694\n",
            "step: 50, loss: 0.001976566156372428\n",
            "step: 60, loss: 0.0014712570700794458\n",
            "step: 70, loss: 0.002641488565132022\n",
            "step: 80, loss: 0.0067573427222669125\n",
            "step: 90, loss: 0.006789698265492916\n",
            "step: 100, loss: 0.0003935233980882913\n",
            "step: 110, loss: 0.0017026205314323306\n",
            "step: 120, loss: 0.0008116101380437613\n",
            "step: 130, loss: 0.0010968308197334409\n",
            "step: 140, loss: 0.00027804929413832724\n",
            "step: 150, loss: 0.010885702446103096\n",
            "step: 160, loss: 0.03949431702494621\n",
            "step: 170, loss: 0.003121807938441634\n",
            "step: 180, loss: 0.003114062361419201\n",
            "step: 190, loss: 0.0002449059102218598\n",
            "step: 200, loss: 0.03408607095479965\n",
            "step: 210, loss: 9.969292295863852e-05\n",
            "step: 220, loss: 0.00023239814618136734\n",
            "step: 230, loss: 0.0046981330960989\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9626274065685164, f1=0.9627118644067796, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00025787318008951843\n",
            "step: 10, loss: 0.0005329395062290132\n",
            "step: 20, loss: 0.00016181808314286172\n",
            "step: 30, loss: 0.0001452345895813778\n",
            "step: 40, loss: 0.008658148348331451\n",
            "step: 50, loss: 0.0033689162228256464\n",
            "step: 60, loss: 9.840037091635168e-05\n",
            "step: 70, loss: 0.00023010374570731074\n",
            "step: 80, loss: 0.0002559804415795952\n",
            "step: 90, loss: 7.615715003339574e-05\n",
            "step: 100, loss: 0.00015304828411899507\n",
            "step: 110, loss: 0.00014258756709750742\n",
            "step: 120, loss: 0.00042053338256664574\n",
            "step: 130, loss: 0.000342707586241886\n",
            "step: 140, loss: 5.749555930378847e-05\n",
            "step: 150, loss: 5.652638719766401e-05\n",
            "step: 160, loss: 6.097367077018134e-05\n",
            "step: 170, loss: 0.00010083952656714246\n",
            "step: 180, loss: 0.00018755563360173255\n",
            "step: 190, loss: 0.0006454075337387621\n",
            "step: 200, loss: 0.0001500455109635368\n",
            "step: 210, loss: 0.00932761188596487\n",
            "step: 220, loss: 0.0023782984353601933\n",
            "step: 230, loss: 0.00025820211158134043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9683972911963882, f1=0.9716231555051079, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.577813762007281e-05\n",
            "step: 10, loss: 0.0005313002620823681\n",
            "step: 20, loss: 0.0012607163516804576\n",
            "step: 30, loss: 9.089201921597123e-05\n",
            "step: 40, loss: 0.0022128578275442123\n",
            "step: 50, loss: 0.0009551187977194786\n",
            "step: 60, loss: 0.00037732371129095554\n",
            "step: 70, loss: 0.002642866689711809\n",
            "step: 80, loss: 0.0006920718587934971\n",
            "step: 90, loss: 0.0002371771406615153\n",
            "step: 100, loss: 0.00020843898528255522\n",
            "step: 110, loss: 6.226902769412845e-05\n",
            "step: 120, loss: 0.00015422103751916438\n",
            "step: 130, loss: 0.0003672555321827531\n",
            "step: 140, loss: 4.2413201299495995e-05\n",
            "step: 150, loss: 0.00046992284478619695\n",
            "step: 160, loss: 0.021673260256648064\n",
            "step: 170, loss: 0.0006622576620429754\n",
            "step: 180, loss: 0.0006855320534668863\n",
            "step: 190, loss: 8.93630858627148e-05\n",
            "step: 200, loss: 0.00017619537538848817\n",
            "step: 210, loss: 4.9384932935936376e-05\n",
            "step: 220, loss: 0.0001947819400811568\n",
            "step: 230, loss: 0.00017853373719844967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9711111111111111, f1=0.9633740288568259, best_f1=0.9633740288568259\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.389554957626387e-05\n",
            "step: 10, loss: 3.3515007089590654e-05\n",
            "step: 20, loss: 4.001132765552029e-05\n",
            "step: 30, loss: 0.00010455241135787219\n",
            "step: 40, loss: 3.8502985262311995e-05\n",
            "step: 50, loss: 0.00012185226660221815\n",
            "step: 60, loss: 0.0002829351869877428\n",
            "step: 70, loss: 0.00031378972926177084\n",
            "step: 80, loss: 7.435508450726047e-05\n",
            "step: 90, loss: 0.0013448643730953336\n",
            "step: 100, loss: 4.9669590225676075e-05\n",
            "step: 110, loss: 0.00011564144369913265\n",
            "step: 120, loss: 5.515383600140922e-05\n",
            "step: 130, loss: 0.00010495687456568703\n",
            "step: 140, loss: 0.0420105904340744\n",
            "step: 150, loss: 0.030273662880063057\n",
            "step: 160, loss: 3.4210755984531716e-05\n",
            "step: 170, loss: 4.247954711900093e-05\n",
            "step: 180, loss: 0.00010681849380489439\n",
            "step: 190, loss: 0.02281268686056137\n",
            "step: 200, loss: 3.0434553991653956e-05\n",
            "step: 210, loss: 7.358835136983544e-05\n",
            "step: 220, loss: 8.084547880571336e-05\n",
            "step: 230, loss: 0.00031317787943407893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9713024282560706, f1=0.966740576496674, best_f1=0.966740576496674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012603153300005943\n",
            "step: 10, loss: 0.00012424009037204087\n",
            "step: 20, loss: 0.002612991724163294\n",
            "step: 30, loss: 0.0003116040024906397\n",
            "step: 40, loss: 0.0003168912953697145\n",
            "step: 50, loss: 0.002869271906092763\n",
            "step: 60, loss: 0.012627310119569302\n",
            "step: 70, loss: 0.0012963837943971157\n",
            "step: 80, loss: 9.745819988893345e-05\n",
            "step: 90, loss: 0.002131757326424122\n",
            "step: 100, loss: 0.00011934623762499541\n",
            "step: 110, loss: 0.00011732470011338592\n",
            "step: 120, loss: 0.0025926369708031416\n",
            "step: 130, loss: 3.684769762912765e-05\n",
            "step: 140, loss: 6.282639515120536e-05\n",
            "step: 150, loss: 0.0034851953387260437\n",
            "step: 160, loss: 9.661618969403207e-05\n",
            "step: 170, loss: 0.0001241008867509663\n",
            "step: 180, loss: 0.0001238172990269959\n",
            "step: 190, loss: 6.446053157560527e-05\n",
            "step: 200, loss: 0.00026765745133161545\n",
            "step: 210, loss: 3.214850221411325e-05\n",
            "step: 220, loss: 5.1684652135008946e-05\n",
            "step: 230, loss: 8.315704326378182e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9720044792833147, f1=0.9740698985343857, best_f1=0.9740698985343857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012665046961046755\n",
            "step: 10, loss: 3.84167542506475e-05\n",
            "step: 20, loss: 6.65660627419129e-05\n",
            "step: 30, loss: 0.0005343802622519433\n",
            "step: 40, loss: 0.001211248803883791\n",
            "step: 50, loss: 0.0006840151036158204\n",
            "step: 60, loss: 0.0008807973354123533\n",
            "step: 70, loss: 0.0006700429366901517\n",
            "step: 80, loss: 0.00010185245628235862\n",
            "step: 90, loss: 5.44132744835224e-05\n",
            "step: 100, loss: 3.057963840547018e-05\n",
            "step: 110, loss: 4.765520498040132e-05\n",
            "step: 120, loss: 0.00014860588998999447\n",
            "step: 130, loss: 5.140569555805996e-05\n",
            "step: 140, loss: 5.210328890825622e-05\n",
            "step: 150, loss: 9.036024857778102e-05\n",
            "step: 160, loss: 0.003976966254413128\n",
            "step: 170, loss: 3.1209456210490316e-05\n",
            "step: 180, loss: 3.8092650356702507e-05\n",
            "step: 190, loss: 0.031920842826366425\n",
            "step: 200, loss: 5.250625326880254e-05\n",
            "step: 210, loss: 6.776987720513716e-05\n",
            "step: 220, loss: 0.0006050681113265455\n",
            "step: 230, loss: 0.00020283233607187867\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9732739420935412, f1=0.9741863075196409, best_f1=0.9741863075196409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.142366328276694e-05\n",
            "step: 10, loss: 9.809080074774101e-05\n",
            "step: 20, loss: 6.375390512403101e-05\n",
            "step: 30, loss: 0.00020143736037425697\n",
            "step: 40, loss: 4.8878406232688576e-05\n",
            "step: 50, loss: 0.010259118862450123\n",
            "step: 60, loss: 0.05733020603656769\n",
            "step: 70, loss: 0.002289160154759884\n",
            "step: 80, loss: 6.055839185137302e-05\n",
            "step: 90, loss: 8.843211981002241e-05\n",
            "step: 100, loss: 3.778716563829221e-05\n",
            "step: 110, loss: 0.016159774735569954\n",
            "step: 120, loss: 0.0010747717460617423\n",
            "step: 130, loss: 5.4898278904147446e-05\n",
            "step: 140, loss: 0.00011628969514276832\n",
            "step: 150, loss: 4.0477119910065085e-05\n",
            "step: 160, loss: 6.0830840084236115e-05\n",
            "step: 170, loss: 0.0001112866448238492\n",
            "step: 180, loss: 3.505387212499045e-05\n",
            "step: 190, loss: 3.563069549272768e-05\n",
            "step: 200, loss: 7.888636901043355e-05\n",
            "step: 210, loss: 2.1900526917306706e-05\n",
            "step: 220, loss: 3.9582911995239556e-05\n",
            "step: 230, loss: 5.4418585932580754e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9696969696969697, f1=0.9708520179372198, best_f1=0.9741863075196409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002459439856465906\n",
            "step: 10, loss: 6.914579716976732e-05\n",
            "step: 20, loss: 2.5975741664296947e-05\n",
            "step: 30, loss: 4.036188329337165e-05\n",
            "step: 40, loss: 3.506320354063064e-05\n",
            "step: 50, loss: 5.294669608701952e-05\n",
            "step: 60, loss: 0.0008331572753377259\n",
            "step: 70, loss: 6.642404332524166e-05\n",
            "step: 80, loss: 5.59004838578403e-05\n",
            "step: 90, loss: 8.489149331580848e-05\n",
            "step: 100, loss: 3.712359830387868e-05\n",
            "step: 110, loss: 0.0001552381581859663\n",
            "step: 120, loss: 2.2719978005625308e-05\n",
            "step: 130, loss: 3.5698350984603167e-05\n",
            "step: 140, loss: 0.00013094209134578705\n",
            "step: 150, loss: 2.6858462661039084e-05\n",
            "step: 160, loss: 0.00012578078894875944\n",
            "step: 170, loss: 2.2947246179683134e-05\n",
            "step: 180, loss: 0.0001427863899152726\n",
            "step: 190, loss: 5.929296821705066e-05\n",
            "step: 200, loss: 4.7211669880198315e-05\n",
            "step: 210, loss: 3.528477463987656e-05\n",
            "step: 220, loss: 7.426505908370018e-05\n",
            "step: 230, loss: 4.523422830970958e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9765363128491621, f1=0.9741863075196409, best_f1=0.9741863075196409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.7081820007879287e-05\n",
            "step: 10, loss: 1.985157359740697e-05\n",
            "step: 20, loss: 4.268203701940365e-05\n",
            "step: 30, loss: 0.00040701578836888075\n",
            "step: 40, loss: 0.00225495221093297\n",
            "step: 50, loss: 0.024871936067938805\n",
            "step: 60, loss: 0.00010364632908022031\n",
            "step: 70, loss: 3.9731508877594024e-05\n",
            "step: 80, loss: 0.0004954891628585756\n",
            "step: 90, loss: 6.046359703759663e-05\n",
            "step: 100, loss: 5.593514651991427e-05\n",
            "step: 110, loss: 3.016206574102398e-05\n",
            "step: 120, loss: 6.362241401802748e-05\n",
            "step: 130, loss: 3.712862962856889e-05\n",
            "step: 140, loss: 2.974550261569675e-05\n",
            "step: 150, loss: 9.531294927001e-05\n",
            "step: 160, loss: 3.359527181601152e-05\n",
            "step: 170, loss: 2.406844578217715e-05\n",
            "step: 180, loss: 0.00025534952874295413\n",
            "step: 190, loss: 0.0009832537034526467\n",
            "step: 200, loss: 4.2562358430586755e-05\n",
            "step: 210, loss: 7.216700032586232e-05\n",
            "step: 220, loss: 5.792981755803339e-05\n",
            "step: 230, loss: 0.0001079776047845371\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9765363128491621, f1=0.9730941704035874, best_f1=0.9741863075196409\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 194.28it/s]\n",
            "load_f1 = 0.9723756906077348\n",
            "real_f1 = 0.9680264608599779\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 227.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10fd5ac-09cf-4813-ca8c-736af898c5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6217563152313232\n",
            "step: 10, loss: 0.5185362100601196\n",
            "step: 20, loss: 0.528659462928772\n",
            "step: 30, loss: 0.20974169671535492\n",
            "step: 40, loss: 0.23954269289970398\n",
            "step: 50, loss: 0.19369393587112427\n",
            "step: 60, loss: 0.11263235658407211\n",
            "step: 70, loss: 0.16929379105567932\n",
            "step: 80, loss: 0.036021456122398376\n",
            "step: 90, loss: 0.3868634104728699\n",
            "step: 100, loss: 0.058321814984083176\n",
            "step: 110, loss: 0.17388930916786194\n",
            "step: 120, loss: 0.16949546337127686\n",
            "step: 130, loss: 0.08196000754833221\n",
            "step: 140, loss: 0.10859055817127228\n",
            "step: 150, loss: 0.06975255161523819\n",
            "step: 160, loss: 0.03722602501511574\n",
            "step: 170, loss: 0.25539934635162354\n",
            "step: 180, loss: 0.19951701164245605\n",
            "step: 190, loss: 0.00877998024225235\n",
            "step: 200, loss: 0.22396545112133026\n",
            "step: 210, loss: 0.06868281960487366\n",
            "step: 220, loss: 0.10116655379533768\n",
            "step: 230, loss: 0.21766002476215363\n",
            "step: 240, loss: 0.11056974530220032\n",
            "step: 250, loss: 0.03835686668753624\n",
            "step: 260, loss: 0.15007208287715912\n",
            "step: 270, loss: 0.025766704231500626\n",
            "step: 280, loss: 0.09257613122463226\n",
            "step: 290, loss: 0.25040900707244873\n",
            "step: 300, loss: 0.10394662618637085\n",
            "step: 310, loss: 0.26456400752067566\n",
            "step: 320, loss: 0.1837003380060196\n",
            "step: 330, loss: 0.09954865276813507\n",
            "step: 340, loss: 0.13997527956962585\n",
            "step: 350, loss: 0.1885589212179184\n",
            "step: 360, loss: 0.06489942222833633\n",
            "step: 370, loss: 0.11780085414648056\n",
            "step: 380, loss: 0.01993892341852188\n",
            "step: 390, loss: 0.19154612720012665\n",
            "step: 400, loss: 0.2775323688983917\n",
            "step: 410, loss: 0.03784952312707901\n",
            "step: 420, loss: 0.0956198200583458\n",
            "step: 430, loss: 0.14993180334568024\n",
            "step: 440, loss: 0.01927666924893856\n",
            "step: 450, loss: 0.026170678436756134\n",
            "step: 460, loss: 0.02658405900001526\n",
            "step: 470, loss: 0.1304294317960739\n",
            "step: 480, loss: 0.07926707714796066\n",
            "step: 490, loss: 0.07495051622390747\n",
            "step: 500, loss: 0.08639046549797058\n",
            "step: 510, loss: 0.12532928586006165\n",
            "step: 520, loss: 0.03802694380283356\n",
            "step: 530, loss: 0.018738681450486183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9145259224661373, f1=0.9183202584217812, best_f1=0.9183202584217812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1174502745270729\n",
            "step: 10, loss: 0.11198265105485916\n",
            "step: 20, loss: 0.03513826057314873\n",
            "step: 30, loss: 0.03839811310172081\n",
            "step: 40, loss: 0.07544117420911789\n",
            "step: 50, loss: 0.1918499767780304\n",
            "step: 60, loss: 0.05411916971206665\n",
            "step: 70, loss: 0.11501374840736389\n",
            "step: 80, loss: 0.11373814195394516\n",
            "step: 90, loss: 0.022605741396546364\n",
            "step: 100, loss: 0.10951391607522964\n",
            "step: 110, loss: 0.07377022504806519\n",
            "step: 120, loss: 0.2617008090019226\n",
            "step: 130, loss: 0.16877806186676025\n",
            "step: 140, loss: 0.11270293593406677\n",
            "step: 150, loss: 0.11861884593963623\n",
            "step: 160, loss: 0.08828790485858917\n",
            "step: 170, loss: 0.07434945553541183\n",
            "step: 180, loss: 0.02767482027411461\n",
            "step: 190, loss: 0.08144467324018478\n",
            "step: 200, loss: 0.04010473191738129\n",
            "step: 210, loss: 0.04425504803657532\n",
            "step: 220, loss: 0.07749725133180618\n",
            "step: 230, loss: 0.011293178424239159\n",
            "step: 240, loss: 0.01196235977113247\n",
            "step: 250, loss: 0.08577824383974075\n",
            "step: 260, loss: 0.004623805172741413\n",
            "step: 270, loss: 0.24940446019172668\n",
            "step: 280, loss: 0.12377195805311203\n",
            "step: 290, loss: 0.05874595791101456\n",
            "step: 300, loss: 0.179429829120636\n",
            "step: 310, loss: 0.023624107241630554\n",
            "step: 320, loss: 0.18986548483371735\n",
            "step: 330, loss: 0.06934400647878647\n",
            "step: 340, loss: 0.07054231315851212\n",
            "step: 350, loss: 0.0032262869644910097\n",
            "step: 360, loss: 0.16680796444416046\n",
            "step: 370, loss: 0.12653300166130066\n",
            "step: 380, loss: 0.04076289013028145\n",
            "step: 390, loss: 0.080286405980587\n",
            "step: 400, loss: 0.05600016564130783\n",
            "step: 410, loss: 0.05144456773996353\n",
            "step: 420, loss: 0.014797277748584747\n",
            "step: 430, loss: 0.017074773088097572\n",
            "step: 440, loss: 0.1060648113489151\n",
            "step: 450, loss: 0.09057100862264633\n",
            "step: 460, loss: 0.02069653384387493\n",
            "step: 470, loss: 0.0347505621612072\n",
            "step: 480, loss: 0.19630517065525055\n",
            "step: 490, loss: 0.013121750205755234\n",
            "step: 500, loss: 0.3157868981361389\n",
            "step: 510, loss: 0.039313312619924545\n",
            "step: 520, loss: 0.10427746921777725\n",
            "step: 530, loss: 0.11055080592632294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9198350893266148, f1=0.9175824175824175, best_f1=0.9175824175824175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14752601087093353\n",
            "step: 10, loss: 0.04301169514656067\n",
            "step: 20, loss: 0.14183299243450165\n",
            "step: 30, loss: 0.07481282949447632\n",
            "step: 40, loss: 0.08399984985589981\n",
            "step: 50, loss: 0.28551170229911804\n",
            "step: 60, loss: 0.09912621229887009\n",
            "step: 70, loss: 0.005815268028527498\n",
            "step: 80, loss: 0.0025211747270077467\n",
            "step: 90, loss: 0.006642411928623915\n",
            "step: 100, loss: 0.04438835754990578\n",
            "step: 110, loss: 0.01080573070794344\n",
            "step: 120, loss: 0.007325942628085613\n",
            "step: 130, loss: 0.028072042390704155\n",
            "step: 140, loss: 0.09890975058078766\n",
            "step: 150, loss: 0.09789669513702393\n",
            "step: 160, loss: 0.022134840488433838\n",
            "step: 170, loss: 0.11381950974464417\n",
            "step: 180, loss: 0.020424241200089455\n",
            "step: 190, loss: 0.00575905479490757\n",
            "step: 200, loss: 0.012880099937319756\n",
            "step: 210, loss: 0.04377387464046478\n",
            "step: 220, loss: 0.13630685210227966\n",
            "step: 230, loss: 0.07476348429918289\n",
            "step: 240, loss: 0.07298135757446289\n",
            "step: 250, loss: 0.03175550326704979\n",
            "step: 260, loss: 0.016509834676980972\n",
            "step: 270, loss: 0.016600582748651505\n",
            "step: 280, loss: 0.1564587503671646\n",
            "step: 290, loss: 0.018444547429680824\n",
            "step: 300, loss: 0.008675392717123032\n",
            "step: 310, loss: 0.04886860027909279\n",
            "step: 320, loss: 0.007505382411181927\n",
            "step: 330, loss: 0.009211632423102856\n",
            "step: 340, loss: 0.03927475959062576\n",
            "step: 350, loss: 0.008387796580791473\n",
            "step: 360, loss: 0.02026875503361225\n",
            "step: 370, loss: 0.03501884266734123\n",
            "step: 380, loss: 0.01608789712190628\n",
            "step: 390, loss: 0.018634753301739693\n",
            "step: 400, loss: 0.029223496094346046\n",
            "step: 410, loss: 0.006154345814138651\n",
            "step: 420, loss: 0.11110235005617142\n",
            "step: 430, loss: 0.05328451097011566\n",
            "step: 440, loss: 0.0373140312731266\n",
            "step: 450, loss: 0.07822777330875397\n",
            "step: 460, loss: 0.047632988542318344\n",
            "step: 470, loss: 0.16104555130004883\n",
            "step: 480, loss: 0.013920470140874386\n",
            "step: 490, loss: 0.006044168956577778\n",
            "step: 500, loss: 0.12293573468923569\n",
            "step: 510, loss: 0.038633398711681366\n",
            "step: 520, loss: 0.024924710392951965\n",
            "step: 530, loss: 0.10297740995883942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9251950435979808, f1=0.9174646602827177, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011501934379339218\n",
            "step: 10, loss: 0.006143192760646343\n",
            "step: 20, loss: 0.011187439784407616\n",
            "step: 30, loss: 0.006300085689872503\n",
            "step: 40, loss: 0.10656970739364624\n",
            "step: 50, loss: 0.011358950287103653\n",
            "step: 60, loss: 0.02408277802169323\n",
            "step: 70, loss: 0.03826650232076645\n",
            "step: 80, loss: 0.0019520242931321263\n",
            "step: 90, loss: 0.14116743206977844\n",
            "step: 100, loss: 0.0195135697722435\n",
            "step: 110, loss: 0.021271368488669395\n",
            "step: 120, loss: 0.021892907097935677\n",
            "step: 130, loss: 0.02959187887609005\n",
            "step: 140, loss: 0.009629689157009125\n",
            "step: 150, loss: 0.0864962562918663\n",
            "step: 160, loss: 0.05791965126991272\n",
            "step: 170, loss: 0.0225607231259346\n",
            "step: 180, loss: 0.011915854178369045\n",
            "step: 190, loss: 0.0188397616147995\n",
            "step: 200, loss: 0.05696810036897659\n",
            "step: 210, loss: 0.061169542372226715\n",
            "step: 220, loss: 0.005242137238383293\n",
            "step: 230, loss: 0.09382117539644241\n",
            "step: 240, loss: 0.017387978732585907\n",
            "step: 250, loss: 0.037084102630615234\n",
            "step: 260, loss: 0.03155040368437767\n",
            "step: 270, loss: 0.011743546463549137\n",
            "step: 280, loss: 0.14774876832962036\n",
            "step: 290, loss: 0.03353733569383621\n",
            "step: 300, loss: 0.00048484973376616836\n",
            "step: 310, loss: 0.00494310911744833\n",
            "step: 320, loss: 0.05927431955933571\n",
            "step: 330, loss: 0.13166429102420807\n",
            "step: 340, loss: 0.19164764881134033\n",
            "step: 350, loss: 0.047946371138095856\n",
            "step: 360, loss: 0.06767883151769638\n",
            "step: 370, loss: 0.04299916699528694\n",
            "step: 380, loss: 0.02901976741850376\n",
            "step: 390, loss: 0.009637993760406971\n",
            "step: 400, loss: 0.0007529295980930328\n",
            "step: 410, loss: 0.0032056362833827734\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 420, loss: 0.17642678320407867\n",
            "step: 430, loss: 0.10943920910358429\n",
            "step: 440, loss: 0.005009010899811983\n",
            "step: 450, loss: 0.00944541022181511\n",
            "step: 460, loss: 0.004829959943890572\n",
            "step: 470, loss: 0.1131267249584198\n",
            "step: 480, loss: 0.04866577684879303\n",
            "step: 490, loss: 0.011057766154408455\n",
            "step: 500, loss: 0.008993654511868954\n",
            "step: 510, loss: 0.014035049825906754\n",
            "step: 520, loss: 0.09119591116905212\n",
            "step: 530, loss: 0.030052676796913147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9189439555349699, f1=0.9191919191919192, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009686539880931377\n",
            "step: 10, loss: 0.05663127824664116\n",
            "step: 20, loss: 0.005653551314026117\n",
            "step: 30, loss: 0.0010664118453860283\n",
            "step: 40, loss: 0.0034200744703412056\n",
            "step: 50, loss: 0.16538400948047638\n",
            "step: 60, loss: 0.1539832055568695\n",
            "step: 70, loss: 0.025186553597450256\n",
            "step: 80, loss: 0.0033476930111646652\n",
            "step: 90, loss: 0.0047604721039533615\n",
            "step: 100, loss: 0.010415850207209587\n",
            "step: 110, loss: 0.0016223996644839644\n",
            "step: 120, loss: 0.00136427185498178\n",
            "step: 130, loss: 0.03846132382750511\n",
            "step: 140, loss: 0.003900137497112155\n",
            "step: 150, loss: 0.0010695658857002854\n",
            "step: 160, loss: 0.0018838171381503344\n",
            "step: 170, loss: 0.03300345689058304\n",
            "step: 180, loss: 0.0008358415798284113\n",
            "step: 190, loss: 0.005972633138298988\n",
            "step: 200, loss: 0.005494397599250078\n",
            "step: 210, loss: 0.010531209409236908\n",
            "step: 220, loss: 0.012712971307337284\n",
            "step: 230, loss: 0.09708607941865921\n",
            "step: 240, loss: 0.0009875886607915163\n",
            "step: 250, loss: 0.037088293582201004\n",
            "step: 260, loss: 0.005805760156363249\n",
            "step: 270, loss: 0.003658795729279518\n",
            "step: 280, loss: 0.003801817772909999\n",
            "step: 290, loss: 0.12870706617832184\n",
            "step: 300, loss: 0.00627620005980134\n",
            "step: 310, loss: 0.0005597452400252223\n",
            "step: 320, loss: 0.09968040138483047\n",
            "step: 330, loss: 0.008604477159678936\n",
            "step: 340, loss: 0.006554785650223494\n",
            "step: 350, loss: 0.0009778772946447134\n",
            "step: 360, loss: 0.06218770891427994\n",
            "step: 370, loss: 0.09722001105546951\n",
            "step: 380, loss: 0.0017296909354627132\n",
            "step: 390, loss: 0.0006407503969967365\n",
            "step: 400, loss: 0.0592154823243618\n",
            "step: 410, loss: 0.002572778146713972\n",
            "step: 420, loss: 0.002174091525375843\n",
            "step: 430, loss: 0.05487305298447609\n",
            "step: 440, loss: 0.028773557394742966\n",
            "step: 450, loss: 0.010570275597274303\n",
            "step: 460, loss: 0.007475643884390593\n",
            "step: 470, loss: 0.10109750926494598\n",
            "step: 480, loss: 0.004732798784971237\n",
            "step: 490, loss: 0.0015453404048457742\n",
            "step: 500, loss: 0.015867402777075768\n",
            "step: 510, loss: 0.02921769581735134\n",
            "step: 520, loss: 0.031727004796266556\n",
            "step: 530, loss: 0.08913427591323853\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.909437559580553, f1=0.9157196969696969, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025604039430618286\n",
            "step: 10, loss: 0.004634636454284191\n",
            "step: 20, loss: 0.09059256315231323\n",
            "step: 30, loss: 0.0018550417153164744\n",
            "step: 40, loss: 0.029675807803869247\n",
            "step: 50, loss: 0.016085924580693245\n",
            "step: 60, loss: 0.0017454701010137796\n",
            "step: 70, loss: 0.010218623094260693\n",
            "step: 80, loss: 0.011358664371073246\n",
            "step: 90, loss: 0.0047526732087135315\n",
            "step: 100, loss: 0.013384304009377956\n",
            "step: 110, loss: 0.01404107827693224\n",
            "step: 120, loss: 0.001774753793142736\n",
            "step: 130, loss: 0.0990816205739975\n",
            "step: 140, loss: 0.004161810036748648\n",
            "step: 150, loss: 0.09930930286645889\n",
            "step: 160, loss: 0.003938819281756878\n",
            "step: 170, loss: 0.020587844774127007\n",
            "step: 180, loss: 0.0005835209158249199\n",
            "step: 190, loss: 0.002757801441475749\n",
            "step: 200, loss: 0.0028648697771131992\n",
            "step: 210, loss: 0.1098112165927887\n",
            "step: 220, loss: 0.014929983764886856\n",
            "step: 230, loss: 0.004218988120555878\n",
            "step: 240, loss: 0.07088574767112732\n",
            "step: 250, loss: 0.025131968781352043\n",
            "step: 260, loss: 0.06634501367807388\n",
            "step: 270, loss: 0.13532011210918427\n",
            "step: 280, loss: 0.0020011723972857\n",
            "step: 290, loss: 0.0008166754269041121\n",
            "step: 300, loss: 0.010964024811983109\n",
            "step: 310, loss: 0.008536754176020622\n",
            "step: 320, loss: 0.0015435630921274424\n",
            "step: 330, loss: 0.0009292737813666463\n",
            "step: 340, loss: 0.07518082112073898\n",
            "step: 350, loss: 0.0022662030532956123\n",
            "step: 360, loss: 0.00740521214902401\n",
            "step: 370, loss: 0.011883476749062538\n",
            "step: 380, loss: 0.004472378175705671\n",
            "step: 390, loss: 0.015551092103123665\n",
            "step: 400, loss: 0.0007772153476253152\n",
            "step: 410, loss: 0.005458640865981579\n",
            "step: 420, loss: 0.003814173396676779\n",
            "step: 430, loss: 0.0014043508563190699\n",
            "step: 440, loss: 0.006241587456315756\n",
            "step: 450, loss: 0.0001484852546127513\n",
            "step: 460, loss: 0.05141366645693779\n",
            "step: 470, loss: 0.01665542833507061\n",
            "step: 480, loss: 0.001123068737797439\n",
            "step: 490, loss: 0.006009036675095558\n",
            "step: 500, loss: 0.00921922829002142\n",
            "step: 510, loss: 0.017269229516386986\n",
            "step: 520, loss: 0.005457589402794838\n",
            "step: 530, loss: 0.010273654013872147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9149232914923291, f1=0.9167816091954023, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003734482452273369\n",
            "step: 10, loss: 0.000768407538998872\n",
            "step: 20, loss: 0.010275071486830711\n",
            "step: 30, loss: 0.001637682900764048\n",
            "step: 40, loss: 0.0009067517821677029\n",
            "step: 50, loss: 0.0695062205195427\n",
            "step: 60, loss: 0.003255739575251937\n",
            "step: 70, loss: 0.010675166733562946\n",
            "step: 80, loss: 0.011289029382169247\n",
            "step: 90, loss: 0.008016102947294712\n",
            "step: 100, loss: 8.113391231745481e-05\n",
            "step: 110, loss: 0.0010983203537762165\n",
            "step: 120, loss: 0.0010792150860652328\n",
            "step: 130, loss: 0.00130807317327708\n",
            "step: 140, loss: 0.00010351962555432692\n",
            "step: 150, loss: 0.11124274134635925\n",
            "step: 160, loss: 0.002039629267528653\n",
            "step: 170, loss: 0.001198777463287115\n",
            "step: 180, loss: 0.0029244539327919483\n",
            "step: 190, loss: 0.000742583186365664\n",
            "step: 200, loss: 0.0011950157349929214\n",
            "step: 210, loss: 0.00709048705175519\n",
            "step: 220, loss: 0.00040037903818301857\n",
            "step: 230, loss: 0.0009652001899667084\n",
            "step: 240, loss: 0.0002542250440455973\n",
            "step: 250, loss: 0.02399519644677639\n",
            "step: 260, loss: 0.00024430020130239427\n",
            "step: 270, loss: 0.0005863035912625492\n",
            "step: 280, loss: 0.00030163463088683784\n",
            "step: 290, loss: 7.915085006970912e-05\n",
            "step: 300, loss: 0.0025370996445417404\n",
            "step: 310, loss: 0.00014316110173240304\n",
            "step: 320, loss: 8.725855877855793e-05\n",
            "step: 330, loss: 0.018305039033293724\n",
            "step: 340, loss: 0.09980391710996628\n",
            "step: 350, loss: 0.0038321861065924168\n",
            "step: 360, loss: 0.0007325591868720949\n",
            "step: 370, loss: 0.00015722482930868864\n",
            "step: 380, loss: 0.005348801612854004\n",
            "step: 390, loss: 0.026121627539396286\n",
            "step: 400, loss: 0.0061453054659068584\n",
            "step: 410, loss: 0.012065837159752846\n",
            "step: 420, loss: 0.0036588481161743402\n",
            "step: 430, loss: 0.001399954897351563\n",
            "step: 440, loss: 0.027823064476251602\n",
            "step: 450, loss: 0.00034276340738870203\n",
            "step: 460, loss: 0.000507975579239428\n",
            "step: 470, loss: 0.00798630528151989\n",
            "step: 480, loss: 0.09809070825576782\n",
            "step: 490, loss: 0.057895079255104065\n",
            "step: 500, loss: 0.009334301576018333\n",
            "step: 510, loss: 0.005485821049660444\n",
            "step: 520, loss: 0.014415014535188675\n",
            "step: 530, loss: 0.008521633222699165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9182736455463728, f1=0.9176149294492489, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034271632321178913\n",
            "step: 10, loss: 0.014243828132748604\n",
            "step: 20, loss: 0.000576638849452138\n",
            "step: 30, loss: 0.07071506232023239\n",
            "step: 40, loss: 0.0018624382792040706\n",
            "step: 50, loss: 0.0024699755012989044\n",
            "step: 60, loss: 0.014319933019578457\n",
            "step: 70, loss: 0.0007009622640907764\n",
            "step: 80, loss: 0.0023836358450353146\n",
            "step: 90, loss: 0.0014176021795719862\n",
            "step: 100, loss: 0.002229986246675253\n",
            "step: 110, loss: 0.003953293897211552\n",
            "step: 120, loss: 0.057420555502176285\n",
            "step: 130, loss: 0.002059730002656579\n",
            "step: 140, loss: 0.0018884413875639439\n",
            "step: 150, loss: 0.003950256388634443\n",
            "step: 160, loss: 0.007761215325444937\n",
            "step: 170, loss: 0.004137315787374973\n",
            "step: 180, loss: 0.0007038542535156012\n",
            "step: 190, loss: 0.00452418252825737\n",
            "step: 200, loss: 0.0010942245135083795\n",
            "step: 210, loss: 0.01808985322713852\n",
            "step: 220, loss: 0.00371425854973495\n",
            "step: 230, loss: 0.0027921844739466906\n",
            "step: 240, loss: 0.005178879480808973\n",
            "step: 250, loss: 0.0010559671791270375\n",
            "step: 260, loss: 0.0005066763260401785\n",
            "step: 270, loss: 0.06130650267004967\n",
            "step: 280, loss: 0.003235835814848542\n",
            "step: 290, loss: 0.003853185335174203\n",
            "step: 300, loss: 0.01610385626554489\n",
            "step: 310, loss: 0.005450564902275801\n",
            "step: 320, loss: 0.02619277499616146\n",
            "step: 330, loss: 0.02107091434299946\n",
            "step: 340, loss: 0.003737544408068061\n",
            "step: 350, loss: 0.0009989844402298331\n",
            "step: 360, loss: 0.023610442876815796\n",
            "step: 370, loss: 0.0011208690702915192\n",
            "step: 380, loss: 0.001655382220633328\n",
            "step: 390, loss: 0.12692712247371674\n",
            "step: 400, loss: 0.00882565975189209\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 410, loss: 0.011370580643415451\n",
            "step: 420, loss: 0.005850100889801979\n",
            "step: 430, loss: 0.021229639649391174\n",
            "step: 440, loss: 0.006460669916123152\n",
            "step: 450, loss: 0.005369971040636301\n",
            "step: 460, loss: 0.00030409145983867347\n",
            "step: 470, loss: 0.0009203821537084877\n",
            "step: 480, loss: 0.0008511372143402696\n",
            "step: 490, loss: 0.009816989302635193\n",
            "step: 500, loss: 0.005991769023239613\n",
            "step: 510, loss: 0.0010354453697800636\n",
            "step: 520, loss: 0.0020484908018261194\n",
            "step: 530, loss: 0.02333933115005493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9227889564810482, f1=0.9239981575310916, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004147982224822044\n",
            "step: 10, loss: 8.837541827233508e-05\n",
            "step: 20, loss: 0.003781047184020281\n",
            "step: 30, loss: 0.0011413736501708627\n",
            "step: 40, loss: 0.02226177230477333\n",
            "step: 50, loss: 0.004975943360477686\n",
            "step: 60, loss: 0.006685927975922823\n",
            "step: 70, loss: 0.0008535824599675834\n",
            "step: 80, loss: 0.0017192417290061712\n",
            "step: 90, loss: 0.0001254996459465474\n",
            "step: 100, loss: 0.005582455080002546\n",
            "step: 110, loss: 0.005734928417950869\n",
            "step: 120, loss: 0.0027009393088519573\n",
            "step: 130, loss: 0.10319715738296509\n",
            "step: 140, loss: 0.0002029176102951169\n",
            "step: 150, loss: 0.0001929739082697779\n",
            "step: 160, loss: 0.0023068315349519253\n",
            "step: 170, loss: 0.013773113489151001\n",
            "step: 180, loss: 8.836296910885721e-05\n",
            "step: 190, loss: 0.0004001839552074671\n",
            "step: 200, loss: 0.010436399839818478\n",
            "step: 210, loss: 0.0009182948269881308\n",
            "step: 220, loss: 0.001189122791402042\n",
            "step: 230, loss: 0.00986037589609623\n",
            "step: 240, loss: 0.0024363186676055193\n",
            "step: 250, loss: 0.001500299316830933\n",
            "step: 260, loss: 0.06249920278787613\n",
            "step: 270, loss: 0.00030675798188894987\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 280, loss: 0.0009765855502337217\n",
            "step: 290, loss: 0.03340183570981026\n",
            "step: 300, loss: 0.00019416007853578776\n",
            "step: 310, loss: 0.006105377338826656\n",
            "step: 320, loss: 0.0004488098784349859\n",
            "step: 330, loss: 0.00019891810370609164\n",
            "step: 340, loss: 0.013973968103528023\n",
            "step: 350, loss: 0.005790213588625193\n",
            "step: 360, loss: 0.0010698933620005846\n",
            "step: 370, loss: 0.0004031872085761279\n",
            "step: 380, loss: 0.0005822023958899081\n",
            "step: 390, loss: 0.0002632205723784864\n",
            "step: 400, loss: 0.03889031335711479\n",
            "step: 410, loss: 0.000251934805419296\n",
            "step: 420, loss: 0.00042921132990159094\n",
            "step: 430, loss: 0.0001331416133325547\n",
            "step: 440, loss: 0.0018485180335119367\n",
            "step: 450, loss: 0.0002856035716831684\n",
            "step: 460, loss: 0.013289774768054485\n",
            "step: 470, loss: 0.00011462329712230712\n",
            "step: 480, loss: 6.261152884690091e-05\n",
            "step: 490, loss: 0.002180794719606638\n",
            "step: 500, loss: 0.004921550862491131\n",
            "step: 510, loss: 0.002492288127541542\n",
            "step: 520, loss: 0.009473070502281189\n",
            "step: 530, loss: 8.62525703269057e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9185882352941176, f1=0.9258572752548655, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.074679160723463e-05\n",
            "step: 10, loss: 0.00025707384338602424\n",
            "step: 20, loss: 7.938336784718558e-05\n",
            "step: 30, loss: 0.000472605403047055\n",
            "step: 40, loss: 0.019649406895041466\n",
            "step: 50, loss: 0.00010922344517894089\n",
            "step: 60, loss: 9.829731425270438e-05\n",
            "step: 70, loss: 8.330936543643475e-05\n",
            "step: 80, loss: 0.0011018087388947606\n",
            "step: 90, loss: 0.0011071563931182027\n",
            "step: 100, loss: 0.00011383829405531287\n",
            "step: 110, loss: 0.0001703844463918358\n",
            "step: 120, loss: 0.0004142624093219638\n",
            "step: 130, loss: 0.00875110737979412\n",
            "step: 140, loss: 0.0071732401847839355\n",
            "step: 150, loss: 0.00010226992162643\n",
            "step: 160, loss: 8.383275417145342e-05\n",
            "step: 170, loss: 0.00271188304759562\n",
            "step: 180, loss: 0.023488689213991165\n",
            "step: 190, loss: 0.003158877370879054\n",
            "step: 200, loss: 0.03517809510231018\n",
            "step: 210, loss: 0.0007349832449108362\n",
            "step: 220, loss: 0.004539196379482746\n",
            "step: 230, loss: 0.014304596930742264\n",
            "step: 240, loss: 0.005686427466571331\n",
            "step: 250, loss: 0.14663074910640717\n",
            "step: 260, loss: 0.007557201664894819\n",
            "step: 270, loss: 0.00902665127068758\n",
            "step: 280, loss: 0.0003810609341599047\n",
            "step: 290, loss: 0.0005192285170778632\n",
            "step: 300, loss: 0.00011492316116346046\n",
            "step: 310, loss: 0.00013249907351564616\n",
            "step: 320, loss: 0.0007875696755945683\n",
            "step: 330, loss: 0.0006643979577347636\n",
            "step: 340, loss: 0.1762540638446808\n",
            "step: 350, loss: 0.0012528970837593079\n",
            "step: 360, loss: 0.00013782858150079846\n",
            "step: 370, loss: 0.005995402578264475\n",
            "step: 380, loss: 0.031468529254198074\n",
            "step: 390, loss: 0.0042780605144798756\n",
            "step: 400, loss: 0.0028122905641794205\n",
            "step: 410, loss: 0.002429006388410926\n",
            "step: 420, loss: 0.012888840399682522\n",
            "step: 430, loss: 0.0016265141312032938\n",
            "step: 440, loss: 0.0021976574789732695\n",
            "step: 450, loss: 0.00019825242634397\n",
            "step: 460, loss: 0.0010133336763828993\n",
            "step: 470, loss: 0.00028561236103996634\n",
            "step: 480, loss: 0.004970952868461609\n",
            "step: 490, loss: 0.0004987919237464666\n",
            "step: 500, loss: 0.02453945204615593\n",
            "step: 510, loss: 0.0008200597949326038\n",
            "step: 520, loss: 0.0018168180249631405\n",
            "step: 530, loss: 0.0007548542926087976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9204440333024977, f1=0.9249771271729187, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019902453641407192\n",
            "step: 10, loss: 0.00022574390459340066\n",
            "step: 20, loss: 8.001309470273554e-05\n",
            "step: 30, loss: 0.0002656195720192045\n",
            "step: 40, loss: 0.00012115306162741035\n",
            "step: 50, loss: 0.0015586789231747389\n",
            "step: 60, loss: 0.0012189324479550123\n",
            "step: 70, loss: 0.0008638356812298298\n",
            "step: 80, loss: 0.00018014964007306844\n",
            "step: 90, loss: 5.986296309856698e-05\n",
            "step: 100, loss: 0.007591857109218836\n",
            "step: 110, loss: 0.0010744680184870958\n",
            "step: 120, loss: 0.0030493815429508686\n",
            "step: 130, loss: 0.0012174805160611868\n",
            "step: 140, loss: 0.008169568143785\n",
            "step: 150, loss: 0.00016228447202593088\n",
            "step: 160, loss: 0.00020286475773900747\n",
            "step: 170, loss: 0.00011223067849641666\n",
            "step: 180, loss: 0.0923956111073494\n",
            "step: 190, loss: 0.006320939864963293\n",
            "step: 200, loss: 0.013907834887504578\n",
            "step: 210, loss: 0.014747225679457188\n",
            "step: 220, loss: 0.0011747288517653942\n",
            "step: 230, loss: 0.009381107985973358\n",
            "step: 240, loss: 0.0005618251161649823\n",
            "step: 250, loss: 0.00021443409787025303\n",
            "step: 260, loss: 5.605124169960618e-05\n",
            "step: 270, loss: 0.004706528969109058\n",
            "step: 280, loss: 0.00015732347674202174\n",
            "step: 290, loss: 0.00034953447175212204\n",
            "step: 300, loss: 0.0005823380197398365\n",
            "step: 310, loss: 0.027177808806300163\n",
            "step: 320, loss: 0.000152356835315004\n",
            "step: 330, loss: 0.0009752095793373883\n",
            "step: 340, loss: 0.0018902519950643182\n",
            "step: 350, loss: 0.0001544550177641213\n",
            "step: 360, loss: 0.0005371569423004985\n",
            "step: 370, loss: 0.0013362576719373465\n",
            "step: 380, loss: 0.0004904234665445983\n",
            "step: 390, loss: 0.00011896469368366525\n",
            "step: 400, loss: 0.0002370600268477574\n",
            "step: 410, loss: 0.020145233720541\n",
            "step: 420, loss: 0.00022671328042633832\n",
            "step: 430, loss: 0.00020566358580254018\n",
            "step: 440, loss: 0.015009026043117046\n",
            "step: 450, loss: 0.00029859310598112643\n",
            "step: 460, loss: 0.028303343802690506\n",
            "step: 470, loss: 0.00018740029190666974\n",
            "step: 480, loss: 0.002475072629749775\n",
            "step: 490, loss: 0.00046673277392983437\n",
            "step: 500, loss: 0.00027461082208901644\n",
            "step: 510, loss: 0.0005896096699871123\n",
            "step: 520, loss: 0.00020549212058540434\n",
            "step: 530, loss: 0.00017228422802872956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9170506912442397, f1=0.9208237986270023, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035702092573046684\n",
            "step: 10, loss: 7.762904715491459e-05\n",
            "step: 20, loss: 0.0012714850017800927\n",
            "step: 30, loss: 0.0012073608813807368\n",
            "step: 40, loss: 0.0004160538665018976\n",
            "step: 50, loss: 0.014720690436661243\n",
            "step: 60, loss: 0.01051345281302929\n",
            "step: 70, loss: 0.0007411204860545695\n",
            "step: 80, loss: 0.001558296149596572\n",
            "step: 90, loss: 0.13285687565803528\n",
            "step: 100, loss: 0.0017570850905030966\n",
            "step: 110, loss: 0.00552802998572588\n",
            "step: 120, loss: 8.450917812297121e-05\n",
            "step: 130, loss: 0.0001196876764879562\n",
            "step: 140, loss: 0.11428089439868927\n",
            "step: 150, loss: 0.00011997255205642432\n",
            "step: 160, loss: 0.0002240406902274117\n",
            "step: 170, loss: 0.00011860050290124491\n",
            "step: 180, loss: 0.0004945110413245857\n",
            "step: 190, loss: 0.0024173157289624214\n",
            "step: 200, loss: 0.0008156162803061306\n",
            "step: 210, loss: 0.02740553952753544\n",
            "step: 220, loss: 0.00013850731193087995\n",
            "step: 230, loss: 0.0001365634088870138\n",
            "step: 240, loss: 0.00031828947248868644\n",
            "step: 250, loss: 0.0008115139207802713\n",
            "step: 260, loss: 0.0003638619964476675\n",
            "step: 270, loss: 8.770491695031524e-05\n",
            "step: 280, loss: 0.0010467958636581898\n",
            "step: 290, loss: 0.006542374845594168\n",
            "step: 300, loss: 0.00014233410183805972\n",
            "step: 310, loss: 0.0012258798815310001\n",
            "step: 320, loss: 0.00014734367141500115\n",
            "step: 330, loss: 0.0001878467737697065\n",
            "step: 340, loss: 0.0004868896212428808\n",
            "step: 350, loss: 0.0021698083728551865\n",
            "step: 360, loss: 0.004899096209555864\n",
            "step: 370, loss: 0.00029409147100523114\n",
            "step: 380, loss: 0.0006644375971518457\n",
            "step: 390, loss: 0.0008773928275331855\n",
            "step: 400, loss: 0.006665350869297981\n",
            "step: 410, loss: 0.0021141923498362303\n",
            "step: 420, loss: 0.004373802803456783\n",
            "step: 430, loss: 0.0027133708354085684\n",
            "step: 440, loss: 0.042404089123010635\n",
            "step: 450, loss: 0.0011881670216098428\n",
            "step: 460, loss: 0.02181342802941799\n",
            "step: 470, loss: 0.00015229660493787378\n",
            "step: 480, loss: 0.012250143103301525\n",
            "step: 490, loss: 0.005198618862777948\n",
            "step: 500, loss: 0.006084786728024483\n",
            "step: 510, loss: 0.01095537655055523\n",
            "step: 520, loss: 0.0002255066647194326\n",
            "step: 530, loss: 0.024790242314338684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.920796665122742, f1=0.925756186984418, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011923894053325057\n",
            "step: 10, loss: 9.357493399875239e-05\n",
            "step: 20, loss: 0.004368244204670191\n",
            "step: 30, loss: 0.006468669045716524\n",
            "step: 40, loss: 0.0001008523249765858\n",
            "step: 50, loss: 0.0021011284552514553\n",
            "step: 60, loss: 0.00020511132606770843\n",
            "step: 70, loss: 0.0010261539136990905\n",
            "step: 80, loss: 0.00030074568348936737\n",
            "step: 90, loss: 0.008163295686244965\n",
            "step: 100, loss: 0.0008296962478198111\n",
            "step: 110, loss: 0.0011118859983980656\n",
            "step: 120, loss: 0.0033047304023057222\n",
            "step: 130, loss: 6.444181053666398e-05\n",
            "step: 140, loss: 4.241010174155235e-05\n",
            "step: 150, loss: 0.0014168473426252604\n",
            "step: 160, loss: 6.636572652496397e-05\n",
            "step: 170, loss: 0.0001347253128187731\n",
            "step: 180, loss: 0.00011373758752597496\n",
            "step: 190, loss: 0.0010269561316817999\n",
            "step: 200, loss: 0.0013014592695981264\n",
            "step: 210, loss: 0.00026274501578882337\n",
            "step: 220, loss: 0.002895804587751627\n",
            "step: 230, loss: 0.00015526151400990784\n",
            "step: 240, loss: 0.0010585533455014229\n",
            "step: 250, loss: 0.0005322841461747885\n",
            "step: 260, loss: 0.0011385775869712234\n",
            "step: 270, loss: 0.012893025763332844\n",
            "step: 280, loss: 5.023988342145458e-05\n",
            "step: 290, loss: 0.010342186316847801\n",
            "step: 300, loss: 0.0006417605327442288\n",
            "step: 310, loss: 0.0009835645323619246\n",
            "step: 320, loss: 0.0008149456116370857\n",
            "step: 330, loss: 0.0006620942731387913\n",
            "step: 340, loss: 0.0004264703020453453\n",
            "step: 350, loss: 0.004954120144248009\n",
            "step: 360, loss: 0.00011036200157832354\n",
            "step: 370, loss: 8.282621274702251e-05\n",
            "step: 380, loss: 0.00011239058221690357\n",
            "step: 390, loss: 0.002541170222684741\n",
            "step: 400, loss: 0.0007657908136025071\n",
            "step: 410, loss: 0.0038978192023932934\n",
            "step: 420, loss: 9.451190271647647e-05\n",
            "step: 430, loss: 0.01953800581395626\n",
            "step: 440, loss: 5.956301902187988e-05\n",
            "step: 450, loss: 0.00010782812023535371\n",
            "step: 460, loss: 0.0010219912510365248\n",
            "step: 470, loss: 0.0003292569308541715\n",
            "step: 480, loss: 0.0004012601566500962\n",
            "step: 490, loss: 0.0010376936988905072\n",
            "step: 500, loss: 0.00024576764553785324\n",
            "step: 510, loss: 0.000880913226865232\n",
            "step: 520, loss: 0.00435156375169754\n",
            "step: 530, loss: 9.089234663406387e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9191489361702128, f1=0.9203373945641987, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033970444928854704\n",
            "step: 10, loss: 7.183759589679539e-05\n",
            "step: 20, loss: 0.0005866995197720826\n",
            "step: 30, loss: 5.522717037820257e-05\n",
            "step: 40, loss: 0.0004769757797475904\n",
            "step: 50, loss: 0.0035564859863370657\n",
            "step: 60, loss: 2.6127903765882365e-05\n",
            "step: 70, loss: 0.0022275669034570456\n",
            "step: 80, loss: 2.061909981421195e-05\n",
            "step: 90, loss: 0.000448661798145622\n",
            "step: 100, loss: 0.0006038465071469545\n",
            "step: 110, loss: 7.306971383513883e-05\n",
            "step: 120, loss: 6.006751937093213e-05\n",
            "step: 130, loss: 0.004100047051906586\n",
            "step: 140, loss: 0.0003238815988879651\n",
            "step: 150, loss: 6.813877553213388e-05\n",
            "step: 160, loss: 0.0002923952997662127\n",
            "step: 170, loss: 0.0010318078566342592\n",
            "step: 180, loss: 7.972581806825474e-05\n",
            "step: 190, loss: 0.00023992233036551625\n",
            "step: 200, loss: 0.0001275994727620855\n",
            "step: 210, loss: 0.0005093562067486346\n",
            "step: 220, loss: 0.00023561489069834352\n",
            "step: 230, loss: 0.002370659727603197\n",
            "step: 240, loss: 0.0006922188331373036\n",
            "step: 250, loss: 0.00024199405743274838\n",
            "step: 260, loss: 0.005184466019272804\n",
            "step: 270, loss: 0.00013202163972891867\n",
            "step: 280, loss: 7.779520092299208e-05\n",
            "step: 290, loss: 0.0016236023511737585\n",
            "step: 300, loss: 0.00048818462528288364\n",
            "step: 310, loss: 0.005723448004573584\n",
            "step: 320, loss: 0.0004313979879952967\n",
            "step: 330, loss: 7.919744530227035e-05\n",
            "step: 340, loss: 0.00017782991926651448\n",
            "step: 350, loss: 0.0002295806771144271\n",
            "step: 360, loss: 0.01210180763155222\n",
            "step: 370, loss: 6.89472581143491e-05\n",
            "step: 380, loss: 0.00028211172320879996\n",
            "step: 390, loss: 0.11877045780420303\n",
            "step: 400, loss: 0.0005114839877933264\n",
            "step: 410, loss: 0.00010491681314306334\n",
            "step: 420, loss: 0.0003252976748626679\n",
            "step: 430, loss: 0.0005071343621239066\n",
            "step: 440, loss: 0.0013250703923404217\n",
            "step: 450, loss: 0.005569308064877987\n",
            "step: 460, loss: 0.0007125437259674072\n",
            "step: 470, loss: 0.00831100344657898\n",
            "step: 480, loss: 0.0006342613487504423\n",
            "step: 490, loss: 0.0001125643975683488\n",
            "step: 500, loss: 0.0004426889354363084\n",
            "step: 510, loss: 0.06589341163635254\n",
            "step: 520, loss: 0.0006362995482049882\n",
            "step: 530, loss: 0.005678115412592888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9212962962962964, f1=0.9221611721611722, best_f1=0.9174646602827177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.671069372212514e-05\n",
            "step: 10, loss: 0.0004629641189239919\n",
            "step: 20, loss: 0.00019593455363065004\n",
            "step: 30, loss: 0.0007766030030325055\n",
            "step: 40, loss: 0.09884587675333023\n",
            "step: 50, loss: 0.00016452015552204102\n",
            "step: 60, loss: 4.6022749302210286e-05\n",
            "step: 70, loss: 0.000324743683449924\n",
            "step: 80, loss: 0.00018583814380690455\n",
            "step: 90, loss: 6.411317735910416e-05\n",
            "step: 100, loss: 0.0002910315524786711\n",
            "step: 110, loss: 0.00036470999475568533\n",
            "step: 120, loss: 0.0012887684861198068\n",
            "step: 130, loss: 0.011371297761797905\n",
            "step: 140, loss: 0.010496111586689949\n",
            "step: 150, loss: 7.922924851300195e-05\n",
            "step: 160, loss: 0.00019189031445421278\n",
            "step: 170, loss: 0.0005839613731950521\n",
            "step: 180, loss: 0.011476105079054832\n",
            "step: 190, loss: 0.00013994067558087409\n",
            "step: 200, loss: 0.000385129387723282\n",
            "step: 210, loss: 0.0002393887407379225\n",
            "step: 220, loss: 0.00012478831922635436\n",
            "step: 230, loss: 0.000295048434054479\n",
            "step: 240, loss: 0.0003802075225394219\n",
            "step: 250, loss: 8.216577407438308e-05\n",
            "step: 260, loss: 7.59199247113429e-05\n",
            "step: 270, loss: 3.0348477594088763e-05\n",
            "step: 280, loss: 4.5217020669952035e-05\n",
            "step: 290, loss: 3.169731644447893e-05\n",
            "step: 300, loss: 5.29974713572301e-05\n",
            "step: 310, loss: 0.001330302911810577\n",
            "step: 320, loss: 0.00026951791369356215\n",
            "step: 330, loss: 7.983206160133705e-05\n",
            "step: 340, loss: 0.000162066615303047\n",
            "step: 350, loss: 0.00010463589569553733\n",
            "step: 360, loss: 0.0005968224722892046\n",
            "step: 370, loss: 0.0013611163012683392\n",
            "step: 380, loss: 0.00021938139980193228\n",
            "step: 390, loss: 0.0006468724459409714\n",
            "step: 400, loss: 5.7332021242473274e-05\n",
            "step: 410, loss: 0.000546337862033397\n",
            "step: 420, loss: 0.00028231466421857476\n",
            "step: 430, loss: 0.00010717361874412745\n",
            "step: 440, loss: 0.13805480301380157\n",
            "step: 450, loss: 5.29938806721475e-05\n",
            "step: 460, loss: 5.9068195696454495e-05\n",
            "step: 470, loss: 0.0011957735987380147\n",
            "step: 480, loss: 4.976085983798839e-05\n",
            "step: 490, loss: 0.1155642569065094\n",
            "step: 500, loss: 7.337704300880432e-05\n",
            "step: 510, loss: 0.003429833799600601\n",
            "step: 520, loss: 3.540758916642517e-05\n",
            "step: 530, loss: 0.00013174532796256244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9216241737488197, f1=0.9263746505125815, best_f1=0.9174646602827177\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 232.17it/s]\n",
            "load_f1 = 0.9234382339126351\n",
            "real_f1 = 0.9217065166432256\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 240.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6482d906-c37d-4a15-a328-c46e5a0e6c30"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5754514336585999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.41666666666666663, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4815097451210022\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5789473684210527, f1=0.4090909090909091, best_f1=0.4090909090909091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4098399877548218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.5365853658536585, f1=0.43902439024390244, best_f1=0.4090909090909091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3025938868522644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5945945945945946, f1=0.43902439024390244, best_f1=0.43902439024390244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21911458671092987\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5714285714285714, f1=0.56, best_f1=0.43902439024390244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38353821635246277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.689655172413793, f1=0.64, best_f1=0.64\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11779070645570755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6486486486486486, f1=0.43902439024390244, best_f1=0.64\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17404553294181824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6666666666666666, f1=0.5714285714285714, best_f1=0.64\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0044101811945438385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.75, f1=0.5806451612903226, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024201102554798126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.75, f1=0.6206896551724138, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006945349276065826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7272727272727273, f1=0.6470588235294117, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009030059911310673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7272727272727273, f1=0.6470588235294117, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016922518610954285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7096774193548386, f1=0.6666666666666666, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011369781568646431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7096774193548386, f1=0.6666666666666666, best_f1=0.5806451612903226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022354116663336754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7096774193548386, f1=0.6666666666666666, best_f1=0.5806451612903226\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 92528.89it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7096774193548386\n",
            "real_f1 = 0.75\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 270.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cdf8f1-01f0-46ed-8a67-606df14e6bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6129496693611145\n",
            "step: 10, loss: 0.6375404000282288\n",
            "step: 20, loss: 0.4277620017528534\n",
            "step: 30, loss: 0.28810226917266846\n",
            "step: 40, loss: 0.26520270109176636\n",
            "step: 50, loss: 0.09274791926145554\n",
            "step: 60, loss: 0.05461683124303818\n",
            "step: 70, loss: 0.06618402898311615\n",
            "step: 80, loss: 0.06476642936468124\n",
            "step: 90, loss: 0.07131904363632202\n",
            "step: 100, loss: 0.017403025180101395\n",
            "step: 110, loss: 0.07739272713661194\n",
            "step: 120, loss: 0.039883796125650406\n",
            "step: 130, loss: 0.0036890546325594187\n",
            "step: 140, loss: 0.003138255560770631\n",
            "step: 150, loss: 0.10137316584587097\n",
            "step: 160, loss: 0.021350713446736336\n",
            "step: 170, loss: 0.04792998731136322\n",
            "step: 180, loss: 0.07295144349336624\n",
            "step: 190, loss: 0.002142801182344556\n",
            "step: 200, loss: 0.006815146189182997\n",
            "step: 210, loss: 0.006028422154486179\n",
            "step: 220, loss: 0.04361306503415108\n",
            "step: 230, loss: 0.0431070551276207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9694915254237287, f1=0.9580973952434882, best_f1=0.9580973952434882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021361857652664185\n",
            "step: 10, loss: 0.0022264611907303333\n",
            "step: 20, loss: 0.10712692141532898\n",
            "step: 30, loss: 0.19790019094944\n",
            "step: 40, loss: 0.023965388536453247\n",
            "step: 50, loss: 0.0045559825375676155\n",
            "step: 60, loss: 0.0013913246802985668\n",
            "step: 70, loss: 0.22097283601760864\n",
            "step: 80, loss: 0.038217995315790176\n",
            "step: 90, loss: 0.07921551167964935\n",
            "step: 100, loss: 0.042564719915390015\n",
            "step: 110, loss: 0.08459364622831345\n",
            "step: 120, loss: 0.044662538915872574\n",
            "step: 130, loss: 0.0254659540951252\n",
            "step: 140, loss: 0.04900424927473068\n",
            "step: 150, loss: 0.017383446916937828\n",
            "step: 160, loss: 0.04134754464030266\n",
            "step: 170, loss: 0.009442275390028954\n",
            "step: 180, loss: 0.004709662403911352\n",
            "step: 190, loss: 0.07769103348255157\n",
            "step: 200, loss: 0.0033877568785101175\n",
            "step: 210, loss: 0.0039790901355445385\n",
            "step: 220, loss: 0.009939252398908138\n",
            "step: 230, loss: 0.01182866282761097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9764309764309763, f1=0.9653631284916202, best_f1=0.9653631284916202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0339583158493042\n",
            "step: 10, loss: 0.0034414336550980806\n",
            "step: 20, loss: 0.0012856874382123351\n",
            "step: 30, loss: 0.05162648484110832\n",
            "step: 40, loss: 0.10526429861783981\n",
            "step: 50, loss: 0.00803305208683014\n",
            "step: 60, loss: 0.0021752063184976578\n",
            "step: 70, loss: 0.008346067741513252\n",
            "step: 80, loss: 0.0006439880817197263\n",
            "step: 90, loss: 0.09038043767213821\n",
            "step: 100, loss: 0.0030139253940433264\n",
            "step: 110, loss: 0.0007677703979425132\n",
            "step: 120, loss: 0.008278748020529747\n",
            "step: 130, loss: 0.03155673295259476\n",
            "step: 140, loss: 0.012666665017604828\n",
            "step: 150, loss: 0.0012226506369188428\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.017286764457821846\n",
            "step: 170, loss: 0.0032086381688714027\n",
            "step: 180, loss: 0.09604720026254654\n",
            "step: 190, loss: 0.005430400837212801\n",
            "step: 200, loss: 0.01985541358590126\n",
            "step: 210, loss: 0.003864741651341319\n",
            "step: 220, loss: 0.0010857549495995045\n",
            "step: 230, loss: 0.0007396753062494099\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9910112359550561, f1=0.9743016759776536, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015744275879114866\n",
            "step: 10, loss: 0.0006542433402501047\n",
            "step: 20, loss: 0.0007484295638278127\n",
            "step: 30, loss: 0.001127365743741393\n",
            "step: 40, loss: 0.009473512880504131\n",
            "step: 50, loss: 0.021080520004034042\n",
            "step: 60, loss: 0.0010970542207360268\n",
            "step: 70, loss: 0.0011028676526620984\n",
            "step: 80, loss: 0.0007992906612344086\n",
            "step: 90, loss: 0.0012987249065190554\n",
            "step: 100, loss: 0.0005873513291589916\n",
            "step: 110, loss: 0.0002498249523341656\n",
            "step: 120, loss: 0.06147861108183861\n",
            "step: 130, loss: 0.006372969131916761\n",
            "step: 140, loss: 0.0004138632502872497\n",
            "step: 150, loss: 0.15383322536945343\n",
            "step: 160, loss: 0.0009255342301912606\n",
            "step: 170, loss: 0.0025441087782382965\n",
            "step: 180, loss: 0.000403142417781055\n",
            "step: 190, loss: 0.00041228061309084296\n",
            "step: 200, loss: 0.0009020318975672126\n",
            "step: 210, loss: 0.06681688129901886\n",
            "step: 220, loss: 0.00014427992573473603\n",
            "step: 230, loss: 0.003459481755271554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9818181818181818, f1=0.9830124575311437, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013353998074308038\n",
            "step: 10, loss: 0.00044223887380212545\n",
            "step: 20, loss: 0.0003624427190516144\n",
            "step: 30, loss: 8.851233724271879e-05\n",
            "step: 40, loss: 0.0004478997434489429\n",
            "step: 50, loss: 0.00035274887341074646\n",
            "step: 60, loss: 0.0017034201882779598\n",
            "step: 70, loss: 0.00018936069682240486\n",
            "step: 80, loss: 0.00017381134966854006\n",
            "step: 90, loss: 0.0002782318042591214\n",
            "step: 100, loss: 0.0005703953793272376\n",
            "step: 110, loss: 0.003589578904211521\n",
            "step: 120, loss: 0.00012165392399765551\n",
            "step: 130, loss: 0.00391353340819478\n",
            "step: 140, loss: 0.013741250149905682\n",
            "step: 150, loss: 0.0027225418016314507\n",
            "step: 160, loss: 0.0020522032864391804\n",
            "step: 170, loss: 0.003186500631272793\n",
            "step: 180, loss: 0.005268519744277\n",
            "step: 190, loss: 0.004702058155089617\n",
            "step: 200, loss: 0.0013064551167190075\n",
            "step: 210, loss: 0.0002241163165308535\n",
            "step: 220, loss: 0.0003727532457560301\n",
            "step: 230, loss: 0.00015070743393152952\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9887387387387387, f1=0.9753363228699552, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005849949666298926\n",
            "step: 10, loss: 0.00023262637841980904\n",
            "step: 20, loss: 0.000892729323823005\n",
            "step: 30, loss: 0.0003658281930256635\n",
            "step: 40, loss: 0.00033886998426169157\n",
            "step: 50, loss: 0.00017927755834534764\n",
            "step: 60, loss: 9.711753955343738e-05\n",
            "step: 70, loss: 0.009340344928205013\n",
            "step: 80, loss: 0.00084225571481511\n",
            "step: 90, loss: 0.0006317587685771286\n",
            "step: 100, loss: 0.022898074239492416\n",
            "step: 110, loss: 0.0006622656947001815\n",
            "step: 120, loss: 0.0005280671757645905\n",
            "step: 130, loss: 0.0013962909579277039\n",
            "step: 140, loss: 0.00017539468535687774\n",
            "step: 150, loss: 0.006595706567168236\n",
            "step: 160, loss: 0.008880693465471268\n",
            "step: 170, loss: 0.0028491169214248657\n",
            "step: 180, loss: 0.00197820202447474\n",
            "step: 190, loss: 0.005365481600165367\n",
            "step: 200, loss: 0.00862516462802887\n",
            "step: 210, loss: 0.0014486489817500114\n",
            "step: 220, loss: 0.0025745683815330267\n",
            "step: 230, loss: 0.03561199828982353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9819819819819819, f1=0.9797752808988766, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00236222380772233\n",
            "step: 10, loss: 0.00021380270482040942\n",
            "step: 20, loss: 0.00042798288632184267\n",
            "step: 30, loss: 0.00016401908942498267\n",
            "step: 40, loss: 0.0012161097256466746\n",
            "step: 50, loss: 0.0003731815377250314\n",
            "step: 60, loss: 0.007742900401353836\n",
            "step: 70, loss: 0.0002601500600576401\n",
            "step: 80, loss: 0.0002714334987103939\n",
            "step: 90, loss: 9.254673204850405e-05\n",
            "step: 100, loss: 0.0001730183867039159\n",
            "step: 110, loss: 0.00020457126083783805\n",
            "step: 120, loss: 5.937298192293383e-05\n",
            "step: 130, loss: 0.00019056166638620198\n",
            "step: 140, loss: 0.00020829844288527966\n",
            "step: 150, loss: 0.0005826108390465379\n",
            "step: 160, loss: 0.04037719964981079\n",
            "step: 170, loss: 0.005856754258275032\n",
            "step: 180, loss: 0.001398024964146316\n",
            "step: 190, loss: 0.0001268013147637248\n",
            "step: 200, loss: 0.023443050682544708\n",
            "step: 210, loss: 7.288032793439925e-05\n",
            "step: 220, loss: 0.10108956694602966\n",
            "step: 230, loss: 0.03804461285471916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9798206278026906, f1=0.9775280898876404, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006884881877340376\n",
            "step: 10, loss: 0.002170105930417776\n",
            "step: 20, loss: 0.007650247309356928\n",
            "step: 30, loss: 0.006039801519364119\n",
            "step: 40, loss: 0.0017760028131306171\n",
            "step: 50, loss: 0.0010803454788401723\n",
            "step: 60, loss: 0.0013416099827736616\n",
            "step: 70, loss: 0.00034157244954258204\n",
            "step: 80, loss: 0.0086625711992383\n",
            "step: 90, loss: 0.0021608974784612656\n",
            "step: 100, loss: 0.0013418191811069846\n",
            "step: 110, loss: 0.0008643389446660876\n",
            "step: 120, loss: 0.0013577472418546677\n",
            "step: 130, loss: 0.0007511871517635882\n",
            "step: 140, loss: 0.00011356112372595817\n",
            "step: 150, loss: 9.290916204918176e-05\n",
            "step: 160, loss: 0.00029169811750762165\n",
            "step: 170, loss: 7.637299131602049e-05\n",
            "step: 180, loss: 0.00011685190111165866\n",
            "step: 190, loss: 0.00014670102973468602\n",
            "step: 200, loss: 6.965562351979315e-05\n",
            "step: 210, loss: 0.00013156404020264745\n",
            "step: 220, loss: 0.00013710437633562833\n",
            "step: 230, loss: 0.00011206539056729525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.983050847457627, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020450558804441243\n",
            "step: 10, loss: 0.00010390285751782358\n",
            "step: 20, loss: 0.00025301415007561445\n",
            "step: 30, loss: 7.194047066150233e-05\n",
            "step: 40, loss: 0.0001999959204113111\n",
            "step: 50, loss: 4.6649300202261657e-05\n",
            "step: 60, loss: 0.0010205814614892006\n",
            "step: 70, loss: 0.00013559315993916243\n",
            "step: 80, loss: 0.00011777831241488457\n",
            "step: 90, loss: 0.00017869099974632263\n",
            "step: 100, loss: 0.00015417550457641482\n",
            "step: 110, loss: 4.2928295442834496e-05\n",
            "step: 120, loss: 0.0027995642740279436\n",
            "step: 130, loss: 0.00015264379908330739\n",
            "step: 140, loss: 5.717584281228483e-05\n",
            "step: 150, loss: 0.0005587390623986721\n",
            "step: 160, loss: 5.857647192897275e-05\n",
            "step: 170, loss: 0.00016440835315734148\n",
            "step: 180, loss: 0.0015158579917624593\n",
            "step: 190, loss: 9.759235399542376e-05\n",
            "step: 200, loss: 7.068775448715314e-05\n",
            "step: 210, loss: 0.0007479332271032035\n",
            "step: 220, loss: 0.0003923253680113703\n",
            "step: 230, loss: 0.00022834203264210373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9831271091113611, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021070205548312515\n",
            "step: 10, loss: 3.9925424061948434e-05\n",
            "step: 20, loss: 0.00023556704400107265\n",
            "step: 30, loss: 0.00017521395056974143\n",
            "step: 40, loss: 6.723563274135813e-05\n",
            "step: 50, loss: 5.4234504204941913e-05\n",
            "step: 60, loss: 0.00011758752953028306\n",
            "step: 70, loss: 0.00310001103207469\n",
            "step: 80, loss: 6.148249667603523e-05\n",
            "step: 90, loss: 0.00036010792246088386\n",
            "step: 100, loss: 8.037850784603506e-05\n",
            "step: 110, loss: 7.045917300274596e-05\n",
            "step: 120, loss: 0.000709695159457624\n",
            "step: 130, loss: 5.85975285503082e-05\n",
            "step: 140, loss: 0.00011066385923186317\n",
            "step: 150, loss: 0.0028779353015124798\n",
            "step: 160, loss: 4.0395709220319986e-05\n",
            "step: 170, loss: 5.583345046034083e-05\n",
            "step: 180, loss: 9.536058496451005e-05\n",
            "step: 190, loss: 0.00022295815870165825\n",
            "step: 200, loss: 8.015503408387303e-05\n",
            "step: 210, loss: 0.16722464561462402\n",
            "step: 220, loss: 0.000369061017408967\n",
            "step: 230, loss: 0.0014008956495672464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9808773903262092, f1=0.9807909604519773, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.076155791059136e-05\n",
            "step: 10, loss: 0.0002787252014968544\n",
            "step: 20, loss: 8.160428114933893e-05\n",
            "step: 30, loss: 0.0015460134018212557\n",
            "step: 40, loss: 9.91354972939007e-05\n",
            "step: 50, loss: 7.701127469772473e-05\n",
            "step: 60, loss: 0.00010074477904709056\n",
            "step: 70, loss: 0.00028279656544327736\n",
            "step: 80, loss: 4.890447235084139e-05\n",
            "step: 90, loss: 7.181249384302646e-05\n",
            "step: 100, loss: 7.620918768225238e-05\n",
            "step: 110, loss: 0.0002261526242364198\n",
            "step: 120, loss: 4.57028036180418e-05\n",
            "step: 130, loss: 4.237044777255505e-05\n",
            "step: 140, loss: 0.0002225003991043195\n",
            "step: 150, loss: 0.00011933943460462615\n",
            "step: 160, loss: 6.500444578705356e-05\n",
            "step: 170, loss: 6.317361112451181e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 180, loss: 8.949101902544498e-05\n",
            "step: 190, loss: 4.248804543749429e-05\n",
            "step: 200, loss: 9.17277138796635e-05\n",
            "step: 210, loss: 4.386251384858042e-05\n",
            "step: 220, loss: 5.816786506329663e-05\n",
            "step: 230, loss: 8.30954231787473e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9820627802690582, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.6451936213998124e-05\n",
            "step: 10, loss: 4.47759339294862e-05\n",
            "step: 20, loss: 7.035469025140628e-05\n",
            "step: 30, loss: 5.443822738016024e-05\n",
            "step: 40, loss: 0.0024336192291229963\n",
            "step: 50, loss: 9.117367153521627e-05\n",
            "step: 60, loss: 9.05894412426278e-05\n",
            "step: 70, loss: 0.00010771643428597599\n",
            "step: 80, loss: 5.9257068642182276e-05\n",
            "step: 90, loss: 3.986190495197661e-05\n",
            "step: 100, loss: 4.2270934500265867e-05\n",
            "step: 110, loss: 4.33935783803463e-05\n",
            "step: 120, loss: 7.582946273032576e-05\n",
            "step: 130, loss: 8.4918639913667e-05\n",
            "step: 140, loss: 7.657882088096812e-05\n",
            "step: 150, loss: 6.699138612020761e-05\n",
            "step: 160, loss: 0.0015615207375958562\n",
            "step: 170, loss: 4.518541754805483e-05\n",
            "step: 180, loss: 0.0001225386222358793\n",
            "step: 190, loss: 4.00744829676114e-05\n",
            "step: 200, loss: 3.3350825106026605e-05\n",
            "step: 210, loss: 0.00010294596722815186\n",
            "step: 220, loss: 8.657936268718913e-05\n",
            "step: 230, loss: 0.03214907646179199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9842696629213483, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.754795322194695e-05\n",
            "step: 10, loss: 5.651185711030848e-05\n",
            "step: 20, loss: 0.00015075437841005623\n",
            "step: 30, loss: 8.224845805671066e-05\n",
            "step: 40, loss: 3.992931669927202e-05\n",
            "step: 50, loss: 5.065257937530987e-05\n",
            "step: 60, loss: 5.895052890991792e-05\n",
            "step: 70, loss: 3.467334317974746e-05\n",
            "step: 80, loss: 3.793583164224401e-05\n",
            "step: 90, loss: 4.131788227823563e-05\n",
            "step: 100, loss: 3.5063945688307285e-05\n",
            "step: 110, loss: 9.883807797450572e-05\n",
            "step: 120, loss: 0.00010381525498814881\n",
            "step: 130, loss: 4.993127367924899e-05\n",
            "step: 140, loss: 5.176833292352967e-05\n",
            "step: 150, loss: 3.73967232007999e-05\n",
            "step: 160, loss: 3.317206574138254e-05\n",
            "step: 170, loss: 7.318529969779775e-05\n",
            "step: 180, loss: 4.4190281187184155e-05\n",
            "step: 190, loss: 2.7447189495433122e-05\n",
            "step: 200, loss: 5.8454417739994824e-05\n",
            "step: 210, loss: 2.1665746317012236e-05\n",
            "step: 220, loss: 4.2148611100856215e-05\n",
            "step: 230, loss: 3.8084072002675384e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9830890642615557, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.158986484981142e-05\n",
            "step: 10, loss: 4.2920779378619045e-05\n",
            "step: 20, loss: 4.4844928197562695e-05\n",
            "step: 30, loss: 7.833864219719544e-05\n",
            "step: 40, loss: 9.334102651337162e-05\n",
            "step: 50, loss: 5.324737139744684e-05\n",
            "step: 60, loss: 4.281894143787213e-05\n",
            "step: 70, loss: 7.545825064880773e-05\n",
            "step: 80, loss: 3.7970276025589556e-05\n",
            "step: 90, loss: 0.00011406916019041091\n",
            "step: 100, loss: 0.00017818310880102217\n",
            "step: 110, loss: 3.0479235647362657e-05\n",
            "step: 120, loss: 2.136016519216355e-05\n",
            "step: 130, loss: 4.7204113798215985e-05\n",
            "step: 140, loss: 5.1577251724665985e-05\n",
            "step: 150, loss: 4.9599188059801236e-05\n",
            "step: 160, loss: 0.000397457362851128\n",
            "step: 170, loss: 1.7609256246942095e-05\n",
            "step: 180, loss: 5.149143180460669e-05\n",
            "step: 190, loss: 4.4900578359374776e-05\n",
            "step: 200, loss: 2.993891939695459e-05\n",
            "step: 210, loss: 3.953485065721907e-05\n",
            "step: 220, loss: 2.3412594600813463e-05\n",
            "step: 230, loss: 5.31346449861303e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9830890642615557, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010655359801603481\n",
            "step: 10, loss: 2.0998657419113442e-05\n",
            "step: 20, loss: 5.4887274018255994e-05\n",
            "step: 30, loss: 6.632118311244994e-05\n",
            "step: 40, loss: 5.0569586164783686e-05\n",
            "step: 50, loss: 6.0312428104225546e-05\n",
            "step: 60, loss: 7.021619967417791e-05\n",
            "step: 70, loss: 3.55296979250852e-05\n",
            "step: 80, loss: 0.0001382539776386693\n",
            "step: 90, loss: 4.87578145111911e-05\n",
            "step: 100, loss: 3.796192686422728e-05\n",
            "step: 110, loss: 3.5450735595077276e-05\n",
            "step: 120, loss: 5.4440737585537136e-05\n",
            "step: 130, loss: 9.0947920398321e-05\n",
            "step: 140, loss: 2.8087566533940844e-05\n",
            "step: 150, loss: 0.00010695866512833163\n",
            "step: 160, loss: 3.279158772784285e-05\n",
            "step: 170, loss: 2.9558677852037363e-05\n",
            "step: 180, loss: 7.773865945637226e-05\n",
            "step: 190, loss: 4.766130950883962e-05\n",
            "step: 200, loss: 3.126524461549707e-05\n",
            "step: 210, loss: 3.3492615330033004e-05\n",
            "step: 220, loss: 5.0559730880195275e-05\n",
            "step: 230, loss: 3.7917539884801954e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9830890642615557, f1=0.9819413092550789, best_f1=0.9743016759776536\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 219.47it/s]\n",
            "load_f1 = 0.9910112359550561\n",
            "real_f1 = 0.990990990990991\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 267.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb2110f-f710-4267-844a-a7fcc723ab1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6401656270027161\n",
            "step: 10, loss: 0.548403263092041\n",
            "step: 20, loss: 0.5578916668891907\n",
            "step: 30, loss: 0.2820023000240326\n",
            "step: 40, loss: 0.1763148009777069\n",
            "step: 50, loss: 0.2568945288658142\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.12546811997890472\n",
            "step: 70, loss: 0.21059413254261017\n",
            "step: 80, loss: 0.0928536057472229\n",
            "step: 90, loss: 0.41654831171035767\n",
            "step: 100, loss: 0.12325279414653778\n",
            "step: 110, loss: 0.19390611350536346\n",
            "step: 120, loss: 0.15825526416301727\n",
            "step: 130, loss: 0.051922086626291275\n",
            "step: 140, loss: 0.057387180626392365\n",
            "step: 150, loss: 0.07771595567464828\n",
            "step: 160, loss: 0.04156766086816788\n",
            "step: 170, loss: 0.22924701869487762\n",
            "step: 180, loss: 0.1157885417342186\n",
            "step: 190, loss: 0.011445362120866776\n",
            "step: 200, loss: 0.15461516380310059\n",
            "step: 210, loss: 0.09560216963291168\n",
            "step: 220, loss: 0.14395104348659515\n",
            "step: 230, loss: 0.1446991264820099\n",
            "step: 240, loss: 0.1212119534611702\n",
            "step: 250, loss: 0.045684050768613815\n",
            "step: 260, loss: 0.16396602988243103\n",
            "step: 270, loss: 0.02199603244662285\n",
            "step: 280, loss: 0.04839450120925903\n",
            "step: 290, loss: 0.22253724932670593\n",
            "step: 300, loss: 0.09284356236457825\n",
            "step: 310, loss: 0.19159989058971405\n",
            "step: 320, loss: 0.108310267329216\n",
            "step: 330, loss: 0.05126103386282921\n",
            "step: 340, loss: 0.1055629551410675\n",
            "step: 350, loss: 0.06981503218412399\n",
            "step: 360, loss: 0.09868897497653961\n",
            "step: 370, loss: 0.10287521779537201\n",
            "step: 380, loss: 0.048004016280174255\n",
            "step: 390, loss: 0.19773609936237335\n",
            "step: 400, loss: 0.23057270050048828\n",
            "step: 410, loss: 0.07325434684753418\n",
            "step: 420, loss: 0.08507898449897766\n",
            "step: 430, loss: 0.21569636464118958\n",
            "step: 440, loss: 0.04574543237686157\n",
            "step: 450, loss: 0.034309159964323044\n",
            "step: 460, loss: 0.08705279231071472\n",
            "step: 470, loss: 0.1440696120262146\n",
            "step: 480, loss: 0.12845635414123535\n",
            "step: 490, loss: 0.13556265830993652\n",
            "step: 500, loss: 0.09788487106561661\n",
            "step: 510, loss: 0.03229653090238571\n",
            "step: 520, loss: 0.030654048547148705\n",
            "step: 530, loss: 0.009390585124492645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9201307800093415, f1=0.9094304388422036, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11799515783786774\n",
            "step: 10, loss: 0.06421452760696411\n",
            "step: 20, loss: 0.02518618106842041\n",
            "step: 30, loss: 0.05513453856110573\n",
            "step: 40, loss: 0.038343220949172974\n",
            "step: 50, loss: 0.1948377937078476\n",
            "step: 60, loss: 0.039441756904125214\n",
            "step: 70, loss: 0.04744437336921692\n",
            "step: 80, loss: 0.04039846360683441\n",
            "step: 90, loss: 0.017050784081220627\n",
            "step: 100, loss: 0.041043784469366074\n",
            "step: 110, loss: 0.10620772838592529\n",
            "step: 120, loss: 0.10146015882492065\n",
            "step: 130, loss: 0.05336228758096695\n",
            "step: 140, loss: 0.07658109068870544\n",
            "step: 150, loss: 0.04143499955534935\n",
            "step: 160, loss: 0.0133061483502388\n",
            "step: 170, loss: 0.06880736351013184\n",
            "step: 180, loss: 0.03719785809516907\n",
            "step: 190, loss: 0.0928630381822586\n",
            "step: 200, loss: 0.021252162754535675\n",
            "step: 210, loss: 0.12258660793304443\n",
            "step: 220, loss: 0.10722000896930695\n",
            "step: 230, loss: 0.021698186174035072\n",
            "step: 240, loss: 0.18800708651542664\n",
            "step: 250, loss: 0.02133638598024845\n",
            "step: 260, loss: 0.011286098510026932\n",
            "step: 270, loss: 0.27874770760536194\n",
            "step: 280, loss: 0.05875053629279137\n",
            "step: 290, loss: 0.03374618664383888\n",
            "step: 300, loss: 0.18674324452877045\n",
            "step: 310, loss: 0.01564345695078373\n",
            "step: 320, loss: 0.09280672669410706\n",
            "step: 330, loss: 0.07849274575710297\n",
            "step: 340, loss: 0.02523350715637207\n",
            "step: 350, loss: 0.005467272363603115\n",
            "step: 360, loss: 0.10945781320333481\n",
            "step: 370, loss: 0.17246553301811218\n",
            "step: 380, loss: 0.04794597998261452\n",
            "step: 390, loss: 0.10619291663169861\n",
            "step: 400, loss: 0.07227381318807602\n",
            "step: 410, loss: 0.025868268683552742\n",
            "step: 420, loss: 0.022689884528517723\n",
            "step: 430, loss: 0.013823182322084904\n",
            "step: 440, loss: 0.02818380668759346\n",
            "step: 450, loss: 0.01954442821443081\n",
            "step: 460, loss: 0.11699336767196655\n",
            "step: 470, loss: 0.05804017558693886\n",
            "step: 480, loss: 0.2952296733856201\n",
            "step: 490, loss: 0.022371530532836914\n",
            "step: 500, loss: 0.4020850658416748\n",
            "step: 510, loss: 0.06131164729595184\n",
            "step: 520, loss: 0.09585634618997574\n",
            "step: 530, loss: 0.0660601407289505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9140027014858171, f1=0.912375790424571, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28637149930000305\n",
            "step: 10, loss: 0.0534142442047596\n",
            "step: 20, loss: 0.08855294436216354\n",
            "step: 30, loss: 0.2279268056154251\n",
            "step: 40, loss: 0.024940699338912964\n",
            "step: 50, loss: 0.018772747367620468\n",
            "step: 60, loss: 0.04428652673959732\n",
            "step: 70, loss: 0.0060815829783678055\n",
            "step: 80, loss: 0.0015911273658275604\n",
            "step: 90, loss: 0.004826363641768694\n",
            "step: 100, loss: 0.1027444452047348\n",
            "step: 110, loss: 0.026483887806534767\n",
            "step: 120, loss: 0.006823614239692688\n",
            "step: 130, loss: 0.00610200222581625\n",
            "step: 140, loss: 0.025945499539375305\n",
            "step: 150, loss: 0.024450326338410378\n",
            "step: 160, loss: 0.00908638909459114\n",
            "step: 170, loss: 0.10300327092409134\n",
            "step: 180, loss: 0.13360126316547394\n",
            "step: 190, loss: 0.005314178299158812\n",
            "step: 200, loss: 0.02317933924496174\n",
            "step: 210, loss: 0.031215069815516472\n",
            "step: 220, loss: 0.16607868671417236\n",
            "step: 230, loss: 0.1419452726840973\n",
            "step: 240, loss: 0.00693834712728858\n",
            "step: 250, loss: 0.050619449466466904\n",
            "step: 260, loss: 0.025243712589144707\n",
            "step: 270, loss: 0.026112401857972145\n",
            "step: 280, loss: 0.027881793677806854\n",
            "step: 290, loss: 0.009629840962588787\n",
            "step: 300, loss: 0.07862608879804611\n",
            "step: 310, loss: 0.016031835228204727\n",
            "step: 320, loss: 0.046212922781705856\n",
            "step: 330, loss: 0.009331751614809036\n",
            "step: 340, loss: 0.10184039175510406\n",
            "step: 350, loss: 0.04409712180495262\n",
            "step: 360, loss: 0.06560022383928299\n",
            "step: 370, loss: 0.034785155206918716\n",
            "step: 380, loss: 0.04946495592594147\n",
            "step: 390, loss: 0.010645432397723198\n",
            "step: 400, loss: 0.020098671317100525\n",
            "step: 410, loss: 0.018630104139447212\n",
            "step: 420, loss: 0.2208569198846817\n",
            "step: 430, loss: 0.02200811356306076\n",
            "step: 440, loss: 0.003585233585909009\n",
            "step: 450, loss: 0.07534842938184738\n",
            "step: 460, loss: 0.05609137564897537\n",
            "step: 470, loss: 0.10103005915880203\n",
            "step: 480, loss: 0.009879985824227333\n",
            "step: 490, loss: 0.008819356560707092\n",
            "step: 500, loss: 0.0063043925911188126\n",
            "step: 510, loss: 0.018696192651987076\n",
            "step: 520, loss: 0.036079879850149155\n",
            "step: 530, loss: 0.058836039155721664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9145454545454544, f1=0.9166666666666667, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03262300416827202\n",
            "step: 10, loss: 0.008293005637824535\n",
            "step: 20, loss: 0.0015189124969765544\n",
            "step: 30, loss: 0.020679937675595284\n",
            "step: 40, loss: 0.009657951071858406\n",
            "step: 50, loss: 0.012889436446130276\n",
            "step: 60, loss: 0.006811486557126045\n",
            "step: 70, loss: 0.008199627511203289\n",
            "step: 80, loss: 0.0032841283828020096\n",
            "step: 90, loss: 0.08448174595832825\n",
            "step: 100, loss: 0.05759904906153679\n",
            "step: 110, loss: 0.08283467590808868\n",
            "step: 120, loss: 0.000879524159245193\n",
            "step: 130, loss: 0.014978736639022827\n",
            "step: 140, loss: 0.00427668821066618\n",
            "step: 150, loss: 0.030560864135622978\n",
            "step: 160, loss: 0.09218788146972656\n",
            "step: 170, loss: 0.00917276181280613\n",
            "step: 180, loss: 0.004552057478576899\n",
            "step: 190, loss: 0.0025390007067471743\n",
            "step: 200, loss: 0.031269509345293045\n",
            "step: 210, loss: 0.1613999605178833\n",
            "step: 220, loss: 0.0071815489791333675\n",
            "step: 230, loss: 0.06160494312644005\n",
            "step: 240, loss: 0.00987508799880743\n",
            "step: 250, loss: 0.12924101948738098\n",
            "step: 260, loss: 0.0026537058874964714\n",
            "step: 270, loss: 0.0806698426604271\n",
            "step: 280, loss: 0.11370056867599487\n",
            "step: 290, loss: 0.03795306012034416\n",
            "step: 300, loss: 0.023649973794817924\n",
            "step: 310, loss: 0.08483918756246567\n",
            "step: 320, loss: 0.0032216007821261883\n",
            "step: 330, loss: 0.0037346426397562027\n",
            "step: 340, loss: 0.0070657976903021336\n",
            "step: 350, loss: 0.0048961034044623375\n",
            "step: 360, loss: 0.024276966229081154\n",
            "step: 370, loss: 0.0065468959510326385\n",
            "step: 380, loss: 0.021091513335704803\n",
            "step: 390, loss: 0.029259657487273216\n",
            "step: 400, loss: 0.027950067073106766\n",
            "step: 410, loss: 0.005062004551291466\n",
            "step: 420, loss: 0.020648209378123283\n",
            "step: 430, loss: 0.10759465396404266\n",
            "step: 440, loss: 0.036981746554374695\n",
            "step: 450, loss: 0.059811074286699295\n",
            "step: 460, loss: 0.01614619791507721\n",
            "step: 470, loss: 0.0346820205450058\n",
            "step: 480, loss: 0.008912820369005203\n",
            "step: 490, loss: 0.007226788438856602\n",
            "step: 500, loss: 0.03792177513241768\n",
            "step: 510, loss: 0.016789710149168968\n",
            "step: 520, loss: 0.07044201344251633\n",
            "step: 530, loss: 0.025195740163326263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9085872576177285, f1=0.9114391143911439, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18542005121707916\n",
            "step: 10, loss: 0.06727973371744156\n",
            "step: 20, loss: 0.0781380906701088\n",
            "step: 30, loss: 0.0019179201917722821\n",
            "step: 40, loss: 0.004981784150004387\n",
            "step: 50, loss: 0.006964416708797216\n",
            "step: 60, loss: 0.03579100966453552\n",
            "step: 70, loss: 0.03727424144744873\n",
            "step: 80, loss: 0.0013326979242265224\n",
            "step: 90, loss: 0.04618821293115616\n",
            "step: 100, loss: 0.013198843225836754\n",
            "step: 110, loss: 0.022630246356129646\n",
            "step: 120, loss: 0.0032452959567308426\n",
            "step: 130, loss: 0.000993763329461217\n",
            "step: 140, loss: 0.004526820965111256\n",
            "step: 150, loss: 0.0007090538274496794\n",
            "step: 160, loss: 0.0010285477619618177\n",
            "step: 170, loss: 0.014635191299021244\n",
            "step: 180, loss: 0.0003215721226297319\n",
            "step: 190, loss: 0.0018574486020952463\n",
            "step: 200, loss: 0.00332138454541564\n",
            "step: 210, loss: 0.010706671513617039\n",
            "step: 220, loss: 0.0028332474175840616\n",
            "step: 230, loss: 0.0044870986603200436\n",
            "step: 240, loss: 0.00793865229934454\n",
            "step: 250, loss: 0.0006601769127883017\n",
            "step: 260, loss: 0.005956884939223528\n",
            "step: 270, loss: 0.017000148072838783\n",
            "step: 280, loss: 0.008502914570271969\n",
            "step: 290, loss: 0.22260624170303345\n",
            "step: 300, loss: 0.004804960917681456\n",
            "step: 310, loss: 0.0014086677692830563\n",
            "step: 320, loss: 0.004899685271084309\n",
            "step: 330, loss: 0.015576068311929703\n",
            "step: 340, loss: 0.0026593529619276524\n",
            "step: 350, loss: 0.011866766959428787\n",
            "step: 360, loss: 0.014502612873911858\n",
            "step: 370, loss: 0.0928075909614563\n",
            "step: 380, loss: 0.0016682625282555819\n",
            "step: 390, loss: 0.000428446801379323\n",
            "step: 400, loss: 0.02271287888288498\n",
            "step: 410, loss: 0.0008707576198503375\n",
            "step: 420, loss: 0.002678428776562214\n",
            "step: 430, loss: 0.001983599504455924\n",
            "step: 440, loss: 0.014816178008913994\n",
            "step: 450, loss: 0.0014469235902652144\n",
            "step: 460, loss: 0.009423049166798592\n",
            "step: 470, loss: 0.0009958164300769567\n",
            "step: 480, loss: 0.016396719962358475\n",
            "step: 490, loss: 0.0006591746932826936\n",
            "step: 500, loss: 0.012513939291238785\n",
            "step: 510, loss: 0.045517925173044205\n",
            "step: 520, loss: 0.004745967220515013\n",
            "step: 530, loss: 0.0008910807082429528\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9138823529411764, f1=0.9066918001885014, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0025951277930289507\n",
            "step: 10, loss: 0.0010137355420738459\n",
            "step: 20, loss: 0.058443523943424225\n",
            "step: 30, loss: 0.014291886240243912\n",
            "step: 40, loss: 0.029803873971104622\n",
            "step: 50, loss: 0.02585124596953392\n",
            "step: 60, loss: 0.0013539539650082588\n",
            "step: 70, loss: 0.006077876314520836\n",
            "step: 80, loss: 0.0017450586892664433\n",
            "step: 90, loss: 0.0019030902767553926\n",
            "step: 100, loss: 0.0034091060515493155\n",
            "step: 110, loss: 0.002590242540463805\n",
            "step: 120, loss: 0.0012333259219303727\n",
            "step: 130, loss: 0.00129959755577147\n",
            "step: 140, loss: 0.18697185814380646\n",
            "step: 150, loss: 0.0011780220083892345\n",
            "step: 160, loss: 0.005205962341278791\n",
            "step: 170, loss: 0.0038690040819346905\n",
            "step: 180, loss: 0.00026012014131993055\n",
            "step: 190, loss: 0.00363499135710299\n",
            "step: 200, loss: 0.002284741261973977\n",
            "step: 210, loss: 0.0024792090989649296\n",
            "step: 220, loss: 0.002287053968757391\n",
            "step: 230, loss: 0.0004852268612012267\n",
            "step: 240, loss: 0.003590776352211833\n",
            "step: 250, loss: 0.010212046094238758\n",
            "step: 260, loss: 0.0006417833501473069\n",
            "step: 270, loss: 0.11922129988670349\n",
            "step: 280, loss: 0.08108306676149368\n",
            "step: 290, loss: 0.0009587758686393499\n",
            "step: 300, loss: 0.0003522929619066417\n",
            "step: 310, loss: 0.0005858317017555237\n",
            "step: 320, loss: 0.00046292191836982965\n",
            "step: 330, loss: 0.0024228328838944435\n",
            "step: 340, loss: 0.15088613331317902\n",
            "step: 350, loss: 0.008663907647132874\n",
            "step: 360, loss: 0.0074203768745064735\n",
            "step: 370, loss: 0.029101891443133354\n",
            "step: 380, loss: 0.007404194213449955\n",
            "step: 390, loss: 0.003193833166733384\n",
            "step: 400, loss: 0.025502949953079224\n",
            "step: 410, loss: 0.0015063921455293894\n",
            "step: 420, loss: 0.011638288386166096\n",
            "step: 430, loss: 0.0010246913880109787\n",
            "step: 440, loss: 0.0003254944458603859\n",
            "step: 450, loss: 0.000548487703781575\n",
            "step: 460, loss: 0.00017885270062834024\n",
            "step: 470, loss: 0.008978284895420074\n",
            "step: 480, loss: 0.002614029450342059\n",
            "step: 490, loss: 0.0035849595442414284\n",
            "step: 500, loss: 0.10534856468439102\n",
            "step: 510, loss: 0.0031874768901616335\n",
            "step: 520, loss: 0.10527743399143219\n",
            "step: 530, loss: 0.021672924980521202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9096864763687412, f1=0.9140989729225022, best_f1=0.9094304388422036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006613505538553\n",
            "step: 10, loss: 0.0012190265115350485\n",
            "step: 20, loss: 0.0014491943875327706\n",
            "step: 30, loss: 0.002621658146381378\n",
            "step: 40, loss: 0.003953542094677687\n",
            "step: 50, loss: 0.0022209591697901487\n",
            "step: 60, loss: 0.07669061422348022\n",
            "step: 70, loss: 0.037964705377817154\n",
            "step: 80, loss: 0.0669066458940506\n",
            "step: 90, loss: 0.0012577433371916413\n",
            "step: 100, loss: 0.0008769165142439306\n",
            "step: 110, loss: 0.0005076148081570864\n",
            "step: 120, loss: 0.0005082474090158939\n",
            "step: 130, loss: 0.0006627518450841308\n",
            "step: 140, loss: 0.001256463467143476\n",
            "step: 150, loss: 0.0026942577678710222\n",
            "step: 160, loss: 0.00010433196439407766\n",
            "step: 170, loss: 0.0005914315115660429\n",
            "step: 180, loss: 0.0009070396772585809\n",
            "step: 190, loss: 0.0006506310892291367\n",
            "step: 200, loss: 0.0006909586954861879\n",
            "step: 210, loss: 0.13477732241153717\n",
            "step: 220, loss: 0.0002372202288825065\n",
            "step: 230, loss: 0.17931558191776276\n",
            "step: 240, loss: 0.00422305054962635\n",
            "step: 250, loss: 0.0012137752491980791\n",
            "step: 260, loss: 0.01871895045042038\n",
            "step: 270, loss: 0.00017950274923350662\n",
            "step: 280, loss: 0.0021252771839499474\n",
            "step: 290, loss: 0.004264416638761759\n",
            "step: 300, loss: 0.0004773074761033058\n",
            "step: 310, loss: 0.02889949455857277\n",
            "step: 320, loss: 0.002586348680779338\n",
            "step: 330, loss: 0.0002107762120431289\n",
            "step: 340, loss: 0.0018796140793710947\n",
            "step: 350, loss: 0.002318108920007944\n",
            "step: 360, loss: 0.0012525967322289944\n",
            "step: 370, loss: 0.0023347234819084406\n",
            "step: 380, loss: 0.0008434999617747962\n",
            "step: 390, loss: 0.00025965546956285834\n",
            "step: 400, loss: 0.0005829064175486565\n",
            "step: 410, loss: 0.0059595247730612755\n",
            "step: 420, loss: 0.004065598826855421\n",
            "step: 430, loss: 0.006092015188187361\n",
            "step: 440, loss: 0.00023194565437734127\n",
            "step: 450, loss: 0.00021498952992260456\n",
            "step: 460, loss: 0.0006896554259583354\n",
            "step: 470, loss: 0.0025351662188768387\n",
            "step: 480, loss: 0.2095939964056015\n",
            "step: 490, loss: 0.0022275305818766356\n",
            "step: 500, loss: 0.0006884145550429821\n",
            "step: 510, loss: 0.017464011907577515\n",
            "step: 520, loss: 0.038664624094963074\n",
            "step: 530, loss: 0.0012507871724665165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9206349206349207, f1=0.9217148182665423, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005650388775393367\n",
            "step: 10, loss: 0.0018671437865123153\n",
            "step: 20, loss: 0.0040298751555383205\n",
            "step: 30, loss: 0.0030187792144715786\n",
            "step: 40, loss: 0.0037945189978927374\n",
            "step: 50, loss: 0.020234690979123116\n",
            "step: 60, loss: 0.001059631584212184\n",
            "step: 70, loss: 0.007919171825051308\n",
            "step: 80, loss: 0.006039959378540516\n",
            "step: 90, loss: 0.0010351224336773157\n",
            "step: 100, loss: 0.0010892859427258372\n",
            "step: 110, loss: 0.00020653374667745084\n",
            "step: 120, loss: 0.0008611121447756886\n",
            "step: 130, loss: 0.00022201014508027583\n",
            "step: 140, loss: 0.009934407658874989\n",
            "step: 150, loss: 0.00039981905138120055\n",
            "step: 160, loss: 0.014933893457055092\n",
            "step: 170, loss: 0.0985182374715805\n",
            "step: 180, loss: 0.0038950778543949127\n",
            "step: 190, loss: 0.00824741180986166\n",
            "step: 200, loss: 0.0007228046888485551\n",
            "step: 210, loss: 0.0011049280874431133\n",
            "step: 220, loss: 0.005849468521773815\n",
            "step: 230, loss: 0.02762734703719616\n",
            "step: 240, loss: 0.0007493699085898697\n",
            "step: 250, loss: 6.621843203902245e-05\n",
            "step: 260, loss: 0.0008840385708026588\n",
            "step: 270, loss: 0.0022149852011352777\n",
            "step: 280, loss: 0.04242132231593132\n",
            "step: 290, loss: 9.16691351449117e-05\n",
            "step: 300, loss: 0.009626622311770916\n",
            "step: 310, loss: 0.0005585594335570931\n",
            "step: 320, loss: 0.012573247775435448\n",
            "step: 330, loss: 0.003999853506684303\n",
            "step: 340, loss: 0.0004972160677425563\n",
            "step: 350, loss: 0.00018910481594502926\n",
            "step: 360, loss: 0.012378744781017303\n",
            "step: 370, loss: 8.706390508450568e-05\n",
            "step: 380, loss: 0.0019927467219531536\n",
            "step: 390, loss: 0.0671495720744133\n",
            "step: 400, loss: 0.0001457906182622537\n",
            "step: 410, loss: 0.00033553093089722097\n",
            "step: 420, loss: 0.0004097643541172147\n",
            "step: 430, loss: 0.0008826041594147682\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 440, loss: 0.0002603552129585296\n",
            "step: 450, loss: 0.0006157233146950603\n",
            "step: 460, loss: 0.00023841277288738638\n",
            "step: 470, loss: 0.00029017950873821974\n",
            "step: 480, loss: 0.0009677457856014371\n",
            "step: 490, loss: 0.00027685289387591183\n",
            "step: 500, loss: 0.001551009132526815\n",
            "step: 510, loss: 0.0003515036078169942\n",
            "step: 520, loss: 0.0008340540225617588\n",
            "step: 530, loss: 8.716130105312914e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9166666666666667, f1=0.9168949771689499, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006763663841411471\n",
            "step: 10, loss: 9.939727169694379e-05\n",
            "step: 20, loss: 0.0003086072101723403\n",
            "step: 30, loss: 0.0004994016489945352\n",
            "step: 40, loss: 7.849602843634784e-05\n",
            "step: 50, loss: 6.0797890910180286e-05\n",
            "step: 60, loss: 4.2525469325482845e-05\n",
            "step: 70, loss: 0.00018514586554374546\n",
            "step: 80, loss: 0.000295343721518293\n",
            "step: 90, loss: 9.431268699700013e-05\n",
            "step: 100, loss: 9.560255421092734e-05\n",
            "step: 110, loss: 0.00011171709047630429\n",
            "step: 120, loss: 6.419309647753835e-05\n",
            "step: 130, loss: 0.001816938747651875\n",
            "step: 140, loss: 0.000837742059957236\n",
            "step: 150, loss: 7.871480192989111e-05\n",
            "step: 160, loss: 0.0009772501653060317\n",
            "step: 170, loss: 0.013661985285580158\n",
            "step: 180, loss: 7.174986239988357e-05\n",
            "step: 190, loss: 0.0016980392392724752\n",
            "step: 200, loss: 0.00011108517355751246\n",
            "step: 210, loss: 8.8558103016112e-05\n",
            "step: 220, loss: 0.00033863188582472503\n",
            "step: 230, loss: 6.232516898307949e-05\n",
            "step: 240, loss: 9.22418330446817e-05\n",
            "step: 250, loss: 0.0005831416929140687\n",
            "step: 260, loss: 0.04116189107298851\n",
            "step: 270, loss: 5.734899968956597e-05\n",
            "step: 280, loss: 7.135972555261105e-05\n",
            "step: 290, loss: 0.0005692597478628159\n",
            "step: 300, loss: 0.0001064229363691993\n",
            "step: 310, loss: 0.00031102797947824\n",
            "step: 320, loss: 0.00011119338887510821\n",
            "step: 330, loss: 0.055714935064315796\n",
            "step: 340, loss: 7.10840686224401e-05\n",
            "step: 350, loss: 0.0077296169474720955\n",
            "step: 360, loss: 5.6981269153766334e-05\n",
            "step: 370, loss: 0.0007043425575830042\n",
            "step: 380, loss: 0.0003625699901022017\n",
            "step: 390, loss: 0.00023487793805543333\n",
            "step: 400, loss: 0.00041448144474998116\n",
            "step: 410, loss: 6.568709795828909e-05\n",
            "step: 420, loss: 0.00010407637455500662\n",
            "step: 430, loss: 0.003749329363927245\n",
            "step: 440, loss: 0.002058332087472081\n",
            "step: 450, loss: 9.323958511231467e-05\n",
            "step: 460, loss: 0.001286929240450263\n",
            "step: 470, loss: 3.388798359083012e-05\n",
            "step: 480, loss: 7.73786596255377e-05\n",
            "step: 490, loss: 0.00012157404125900939\n",
            "step: 500, loss: 0.002379857935011387\n",
            "step: 510, loss: 0.00010700523853302002\n",
            "step: 520, loss: 0.00016060253256000578\n",
            "step: 530, loss: 5.2331281040096655e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.912330051570558, f1=0.9187180678123548, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.255136344814673e-05\n",
            "step: 10, loss: 6.30632130196318e-05\n",
            "step: 20, loss: 0.00020757026504725218\n",
            "step: 30, loss: 0.0013222479028627276\n",
            "step: 40, loss: 0.00010134552576346323\n",
            "step: 50, loss: 0.000976894749328494\n",
            "step: 60, loss: 5.438566586235538e-05\n",
            "step: 70, loss: 5.3982359531801194e-05\n",
            "step: 80, loss: 7.659979746676981e-05\n",
            "step: 90, loss: 6.959021266084164e-05\n",
            "step: 100, loss: 7.551562885055318e-05\n",
            "step: 110, loss: 4.9069232773035765e-05\n",
            "step: 120, loss: 0.0003717165964189917\n",
            "step: 130, loss: 0.00010920553904725239\n",
            "step: 140, loss: 0.0015061793383210897\n",
            "step: 150, loss: 0.00027083721943199635\n",
            "step: 160, loss: 6.40108555671759e-05\n",
            "step: 170, loss: 0.00011403641838114709\n",
            "step: 180, loss: 7.215445657493547e-05\n",
            "step: 190, loss: 8.870491728885099e-05\n",
            "step: 200, loss: 0.0004221806884743273\n",
            "step: 210, loss: 0.0011637161951512098\n",
            "step: 220, loss: 0.01908283866941929\n",
            "step: 230, loss: 0.00020676500571426004\n",
            "step: 240, loss: 0.00016398528532590717\n",
            "step: 250, loss: 5.245941429166123e-05\n",
            "step: 260, loss: 0.0006227224366739392\n",
            "step: 270, loss: 0.00016089082055259496\n",
            "step: 280, loss: 0.0004532256571110338\n",
            "step: 290, loss: 0.00011889773304574192\n",
            "step: 300, loss: 4.1318849980598316e-05\n",
            "step: 310, loss: 0.038059089332818985\n",
            "step: 320, loss: 0.0002890423056669533\n",
            "step: 330, loss: 4.164514393778518e-05\n",
            "step: 340, loss: 0.00036558497231453657\n",
            "step: 350, loss: 0.00025720111443661153\n",
            "step: 360, loss: 0.0001996352366404608\n",
            "step: 370, loss: 0.0029504194390028715\n",
            "step: 380, loss: 0.00013740982103627175\n",
            "step: 390, loss: 0.0012242940720170736\n",
            "step: 400, loss: 0.00012197878095321357\n",
            "step: 410, loss: 0.000264778733253479\n",
            "step: 420, loss: 0.0007940392242744565\n",
            "step: 430, loss: 3.589110201573931e-05\n",
            "step: 440, loss: 0.011596345342695713\n",
            "step: 450, loss: 5.3007519454695284e-05\n",
            "step: 460, loss: 3.9887996535981074e-05\n",
            "step: 470, loss: 9.435562242288142e-05\n",
            "step: 480, loss: 0.00011534568329807371\n",
            "step: 490, loss: 0.0005237425793893635\n",
            "step: 500, loss: 0.006045225076377392\n",
            "step: 510, loss: 3.7392597732832655e-05\n",
            "step: 520, loss: 8.660013554617763e-05\n",
            "step: 530, loss: 0.00015465117758139968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9109004739336494, f1=0.9116541353383459, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040583443478681147\n",
            "step: 10, loss: 0.0017220880836248398\n",
            "step: 20, loss: 7.135054329410195e-05\n",
            "step: 30, loss: 0.0007998099317774177\n",
            "step: 40, loss: 0.03720865026116371\n",
            "step: 50, loss: 5.8461810112930834e-05\n",
            "step: 60, loss: 0.0008020593668334186\n",
            "step: 70, loss: 0.00012480041186790913\n",
            "step: 80, loss: 4.009261101600714e-05\n",
            "step: 90, loss: 9.656733163865283e-05\n",
            "step: 100, loss: 0.002373303985223174\n",
            "step: 110, loss: 0.00025161131634376943\n",
            "step: 120, loss: 4.2830462916754186e-05\n",
            "step: 130, loss: 4.398117016535252e-05\n",
            "step: 140, loss: 0.00033955954131670296\n",
            "step: 150, loss: 2.621045678097289e-05\n",
            "step: 160, loss: 2.9626427931361832e-05\n",
            "step: 170, loss: 5.567954940488562e-05\n",
            "step: 180, loss: 5.974435043754056e-05\n",
            "step: 190, loss: 0.0003876701812259853\n",
            "step: 200, loss: 0.07276774942874908\n",
            "step: 210, loss: 4.6666253183502704e-05\n",
            "step: 220, loss: 0.01226213201880455\n",
            "step: 230, loss: 0.01772150956094265\n",
            "step: 240, loss: 0.0001030282219289802\n",
            "step: 250, loss: 5.022447294322774e-05\n",
            "step: 260, loss: 3.846890467684716e-05\n",
            "step: 270, loss: 9.751049219630659e-05\n",
            "step: 280, loss: 0.0001469978888053447\n",
            "step: 290, loss: 5.8799239923246205e-05\n",
            "step: 300, loss: 0.0012087946524843574\n",
            "step: 310, loss: 0.00046300876419991255\n",
            "step: 320, loss: 0.00037817409611307085\n",
            "step: 330, loss: 7.045833626762033e-05\n",
            "step: 340, loss: 0.0005904326098971069\n",
            "step: 350, loss: 0.00023739435710012913\n",
            "step: 360, loss: 2.607981878099963e-05\n",
            "step: 370, loss: 0.0013802580069750547\n",
            "step: 380, loss: 4.665645974455401e-05\n",
            "step: 390, loss: 4.35433721577283e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 7.441703928634524e-05\n",
            "step: 410, loss: 8.693610288901255e-05\n",
            "step: 420, loss: 0.00031432227115146816\n",
            "step: 430, loss: 5.702791895600967e-05\n",
            "step: 440, loss: 7.774481491651386e-05\n",
            "step: 450, loss: 6.3696141296532e-05\n",
            "step: 460, loss: 0.05062633752822876\n",
            "step: 470, loss: 0.0001053093874361366\n",
            "step: 480, loss: 0.00010337492130929604\n",
            "step: 490, loss: 6.769036554032937e-05\n",
            "step: 500, loss: 0.008400246500968933\n",
            "step: 510, loss: 4.694828385254368e-05\n",
            "step: 520, loss: 0.0001598810777068138\n",
            "step: 530, loss: 0.00016703479923307896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9172093023255814, f1=0.9126213592233011, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.121121310163289e-05\n",
            "step: 10, loss: 0.00010874636063817888\n",
            "step: 20, loss: 4.8470988986082375e-05\n",
            "step: 30, loss: 4.488399645197205e-05\n",
            "step: 40, loss: 4.43886783614289e-05\n",
            "step: 50, loss: 0.00573759013786912\n",
            "step: 60, loss: 0.0012242953525856137\n",
            "step: 70, loss: 0.00012600512127391994\n",
            "step: 80, loss: 2.7972724637947977e-05\n",
            "step: 90, loss: 8.51728837005794e-05\n",
            "step: 100, loss: 0.00015457489644177258\n",
            "step: 110, loss: 4.18292147514876e-05\n",
            "step: 120, loss: 3.0602532206103206e-05\n",
            "step: 130, loss: 0.00010600533278193325\n",
            "step: 140, loss: 3.6189983802614734e-05\n",
            "step: 150, loss: 3.1496540032094344e-05\n",
            "step: 160, loss: 3.600010313675739e-05\n",
            "step: 170, loss: 0.00018579259631223977\n",
            "step: 180, loss: 4.14560163335409e-05\n",
            "step: 190, loss: 8.503046410623938e-05\n",
            "step: 200, loss: 4.631328920368105e-05\n",
            "step: 210, loss: 6.216738256625831e-05\n",
            "step: 220, loss: 8.722685015527532e-05\n",
            "step: 230, loss: 0.0001610312465345487\n",
            "step: 240, loss: 0.00028941838536411524\n",
            "step: 250, loss: 5.167450581211597e-05\n",
            "step: 260, loss: 3.1202078389469534e-05\n",
            "step: 270, loss: 5.01617141708266e-05\n",
            "step: 280, loss: 6.109073729021475e-05\n",
            "step: 290, loss: 3.514694617479108e-05\n",
            "step: 300, loss: 0.0001582438126206398\n",
            "step: 310, loss: 5.987327313050628e-05\n",
            "step: 320, loss: 3.086296055698767e-05\n",
            "step: 330, loss: 0.00013075278548058122\n",
            "step: 340, loss: 2.962644248327706e-05\n",
            "step: 350, loss: 8.727169188205153e-05\n",
            "step: 360, loss: 0.00011677419388433918\n",
            "step: 370, loss: 4.6538960305042565e-05\n",
            "step: 380, loss: 3.241653030272573e-05\n",
            "step: 390, loss: 3.024119359906763e-05\n",
            "step: 400, loss: 0.00012174983567092568\n",
            "step: 410, loss: 0.0004303231544326991\n",
            "step: 420, loss: 0.000520272005815059\n",
            "step: 430, loss: 0.000365823827451095\n",
            "step: 440, loss: 3.713175465236418e-05\n",
            "step: 450, loss: 4.4581662223208696e-05\n",
            "step: 460, loss: 2.7033685910282657e-05\n",
            "step: 470, loss: 8.082083513727412e-05\n",
            "step: 480, loss: 0.0017260446911677718\n",
            "step: 490, loss: 0.00043435022234916687\n",
            "step: 500, loss: 3.873186869896017e-05\n",
            "step: 510, loss: 8.6315514636226e-05\n",
            "step: 520, loss: 3.0691571737406775e-05\n",
            "step: 530, loss: 4.2420633690198883e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9111424541607899, f1=0.9119025304592315, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021738240029662848\n",
            "step: 10, loss: 0.004684855230152607\n",
            "step: 20, loss: 6.286859570536762e-05\n",
            "step: 30, loss: 6.389882764779031e-05\n",
            "step: 40, loss: 2.133071029675193e-05\n",
            "step: 50, loss: 3.5813147405860946e-05\n",
            "step: 60, loss: 3.320858741062693e-05\n",
            "step: 70, loss: 3.6467696190811694e-05\n",
            "step: 80, loss: 1.7579348423168994e-05\n",
            "step: 90, loss: 0.0004220016417093575\n",
            "step: 100, loss: 7.744241156615317e-05\n",
            "step: 110, loss: 2.2246800654102117e-05\n",
            "step: 120, loss: 2.702602796489373e-05\n",
            "step: 130, loss: 2.07493740163045e-05\n",
            "step: 140, loss: 4.394137067720294e-05\n",
            "step: 150, loss: 0.07993196696043015\n",
            "step: 160, loss: 2.032472548307851e-05\n",
            "step: 170, loss: 0.00018914291285909712\n",
            "step: 180, loss: 4.059444472659379e-05\n",
            "step: 190, loss: 7.513039599871263e-05\n",
            "step: 200, loss: 3.2874333555810153e-05\n",
            "step: 210, loss: 4.548911238089204e-05\n",
            "step: 220, loss: 2.8504793590400368e-05\n",
            "step: 230, loss: 1.9173761756974272e-05\n",
            "step: 240, loss: 3.786956949625164e-05\n",
            "step: 250, loss: 2.3044171030051075e-05\n",
            "step: 260, loss: 1.9158876966685057e-05\n",
            "step: 270, loss: 2.0473746189964004e-05\n",
            "step: 280, loss: 2.7953568860539235e-05\n",
            "step: 290, loss: 1.5839763364056125e-05\n",
            "step: 300, loss: 2.640049933688715e-05\n",
            "step: 310, loss: 0.001467360183596611\n",
            "step: 320, loss: 0.0015486617339774966\n",
            "step: 330, loss: 0.007711879443377256\n",
            "step: 340, loss: 2.8895448849652894e-05\n",
            "step: 350, loss: 2.2366204575519077e-05\n",
            "step: 360, loss: 3.4864991903305054e-05\n",
            "step: 370, loss: 5.1317663746885955e-05\n",
            "step: 380, loss: 0.0012646878603845835\n",
            "step: 390, loss: 0.0003970117249991745\n",
            "step: 400, loss: 2.4370394385186955e-05\n",
            "step: 410, loss: 5.979060006211512e-05\n",
            "step: 420, loss: 4.6525474317604676e-05\n",
            "step: 430, loss: 0.051170893013477325\n",
            "step: 440, loss: 0.0001008570397971198\n",
            "step: 450, loss: 9.679418144514784e-05\n",
            "step: 460, loss: 0.0023260649759322405\n",
            "step: 470, loss: 5.4264775826595724e-05\n",
            "step: 480, loss: 2.9108448870829307e-05\n",
            "step: 490, loss: 2.8441088943509385e-05\n",
            "step: 500, loss: 2.0622153897420503e-05\n",
            "step: 510, loss: 2.1714222384616733e-05\n",
            "step: 520, loss: 0.0045199403539299965\n",
            "step: 530, loss: 0.0001350540405837819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9110251450676983, f1=0.901884968583857, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.86422349140048e-05\n",
            "step: 10, loss: 2.0440129446797073e-05\n",
            "step: 20, loss: 2.3315413272939622e-05\n",
            "step: 30, loss: 2.0894798581139185e-05\n",
            "step: 40, loss: 3.069510421482846e-05\n",
            "step: 50, loss: 0.00010482859215699136\n",
            "step: 60, loss: 1.6703921573935077e-05\n",
            "step: 70, loss: 0.0012391365598887205\n",
            "step: 80, loss: 3.514172931318171e-05\n",
            "step: 90, loss: 2.3796676032361574e-05\n",
            "step: 100, loss: 3.457610364421271e-05\n",
            "step: 110, loss: 1.9937464458053e-05\n",
            "step: 120, loss: 3.917906724382192e-05\n",
            "step: 130, loss: 0.00032343470957130194\n",
            "step: 140, loss: 1.9043387510464527e-05\n",
            "step: 150, loss: 1.4323602044896688e-05\n",
            "step: 160, loss: 2.0090117686777376e-05\n",
            "step: 170, loss: 3.4016542485915124e-05\n",
            "step: 180, loss: 2.2824089683126658e-05\n",
            "step: 190, loss: 5.14735329488758e-05\n",
            "step: 200, loss: 2.5256862500100397e-05\n",
            "step: 210, loss: 2.0186940673738718e-05\n",
            "step: 220, loss: 0.00016572764434386045\n",
            "step: 230, loss: 3.8355017750291154e-05\n",
            "step: 240, loss: 3.229077628930099e-05\n",
            "step: 250, loss: 2.28057979256846e-05\n",
            "step: 260, loss: 8.492390770697966e-05\n",
            "step: 270, loss: 3.0205857910914347e-05\n",
            "step: 280, loss: 1.9803253962891176e-05\n",
            "step: 290, loss: 0.00018728923168964684\n",
            "step: 300, loss: 2.8620097509701736e-05\n",
            "step: 310, loss: 0.0013218637323006988\n",
            "step: 320, loss: 4.0396236727247015e-05\n",
            "step: 330, loss: 0.00024313764879480004\n",
            "step: 340, loss: 3.277305222582072e-05\n",
            "step: 350, loss: 4.1815870645223185e-05\n",
            "step: 360, loss: 0.0003849854110740125\n",
            "step: 370, loss: 3.5779961763182655e-05\n",
            "step: 380, loss: 0.00011059325333917513\n",
            "step: 390, loss: 0.011282576248049736\n",
            "step: 400, loss: 0.0011931052431464195\n",
            "step: 410, loss: 2.9335238650674e-05\n",
            "step: 420, loss: 5.193801553105004e-05\n",
            "step: 430, loss: 2.9175558665883727e-05\n",
            "step: 440, loss: 0.00028444171766750515\n",
            "step: 450, loss: 0.0014502472477033734\n",
            "step: 460, loss: 2.1662192011717707e-05\n",
            "step: 470, loss: 3.656798435258679e-05\n",
            "step: 480, loss: 3.734367055585608e-05\n",
            "step: 490, loss: 8.57944687595591e-05\n",
            "step: 500, loss: 1.6759891877882183e-05\n",
            "step: 510, loss: 0.008315177634358406\n",
            "step: 520, loss: 3.807796383625828e-05\n",
            "step: 530, loss: 0.00010337359708501026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9146853146853147, f1=0.9185185185185185, best_f1=0.9217148182665423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.6955602152156644e-05\n",
            "step: 10, loss: 2.8318325348664075e-05\n",
            "step: 20, loss: 3.801352795562707e-05\n",
            "step: 30, loss: 0.00018729858857113868\n",
            "step: 40, loss: 0.047205738723278046\n",
            "step: 50, loss: 9.551606490276754e-05\n",
            "step: 60, loss: 2.5089406335609965e-05\n",
            "step: 70, loss: 0.00027408695314079523\n",
            "step: 80, loss: 9.521297761239111e-05\n",
            "step: 90, loss: 2.845646122295875e-05\n",
            "step: 100, loss: 4.0972401620820165e-05\n",
            "step: 110, loss: 9.221676737070084e-05\n",
            "step: 120, loss: 0.0010312494123354554\n",
            "step: 130, loss: 0.0049126967787742615\n",
            "step: 140, loss: 2.3200573195936158e-05\n",
            "step: 150, loss: 2.808809767884668e-05\n",
            "step: 160, loss: 2.9525750505854376e-05\n",
            "step: 170, loss: 2.496572778909467e-05\n",
            "step: 180, loss: 1.867462196969427e-05\n",
            "step: 190, loss: 6.292795296758413e-05\n",
            "step: 200, loss: 3.0169592719175853e-05\n",
            "step: 210, loss: 0.0004147505678702146\n",
            "step: 220, loss: 2.498106005077716e-05\n",
            "step: 230, loss: 2.589371615613345e-05\n",
            "step: 240, loss: 2.7785548809333704e-05\n",
            "step: 250, loss: 2.4586039216956124e-05\n",
            "step: 260, loss: 1.985923881875351e-05\n",
            "step: 270, loss: 2.108480293827597e-05\n",
            "step: 280, loss: 7.471974095096812e-05\n",
            "step: 290, loss: 1.8749149603536353e-05\n",
            "step: 300, loss: 2.3584174414281733e-05\n",
            "step: 310, loss: 4.1689942008815706e-05\n",
            "step: 320, loss: 0.00016299469280056655\n",
            "step: 330, loss: 0.0002987776242662221\n",
            "step: 340, loss: 2.8369922802085057e-05\n",
            "step: 350, loss: 2.6321706172893755e-05\n",
            "step: 360, loss: 0.00017333502182736993\n",
            "step: 370, loss: 1.9006141883437522e-05\n",
            "step: 380, loss: 5.792810770799406e-05\n",
            "step: 390, loss: 9.187281830236316e-05\n",
            "step: 400, loss: 3.503073821775615e-05\n",
            "step: 410, loss: 2.550215504015796e-05\n",
            "step: 420, loss: 0.007548716850578785\n",
            "step: 430, loss: 8.820556104183197e-05\n",
            "step: 440, loss: 3.1961561035132036e-05\n",
            "step: 450, loss: 2.1509449652512558e-05\n",
            "step: 460, loss: 2.488409518264234e-05\n",
            "step: 470, loss: 0.008780020289123058\n",
            "step: 480, loss: 1.5277260899893008e-05\n",
            "step: 490, loss: 1.8741669919108972e-05\n",
            "step: 500, loss: 2.7525205950951204e-05\n",
            "step: 510, loss: 3.514491982059553e-05\n",
            "step: 520, loss: 3.206974361091852e-05\n",
            "step: 530, loss: 3.35348850057926e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9123134328358209, f1=0.9181692094313454, best_f1=0.9217148182665423\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:21, 269.46it/s]\n",
            "load_f1 = 0.9218604651162791\n",
            "real_f1 = 0.9205020920502092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 270.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf01f29-348c-4bd4-e3ae-e2e81f1e834a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5511749982833862\n",
            "step: 10, loss: 0.366182416677475\n",
            "step: 20, loss: 0.3707675039768219\n",
            "step: 30, loss: 0.33046072721481323\n",
            "step: 40, loss: 0.2013465315103531\n",
            "step: 50, loss: 0.44803348183631897\n",
            "step: 60, loss: 0.33485835790634155\n",
            "step: 70, loss: 0.23659294843673706\n",
            "step: 80, loss: 0.20745640993118286\n",
            "step: 90, loss: 0.46275436878204346\n",
            "step: 100, loss: 0.5162782669067383\n",
            "step: 110, loss: 0.2588721513748169\n",
            "step: 120, loss: 0.21401843428611755\n",
            "step: 130, loss: 0.26725494861602783\n",
            "step: 140, loss: 0.25542283058166504\n",
            "step: 150, loss: 0.3431297540664673\n",
            "step: 160, loss: 0.29818177223205566\n",
            "step: 170, loss: 0.31328216195106506\n",
            "step: 180, loss: 0.1020389273762703\n",
            "step: 190, loss: 0.19218020141124725\n",
            "step: 200, loss: 0.3173246383666992\n",
            "step: 210, loss: 0.2077813446521759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5141843971631205, f1=0.5549738219895288, best_f1=0.5549738219895288\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13466346263885498\n",
            "step: 10, loss: 0.2412434071302414\n",
            "step: 20, loss: 0.17177319526672363\n",
            "step: 30, loss: 0.19241701066493988\n",
            "step: 40, loss: 0.17342735826969147\n",
            "step: 50, loss: 0.1817523092031479\n",
            "step: 60, loss: 0.416681706905365\n",
            "step: 70, loss: 0.1303916871547699\n",
            "step: 80, loss: 0.24903303384780884\n",
            "step: 90, loss: 0.14727701246738434\n",
            "step: 100, loss: 0.060544826090335846\n",
            "step: 110, loss: 0.11061209440231323\n",
            "step: 120, loss: 0.1754954308271408\n",
            "step: 130, loss: 0.0455249659717083\n",
            "step: 140, loss: 0.21849335730075836\n",
            "step: 150, loss: 0.28749752044677734\n",
            "step: 160, loss: 0.19872166216373444\n",
            "step: 170, loss: 0.17957235872745514\n",
            "step: 180, loss: 0.22695370018482208\n",
            "step: 190, loss: 0.1942475438117981\n",
            "step: 200, loss: 0.06629502773284912\n",
            "step: 210, loss: 0.09032202512025833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5784753363228701, f1=0.501240694789082, best_f1=0.501240694789082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06278422474861145\n",
            "step: 10, loss: 0.20109079778194427\n",
            "step: 20, loss: 0.16583503782749176\n",
            "step: 30, loss: 0.2854541540145874\n",
            "step: 40, loss: 0.046691231429576874\n",
            "step: 50, loss: 0.12370912730693817\n",
            "step: 60, loss: 0.2065567523241043\n",
            "step: 70, loss: 0.20588736236095428\n",
            "step: 80, loss: 0.11195869743824005\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.05866504833102226\n",
            "step: 100, loss: 0.10154219716787338\n",
            "step: 110, loss: 0.14239725470542908\n",
            "step: 120, loss: 0.22616414725780487\n",
            "step: 130, loss: 0.15047594904899597\n",
            "step: 140, loss: 0.08509472757577896\n",
            "step: 150, loss: 0.2119530439376831\n",
            "step: 160, loss: 0.05866415798664093\n",
            "step: 170, loss: 0.08840569853782654\n",
            "step: 180, loss: 0.058072708547115326\n",
            "step: 190, loss: 0.16707001626491547\n",
            "step: 200, loss: 0.045936521142721176\n",
            "step: 210, loss: 0.0973811224102974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5848375451263538, f1=0.596745027124774, best_f1=0.596745027124774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.320909321308136\n",
            "step: 10, loss: 0.12331082671880722\n",
            "step: 20, loss: 0.10462630540132523\n",
            "step: 30, loss: 0.3499193489551544\n",
            "step: 40, loss: 0.06854365020990372\n",
            "step: 50, loss: 0.15896283090114594\n",
            "step: 60, loss: 0.14231239259243011\n",
            "step: 70, loss: 0.3141816258430481\n",
            "step: 80, loss: 0.13184918463230133\n",
            "step: 90, loss: 0.05803495645523071\n",
            "step: 100, loss: 0.27958810329437256\n",
            "step: 110, loss: 0.09275826811790466\n",
            "step: 120, loss: 0.13413439691066742\n",
            "step: 130, loss: 0.1624361276626587\n",
            "step: 140, loss: 0.23726606369018555\n",
            "step: 150, loss: 0.08997891843318939\n",
            "step: 160, loss: 0.033671360462903976\n",
            "step: 170, loss: 0.08825062960386276\n",
            "step: 180, loss: 0.33008164167404175\n",
            "step: 190, loss: 0.05936490371823311\n",
            "step: 200, loss: 0.18463142216205597\n",
            "step: 210, loss: 0.1599002629518509\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5996055226824457, f1=0.5806451612903225, best_f1=0.5806451612903225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1953476220369339\n",
            "step: 10, loss: 0.025740928947925568\n",
            "step: 20, loss: 0.21210703253746033\n",
            "step: 30, loss: 0.033502157777547836\n",
            "step: 40, loss: 0.04560543969273567\n",
            "step: 50, loss: 0.12349800020456314\n",
            "step: 60, loss: 0.156165212392807\n",
            "step: 70, loss: 0.26554664969444275\n",
            "step: 80, loss: 0.040700070559978485\n",
            "step: 90, loss: 0.05584985762834549\n",
            "step: 100, loss: 0.010340450331568718\n",
            "step: 110, loss: 0.15664911270141602\n",
            "step: 120, loss: 0.1538524329662323\n",
            "step: 130, loss: 0.12406452745199203\n",
            "step: 140, loss: 0.05935625359416008\n",
            "step: 150, loss: 0.040500398725271225\n",
            "step: 160, loss: 0.14283747971057892\n",
            "step: 170, loss: 0.06873826682567596\n",
            "step: 180, loss: 0.07027847319841385\n",
            "step: 190, loss: 0.039688531309366226\n",
            "step: 200, loss: 0.0688035860657692\n",
            "step: 210, loss: 0.038344766944646835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.610752688172043, f1=0.5720430107526882, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03535377234220505\n",
            "step: 10, loss: 0.07975593209266663\n",
            "step: 20, loss: 0.07366737723350525\n",
            "step: 30, loss: 0.004489730577915907\n",
            "step: 40, loss: 0.08611752092838287\n",
            "step: 50, loss: 0.011642887257039547\n",
            "step: 60, loss: 0.08366288244724274\n",
            "step: 70, loss: 0.0217424426227808\n",
            "step: 80, loss: 0.019633665680885315\n",
            "step: 90, loss: 0.07630892097949982\n",
            "step: 100, loss: 0.007801597472280264\n",
            "step: 110, loss: 0.0033159914892166853\n",
            "step: 120, loss: 0.021465396508574486\n",
            "step: 130, loss: 0.008629241026937962\n",
            "step: 140, loss: 0.13476857542991638\n",
            "step: 150, loss: 0.03208759427070618\n",
            "step: 160, loss: 0.02214445173740387\n",
            "step: 170, loss: 0.14349918067455292\n",
            "step: 180, loss: 0.06041873246431351\n",
            "step: 190, loss: 0.10130239278078079\n",
            "step: 200, loss: 0.00831328984349966\n",
            "step: 210, loss: 0.01715260185301304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5601750547045952, f1=0.5387931034482758, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001117223990149796\n",
            "step: 10, loss: 0.0026957800146192312\n",
            "step: 20, loss: 0.00589274475350976\n",
            "step: 30, loss: 0.2844163179397583\n",
            "step: 40, loss: 0.009759011678397655\n",
            "step: 50, loss: 0.07826033979654312\n",
            "step: 60, loss: 0.01342763565480709\n",
            "step: 70, loss: 0.037232864648103714\n",
            "step: 80, loss: 0.03862684220075607\n",
            "step: 90, loss: 0.15690074861049652\n",
            "step: 100, loss: 0.0042364210821688175\n",
            "step: 110, loss: 0.04560106247663498\n",
            "step: 120, loss: 0.1241108775138855\n",
            "step: 130, loss: 0.07751170545816422\n",
            "step: 140, loss: 0.014990096911787987\n",
            "step: 150, loss: 0.07187534868717194\n",
            "step: 160, loss: 0.0208045095205307\n",
            "step: 170, loss: 0.01507838536053896\n",
            "step: 180, loss: 0.005490211304277182\n",
            "step: 190, loss: 0.09241436421871185\n",
            "step: 200, loss: 0.008832084946334362\n",
            "step: 210, loss: 0.10592106729745865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5189873417721518, f1=0.5324947589098533, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0526016466319561\n",
            "step: 10, loss: 0.04585672914981842\n",
            "step: 20, loss: 0.02368415892124176\n",
            "step: 30, loss: 0.018340809270739555\n",
            "step: 40, loss: 0.04744570329785347\n",
            "step: 50, loss: 0.060997433960437775\n",
            "step: 60, loss: 0.02151540294289589\n",
            "step: 70, loss: 0.1312265396118164\n",
            "step: 80, loss: 0.08739916235208511\n",
            "step: 90, loss: 0.002228862838819623\n",
            "step: 100, loss: 0.022967655211687088\n",
            "step: 110, loss: 0.040423933416604996\n",
            "step: 120, loss: 0.01045741606503725\n",
            "step: 130, loss: 0.011757639236748219\n",
            "step: 140, loss: 0.051656901836395264\n",
            "step: 150, loss: 0.011319179087877274\n",
            "step: 160, loss: 0.04932812601327896\n",
            "step: 170, loss: 0.019752727821469307\n",
            "step: 180, loss: 0.07047766447067261\n",
            "step: 190, loss: 0.14845529198646545\n",
            "step: 200, loss: 0.00043714611092582345\n",
            "step: 210, loss: 0.022275593131780624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5406593406593406, f1=0.5353982300884956, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015405288897454739\n",
            "step: 10, loss: 0.025812968611717224\n",
            "step: 20, loss: 0.001614188775420189\n",
            "step: 30, loss: 0.02681724540889263\n",
            "step: 40, loss: 0.03517087921500206\n",
            "step: 50, loss: 0.012913281098008156\n",
            "step: 60, loss: 0.10767042636871338\n",
            "step: 70, loss: 0.05549526959657669\n",
            "step: 80, loss: 0.000619060592725873\n",
            "step: 90, loss: 0.0031624312978237867\n",
            "step: 100, loss: 0.07968215644359589\n",
            "step: 110, loss: 0.01930244266986847\n",
            "step: 120, loss: 0.0066214934922754765\n",
            "step: 130, loss: 0.0015774712665006518\n",
            "step: 140, loss: 0.0938350260257721\n",
            "step: 150, loss: 0.2110792100429535\n",
            "step: 160, loss: 0.03408084437251091\n",
            "step: 170, loss: 0.002781183458864689\n",
            "step: 180, loss: 0.0527152419090271\n",
            "step: 190, loss: 0.0010315484832972288\n",
            "step: 200, loss: 0.016842355951666832\n",
            "step: 210, loss: 0.00094818533398211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5684647302904564, f1=0.5308641975308642, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026407772675156593\n",
            "step: 10, loss: 0.050160616636276245\n",
            "step: 20, loss: 0.0014759297482669353\n",
            "step: 30, loss: 0.06600965559482574\n",
            "step: 40, loss: 0.003663807874545455\n",
            "step: 50, loss: 0.0008399923681281507\n",
            "step: 60, loss: 0.03282426670193672\n",
            "step: 70, loss: 0.04336530715227127\n",
            "step: 80, loss: 0.006788403727114201\n",
            "step: 90, loss: 0.008040199056267738\n",
            "step: 100, loss: 0.07859454303979874\n",
            "step: 110, loss: 0.002083119936287403\n",
            "step: 120, loss: 0.0005342165823094547\n",
            "step: 130, loss: 0.1114068254828453\n",
            "step: 140, loss: 0.0005542529397644103\n",
            "step: 150, loss: 0.0050441669300198555\n",
            "step: 160, loss: 0.004208774771541357\n",
            "step: 170, loss: 0.10038872808218002\n",
            "step: 180, loss: 0.022134464234113693\n",
            "step: 190, loss: 0.02448248490691185\n",
            "step: 200, loss: 0.0008513598586432636\n",
            "step: 210, loss: 0.054069049656391144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5418502202643173, f1=0.49557522123893805, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23760654032230377\n",
            "step: 10, loss: 0.019122416153550148\n",
            "step: 20, loss: 0.0023415363393723965\n",
            "step: 30, loss: 0.0032497846987098455\n",
            "step: 40, loss: 0.0011866550194099545\n",
            "step: 50, loss: 0.0005645027267746627\n",
            "step: 60, loss: 0.034654758870601654\n",
            "step: 70, loss: 0.0023983579594641924\n",
            "step: 80, loss: 0.0061037577688694\n",
            "step: 90, loss: 0.000979534350335598\n",
            "step: 100, loss: 0.0003939489251933992\n",
            "step: 110, loss: 0.024915484711527824\n",
            "step: 120, loss: 0.017129207029938698\n",
            "step: 130, loss: 0.010307217948138714\n",
            "step: 140, loss: 0.0011497247032821178\n",
            "step: 150, loss: 0.0002875995996873826\n",
            "step: 160, loss: 0.0005681785987690091\n",
            "step: 170, loss: 0.0016369407530874014\n",
            "step: 180, loss: 0.003328090999275446\n",
            "step: 190, loss: 0.0010164041304960847\n",
            "step: 200, loss: 0.017027515918016434\n",
            "step: 210, loss: 0.00020922160183545202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5520361990950227, f1=0.5059101654846335, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012368780560791492\n",
            "step: 10, loss: 0.0005627256468869746\n",
            "step: 20, loss: 0.001323063625022769\n",
            "step: 30, loss: 0.0032912783790379763\n",
            "step: 40, loss: 0.01355877984315157\n",
            "step: 50, loss: 0.0005958823603577912\n",
            "step: 60, loss: 0.001245562918484211\n",
            "step: 70, loss: 0.0012533963890746236\n",
            "step: 80, loss: 0.0002436558424960822\n",
            "step: 90, loss: 0.00030270760180428624\n",
            "step: 100, loss: 0.0007891844725236297\n",
            "step: 110, loss: 0.022283043712377548\n",
            "step: 120, loss: 0.0006510847597382963\n",
            "step: 130, loss: 0.0015017502009868622\n",
            "step: 140, loss: 0.0027388210874050856\n",
            "step: 150, loss: 0.0003744781424757093\n",
            "step: 160, loss: 0.0009179514017887414\n",
            "step: 170, loss: 0.0005963305593468249\n",
            "step: 180, loss: 0.0007726870244368911\n",
            "step: 190, loss: 0.007493926212191582\n",
            "step: 200, loss: 0.0005689905374310911\n",
            "step: 210, loss: 0.004640186205506325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5622317596566524, f1=0.5539112050739957, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03061828203499317\n",
            "step: 10, loss: 0.000249088421696797\n",
            "step: 20, loss: 0.018851127475500107\n",
            "step: 30, loss: 0.002319689141586423\n",
            "step: 40, loss: 0.0010493253357708454\n",
            "step: 50, loss: 0.007389931008219719\n",
            "step: 60, loss: 0.0021089590154588223\n",
            "step: 70, loss: 0.09759610146284103\n",
            "step: 80, loss: 0.030239837244153023\n",
            "step: 90, loss: 0.00029463027021847665\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.0001255377719644457\n",
            "step: 110, loss: 0.0010422088671475649\n",
            "step: 120, loss: 0.00018901104340329766\n",
            "step: 130, loss: 0.00014375218597706407\n",
            "step: 140, loss: 0.0006792726344428957\n",
            "step: 150, loss: 0.008908933028578758\n",
            "step: 160, loss: 0.003459748812019825\n",
            "step: 170, loss: 0.001042556599713862\n",
            "step: 180, loss: 0.019772443920373917\n",
            "step: 190, loss: 0.0003694502520374954\n",
            "step: 200, loss: 0.03375299647450447\n",
            "step: 210, loss: 0.0017098491080105305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5511111111111111, f1=0.5312499999999999, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002203792566433549\n",
            "step: 10, loss: 0.01918868161737919\n",
            "step: 20, loss: 0.0027525753248482943\n",
            "step: 30, loss: 0.003555507166311145\n",
            "step: 40, loss: 0.0008162501617334783\n",
            "step: 50, loss: 0.0014963659923523664\n",
            "step: 60, loss: 0.0001023632867145352\n",
            "step: 70, loss: 0.001372376922518015\n",
            "step: 80, loss: 0.000451058178441599\n",
            "step: 90, loss: 0.0008110243361443281\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.00023023574613034725\n",
            "step: 110, loss: 0.00021613638091366738\n",
            "step: 120, loss: 0.08912710100412369\n",
            "step: 130, loss: 0.0011021653190255165\n",
            "step: 140, loss: 0.0021936462726444006\n",
            "step: 150, loss: 0.00022204082051757723\n",
            "step: 160, loss: 0.0005207753274589777\n",
            "step: 170, loss: 0.000273180368822068\n",
            "step: 180, loss: 0.060152553021907806\n",
            "step: 190, loss: 0.0005029558669775724\n",
            "step: 200, loss: 0.00016129126015584916\n",
            "step: 210, loss: 0.003945199307054281\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5475113122171946, f1=0.5079365079365079, best_f1=0.5720430107526882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002087611355818808\n",
            "step: 10, loss: 0.00013374547415878624\n",
            "step: 20, loss: 0.0004414421273395419\n",
            "step: 30, loss: 0.0009411587379872799\n",
            "step: 40, loss: 0.0003972258709836751\n",
            "step: 50, loss: 0.00022729436750523746\n",
            "step: 60, loss: 0.0006089657545089722\n",
            "step: 70, loss: 0.0006554850842803717\n",
            "step: 80, loss: 0.0007573581533506513\n",
            "step: 90, loss: 0.000969260057900101\n",
            "step: 100, loss: 0.00022452045232057571\n",
            "step: 110, loss: 0.00016333071107510477\n",
            "step: 120, loss: 0.07452871650457382\n",
            "step: 130, loss: 0.0003526149957906455\n",
            "step: 140, loss: 0.00013668743486050516\n",
            "step: 150, loss: 0.00022308275219984353\n",
            "step: 160, loss: 0.0004102396487724036\n",
            "step: 170, loss: 0.0002238692541141063\n",
            "step: 180, loss: 0.0020769331604242325\n",
            "step: 190, loss: 0.000748312333598733\n",
            "step: 200, loss: 0.0004767839564010501\n",
            "step: 210, loss: 0.0027972201351076365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5515695067264574, f1=0.5135135135135135, best_f1=0.5720430107526882\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 460.10it/s]\n",
            "load_f1 = 0.605543710021322\n",
            "real_f1 = 0.610752688172043\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:15, 277.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2451bc29-bf1d-4b20-a9fc-d946296148e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.551241934299469\n",
            "step: 10, loss: 0.3442268967628479\n",
            "step: 20, loss: 0.2754955589771271\n",
            "step: 30, loss: 0.42331406474113464\n",
            "step: 40, loss: 0.4329490661621094\n",
            "step: 50, loss: 0.3074737787246704\n",
            "step: 60, loss: 0.32702839374542236\n",
            "step: 70, loss: 0.28265485167503357\n",
            "step: 80, loss: 0.21767285466194153\n",
            "step: 90, loss: 0.2704833149909973\n",
            "step: 100, loss: 0.3323630094528198\n",
            "step: 110, loss: 0.4107508659362793\n",
            "step: 120, loss: 0.21550558507442474\n",
            "step: 130, loss: 0.2025929093360901\n",
            "step: 140, loss: 0.09187358617782593\n",
            "step: 150, loss: 0.20400075614452362\n",
            "step: 160, loss: 0.21865563094615936\n",
            "step: 170, loss: 0.21478836238384247\n",
            "step: 180, loss: 0.05956636369228363\n",
            "step: 190, loss: 0.29916244745254517\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.603448275862069, f1=0.6049046321525886, best_f1=0.6049046321525886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26918888092041016\n",
            "step: 10, loss: 0.11929988861083984\n",
            "step: 20, loss: 0.13446810841560364\n",
            "step: 30, loss: 0.1553235650062561\n",
            "step: 40, loss: 0.22984234988689423\n",
            "step: 50, loss: 0.21607540547847748\n",
            "step: 60, loss: 0.45593971014022827\n",
            "step: 70, loss: 0.1992189586162567\n",
            "step: 80, loss: 0.1565658301115036\n",
            "step: 90, loss: 0.1664285659790039\n",
            "step: 100, loss: 0.03060387447476387\n",
            "step: 110, loss: 0.12745165824890137\n",
            "step: 120, loss: 0.3647100627422333\n",
            "step: 130, loss: 0.22301313281059265\n",
            "step: 140, loss: 0.1838911920785904\n",
            "step: 150, loss: 0.17188382148742676\n",
            "step: 160, loss: 0.0469038300216198\n",
            "step: 170, loss: 0.15868493914604187\n",
            "step: 180, loss: 0.1781843602657318\n",
            "step: 190, loss: 0.09572729468345642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7853403141361257, f1=0.7475247524752476, best_f1=0.7475247524752476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.040263425558805466\n",
            "step: 10, loss: 0.41498517990112305\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.10758513957262039\n",
            "step: 30, loss: 0.0921429693698883\n",
            "step: 40, loss: 0.08793646842241287\n",
            "step: 50, loss: 0.28443047404289246\n",
            "step: 60, loss: 0.03144243359565735\n",
            "step: 70, loss: 0.19332902133464813\n",
            "step: 80, loss: 0.10272983461618423\n",
            "step: 90, loss: 0.12993299961090088\n",
            "step: 100, loss: 0.010468574240803719\n",
            "step: 110, loss: 0.014572306536138058\n",
            "step: 120, loss: 0.08075951039791107\n",
            "step: 130, loss: 0.020747289061546326\n",
            "step: 140, loss: 0.02424142323434353\n",
            "step: 150, loss: 0.17717473208904266\n",
            "step: 160, loss: 0.048288438469171524\n",
            "step: 170, loss: 0.0870194062590599\n",
            "step: 180, loss: 0.04472934454679489\n",
            "step: 190, loss: 0.09943251311779022\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.784, f1=0.7923497267759563, best_f1=0.7475247524752476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012507784180343151\n",
            "step: 10, loss: 0.1455596685409546\n",
            "step: 20, loss: 0.02687423676252365\n",
            "step: 30, loss: 0.03559611737728119\n",
            "step: 40, loss: 0.04662146791815758\n",
            "step: 50, loss: 0.027737567201256752\n",
            "step: 60, loss: 0.015153501182794571\n",
            "step: 70, loss: 0.12479528784751892\n",
            "step: 80, loss: 0.25576332211494446\n",
            "step: 90, loss: 0.017922915518283844\n",
            "step: 100, loss: 0.0168040469288826\n",
            "step: 110, loss: 0.005659319460391998\n",
            "step: 120, loss: 0.13925430178642273\n",
            "step: 130, loss: 0.13123562932014465\n",
            "step: 140, loss: 0.05354843661189079\n",
            "step: 150, loss: 0.11001986265182495\n",
            "step: 160, loss: 0.025357360020279884\n",
            "step: 170, loss: 0.21307343244552612\n",
            "step: 180, loss: 0.02536970004439354\n",
            "step: 190, loss: 0.16815361380577087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7602905569007263, f1=0.762589928057554, best_f1=0.7475247524752476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07823115587234497\n",
            "step: 10, loss: 0.022330613806843758\n",
            "step: 20, loss: 0.10472246259450912\n",
            "step: 30, loss: 0.008697472512722015\n",
            "step: 40, loss: 0.15376994013786316\n",
            "step: 50, loss: 0.15108168125152588\n",
            "step: 60, loss: 0.11902771890163422\n",
            "step: 70, loss: 0.03295550495386124\n",
            "step: 80, loss: 0.007712875492870808\n",
            "step: 90, loss: 0.022803623229265213\n",
            "step: 100, loss: 0.022391576319932938\n",
            "step: 110, loss: 0.005040720105171204\n",
            "step: 120, loss: 0.00699401693418622\n",
            "step: 130, loss: 0.10202246159315109\n",
            "step: 140, loss: 0.020743347704410553\n",
            "step: 150, loss: 0.07879636436700821\n",
            "step: 160, loss: 0.14121828973293304\n",
            "step: 170, loss: 0.003691786900162697\n",
            "step: 180, loss: 0.18582087755203247\n",
            "step: 190, loss: 0.052441082894802094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7887323943661971, f1=0.8057142857142858, best_f1=0.8057142857142858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020304309204220772\n",
            "step: 10, loss: 0.07706576585769653\n",
            "step: 20, loss: 0.003451837692409754\n",
            "step: 30, loss: 0.007443738169968128\n",
            "step: 40, loss: 0.06954097747802734\n",
            "step: 50, loss: 0.11632711440324783\n",
            "step: 60, loss: 0.007442977745085955\n",
            "step: 70, loss: 0.004386697895824909\n",
            "step: 80, loss: 0.20977771282196045\n",
            "step: 90, loss: 0.053717635571956635\n",
            "step: 100, loss: 0.002976436633616686\n",
            "step: 110, loss: 0.09932296723127365\n",
            "step: 120, loss: 0.29791128635406494\n",
            "step: 130, loss: 0.031226173043251038\n",
            "step: 140, loss: 0.044157758355140686\n",
            "step: 150, loss: 0.06911744922399521\n",
            "step: 160, loss: 0.019175520166754723\n",
            "step: 170, loss: 0.1988099366426468\n",
            "step: 180, loss: 0.02370123378932476\n",
            "step: 190, loss: 0.018042566254734993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7988826815642459, f1=0.8067226890756303, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0040992246940732\n",
            "step: 10, loss: 0.02919011190533638\n",
            "step: 20, loss: 0.15692676603794098\n",
            "step: 30, loss: 0.10515639185905457\n",
            "step: 40, loss: 0.02144516259431839\n",
            "step: 50, loss: 0.10764260590076447\n",
            "step: 60, loss: 0.04216141253709793\n",
            "step: 70, loss: 0.021687153726816177\n",
            "step: 80, loss: 0.013223519548773766\n",
            "step: 90, loss: 0.01676744408905506\n",
            "step: 100, loss: 0.006197718437761068\n",
            "step: 110, loss: 0.10050424933433533\n",
            "step: 120, loss: 0.003146782983094454\n",
            "step: 130, loss: 0.0017813274171203375\n",
            "step: 140, loss: 0.0016747668851166964\n",
            "step: 150, loss: 0.02229686826467514\n",
            "step: 160, loss: 0.07272855937480927\n",
            "step: 170, loss: 0.0014064250281080604\n",
            "step: 180, loss: 0.016249293461441994\n",
            "step: 190, loss: 0.0018115765415132046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7804878048780487, f1=0.8319559228650137, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0787472352385521\n",
            "step: 10, loss: 0.0026257894933223724\n",
            "step: 20, loss: 0.0020388017874211073\n",
            "step: 30, loss: 0.1274370700120926\n",
            "step: 40, loss: 0.0006861822912469506\n",
            "step: 50, loss: 0.00102017552126199\n",
            "step: 60, loss: 0.0012811095220968127\n",
            "step: 70, loss: 0.010284355841577053\n",
            "step: 80, loss: 0.0002576157567091286\n",
            "step: 90, loss: 0.010031024925410748\n",
            "step: 100, loss: 0.03209471330046654\n",
            "step: 110, loss: 0.0005473730270750821\n",
            "step: 120, loss: 0.00034864956978708506\n",
            "step: 130, loss: 0.008524986915290356\n",
            "step: 140, loss: 0.007316168397665024\n",
            "step: 150, loss: 0.0058969114907085896\n",
            "step: 160, loss: 0.010681867599487305\n",
            "step: 170, loss: 0.11750589311122894\n",
            "step: 180, loss: 0.010059828869998455\n",
            "step: 190, loss: 0.040114372968673706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7696202531645571, f1=0.7783505154639175, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009820666164159775\n",
            "step: 10, loss: 0.13492487370967865\n",
            "step: 20, loss: 0.005688570439815521\n",
            "step: 30, loss: 0.0006328211165964603\n",
            "step: 40, loss: 0.00035711549571715295\n",
            "step: 50, loss: 0.001871917163953185\n",
            "step: 60, loss: 0.00160794158000499\n",
            "step: 70, loss: 0.0002917067613452673\n",
            "step: 80, loss: 0.002018687780946493\n",
            "step: 90, loss: 0.0032091462053358555\n",
            "step: 100, loss: 0.004095065873116255\n",
            "step: 110, loss: 0.0028945605736225843\n",
            "step: 120, loss: 0.03197278082370758\n",
            "step: 130, loss: 0.0017782892100512981\n",
            "step: 140, loss: 0.0007806222420185804\n",
            "step: 150, loss: 0.0033182206097990274\n",
            "step: 160, loss: 0.0008195765549317002\n",
            "step: 170, loss: 0.00284725078381598\n",
            "step: 180, loss: 0.002130824141204357\n",
            "step: 190, loss: 0.012821988202631474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7720207253886011, f1=0.7803617571059432, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011551848147064447\n",
            "step: 10, loss: 0.0006589835975319147\n",
            "step: 20, loss: 0.001139806816354394\n",
            "step: 30, loss: 0.0006953968550078571\n",
            "step: 40, loss: 0.018318425863981247\n",
            "step: 50, loss: 0.0059136259369552135\n",
            "step: 60, loss: 0.00034705453435890377\n",
            "step: 70, loss: 0.0004894059966318309\n",
            "step: 80, loss: 0.0002386409614700824\n",
            "step: 90, loss: 0.0016448753885924816\n",
            "step: 100, loss: 0.0002094080700771883\n",
            "step: 110, loss: 0.0020400627981871367\n",
            "step: 120, loss: 0.07876306772232056\n",
            "step: 130, loss: 0.0020348099060356617\n",
            "step: 140, loss: 0.0732012391090393\n",
            "step: 150, loss: 0.0945490151643753\n",
            "step: 160, loss: 0.010322893969714642\n",
            "step: 170, loss: 0.000823065871372819\n",
            "step: 180, loss: 0.001282755401916802\n",
            "step: 190, loss: 0.012327992357313633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7843137254901961, f1=0.8133704735376045, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000496394990477711\n",
            "step: 10, loss: 0.003897612914443016\n",
            "step: 20, loss: 0.0007758729625493288\n",
            "step: 30, loss: 0.0040021538734436035\n",
            "step: 40, loss: 0.00040541545604355633\n",
            "step: 50, loss: 0.004107883665710688\n",
            "step: 60, loss: 0.0077058179304003716\n",
            "step: 70, loss: 0.000767529709264636\n",
            "step: 80, loss: 0.0002465029829181731\n",
            "step: 90, loss: 0.0001178368620458059\n",
            "step: 100, loss: 0.00694124074652791\n",
            "step: 110, loss: 0.00023470654559787363\n",
            "step: 120, loss: 0.0002704511280171573\n",
            "step: 130, loss: 0.0020691603422164917\n",
            "step: 140, loss: 0.00033130444353446364\n",
            "step: 150, loss: 0.00022390068625099957\n",
            "step: 160, loss: 0.0019891206175088882\n",
            "step: 170, loss: 0.00014251991524361074\n",
            "step: 180, loss: 0.002291143173351884\n",
            "step: 190, loss: 0.010583478026092052\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.79155672823219, f1=0.8074866310160429, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011895773059222847\n",
            "step: 10, loss: 0.008260332979261875\n",
            "step: 20, loss: 0.00020943333220202476\n",
            "step: 30, loss: 0.00019653482013382018\n",
            "step: 40, loss: 0.0013473685830831528\n",
            "step: 50, loss: 0.0005141290603205562\n",
            "step: 60, loss: 0.003715305356308818\n",
            "step: 70, loss: 0.00012155337753938511\n",
            "step: 80, loss: 0.00014821565127931535\n",
            "step: 90, loss: 0.00011065641592722386\n",
            "step: 100, loss: 0.00016515652532689273\n",
            "step: 110, loss: 0.0002605137415230274\n",
            "step: 120, loss: 0.00011150308273499832\n",
            "step: 130, loss: 0.0019621264655143023\n",
            "step: 140, loss: 0.05533221364021301\n",
            "step: 150, loss: 0.00302359601482749\n",
            "step: 160, loss: 6.909709918545559e-05\n",
            "step: 170, loss: 0.0016791446832939982\n",
            "step: 180, loss: 8.491877815686166e-05\n",
            "step: 190, loss: 0.00027892139041796327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7819148936170213, f1=0.8074866310160429, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021661966457031667\n",
            "step: 10, loss: 0.006949624978005886\n",
            "step: 20, loss: 0.0006368350004777312\n",
            "step: 30, loss: 0.0005057216621935368\n",
            "step: 40, loss: 0.001504602376371622\n",
            "step: 50, loss: 0.0004919660859741271\n",
            "step: 60, loss: 0.00021544632909353822\n",
            "step: 70, loss: 0.011352067813277245\n",
            "step: 80, loss: 0.0014820432988926768\n",
            "step: 90, loss: 0.0008096472593024373\n",
            "step: 100, loss: 0.0003163034562021494\n",
            "step: 110, loss: 0.001254456932656467\n",
            "step: 120, loss: 0.002014310797676444\n",
            "step: 130, loss: 0.00016114956815727055\n",
            "step: 140, loss: 0.00015231952420435846\n",
            "step: 150, loss: 0.0001448726252419874\n",
            "step: 160, loss: 0.0002599644649308175\n",
            "step: 170, loss: 0.00013737894187215716\n",
            "step: 180, loss: 0.00010899094195337966\n",
            "step: 190, loss: 0.0035384241491556168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7927461139896373, f1=0.8041775456919059, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017575712408870459\n",
            "step: 10, loss: 0.00010057335020974278\n",
            "step: 20, loss: 0.00023732347472105175\n",
            "step: 30, loss: 0.00013606117863673717\n",
            "step: 40, loss: 0.005826808512210846\n",
            "step: 50, loss: 0.00024187754024751484\n",
            "step: 60, loss: 0.0024171287659555674\n",
            "step: 70, loss: 0.00014441073290072381\n",
            "step: 80, loss: 9.79569012997672e-05\n",
            "step: 90, loss: 0.015694869682192802\n",
            "step: 100, loss: 0.0011931498302146792\n",
            "step: 110, loss: 0.0008432154427282512\n",
            "step: 120, loss: 0.0006743901176378131\n",
            "step: 130, loss: 0.00017030244634952396\n",
            "step: 140, loss: 0.00013509979180525988\n",
            "step: 150, loss: 0.000307488429825753\n",
            "step: 160, loss: 0.003292315872386098\n",
            "step: 170, loss: 0.0001792072580428794\n",
            "step: 180, loss: 0.00037515279836952686\n",
            "step: 190, loss: 0.00040341776912100613\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7886597938144331, f1=0.8031914893617021, best_f1=0.8067226890756303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030568061396479607\n",
            "step: 10, loss: 0.00012478334247134626\n",
            "step: 20, loss: 0.00027884787414222956\n",
            "step: 30, loss: 7.549197471234947e-05\n",
            "step: 40, loss: 0.00028444372583180666\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.0004198344540782273\n",
            "step: 60, loss: 0.0005897119990549982\n",
            "step: 70, loss: 0.00333242304623127\n",
            "step: 80, loss: 0.00019406255159992725\n",
            "step: 90, loss: 0.00015144811186473817\n",
            "step: 100, loss: 0.0034373304806649685\n",
            "step: 110, loss: 0.00027443678118288517\n",
            "step: 120, loss: 0.00019577248895075172\n",
            "step: 130, loss: 0.00011307690147077665\n",
            "step: 140, loss: 0.0004192823253106326\n",
            "step: 150, loss: 0.0004650400369428098\n",
            "step: 160, loss: 0.00039087337790988386\n",
            "step: 170, loss: 0.0001086026313714683\n",
            "step: 180, loss: 0.0033409115858376026\n",
            "step: 190, loss: 0.001903884345665574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7877237851662403, f1=0.7989556135770235, best_f1=0.8067226890756303\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 236.24it/s]\n",
            "load_f1 = 0.7888888888888889\n",
            "real_f1 = 0.7922437673130193\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 269.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17600101-b307-48d2-fb33-2ff8c7ed1dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6234651207923889\n",
            "step: 10, loss: 0.3660033345222473\n",
            "step: 20, loss: 0.3052654564380646\n",
            "step: 30, loss: 0.3816978335380554\n",
            "step: 40, loss: 0.2901850640773773\n",
            "step: 50, loss: 0.23998615145683289\n",
            "step: 60, loss: 0.24358031153678894\n",
            "step: 70, loss: 0.359989732503891\n",
            "step: 80, loss: 0.33422017097473145\n",
            "step: 90, loss: 0.21354743838310242\n",
            "step: 100, loss: 0.1895367056131363\n",
            "step: 110, loss: 0.2970481514930725\n",
            "step: 120, loss: 0.23095208406448364\n",
            "step: 130, loss: 0.06381166726350784\n",
            "step: 140, loss: 0.17095507681369781\n",
            "step: 150, loss: 0.40349850058555603\n",
            "step: 160, loss: 0.1441141664981842\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.27909743785858154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6741071428571428, f1=0.6582278481012658, best_f1=0.6582278481012658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16510754823684692\n",
            "step: 10, loss: 0.14475712180137634\n",
            "step: 20, loss: 0.12593019008636475\n",
            "step: 30, loss: 0.3485892117023468\n",
            "step: 40, loss: 0.11496519297361374\n",
            "step: 50, loss: 0.022241033613681793\n",
            "step: 60, loss: 0.065445676445961\n",
            "step: 70, loss: 0.09229028224945068\n",
            "step: 80, loss: 0.08351104706525803\n",
            "step: 90, loss: 0.07766825705766678\n",
            "step: 100, loss: 0.20404177904129028\n",
            "step: 110, loss: 0.02346600592136383\n",
            "step: 120, loss: 0.07156392931938171\n",
            "step: 130, loss: 0.07365410774946213\n",
            "step: 140, loss: 0.2606634497642517\n",
            "step: 150, loss: 0.4033121168613434\n",
            "step: 160, loss: 0.15904513001441956\n",
            "step: 170, loss: 0.0524023175239563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7536945812807883, f1=0.7547169811320755, best_f1=0.7547169811320755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06392024457454681\n",
            "step: 10, loss: 0.027947118505835533\n",
            "step: 20, loss: 0.028427081182599068\n",
            "step: 30, loss: 0.0853901132941246\n",
            "step: 40, loss: 0.078388512134552\n",
            "step: 50, loss: 0.08903764933347702\n",
            "step: 60, loss: 0.07479194551706314\n",
            "step: 70, loss: 0.15792442858219147\n",
            "step: 80, loss: 0.07438082993030548\n",
            "step: 90, loss: 0.16768231987953186\n",
            "step: 100, loss: 0.004501659423112869\n",
            "step: 110, loss: 0.06917589902877808\n",
            "step: 120, loss: 0.09151157736778259\n",
            "step: 130, loss: 0.1701078563928604\n",
            "step: 140, loss: 0.04875453934073448\n",
            "step: 150, loss: 0.04042021930217743\n",
            "step: 160, loss: 0.044332873076200485\n",
            "step: 170, loss: 0.21440911293029785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7397959183673468, f1=0.7462686567164178, best_f1=0.7547169811320755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01219121553003788\n",
            "step: 10, loss: 0.18438521027565002\n",
            "step: 20, loss: 0.005622598342597485\n",
            "step: 30, loss: 0.07013305276632309\n",
            "step: 40, loss: 0.0019618633668869734\n",
            "step: 50, loss: 0.09663866460323334\n",
            "step: 60, loss: 0.44163286685943604\n",
            "step: 70, loss: 0.0038427787367254496\n",
            "step: 80, loss: 0.1069841980934143\n",
            "step: 90, loss: 0.06874772161245346\n",
            "step: 100, loss: 0.18523643910884857\n",
            "step: 110, loss: 0.06850843131542206\n",
            "step: 120, loss: 0.004783527925610542\n",
            "step: 130, loss: 0.04360024631023407\n",
            "step: 140, loss: 0.07346707582473755\n",
            "step: 150, loss: 0.04748911038041115\n",
            "step: 160, loss: 0.1131085455417633\n",
            "step: 170, loss: 0.04606867954134941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7581047381546135, f1=0.800982800982801, best_f1=0.800982800982801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006808717735111713\n",
            "step: 10, loss: 0.02426321618258953\n",
            "step: 20, loss: 0.01837329939007759\n",
            "step: 30, loss: 0.13374827802181244\n",
            "step: 40, loss: 0.02090870402753353\n",
            "step: 50, loss: 0.14109748601913452\n",
            "step: 60, loss: 0.07171876728534698\n",
            "step: 70, loss: 0.3178672790527344\n",
            "step: 80, loss: 0.07979930937290192\n",
            "step: 90, loss: 0.07838863134384155\n",
            "step: 100, loss: 0.0434732548892498\n",
            "step: 110, loss: 0.09291849285364151\n",
            "step: 120, loss: 0.03749977424740791\n",
            "step: 130, loss: 0.04461924731731415\n",
            "step: 140, loss: 0.06415316462516785\n",
            "step: 150, loss: 0.02606600522994995\n",
            "step: 160, loss: 0.054839469492435455\n",
            "step: 170, loss: 0.020643463358283043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7399463806970509, f1=0.7792207792207791, best_f1=0.800982800982801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016711367294192314\n",
            "step: 10, loss: 0.0032185146119445562\n",
            "step: 20, loss: 0.014074756763875484\n",
            "step: 30, loss: 0.0033370519522577524\n",
            "step: 40, loss: 0.00851406529545784\n",
            "step: 50, loss: 0.10108137875795364\n",
            "step: 60, loss: 0.044846758246421814\n",
            "step: 70, loss: 0.13703197240829468\n",
            "step: 80, loss: 0.19399313628673553\n",
            "step: 90, loss: 0.029525576159358025\n",
            "step: 100, loss: 0.015259717591106892\n",
            "step: 110, loss: 0.0012821122072637081\n",
            "step: 120, loss: 0.01214972510933876\n",
            "step: 130, loss: 0.010121370665729046\n",
            "step: 140, loss: 0.011471251025795937\n",
            "step: 150, loss: 0.04914963245391846\n",
            "step: 160, loss: 0.08880621194839478\n",
            "step: 170, loss: 0.007943261414766312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7318181818181818, f1=0.7418655097613883, best_f1=0.800982800982801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014860739465802908\n",
            "step: 10, loss: 0.005053987726569176\n",
            "step: 20, loss: 0.02337154559791088\n",
            "step: 30, loss: 0.01364572811871767\n",
            "step: 40, loss: 0.011544949375092983\n",
            "step: 50, loss: 0.011757755652070045\n",
            "step: 60, loss: 0.002221804577857256\n",
            "step: 70, loss: 0.00029469977016560733\n",
            "step: 80, loss: 0.02614353969693184\n",
            "step: 90, loss: 0.0004565906128846109\n",
            "step: 100, loss: 0.24261701107025146\n",
            "step: 110, loss: 0.006531793624162674\n",
            "step: 120, loss: 0.0018170720431953669\n",
            "step: 130, loss: 0.13500452041625977\n",
            "step: 140, loss: 0.002355113159865141\n",
            "step: 150, loss: 0.006774050183594227\n",
            "step: 160, loss: 0.00045016148942522705\n",
            "step: 170, loss: 0.024367090314626694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7286063569682152, f1=0.7440758293838862, best_f1=0.800982800982801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03718109428882599\n",
            "step: 10, loss: 0.0008889214368537068\n",
            "step: 20, loss: 0.008922005072236061\n",
            "step: 30, loss: 0.0399584136903286\n",
            "step: 40, loss: 0.00027961324667558074\n",
            "step: 50, loss: 0.0011420889059081674\n",
            "step: 60, loss: 0.00612499937415123\n",
            "step: 70, loss: 0.0005624795448966324\n",
            "step: 80, loss: 0.007005623076111078\n",
            "step: 90, loss: 0.0008221374009735882\n",
            "step: 100, loss: 0.07037787139415741\n",
            "step: 110, loss: 0.05091012641787529\n",
            "step: 120, loss: 0.2472604364156723\n",
            "step: 130, loss: 0.0028903144411742687\n",
            "step: 140, loss: 0.0019419884774833918\n",
            "step: 150, loss: 0.02345333620905876\n",
            "step: 160, loss: 0.00764778396114707\n",
            "step: 170, loss: 0.005398216191679239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7594339622641509, f1=0.7586206896551724, best_f1=0.7586206896551724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006221872754395008\n",
            "step: 10, loss: 0.0138468649238348\n",
            "step: 20, loss: 0.0025925994850695133\n",
            "step: 30, loss: 0.017405645921826363\n",
            "step: 40, loss: 0.0030084867030382156\n",
            "step: 50, loss: 0.00029376090969890356\n",
            "step: 60, loss: 0.00566469132900238\n",
            "step: 70, loss: 0.07225128263235092\n",
            "step: 80, loss: 0.005776825360953808\n",
            "step: 90, loss: 0.0362560898065567\n",
            "step: 100, loss: 0.0010181987890973687\n",
            "step: 110, loss: 0.0005575891118496656\n",
            "step: 120, loss: 0.008736607618629932\n",
            "step: 130, loss: 0.07679009437561035\n",
            "step: 140, loss: 0.0007005081279203296\n",
            "step: 150, loss: 0.0012759589590132236\n",
            "step: 160, loss: 0.03539247810840607\n",
            "step: 170, loss: 0.08338268846273422\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7634408602150538, f1=0.781725888324873, best_f1=0.781725888324873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05263035371899605\n",
            "step: 10, loss: 0.004972298163920641\n",
            "step: 20, loss: 0.027131337672472\n",
            "step: 30, loss: 0.0008762114448472857\n",
            "step: 40, loss: 0.011385058052837849\n",
            "step: 50, loss: 0.03956514596939087\n",
            "step: 60, loss: 0.002766585675999522\n",
            "step: 70, loss: 0.002285279333591461\n",
            "step: 80, loss: 0.006619642488658428\n",
            "step: 90, loss: 0.011255850084125996\n",
            "step: 100, loss: 0.0003723430563695729\n",
            "step: 110, loss: 0.005823239218443632\n",
            "step: 120, loss: 0.000856635335367173\n",
            "step: 130, loss: 0.01514729019254446\n",
            "step: 140, loss: 0.008625940419733524\n",
            "step: 150, loss: 0.0055745383724570274\n",
            "step: 160, loss: 0.004528796300292015\n",
            "step: 170, loss: 0.0007149585289880633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7666666666666666, f1=0.771764705882353, best_f1=0.771764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03601359575986862\n",
            "step: 10, loss: 0.02459665946662426\n",
            "step: 20, loss: 0.012121854349970818\n",
            "step: 30, loss: 0.00034163129748776555\n",
            "step: 40, loss: 0.001837565447203815\n",
            "step: 50, loss: 0.0035295295529067516\n",
            "step: 60, loss: 0.01546016987413168\n",
            "step: 70, loss: 0.00010267527250107378\n",
            "step: 80, loss: 0.00012374226935207844\n",
            "step: 90, loss: 0.00017265968199353665\n",
            "step: 100, loss: 0.0001422372879460454\n",
            "step: 110, loss: 0.016332248225808144\n",
            "step: 120, loss: 0.002662380924448371\n",
            "step: 130, loss: 0.0003295237256679684\n",
            "step: 140, loss: 0.11095841228961945\n",
            "step: 150, loss: 0.11507043242454529\n",
            "step: 160, loss: 0.019585512578487396\n",
            "step: 170, loss: 0.004997307434678078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7572815533980582, f1=0.7540983606557378, best_f1=0.771764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014192926464602351\n",
            "step: 10, loss: 0.0007118404610082507\n",
            "step: 20, loss: 0.00046362928696908057\n",
            "step: 30, loss: 0.0001835105795180425\n",
            "step: 40, loss: 0.0002063263236777857\n",
            "step: 50, loss: 0.00010488492989679798\n",
            "step: 60, loss: 0.00017516470688860863\n",
            "step: 70, loss: 0.11211751401424408\n",
            "step: 80, loss: 0.00013388645311351866\n",
            "step: 90, loss: 0.00048263900680467486\n",
            "step: 100, loss: 0.0036939906422048807\n",
            "step: 110, loss: 0.00041188058094121516\n",
            "step: 120, loss: 0.17406325042247772\n",
            "step: 130, loss: 0.0006461957818828523\n",
            "step: 140, loss: 0.002347957342863083\n",
            "step: 150, loss: 0.004322394262999296\n",
            "step: 160, loss: 0.000948545231949538\n",
            "step: 170, loss: 0.00027815226349048316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7506297229219144, f1=0.7745098039215688, best_f1=0.771764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003950126003473997\n",
            "step: 10, loss: 0.0011578170815482736\n",
            "step: 20, loss: 0.11389105021953583\n",
            "step: 30, loss: 0.00014665478374809027\n",
            "step: 40, loss: 0.005711263045668602\n",
            "step: 50, loss: 0.00019099605560768396\n",
            "step: 60, loss: 0.0017486430006101727\n",
            "step: 70, loss: 0.0008412525057792664\n",
            "step: 80, loss: 0.0007300340221263468\n",
            "step: 90, loss: 0.0001155482605099678\n",
            "step: 100, loss: 0.00022515324235428125\n",
            "step: 110, loss: 0.00010943094093818218\n",
            "step: 120, loss: 0.007887208834290504\n",
            "step: 130, loss: 0.00047874372103251517\n",
            "step: 140, loss: 0.0003838582488242537\n",
            "step: 150, loss: 0.00391613133251667\n",
            "step: 160, loss: 0.001005985657684505\n",
            "step: 170, loss: 0.002935280092060566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7575757575757577, f1=0.7853658536585365, best_f1=0.771764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00028780687716789544\n",
            "step: 10, loss: 0.00011592009832384065\n",
            "step: 20, loss: 0.00020596683316398412\n",
            "step: 30, loss: 0.0002020819520112127\n",
            "step: 40, loss: 0.00023810061975382268\n",
            "step: 50, loss: 0.00023026936105452478\n",
            "step: 60, loss: 0.00018723808170761913\n",
            "step: 70, loss: 0.00023086011060513556\n",
            "step: 80, loss: 0.000656842952594161\n",
            "step: 90, loss: 0.0001281618169741705\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.0029067588038742542\n",
            "step: 110, loss: 0.0004798480949830264\n",
            "step: 120, loss: 0.0014679552987217903\n",
            "step: 130, loss: 0.0007649438921362162\n",
            "step: 140, loss: 0.0003075620043091476\n",
            "step: 150, loss: 8.468585292575881e-05\n",
            "step: 160, loss: 0.00012308389705140144\n",
            "step: 170, loss: 0.00042754338937811553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.742857142857143, f1=0.7791563275434243, best_f1=0.771764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004946084809489548\n",
            "step: 10, loss: 0.0011764372466132045\n",
            "step: 20, loss: 0.002080217469483614\n",
            "step: 30, loss: 0.00031602371018379927\n",
            "step: 40, loss: 0.002493385225534439\n",
            "step: 50, loss: 0.00019321056606713682\n",
            "step: 60, loss: 0.0019694326911121607\n",
            "step: 70, loss: 9.231232252204791e-05\n",
            "step: 80, loss: 0.0007332575041800737\n",
            "step: 90, loss: 0.00019148230785503983\n",
            "step: 100, loss: 0.041867852210998535\n",
            "step: 110, loss: 0.00013775669503957033\n",
            "step: 120, loss: 0.0017039697850123048\n",
            "step: 130, loss: 0.010080907493829727\n",
            "step: 140, loss: 0.0014891327591612935\n",
            "step: 150, loss: 0.0003527305962052196\n",
            "step: 160, loss: 0.0002127970365108922\n",
            "step: 170, loss: 0.00022153090685606003\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7450980392156863, f1=0.776595744680851, best_f1=0.771764705882353\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 267.33it/s]\n",
            "load_f1 = 0.7673860911270983\n",
            "real_f1 = 0.7677725118483412\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 232.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a02519-c9c0-4713-d375-9c239bedb091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6441285014152527\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.6109216213226318\n",
            "step: 20, loss: 0.4959302842617035\n",
            "step: 30, loss: 0.305388867855072\n",
            "step: 40, loss: 0.29025641083717346\n",
            "step: 50, loss: 0.12691541016101837\n",
            "step: 60, loss: 0.25182417035102844\n",
            "step: 70, loss: 0.12026908993721008\n",
            "step: 80, loss: 0.08253105729818344\n",
            "step: 90, loss: 0.14339222013950348\n",
            "step: 100, loss: 0.010541040450334549\n",
            "step: 110, loss: 0.21868067979812622\n",
            "step: 120, loss: 0.03393853083252907\n",
            "step: 130, loss: 0.10634499788284302\n",
            "step: 140, loss: 0.02894618548452854\n",
            "step: 150, loss: 0.09402076154947281\n",
            "step: 160, loss: 0.04380695894360542\n",
            "step: 170, loss: 0.019305579364299774\n",
            "step: 180, loss: 0.04629803076386452\n",
            "step: 190, loss: 0.05862690880894661\n",
            "step: 200, loss: 0.15840040147304535\n",
            "step: 210, loss: 0.06474887579679489\n",
            "step: 220, loss: 0.0303948987275362\n",
            "step: 230, loss: 0.12013732641935349\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9597315436241611, f1=0.9561304836895389, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006581639405339956\n",
            "step: 10, loss: 0.02444346621632576\n",
            "step: 20, loss: 0.3877164125442505\n",
            "step: 30, loss: 0.04399847984313965\n",
            "step: 40, loss: 0.07888227701187134\n",
            "step: 50, loss: 0.015212387777864933\n",
            "step: 60, loss: 0.014061784371733665\n",
            "step: 70, loss: 0.06905391067266464\n",
            "step: 80, loss: 0.007679387461394072\n",
            "step: 90, loss: 0.0432644747197628\n",
            "step: 100, loss: 0.018896058201789856\n",
            "step: 110, loss: 0.07210248708724976\n",
            "step: 120, loss: 0.09167592227458954\n",
            "step: 130, loss: 0.055799294263124466\n",
            "step: 140, loss: 0.019588235765695572\n",
            "step: 150, loss: 0.035338543355464935\n",
            "step: 160, loss: 0.12429646402597427\n",
            "step: 170, loss: 0.0056664771400392056\n",
            "step: 180, loss: 0.08816742151975632\n",
            "step: 190, loss: 0.006213505752384663\n",
            "step: 200, loss: 0.027011172845959663\n",
            "step: 210, loss: 0.0071349297650158405\n",
            "step: 220, loss: 0.05038592219352722\n",
            "step: 230, loss: 0.010538686998188496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9613259668508287, f1=0.9565217391304347, best_f1=0.9565217391304347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05183551087975502\n",
            "step: 10, loss: 0.0073532527312636375\n",
            "step: 20, loss: 0.0779389813542366\n",
            "step: 30, loss: 0.01109229400753975\n",
            "step: 40, loss: 0.12400711327791214\n",
            "step: 50, loss: 0.053965386003255844\n",
            "step: 60, loss: 0.018795805051922798\n",
            "step: 70, loss: 0.027045149356126785\n",
            "step: 80, loss: 0.013146914541721344\n",
            "step: 90, loss: 0.00927499495446682\n",
            "step: 100, loss: 0.005408504977822304\n",
            "step: 110, loss: 0.001675393315963447\n",
            "step: 120, loss: 0.026576122269034386\n",
            "step: 130, loss: 0.0015247536357492208\n",
            "step: 140, loss: 0.013556737452745438\n",
            "step: 150, loss: 0.028170380741357803\n",
            "step: 160, loss: 0.016569755971431732\n",
            "step: 170, loss: 0.005048944614827633\n",
            "step: 180, loss: 0.00506582111120224\n",
            "step: 190, loss: 0.029078125953674316\n",
            "step: 200, loss: 0.002852616598829627\n",
            "step: 210, loss: 0.009671502746641636\n",
            "step: 220, loss: 0.03163892775774002\n",
            "step: 230, loss: 0.0019614044576883316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9696287964004499, f1=0.9683972911963882, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002992290072143078\n",
            "step: 10, loss: 0.011452151462435722\n",
            "step: 20, loss: 0.0017823732923716307\n",
            "step: 30, loss: 0.0007445430965162814\n",
            "step: 40, loss: 0.023545093834400177\n",
            "step: 50, loss: 0.000964981212746352\n",
            "step: 60, loss: 0.020623652264475822\n",
            "step: 70, loss: 0.028081053867936134\n",
            "step: 80, loss: 0.004126721061766148\n",
            "step: 90, loss: 0.008289230987429619\n",
            "step: 100, loss: 0.0030258281622081995\n",
            "step: 110, loss: 0.001063512871041894\n",
            "step: 120, loss: 0.014775888994336128\n",
            "step: 130, loss: 0.008534924127161503\n",
            "step: 140, loss: 0.004071706440299749\n",
            "step: 150, loss: 0.11078827828168869\n",
            "step: 160, loss: 0.003531928174197674\n",
            "step: 170, loss: 0.008117295801639557\n",
            "step: 180, loss: 0.0005587866180576384\n",
            "step: 190, loss: 0.0036432179622352123\n",
            "step: 200, loss: 0.0023055539932101965\n",
            "step: 210, loss: 0.004601523280143738\n",
            "step: 220, loss: 0.0010733745293691754\n",
            "step: 230, loss: 0.01672612689435482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9664429530201343, f1=0.967741935483871, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002645728411152959\n",
            "step: 10, loss: 0.0981890857219696\n",
            "step: 20, loss: 0.0019644177518785\n",
            "step: 30, loss: 0.00853273831307888\n",
            "step: 40, loss: 0.0006614226149395108\n",
            "step: 50, loss: 0.0010820950847119093\n",
            "step: 60, loss: 0.1385490447282791\n",
            "step: 70, loss: 0.02902495674788952\n",
            "step: 80, loss: 0.000605712819378823\n",
            "step: 90, loss: 0.0010530742583796382\n",
            "step: 100, loss: 0.0007243221043609083\n",
            "step: 110, loss: 0.0005063890712335706\n",
            "step: 120, loss: 0.00015138419985305518\n",
            "step: 130, loss: 0.01114693284034729\n",
            "step: 140, loss: 0.0006444664322771132\n",
            "step: 150, loss: 0.023649778217077255\n",
            "step: 160, loss: 0.0003393237420823425\n",
            "step: 170, loss: 0.0008363024680875242\n",
            "step: 180, loss: 0.0007109529688023031\n",
            "step: 190, loss: 0.2134261280298233\n",
            "step: 200, loss: 0.0047856541350483894\n",
            "step: 210, loss: 0.000984686310403049\n",
            "step: 220, loss: 0.0023001490626484156\n",
            "step: 230, loss: 0.0008477923111058772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9657458563535912, f1=0.954895489548955, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003907154779881239\n",
            "step: 10, loss: 0.0007890769629739225\n",
            "step: 20, loss: 0.0018033962696790695\n",
            "step: 30, loss: 0.0006103129708208144\n",
            "step: 40, loss: 0.0002595445257611573\n",
            "step: 50, loss: 0.0011723871575668454\n",
            "step: 60, loss: 0.0002663711493369192\n",
            "step: 70, loss: 0.0019463502103462815\n",
            "step: 80, loss: 0.002356046810746193\n",
            "step: 90, loss: 0.0013341910671442747\n",
            "step: 100, loss: 0.004067517351359129\n",
            "step: 110, loss: 0.002404573140665889\n",
            "step: 120, loss: 0.0019287285394966602\n",
            "step: 130, loss: 0.00025630276650190353\n",
            "step: 140, loss: 0.00018497841665521264\n",
            "step: 150, loss: 0.0005761755164712667\n",
            "step: 160, loss: 0.0007608196465298533\n",
            "step: 170, loss: 0.00022149710275698453\n",
            "step: 180, loss: 0.011092865839600563\n",
            "step: 190, loss: 0.008051086217164993\n",
            "step: 200, loss: 0.00024962707539089024\n",
            "step: 210, loss: 0.0005959462723694742\n",
            "step: 220, loss: 0.0009968926897272468\n",
            "step: 230, loss: 0.03883456066250801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9688195991091313, f1=0.9643652561247216, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002532112877815962\n",
            "step: 10, loss: 0.0003529653185978532\n",
            "step: 20, loss: 0.000301858177408576\n",
            "step: 30, loss: 0.0004533208557404578\n",
            "step: 40, loss: 0.002374847186729312\n",
            "step: 50, loss: 0.0005343423690646887\n",
            "step: 60, loss: 0.0004313638783060014\n",
            "step: 70, loss: 0.0004896354512311518\n",
            "step: 80, loss: 0.0008418255602009594\n",
            "step: 90, loss: 0.0023588119074702263\n",
            "step: 100, loss: 0.00040800878196023405\n",
            "step: 110, loss: 0.026280928403139114\n",
            "step: 120, loss: 0.00040508172241970897\n",
            "step: 130, loss: 0.0001363862829748541\n",
            "step: 140, loss: 0.0011369847925379872\n",
            "step: 150, loss: 0.0009126010118052363\n",
            "step: 160, loss: 0.033131301403045654\n",
            "step: 170, loss: 0.01001206785440445\n",
            "step: 180, loss: 0.0017610522918403149\n",
            "step: 190, loss: 0.0004554844636004418\n",
            "step: 200, loss: 0.013862443156540394\n",
            "step: 210, loss: 0.00022463641653303057\n",
            "step: 220, loss: 0.0004381001053843647\n",
            "step: 230, loss: 0.00035338912857696414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9596412556053813, f1=0.9562289562289561, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021637449390254915\n",
            "step: 10, loss: 0.0004878729523625225\n",
            "step: 20, loss: 0.05729437619447708\n",
            "step: 30, loss: 0.0022415448911488056\n",
            "step: 40, loss: 0.009536578319966793\n",
            "step: 50, loss: 0.0005912721389904618\n",
            "step: 60, loss: 0.0001943773531820625\n",
            "step: 70, loss: 0.00043002620805054903\n",
            "step: 80, loss: 0.0004319207218941301\n",
            "step: 90, loss: 0.0001775069977156818\n",
            "step: 100, loss: 0.00016062681970652193\n",
            "step: 110, loss: 0.000540036300662905\n",
            "step: 120, loss: 0.004338833969086409\n",
            "step: 130, loss: 0.0005393823958002031\n",
            "step: 140, loss: 0.0029930558521300554\n",
            "step: 150, loss: 0.00011765466479118913\n",
            "step: 160, loss: 0.0005368213169276714\n",
            "step: 170, loss: 0.00038687564665451646\n",
            "step: 180, loss: 0.02079421654343605\n",
            "step: 190, loss: 0.0007685561431571841\n",
            "step: 200, loss: 0.0003082443727180362\n",
            "step: 210, loss: 0.0007295868126675487\n",
            "step: 220, loss: 0.00029179290868341923\n",
            "step: 230, loss: 0.0002504356671124697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9642857142857144, f1=0.9608938547486034, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018132510012947023\n",
            "step: 10, loss: 0.0003644790267571807\n",
            "step: 20, loss: 0.00040490340325050056\n",
            "step: 30, loss: 0.00013686400779988617\n",
            "step: 40, loss: 0.04987344518303871\n",
            "step: 50, loss: 0.0005689330282621086\n",
            "step: 60, loss: 0.0002453587076161057\n",
            "step: 70, loss: 0.0009130765683948994\n",
            "step: 80, loss: 0.00020062307885382324\n",
            "step: 90, loss: 0.000441663694800809\n",
            "step: 100, loss: 0.0004454999871086329\n",
            "step: 110, loss: 9.665799734648317e-05\n",
            "step: 120, loss: 9.135348227573559e-05\n",
            "step: 130, loss: 0.0001335780107183382\n",
            "step: 140, loss: 9.737162326928228e-05\n",
            "step: 150, loss: 0.013332456350326538\n",
            "step: 160, loss: 0.00013515830505639315\n",
            "step: 170, loss: 0.00015856184472795576\n",
            "step: 180, loss: 0.00247013452462852\n",
            "step: 190, loss: 0.00017068098532035947\n",
            "step: 200, loss: 9.750050230650231e-05\n",
            "step: 210, loss: 5.827046697959304e-05\n",
            "step: 220, loss: 0.0001055106331477873\n",
            "step: 230, loss: 0.0001484119420638308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9679558011049725, f1=0.9621380846325166, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.864873234415427e-05\n",
            "step: 10, loss: 5.82476714043878e-05\n",
            "step: 20, loss: 7.092941086739302e-05\n",
            "step: 30, loss: 8.958144462667406e-05\n",
            "step: 40, loss: 0.00011541189451236278\n",
            "step: 50, loss: 0.0005322793149389327\n",
            "step: 60, loss: 0.0001023961667669937\n",
            "step: 70, loss: 0.001350433100014925\n",
            "step: 80, loss: 0.000123656791402027\n",
            "step: 90, loss: 0.00011586394248297438\n",
            "step: 100, loss: 8.240182069130242e-05\n",
            "step: 110, loss: 0.0008995484095066786\n",
            "step: 120, loss: 0.00034192309249192476\n",
            "step: 130, loss: 0.00012951833195984364\n",
            "step: 140, loss: 0.03884255513548851\n",
            "step: 150, loss: 0.03426220268011093\n",
            "step: 160, loss: 8.996805263450369e-05\n",
            "step: 170, loss: 0.00013409004895947874\n",
            "step: 180, loss: 0.000586646725423634\n",
            "step: 190, loss: 0.003156122984364629\n",
            "step: 200, loss: 0.00011232511315029114\n",
            "step: 210, loss: 0.00023139787663239986\n",
            "step: 220, loss: 8.363651431864128e-05\n",
            "step: 230, loss: 0.0003643095842562616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9656699889258028, f1=0.9588431590656283, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006669017020612955\n",
            "step: 10, loss: 0.00017147143080364913\n",
            "step: 20, loss: 0.0010471061104908586\n",
            "step: 30, loss: 0.0008873838814906776\n",
            "step: 40, loss: 0.00010161793761653826\n",
            "step: 50, loss: 0.0001619568356545642\n",
            "step: 60, loss: 0.0004946395056322217\n",
            "step: 70, loss: 6.816424138378352e-05\n",
            "step: 80, loss: 0.00020971171034034342\n",
            "step: 90, loss: 9.495942504145205e-05\n",
            "step: 100, loss: 0.00014697939332108945\n",
            "step: 110, loss: 0.00012106277426937595\n",
            "step: 120, loss: 5.472785414895043e-05\n",
            "step: 130, loss: 3.466987982392311e-05\n",
            "step: 140, loss: 5.359394344850443e-05\n",
            "step: 150, loss: 0.03722933679819107\n",
            "step: 160, loss: 4.9621768994256854e-05\n",
            "step: 170, loss: 0.005903564393520355\n",
            "step: 180, loss: 0.00013775346451438963\n",
            "step: 190, loss: 5.452154437080026e-05\n",
            "step: 200, loss: 9.922509343596175e-05\n",
            "step: 210, loss: 3.954954809159972e-05\n",
            "step: 220, loss: 6.796640082029626e-05\n",
            "step: 230, loss: 7.199739047791809e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9615384615384616, f1=0.9522727272727274, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.797533958684653e-05\n",
            "step: 10, loss: 3.800002014031634e-05\n",
            "step: 20, loss: 0.00012199994671391323\n",
            "step: 30, loss: 9.559416503179818e-05\n",
            "step: 40, loss: 0.00010029796976596117\n",
            "step: 50, loss: 0.0011065161088481545\n",
            "step: 60, loss: 0.0003205709799658507\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 4.26821134169586e-05\n",
            "step: 80, loss: 0.0004494424210861325\n",
            "step: 90, loss: 4.492053994908929e-05\n",
            "step: 100, loss: 3.9348120481008664e-05\n",
            "step: 110, loss: 0.00010157944780075923\n",
            "step: 120, loss: 5.484707435243763e-05\n",
            "step: 130, loss: 5.5753782362444326e-05\n",
            "step: 140, loss: 9.449598292121664e-05\n",
            "step: 150, loss: 4.9428152124164626e-05\n",
            "step: 160, loss: 5.621396121568978e-05\n",
            "step: 170, loss: 4.585248461808078e-05\n",
            "step: 180, loss: 0.00014772414579056203\n",
            "step: 190, loss: 0.00018485462351236492\n",
            "step: 200, loss: 3.0606035579694435e-05\n",
            "step: 210, loss: 5.162590605323203e-05\n",
            "step: 220, loss: 3.945959542761557e-05\n",
            "step: 230, loss: 6.824937008786947e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9672316384180792, f1=0.9592760180995475, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023807960678823292\n",
            "step: 10, loss: 7.557478966191411e-05\n",
            "step: 20, loss: 4.583743793773465e-05\n",
            "step: 30, loss: 5.570462963078171e-05\n",
            "step: 40, loss: 0.00015925892512314022\n",
            "step: 50, loss: 0.00012097090802853927\n",
            "step: 60, loss: 0.007685823831707239\n",
            "step: 70, loss: 3.172685319441371e-05\n",
            "step: 80, loss: 4.704781895270571e-05\n",
            "step: 90, loss: 0.00025740868295542896\n",
            "step: 100, loss: 2.9071139579173177e-05\n",
            "step: 110, loss: 0.00047603031271137297\n",
            "step: 120, loss: 0.015414129942655563\n",
            "step: 130, loss: 0.00013136942288838327\n",
            "step: 140, loss: 4.56363704870455e-05\n",
            "step: 150, loss: 4.636657831724733e-05\n",
            "step: 160, loss: 0.00017696956638246775\n",
            "step: 170, loss: 0.0001619770482648164\n",
            "step: 180, loss: 4.4362353946780786e-05\n",
            "step: 190, loss: 3.692311656777747e-05\n",
            "step: 200, loss: 4.459583942661993e-05\n",
            "step: 210, loss: 2.6221616280963644e-05\n",
            "step: 220, loss: 4.14454989368096e-05\n",
            "step: 230, loss: 4.948806235915981e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9688888888888889, f1=0.9617977528089887, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.938488953281194e-05\n",
            "step: 10, loss: 0.000261232431512326\n",
            "step: 20, loss: 4.5131484512239695e-05\n",
            "step: 30, loss: 4.8772344598546624e-05\n",
            "step: 40, loss: 0.0006730247405357659\n",
            "step: 50, loss: 4.033954974147491e-05\n",
            "step: 60, loss: 7.14636844350025e-05\n",
            "step: 70, loss: 7.219616236397997e-05\n",
            "step: 80, loss: 5.676085493178107e-05\n",
            "step: 90, loss: 4.343774344306439e-05\n",
            "step: 100, loss: 3.690856829052791e-05\n",
            "step: 110, loss: 3.427137926337309e-05\n",
            "step: 120, loss: 2.24258255911991e-05\n",
            "step: 130, loss: 4.0008042560657486e-05\n",
            "step: 140, loss: 4.377400910016149e-05\n",
            "step: 150, loss: 2.9749058739980683e-05\n",
            "step: 160, loss: 0.0012832671636715531\n",
            "step: 170, loss: 1.9881577827618457e-05\n",
            "step: 180, loss: 4.677494871430099e-05\n",
            "step: 190, loss: 4.927003101329319e-05\n",
            "step: 200, loss: 3.150729389744811e-05\n",
            "step: 210, loss: 4.305124093662016e-05\n",
            "step: 220, loss: 2.309608316863887e-05\n",
            "step: 230, loss: 0.0006046998896636069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9707207207207207, f1=0.9625425652667423, best_f1=0.9625425652667423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.77339913736796e-05\n",
            "step: 10, loss: 2.461591429891996e-05\n",
            "step: 20, loss: 6.0058169765397906e-05\n",
            "step: 30, loss: 4.18702547904104e-05\n",
            "step: 40, loss: 3.902839671354741e-05\n",
            "step: 50, loss: 0.015735724940896034\n",
            "step: 60, loss: 7.652047497685999e-05\n",
            "step: 70, loss: 4.2849889723584056e-05\n",
            "step: 80, loss: 4.198448732495308e-05\n",
            "step: 90, loss: 0.0001337937719654292\n",
            "step: 100, loss: 6.210917490534484e-05\n",
            "step: 110, loss: 3.5314140404807404e-05\n",
            "step: 120, loss: 5.4481792176375166e-05\n",
            "step: 130, loss: 3.699060471262783e-05\n",
            "step: 140, loss: 3.259150616941042e-05\n",
            "step: 150, loss: 6.220671639312059e-05\n",
            "step: 160, loss: 2.8467604352044873e-05\n",
            "step: 170, loss: 2.733155997702852e-05\n",
            "step: 180, loss: 6.65540574118495e-05\n",
            "step: 190, loss: 5.794252501800656e-05\n",
            "step: 200, loss: 5.066532321507111e-05\n",
            "step: 210, loss: 3.578385440050624e-05\n",
            "step: 220, loss: 9.2658432549797e-05\n",
            "step: 230, loss: 3.893098619300872e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9696287964004499, f1=0.9626274065685164, best_f1=0.9625425652667423\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 178.43it/s]\n",
            "load_f1 = 0.9706546275395034\n",
            "real_f1 = 0.9685393258426966\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 231.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24a790f-d26d-45e3-bae3-cfd26310f89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6434397101402283\n",
            "step: 10, loss: 0.5378242135047913\n",
            "step: 20, loss: 0.5508438944816589\n",
            "step: 30, loss: 0.28909194469451904\n",
            "step: 40, loss: 0.21086615324020386\n",
            "step: 50, loss: 0.2620730996131897\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.14341111481189728\n",
            "step: 70, loss: 0.1557777225971222\n",
            "step: 80, loss: 0.18285831809043884\n",
            "step: 90, loss: 0.1879182904958725\n",
            "step: 100, loss: 0.055151667445898056\n",
            "step: 110, loss: 0.15539368987083435\n",
            "step: 120, loss: 0.1376592367887497\n",
            "step: 130, loss: 0.08112350106239319\n",
            "step: 140, loss: 0.09381970763206482\n",
            "step: 150, loss: 0.051277074962854385\n",
            "step: 160, loss: 0.02501608431339264\n",
            "step: 170, loss: 0.330187052488327\n",
            "step: 180, loss: 0.11588793247938156\n",
            "step: 190, loss: 0.01837746426463127\n",
            "step: 200, loss: 0.2165653258562088\n",
            "step: 210, loss: 0.0893593356013298\n",
            "step: 220, loss: 0.19313760101795197\n",
            "step: 230, loss: 0.16423268616199493\n",
            "step: 240, loss: 0.0740782618522644\n",
            "step: 250, loss: 0.06980437785387039\n",
            "step: 260, loss: 0.21048061549663544\n",
            "step: 270, loss: 0.029024651274085045\n",
            "step: 280, loss: 0.06965041160583496\n",
            "step: 290, loss: 0.23357471823692322\n",
            "step: 300, loss: 0.04537446051836014\n",
            "step: 310, loss: 0.2443067580461502\n",
            "step: 320, loss: 0.1446043699979782\n",
            "step: 330, loss: 0.05266736447811127\n",
            "step: 340, loss: 0.03287513926625252\n",
            "step: 350, loss: 0.1286092847585678\n",
            "step: 360, loss: 0.07144635170698166\n",
            "step: 370, loss: 0.1582929491996765\n",
            "step: 380, loss: 0.030854441225528717\n",
            "step: 390, loss: 0.2832420766353607\n",
            "step: 400, loss: 0.29775795340538025\n",
            "step: 410, loss: 0.06783635169267654\n",
            "step: 420, loss: 0.10455572605133057\n",
            "step: 430, loss: 0.11124283820390701\n",
            "step: 440, loss: 0.024779126048088074\n",
            "step: 450, loss: 0.02290944568812847\n",
            "step: 460, loss: 0.011413751170039177\n",
            "step: 470, loss: 0.1442476063966751\n",
            "step: 480, loss: 0.05772489309310913\n",
            "step: 490, loss: 0.07790610194206238\n",
            "step: 500, loss: 0.048180751502513885\n",
            "step: 510, loss: 0.059100158512592316\n",
            "step: 520, loss: 0.08379337936639786\n",
            "step: 530, loss: 0.01876796782016754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9232183908045977, f1=0.9137931034482759, best_f1=0.9137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11643245816230774\n",
            "step: 10, loss: 0.18706384301185608\n",
            "step: 20, loss: 0.06453488767147064\n",
            "step: 30, loss: 0.023183535784482956\n",
            "step: 40, loss: 0.08931440114974976\n",
            "step: 50, loss: 0.17925409972667694\n",
            "step: 60, loss: 0.01874520443379879\n",
            "step: 70, loss: 0.03319424390792847\n",
            "step: 80, loss: 0.16972891986370087\n",
            "step: 90, loss: 0.013384317979216576\n",
            "step: 100, loss: 0.07410163432359695\n",
            "step: 110, loss: 0.0215121041983366\n",
            "step: 120, loss: 0.13092255592346191\n",
            "step: 130, loss: 0.05324448272585869\n",
            "step: 140, loss: 0.0648813247680664\n",
            "step: 150, loss: 0.07819919288158417\n",
            "step: 160, loss: 0.10278784483671188\n",
            "step: 170, loss: 0.08617507666349411\n",
            "step: 180, loss: 0.0518556609749794\n",
            "step: 190, loss: 0.029959242790937424\n",
            "step: 200, loss: 0.03967658057808876\n",
            "step: 210, loss: 0.06988416612148285\n",
            "step: 220, loss: 0.057042255997657776\n",
            "step: 230, loss: 0.011354928836226463\n",
            "step: 240, loss: 0.05498243123292923\n",
            "step: 250, loss: 0.01615300588309765\n",
            "step: 260, loss: 0.00349560659378767\n",
            "step: 270, loss: 0.2735513150691986\n",
            "step: 280, loss: 0.16380757093429565\n",
            "step: 290, loss: 0.02332318015396595\n",
            "step: 300, loss: 0.15154637396335602\n",
            "step: 310, loss: 0.008965893648564816\n",
            "step: 320, loss: 0.20111201703548431\n",
            "step: 330, loss: 0.07879937440156937\n",
            "step: 340, loss: 0.027778789401054382\n",
            "step: 350, loss: 0.005369293969124556\n",
            "step: 360, loss: 0.04881337657570839\n",
            "step: 370, loss: 0.15888598561286926\n",
            "step: 380, loss: 0.043742358684539795\n",
            "step: 390, loss: 0.11084030568599701\n",
            "step: 400, loss: 0.04354004189372063\n",
            "step: 410, loss: 0.021875489503145218\n",
            "step: 420, loss: 0.14340856671333313\n",
            "step: 430, loss: 0.02579553984105587\n",
            "step: 440, loss: 0.10004041343927383\n",
            "step: 450, loss: 0.052376072853803635\n",
            "step: 460, loss: 0.06698812544345856\n",
            "step: 470, loss: 0.1000409722328186\n",
            "step: 480, loss: 0.17168405652046204\n",
            "step: 490, loss: 0.014373158104717731\n",
            "step: 500, loss: 0.14914470911026\n",
            "step: 510, loss: 0.03501846268773079\n",
            "step: 520, loss: 0.04284599423408508\n",
            "step: 530, loss: 0.05514281988143921\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.919294990723562, f1=0.9196470041802136, best_f1=0.9137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08807898312807083\n",
            "step: 10, loss: 0.04845432937145233\n",
            "step: 20, loss: 0.06142435595393181\n",
            "step: 30, loss: 0.08439323306083679\n",
            "step: 40, loss: 0.11271840333938599\n",
            "step: 50, loss: 0.1403663009405136\n",
            "step: 60, loss: 0.034616172313690186\n",
            "step: 70, loss: 0.015391386114060879\n",
            "step: 80, loss: 0.0007827401859685779\n",
            "step: 90, loss: 0.004634593613445759\n",
            "step: 100, loss: 0.01270271185785532\n",
            "step: 110, loss: 0.0037941315677016973\n",
            "step: 120, loss: 0.00476818298920989\n",
            "step: 130, loss: 0.0030742797534912825\n",
            "step: 140, loss: 0.01700134389102459\n",
            "step: 150, loss: 0.06923218816518784\n",
            "step: 160, loss: 0.01930665783584118\n",
            "step: 170, loss: 0.15370449423789978\n",
            "step: 180, loss: 0.019712327048182487\n",
            "step: 190, loss: 0.0167235117405653\n",
            "step: 200, loss: 0.005654735956341028\n",
            "step: 210, loss: 0.04914185777306557\n",
            "step: 220, loss: 0.2285894900560379\n",
            "step: 230, loss: 0.07005481421947479\n",
            "step: 240, loss: 0.004330806899815798\n",
            "step: 250, loss: 0.026661256328225136\n",
            "step: 260, loss: 0.020218422636389732\n",
            "step: 270, loss: 0.00918429996818304\n",
            "step: 280, loss: 0.2775265574455261\n",
            "step: 290, loss: 0.007971461862325668\n",
            "step: 300, loss: 0.018958164379000664\n",
            "step: 310, loss: 0.003982999362051487\n",
            "step: 320, loss: 0.00605979235842824\n",
            "step: 330, loss: 0.015651021152734756\n",
            "step: 340, loss: 0.025043407455086708\n",
            "step: 350, loss: 0.158258855342865\n",
            "step: 360, loss: 0.04484665393829346\n",
            "step: 370, loss: 0.030256621539592743\n",
            "step: 380, loss: 0.0497741661965847\n",
            "step: 390, loss: 0.024465026333928108\n",
            "step: 400, loss: 0.02100278250873089\n",
            "step: 410, loss: 0.029015399515628815\n",
            "step: 420, loss: 0.12255379557609558\n",
            "step: 430, loss: 0.05273621529340744\n",
            "step: 440, loss: 0.007167628034949303\n",
            "step: 450, loss: 0.08829446882009506\n",
            "step: 460, loss: 0.10493530333042145\n",
            "step: 470, loss: 0.02916329726576805\n",
            "step: 480, loss: 0.008696830831468105\n",
            "step: 490, loss: 0.006268684286624193\n",
            "step: 500, loss: 0.2611404061317444\n",
            "step: 510, loss: 0.03518218919634819\n",
            "step: 520, loss: 0.05674657225608826\n",
            "step: 530, loss: 0.09271793067455292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9217311233885819, f1=0.9164764947512551, best_f1=0.9137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004849723540246487\n",
            "step: 10, loss: 0.004482378251850605\n",
            "step: 20, loss: 0.0023784360382705927\n",
            "step: 30, loss: 0.003966504707932472\n",
            "step: 40, loss: 0.011802173219621181\n",
            "step: 50, loss: 0.03138786181807518\n",
            "step: 60, loss: 0.00416486244648695\n",
            "step: 70, loss: 0.007561716716736555\n",
            "step: 80, loss: 0.0021248222328722477\n",
            "step: 90, loss: 0.1090255007147789\n",
            "step: 100, loss: 0.0040401918813586235\n",
            "step: 110, loss: 0.10962199419736862\n",
            "step: 120, loss: 0.10746313631534576\n",
            "step: 130, loss: 0.0022417374420911074\n",
            "step: 140, loss: 0.0018838284304365516\n",
            "step: 150, loss: 0.02501172386109829\n",
            "step: 160, loss: 0.03605464845895767\n",
            "step: 170, loss: 0.005214938428252935\n",
            "step: 180, loss: 0.001498937956057489\n",
            "step: 190, loss: 0.031077628955245018\n",
            "step: 200, loss: 0.06001976132392883\n",
            "step: 210, loss: 0.029787492007017136\n",
            "step: 220, loss: 0.010995608754456043\n",
            "step: 230, loss: 0.1798762083053589\n",
            "step: 240, loss: 0.018070697784423828\n",
            "step: 250, loss: 0.019923623651266098\n",
            "step: 260, loss: 0.006976708769798279\n",
            "step: 270, loss: 0.008429375477135181\n",
            "step: 280, loss: 0.025221319869160652\n",
            "step: 290, loss: 0.016496527940034866\n",
            "step: 300, loss: 0.0003466905909590423\n",
            "step: 310, loss: 0.0025122826918959618\n",
            "step: 320, loss: 0.001906127668917179\n",
            "step: 330, loss: 0.020006088539958\n",
            "step: 340, loss: 0.004930358380079269\n",
            "step: 350, loss: 0.007200541906058788\n",
            "step: 360, loss: 0.11077626794576645\n",
            "step: 370, loss: 0.009054267778992653\n",
            "step: 380, loss: 0.0052681565284729\n",
            "step: 390, loss: 0.009950417093932629\n",
            "step: 400, loss: 0.0022019040770828724\n",
            "step: 410, loss: 0.0044242385774850845\n",
            "step: 420, loss: 0.007756626233458519\n",
            "step: 430, loss: 0.0483374185860157\n",
            "step: 440, loss: 0.08385147154331207\n",
            "step: 450, loss: 0.0030037774704396725\n",
            "step: 460, loss: 0.023379158228635788\n",
            "step: 470, loss: 0.009564675390720367\n",
            "step: 480, loss: 0.0392158105969429\n",
            "step: 490, loss: 0.005089115351438522\n",
            "step: 500, loss: 0.0023274302948266268\n",
            "step: 510, loss: 0.003044974058866501\n",
            "step: 520, loss: 0.01811772584915161\n",
            "step: 530, loss: 0.04035509005188942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9128668171557561, f1=0.9168173598553345, best_f1=0.9137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01880708523094654\n",
            "step: 10, loss: 0.02581530250608921\n",
            "step: 20, loss: 0.0035921777598559856\n",
            "step: 30, loss: 0.13204224407672882\n",
            "step: 40, loss: 0.0864294171333313\n",
            "step: 50, loss: 0.041495371609926224\n",
            "step: 60, loss: 0.095211923122406\n",
            "step: 70, loss: 0.01595485210418701\n",
            "step: 80, loss: 0.004986106883734465\n",
            "step: 90, loss: 0.0026693374384194613\n",
            "step: 100, loss: 0.0025271927006542683\n",
            "step: 110, loss: 0.012039121240377426\n",
            "step: 120, loss: 0.049634162336587906\n",
            "step: 130, loss: 0.01793290115892887\n",
            "step: 140, loss: 0.0007306007901206613\n",
            "step: 150, loss: 0.001811023335903883\n",
            "step: 160, loss: 0.0018053185194730759\n",
            "step: 170, loss: 0.023408744484186172\n",
            "step: 180, loss: 0.0018712312448769808\n",
            "step: 190, loss: 0.0013584501575678587\n",
            "step: 200, loss: 0.0007407473167404532\n",
            "step: 210, loss: 0.006662196945399046\n",
            "step: 220, loss: 0.0014263689517974854\n",
            "step: 230, loss: 0.00785746704787016\n",
            "step: 240, loss: 0.0012481800513342023\n",
            "step: 250, loss: 0.3117237687110901\n",
            "step: 260, loss: 0.13881923258304596\n",
            "step: 270, loss: 0.04206959530711174\n",
            "step: 280, loss: 0.006306846160441637\n",
            "step: 290, loss: 0.17208965122699738\n",
            "step: 300, loss: 0.007663773372769356\n",
            "step: 310, loss: 0.0003859448479488492\n",
            "step: 320, loss: 0.16664516925811768\n",
            "step: 330, loss: 0.007465972099453211\n",
            "step: 340, loss: 0.0022199642844498158\n",
            "step: 350, loss: 0.005567531101405621\n",
            "step: 360, loss: 0.006516800262033939\n",
            "step: 370, loss: 0.01271621510386467\n",
            "step: 380, loss: 0.0004207420570310205\n",
            "step: 390, loss: 0.0019113158341497183\n",
            "step: 400, loss: 0.01554343942552805\n",
            "step: 410, loss: 0.0008141085272654891\n",
            "step: 420, loss: 0.0019825417548418045\n",
            "step: 430, loss: 0.01252529863268137\n",
            "step: 440, loss: 0.004201109521090984\n",
            "step: 450, loss: 0.0014982790453359485\n",
            "step: 460, loss: 0.004111847374588251\n",
            "step: 470, loss: 0.004733523819595575\n",
            "step: 480, loss: 0.0020604762248694897\n",
            "step: 490, loss: 0.002246921183541417\n",
            "step: 500, loss: 0.00657288171350956\n",
            "step: 510, loss: 0.0043153720907866955\n",
            "step: 520, loss: 0.012928865849971771\n",
            "step: 530, loss: 0.002197843976318836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9232927970065481, f1=0.9250814332247558, best_f1=0.9250814332247558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004198269918560982\n",
            "step: 10, loss: 0.00016825196507852525\n",
            "step: 20, loss: 0.14010384678840637\n",
            "step: 30, loss: 0.0009831845527514815\n",
            "step: 40, loss: 0.0022861582692712545\n",
            "step: 50, loss: 0.016566231846809387\n",
            "step: 60, loss: 0.0019863820634782314\n",
            "step: 70, loss: 0.005214978475123644\n",
            "step: 80, loss: 0.001682356814853847\n",
            "step: 90, loss: 0.004578023683279753\n",
            "step: 100, loss: 0.012183954939246178\n",
            "step: 110, loss: 0.0007094950997270644\n",
            "step: 120, loss: 0.0003271047316957265\n",
            "step: 130, loss: 0.061979904770851135\n",
            "step: 140, loss: 0.031225129961967468\n",
            "step: 150, loss: 0.050861574709415436\n",
            "step: 160, loss: 0.0009602899081073701\n",
            "step: 170, loss: 0.004831794183701277\n",
            "step: 180, loss: 0.015880059450864792\n",
            "step: 190, loss: 0.0011158377164974809\n",
            "step: 200, loss: 0.006729897577315569\n",
            "step: 210, loss: 0.019763333722949028\n",
            "step: 220, loss: 0.0006661724764853716\n",
            "step: 230, loss: 0.001259073382243514\n",
            "step: 240, loss: 0.07754389196634293\n",
            "step: 250, loss: 0.0009777318919077516\n",
            "step: 260, loss: 0.00027012432110495865\n",
            "step: 270, loss: 0.0722898542881012\n",
            "step: 280, loss: 0.0012736795470118523\n",
            "step: 290, loss: 0.0004438654868863523\n",
            "step: 300, loss: 0.0012775621144101024\n",
            "step: 310, loss: 0.00439996924251318\n",
            "step: 320, loss: 0.002834339626133442\n",
            "step: 330, loss: 0.0004000192566309124\n",
            "step: 340, loss: 0.05304238200187683\n",
            "step: 350, loss: 0.0014265027130022645\n",
            "step: 360, loss: 0.0028706397861242294\n",
            "step: 370, loss: 0.0004222370916977525\n",
            "step: 380, loss: 0.00024659818154759705\n",
            "step: 390, loss: 0.01398205105215311\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 0.12133575230836868\n",
            "step: 410, loss: 0.0002739351475611329\n",
            "step: 420, loss: 0.0023963383864611387\n",
            "step: 430, loss: 0.0001607222220627591\n",
            "step: 440, loss: 0.00011240437743254006\n",
            "step: 450, loss: 0.0005305737140588462\n",
            "step: 460, loss: 0.0001624017022550106\n",
            "step: 470, loss: 0.0014962460845708847\n",
            "step: 480, loss: 0.02202105149626732\n",
            "step: 490, loss: 0.000531012483406812\n",
            "step: 500, loss: 0.00016741118452046067\n",
            "step: 510, loss: 0.0019818011205643415\n",
            "step: 520, loss: 0.0011465736897662282\n",
            "step: 530, loss: 0.04273118078708649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.919020715630885, f1=0.9163160355306218, best_f1=0.9250814332247558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006490717642009258\n",
            "step: 10, loss: 0.00019227559096179903\n",
            "step: 20, loss: 0.00043063017074018717\n",
            "step: 30, loss: 0.0004940208746120334\n",
            "step: 40, loss: 0.10041859745979309\n",
            "step: 50, loss: 0.0008059705141931772\n",
            "step: 60, loss: 0.009416261687874794\n",
            "step: 70, loss: 0.03966149315237999\n",
            "step: 80, loss: 0.005597769282758236\n",
            "step: 90, loss: 0.00028744604787789285\n",
            "step: 100, loss: 0.002864591544494033\n",
            "step: 110, loss: 0.0004603659035637975\n",
            "step: 120, loss: 0.00026390518178232014\n",
            "step: 130, loss: 0.09393730759620667\n",
            "step: 140, loss: 0.00018242117948830128\n",
            "step: 150, loss: 0.000206462835194543\n",
            "step: 160, loss: 8.959163824329153e-05\n",
            "step: 170, loss: 0.0006346361478790641\n",
            "step: 180, loss: 0.0005089825135655701\n",
            "step: 190, loss: 0.0001663690054556355\n",
            "step: 200, loss: 0.00010269471385981888\n",
            "step: 210, loss: 0.11960452049970627\n",
            "step: 220, loss: 8.126216562232003e-05\n",
            "step: 230, loss: 0.01789427362382412\n",
            "step: 240, loss: 0.000853582052513957\n",
            "step: 250, loss: 0.0005548386834561825\n",
            "step: 260, loss: 0.0011360736098140478\n",
            "step: 270, loss: 0.028539977967739105\n",
            "step: 280, loss: 7.512732554459944e-05\n",
            "step: 290, loss: 0.00017657178977970034\n",
            "step: 300, loss: 0.1305568665266037\n",
            "step: 310, loss: 0.0004130998859182\n",
            "step: 320, loss: 7.308850763365626e-05\n",
            "step: 330, loss: 0.0014526807935908437\n",
            "step: 340, loss: 0.0811847522854805\n",
            "step: 350, loss: 0.004070157650858164\n",
            "step: 360, loss: 0.009612586349248886\n",
            "step: 370, loss: 0.0007204993162304163\n",
            "step: 380, loss: 0.005558141507208347\n",
            "step: 390, loss: 0.0014100114349275827\n",
            "step: 400, loss: 0.0102480323985219\n",
            "step: 410, loss: 0.004183376673609018\n",
            "step: 420, loss: 0.0006334320059977472\n",
            "step: 430, loss: 0.0003245319821871817\n",
            "step: 440, loss: 0.0002876554208341986\n",
            "step: 450, loss: 0.0002943317813333124\n",
            "step: 460, loss: 0.00047092378372326493\n",
            "step: 470, loss: 0.0022038938477635384\n",
            "step: 480, loss: 0.07758425921201706\n",
            "step: 490, loss: 0.02232929691672325\n",
            "step: 500, loss: 0.07263181358575821\n",
            "step: 510, loss: 0.004903942812234163\n",
            "step: 520, loss: 0.01858852431178093\n",
            "step: 530, loss: 0.0013654881622642279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9236396890717878, f1=0.9231468849477036, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003718295774888247\n",
            "step: 10, loss: 0.029670067131519318\n",
            "step: 20, loss: 0.00010634477075655013\n",
            "step: 30, loss: 0.03313319385051727\n",
            "step: 40, loss: 0.005824750289320946\n",
            "step: 50, loss: 0.006026541348546743\n",
            "step: 60, loss: 0.005793341901153326\n",
            "step: 70, loss: 0.003976090345531702\n",
            "step: 80, loss: 0.0008985574822872877\n",
            "step: 90, loss: 0.00034448530641384423\n",
            "step: 100, loss: 0.0005802613450214267\n",
            "step: 110, loss: 0.0002602569875307381\n",
            "step: 120, loss: 0.0003268627915531397\n",
            "step: 130, loss: 0.008222161792218685\n",
            "step: 140, loss: 0.0009181100176647305\n",
            "step: 150, loss: 6.828139157732949e-05\n",
            "step: 160, loss: 0.0005342381773516536\n",
            "step: 170, loss: 0.000677023665048182\n",
            "step: 180, loss: 0.00011273905693087727\n",
            "step: 190, loss: 0.0007620507385581732\n",
            "step: 200, loss: 0.00033274738234467804\n",
            "step: 210, loss: 0.0024355670902878046\n",
            "step: 220, loss: 0.0011876297648996115\n",
            "step: 230, loss: 0.03070945106446743\n",
            "step: 240, loss: 0.00013891799608245492\n",
            "step: 250, loss: 0.0001098791544791311\n",
            "step: 260, loss: 0.0002696355804800987\n",
            "step: 270, loss: 0.0050420318730175495\n",
            "step: 280, loss: 0.006277227774262428\n",
            "step: 290, loss: 0.014461299404501915\n",
            "step: 300, loss: 0.0016836802242323756\n",
            "step: 310, loss: 0.028160016983747482\n",
            "step: 320, loss: 0.021506115794181824\n",
            "step: 330, loss: 0.009891824796795845\n",
            "step: 340, loss: 0.00033065120805986226\n",
            "step: 350, loss: 0.04839123785495758\n",
            "step: 360, loss: 0.09852293878793716\n",
            "step: 370, loss: 9.909514483297244e-05\n",
            "step: 380, loss: 0.006015400402247906\n",
            "step: 390, loss: 0.0006547014345414937\n",
            "step: 400, loss: 0.00013194164785090834\n",
            "step: 410, loss: 4.271962461643852e-05\n",
            "step: 420, loss: 0.000317638274282217\n",
            "step: 430, loss: 0.06139913946390152\n",
            "step: 440, loss: 0.0059999278746545315\n",
            "step: 450, loss: 0.0004621703992597759\n",
            "step: 460, loss: 0.043028607964515686\n",
            "step: 470, loss: 8.750735287321731e-05\n",
            "step: 480, loss: 0.000520825560670346\n",
            "step: 490, loss: 0.09439483284950256\n",
            "step: 500, loss: 0.010577445849776268\n",
            "step: 510, loss: 0.008959801867604256\n",
            "step: 520, loss: 0.0014344804221764207\n",
            "step: 530, loss: 0.0006896275444887578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9181692094313454, f1=0.9224376731301939, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001232940354384482\n",
            "step: 10, loss: 7.346767233684659e-05\n",
            "step: 20, loss: 0.00042749333078972995\n",
            "step: 30, loss: 0.00011704774806275964\n",
            "step: 40, loss: 4.4310341763775796e-05\n",
            "step: 50, loss: 0.0002583555760793388\n",
            "step: 60, loss: 5.509636321221478e-05\n",
            "step: 70, loss: 0.00026263235486112535\n",
            "step: 80, loss: 0.003359345719218254\n",
            "step: 90, loss: 0.00011629951040958986\n",
            "step: 100, loss: 4.9768234021030366e-05\n",
            "step: 110, loss: 4.230555714457296e-05\n",
            "step: 120, loss: 0.00027219587354920805\n",
            "step: 130, loss: 0.00047947486746124923\n",
            "step: 140, loss: 0.000109684988274239\n",
            "step: 150, loss: 0.00010741678124759346\n",
            "step: 160, loss: 4.438921678229235e-05\n",
            "step: 170, loss: 0.0011335494928061962\n",
            "step: 180, loss: 4.316862759878859e-05\n",
            "step: 190, loss: 0.00011336867464706302\n",
            "step: 200, loss: 0.00012515668640844524\n",
            "step: 210, loss: 0.00011878268560394645\n",
            "step: 220, loss: 0.0007942196680232882\n",
            "step: 230, loss: 0.00025534184533171356\n",
            "step: 240, loss: 5.77219580009114e-05\n",
            "step: 250, loss: 6.474983092630282e-05\n",
            "step: 260, loss: 0.00134507124312222\n",
            "step: 270, loss: 8.817190973786637e-05\n",
            "step: 280, loss: 0.009471439756453037\n",
            "step: 290, loss: 0.04795824736356735\n",
            "step: 300, loss: 0.000116143943159841\n",
            "step: 310, loss: 0.00240780645981431\n",
            "step: 320, loss: 0.00020073342602699995\n",
            "step: 330, loss: 0.0003068959340453148\n",
            "step: 340, loss: 0.00013237394159659743\n",
            "step: 350, loss: 0.005845887120813131\n",
            "step: 360, loss: 0.00018065865151584148\n",
            "step: 370, loss: 0.00027557279099710286\n",
            "step: 380, loss: 4.592612094711512e-05\n",
            "step: 390, loss: 4.348819129518233e-05\n",
            "step: 400, loss: 0.003839269280433655\n",
            "step: 410, loss: 0.0019656952936202288\n",
            "step: 420, loss: 0.00014962356362957507\n",
            "step: 430, loss: 0.0033976042177528143\n",
            "step: 440, loss: 0.0026058319490402937\n",
            "step: 450, loss: 0.0001159970197477378\n",
            "step: 460, loss: 0.00021887969342060387\n",
            "step: 470, loss: 0.0011943718418478966\n",
            "step: 480, loss: 0.00012835334928240627\n",
            "step: 490, loss: 0.0005985737661831081\n",
            "step: 500, loss: 0.000883443804923445\n",
            "step: 510, loss: 0.00037753849755972624\n",
            "step: 520, loss: 0.0003666893462650478\n",
            "step: 530, loss: 0.00017523397400509566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9103840682788051, f1=0.9168241965973536, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00025038898456841707\n",
            "step: 10, loss: 4.004327274742536e-05\n",
            "step: 20, loss: 2.7577219952945597e-05\n",
            "step: 30, loss: 0.0001514641917310655\n",
            "step: 40, loss: 0.00024406693410128355\n",
            "step: 50, loss: 0.00037805380998179317\n",
            "step: 60, loss: 0.00018643811927177012\n",
            "step: 70, loss: 0.0014055358478799462\n",
            "step: 80, loss: 4.0681730752112344e-05\n",
            "step: 90, loss: 0.0006535246502608061\n",
            "step: 100, loss: 4.4073884055251256e-05\n",
            "step: 110, loss: 2.3293319827644154e-05\n",
            "step: 120, loss: 3.8479916838696226e-05\n",
            "step: 130, loss: 0.00039254711009562016\n",
            "step: 140, loss: 0.0165173951536417\n",
            "step: 150, loss: 0.0004340809246059507\n",
            "step: 160, loss: 3.7973568396409974e-05\n",
            "step: 170, loss: 0.0002370672591496259\n",
            "step: 180, loss: 6.921061867615208e-05\n",
            "step: 190, loss: 0.0003845598257612437\n",
            "step: 200, loss: 0.0013988139107823372\n",
            "step: 210, loss: 0.0007424675859510899\n",
            "step: 220, loss: 0.0001311264350079\n",
            "step: 230, loss: 0.011216796934604645\n",
            "step: 240, loss: 0.006070106755942106\n",
            "step: 250, loss: 4.392546907183714e-05\n",
            "step: 260, loss: 0.012644169852137566\n",
            "step: 270, loss: 0.00010385709902038798\n",
            "step: 280, loss: 0.00023266306379809976\n",
            "step: 290, loss: 5.734811566071585e-05\n",
            "step: 300, loss: 0.00011531866039149463\n",
            "step: 310, loss: 0.00011836691555799916\n",
            "step: 320, loss: 0.0034520362969487906\n",
            "step: 330, loss: 4.935732067679055e-05\n",
            "step: 340, loss: 0.04121209681034088\n",
            "step: 350, loss: 0.00012945827620569617\n",
            "step: 360, loss: 0.0005559457931667566\n",
            "step: 370, loss: 0.00031842160387896\n",
            "step: 380, loss: 0.00039124811883084476\n",
            "step: 390, loss: 0.013088593259453773\n",
            "step: 400, loss: 0.003422756213694811\n",
            "step: 410, loss: 0.0029302246402949095\n",
            "step: 420, loss: 0.0009665403049439192\n",
            "step: 430, loss: 0.0008826549747027457\n",
            "step: 440, loss: 0.007330482825636864\n",
            "step: 450, loss: 9.822897118283436e-05\n",
            "step: 460, loss: 8.266422810265794e-05\n",
            "step: 470, loss: 8.824786345940083e-05\n",
            "step: 480, loss: 6.733728514518589e-05\n",
            "step: 490, loss: 0.00013591547030955553\n",
            "step: 500, loss: 0.1356464922428131\n",
            "step: 510, loss: 0.00011034258204745129\n",
            "step: 520, loss: 5.2176721510477364e-05\n",
            "step: 530, loss: 0.00044527294812723994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9216589861751152, f1=0.9223659889094269, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003629823913797736\n",
            "step: 10, loss: 0.00018700752116274089\n",
            "step: 20, loss: 8.048838208196685e-05\n",
            "step: 30, loss: 0.0010410496033728123\n",
            "step: 40, loss: 0.0006278094951994717\n",
            "step: 50, loss: 0.00020958969253115356\n",
            "step: 60, loss: 0.02921128459274769\n",
            "step: 70, loss: 0.0017346561653539538\n",
            "step: 80, loss: 3.121322879451327e-05\n",
            "step: 90, loss: 0.000707640778273344\n",
            "step: 100, loss: 0.00043858433491550386\n",
            "step: 110, loss: 0.003184923203662038\n",
            "step: 120, loss: 4.956566044711508e-05\n",
            "step: 130, loss: 9.019537537824363e-05\n",
            "step: 140, loss: 2.854238118743524e-05\n",
            "step: 150, loss: 2.6861478545470163e-05\n",
            "step: 160, loss: 3.3063868613680825e-05\n",
            "step: 170, loss: 3.6352550523588434e-05\n",
            "step: 180, loss: 4.900147905573249e-05\n",
            "step: 190, loss: 0.005470622330904007\n",
            "step: 200, loss: 0.03180735558271408\n",
            "step: 210, loss: 3.913544423994608e-05\n",
            "step: 220, loss: 0.0022400261368602514\n",
            "step: 230, loss: 0.00037624480319209397\n",
            "step: 240, loss: 8.391439041588455e-05\n",
            "step: 250, loss: 2.1684551029466093e-05\n",
            "step: 260, loss: 3.579024996724911e-05\n",
            "step: 270, loss: 0.0001905894750962034\n",
            "step: 280, loss: 0.00031342709553427994\n",
            "step: 290, loss: 0.005814043805003166\n",
            "step: 300, loss: 0.0005973337101750076\n",
            "step: 310, loss: 0.0060964603908360004\n",
            "step: 320, loss: 9.393696382176131e-05\n",
            "step: 330, loss: 0.00012555862485896796\n",
            "step: 340, loss: 0.0002787297125905752\n",
            "step: 350, loss: 3.810791531577706e-05\n",
            "step: 360, loss: 3.30754483002238e-05\n",
            "step: 370, loss: 0.00018864471348933876\n",
            "step: 380, loss: 4.037213511765003e-05\n",
            "step: 390, loss: 2.8221718821441755e-05\n",
            "step: 400, loss: 5.26429976162035e-05\n",
            "step: 410, loss: 2.2444552087108605e-05\n",
            "step: 420, loss: 5.771850919700228e-05\n",
            "step: 430, loss: 9.664794197306037e-05\n",
            "step: 440, loss: 0.00020506879081949592\n",
            "step: 450, loss: 2.809530997183174e-05\n",
            "step: 460, loss: 0.017859792336821556\n",
            "step: 470, loss: 4.951663868268952e-05\n",
            "step: 480, loss: 4.7418594476766884e-05\n",
            "step: 490, loss: 0.0001897506444947794\n",
            "step: 500, loss: 0.0012201642384752631\n",
            "step: 510, loss: 2.609420334920287e-05\n",
            "step: 520, loss: 4.742216333397664e-05\n",
            "step: 530, loss: 2.862768633349333e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9091755938518863, f1=0.9169776119402985, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018088123761117458\n",
            "step: 10, loss: 2.9942189939902164e-05\n",
            "step: 20, loss: 2.9253356842673384e-05\n",
            "step: 30, loss: 0.002546164905652404\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.00019100680947303772\n",
            "step: 50, loss: 0.0016163423424586654\n",
            "step: 60, loss: 0.00013465674419421703\n",
            "step: 70, loss: 2.8095482775825076e-05\n",
            "step: 80, loss: 1.8398915926809423e-05\n",
            "step: 90, loss: 0.0005142384325154126\n",
            "step: 100, loss: 3.6572208045981824e-05\n",
            "step: 110, loss: 2.0615476387320086e-05\n",
            "step: 120, loss: 2.4712306185392663e-05\n",
            "step: 130, loss: 2.65339531324571e-05\n",
            "step: 140, loss: 0.0002573409001342952\n",
            "step: 150, loss: 3.646667755674571e-05\n",
            "step: 160, loss: 1.9728717234102078e-05\n",
            "step: 170, loss: 3.377182292751968e-05\n",
            "step: 180, loss: 4.019852713099681e-05\n",
            "step: 190, loss: 0.00015212022117339075\n",
            "step: 200, loss: 2.285795744683128e-05\n",
            "step: 210, loss: 0.0008453777409158647\n",
            "step: 220, loss: 5.414510451373644e-05\n",
            "step: 230, loss: 0.001060683629475534\n",
            "step: 240, loss: 0.00013354157272260636\n",
            "step: 250, loss: 0.012730028480291367\n",
            "step: 260, loss: 0.0001834847789723426\n",
            "step: 270, loss: 9.8405871540308e-05\n",
            "step: 280, loss: 2.2160971639095806e-05\n",
            "step: 290, loss: 3.965984797105193e-05\n",
            "step: 300, loss: 0.0003374536754563451\n",
            "step: 310, loss: 7.322720193769783e-05\n",
            "step: 320, loss: 0.00010163435945287347\n",
            "step: 330, loss: 5.621254604193382e-05\n",
            "step: 340, loss: 6.499156734207645e-05\n",
            "step: 350, loss: 9.63422644417733e-05\n",
            "step: 360, loss: 0.0001616410881979391\n",
            "step: 370, loss: 0.00157643249258399\n",
            "step: 380, loss: 0.0012952563120052218\n",
            "step: 390, loss: 7.106162229320034e-05\n",
            "step: 400, loss: 4.935550896334462e-05\n",
            "step: 410, loss: 0.0004548759898170829\n",
            "step: 420, loss: 0.0033378254156559706\n",
            "step: 430, loss: 0.015128914266824722\n",
            "step: 440, loss: 0.001229125540703535\n",
            "step: 450, loss: 5.732976205763407e-05\n",
            "step: 460, loss: 3.608084807638079e-05\n",
            "step: 470, loss: 3.955667489208281e-05\n",
            "step: 480, loss: 0.006396490149199963\n",
            "step: 490, loss: 0.00030901809805072844\n",
            "step: 500, loss: 0.0003682984388433397\n",
            "step: 510, loss: 0.004941081162542105\n",
            "step: 520, loss: 0.006390131078660488\n",
            "step: 530, loss: 0.0004957308992743492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9151765245300322, f1=0.91804788213628, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016309495549649\n",
            "step: 10, loss: 8.997219265438616e-05\n",
            "step: 20, loss: 0.0008246239158324897\n",
            "step: 30, loss: 6.109229434514418e-05\n",
            "step: 40, loss: 2.3386808607028797e-05\n",
            "step: 50, loss: 0.0006854504463262856\n",
            "step: 60, loss: 0.0005071907653473318\n",
            "step: 70, loss: 5.1717976020881906e-05\n",
            "step: 80, loss: 5.040510222897865e-05\n",
            "step: 90, loss: 0.015562167391180992\n",
            "step: 100, loss: 0.00011964979057665914\n",
            "step: 110, loss: 0.00014837415073998272\n",
            "step: 120, loss: 2.3282671463675797e-05\n",
            "step: 130, loss: 2.065262924588751e-05\n",
            "step: 140, loss: 0.00015084927144926041\n",
            "step: 150, loss: 9.447153570363298e-05\n",
            "step: 160, loss: 0.00037116603925824165\n",
            "step: 170, loss: 6.266349373618141e-05\n",
            "step: 180, loss: 6.333143392112106e-05\n",
            "step: 190, loss: 0.0003725734131876379\n",
            "step: 200, loss: 0.00013189055607654154\n",
            "step: 210, loss: 4.4694195821648464e-05\n",
            "step: 220, loss: 0.0004326516354922205\n",
            "step: 230, loss: 2.153910645574797e-05\n",
            "step: 240, loss: 0.006293728947639465\n",
            "step: 250, loss: 3.6717923649121076e-05\n",
            "step: 260, loss: 3.937500878237188e-05\n",
            "step: 270, loss: 1.7478803783887997e-05\n",
            "step: 280, loss: 2.5431872927583754e-05\n",
            "step: 290, loss: 1.9963285012636334e-05\n",
            "step: 300, loss: 4.2985375330317765e-05\n",
            "step: 310, loss: 0.0027297933120280504\n",
            "step: 320, loss: 0.005433185957372189\n",
            "step: 330, loss: 0.0001565029815537855\n",
            "step: 340, loss: 2.5755087335710414e-05\n",
            "step: 350, loss: 2.2883945348439738e-05\n",
            "step: 360, loss: 2.2373717001755722e-05\n",
            "step: 370, loss: 0.0010255263186991215\n",
            "step: 380, loss: 0.00012357017840258777\n",
            "step: 390, loss: 0.00014457864745054394\n",
            "step: 400, loss: 0.0003351573832333088\n",
            "step: 410, loss: 0.0002740335476119071\n",
            "step: 420, loss: 3.67562570318114e-05\n",
            "step: 430, loss: 0.00011349433043505996\n",
            "step: 440, loss: 6.632595614064485e-05\n",
            "step: 450, loss: 5.205048364587128e-05\n",
            "step: 460, loss: 0.0004598810337483883\n",
            "step: 470, loss: 0.00010223518620477989\n",
            "step: 480, loss: 4.490119681577198e-05\n",
            "step: 490, loss: 3.605077654356137e-05\n",
            "step: 500, loss: 2.719748044910375e-05\n",
            "step: 510, loss: 3.300422758911736e-05\n",
            "step: 520, loss: 0.0006749233580194414\n",
            "step: 530, loss: 2.5480465410510078e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9130636913063691, f1=0.9164722351843211, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.4538701584097e-05\n",
            "step: 10, loss: 2.4526809283997864e-05\n",
            "step: 20, loss: 9.465856419410557e-05\n",
            "step: 30, loss: 3.3700514904921874e-05\n",
            "step: 40, loss: 0.00010776826093206182\n",
            "step: 50, loss: 0.0003126589290332049\n",
            "step: 60, loss: 1.86485249287216e-05\n",
            "step: 70, loss: 0.000616741250269115\n",
            "step: 80, loss: 4.258881017449312e-05\n",
            "step: 90, loss: 3.7191202864050865e-05\n",
            "step: 100, loss: 0.00023063791741151363\n",
            "step: 110, loss: 2.657191907928791e-05\n",
            "step: 120, loss: 3.3328797144349664e-05\n",
            "step: 130, loss: 0.0007593858754262328\n",
            "step: 140, loss: 3.992296478827484e-05\n",
            "step: 150, loss: 0.0002620806044433266\n",
            "step: 160, loss: 2.8437858418328688e-05\n",
            "step: 170, loss: 0.0005727573879994452\n",
            "step: 180, loss: 0.00021595723228529096\n",
            "step: 190, loss: 3.960693356930278e-05\n",
            "step: 200, loss: 3.775977893383242e-05\n",
            "step: 210, loss: 4.370357055449858e-05\n",
            "step: 220, loss: 2.3938160666148178e-05\n",
            "step: 230, loss: 0.00013756075350102037\n",
            "step: 240, loss: 2.060801489278674e-05\n",
            "step: 250, loss: 3.0147670258884318e-05\n",
            "step: 260, loss: 5.4047432058723643e-05\n",
            "step: 270, loss: 4.467583858058788e-05\n",
            "step: 280, loss: 1.9166365746059455e-05\n",
            "step: 290, loss: 4.49307553935796e-05\n",
            "step: 300, loss: 5.554275048780255e-05\n",
            "step: 310, loss: 2.27833788812859e-05\n",
            "step: 320, loss: 6.0259553720243275e-05\n",
            "step: 330, loss: 3.955247302656062e-05\n",
            "step: 340, loss: 4.6534270950360224e-05\n",
            "step: 350, loss: 0.00011351909051882103\n",
            "step: 360, loss: 0.0012671987060457468\n",
            "step: 370, loss: 2.7759366275859065e-05\n",
            "step: 380, loss: 0.00016154476907104254\n",
            "step: 390, loss: 0.030920447781682014\n",
            "step: 400, loss: 3.430789729463868e-05\n",
            "step: 410, loss: 0.00013246967864688486\n",
            "step: 420, loss: 3.524188286974095e-05\n",
            "step: 430, loss: 3.9869482861831784e-05\n",
            "step: 440, loss: 5.827058703289367e-05\n",
            "step: 450, loss: 0.000261332985246554\n",
            "step: 460, loss: 2.4198963728849776e-05\n",
            "step: 470, loss: 4.751637243316509e-05\n",
            "step: 480, loss: 3.3421201806049794e-05\n",
            "step: 490, loss: 2.29771358135622e-05\n",
            "step: 500, loss: 2.8362497687339783e-05\n",
            "step: 510, loss: 0.00010350728553021327\n",
            "step: 520, loss: 3.6456349334912375e-05\n",
            "step: 530, loss: 4.123570033698343e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9148148148148147, f1=0.9213691026827011, best_f1=0.9231468849477036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005639622453600168\n",
            "step: 10, loss: 0.00013023878273088485\n",
            "step: 20, loss: 2.2444346541306004e-05\n",
            "step: 30, loss: 0.00039045122684910893\n",
            "step: 40, loss: 0.000691338733304292\n",
            "step: 50, loss: 0.0007553871837444603\n",
            "step: 60, loss: 1.8995013306266628e-05\n",
            "step: 70, loss: 0.00043409812496975064\n",
            "step: 80, loss: 4.0181304939324036e-05\n",
            "step: 90, loss: 2.1781379473395646e-05\n",
            "step: 100, loss: 0.000704100530128926\n",
            "step: 110, loss: 2.558094638516195e-05\n",
            "step: 120, loss: 2.761025280051399e-05\n",
            "step: 130, loss: 0.01740448549389839\n",
            "step: 140, loss: 2.017956467170734e-05\n",
            "step: 150, loss: 2.6061092285090126e-05\n",
            "step: 160, loss: 5.745997987105511e-05\n",
            "step: 170, loss: 0.00033976894337683916\n",
            "step: 180, loss: 1.7873726392281242e-05\n",
            "step: 190, loss: 2.485829645593185e-05\n",
            "step: 200, loss: 0.00011557290417840704\n",
            "step: 210, loss: 4.923213418805972e-05\n",
            "step: 220, loss: 2.7938544008065946e-05\n",
            "step: 230, loss: 3.673899846035056e-05\n",
            "step: 240, loss: 2.184459663112648e-05\n",
            "step: 250, loss: 3.5682565794559196e-05\n",
            "step: 260, loss: 1.8514443581807427e-05\n",
            "step: 270, loss: 4.142355828662403e-05\n",
            "step: 280, loss: 3.264495171606541e-05\n",
            "step: 290, loss: 1.8015292880591005e-05\n",
            "step: 300, loss: 9.739780944073573e-05\n",
            "step: 310, loss: 3.171192656736821e-05\n",
            "step: 320, loss: 2.2767819245927967e-05\n",
            "step: 330, loss: 2.5688948880997486e-05\n",
            "step: 340, loss: 1.9401000827201642e-05\n",
            "step: 350, loss: 1.6525180399185047e-05\n",
            "step: 360, loss: 0.0002249307290185243\n",
            "step: 370, loss: 1.4811621440458111e-05\n",
            "step: 380, loss: 2.4668313926667906e-05\n",
            "step: 390, loss: 6.325276626739651e-05\n",
            "step: 400, loss: 0.00016562930250074714\n",
            "step: 410, loss: 3.7437814171426e-05\n",
            "step: 420, loss: 0.0003343595308251679\n",
            "step: 430, loss: 0.00010487224062671885\n",
            "step: 440, loss: 3.5811797715723515e-05\n",
            "step: 450, loss: 1.7050439055310562e-05\n",
            "step: 460, loss: 1.728139613987878e-05\n",
            "step: 470, loss: 3.7783152947667986e-05\n",
            "step: 480, loss: 1.6480467820656486e-05\n",
            "step: 490, loss: 2.1110776287969202e-05\n",
            "step: 500, loss: 2.134538590325974e-05\n",
            "step: 510, loss: 3.0441011404036544e-05\n",
            "step: 520, loss: 2.2448215531767346e-05\n",
            "step: 530, loss: 2.5941792046069168e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.912739150723285, f1=0.9197761194029851, best_f1=0.9231468849477036\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 230.70it/s]\n",
            "load_f1 = 0.9268738574040218\n",
            "real_f1 = 0.9201661282879556\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 240.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72a6a2f-b7b0-4ceb-defa-56d6340d1bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 83.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=62477ee1a058321f1b20e81b3850c232336b7b944764cdf30195b3094370e073\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y_4au5fk/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db7cc1e-b57e-46e3-db92-5443e7ab78fa"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5633316040039062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.423076923076923, f1=0.4444444444444444, best_f1=0.4444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5009447932243347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.3880597014925373, f1=0.42857142857142855, best_f1=0.4444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6335073113441467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.39285714285714285, f1=0.43478260869565216, best_f1=0.4444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 0, loss: 0.4546206295490265\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6285714285714286, f1=0.39999999999999997, best_f1=0.39999999999999997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.36902734637260437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6875000000000001, f1=0.4137931034482759, best_f1=0.4137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4033195376396179\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.56, f1=0.5, best_f1=0.4137931034482759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.357076495885849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7058823529411764, f1=0.5405405405405405, best_f1=0.5405405405405405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06275928765535355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7647058823529412, f1=0.5294117647058824, best_f1=0.5294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08164987713098526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7647058823529412, f1=0.689655172413793, best_f1=0.5294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.047943536192178726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7857142857142857, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01596740260720253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7999999999999999, f1=0.5925925925925927, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01036353874951601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7999999999999999, f1=0.6666666666666666, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02195422537624836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8275862068965518, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010301643051207066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7999999999999999, f1=0.6923076923076924, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008771195076406002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7999999999999999, f1=0.6923076923076924, best_f1=0.6666666666666666\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 112923.57it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8125000000000001\n",
            "real_f1 = 0.8235294117647058\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 197.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044c8c42-6d7c-4a69-8c50-d4a2e59a130f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6055591702461243\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5852863192558289\n",
            "step: 20, loss: 0.454665869474411\n",
            "step: 30, loss: 0.3573714792728424\n",
            "step: 40, loss: 0.20722141861915588\n",
            "step: 50, loss: 0.025822214782238007\n",
            "step: 60, loss: 0.03861261159181595\n",
            "step: 70, loss: 0.16726341843605042\n",
            "step: 80, loss: 0.10879834741353989\n",
            "step: 90, loss: 0.14676155149936676\n",
            "step: 100, loss: 0.007978096604347229\n",
            "step: 110, loss: 0.09027930349111557\n",
            "step: 120, loss: 0.008005037903785706\n",
            "step: 130, loss: 0.07899255305528641\n",
            "step: 140, loss: 0.0035501988604664803\n",
            "step: 150, loss: 0.18242545425891876\n",
            "step: 160, loss: 0.04731030762195587\n",
            "step: 170, loss: 0.16509166359901428\n",
            "step: 180, loss: 0.13779063522815704\n",
            "step: 190, loss: 0.06580109894275665\n",
            "step: 200, loss: 0.030287783592939377\n",
            "step: 210, loss: 0.007262904662638903\n",
            "step: 220, loss: 0.020601186901330948\n",
            "step: 230, loss: 0.03957098349928856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9774774774774775, f1=0.9688195991091313, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007579081691801548\n",
            "step: 10, loss: 0.0030141756869852543\n",
            "step: 20, loss: 0.1023208349943161\n",
            "step: 30, loss: 0.17158812284469604\n",
            "step: 40, loss: 0.04657228663563728\n",
            "step: 50, loss: 0.01158084161579609\n",
            "step: 60, loss: 0.002663396066054702\n",
            "step: 70, loss: 0.1757393181324005\n",
            "step: 80, loss: 0.01007208600640297\n",
            "step: 90, loss: 0.14558203518390656\n",
            "step: 100, loss: 0.05346151068806648\n",
            "step: 110, loss: 0.0636150911450386\n",
            "step: 120, loss: 0.10839402675628662\n",
            "step: 130, loss: 0.053406935185194016\n",
            "step: 140, loss: 0.0018055529799312353\n",
            "step: 150, loss: 0.06165565922856331\n",
            "step: 160, loss: 0.008991433307528496\n",
            "step: 170, loss: 0.008564339950680733\n",
            "step: 180, loss: 0.005043931771069765\n",
            "step: 190, loss: 0.09418854862451553\n",
            "step: 200, loss: 0.0016451262636110187\n",
            "step: 210, loss: 0.0011603685561567545\n",
            "step: 220, loss: 0.10147646069526672\n",
            "step: 230, loss: 0.04993358999490738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.977728285077951, f1=0.9788182831661093, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011733615770936012\n",
            "step: 10, loss: 0.0036573426332324743\n",
            "step: 20, loss: 0.0010944780660793185\n",
            "step: 30, loss: 0.0515265092253685\n",
            "step: 40, loss: 0.10680070519447327\n",
            "step: 50, loss: 0.003712903708219528\n",
            "step: 60, loss: 0.11637851595878601\n",
            "step: 70, loss: 0.0027670543640851974\n",
            "step: 80, loss: 0.0013706773752346635\n",
            "step: 90, loss: 0.028028061613440514\n",
            "step: 100, loss: 0.004516322165727615\n",
            "step: 110, loss: 0.0005310337874107063\n",
            "step: 120, loss: 0.004335614386945963\n",
            "step: 130, loss: 0.0039062106516212225\n",
            "step: 140, loss: 0.016710884869098663\n",
            "step: 150, loss: 0.0010372984688729048\n",
            "step: 160, loss: 0.04700840637087822\n",
            "step: 170, loss: 0.0034742930438369513\n",
            "step: 180, loss: 0.015604253858327866\n",
            "step: 190, loss: 0.001740228501148522\n",
            "step: 200, loss: 0.03255200386047363\n",
            "step: 210, loss: 0.000547505565918982\n",
            "step: 220, loss: 0.0003407473850529641\n",
            "step: 230, loss: 0.0005453316261991858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.975, f1=0.9625425652667423, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023915874771773815\n",
            "step: 10, loss: 0.0005011584726162255\n",
            "step: 20, loss: 0.0005161755252629519\n",
            "step: 30, loss: 0.0010341200977563858\n",
            "step: 40, loss: 0.0076985531486570835\n",
            "step: 50, loss: 0.001397031475789845\n",
            "step: 60, loss: 0.0184420607984066\n",
            "step: 70, loss: 0.0004724173923023045\n",
            "step: 80, loss: 0.00041587563464418054\n",
            "step: 90, loss: 0.017065279185771942\n",
            "step: 100, loss: 0.0003661029040813446\n",
            "step: 110, loss: 0.00034144226810894907\n",
            "step: 120, loss: 0.0032880380749702454\n",
            "step: 130, loss: 0.016055583953857422\n",
            "step: 140, loss: 0.002790022175759077\n",
            "step: 150, loss: 0.14646948873996735\n",
            "step: 160, loss: 0.0013623839477077127\n",
            "step: 170, loss: 0.006577782332897186\n",
            "step: 180, loss: 0.0006034629186615348\n",
            "step: 190, loss: 0.0047114151529967785\n",
            "step: 200, loss: 0.00046504041529260576\n",
            "step: 210, loss: 0.06941074132919312\n",
            "step: 220, loss: 0.00044915222679264843\n",
            "step: 230, loss: 0.06174978241324425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9771689497716894, f1=0.9761634506242906, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004730393644422293\n",
            "step: 10, loss: 0.0012942003086209297\n",
            "step: 20, loss: 0.0012684406246989965\n",
            "step: 30, loss: 0.0004096674092579633\n",
            "step: 40, loss: 0.00038797827437520027\n",
            "step: 50, loss: 0.00027478020638227463\n",
            "step: 60, loss: 0.00036905601155012846\n",
            "step: 70, loss: 0.00014388565614353865\n",
            "step: 80, loss: 0.00016197661170735955\n",
            "step: 90, loss: 0.0003225479740649462\n",
            "step: 100, loss: 0.0003884286852553487\n",
            "step: 110, loss: 0.0002618397120386362\n",
            "step: 120, loss: 9.015649266075343e-05\n",
            "step: 130, loss: 0.000720409385394305\n",
            "step: 140, loss: 0.0015755366766825318\n",
            "step: 150, loss: 0.00010506659600650892\n",
            "step: 160, loss: 0.00034737412352114916\n",
            "step: 170, loss: 0.000579283747356385\n",
            "step: 180, loss: 0.0007977465866133571\n",
            "step: 190, loss: 0.0010968631831929088\n",
            "step: 200, loss: 0.005084012169390917\n",
            "step: 210, loss: 0.0003079433226957917\n",
            "step: 220, loss: 0.0020095654763281345\n",
            "step: 230, loss: 0.0017903036205098033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9832402234636871, f1=0.9755555555555556, best_f1=0.9755555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004256174899637699\n",
            "step: 10, loss: 0.00041093354229815304\n",
            "step: 20, loss: 0.043142642825841904\n",
            "step: 30, loss: 0.003404435934498906\n",
            "step: 40, loss: 0.00046823505545035005\n",
            "step: 50, loss: 0.0007905569509603083\n",
            "step: 60, loss: 0.00023590629280079156\n",
            "step: 70, loss: 0.006043487228453159\n",
            "step: 80, loss: 0.004146998282521963\n",
            "step: 90, loss: 0.000492514984216541\n",
            "step: 100, loss: 0.05531885847449303\n",
            "step: 110, loss: 0.0020854424219578505\n",
            "step: 120, loss: 0.010133427567780018\n",
            "step: 130, loss: 0.0009283495601266623\n",
            "step: 140, loss: 0.0002475869841873646\n",
            "step: 150, loss: 0.0006435008253902197\n",
            "step: 160, loss: 0.19604937732219696\n",
            "step: 170, loss: 0.0056843943893909454\n",
            "step: 180, loss: 0.004572867415845394\n",
            "step: 190, loss: 0.014902906492352486\n",
            "step: 200, loss: 0.0008877007639966905\n",
            "step: 210, loss: 0.0007033881265670061\n",
            "step: 220, loss: 0.00031349711935035884\n",
            "step: 230, loss: 0.006228654179722071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9809203142536477, f1=0.9787234042553192, best_f1=0.9755555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005255151190795004\n",
            "step: 10, loss: 0.0002334881282877177\n",
            "step: 20, loss: 0.0002645023923832923\n",
            "step: 30, loss: 0.0003831123758573085\n",
            "step: 40, loss: 0.00020710818353109062\n",
            "step: 50, loss: 0.00010692428622860461\n",
            "step: 60, loss: 0.00020009076979476959\n",
            "step: 70, loss: 0.0005452879122458398\n",
            "step: 80, loss: 0.0006875034305267036\n",
            "step: 90, loss: 0.00020116163068450987\n",
            "step: 100, loss: 0.00026281282771378756\n",
            "step: 110, loss: 0.025745224207639694\n",
            "step: 120, loss: 0.007178918924182653\n",
            "step: 130, loss: 0.000338055775500834\n",
            "step: 140, loss: 0.0006752467015758157\n",
            "step: 150, loss: 0.00016820286691654474\n",
            "step: 160, loss: 0.016375236213207245\n",
            "step: 170, loss: 0.014085136353969574\n",
            "step: 180, loss: 0.002314559882506728\n",
            "step: 190, loss: 0.0002086028689518571\n",
            "step: 200, loss: 0.04364656284451485\n",
            "step: 210, loss: 8.333216101163998e-05\n",
            "step: 220, loss: 0.0004702533478848636\n",
            "step: 230, loss: 0.014432416297495365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9775280898876404, f1=0.9788182831661093, best_f1=0.9755555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004350001981947571\n",
            "step: 10, loss: 0.0004767899517901242\n",
            "step: 20, loss: 0.00010109033610206097\n",
            "step: 30, loss: 0.0003835110692307353\n",
            "step: 40, loss: 0.00020936198416166008\n",
            "step: 50, loss: 0.00011552366777323186\n",
            "step: 60, loss: 0.010409562848508358\n",
            "step: 70, loss: 0.0011659780284389853\n",
            "step: 80, loss: 0.0017969643231481314\n",
            "step: 90, loss: 0.00020523136481642723\n",
            "step: 100, loss: 0.002508903155103326\n",
            "step: 110, loss: 0.0004043311928398907\n",
            "step: 120, loss: 0.006769102998077869\n",
            "step: 130, loss: 0.0008587163174524903\n",
            "step: 140, loss: 0.0001996508362935856\n",
            "step: 150, loss: 0.00038868660340085626\n",
            "step: 160, loss: 0.00036929413909092546\n",
            "step: 170, loss: 9.068537474377081e-05\n",
            "step: 180, loss: 0.00029200356220826507\n",
            "step: 190, loss: 0.0002668771194294095\n",
            "step: 200, loss: 0.00013441329065244645\n",
            "step: 210, loss: 0.00018455601821187884\n",
            "step: 220, loss: 0.0003412798105273396\n",
            "step: 230, loss: 0.00018683556118048728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9796380090497738, f1=0.9842696629213483, best_f1=0.9755555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.923019777284935e-05\n",
            "step: 10, loss: 0.00034609783324413\n",
            "step: 20, loss: 0.00021286441187839955\n",
            "step: 30, loss: 5.733142461394891e-05\n",
            "step: 40, loss: 0.00028014928102493286\n",
            "step: 50, loss: 7.96669046394527e-05\n",
            "step: 60, loss: 0.00017369927081745118\n",
            "step: 70, loss: 0.0006317768711596727\n",
            "step: 90, loss: 0.00013084657257422805\n",
            "step: 100, loss: 0.00041396712185814977\n",
            "step: 110, loss: 5.038766539655626e-05\n",
            "step: 120, loss: 6.820580165367573e-05\n",
            "step: 130, loss: 0.0001970106241060421\n",
            "step: 140, loss: 0.0002912391792051494\n",
            "step: 150, loss: 0.0015094360569491982\n",
            "step: 160, loss: 0.00011692170664900914\n",
            "step: 170, loss: 0.0001311389059992507\n",
            "step: 180, loss: 0.0077492413111031055\n",
            "step: 190, loss: 0.005877101793885231\n",
            "step: 200, loss: 7.906121027190238e-05\n",
            "step: 210, loss: 0.002270861528813839\n",
            "step: 220, loss: 0.00010900259803747758\n",
            "step: 230, loss: 0.00018289685249328613\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9841269841269841, f1=0.9853768278965129, best_f1=0.9853768278965129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011667631042655557\n",
            "step: 10, loss: 4.579444066621363e-05\n",
            "step: 20, loss: 9.46272411965765e-05\n",
            "step: 30, loss: 0.0029729041270911694\n",
            "step: 40, loss: 7.262279541464522e-05\n",
            "step: 50, loss: 0.00023935602803248912\n",
            "step: 60, loss: 0.000375115021597594\n",
            "step: 70, loss: 0.00034313034848310053\n",
            "step: 80, loss: 6.386646418832242e-05\n",
            "step: 90, loss: 0.00034265968133695424\n",
            "step: 100, loss: 0.00011812376760644838\n",
            "step: 110, loss: 0.006791003979742527\n",
            "step: 120, loss: 9.747875446919352e-05\n",
            "step: 130, loss: 6.374280928866938e-05\n",
            "step: 140, loss: 0.0004829768731724471\n",
            "step: 150, loss: 0.0005071352934464812\n",
            "step: 160, loss: 4.9777016101870686e-05\n",
            "step: 170, loss: 4.727518535219133e-05\n",
            "step: 180, loss: 0.00010075936006614938\n",
            "step: 190, loss: 0.00017739862960297614\n",
            "step: 200, loss: 5.29286298842635e-05\n",
            "step: 210, loss: 4.012325734947808e-05\n",
            "step: 220, loss: 0.0001633658102946356\n",
            "step: 230, loss: 0.00027923492598347366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9841986455981941, f1=0.9865168539325843, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.540679896716028e-05\n",
            "step: 10, loss: 0.0003059861483052373\n",
            "step: 20, loss: 9.368098108097911e-05\n",
            "step: 30, loss: 0.0002037174126598984\n",
            "step: 40, loss: 0.00012565402721520513\n",
            "step: 50, loss: 9.387926547788084e-05\n",
            "step: 60, loss: 8.189101936295629e-05\n",
            "step: 70, loss: 9.774988575372845e-05\n",
            "step: 80, loss: 0.000752945605199784\n",
            "step: 90, loss: 5.9076577599626034e-05\n",
            "step: 100, loss: 4.632482887245715e-05\n",
            "step: 110, loss: 0.00024517549900338054\n",
            "step: 120, loss: 4.070086288265884e-05\n",
            "step: 130, loss: 4.485599129111506e-05\n",
            "step: 140, loss: 6.434762326534837e-05\n",
            "step: 150, loss: 6.509339436888695e-05\n",
            "step: 160, loss: 4.661051571019925e-05\n",
            "step: 170, loss: 7.23503835615702e-05\n",
            "step: 180, loss: 5.5666281696176156e-05\n",
            "step: 190, loss: 4.3761621782323346e-05\n",
            "step: 200, loss: 9.43678169278428e-05\n",
            "step: 210, loss: 4.8191286623477936e-05\n",
            "step: 220, loss: 5.539829726330936e-05\n",
            "step: 230, loss: 6.692943861708045e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9864253393665158, f1=0.9865168539325843, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011602149606915191\n",
            "step: 10, loss: 0.00012402806896716356\n",
            "step: 20, loss: 5.177422281121835e-05\n",
            "step: 30, loss: 0.0002925751614384353\n",
            "step: 40, loss: 5.9694684750866145e-05\n",
            "step: 50, loss: 0.00013581149687524885\n",
            "step: 60, loss: 5.2787123422604054e-05\n",
            "step: 70, loss: 5.036059155827388e-05\n",
            "step: 80, loss: 4.112540773348883e-05\n",
            "step: 90, loss: 3.408854900044389e-05\n",
            "step: 100, loss: 3.33287644025404e-05\n",
            "step: 110, loss: 4.9054720875574276e-05\n",
            "step: 120, loss: 0.00012745067942887545\n",
            "step: 130, loss: 3.493456097203307e-05\n",
            "step: 140, loss: 0.009222407825291157\n",
            "step: 150, loss: 5.101411443320103e-05\n",
            "step: 160, loss: 4.6359156840480864e-05\n",
            "step: 170, loss: 4.0224076656159014e-05\n",
            "step: 180, loss: 0.00021235646272543818\n",
            "step: 190, loss: 3.601492426241748e-05\n",
            "step: 200, loss: 6.097215009503998e-05\n",
            "step: 210, loss: 5.954681182629429e-05\n",
            "step: 220, loss: 3.987228774349205e-05\n",
            "step: 230, loss: 0.05112675204873085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9864559819413092, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011620075383689255\n",
            "step: 10, loss: 0.0001545722916489467\n",
            "step: 20, loss: 6.720466626575217e-05\n",
            "step: 30, loss: 0.0003755600191652775\n",
            "step: 40, loss: 4.7412588173756376e-05\n",
            "step: 50, loss: 5.9369202062953264e-05\n",
            "step: 60, loss: 0.00014272346743382514\n",
            "step: 70, loss: 2.8959608243894763e-05\n",
            "step: 80, loss: 3.57466415152885e-05\n",
            "step: 90, loss: 0.0001681749999988824\n",
            "step: 100, loss: 2.4832284907461144e-05\n",
            "step: 110, loss: 6.935890269232914e-05\n",
            "step: 120, loss: 0.0026731661055237055\n",
            "step: 130, loss: 4.504382377490401e-05\n",
            "step: 140, loss: 3.7735069781774655e-05\n",
            "step: 150, loss: 3.5419016057858244e-05\n",
            "step: 160, loss: 3.69305198546499e-05\n",
            "step: 170, loss: 0.05706063285470009\n",
            "step: 180, loss: 3.8570135075133294e-05\n",
            "step: 190, loss: 3.970802208641544e-05\n",
            "step: 200, loss: 0.0001661783317103982\n",
            "step: 210, loss: 4.754496330861002e-05\n",
            "step: 220, loss: 4.1440849599894136e-05\n",
            "step: 230, loss: 3.816004755208269e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.984090909090909, f1=0.9864864864864865, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.97391922888346e-05\n",
            "step: 10, loss: 6.414781091734767e-05\n",
            "step: 20, loss: 5.3355030104285106e-05\n",
            "step: 30, loss: 8.327376417582855e-05\n",
            "step: 40, loss: 6.772499909857288e-05\n",
            "step: 50, loss: 5.031728869653307e-05\n",
            "step: 60, loss: 3.849890345009044e-05\n",
            "step: 70, loss: 5.862631951458752e-05\n",
            "step: 80, loss: 4.310319127398543e-05\n",
            "step: 90, loss: 7.534661563113332e-05\n",
            "step: 100, loss: 4.641398481908254e-05\n",
            "step: 110, loss: 4.542419264907949e-05\n",
            "step: 120, loss: 2.222090733994264e-05\n",
            "step: 130, loss: 3.683054819703102e-05\n",
            "step: 140, loss: 8.825497206998989e-05\n",
            "step: 150, loss: 7.776134589221328e-05\n",
            "step: 160, loss: 3.000253127538599e-05\n",
            "step: 170, loss: 2.386728192504961e-05\n",
            "step: 180, loss: 0.00010600915993563831\n",
            "step: 190, loss: 4.47107340733055e-05\n",
            "step: 200, loss: 3.9383379771607e-05\n",
            "step: 210, loss: 4.277205152902752e-05\n",
            "step: 220, loss: 4.240070120431483e-05\n",
            "step: 230, loss: 3.469258081167936e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9852774631936579, f1=0.9853768278965129, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.0171998989535496e-05\n",
            "step: 10, loss: 2.6891930247074924e-05\n",
            "step: 20, loss: 4.3973999709123746e-05\n",
            "step: 30, loss: 4.8996316763805225e-05\n",
            "step: 40, loss: 9.17761935852468e-05\n",
            "step: 50, loss: 0.0002851650060620159\n",
            "step: 60, loss: 4.9039961595553905e-05\n",
            "step: 70, loss: 0.005219289567321539\n",
            "step: 80, loss: 3.586556340451352e-05\n",
            "step: 90, loss: 0.0006977440789341927\n",
            "step: 100, loss: 3.431243749218993e-05\n",
            "step: 110, loss: 0.03527999296784401\n",
            "step: 120, loss: 9.186115494230762e-05\n",
            "step: 130, loss: 8.10051933513023e-05\n",
            "step: 140, loss: 2.9037846616120078e-05\n",
            "step: 150, loss: 0.00040804955642670393\n",
            "step: 160, loss: 2.666487307578791e-05\n",
            "step: 170, loss: 2.4321834644069895e-05\n",
            "step: 180, loss: 4.708948836196214e-05\n",
            "step: 190, loss: 4.489426646614447e-05\n",
            "step: 200, loss: 3.5389231925364584e-05\n",
            "step: 210, loss: 3.3086820621974766e-05\n",
            "step: 220, loss: 4.709099084720947e-05\n",
            "step: 230, loss: 2.9682436434086412e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9852774631936579, f1=0.9853768278965129, best_f1=0.9854096520763187\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 165.89it/s]\n",
            "load_f1 = 0.9842342342342343\n",
            "real_f1 = 0.9864864864864865\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 187.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae7206f-eb8a-4e4f-e66e-28ecc4b4b4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6475526690483093\n",
            "step: 10, loss: 0.5194084644317627\n",
            "step: 20, loss: 0.5812554359436035\n",
            "step: 30, loss: 0.3102135956287384\n",
            "step: 40, loss: 0.28648480772972107\n",
            "step: 50, loss: 0.36937415599823\n",
            "step: 60, loss: 0.13737131655216217\n",
            "step: 70, loss: 0.10971707850694656\n",
            "step: 80, loss: 0.1269596666097641\n",
            "step: 90, loss: 0.3133040964603424\n",
            "step: 100, loss: 0.046357687562704086\n",
            "step: 110, loss: 0.12801088392734528\n",
            "step: 120, loss: 0.13510605692863464\n",
            "step: 130, loss: 0.05705660209059715\n",
            "step: 140, loss: 0.048300810158252716\n",
            "step: 150, loss: 0.10401435196399689\n",
            "step: 160, loss: 0.04120679199695587\n",
            "step: 170, loss: 0.26204371452331543\n",
            "step: 180, loss: 0.07638775557279587\n",
            "step: 190, loss: 0.03451981395483017\n",
            "step: 200, loss: 0.14604265987873077\n",
            "step: 210, loss: 0.07636280357837677\n",
            "step: 220, loss: 0.2194308489561081\n",
            "step: 230, loss: 0.17632916569709778\n",
            "step: 240, loss: 0.06940022855997086\n",
            "step: 250, loss: 0.045930683612823486\n",
            "step: 260, loss: 0.14694996178150177\n",
            "step: 270, loss: 0.028020335361361504\n",
            "step: 280, loss: 0.047457654029130936\n",
            "step: 290, loss: 0.2523806095123291\n",
            "step: 300, loss: 0.11165594309568405\n",
            "step: 310, loss: 0.1951006054878235\n",
            "step: 320, loss: 0.05837387591600418\n",
            "step: 330, loss: 0.057944945991039276\n",
            "step: 340, loss: 0.071994848549366\n",
            "step: 350, loss: 0.055454425513744354\n",
            "step: 360, loss: 0.10502538830041885\n",
            "step: 370, loss: 0.12246477603912354\n",
            "step: 380, loss: 0.009781117551028728\n",
            "step: 390, loss: 0.1604590117931366\n",
            "step: 400, loss: 0.31134891510009766\n",
            "step: 410, loss: 0.08445728570222855\n",
            "step: 420, loss: 0.06073335185647011\n",
            "step: 430, loss: 0.19966831803321838\n",
            "step: 440, loss: 0.02099340222775936\n",
            "step: 450, loss: 0.03957200050354004\n",
            "step: 460, loss: 0.10918697714805603\n",
            "step: 470, loss: 0.14604565501213074\n",
            "step: 480, loss: 0.09011848270893097\n",
            "step: 490, loss: 0.239435076713562\n",
            "step: 500, loss: 0.07203640788793564\n",
            "step: 510, loss: 0.04118490591645241\n",
            "step: 520, loss: 0.04691878706216812\n",
            "step: 530, loss: 0.003694736398756504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9129411764705881, f1=0.9072941176470588, best_f1=0.9072941176470588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15728911757469177\n",
            "step: 10, loss: 0.11061270534992218\n",
            "step: 20, loss: 0.12922589480876923\n",
            "step: 30, loss: 0.055941883474588394\n",
            "step: 40, loss: 0.09487546980381012\n",
            "step: 50, loss: 0.14391423761844635\n",
            "step: 60, loss: 0.030963772907853127\n",
            "step: 70, loss: 0.09032680839300156\n",
            "step: 80, loss: 0.11511018872261047\n",
            "step: 90, loss: 0.02245618775486946\n",
            "step: 100, loss: 0.030970066785812378\n",
            "step: 110, loss: 0.0697912648320198\n",
            "step: 120, loss: 0.0937146544456482\n",
            "step: 130, loss: 0.07676251977682114\n",
            "step: 140, loss: 0.039088450372219086\n",
            "step: 150, loss: 0.06688801944255829\n",
            "step: 160, loss: 0.02440699003636837\n",
            "step: 170, loss: 0.015143631026148796\n",
            "step: 180, loss: 0.023586289957165718\n",
            "step: 190, loss: 0.09224667400121689\n",
            "step: 200, loss: 0.015205373987555504\n",
            "step: 210, loss: 0.08214057981967926\n",
            "step: 220, loss: 0.07118942588567734\n",
            "step: 230, loss: 0.005542594939470291\n",
            "step: 240, loss: 0.12378881126642227\n",
            "step: 250, loss: 0.0404609851539135\n",
            "step: 260, loss: 0.004401014186441898\n",
            "step: 270, loss: 0.1385708749294281\n",
            "step: 280, loss: 0.0224036555737257\n",
            "step: 290, loss: 0.08603270351886749\n",
            "step: 300, loss: 0.15425458550453186\n",
            "step: 310, loss: 0.012561622075736523\n",
            "step: 320, loss: 0.08103913813829422\n",
            "step: 330, loss: 0.0716654509305954\n",
            "step: 340, loss: 0.01530509814620018\n",
            "step: 350, loss: 0.001814250135794282\n",
            "step: 360, loss: 0.0280461348593235\n",
            "step: 370, loss: 0.09600129723548889\n",
            "step: 380, loss: 0.043592147529125214\n",
            "step: 390, loss: 0.05747106298804283\n",
            "step: 400, loss: 0.08506372570991516\n",
            "step: 410, loss: 0.023884059861302376\n",
            "step: 420, loss: 0.03390556946396828\n",
            "step: 430, loss: 0.013480843976140022\n",
            "step: 440, loss: 0.03056824952363968\n",
            "step: 450, loss: 0.03936490789055824\n",
            "step: 460, loss: 0.09872011840343475\n",
            "step: 470, loss: 0.003796419594436884\n",
            "step: 480, loss: 0.19550542533397675\n",
            "step: 490, loss: 0.0235460102558136\n",
            "step: 500, loss: 0.3559230864048004\n",
            "step: 510, loss: 0.03170171007514\n",
            "step: 520, loss: 0.058750394731760025\n",
            "step: 530, loss: 0.1061643660068512\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.918869644484959, f1=0.9142335766423357, best_f1=0.9142335766423357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19976618885993958\n",
            "step: 10, loss: 0.0820949375629425\n",
            "step: 20, loss: 0.08761131763458252\n",
            "step: 30, loss: 0.1924351453781128\n",
            "step: 40, loss: 0.008697974495589733\n",
            "step: 50, loss: 0.03972434252500534\n",
            "step: 60, loss: 0.014728819951415062\n",
            "step: 70, loss: 0.01710415631532669\n",
            "step: 80, loss: 0.00793164037168026\n",
            "step: 90, loss: 0.005637395661324263\n",
            "step: 100, loss: 0.044168781489133835\n",
            "step: 110, loss: 0.004270361270755529\n",
            "step: 120, loss: 0.007858308963477612\n",
            "step: 130, loss: 0.009285910986363888\n",
            "step: 140, loss: 0.027796056121587753\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.048940662294626236\n",
            "step: 160, loss: 0.00344594893977046\n",
            "step: 170, loss: 0.07017601281404495\n",
            "step: 180, loss: 0.043610554188489914\n",
            "step: 190, loss: 0.01241295412182808\n",
            "step: 200, loss: 0.11310680955648422\n",
            "step: 210, loss: 0.029159672558307648\n",
            "step: 220, loss: 0.06354827433824539\n",
            "step: 230, loss: 0.08465202152729034\n",
            "step: 240, loss: 0.0019313807133585215\n",
            "step: 250, loss: 0.02282462641596794\n",
            "step: 260, loss: 0.03269397094845772\n",
            "step: 270, loss: 0.0036916041281074286\n",
            "step: 280, loss: 0.06213121861219406\n",
            "step: 290, loss: 0.006771902088075876\n",
            "step: 300, loss: 0.028781859204173088\n",
            "step: 310, loss: 0.045706938952207565\n",
            "step: 320, loss: 0.05036931857466698\n",
            "step: 330, loss: 0.01942676678299904\n",
            "step: 340, loss: 0.09785372763872147\n",
            "step: 350, loss: 0.01760297454893589\n",
            "step: 360, loss: 0.05505107343196869\n",
            "step: 370, loss: 0.013960686512291431\n",
            "step: 380, loss: 0.009744953364133835\n",
            "step: 390, loss: 0.01437817607074976\n",
            "step: 400, loss: 0.015125283971428871\n",
            "step: 410, loss: 0.03425661474466324\n",
            "step: 420, loss: 0.12420769780874252\n",
            "step: 430, loss: 0.050870586186647415\n",
            "step: 440, loss: 0.019549265503883362\n",
            "step: 450, loss: 0.09778425097465515\n",
            "step: 460, loss: 0.03343911096453667\n",
            "step: 470, loss: 0.16689719259738922\n",
            "step: 480, loss: 0.1302901953458786\n",
            "step: 490, loss: 0.04413819685578346\n",
            "step: 500, loss: 0.024071233347058296\n",
            "step: 510, loss: 0.0253940187394619\n",
            "step: 520, loss: 0.013018791563808918\n",
            "step: 530, loss: 0.11614716798067093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.912639405204461, f1=0.9171322160148976, best_f1=0.9142335766423357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006903354078531265\n",
            "step: 10, loss: 0.012147132307291031\n",
            "step: 20, loss: 0.0021262443624436855\n",
            "step: 30, loss: 0.0022768815979361534\n",
            "step: 40, loss: 0.011285101994872093\n",
            "step: 50, loss: 0.003503953106701374\n",
            "step: 60, loss: 0.001661324524320662\n",
            "step: 70, loss: 0.11794069409370422\n",
            "step: 80, loss: 0.014990943484008312\n",
            "step: 90, loss: 0.03691495209932327\n",
            "step: 100, loss: 0.020360112190246582\n",
            "step: 110, loss: 0.05424991995096207\n",
            "step: 120, loss: 0.00034418547875247896\n",
            "step: 130, loss: 0.000971839064732194\n",
            "step: 140, loss: 0.010690946131944656\n",
            "step: 150, loss: 0.030806317925453186\n",
            "step: 160, loss: 0.022411614656448364\n",
            "step: 170, loss: 0.017796728760004044\n",
            "step: 180, loss: 0.0004859252367168665\n",
            "step: 190, loss: 0.05085033178329468\n",
            "step: 200, loss: 0.0011702632764354348\n",
            "step: 210, loss: 0.1291157752275467\n",
            "step: 220, loss: 0.002084763254970312\n",
            "step: 230, loss: 0.021368341520428658\n",
            "step: 240, loss: 0.008010433986783028\n",
            "step: 250, loss: 0.20227251946926117\n",
            "step: 260, loss: 0.0012724354164674878\n",
            "step: 270, loss: 0.018110981211066246\n",
            "step: 280, loss: 0.0792132019996643\n",
            "step: 290, loss: 0.0762021392583847\n",
            "step: 300, loss: 0.0010740881552919745\n",
            "step: 310, loss: 0.02886449359357357\n",
            "step: 320, loss: 0.002679713536053896\n",
            "step: 330, loss: 0.005405691917985678\n",
            "step: 340, loss: 0.0007742526941001415\n",
            "step: 350, loss: 0.00915561243891716\n",
            "step: 360, loss: 0.002197782974690199\n",
            "step: 370, loss: 0.0039216154254972935\n",
            "step: 380, loss: 0.0005355955800041556\n",
            "step: 390, loss: 0.004137495532631874\n",
            "step: 400, loss: 0.0032799215987324715\n",
            "step: 410, loss: 0.060222119092941284\n",
            "step: 420, loss: 0.009503178298473358\n",
            "step: 430, loss: 0.06631256639957428\n",
            "step: 440, loss: 0.0012272284366190434\n",
            "step: 450, loss: 0.0028488347306847572\n",
            "step: 460, loss: 0.007946287281811237\n",
            "step: 470, loss: 0.006103556137531996\n",
            "step: 480, loss: 0.002278259489685297\n",
            "step: 490, loss: 0.002272023120895028\n",
            "step: 500, loss: 0.0022970796562731266\n",
            "step: 510, loss: 0.04699166864156723\n",
            "step: 520, loss: 0.07630971074104309\n",
            "step: 530, loss: 0.026548707857728004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9189695550351288, f1=0.9094321914594088, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.034303952008485794\n",
            "step: 10, loss: 0.031059468165040016\n",
            "step: 20, loss: 0.013501479290425777\n",
            "step: 30, loss: 0.0066389041021466255\n",
            "step: 40, loss: 0.05039110407233238\n",
            "step: 50, loss: 0.014609509147703648\n",
            "step: 60, loss: 0.020137421786785126\n",
            "step: 70, loss: 0.06300407648086548\n",
            "step: 80, loss: 0.0006804143777117133\n",
            "step: 90, loss: 0.002005471847951412\n",
            "step: 100, loss: 0.012696090154349804\n",
            "step: 110, loss: 0.0007549701840616763\n",
            "step: 120, loss: 0.02213624306023121\n",
            "step: 130, loss: 0.0013526736292988062\n",
            "step: 140, loss: 0.007536903489381075\n",
            "step: 150, loss: 0.0012632521102204919\n",
            "step: 160, loss: 0.001119065098464489\n",
            "step: 170, loss: 0.007447876036167145\n",
            "step: 180, loss: 0.0004975746851414442\n",
            "step: 190, loss: 0.0051772985607385635\n",
            "step: 200, loss: 0.002542173722758889\n",
            "step: 210, loss: 0.04376976564526558\n",
            "step: 220, loss: 0.0010791313834488392\n",
            "step: 230, loss: 0.009879285469651222\n",
            "step: 240, loss: 0.031135134398937225\n",
            "step: 250, loss: 0.006718350574374199\n",
            "step: 260, loss: 0.022000109776854515\n",
            "step: 270, loss: 0.0019955073948949575\n",
            "step: 280, loss: 0.0004760606971103698\n",
            "step: 290, loss: 0.14881247282028198\n",
            "step: 300, loss: 0.014348629862070084\n",
            "step: 310, loss: 0.0036801618989557028\n",
            "step: 320, loss: 0.019157832488417625\n",
            "step: 330, loss: 0.09921836107969284\n",
            "step: 340, loss: 0.002393892500549555\n",
            "step: 350, loss: 0.0013666331069543958\n",
            "step: 360, loss: 0.00544959120452404\n",
            "step: 370, loss: 0.0090364133939147\n",
            "step: 380, loss: 0.0016467898385599256\n",
            "step: 390, loss: 0.0007515784818679094\n",
            "step: 400, loss: 0.0018094347324222326\n",
            "step: 410, loss: 0.0003310876782052219\n",
            "step: 420, loss: 0.0013067044783383608\n",
            "step: 430, loss: 0.0005280300974845886\n",
            "step: 440, loss: 0.004577052779495716\n",
            "step: 450, loss: 0.001415921957232058\n",
            "step: 460, loss: 0.007297406904399395\n",
            "step: 470, loss: 0.018342524766921997\n",
            "step: 480, loss: 0.0016014763386920094\n",
            "step: 490, loss: 0.0009209089330397546\n",
            "step: 500, loss: 0.010652400553226471\n",
            "step: 510, loss: 0.030674202367663383\n",
            "step: 520, loss: 0.0029225987382233143\n",
            "step: 530, loss: 0.01148254331201315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8976225133430374, f1=0.8847474252084355, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00859438069164753\n",
            "step: 10, loss: 0.00025668839225545526\n",
            "step: 20, loss: 0.031083494424819946\n",
            "step: 30, loss: 0.003140011103823781\n",
            "step: 40, loss: 0.0004734103276859969\n",
            "step: 50, loss: 0.026746060699224472\n",
            "step: 60, loss: 0.0008256566361524165\n",
            "step: 70, loss: 0.0011417287169024348\n",
            "step: 80, loss: 0.001190517912618816\n",
            "step: 90, loss: 0.0007684879237785935\n",
            "step: 100, loss: 0.004496612120419741\n",
            "step: 110, loss: 0.059142641723155975\n",
            "step: 120, loss: 0.00034274545032531023\n",
            "step: 130, loss: 0.0015135827707126737\n",
            "step: 140, loss: 0.05535205826163292\n",
            "step: 150, loss: 0.0010386689100414515\n",
            "step: 160, loss: 0.013195361010730267\n",
            "step: 170, loss: 0.0015411829808726907\n",
            "step: 180, loss: 0.00435964111238718\n",
            "step: 190, loss: 0.003603446064516902\n",
            "step: 200, loss: 0.000837191881146282\n",
            "step: 210, loss: 0.02132531814277172\n",
            "step: 220, loss: 0.0007057043840177357\n",
            "step: 230, loss: 0.00025132435257546604\n",
            "step: 240, loss: 0.010174205526709557\n",
            "step: 250, loss: 0.00011250031820964068\n",
            "step: 260, loss: 0.00010776556155178696\n",
            "step: 270, loss: 0.03616037964820862\n",
            "step: 280, loss: 0.035410430282354355\n",
            "step: 290, loss: 0.00023417483316734433\n",
            "step: 300, loss: 0.00038173282518982887\n",
            "step: 310, loss: 0.03294592350721359\n",
            "step: 320, loss: 0.00017396874318365008\n",
            "step: 330, loss: 0.000299056846415624\n",
            "step: 340, loss: 0.1343594491481781\n",
            "step: 350, loss: 0.015233081765472889\n",
            "step: 360, loss: 0.05749950557947159\n",
            "step: 370, loss: 0.0016004249919205904\n",
            "step: 380, loss: 0.0021673724986612797\n",
            "step: 390, loss: 0.011942695826292038\n",
            "step: 400, loss: 0.004116641357541084\n",
            "step: 410, loss: 0.0012052905512973666\n",
            "step: 420, loss: 0.0006830890197306871\n",
            "step: 430, loss: 0.0006144685903564095\n",
            "step: 440, loss: 6.882245361339301e-05\n",
            "step: 450, loss: 0.0005076533416286111\n",
            "step: 460, loss: 0.00029888463905081153\n",
            "step: 470, loss: 0.0032752761617302895\n",
            "step: 480, loss: 0.03372107818722725\n",
            "step: 490, loss: 0.005223463289439678\n",
            "step: 500, loss: 0.041612379252910614\n",
            "step: 510, loss: 0.006309003569185734\n",
            "step: 520, loss: 0.004039199557155371\n",
            "step: 530, loss: 0.0631539449095726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9168224299065421, f1=0.914365933551708, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021138740703463554\n",
            "step: 10, loss: 0.030865512788295746\n",
            "step: 20, loss: 0.009038502350449562\n",
            "step: 30, loss: 0.0028522962238639593\n",
            "step: 40, loss: 0.0021280935034155846\n",
            "step: 50, loss: 0.0022603515535593033\n",
            "step: 60, loss: 0.0012286623241379857\n",
            "step: 70, loss: 0.0012262089876458049\n",
            "step: 80, loss: 0.000744001823477447\n",
            "step: 90, loss: 0.00022187686408869922\n",
            "step: 100, loss: 0.0007984760450199246\n",
            "step: 110, loss: 0.007247973699122667\n",
            "step: 120, loss: 0.00019305842579342425\n",
            "step: 130, loss: 0.0011224589543417096\n",
            "step: 140, loss: 0.0007738239946775138\n",
            "step: 150, loss: 0.03312620148062706\n",
            "step: 160, loss: 0.0005327602848410606\n",
            "step: 170, loss: 0.009529408067464828\n",
            "step: 180, loss: 0.013860146515071392\n",
            "step: 190, loss: 0.005603880155831575\n",
            "step: 200, loss: 0.000738787348382175\n",
            "step: 210, loss: 0.04532988741993904\n",
            "step: 220, loss: 0.0003861208679154515\n",
            "step: 230, loss: 0.0001624583383090794\n",
            "step: 240, loss: 0.000511294521857053\n",
            "step: 250, loss: 0.01257243100553751\n",
            "step: 260, loss: 0.0004226786259096116\n",
            "step: 270, loss: 0.008291563019156456\n",
            "step: 280, loss: 0.000254934246186167\n",
            "step: 290, loss: 0.00011912180343642831\n",
            "step: 300, loss: 0.00045018200762569904\n",
            "step: 310, loss: 0.0005312298890203238\n",
            "step: 320, loss: 0.0008488434250466526\n",
            "step: 330, loss: 0.0009312854381278157\n",
            "step: 340, loss: 0.01944854110479355\n",
            "step: 350, loss: 0.0013798049185425043\n",
            "step: 360, loss: 0.0005721324705518782\n",
            "step: 370, loss: 0.00020718068117275834\n",
            "step: 380, loss: 0.01058126986026764\n",
            "step: 390, loss: 0.0009710180456750095\n",
            "step: 400, loss: 0.0005619517178274691\n",
            "step: 410, loss: 0.003171006916090846\n",
            "step: 420, loss: 0.0010704899905249476\n",
            "step: 430, loss: 0.00038121023681014776\n",
            "step: 440, loss: 0.00014395762991625816\n",
            "step: 450, loss: 0.0001264369348064065\n",
            "step: 460, loss: 0.00026405329117551446\n",
            "step: 470, loss: 0.11190611124038696\n",
            "step: 480, loss: 0.32346418499946594\n",
            "step: 490, loss: 0.00036436025402508676\n",
            "step: 500, loss: 0.0003204247332178056\n",
            "step: 510, loss: 0.0006574979051947594\n",
            "step: 520, loss: 0.005574389826506376\n",
            "step: 530, loss: 0.0056871334090828896\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9085027726432533, f1=0.914757103574702, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008161892183125019\n",
            "step: 10, loss: 0.0021981035824865103\n",
            "step: 20, loss: 0.00013390230014920235\n",
            "step: 30, loss: 0.003617379814386368\n",
            "step: 40, loss: 6.376604869728908e-05\n",
            "step: 50, loss: 0.007060988340526819\n",
            "step: 60, loss: 0.01484239473938942\n",
            "step: 70, loss: 0.00016919299378059804\n",
            "step: 80, loss: 0.009913679212331772\n",
            "step: 90, loss: 0.00020645074255298823\n",
            "step: 100, loss: 4.952052404405549e-05\n",
            "step: 110, loss: 6.47742926958017e-05\n",
            "step: 120, loss: 0.0001111838428187184\n",
            "step: 130, loss: 4.0816452383296564e-05\n",
            "step: 140, loss: 0.0007649147883057594\n",
            "step: 150, loss: 0.00026272441027686\n",
            "step: 160, loss: 0.0005826317938044667\n",
            "step: 170, loss: 0.006108174566179514\n",
            "step: 180, loss: 0.00013319934078026563\n",
            "step: 190, loss: 0.005233055911958218\n",
            "step: 200, loss: 0.0027583600021898746\n",
            "step: 210, loss: 0.05165165662765503\n",
            "step: 220, loss: 0.003255230840295553\n",
            "step: 230, loss: 0.0007225411827675998\n",
            "step: 240, loss: 9.11590686882846e-05\n",
            "step: 250, loss: 0.00014980587002355605\n",
            "step: 260, loss: 0.0005630443920381367\n",
            "step: 270, loss: 0.0004092224990017712\n",
            "step: 280, loss: 0.04261539503931999\n",
            "step: 290, loss: 4.8462847189512104e-05\n",
            "step: 300, loss: 0.06705867499113083\n",
            "step: 310, loss: 0.04321665316820145\n",
            "step: 320, loss: 0.0025792468804866076\n",
            "step: 330, loss: 0.000397457362851128\n",
            "step: 340, loss: 0.0009953195694833994\n",
            "step: 350, loss: 0.001012341002933681\n",
            "step: 360, loss: 6.930910603841767e-05\n",
            "step: 370, loss: 0.009330583736300468\n",
            "step: 380, loss: 0.0005323751829564571\n",
            "step: 390, loss: 0.09089380502700806\n",
            "step: 400, loss: 0.0003556983429007232\n",
            "step: 410, loss: 0.00010271271457895637\n",
            "step: 420, loss: 0.002824322087690234\n",
            "step: 430, loss: 0.0003479104198049754\n",
            "step: 440, loss: 0.00011791344149969518\n",
            "step: 450, loss: 0.00014205207116901875\n",
            "step: 460, loss: 6.203891098266467e-05\n",
            "step: 470, loss: 0.00016395142301917076\n",
            "step: 480, loss: 4.987855572835542e-05\n",
            "step: 490, loss: 0.01634739153087139\n",
            "step: 500, loss: 0.0001761188468663022\n",
            "step: 510, loss: 0.0001671329519012943\n",
            "step: 520, loss: 0.0016035328153520823\n",
            "step: 530, loss: 4.0119011828210205e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9079869219990658, f1=0.9154600653900047, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004371672694105655\n",
            "step: 10, loss: 7.417505548801273e-05\n",
            "step: 20, loss: 0.0006395485252141953\n",
            "step: 30, loss: 0.0009607246611267328\n",
            "step: 40, loss: 0.007309927139431238\n",
            "step: 50, loss: 8.383204840356484e-05\n",
            "step: 60, loss: 6.706149724777788e-05\n",
            "step: 70, loss: 0.0004886408569291234\n",
            "step: 80, loss: 0.042594168335199356\n",
            "step: 90, loss: 0.0003232956805732101\n",
            "step: 100, loss: 4.544291732599959e-05\n",
            "step: 110, loss: 0.0013132726307958364\n",
            "step: 120, loss: 0.0007336000562645495\n",
            "step: 130, loss: 0.000475927023217082\n",
            "step: 140, loss: 0.008268343284726143\n",
            "step: 150, loss: 0.0013823435874655843\n",
            "step: 160, loss: 0.0006200443604029715\n",
            "step: 170, loss: 0.05145777016878128\n",
            "step: 180, loss: 0.00011899933451786637\n",
            "step: 190, loss: 6.754262722097337e-05\n",
            "step: 200, loss: 0.000317490310408175\n",
            "step: 210, loss: 6.595835293410346e-05\n",
            "step: 220, loss: 0.00048510098713450134\n",
            "step: 230, loss: 0.0003027700586244464\n",
            "step: 240, loss: 0.0020124625880271196\n",
            "step: 250, loss: 0.0004322564054746181\n",
            "step: 260, loss: 0.0030160201713442802\n",
            "step: 270, loss: 0.0002445354766678065\n",
            "step: 280, loss: 0.0030341814272105694\n",
            "step: 290, loss: 0.0024900685530155897\n",
            "step: 300, loss: 0.00026158447144553065\n",
            "step: 310, loss: 0.00625130208209157\n",
            "step: 320, loss: 0.0011897783260792494\n",
            "step: 330, loss: 0.00014676763385068625\n",
            "step: 340, loss: 3.15670549753122e-05\n",
            "step: 350, loss: 0.0009212181903421879\n",
            "step: 360, loss: 2.550260251155123e-05\n",
            "step: 370, loss: 0.0018625385127961636\n",
            "step: 380, loss: 4.490962965064682e-05\n",
            "step: 390, loss: 6.171272980282083e-05\n",
            "step: 400, loss: 0.0023990727495402098\n",
            "step: 410, loss: 3.303799530840479e-05\n",
            "step: 420, loss: 3.260256926296279e-05\n",
            "step: 430, loss: 5.1700477342819795e-05\n",
            "step: 440, loss: 0.0005697357119061053\n",
            "step: 450, loss: 3.293026384199038e-05\n",
            "step: 460, loss: 0.00013391293759923428\n",
            "step: 470, loss: 4.238479596097022e-05\n",
            "step: 480, loss: 3.1793802918400615e-05\n",
            "step: 490, loss: 3.2094645575853065e-05\n",
            "step: 500, loss: 0.0016696015372872353\n",
            "step: 510, loss: 0.00042989078792743385\n",
            "step: 520, loss: 0.0004452371213119477\n",
            "step: 530, loss: 5.539916310226545e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9181094992980814, f1=0.911970190964136, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006327387527562678\n",
            "step: 10, loss: 0.002874536206945777\n",
            "step: 20, loss: 0.00015750338206999004\n",
            "step: 30, loss: 5.089142723591067e-05\n",
            "step: 40, loss: 0.01245136745274067\n",
            "step: 50, loss: 7.135529449442402e-05\n",
            "step: 60, loss: 2.8843987820437178e-05\n",
            "step: 70, loss: 4.4989857997279614e-05\n",
            "step: 80, loss: 0.00015731978055555373\n",
            "step: 90, loss: 0.002465380821377039\n",
            "step: 100, loss: 3.591025961213745e-05\n",
            "step: 110, loss: 3.624162127380259e-05\n",
            "step: 120, loss: 6.012593803461641e-05\n",
            "step: 130, loss: 3.1886971555650234e-05\n",
            "step: 140, loss: 0.17289046943187714\n",
            "step: 150, loss: 0.00010355298581998795\n",
            "step: 160, loss: 0.0003673071041703224\n",
            "step: 170, loss: 0.00023711164249107242\n",
            "step: 180, loss: 0.001549050910398364\n",
            "step: 190, loss: 0.000267320399871096\n",
            "step: 200, loss: 0.010186060331761837\n",
            "step: 210, loss: 0.00010821336036315188\n",
            "step: 220, loss: 0.00028093188302591443\n",
            "step: 230, loss: 0.001608367427252233\n",
            "step: 240, loss: 9.616761235520244e-05\n",
            "step: 250, loss: 3.724326234078035e-05\n",
            "step: 260, loss: 0.00023918182705529034\n",
            "step: 270, loss: 0.00035474926698952913\n",
            "step: 280, loss: 0.0010498351184651256\n",
            "step: 290, loss: 3.141437628073618e-05\n",
            "step: 300, loss: 0.0003781050618272275\n",
            "step: 310, loss: 4.355792043497786e-05\n",
            "step: 320, loss: 0.010538417845964432\n",
            "step: 330, loss: 2.4768929506535642e-05\n",
            "step: 340, loss: 8.455736679024994e-05\n",
            "step: 350, loss: 9.659736679168418e-05\n",
            "step: 360, loss: 0.00020756985759362578\n",
            "step: 370, loss: 0.0009468315402045846\n",
            "step: 380, loss: 0.0010674792574718595\n",
            "step: 390, loss: 0.024636121466755867\n",
            "step: 400, loss: 0.001711286255158484\n",
            "step: 410, loss: 0.0007059540948830545\n",
            "step: 420, loss: 0.0098556624725461\n",
            "step: 430, loss: 5.4747684771427885e-05\n",
            "step: 440, loss: 0.004472455009818077\n",
            "step: 450, loss: 0.00013354394468478858\n",
            "step: 460, loss: 0.00017371762078255415\n",
            "step: 470, loss: 7.426935917465016e-05\n",
            "step: 480, loss: 4.8711361159803346e-05\n",
            "step: 490, loss: 8.08742770459503e-05\n",
            "step: 500, loss: 0.0024531958624720573\n",
            "step: 510, loss: 5.4534692026209086e-05\n",
            "step: 520, loss: 8.727952081244439e-05\n",
            "step: 530, loss: 0.00039252961869351566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9113323850165956, f1=0.9101283880171184, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026120280381292105\n",
            "step: 10, loss: 6.169656262500212e-05\n",
            "step: 20, loss: 4.4379641622072086e-05\n",
            "step: 30, loss: 3.2474883482791483e-05\n",
            "step: 40, loss: 0.0017637934070080519\n",
            "step: 50, loss: 0.00010025978554040194\n",
            "step: 60, loss: 0.008205479942262173\n",
            "step: 70, loss: 2.7238464099355042e-05\n",
            "step: 80, loss: 2.3025499103823677e-05\n",
            "step: 90, loss: 0.002746188547462225\n",
            "step: 100, loss: 0.00043296627700328827\n",
            "step: 110, loss: 5.162119850865565e-05\n",
            "step: 120, loss: 0.00012714246986433864\n",
            "step: 130, loss: 0.0001328450598521158\n",
            "step: 140, loss: 0.04115237668156624\n",
            "step: 150, loss: 2.6526344299782068e-05\n",
            "step: 160, loss: 3.783759893849492e-05\n",
            "step: 170, loss: 9.012466762214899e-05\n",
            "step: 180, loss: 0.0003535900905262679\n",
            "step: 190, loss: 0.00013664495781995356\n",
            "step: 200, loss: 0.02363531105220318\n",
            "step: 210, loss: 0.00017071266483981162\n",
            "step: 220, loss: 0.0041383784264326096\n",
            "step: 230, loss: 9.06982459127903e-05\n",
            "step: 240, loss: 0.00026738987071439624\n",
            "step: 250, loss: 3.4144159144489095e-05\n",
            "step: 260, loss: 4.540822919807397e-05\n",
            "step: 270, loss: 0.0001253620139323175\n",
            "step: 280, loss: 9.040463191922754e-05\n",
            "step: 290, loss: 0.00012706495181191713\n",
            "step: 300, loss: 0.000779221358243376\n",
            "step: 310, loss: 5.803669409942813e-05\n",
            "step: 320, loss: 0.00053317507263273\n",
            "step: 330, loss: 0.00016793468967080116\n",
            "step: 340, loss: 0.0008949824841693044\n",
            "step: 350, loss: 0.0008875555940903723\n",
            "step: 360, loss: 0.0006183020304888487\n",
            "step: 370, loss: 0.06932580471038818\n",
            "step: 380, loss: 0.00015069539949763566\n",
            "step: 390, loss: 8.285077637992799e-05\n",
            "step: 400, loss: 0.00021565098722930998\n",
            "step: 410, loss: 9.042817691806704e-05\n",
            "step: 420, loss: 0.004548641387373209\n",
            "step: 430, loss: 0.0018959176959469914\n",
            "step: 440, loss: 0.0002067666791845113\n",
            "step: 450, loss: 0.0129261938855052\n",
            "step: 460, loss: 0.00424047838896513\n",
            "step: 470, loss: 4.617167360265739e-05\n",
            "step: 480, loss: 0.0004593001212924719\n",
            "step: 490, loss: 0.0023483210243284702\n",
            "step: 500, loss: 0.00018102661124430597\n",
            "step: 510, loss: 0.0001581309479661286\n",
            "step: 520, loss: 6.30921422271058e-05\n",
            "step: 530, loss: 3.344021024531685e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9156626506024096, f1=0.9152073732718895, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006200377247296274\n",
            "step: 10, loss: 2.8146128897788003e-05\n",
            "step: 20, loss: 0.00017681684403214604\n",
            "step: 30, loss: 5.541515565710142e-05\n",
            "step: 40, loss: 3.739957173820585e-05\n",
            "step: 50, loss: 0.0015049951616674662\n",
            "step: 60, loss: 9.405118908034638e-05\n",
            "step: 70, loss: 3.6763158277608454e-05\n",
            "step: 80, loss: 3.7794627132825553e-05\n",
            "step: 90, loss: 5.3987438150215894e-05\n",
            "step: 100, loss: 7.524218381149694e-05\n",
            "step: 110, loss: 3.505633139866404e-05\n",
            "step: 120, loss: 0.0007504229433834553\n",
            "step: 130, loss: 0.001396653475239873\n",
            "step: 140, loss: 2.0391882571857423e-05\n",
            "step: 150, loss: 1.6577303540543653e-05\n",
            "step: 160, loss: 1.6633188351988792e-05\n",
            "step: 170, loss: 2.7506677724886686e-05\n",
            "step: 180, loss: 3.0973289540270343e-05\n",
            "step: 190, loss: 0.006682906299829483\n",
            "step: 200, loss: 0.0003531241382006556\n",
            "step: 210, loss: 5.0634283979889005e-05\n",
            "step: 220, loss: 0.00012844018056057394\n",
            "step: 230, loss: 6.890261283842847e-05\n",
            "step: 240, loss: 0.000489668978843838\n",
            "step: 250, loss: 1.9821891328319907e-05\n",
            "step: 260, loss: 1.7866232155938633e-05\n",
            "step: 270, loss: 0.0001494480384280905\n",
            "step: 280, loss: 9.416676039109007e-05\n",
            "step: 290, loss: 2.9141259801690467e-05\n",
            "step: 300, loss: 0.00024460643180646\n",
            "step: 310, loss: 6.855295941932127e-05\n",
            "step: 320, loss: 2.8643562473007478e-05\n",
            "step: 330, loss: 2.4265771571663208e-05\n",
            "step: 340, loss: 2.074938856821973e-05\n",
            "step: 350, loss: 0.00011154602543683723\n",
            "step: 360, loss: 0.012027988210320473\n",
            "step: 370, loss: 0.0006641854415647686\n",
            "step: 380, loss: 2.5201014068443328e-05\n",
            "step: 390, loss: 1.8868251572712325e-05\n",
            "step: 400, loss: 0.00010929592099273577\n",
            "step: 410, loss: 0.00017020072846207768\n",
            "step: 420, loss: 3.457842831267044e-05\n",
            "step: 430, loss: 8.063685527304187e-05\n",
            "step: 440, loss: 1.7814056263887323e-05\n",
            "step: 450, loss: 4.211467603454366e-05\n",
            "step: 460, loss: 2.0827550542890094e-05\n",
            "step: 470, loss: 2.2473899662145413e-05\n",
            "step: 480, loss: 0.032257188111543655\n",
            "step: 490, loss: 6.809565093135461e-05\n",
            "step: 500, loss: 2.242554910480976e-05\n",
            "step: 510, loss: 0.014646990224719048\n",
            "step: 520, loss: 2.5845298296189867e-05\n",
            "step: 530, loss: 0.00035540247336030006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9183577159037282, f1=0.9135338345864661, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038324533961713314\n",
            "step: 10, loss: 0.0001364270137855783\n",
            "step: 20, loss: 1.9758233975153416e-05\n",
            "step: 30, loss: 3.786891829804517e-05\n",
            "step: 40, loss: 2.143109122698661e-05\n",
            "step: 50, loss: 2.5398247089469805e-05\n",
            "step: 60, loss: 0.0008957526879385114\n",
            "step: 70, loss: 2.9623968657688238e-05\n",
            "step: 80, loss: 0.00025584158720448613\n",
            "step: 90, loss: 0.00047344379709102213\n",
            "step: 100, loss: 2.8325908715487458e-05\n",
            "step: 110, loss: 1.516170050308574e-05\n",
            "step: 120, loss: 2.02130431716796e-05\n",
            "step: 130, loss: 1.6316464098053984e-05\n",
            "step: 140, loss: 0.00021948548965156078\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.06960725039243698\n",
            "step: 160, loss: 0.00015390230691991746\n",
            "step: 170, loss: 4.552517566480674e-05\n",
            "step: 180, loss: 8.855920168571174e-05\n",
            "step: 190, loss: 8.76143021741882e-05\n",
            "step: 200, loss: 3.709373777383007e-05\n",
            "step: 210, loss: 6.180688069434837e-05\n",
            "step: 220, loss: 1.8383885617367923e-05\n",
            "step: 230, loss: 6.656058394582942e-05\n",
            "step: 240, loss: 0.00029027790878899395\n",
            "step: 250, loss: 1.3511502402252518e-05\n",
            "step: 260, loss: 1.9084060113527812e-05\n",
            "step: 270, loss: 1.6003607015591115e-05\n",
            "step: 280, loss: 3.371827187947929e-05\n",
            "step: 290, loss: 0.00018226777319796383\n",
            "step: 300, loss: 4.2954652599291876e-05\n",
            "step: 310, loss: 0.0005916185327805579\n",
            "step: 320, loss: 0.0008171998779289424\n",
            "step: 330, loss: 0.002351078437641263\n",
            "step: 340, loss: 4.320326479501091e-05\n",
            "step: 350, loss: 3.502541221678257e-05\n",
            "step: 360, loss: 2.794940519379452e-05\n",
            "step: 370, loss: 5.355244502425194e-05\n",
            "step: 380, loss: 0.00016908282123040408\n",
            "step: 390, loss: 0.00023334268189501017\n",
            "step: 400, loss: 1.7996288079302758e-05\n",
            "step: 410, loss: 6.338913954095915e-05\n",
            "step: 420, loss: 2.2980386347626336e-05\n",
            "step: 430, loss: 7.223717693705112e-05\n",
            "step: 440, loss: 0.003919374197721481\n",
            "step: 450, loss: 6.962820771150291e-05\n",
            "step: 460, loss: 0.0002231677935924381\n",
            "step: 470, loss: 0.0009941416792571545\n",
            "step: 480, loss: 2.8591601221705787e-05\n",
            "step: 490, loss: 9.571696864441037e-05\n",
            "step: 500, loss: 1.3112880878907163e-05\n",
            "step: 510, loss: 5.9667941968655214e-05\n",
            "step: 520, loss: 0.0010325354523956776\n",
            "step: 530, loss: 1.4230446140572894e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9153846153846154, f1=0.910569105691057, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.899928409140557e-05\n",
            "step: 10, loss: 1.8066806660499424e-05\n",
            "step: 20, loss: 0.0009561721817590296\n",
            "step: 30, loss: 4.52007916464936e-05\n",
            "step: 40, loss: 0.00031165804830379784\n",
            "step: 50, loss: 7.018347241682932e-05\n",
            "step: 60, loss: 2.1836551240994595e-05\n",
            "step: 70, loss: 4.7926572733558714e-05\n",
            "step: 80, loss: 3.098417437286116e-05\n",
            "step: 90, loss: 3.6090110370423645e-05\n",
            "step: 100, loss: 6.56280608382076e-05\n",
            "step: 110, loss: 2.327433139726054e-05\n",
            "step: 120, loss: 0.002799044596031308\n",
            "step: 130, loss: 5.6057226174743846e-05\n",
            "step: 140, loss: 1.5236194485623855e-05\n",
            "step: 150, loss: 1.6390788005082868e-05\n",
            "step: 160, loss: 2.779243004624732e-05\n",
            "step: 170, loss: 3.4858585422625765e-05\n",
            "step: 180, loss: 0.0004929906572215259\n",
            "step: 190, loss: 3.3584681659704074e-05\n",
            "step: 200, loss: 2.8560307328007184e-05\n",
            "step: 210, loss: 2.9374696168815717e-05\n",
            "step: 220, loss: 0.00036541357985697687\n",
            "step: 230, loss: 2.6072568289237097e-05\n",
            "step: 240, loss: 2.3401182261295617e-05\n",
            "step: 250, loss: 5.280922050587833e-05\n",
            "step: 260, loss: 0.0039491960778832436\n",
            "step: 270, loss: 3.153272336930968e-05\n",
            "step: 280, loss: 2.973186201415956e-05\n",
            "step: 290, loss: 6.853900413261726e-05\n",
            "step: 300, loss: 4.92453618790023e-05\n",
            "step: 310, loss: 2.381138619966805e-05\n",
            "step: 320, loss: 6.212318839970976e-05\n",
            "step: 330, loss: 3.471978197921999e-05\n",
            "step: 340, loss: 1.9233337297919206e-05\n",
            "step: 350, loss: 4.4660693674813956e-05\n",
            "step: 360, loss: 0.00010850441321963444\n",
            "step: 370, loss: 7.308884960366413e-05\n",
            "step: 380, loss: 1.7858739738585427e-05\n",
            "step: 390, loss: 0.013260887004435062\n",
            "step: 400, loss: 2.9879058274673298e-05\n",
            "step: 410, loss: 3.111628393526189e-05\n",
            "step: 420, loss: 8.56599654071033e-05\n",
            "step: 430, loss: 2.5949557311832905e-05\n",
            "step: 440, loss: 2.740605123108253e-05\n",
            "step: 450, loss: 0.000202861032448709\n",
            "step: 460, loss: 4.21556833316572e-05\n",
            "step: 470, loss: 3.8236266846070066e-05\n",
            "step: 480, loss: 3.0630930268671364e-05\n",
            "step: 490, loss: 1.9024615539819933e-05\n",
            "step: 500, loss: 1.3425776160147507e-05\n",
            "step: 510, loss: 5.657122892444022e-05\n",
            "step: 520, loss: 1.5202686881821137e-05\n",
            "step: 530, loss: 0.00012323373812250793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9173435260391782, f1=0.9158345221112696, best_f1=0.9094321914594088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.5665711291367188e-05\n",
            "step: 10, loss: 0.0006758849485777318\n",
            "step: 20, loss: 1.676722058618907e-05\n",
            "step: 30, loss: 3.122552880086005e-05\n",
            "step: 40, loss: 0.06981639564037323\n",
            "step: 50, loss: 0.0002254262799397111\n",
            "step: 60, loss: 1.2259804861969315e-05\n",
            "step: 70, loss: 0.0002021319087361917\n",
            "step: 80, loss: 8.443683327641338e-05\n",
            "step: 90, loss: 1.782518666004762e-05\n",
            "step: 100, loss: 3.1811017834115773e-05\n",
            "step: 110, loss: 1.9233260900364257e-05\n",
            "step: 120, loss: 1.5832289136596955e-05\n",
            "step: 130, loss: 0.0013349582441151142\n",
            "step: 140, loss: 9.53461931203492e-05\n",
            "step: 150, loss: 2.1695654140785336e-05\n",
            "step: 160, loss: 2.5319523047073744e-05\n",
            "step: 170, loss: 2.1884854504605755e-05\n",
            "step: 180, loss: 1.5824753063498065e-05\n",
            "step: 190, loss: 2.5647766960901208e-05\n",
            "step: 200, loss: 2.728707841015421e-05\n",
            "step: 210, loss: 2.427580693620257e-05\n",
            "step: 220, loss: 1.4286307305155788e-05\n",
            "step: 230, loss: 1.8093462131218985e-05\n",
            "step: 240, loss: 1.6480425983900204e-05\n",
            "step: 250, loss: 1.629416692594532e-05\n",
            "step: 260, loss: 1.6104193491628394e-05\n",
            "step: 270, loss: 1.4629049474024214e-05\n",
            "step: 280, loss: 4.002493369625881e-05\n",
            "step: 290, loss: 1.4178270248521585e-05\n",
            "step: 300, loss: 1.8171549527323805e-05\n",
            "step: 310, loss: 0.001645322423428297\n",
            "step: 320, loss: 1.4081325389270205e-05\n",
            "step: 330, loss: 0.00015273746976163238\n",
            "step: 340, loss: 1.455452547816094e-05\n",
            "step: 350, loss: 1.168614107882604e-05\n",
            "step: 360, loss: 0.00020324117213021964\n",
            "step: 370, loss: 4.6332737838383764e-05\n",
            "step: 380, loss: 7.849063695175573e-05\n",
            "step: 390, loss: 0.0001287843333557248\n",
            "step: 400, loss: 2.1706673578592017e-05\n",
            "step: 410, loss: 6.514896813314408e-05\n",
            "step: 420, loss: 3.988268872490153e-05\n",
            "step: 430, loss: 2.8702186682494357e-05\n",
            "step: 440, loss: 2.7275185857433826e-05\n",
            "step: 450, loss: 1.3332590242498554e-05\n",
            "step: 460, loss: 1.6569774743402377e-05\n",
            "step: 470, loss: 0.004403131082653999\n",
            "step: 480, loss: 1.1540841114765499e-05\n",
            "step: 490, loss: 1.6282991055049933e-05\n",
            "step: 500, loss: 2.8075630325474776e-05\n",
            "step: 510, loss: 1.913267988129519e-05\n",
            "step: 520, loss: 0.0001167122318292968\n",
            "step: 530, loss: 1.8965100025525317e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9184060721062618, f1=0.9217719132893496, best_f1=0.9094321914594088\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 193.73it/s]\n",
            "load_f1 = 0.9137055837563451\n",
            "real_f1 = 0.9118597138901707\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 191.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94436545-81fc-4f0b-8389-395954c002e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5546267628669739\n",
            "step: 10, loss: 0.3706984221935272\n",
            "step: 20, loss: 0.3917650580406189\n",
            "step: 30, loss: 0.3281698226928711\n",
            "step: 40, loss: 0.19150260090827942\n",
            "step: 50, loss: 0.46106916666030884\n",
            "step: 60, loss: 0.3170819878578186\n",
            "step: 70, loss: 0.20616382360458374\n",
            "step: 80, loss: 0.19859230518341064\n",
            "step: 90, loss: 0.41078051924705505\n",
            "step: 100, loss: 0.37071073055267334\n",
            "step: 110, loss: 0.28437939286231995\n",
            "step: 120, loss: 0.24777436256408691\n",
            "step: 130, loss: 0.3017869293689728\n",
            "step: 140, loss: 0.21023693680763245\n",
            "step: 150, loss: 0.28309738636016846\n",
            "step: 160, loss: 0.24851208925247192\n",
            "step: 170, loss: 0.28546035289764404\n",
            "step: 180, loss: 0.11384940892457962\n",
            "step: 190, loss: 0.22546467185020447\n",
            "step: 200, loss: 0.17514555156230927\n",
            "step: 210, loss: 0.26512837409973145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5330739299610895, f1=0.5402298850574712, best_f1=0.5402298850574712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13939817249774933\n",
            "step: 10, loss: 0.2532702386379242\n",
            "step: 20, loss: 0.19068442285060883\n",
            "step: 30, loss: 0.2951449155807495\n",
            "step: 40, loss: 0.15639862418174744\n",
            "step: 50, loss: 0.10967954993247986\n",
            "step: 60, loss: 0.3411538600921631\n",
            "step: 70, loss: 0.08769296109676361\n",
            "step: 80, loss: 0.14382494986057281\n",
            "step: 90, loss: 0.17843279242515564\n",
            "step: 100, loss: 0.060488175600767136\n",
            "step: 110, loss: 0.09324368834495544\n",
            "step: 120, loss: 0.16769948601722717\n",
            "step: 130, loss: 0.06563220918178558\n",
            "step: 140, loss: 0.21980564296245575\n",
            "step: 150, loss: 0.29852813482284546\n",
            "step: 160, loss: 0.24032190442085266\n",
            "step: 170, loss: 0.18672168254852295\n",
            "step: 180, loss: 0.17469710111618042\n",
            "step: 190, loss: 0.22362984716892242\n",
            "step: 200, loss: 0.06321366131305695\n",
            "step: 210, loss: 0.1308053582906723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5557809330628803, f1=0.5269978401727863, best_f1=0.5269978401727863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08784407377243042\n",
            "step: 10, loss: 0.14606241881847382\n",
            "step: 20, loss: 0.10036443173885345\n",
            "step: 30, loss: 0.19921965897083282\n",
            "step: 40, loss: 0.05107968673110008\n",
            "step: 50, loss: 0.06091108173131943\n",
            "step: 60, loss: 0.35216930508613586\n",
            "step: 70, loss: 0.1828518658876419\n",
            "step: 80, loss: 0.12617270648479462\n",
            "step: 90, loss: 0.06574933230876923\n",
            "step: 100, loss: 0.1718505173921585\n",
            "step: 110, loss: 0.16171517968177795\n",
            "step: 120, loss: 0.22614246606826782\n",
            "step: 130, loss: 0.194395050406456\n",
            "step: 140, loss: 0.09833331406116486\n",
            "step: 150, loss: 0.2665715217590332\n",
            "step: 160, loss: 0.037360768765211105\n",
            "step: 170, loss: 0.22010372579097748\n",
            "step: 180, loss: 0.14619113504886627\n",
            "step: 190, loss: 0.13746747374534607\n",
            "step: 200, loss: 0.041152313351631165\n",
            "step: 210, loss: 0.14536090195178986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5631469979296067, f1=0.559322033898305, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2135348916053772\n",
            "step: 10, loss: 0.10975370556116104\n",
            "step: 20, loss: 0.2515109181404114\n",
            "step: 30, loss: 0.24461764097213745\n",
            "step: 40, loss: 0.06556086242198944\n",
            "step: 50, loss: 0.10207764804363251\n",
            "step: 60, loss: 0.16753526031970978\n",
            "step: 70, loss: 0.23647283017635345\n",
            "step: 80, loss: 0.11663813143968582\n",
            "step: 90, loss: 0.04609997570514679\n",
            "step: 100, loss: 0.21941693127155304\n",
            "step: 110, loss: 0.07752154767513275\n",
            "step: 120, loss: 0.09510241448879242\n",
            "step: 130, loss: 0.21711428463459015\n",
            "step: 140, loss: 0.1296544075012207\n",
            "step: 150, loss: 0.1892731636762619\n",
            "step: 160, loss: 0.034320633858442307\n",
            "step: 170, loss: 0.159883052110672\n",
            "step: 180, loss: 0.36177924275398254\n",
            "step: 190, loss: 0.1853923350572586\n",
            "step: 200, loss: 0.22971196472644806\n",
            "step: 210, loss: 0.1961688995361328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.5581395348837208, f1=0.5434782608695652, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07986637949943542\n",
            "step: 10, loss: 0.012438011355698109\n",
            "step: 20, loss: 0.08920589089393616\n",
            "step: 30, loss: 0.028478896245360374\n",
            "step: 40, loss: 0.1120123490691185\n",
            "step: 50, loss: 0.017898626625537872\n",
            "step: 60, loss: 0.1224168911576271\n",
            "step: 70, loss: 0.22434523701667786\n",
            "step: 80, loss: 0.23916520178318024\n",
            "step: 90, loss: 0.1250927746295929\n",
            "step: 100, loss: 0.037918638437986374\n",
            "step: 110, loss: 0.12032382935285568\n",
            "step: 120, loss: 0.05205977335572243\n",
            "step: 130, loss: 0.09760177880525589\n",
            "step: 140, loss: 0.08427602052688599\n",
            "step: 150, loss: 0.022042715921998024\n",
            "step: 160, loss: 0.11283376067876816\n",
            "step: 170, loss: 0.08877435326576233\n",
            "step: 180, loss: 0.10119622200727463\n",
            "step: 190, loss: 0.08203335851430893\n",
            "step: 200, loss: 0.06177103519439697\n",
            "step: 210, loss: 0.04315566644072533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5410821643286573, f1=0.5203252032520326, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.048537299036979675\n",
            "step: 10, loss: 0.0683123990893364\n",
            "step: 20, loss: 0.04246288165450096\n",
            "step: 30, loss: 0.012206786312162876\n",
            "step: 40, loss: 0.06857240945100784\n",
            "step: 50, loss: 0.010477036237716675\n",
            "step: 60, loss: 0.05532844364643097\n",
            "step: 70, loss: 0.0021741942036896944\n",
            "step: 80, loss: 0.09084373712539673\n",
            "step: 90, loss: 0.3633959889411926\n",
            "step: 100, loss: 0.024137156084179878\n",
            "step: 110, loss: 0.012963572517037392\n",
            "step: 120, loss: 0.07262597978115082\n",
            "step: 130, loss: 0.09738346934318542\n",
            "step: 140, loss: 0.16546130180358887\n",
            "step: 150, loss: 0.1381332278251648\n",
            "step: 160, loss: 0.028539394959807396\n",
            "step: 170, loss: 0.0986403077840805\n",
            "step: 180, loss: 0.0051866560243070126\n",
            "step: 190, loss: 0.1396821290254593\n",
            "step: 200, loss: 0.009322868660092354\n",
            "step: 210, loss: 0.1412615180015564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5274261603375527, f1=0.49783549783549785, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01170356199145317\n",
            "step: 10, loss: 0.006382729858160019\n",
            "step: 20, loss: 0.011990259401500225\n",
            "step: 30, loss: 0.021737249568104744\n",
            "step: 40, loss: 0.005095252767205238\n",
            "step: 50, loss: 0.10858387500047684\n",
            "step: 60, loss: 0.03763938322663307\n",
            "step: 70, loss: 0.014209376648068428\n",
            "step: 80, loss: 0.07067367434501648\n",
            "step: 90, loss: 0.022653963416814804\n",
            "step: 100, loss: 0.0008143142331391573\n",
            "step: 110, loss: 0.006224920507520437\n",
            "step: 120, loss: 0.08020038157701492\n",
            "step: 130, loss: 0.09044057130813599\n",
            "step: 140, loss: 0.03073640540242195\n",
            "step: 150, loss: 0.04851997643709183\n",
            "step: 160, loss: 0.02267376147210598\n",
            "step: 170, loss: 0.06348323076963425\n",
            "step: 180, loss: 0.01173819787800312\n",
            "step: 190, loss: 0.029191166162490845\n",
            "step: 200, loss: 0.01724427565932274\n",
            "step: 210, loss: 0.11271966248750687\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5234042553191489, f1=0.502145922746781, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03145546838641167\n",
            "step: 10, loss: 0.021190855652093887\n",
            "step: 20, loss: 0.0019700266420841217\n",
            "step: 30, loss: 0.034458719193935394\n",
            "step: 40, loss: 0.010210487991571426\n",
            "step: 50, loss: 0.006376994773745537\n",
            "step: 60, loss: 0.069615438580513\n",
            "step: 70, loss: 0.19330938160419464\n",
            "step: 80, loss: 0.02241945080459118\n",
            "step: 90, loss: 0.0030247357208281755\n",
            "step: 100, loss: 0.008069063536822796\n",
            "step: 110, loss: 0.009739420376718044\n",
            "step: 120, loss: 0.000713236047886312\n",
            "step: 130, loss: 0.004523781593888998\n",
            "step: 140, loss: 0.0049616750329732895\n",
            "step: 150, loss: 0.021038463339209557\n",
            "step: 160, loss: 0.14868362247943878\n",
            "step: 170, loss: 0.0031675726640969515\n",
            "step: 180, loss: 0.032596688717603683\n",
            "step: 190, loss: 0.0023323078639805317\n",
            "step: 200, loss: 0.001055573346093297\n",
            "step: 210, loss: 0.13613830506801605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5352697095435683, f1=0.5161290322580645, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007909407839179039\n",
            "step: 10, loss: 0.016054445877671242\n",
            "step: 20, loss: 0.002083503408357501\n",
            "step: 30, loss: 0.07630036771297455\n",
            "step: 40, loss: 0.010893273167312145\n",
            "step: 50, loss: 0.014157356694340706\n",
            "step: 60, loss: 0.0648505687713623\n",
            "step: 70, loss: 0.005184843670576811\n",
            "step: 80, loss: 0.0026157123502343893\n",
            "step: 90, loss: 0.0087191853672266\n",
            "step: 100, loss: 0.02004418894648552\n",
            "step: 110, loss: 0.03763315826654434\n",
            "step: 120, loss: 0.0005008678999729455\n",
            "step: 130, loss: 0.07406828552484512\n",
            "step: 140, loss: 0.0411229282617569\n",
            "step: 150, loss: 0.059997525066137314\n",
            "step: 160, loss: 0.0015506462659686804\n",
            "step: 170, loss: 0.000982865341939032\n",
            "step: 180, loss: 0.017264772206544876\n",
            "step: 190, loss: 0.0028825157787650824\n",
            "step: 200, loss: 0.0015963707119226456\n",
            "step: 210, loss: 0.009345428086817265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5566600397614314, f1=0.4861660079051383, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023831842467188835\n",
            "step: 10, loss: 0.039551492780447006\n",
            "step: 20, loss: 0.02853352390229702\n",
            "step: 30, loss: 0.05760025233030319\n",
            "step: 40, loss: 0.006763600278645754\n",
            "step: 50, loss: 0.0006401030113920569\n",
            "step: 60, loss: 0.012040555477142334\n",
            "step: 70, loss: 0.013256575912237167\n",
            "step: 80, loss: 0.003992289770394564\n",
            "step: 90, loss: 0.0325687937438488\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 100, loss: 0.027860965579748154\n",
            "step: 110, loss: 0.040174245834350586\n",
            "step: 120, loss: 0.002053368603810668\n",
            "step: 130, loss: 0.162983700633049\n",
            "step: 140, loss: 0.008081983774900436\n",
            "step: 150, loss: 0.002030531642958522\n",
            "step: 160, loss: 0.0032904702238738537\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.15426577627658844\n",
            "step: 180, loss: 0.00406343350186944\n",
            "step: 190, loss: 0.06998149305582047\n",
            "step: 200, loss: 0.0008490580949001014\n",
            "step: 210, loss: 0.03595159947872162\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5327313769751693, f1=0.4658823529411765, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03678065165877342\n",
            "step: 10, loss: 0.018562588840723038\n",
            "step: 20, loss: 0.030542364344000816\n",
            "step: 30, loss: 0.00016218902601394802\n",
            "step: 40, loss: 0.01748044230043888\n",
            "step: 50, loss: 0.002939770231023431\n",
            "step: 60, loss: 0.06173865124583244\n",
            "step: 70, loss: 0.012449365109205246\n",
            "step: 80, loss: 0.01901380717754364\n",
            "step: 90, loss: 0.0005323176155798137\n",
            "step: 100, loss: 0.001129221636801958\n",
            "step: 110, loss: 0.008953913114964962\n",
            "step: 120, loss: 0.028866562992334366\n",
            "step: 130, loss: 0.011857716366648674\n",
            "step: 140, loss: 0.00020472059259191155\n",
            "step: 150, loss: 0.0002160118892788887\n",
            "step: 160, loss: 0.0003826832980848849\n",
            "step: 170, loss: 0.10085194557905197\n",
            "step: 180, loss: 0.02373097464442253\n",
            "step: 190, loss: 0.005401375703513622\n",
            "step: 200, loss: 0.0006625454407185316\n",
            "step: 210, loss: 0.0003190015268046409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5343511450381679, f1=0.4940239043824701, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08060410618782043\n",
            "step: 10, loss: 0.0007647340535186231\n",
            "step: 20, loss: 0.00420095631852746\n",
            "step: 30, loss: 0.003413196187466383\n",
            "step: 40, loss: 0.01183103397488594\n",
            "step: 50, loss: 0.0035181534476578236\n",
            "step: 60, loss: 0.00608406588435173\n",
            "step: 70, loss: 0.0022330975625663996\n",
            "step: 80, loss: 0.00026484241243451834\n",
            "step: 90, loss: 0.00046344890142790973\n",
            "step: 100, loss: 0.004131681751459837\n",
            "step: 110, loss: 0.002418381394818425\n",
            "step: 120, loss: 0.005728491116315126\n",
            "step: 130, loss: 0.0007227729656733572\n",
            "step: 140, loss: 0.0003773195203393698\n",
            "step: 150, loss: 0.013907392509281635\n",
            "step: 160, loss: 0.00021791439212393016\n",
            "step: 170, loss: 0.0005826516426168382\n",
            "step: 180, loss: 0.0002545176539570093\n",
            "step: 190, loss: 0.003776815254241228\n",
            "step: 200, loss: 0.0004929135320708156\n",
            "step: 210, loss: 0.005808765534311533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5514223194748359, f1=0.4801864801864802, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024107927456498146\n",
            "step: 10, loss: 0.0003024096367880702\n",
            "step: 20, loss: 0.024628661572933197\n",
            "step: 30, loss: 0.010101527906954288\n",
            "step: 40, loss: 0.0004808926023542881\n",
            "step: 50, loss: 0.00022008837549947202\n",
            "step: 60, loss: 0.0017895149067044258\n",
            "step: 70, loss: 0.028990013524889946\n",
            "step: 80, loss: 0.0011411180021241307\n",
            "step: 90, loss: 0.0001292819215450436\n",
            "step: 100, loss: 0.0010358477011322975\n",
            "step: 110, loss: 0.0003591643471736461\n",
            "step: 120, loss: 0.0002536297542974353\n",
            "step: 130, loss: 0.001593122724443674\n",
            "step: 140, loss: 0.00025371077936142683\n",
            "step: 150, loss: 0.017065180465579033\n",
            "step: 160, loss: 0.007873116061091423\n",
            "step: 170, loss: 0.057531777769327164\n",
            "step: 180, loss: 0.018410304561257362\n",
            "step: 190, loss: 0.0006330916658043861\n",
            "step: 200, loss: 0.004116215277463198\n",
            "step: 210, loss: 0.005106380674988031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5415778251599146, f1=0.49777777777777776, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006584248621948063\n",
            "step: 10, loss: 0.02979096956551075\n",
            "step: 20, loss: 0.0006333271157927811\n",
            "step: 30, loss: 0.0013377557042986155\n",
            "step: 40, loss: 0.00036312316660769284\n",
            "step: 50, loss: 0.002968903398141265\n",
            "step: 60, loss: 0.00044109768350608647\n",
            "step: 70, loss: 0.00021876337996218354\n",
            "step: 80, loss: 0.0014988427283242345\n",
            "step: 90, loss: 0.0051051960326731205\n",
            "step: 100, loss: 0.00022594213078264147\n",
            "step: 110, loss: 0.0005430894671007991\n",
            "step: 120, loss: 0.04483213648200035\n",
            "step: 130, loss: 0.0009842216968536377\n",
            "step: 140, loss: 0.0014997866237536073\n",
            "step: 150, loss: 0.0002354552852921188\n",
            "step: 160, loss: 0.000824901566375047\n",
            "step: 170, loss: 0.003196483477950096\n",
            "step: 180, loss: 0.0024800323881208897\n",
            "step: 190, loss: 0.00024236507306341082\n",
            "step: 200, loss: 0.0003370074846316129\n",
            "step: 210, loss: 0.020334526896476746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5412262156448203, f1=0.5077262693156732, best_f1=0.559322033898305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020486237190198153\n",
            "step: 10, loss: 0.00011924895807169378\n",
            "step: 20, loss: 0.0008479770622216165\n",
            "step: 30, loss: 0.0019613902550190687\n",
            "step: 40, loss: 0.0004765300254803151\n",
            "step: 50, loss: 0.0203773844987154\n",
            "step: 60, loss: 0.01500740461051464\n",
            "step: 70, loss: 0.0008240934694185853\n",
            "step: 80, loss: 0.0007144531700760126\n",
            "step: 90, loss: 0.0007302733138203621\n",
            "step: 100, loss: 0.015179691836237907\n",
            "step: 110, loss: 0.00016355101251974702\n",
            "step: 120, loss: 0.0001900758798001334\n",
            "step: 130, loss: 0.00018596855807118118\n",
            "step: 140, loss: 0.00019128907297272235\n",
            "step: 150, loss: 0.00018121513130608946\n",
            "step: 160, loss: 0.0006551998085342348\n",
            "step: 170, loss: 0.000881471554748714\n",
            "step: 180, loss: 0.0008157175616361201\n",
            "step: 190, loss: 0.00036491951323114336\n",
            "step: 200, loss: 0.00039714205195195973\n",
            "step: 210, loss: 0.02160993032157421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5319148936170213, f1=0.5022421524663678, best_f1=0.559322033898305\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 275.61it/s]\n",
            "load_f1 = 0.59375\n",
            "real_f1 = 0.5996055226824457\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eed0059-3659-4a7e-cc17-a5a9b41b5d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5703436136245728\n",
            "step: 10, loss: 0.36792805790901184\n",
            "step: 20, loss: 0.32437416911125183\n",
            "step: 30, loss: 0.4238101840019226\n",
            "step: 40, loss: 0.4253441393375397\n",
            "step: 50, loss: 0.30003267526626587\n",
            "step: 60, loss: 0.2970193028450012\n",
            "step: 70, loss: 0.2732957899570465\n",
            "step: 80, loss: 0.2601363956928253\n",
            "step: 90, loss: 0.302367627620697\n",
            "step: 100, loss: 0.34254178404808044\n",
            "step: 110, loss: 0.36359521746635437\n",
            "step: 120, loss: 0.1917882263660431\n",
            "step: 130, loss: 0.1880785971879959\n",
            "step: 140, loss: 0.05620725080370903\n",
            "step: 150, loss: 0.17305508255958557\n",
            "step: 160, loss: 0.13374711573123932\n",
            "step: 170, loss: 0.24767081439495087\n",
            "step: 180, loss: 0.038666218519210815\n",
            "step: 190, loss: 0.24717940390110016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6334310850439884, f1=0.6046511627906976, best_f1=0.6046511627906976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26283836364746094\n",
            "step: 10, loss: 0.1301906555891037\n",
            "step: 20, loss: 0.20547643303871155\n",
            "step: 30, loss: 0.22031748294830322\n",
            "step: 40, loss: 0.19080683588981628\n",
            "step: 50, loss: 0.12322111427783966\n",
            "step: 60, loss: 0.34905603528022766\n",
            "step: 70, loss: 0.20753420889377594\n",
            "step: 80, loss: 0.30647528171539307\n",
            "step: 90, loss: 0.3711014986038208\n",
            "step: 100, loss: 0.06988214701414108\n",
            "step: 110, loss: 0.1682816743850708\n",
            "step: 120, loss: 0.3422984480857849\n",
            "step: 130, loss: 0.11998416483402252\n",
            "step: 140, loss: 0.09014297276735306\n",
            "step: 150, loss: 0.11748626828193665\n",
            "step: 160, loss: 0.03823975846171379\n",
            "step: 170, loss: 0.19938983023166656\n",
            "step: 180, loss: 0.1771990954875946\n",
            "step: 190, loss: 0.046994224190711975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7183462532299743, f1=0.7076167076167075, best_f1=0.7076167076167075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07958737015724182\n",
            "step: 10, loss: 0.36134809255599976\n",
            "step: 20, loss: 0.12007302790880203\n",
            "step: 30, loss: 0.1522369235754013\n",
            "step: 40, loss: 0.117103710770607\n",
            "step: 50, loss: 0.20986023545265198\n",
            "step: 60, loss: 0.09343495219945908\n",
            "step: 70, loss: 0.09676934033632278\n",
            "step: 80, loss: 0.11069457232952118\n",
            "step: 90, loss: 0.08836106956005096\n",
            "step: 100, loss: 0.13532108068466187\n",
            "step: 110, loss: 0.04775312542915344\n",
            "step: 120, loss: 0.032574933022260666\n",
            "step: 130, loss: 0.008993968367576599\n",
            "step: 140, loss: 0.016420423984527588\n",
            "step: 150, loss: 0.17553186416625977\n",
            "step: 160, loss: 0.12306471168994904\n",
            "step: 170, loss: 0.13261373341083527\n",
            "step: 180, loss: 0.05508045852184296\n",
            "step: 190, loss: 0.06826436519622803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7341040462427746, f1=0.7521367521367521, best_f1=0.7521367521367521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014259236864745617\n",
            "step: 10, loss: 0.15118330717086792\n",
            "step: 20, loss: 0.029089229181408882\n",
            "step: 30, loss: 0.01315529178828001\n",
            "step: 40, loss: 0.010238170623779297\n",
            "step: 50, loss: 0.1584194451570511\n",
            "step: 60, loss: 0.06854937970638275\n",
            "step: 70, loss: 0.06270712614059448\n",
            "step: 80, loss: 0.15260428190231323\n",
            "step: 90, loss: 0.022347629070281982\n",
            "step: 100, loss: 0.13577350974082947\n",
            "step: 110, loss: 0.036925170570611954\n",
            "step: 120, loss: 0.06890463083982468\n",
            "step: 130, loss: 0.2423945665359497\n",
            "step: 140, loss: 0.03347661718726158\n",
            "step: 150, loss: 0.03694763034582138\n",
            "step: 160, loss: 0.027798889204859734\n",
            "step: 170, loss: 0.27024105191230774\n",
            "step: 180, loss: 0.009282679297029972\n",
            "step: 190, loss: 0.16927333176136017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7345844504021448, f1=0.7336683417085428, best_f1=0.7336683417085428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.089076928794384\n",
            "step: 10, loss: 0.010937655344605446\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.07044987380504608\n",
            "step: 30, loss: 0.007144297938793898\n",
            "step: 40, loss: 0.07719486951828003\n",
            "step: 50, loss: 0.18177025020122528\n",
            "step: 60, loss: 0.12446268647909164\n",
            "step: 70, loss: 0.0055403090082108974\n",
            "step: 80, loss: 0.046123385429382324\n",
            "step: 90, loss: 0.03289204463362694\n",
            "step: 100, loss: 0.0022686636075377464\n",
            "step: 110, loss: 0.010759861208498478\n",
            "step: 120, loss: 0.010490956716239452\n",
            "step: 130, loss: 0.04443899169564247\n",
            "step: 140, loss: 0.10976976156234741\n",
            "step: 150, loss: 0.09923660010099411\n",
            "step: 160, loss: 0.04803546890616417\n",
            "step: 170, loss: 0.020180700346827507\n",
            "step: 180, loss: 0.19711445271968842\n",
            "step: 190, loss: 0.08608684688806534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7295285359801489, f1=0.7487684729064039, best_f1=0.7336683417085428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016036666929721832\n",
            "step: 10, loss: 0.0464971661567688\n",
            "step: 20, loss: 0.002707048552110791\n",
            "step: 30, loss: 0.004113068338483572\n",
            "step: 40, loss: 0.017644239589571953\n",
            "step: 50, loss: 0.022174861282110214\n",
            "step: 60, loss: 0.05365774407982826\n",
            "step: 70, loss: 0.09488271176815033\n",
            "step: 80, loss: 0.007040045689791441\n",
            "step: 90, loss: 0.0668630376458168\n",
            "step: 100, loss: 0.0020050513558089733\n",
            "step: 110, loss: 0.12651225924491882\n",
            "step: 120, loss: 0.06374119967222214\n",
            "step: 130, loss: 0.014677093364298344\n",
            "step: 140, loss: 0.01630280539393425\n",
            "step: 150, loss: 0.0020300892647355795\n",
            "step: 160, loss: 0.007331767585128546\n",
            "step: 170, loss: 0.23026441037654877\n",
            "step: 180, loss: 0.0074007101356983185\n",
            "step: 190, loss: 0.009552664123475552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7386363636363636, f1=0.7506849315068493, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004324585665017366\n",
            "step: 10, loss: 0.015859460458159447\n",
            "step: 20, loss: 0.08962202817201614\n",
            "step: 30, loss: 0.062330882996320724\n",
            "step: 40, loss: 0.0106204804033041\n",
            "step: 50, loss: 0.006024419795721769\n",
            "step: 60, loss: 0.004823050927370787\n",
            "step: 70, loss: 0.009285311214625835\n",
            "step: 80, loss: 0.0300692617893219\n",
            "step: 90, loss: 0.03772279620170593\n",
            "step: 100, loss: 0.000827490643132478\n",
            "step: 110, loss: 0.02337065152823925\n",
            "step: 120, loss: 0.0058722710236907005\n",
            "step: 130, loss: 0.0038913448806852102\n",
            "step: 140, loss: 0.08376806229352951\n",
            "step: 150, loss: 0.04230375215411186\n",
            "step: 160, loss: 0.053169459104537964\n",
            "step: 170, loss: 0.006071748211979866\n",
            "step: 180, loss: 0.006698683835566044\n",
            "step: 190, loss: 0.00620218925178051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7044334975369458, f1=0.7383863080684596, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007674987427890301\n",
            "step: 10, loss: 0.022222299128770828\n",
            "step: 20, loss: 0.0005535662057809532\n",
            "step: 30, loss: 0.019369270652532578\n",
            "step: 40, loss: 0.0009230804280377924\n",
            "step: 50, loss: 0.00037116589373908937\n",
            "step: 60, loss: 0.0004694376257248223\n",
            "step: 70, loss: 0.007240189239382744\n",
            "step: 80, loss: 0.00012773234630003572\n",
            "step: 90, loss: 0.0027909378986805677\n",
            "step: 100, loss: 0.06137893721461296\n",
            "step: 110, loss: 0.004357069730758667\n",
            "step: 120, loss: 0.00025914801517501473\n",
            "step: 130, loss: 0.0010349731892347336\n",
            "step: 140, loss: 0.0010446569649502635\n",
            "step: 150, loss: 0.002445588354021311\n",
            "step: 160, loss: 0.0030487962067127228\n",
            "step: 170, loss: 0.03202594816684723\n",
            "step: 180, loss: 0.03239750862121582\n",
            "step: 190, loss: 0.07399027049541473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7191601049868767, f1=0.7448979591836734, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03299075737595558\n",
            "step: 10, loss: 0.010495869442820549\n",
            "step: 20, loss: 0.0009525843197479844\n",
            "step: 30, loss: 0.00024332708562724292\n",
            "step: 40, loss: 0.00025106396060436964\n",
            "step: 50, loss: 0.004190970212221146\n",
            "step: 60, loss: 0.032621871680021286\n",
            "step: 70, loss: 0.00043498011655174196\n",
            "step: 80, loss: 0.00044501895899884403\n",
            "step: 90, loss: 0.005987532902508974\n",
            "step: 100, loss: 0.0024746458511799574\n",
            "step: 110, loss: 0.059946637600660324\n",
            "step: 120, loss: 0.059878770262002945\n",
            "step: 130, loss: 0.0011338673066347837\n",
            "step: 140, loss: 0.013211927376687527\n",
            "step: 150, loss: 0.004932434298098087\n",
            "step: 160, loss: 0.0005081894341856241\n",
            "step: 170, loss: 0.0011439918307587504\n",
            "step: 180, loss: 0.0005712947458960116\n",
            "step: 190, loss: 0.0008832511957734823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7246376811594202, f1=0.7507163323782234, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026075271307490766\n",
            "step: 10, loss: 0.0003165117232128978\n",
            "step: 20, loss: 0.001374400919303298\n",
            "step: 30, loss: 0.0001425138907507062\n",
            "step: 40, loss: 0.034811969846487045\n",
            "step: 50, loss: 0.007911399938166142\n",
            "step: 60, loss: 0.00023264317132998258\n",
            "step: 70, loss: 0.0002465485886204988\n",
            "step: 80, loss: 0.0007803849293850362\n",
            "step: 90, loss: 0.00441198842599988\n",
            "step: 100, loss: 0.0008626877679489553\n",
            "step: 110, loss: 0.02380412258207798\n",
            "step: 120, loss: 0.10076631605625153\n",
            "step: 130, loss: 0.004128111060708761\n",
            "step: 140, loss: 0.020053386688232422\n",
            "step: 150, loss: 0.02458023466169834\n",
            "step: 160, loss: 0.001597483758814633\n",
            "step: 170, loss: 0.00040693487972021103\n",
            "step: 180, loss: 0.001430949429050088\n",
            "step: 190, loss: 0.00020575826056301594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7161803713527852, f1=0.7512953367875648, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038175852969288826\n",
            "step: 10, loss: 0.021858714520931244\n",
            "step: 20, loss: 0.0011790121207013726\n",
            "step: 30, loss: 0.016415797173976898\n",
            "step: 40, loss: 0.000193356754607521\n",
            "step: 50, loss: 0.0019192870240658522\n",
            "step: 60, loss: 0.0013861296465620399\n",
            "step: 70, loss: 0.0011478858068585396\n",
            "step: 80, loss: 0.00018783201812766492\n",
            "step: 90, loss: 0.0001661512505961582\n",
            "step: 100, loss: 0.0009411242208443582\n",
            "step: 110, loss: 0.00042888676398433745\n",
            "step: 120, loss: 0.00038290268275886774\n",
            "step: 130, loss: 0.0006470793741755188\n",
            "step: 140, loss: 0.004053832031786442\n",
            "step: 150, loss: 0.00011045342398574576\n",
            "step: 160, loss: 0.002355254255235195\n",
            "step: 170, loss: 0.00014788127737119794\n",
            "step: 180, loss: 0.00028515412122942507\n",
            "step: 190, loss: 0.0005147833726368845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7262569832402234, f1=0.7409470752089136, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018612443818710744\n",
            "step: 10, loss: 0.009281555190682411\n",
            "step: 20, loss: 0.00016155745834112167\n",
            "step: 30, loss: 0.00039626526995562017\n",
            "step: 40, loss: 0.0007178130908869207\n",
            "step: 50, loss: 0.0146110150963068\n",
            "step: 60, loss: 0.0005117481923662126\n",
            "step: 70, loss: 0.00031762145226821303\n",
            "step: 80, loss: 0.0002088043256662786\n",
            "step: 90, loss: 0.00028595083858817816\n",
            "step: 100, loss: 0.0015736838104203343\n",
            "step: 110, loss: 0.0007734206155873835\n",
            "step: 120, loss: 0.00014837317576166242\n",
            "step: 130, loss: 0.003842931939288974\n",
            "step: 140, loss: 0.009134369902312756\n",
            "step: 150, loss: 0.00034207719727419317\n",
            "step: 160, loss: 0.00014432660827878863\n",
            "step: 170, loss: 0.21081674098968506\n",
            "step: 180, loss: 5.221917308517732e-05\n",
            "step: 190, loss: 0.0005022919503971934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7258064516129032, f1=0.7446808510638299, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005863674450665712\n",
            "step: 10, loss: 0.0006925062043592334\n",
            "step: 20, loss: 0.0013302686857059598\n",
            "step: 30, loss: 0.005415037740021944\n",
            "step: 40, loss: 0.00014856725465506315\n",
            "step: 50, loss: 0.001219072612002492\n",
            "step: 60, loss: 0.00042834412306547165\n",
            "step: 70, loss: 0.0011701899347826838\n",
            "step: 80, loss: 0.0003937122237402946\n",
            "step: 90, loss: 0.0019932540599256754\n",
            "step: 100, loss: 0.00025744098820723593\n",
            "step: 110, loss: 0.004965823143720627\n",
            "step: 120, loss: 0.00331432418897748\n",
            "step: 130, loss: 0.0020152272190898657\n",
            "step: 140, loss: 0.0002795204345602542\n",
            "step: 150, loss: 8.11555510153994e-05\n",
            "step: 160, loss: 0.00028637261129915714\n",
            "step: 170, loss: 0.00025725673185661435\n",
            "step: 180, loss: 0.000371713686035946\n",
            "step: 190, loss: 0.0029196874238550663\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7075208913649025, f1=0.7150837988826816, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020050827879458666\n",
            "step: 10, loss: 0.00017274817219004035\n",
            "step: 20, loss: 0.0007641944685019553\n",
            "step: 30, loss: 0.00021661678329110146\n",
            "step: 40, loss: 0.0004971068119630218\n",
            "step: 50, loss: 0.001057908870279789\n",
            "step: 60, loss: 0.001035910565406084\n",
            "step: 70, loss: 0.00015595744480378926\n",
            "step: 80, loss: 0.00015039385471027344\n",
            "step: 90, loss: 0.0032208182383328676\n",
            "step: 100, loss: 0.06235566735267639\n",
            "step: 110, loss: 0.0002062873973045498\n",
            "step: 120, loss: 0.0003070970706176013\n",
            "step: 130, loss: 0.006311289966106415\n",
            "step: 140, loss: 0.0012822550488635898\n",
            "step: 150, loss: 0.00012220583448652178\n",
            "step: 160, loss: 0.0021614402066916227\n",
            "step: 170, loss: 0.0003821287536993623\n",
            "step: 180, loss: 0.0003900978190358728\n",
            "step: 190, loss: 0.0007516540354117751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7183462532299743, f1=0.739240506329114, best_f1=0.7506849315068493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030819906387478113\n",
            "step: 10, loss: 0.00037800028803758323\n",
            "step: 20, loss: 0.0017740379553288221\n",
            "step: 30, loss: 0.0003099735185969621\n",
            "step: 40, loss: 0.00228230282664299\n",
            "step: 50, loss: 0.00026161724235862494\n",
            "step: 60, loss: 0.023381343111395836\n",
            "step: 70, loss: 0.0019206549040973186\n",
            "step: 80, loss: 0.0015123924240469933\n",
            "step: 90, loss: 0.000549063493963331\n",
            "step: 100, loss: 0.00042740636854432523\n",
            "step: 110, loss: 0.0020508256275206804\n",
            "step: 120, loss: 0.00017718991148285568\n",
            "step: 130, loss: 0.00015018276462797076\n",
            "step: 140, loss: 0.0008164063910953701\n",
            "step: 150, loss: 0.000393153284676373\n",
            "step: 160, loss: 0.0001533728209324181\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.0002822131209541112\n",
            "step: 180, loss: 0.003024768317118287\n",
            "step: 190, loss: 0.0018667856929823756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7142857142857143, f1=0.7313432835820897, best_f1=0.7506849315068493\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 167.92it/s]\n",
            "load_f1 = 0.6595744680851063\n",
            "real_f1 = 0.6351706036745407\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f97e74-7ec3-44df-8e8f-c7eb58f06f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6272090077400208\n",
            "step: 10, loss: 0.38127267360687256\n",
            "step: 20, loss: 0.3090824484825134\n",
            "step: 30, loss: 0.3911212980747223\n",
            "step: 40, loss: 0.3032062351703644\n",
            "step: 50, loss: 0.2918131649494171\n",
            "step: 60, loss: 0.2445298135280609\n",
            "step: 70, loss: 0.38189730048179626\n",
            "step: 80, loss: 0.3876264691352844\n",
            "step: 90, loss: 0.2712785005569458\n",
            "step: 100, loss: 0.19382286071777344\n",
            "step: 110, loss: 0.30169758200645447\n",
            "step: 120, loss: 0.31369197368621826\n",
            "step: 130, loss: 0.08208607882261276\n",
            "step: 140, loss: 0.24860744178295135\n",
            "step: 150, loss: 0.40352728962898254\n",
            "step: 160, loss: 0.15627674758434296\n",
            "step: 170, loss: 0.22119641304016113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6756032171581768, f1=0.6734693877551021, best_f1=0.6734693877551021\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14165236055850983\n",
            "step: 10, loss: 0.22919289767742157\n",
            "step: 20, loss: 0.27633652091026306\n",
            "step: 30, loss: 0.21091653406620026\n",
            "step: 40, loss: 0.0757661685347557\n",
            "step: 50, loss: 0.1178257092833519\n",
            "step: 60, loss: 0.14027275145053864\n",
            "step: 70, loss: 0.07544797658920288\n",
            "step: 80, loss: 0.07636208832263947\n",
            "step: 90, loss: 0.10879365354776382\n",
            "step: 100, loss: 0.1726323366165161\n",
            "step: 110, loss: 0.051858317106962204\n",
            "step: 120, loss: 0.09488281607627869\n",
            "step: 130, loss: 0.12030195444822311\n",
            "step: 140, loss: 0.22853170335292816\n",
            "step: 150, loss: 0.2925478219985962\n",
            "step: 160, loss: 0.16011720895767212\n",
            "step: 170, loss: 0.09213755279779434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.756476683937824, f1=0.7680798004987531, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12841688096523285\n",
            "step: 10, loss: 0.14926093816757202\n",
            "step: 20, loss: 0.008932141587138176\n",
            "step: 30, loss: 0.04640290513634682\n",
            "step: 40, loss: 0.04843549802899361\n",
            "step: 50, loss: 0.03518828749656677\n",
            "step: 60, loss: 0.09173990041017532\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.08927835524082184\n",
            "step: 80, loss: 0.06626749038696289\n",
            "step: 90, loss: 0.1240176409482956\n",
            "step: 100, loss: 0.0033750850707292557\n",
            "step: 110, loss: 0.11365824937820435\n",
            "step: 120, loss: 0.059336550533771515\n",
            "step: 130, loss: 0.16666094958782196\n",
            "step: 140, loss: 0.01857374794781208\n",
            "step: 150, loss: 0.061540380120277405\n",
            "step: 160, loss: 0.07326879352331161\n",
            "step: 170, loss: 0.10716541111469269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7755102040816327, f1=0.7688679245283018, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.058197248727083206\n",
            "step: 10, loss: 0.11673532426357269\n",
            "step: 20, loss: 0.0034445361234247684\n",
            "step: 30, loss: 0.0760282427072525\n",
            "step: 40, loss: 0.02377631701529026\n",
            "step: 50, loss: 0.2253226935863495\n",
            "step: 60, loss: 0.06033085286617279\n",
            "step: 70, loss: 0.03072541020810604\n",
            "step: 80, loss: 0.0828511044383049\n",
            "step: 90, loss: 0.07655578851699829\n",
            "step: 100, loss: 0.03879629075527191\n",
            "step: 110, loss: 0.08079466223716736\n",
            "step: 120, loss: 0.0030069102067500353\n",
            "step: 130, loss: 0.07026499509811401\n",
            "step: 140, loss: 0.03137753903865814\n",
            "step: 150, loss: 0.21978963911533356\n",
            "step: 160, loss: 0.11893024295568466\n",
            "step: 170, loss: 0.04320043697953224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7721822541966428, f1=0.7710280373831775, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028483876958489418\n",
            "step: 10, loss: 0.13459806144237518\n",
            "step: 20, loss: 0.019937116652727127\n",
            "step: 30, loss: 0.09612845629453659\n",
            "step: 40, loss: 0.015561318024992943\n",
            "step: 50, loss: 0.13878552615642548\n",
            "step: 60, loss: 0.021843524649739265\n",
            "step: 70, loss: 0.1177259236574173\n",
            "step: 80, loss: 0.012462607584893703\n",
            "step: 90, loss: 0.0818922370672226\n",
            "step: 100, loss: 0.03715498745441437\n",
            "step: 110, loss: 0.04554593190550804\n",
            "step: 120, loss: 0.006128587760031223\n",
            "step: 130, loss: 0.0260696429759264\n",
            "step: 140, loss: 0.04688117280602455\n",
            "step: 150, loss: 0.013373411260545254\n",
            "step: 160, loss: 0.03252377733588219\n",
            "step: 170, loss: 0.004353160038590431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7657430730478589, f1=0.7548076923076923, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002234310144558549\n",
            "step: 10, loss: 0.006863039452582598\n",
            "step: 20, loss: 0.0011233389377593994\n",
            "step: 30, loss: 0.0016708807088434696\n",
            "step: 40, loss: 0.03227957710623741\n",
            "step: 50, loss: 0.09357087314128876\n",
            "step: 60, loss: 0.07103057205677032\n",
            "step: 70, loss: 0.0867852121591568\n",
            "step: 80, loss: 0.023564213886857033\n",
            "step: 90, loss: 0.0015794681385159492\n",
            "step: 100, loss: 0.0012969280360266566\n",
            "step: 110, loss: 0.0014416440390050411\n",
            "step: 120, loss: 0.010114409029483795\n",
            "step: 130, loss: 0.007782737724483013\n",
            "step: 140, loss: 0.013663779944181442\n",
            "step: 150, loss: 0.06868848204612732\n",
            "step: 160, loss: 0.025752361863851547\n",
            "step: 170, loss: 0.02962392382323742\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7512953367875648, f1=0.7725118483412323, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014567369362339377\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 10, loss: 0.019895410165190697\n",
            "step: 20, loss: 0.0014554045628756285\n",
            "step: 30, loss: 0.017656132578849792\n",
            "step: 40, loss: 0.04253470525145531\n",
            "step: 50, loss: 0.12194421142339706\n",
            "step: 60, loss: 0.005072849337011576\n",
            "step: 70, loss: 0.009602148085832596\n",
            "step: 80, loss: 0.01936723291873932\n",
            "step: 90, loss: 0.00039691090933047235\n",
            "step: 100, loss: 0.0006932539981789887\n",
            "step: 110, loss: 0.012117685750126839\n",
            "step: 120, loss: 0.0007063204539008439\n",
            "step: 130, loss: 0.12963423132896423\n",
            "step: 140, loss: 0.004921023268252611\n",
            "step: 150, loss: 0.002704037819057703\n",
            "step: 160, loss: 0.0009630953427404165\n",
            "step: 170, loss: 0.14317521452903748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7464114832535885, f1=0.7499999999999999, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01625184714794159\n",
            "step: 10, loss: 0.005549589637666941\n",
            "step: 20, loss: 0.0033612444531172514\n",
            "step: 30, loss: 0.008439213037490845\n",
            "step: 40, loss: 0.052495185285806656\n",
            "step: 50, loss: 0.026440147310495377\n",
            "step: 60, loss: 0.0072659822180867195\n",
            "step: 70, loss: 0.0009796220110729337\n",
            "step: 80, loss: 0.00206630933098495\n",
            "step: 90, loss: 0.02392684854567051\n",
            "step: 100, loss: 0.09685068577528\n",
            "step: 110, loss: 0.018778035417199135\n",
            "step: 120, loss: 0.004929743241518736\n",
            "step: 130, loss: 0.003941280767321587\n",
            "step: 140, loss: 0.0014663721667602658\n",
            "step: 150, loss: 0.03704174607992172\n",
            "step: 160, loss: 0.007713672239333391\n",
            "step: 170, loss: 0.036262061446905136\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7651715039577837, f1=0.7524752475247525, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012080197222530842\n",
            "step: 10, loss: 0.004130041692405939\n",
            "step: 20, loss: 0.001967289485037327\n",
            "step: 30, loss: 0.013459998182952404\n",
            "step: 40, loss: 0.0035579949617385864\n",
            "step: 50, loss: 0.0023912754841148853\n",
            "step: 60, loss: 0.05203881114721298\n",
            "step: 70, loss: 0.15002089738845825\n",
            "step: 80, loss: 0.027365846559405327\n",
            "step: 90, loss: 0.0586448609828949\n",
            "step: 100, loss: 0.033907707780599594\n",
            "step: 110, loss: 0.0011771870777010918\n",
            "step: 120, loss: 0.002778099151328206\n",
            "step: 130, loss: 0.07848809659481049\n",
            "step: 140, loss: 0.00877409614622593\n",
            "step: 150, loss: 0.0017744106007739902\n",
            "step: 160, loss: 0.085018590092659\n",
            "step: 170, loss: 0.01622164063155651\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7596153846153846, f1=0.7681818181818182, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10806838423013687\n",
            "step: 10, loss: 0.0092779491096735\n",
            "step: 20, loss: 0.023431140929460526\n",
            "step: 30, loss: 0.0043548764660954475\n",
            "step: 40, loss: 0.0006638760096393526\n",
            "step: 50, loss: 0.08074771612882614\n",
            "step: 60, loss: 0.00036719435593113303\n",
            "step: 70, loss: 0.0038476241752505302\n",
            "step: 80, loss: 0.01433771662414074\n",
            "step: 90, loss: 0.002760587725788355\n",
            "step: 100, loss: 0.0008133301744237542\n",
            "step: 110, loss: 0.004245097283273935\n",
            "step: 120, loss: 0.01385500468313694\n",
            "step: 130, loss: 0.004982966464012861\n",
            "step: 140, loss: 0.05802387744188309\n",
            "step: 150, loss: 0.007778482977300882\n",
            "step: 160, loss: 0.0005050771287642419\n",
            "step: 170, loss: 0.0003119247267022729\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7581047381546135, f1=0.7541766109785203, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0197402685880661\n",
            "step: 10, loss: 0.013437814079225063\n",
            "step: 20, loss: 0.005156477447599173\n",
            "step: 30, loss: 0.001062369323335588\n",
            "step: 40, loss: 0.020047252997756004\n",
            "step: 50, loss: 0.04601817950606346\n",
            "step: 60, loss: 0.009608946740627289\n",
            "step: 70, loss: 0.03704072907567024\n",
            "step: 80, loss: 0.0005911254556849599\n",
            "step: 90, loss: 0.00018102095054928213\n",
            "step: 100, loss: 0.002250737277790904\n",
            "step: 110, loss: 0.05561346933245659\n",
            "step: 120, loss: 0.0009517860598862171\n",
            "step: 130, loss: 0.0003865660692099482\n",
            "step: 140, loss: 0.0006496772984974086\n",
            "step: 150, loss: 0.11108864843845367\n",
            "step: 160, loss: 0.002936396049335599\n",
            "step: 170, loss: 0.004817601293325424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7556675062972292, f1=0.7523809523809523, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01251536887139082\n",
            "step: 10, loss: 0.018253641203045845\n",
            "step: 20, loss: 0.11297564208507538\n",
            "step: 30, loss: 0.005339613649994135\n",
            "step: 40, loss: 0.0004806591023225337\n",
            "step: 50, loss: 0.00018160806212108582\n",
            "step: 60, loss: 0.034884318709373474\n",
            "step: 70, loss: 0.13248953223228455\n",
            "step: 80, loss: 0.0001534604816697538\n",
            "step: 90, loss: 0.14134572446346283\n",
            "step: 100, loss: 0.0037743018474429846\n",
            "step: 110, loss: 0.0011392944725230336\n",
            "step: 120, loss: 0.04287010058760643\n",
            "step: 130, loss: 0.009853308089077473\n",
            "step: 140, loss: 0.0019428411033004522\n",
            "step: 150, loss: 0.00028555651078931987\n",
            "step: 160, loss: 0.00039862070116214454\n",
            "step: 170, loss: 0.011721797287464142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7595628415300546, f1=0.7671957671957672, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007945985533297062\n",
            "step: 10, loss: 0.00035827048122882843\n",
            "step: 20, loss: 0.006617716513574123\n",
            "step: 30, loss: 0.00012997680460102856\n",
            "step: 40, loss: 0.0006516239955089986\n",
            "step: 50, loss: 0.0004936133045703173\n",
            "step: 60, loss: 0.0007184789865277708\n",
            "step: 70, loss: 0.0001804921921575442\n",
            "step: 80, loss: 0.00016871636034920812\n",
            "step: 90, loss: 0.00011573176743695512\n",
            "step: 100, loss: 0.00027511175721883774\n",
            "step: 110, loss: 0.00012143935600761324\n",
            "step: 120, loss: 0.018911877647042274\n",
            "step: 130, loss: 0.0009418231202289462\n",
            "step: 140, loss: 0.0005375276086851954\n",
            "step: 150, loss: 0.0024090022780001163\n",
            "step: 160, loss: 0.00921645388007164\n",
            "step: 170, loss: 0.0018211203860118985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7500000000000001, f1=0.7680412371134021, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0062810806557536125\n",
            "step: 10, loss: 8.476847142446786e-05\n",
            "step: 20, loss: 0.0005069053149782121\n",
            "step: 30, loss: 0.0008151368238031864\n",
            "step: 40, loss: 0.00016786908963695168\n",
            "step: 50, loss: 0.0002784480748232454\n",
            "step: 60, loss: 0.00012219826749060303\n",
            "step: 70, loss: 0.00038152013439685106\n",
            "step: 80, loss: 0.011690761893987656\n",
            "step: 90, loss: 0.0002992596128024161\n",
            "step: 100, loss: 0.009593332186341286\n",
            "step: 110, loss: 0.00034257094375789165\n",
            "step: 120, loss: 0.0019902389030903578\n",
            "step: 130, loss: 0.00026013964088633657\n",
            "step: 140, loss: 0.00035425368696451187\n",
            "step: 150, loss: 0.00012630656419787556\n",
            "step: 160, loss: 0.00010131965245818719\n",
            "step: 170, loss: 0.00016372684331145138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7520435967302452, f1=0.764857881136951, best_f1=0.7688679245283018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000180018600076437\n",
            "step: 10, loss: 0.003104656469076872\n",
            "step: 20, loss: 0.03168192133307457\n",
            "step: 30, loss: 0.00014490759349428117\n",
            "step: 40, loss: 0.0006150020053610206\n",
            "step: 50, loss: 0.00018530416127759963\n",
            "step: 60, loss: 0.013515711762011051\n",
            "step: 70, loss: 0.00157257413957268\n",
            "step: 80, loss: 0.0031721496488898993\n",
            "step: 90, loss: 0.006104915868490934\n",
            "step: 100, loss: 0.001688860240392387\n",
            "step: 110, loss: 0.00015658498159609735\n",
            "step: 120, loss: 0.004132910631597042\n",
            "step: 130, loss: 0.002036832505837083\n",
            "step: 140, loss: 0.0021338665392249823\n",
            "step: 150, loss: 0.00021611513511743397\n",
            "step: 160, loss: 0.00045474700164049864\n",
            "step: 170, loss: 0.0007549778092652559\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7513513513513513, f1=0.7641025641025642, best_f1=0.7688679245283018\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 231.66it/s]\n",
            "load_f1 = 0.6330935251798562\n",
            "real_f1 = 0.6258823529411764\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f3f62c-4eaa-401b-96ec-d5608604d6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5942934155464172\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.6223584413528442\n",
            "step: 20, loss: 0.5023674964904785\n",
            "step: 30, loss: 0.42634424567222595\n",
            "step: 40, loss: 0.22183197736740112\n",
            "step: 50, loss: 0.09503340721130371\n",
            "step: 60, loss: 0.14560693502426147\n",
            "step: 70, loss: 0.09036318957805634\n",
            "step: 80, loss: 0.07510967552661896\n",
            "step: 90, loss: 0.12885206937789917\n",
            "step: 100, loss: 0.007916983217000961\n",
            "step: 110, loss: 0.26320669054985046\n",
            "step: 120, loss: 0.01510709896683693\n",
            "step: 130, loss: 0.06864400953054428\n",
            "step: 140, loss: 0.022758392617106438\n",
            "step: 150, loss: 0.062369246035814285\n",
            "step: 160, loss: 0.011984096840023994\n",
            "step: 170, loss: 0.04449733346700668\n",
            "step: 180, loss: 0.10497231781482697\n",
            "step: 190, loss: 0.10053414106369019\n",
            "step: 200, loss: 0.13519041240215302\n",
            "step: 210, loss: 0.025382397696375847\n",
            "step: 220, loss: 0.03234328702092171\n",
            "step: 230, loss: 0.08869108557701111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9599999999999999, f1=0.9588431590656283, best_f1=0.9588431590656283\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008047888055443764\n",
            "step: 10, loss: 0.021715428680181503\n",
            "step: 20, loss: 0.23411186039447784\n",
            "step: 30, loss: 0.14371611177921295\n",
            "step: 40, loss: 0.1312599778175354\n",
            "step: 50, loss: 0.02533729560673237\n",
            "step: 60, loss: 0.008791483007371426\n",
            "step: 70, loss: 0.06340094655752182\n",
            "step: 80, loss: 0.04656525328755379\n",
            "step: 90, loss: 0.02559984102845192\n",
            "step: 100, loss: 0.1048821285367012\n",
            "step: 110, loss: 0.10128708928823471\n",
            "step: 120, loss: 0.09007735550403595\n",
            "step: 130, loss: 0.13043129444122314\n",
            "step: 140, loss: 0.008941681124269962\n",
            "step: 150, loss: 0.013950178399682045\n",
            "step: 160, loss: 0.024577265605330467\n",
            "step: 170, loss: 0.02999546192586422\n",
            "step: 180, loss: 0.06451800465583801\n",
            "step: 190, loss: 0.03698594868183136\n",
            "step: 200, loss: 0.003693557810038328\n",
            "step: 210, loss: 0.010809729807078838\n",
            "step: 220, loss: 0.05148041248321533\n",
            "step: 230, loss: 0.07886763662099838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9650507328072153, f1=0.9542920847268673, best_f1=0.9542920847268673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018564175814390182\n",
            "step: 10, loss: 0.016285011544823647\n",
            "step: 20, loss: 0.09982914477586746\n",
            "step: 30, loss: 0.0024156842846423388\n",
            "step: 40, loss: 0.011784033849835396\n",
            "step: 50, loss: 0.04000045731663704\n",
            "step: 60, loss: 0.0017688007792457938\n",
            "step: 70, loss: 0.0032028695568442345\n",
            "step: 80, loss: 0.064728744328022\n",
            "step: 90, loss: 0.1363748013973236\n",
            "step: 100, loss: 0.027457214891910553\n",
            "step: 110, loss: 0.0014480644604191184\n",
            "step: 120, loss: 0.052246470004320145\n",
            "step: 130, loss: 0.0005951575003564358\n",
            "step: 140, loss: 0.007271731738001108\n",
            "step: 150, loss: 0.11068245023488998\n",
            "step: 160, loss: 0.23120296001434326\n",
            "step: 170, loss: 0.014856550842523575\n",
            "step: 180, loss: 0.025257380679249763\n",
            "step: 190, loss: 0.008742891252040863\n",
            "step: 200, loss: 0.07254630327224731\n",
            "step: 210, loss: 0.018847908824682236\n",
            "step: 220, loss: 0.04062582924962044\n",
            "step: 230, loss: 0.008349094539880753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9741863075196409, f1=0.962962962962963, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0124070905148983\n",
            "step: 10, loss: 0.0024191592819988728\n",
            "step: 20, loss: 0.00633185263723135\n",
            "step: 30, loss: 0.0006634573801420629\n",
            "step: 40, loss: 0.012174688279628754\n",
            "step: 50, loss: 0.000765860197134316\n",
            "step: 60, loss: 0.002392369555309415\n",
            "step: 70, loss: 0.015490110963582993\n",
            "step: 80, loss: 0.00045258968020789325\n",
            "step: 90, loss: 0.060439079999923706\n",
            "step: 100, loss: 0.007060652133077383\n",
            "step: 110, loss: 0.001730823889374733\n",
            "step: 120, loss: 0.08559790253639221\n",
            "step: 130, loss: 0.02381112426519394\n",
            "step: 140, loss: 0.0029316111467778683\n",
            "step: 150, loss: 0.010115277022123337\n",
            "step: 160, loss: 0.04323989525437355\n",
            "step: 170, loss: 0.008360245265066624\n",
            "step: 180, loss: 0.028247809037566185\n",
            "step: 190, loss: 0.0007357181166298687\n",
            "step: 200, loss: 0.002569507574662566\n",
            "step: 210, loss: 0.009916557930409908\n",
            "step: 220, loss: 0.0004536872438620776\n",
            "step: 230, loss: 0.10064787417650223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9720044792833147, f1=0.9684684684684683, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000666029576677829\n",
            "step: 10, loss: 0.0015127032529562712\n",
            "step: 20, loss: 0.0013800198212265968\n",
            "step: 30, loss: 0.00024373845371883363\n",
            "step: 40, loss: 0.0014881453244015574\n",
            "step: 50, loss: 0.00046048726653680205\n",
            "step: 60, loss: 0.06876261532306671\n",
            "step: 70, loss: 0.0006579623441211879\n",
            "step: 80, loss: 0.0006181250209920108\n",
            "step: 90, loss: 0.00020263706392142922\n",
            "step: 100, loss: 0.00020478165242820978\n",
            "step: 110, loss: 0.00047492951853200793\n",
            "step: 120, loss: 0.00011997273395536467\n",
            "step: 130, loss: 0.0023787971585989\n",
            "step: 140, loss: 0.0006570280529558659\n",
            "step: 150, loss: 0.00047113001346588135\n",
            "step: 160, loss: 0.0008747380925342441\n",
            "step: 170, loss: 0.036828916519880295\n",
            "step: 180, loss: 0.0004029624105896801\n",
            "step: 190, loss: 0.19882087409496307\n",
            "step: 200, loss: 0.00357147422619164\n",
            "step: 210, loss: 0.0010289894416928291\n",
            "step: 220, loss: 0.0010942468652501702\n",
            "step: 230, loss: 0.0006858743727207184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9709821428571428, f1=0.9608938547486034, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007980498485267162\n",
            "step: 10, loss: 0.0012260766234248877\n",
            "step: 20, loss: 0.0020735773723572493\n",
            "step: 30, loss: 0.006343163549900055\n",
            "step: 40, loss: 0.00023090261674951762\n",
            "step: 50, loss: 0.0002955567033495754\n",
            "step: 60, loss: 0.00017653388204053044\n",
            "step: 70, loss: 0.0003744979912880808\n",
            "step: 80, loss: 0.0011075938818976283\n",
            "step: 90, loss: 0.0006049896473996341\n",
            "step: 100, loss: 0.01717032492160797\n",
            "step: 110, loss: 0.000776332279201597\n",
            "step: 120, loss: 0.002128052758052945\n",
            "step: 130, loss: 0.0002089262561639771\n",
            "step: 140, loss: 0.00040168495615944266\n",
            "step: 150, loss: 0.0005745503003709018\n",
            "step: 160, loss: 0.0009912626119330525\n",
            "step: 170, loss: 0.00020998621766921133\n",
            "step: 180, loss: 0.017933491617441177\n",
            "step: 190, loss: 0.0008501936681568623\n",
            "step: 200, loss: 0.00014977519458625466\n",
            "step: 210, loss: 0.0002674727002158761\n",
            "step: 220, loss: 0.0001536237687105313\n",
            "step: 230, loss: 0.038845401257276535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9664429530201343, f1=0.9652855543113102, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03706645220518112\n",
            "step: 10, loss: 0.00023800673079676926\n",
            "step: 20, loss: 0.00013244253932498395\n",
            "step: 30, loss: 9.92726199910976e-05\n",
            "step: 40, loss: 0.0060447631403803825\n",
            "step: 50, loss: 0.00016782748571131378\n",
            "step: 60, loss: 0.00035985917202197015\n",
            "step: 70, loss: 0.00021983204351272434\n",
            "step: 80, loss: 0.0014162735315039754\n",
            "step: 90, loss: 0.000130749904201366\n",
            "step: 100, loss: 0.00013287272304296494\n",
            "step: 110, loss: 0.0005890750326216221\n",
            "step: 120, loss: 0.0006773289642296731\n",
            "step: 130, loss: 0.00010755871335277334\n",
            "step: 140, loss: 0.0001464174856664613\n",
            "step: 150, loss: 0.007302244193851948\n",
            "step: 160, loss: 0.0405389778316021\n",
            "step: 170, loss: 0.00020586659957189113\n",
            "step: 180, loss: 0.0001533509057480842\n",
            "step: 190, loss: 0.00012413671356625855\n",
            "step: 200, loss: 0.01859789900481701\n",
            "step: 210, loss: 7.976614142535254e-05\n",
            "step: 220, loss: 0.0012315824860706925\n",
            "step: 230, loss: 0.004452583380043507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9666666666666666, f1=0.9633740288568259, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005359696224331856\n",
            "step: 10, loss: 0.0006390538183040917\n",
            "step: 20, loss: 0.00019557114865165204\n",
            "step: 30, loss: 0.00017303938511759043\n",
            "step: 40, loss: 0.019014369696378708\n",
            "step: 50, loss: 0.0007108469144441187\n",
            "step: 60, loss: 0.00016309581405948848\n",
            "step: 70, loss: 0.0001404736831318587\n",
            "step: 80, loss: 0.0004273138474673033\n",
            "step: 90, loss: 0.00011209453805349767\n",
            "step: 100, loss: 0.00036398094380274415\n",
            "step: 110, loss: 0.000751428073272109\n",
            "step: 120, loss: 0.003780290950089693\n",
            "step: 130, loss: 0.0016711377538740635\n",
            "step: 140, loss: 0.0001355957647319883\n",
            "step: 150, loss: 0.0007276323740370572\n",
            "step: 160, loss: 0.0004772973188664764\n",
            "step: 170, loss: 0.00014730360999237746\n",
            "step: 180, loss: 0.0002533066435717046\n",
            "step: 190, loss: 0.000592519121710211\n",
            "step: 200, loss: 0.002185328397899866\n",
            "step: 210, loss: 0.02510712668299675\n",
            "step: 220, loss: 0.0025308614131063223\n",
            "step: 230, loss: 0.002061246894299984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9640449438202248, f1=0.9593679458239278, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002202571922680363\n",
            "step: 10, loss: 0.0008188455831259489\n",
            "step: 20, loss: 0.000776022847276181\n",
            "step: 30, loss: 0.0002888769959099591\n",
            "step: 40, loss: 0.0010925988899543881\n",
            "step: 50, loss: 0.0006669831927865744\n",
            "step: 60, loss: 0.00018002190336119384\n",
            "step: 70, loss: 0.0002340326172998175\n",
            "step: 80, loss: 0.0002511347702238709\n",
            "step: 90, loss: 0.0011867471039295197\n",
            "step: 100, loss: 0.00019173999316990376\n",
            "step: 110, loss: 9.737860091263428e-05\n",
            "step: 120, loss: 8.964732842287049e-05\n",
            "step: 130, loss: 0.002393710194155574\n",
            "step: 140, loss: 6.0678907175315544e-05\n",
            "step: 150, loss: 0.0012992055853828788\n",
            "step: 160, loss: 0.000387549982406199\n",
            "step: 170, loss: 0.00011323228682158515\n",
            "step: 180, loss: 0.0033018155954778194\n",
            "step: 190, loss: 0.00011007816647179425\n",
            "step: 200, loss: 0.00011687763617374003\n",
            "step: 210, loss: 0.00031590129947289824\n",
            "step: 220, loss: 9.516568388789892e-05\n",
            "step: 230, loss: 6.799647962907329e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9732142857142857, f1=0.9663677130044843, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.770996257429942e-05\n",
            "step: 10, loss: 0.00015383226855192333\n",
            "step: 20, loss: 5.2385134040378034e-05\n",
            "step: 30, loss: 0.0005859886878170073\n",
            "step: 40, loss: 0.0002640803868416697\n",
            "step: 50, loss: 6.379722617566586e-05\n",
            "step: 60, loss: 0.00021922189625911415\n",
            "step: 70, loss: 0.0001878298062365502\n",
            "step: 80, loss: 8.43964735395275e-05\n",
            "step: 90, loss: 0.00010763064346974716\n",
            "step: 100, loss: 6.932541873538867e-05\n",
            "step: 110, loss: 0.00023423477250616997\n",
            "step: 120, loss: 0.00024045137979555875\n",
            "step: 130, loss: 0.0002820095978677273\n",
            "step: 140, loss: 0.03456759452819824\n",
            "step: 150, loss: 0.0008545666350983083\n",
            "step: 160, loss: 0.0004273036902304739\n",
            "step: 170, loss: 5.0087302952306345e-05\n",
            "step: 180, loss: 0.0002840836241375655\n",
            "step: 190, loss: 0.001154901459813118\n",
            "step: 200, loss: 7.907598774181679e-05\n",
            "step: 210, loss: 0.00010789002408273518\n",
            "step: 220, loss: 5.910839172429405e-05\n",
            "step: 230, loss: 0.0003718712250702083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9745293466223698, f1=0.9546961325966851, best_f1=0.9546961325966851\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002161906595574692\n",
            "step: 10, loss: 7.936025212984532e-05\n",
            "step: 20, loss: 0.00015201029600575566\n",
            "step: 30, loss: 0.018757177516818047\n",
            "step: 40, loss: 0.0004980018711648881\n",
            "step: 50, loss: 0.00014101459237281233\n",
            "step: 60, loss: 0.010260957293212414\n",
            "step: 70, loss: 0.0002955591189675033\n",
            "step: 80, loss: 0.00014064053539186716\n",
            "step: 90, loss: 0.00012906905612908304\n",
            "step: 100, loss: 8.777618495514616e-05\n",
            "step: 110, loss: 5.004241756978445e-05\n",
            "step: 120, loss: 0.0004995919880457222\n",
            "step: 130, loss: 0.0003139539039693773\n",
            "step: 140, loss: 6.648148701060563e-05\n",
            "step: 150, loss: 0.03727380558848381\n",
            "step: 160, loss: 0.0010196119546890259\n",
            "step: 170, loss: 0.005667916499078274\n",
            "step: 180, loss: 0.00013845543435309082\n",
            "step: 190, loss: 5.258977398625575e-05\n",
            "step: 200, loss: 0.0004366462235338986\n",
            "step: 210, loss: 3.446527989581227e-05\n",
            "step: 220, loss: 4.228375473758206e-05\n",
            "step: 230, loss: 6.28934649284929e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9787709497206705, f1=0.968609865470852, best_f1=0.968609865470852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003645766410045326\n",
            "step: 10, loss: 5.165278707863763e-05\n",
            "step: 20, loss: 0.0035341838374733925\n",
            "step: 30, loss: 0.00024455561651848257\n",
            "step: 40, loss: 0.0012943159090355039\n",
            "step: 50, loss: 0.01074350718408823\n",
            "step: 60, loss: 0.0006506209028884768\n",
            "step: 70, loss: 0.00018184138752985746\n",
            "step: 80, loss: 6.386582390405238e-05\n",
            "step: 90, loss: 3.6219465982867405e-05\n",
            "step: 100, loss: 5.537614197237417e-05\n",
            "step: 110, loss: 0.0002802413364406675\n",
            "step: 120, loss: 4.842174894292839e-05\n",
            "step: 130, loss: 0.00019441236509010196\n",
            "step: 140, loss: 8.181680459529161e-05\n",
            "step: 150, loss: 0.003874568035826087\n",
            "step: 160, loss: 5.610975495073944e-05\n",
            "step: 170, loss: 8.104465814540163e-05\n",
            "step: 180, loss: 8.620653534308076e-05\n",
            "step: 190, loss: 0.0008349997806362808\n",
            "step: 200, loss: 4.367953079054132e-05\n",
            "step: 210, loss: 4.765084304381162e-05\n",
            "step: 220, loss: 4.029836054542102e-05\n",
            "step: 230, loss: 0.00044734939001500607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9810479375696767, f1=0.967525195968645, best_f1=0.967525195968645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013293352676555514\n",
            "step: 10, loss: 0.003332240507006645\n",
            "step: 20, loss: 0.0003444541071075946\n",
            "step: 30, loss: 0.0007834902498871088\n",
            "step: 40, loss: 0.00015816903032828122\n",
            "step: 50, loss: 7.98096225480549e-05\n",
            "step: 60, loss: 0.0002878580125980079\n",
            "step: 70, loss: 6.91498935339041e-05\n",
            "step: 80, loss: 9.552847768645734e-05\n",
            "step: 90, loss: 0.005357328802347183\n",
            "step: 100, loss: 3.763879794860259e-05\n",
            "step: 110, loss: 9.629586566006765e-05\n",
            "step: 120, loss: 0.022586364299058914\n",
            "step: 130, loss: 9.087083162739873e-05\n",
            "step: 140, loss: 6.711656169500202e-05\n",
            "step: 150, loss: 5.361109651857987e-05\n",
            "step: 160, loss: 5.247794615570456e-05\n",
            "step: 170, loss: 0.00022686709417030215\n",
            "step: 180, loss: 5.108597542857751e-05\n",
            "step: 190, loss: 6.211180152604356e-05\n",
            "step: 200, loss: 0.004735888447612524\n",
            "step: 210, loss: 3.294894122518599e-05\n",
            "step: 220, loss: 0.00011373167217243463\n",
            "step: 230, loss: 7.882415229687467e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9776785714285714, f1=0.9652076318742986, best_f1=0.967525195968645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.255327753024176e-05\n",
            "step: 10, loss: 0.0022861389443278313\n",
            "step: 20, loss: 8.849816367728636e-05\n",
            "step: 30, loss: 5.2098563173785806e-05\n",
            "step: 40, loss: 0.00016896906890906394\n",
            "step: 50, loss: 8.075936057139188e-05\n",
            "step: 60, loss: 0.0005939262337051332\n",
            "step: 70, loss: 4.444065780262463e-05\n",
            "step: 80, loss: 6.375920202117413e-05\n",
            "step: 90, loss: 0.00010305096657248214\n",
            "step: 100, loss: 4.9539219617145136e-05\n",
            "step: 110, loss: 0.001420677057467401\n",
            "step: 120, loss: 3.454606849118136e-05\n",
            "step: 130, loss: 6.735907663824037e-05\n",
            "step: 140, loss: 0.00043236996862106025\n",
            "step: 150, loss: 0.00010836866567842662\n",
            "step: 160, loss: 0.006388865411281586\n",
            "step: 170, loss: 2.491800842108205e-05\n",
            "step: 180, loss: 7.182966510299593e-05\n",
            "step: 190, loss: 7.004093640716746e-05\n",
            "step: 200, loss: 3.900913361576386e-05\n",
            "step: 210, loss: 0.00017443728575017303\n",
            "step: 220, loss: 0.00020356416644062847\n",
            "step: 230, loss: 0.00015541860193479806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9743589743589743, f1=0.9640449438202248, best_f1=0.967525195968645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.142165587632917e-05\n",
            "step: 10, loss: 3.041549825866241e-05\n",
            "step: 20, loss: 0.0001162485932582058\n",
            "step: 30, loss: 8.959570550359786e-05\n",
            "step: 40, loss: 7.263280713232234e-05\n",
            "step: 50, loss: 0.019346781075000763\n",
            "step: 60, loss: 9.126860095420852e-05\n",
            "step: 70, loss: 4.580732638714835e-05\n",
            "step: 80, loss: 8.016022184165195e-05\n",
            "step: 90, loss: 5.485156725626439e-05\n",
            "step: 100, loss: 4.80494272778742e-05\n",
            "step: 110, loss: 5.3298153943615034e-05\n",
            "step: 120, loss: 0.00010598717926768586\n",
            "step: 130, loss: 5.5599633924430236e-05\n",
            "step: 140, loss: 4.6878692955942824e-05\n",
            "step: 150, loss: 0.0001240180863533169\n",
            "step: 160, loss: 3.2941468816716224e-05\n",
            "step: 170, loss: 4.439753683982417e-05\n",
            "step: 180, loss: 5.978147964924574e-05\n",
            "step: 190, loss: 7.696681859670207e-05\n",
            "step: 200, loss: 7.078200724208727e-05\n",
            "step: 210, loss: 0.00012305534619372338\n",
            "step: 220, loss: 4.7646884922869503e-05\n",
            "step: 230, loss: 4.8842932301340625e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9754464285714286, f1=0.9640449438202248, best_f1=0.967525195968645\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 187.53it/s]\n",
            "load_f1 = 0.9720044792833147\n",
            "real_f1 = 0.9665178571428571\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 230.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d80505-1420-42dc-da56-49594ffa7421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6143800020217896\n",
            "step: 10, loss: 0.6107079982757568\n",
            "step: 20, loss: 0.5495185852050781\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.3344217538833618\n",
            "step: 40, loss: 0.2724335193634033\n",
            "step: 50, loss: 0.2885284125804901\n",
            "step: 60, loss: 0.06977561861276627\n",
            "step: 70, loss: 0.22178246080875397\n",
            "step: 80, loss: 0.07152446359395981\n",
            "step: 90, loss: 0.5112090110778809\n",
            "step: 100, loss: 0.09097181260585785\n",
            "step: 110, loss: 0.10768565535545349\n",
            "step: 120, loss: 0.25375062227249146\n",
            "step: 130, loss: 0.1068514809012413\n",
            "step: 140, loss: 0.07895254343748093\n",
            "step: 150, loss: 0.0529199093580246\n",
            "step: 160, loss: 0.04370030388236046\n",
            "step: 170, loss: 0.27685093879699707\n",
            "step: 180, loss: 0.24286963045597076\n",
            "step: 190, loss: 0.01395308505743742\n",
            "step: 200, loss: 0.18746314942836761\n",
            "step: 210, loss: 0.11097604036331177\n",
            "step: 220, loss: 0.26108336448669434\n",
            "step: 230, loss: 0.1655369997024536\n",
            "step: 240, loss: 0.11037975549697876\n",
            "step: 250, loss: 0.06033053621649742\n",
            "step: 260, loss: 0.18583513796329498\n",
            "step: 270, loss: 0.04364939406514168\n",
            "step: 280, loss: 0.09031523019075394\n",
            "step: 290, loss: 0.24646110832691193\n",
            "step: 300, loss: 0.1088217943906784\n",
            "step: 310, loss: 0.19477808475494385\n",
            "step: 320, loss: 0.15098914504051208\n",
            "step: 330, loss: 0.044062454253435135\n",
            "step: 340, loss: 0.03838520869612694\n",
            "step: 350, loss: 0.14836996793746948\n",
            "step: 360, loss: 0.05935284495353699\n",
            "step: 370, loss: 0.09904457628726959\n",
            "step: 380, loss: 0.016084151342511177\n",
            "step: 390, loss: 0.23482616245746613\n",
            "step: 400, loss: 0.2522397041320801\n",
            "step: 410, loss: 0.04074055328965187\n",
            "step: 420, loss: 0.12202010303735733\n",
            "step: 430, loss: 0.1557614803314209\n",
            "step: 440, loss: 0.041242651641368866\n",
            "step: 450, loss: 0.014418542385101318\n",
            "step: 460, loss: 0.026691723614931107\n",
            "step: 470, loss: 0.12599655985832214\n",
            "step: 480, loss: 0.07886381447315216\n",
            "step: 490, loss: 0.07930061221122742\n",
            "step: 500, loss: 0.04353094473481178\n",
            "step: 510, loss: 0.0962212011218071\n",
            "step: 520, loss: 0.05105546861886978\n",
            "step: 530, loss: 0.007637709379196167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9183387774148392, f1=0.9160092807424594, best_f1=0.9160092807424594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14045186340808868\n",
            "step: 10, loss: 0.13468210399150848\n",
            "step: 20, loss: 0.06261622160673141\n",
            "step: 30, loss: 0.06010819599032402\n",
            "step: 40, loss: 0.052910350263118744\n",
            "step: 50, loss: 0.23315049707889557\n",
            "step: 60, loss: 0.0707065761089325\n",
            "step: 70, loss: 0.055115751922130585\n",
            "step: 80, loss: 0.05437713861465454\n",
            "step: 90, loss: 0.026672013103961945\n",
            "step: 100, loss: 0.09207796305418015\n",
            "step: 110, loss: 0.047013040632009506\n",
            "step: 120, loss: 0.2154548615217209\n",
            "step: 130, loss: 0.06390736252069473\n",
            "step: 140, loss: 0.06325040012598038\n",
            "step: 150, loss: 0.0761009082198143\n",
            "step: 160, loss: 0.10553694516420364\n",
            "step: 170, loss: 0.11634659022092819\n",
            "step: 180, loss: 0.010815311223268509\n",
            "step: 190, loss: 0.07330746203660965\n",
            "step: 200, loss: 0.08418290317058563\n",
            "step: 210, loss: 0.06821706146001816\n",
            "step: 220, loss: 0.08542603254318237\n",
            "step: 230, loss: 0.009576880373060703\n",
            "step: 240, loss: 0.1058693379163742\n",
            "step: 250, loss: 0.04749695584177971\n",
            "step: 260, loss: 0.014094185084104538\n",
            "step: 270, loss: 0.19529592990875244\n",
            "step: 280, loss: 0.15432611107826233\n",
            "step: 290, loss: 0.03387221321463585\n",
            "step: 300, loss: 0.1512218564748764\n",
            "step: 310, loss: 0.01096223946660757\n",
            "step: 320, loss: 0.23378074169158936\n",
            "step: 330, loss: 0.03885616362094879\n",
            "step: 340, loss: 0.03851478919386864\n",
            "step: 350, loss: 0.0049616196192801\n",
            "step: 360, loss: 0.05618948116898537\n",
            "step: 370, loss: 0.23980532586574554\n",
            "step: 380, loss: 0.03443773090839386\n",
            "step: 390, loss: 0.04541506618261337\n",
            "step: 400, loss: 0.00583788612857461\n",
            "step: 410, loss: 0.04063829034566879\n",
            "step: 420, loss: 0.013960538432002068\n",
            "step: 430, loss: 0.04758969321846962\n",
            "step: 440, loss: 0.11626311391592026\n",
            "step: 450, loss: 0.13872846961021423\n",
            "step: 460, loss: 0.024087101221084595\n",
            "step: 470, loss: 0.015539290383458138\n",
            "step: 480, loss: 0.1765856146812439\n",
            "step: 490, loss: 0.027657093480229378\n",
            "step: 500, loss: 0.2511291801929474\n",
            "step: 510, loss: 0.041788164526224136\n",
            "step: 520, loss: 0.08713793009519577\n",
            "step: 530, loss: 0.041429970413446426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9235048678720444, f1=0.9227230910763569, best_f1=0.9227230910763569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0794174000620842\n",
            "step: 10, loss: 0.0912197157740593\n",
            "step: 20, loss: 0.057686809450387955\n",
            "step: 30, loss: 0.04591534659266472\n",
            "step: 40, loss: 0.029309462755918503\n",
            "step: 50, loss: 0.11361280828714371\n",
            "step: 60, loss: 0.053782977163791656\n",
            "step: 70, loss: 0.009070180356502533\n",
            "step: 80, loss: 0.0017217075219377875\n",
            "step: 90, loss: 0.006902695633471012\n",
            "step: 100, loss: 0.008071749471127987\n",
            "step: 110, loss: 0.005535590462386608\n",
            "step: 120, loss: 0.003941611386835575\n",
            "step: 130, loss: 0.0025418216828256845\n",
            "step: 140, loss: 0.030540600419044495\n",
            "step: 150, loss: 0.06942742317914963\n",
            "step: 160, loss: 0.005730052012950182\n",
            "step: 170, loss: 0.07616551965475082\n",
            "step: 180, loss: 0.023010922595858574\n",
            "step: 190, loss: 0.013406168669462204\n",
            "step: 200, loss: 0.011774379760026932\n",
            "step: 210, loss: 0.048392027616500854\n",
            "step: 220, loss: 0.31706616282463074\n",
            "step: 230, loss: 0.09928911924362183\n",
            "step: 240, loss: 0.04009568691253662\n",
            "step: 250, loss: 0.031692542135715485\n",
            "step: 260, loss: 0.04933860898017883\n",
            "step: 270, loss: 0.02719922922551632\n",
            "step: 280, loss: 0.20206716656684875\n",
            "step: 290, loss: 0.02364901639521122\n",
            "step: 300, loss: 0.02975475788116455\n",
            "step: 310, loss: 0.0028214443009346724\n",
            "step: 320, loss: 0.028989462181925774\n",
            "step: 330, loss: 0.011692251078784466\n",
            "step: 340, loss: 0.038967255502939224\n",
            "step: 350, loss: 0.08028235286474228\n",
            "step: 360, loss: 0.041403740644454956\n",
            "step: 370, loss: 0.04511276260018349\n",
            "step: 380, loss: 0.07194174826145172\n",
            "step: 390, loss: 0.02014973573386669\n",
            "step: 400, loss: 0.06849836558103561\n",
            "step: 410, loss: 0.007739088963717222\n",
            "step: 420, loss: 0.21488840878009796\n",
            "step: 430, loss: 0.10481742769479752\n",
            "step: 440, loss: 0.027768755331635475\n",
            "step: 450, loss: 0.058370813727378845\n",
            "step: 460, loss: 0.13656499981880188\n",
            "step: 470, loss: 0.1537276953458786\n",
            "step: 480, loss: 0.03513564169406891\n",
            "step: 490, loss: 0.002395092276856303\n",
            "step: 500, loss: 0.13284389674663544\n",
            "step: 510, loss: 0.05093395337462425\n",
            "step: 520, loss: 0.1444637030363083\n",
            "step: 530, loss: 0.15148626267910004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9243697478991597, f1=0.9187935034802784, best_f1=0.9187935034802784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08470093458890915\n",
            "step: 10, loss: 0.0034293034113943577\n",
            "step: 20, loss: 0.0036531619261950254\n",
            "step: 30, loss: 0.005986770614981651\n",
            "step: 40, loss: 0.035343170166015625\n",
            "step: 50, loss: 0.0024830952752381563\n",
            "step: 60, loss: 0.004348542541265488\n",
            "step: 70, loss: 0.00917125679552555\n",
            "step: 80, loss: 0.0703708678483963\n",
            "step: 90, loss: 0.21332202851772308\n",
            "step: 100, loss: 0.005757179576903582\n",
            "step: 110, loss: 0.012264780700206757\n",
            "step: 120, loss: 0.002559575717896223\n",
            "step: 130, loss: 0.008404947817325592\n",
            "step: 140, loss: 0.007065438665449619\n",
            "step: 150, loss: 0.053603094071149826\n",
            "step: 160, loss: 0.05154614523053169\n",
            "step: 170, loss: 0.039344679564237595\n",
            "step: 180, loss: 0.004451955668628216\n",
            "step: 190, loss: 0.014134763740003109\n",
            "step: 200, loss: 0.011736392043530941\n",
            "step: 210, loss: 0.06970918923616409\n",
            "step: 220, loss: 0.0070346323773264885\n",
            "step: 230, loss: 0.05760076642036438\n",
            "step: 240, loss: 0.009147468954324722\n",
            "step: 250, loss: 0.015964161604642868\n",
            "step: 260, loss: 0.009704791940748692\n",
            "step: 270, loss: 0.009735665284097195\n",
            "step: 280, loss: 0.04866034910082817\n",
            "step: 290, loss: 0.08362667262554169\n",
            "step: 300, loss: 0.0007186868460848927\n",
            "step: 310, loss: 0.0026541040278971195\n",
            "step: 320, loss: 0.0029841274954378605\n",
            "step: 330, loss: 0.002260664477944374\n",
            "step: 340, loss: 0.04167194664478302\n",
            "step: 350, loss: 0.02869049645960331\n",
            "step: 360, loss: 0.03824890777468681\n",
            "step: 370, loss: 0.007616096641868353\n",
            "step: 380, loss: 0.04708746075630188\n",
            "step: 390, loss: 0.0008733168360777199\n",
            "step: 400, loss: 0.005320468917489052\n",
            "step: 410, loss: 0.00224745599552989\n",
            "step: 420, loss: 0.037555016577243805\n",
            "step: 430, loss: 0.1941710114479065\n",
            "step: 440, loss: 0.01157333143055439\n",
            "step: 450, loss: 0.0014886100543662906\n",
            "step: 460, loss: 0.00202228338457644\n",
            "step: 470, loss: 0.006266443058848381\n",
            "step: 480, loss: 0.022444292902946472\n",
            "step: 490, loss: 0.016017859801650047\n",
            "step: 500, loss: 0.0031992383301258087\n",
            "step: 510, loss: 0.03667314723134041\n",
            "step: 520, loss: 0.03291499987244606\n",
            "step: 530, loss: 0.04815642535686493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9238532110091744, f1=0.9187214611872146, best_f1=0.9187935034802784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04118892922997475\n",
            "step: 10, loss: 0.03832879662513733\n",
            "step: 20, loss: 0.004688102286309004\n",
            "step: 30, loss: 0.003060600720345974\n",
            "step: 40, loss: 0.019676445052027702\n",
            "step: 50, loss: 0.016627419739961624\n",
            "step: 60, loss: 0.010351854376494884\n",
            "step: 70, loss: 0.002457307418808341\n",
            "step: 80, loss: 0.00044279469875618815\n",
            "step: 90, loss: 0.07375013828277588\n",
            "step: 100, loss: 0.0009567334200255573\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 110, loss: 0.0014463256811723113\n",
            "step: 120, loss: 0.02276797406375408\n",
            "step: 130, loss: 0.0009370458428747952\n",
            "step: 140, loss: 0.023840108886361122\n",
            "step: 150, loss: 0.04772519692778587\n",
            "step: 160, loss: 0.009607616811990738\n",
            "step: 170, loss: 0.009491724893450737\n",
            "step: 180, loss: 0.0022538460325449705\n",
            "step: 190, loss: 0.0032441888470202684\n",
            "step: 200, loss: 0.03602668642997742\n",
            "step: 210, loss: 0.0026468115393072367\n",
            "step: 220, loss: 0.045459579676389694\n",
            "step: 230, loss: 0.008501755073666573\n",
            "step: 240, loss: 0.0017420969670638442\n",
            "step: 250, loss: 0.0040196748450398445\n",
            "step: 260, loss: 0.08224219083786011\n",
            "step: 270, loss: 0.0004794928536284715\n",
            "step: 280, loss: 0.002031089272350073\n",
            "step: 290, loss: 0.21573208272457123\n",
            "step: 300, loss: 0.017711395397782326\n",
            "step: 310, loss: 0.0004294169775675982\n",
            "step: 320, loss: 0.12062632292509079\n",
            "step: 330, loss: 0.03956364840269089\n",
            "step: 340, loss: 0.00544760562479496\n",
            "step: 350, loss: 0.00575025612488389\n",
            "step: 360, loss: 0.059293631464242935\n",
            "step: 370, loss: 0.03280239924788475\n",
            "step: 380, loss: 0.01659110002219677\n",
            "step: 390, loss: 0.000406081962864846\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 400, loss: 0.01091269962489605\n",
            "step: 410, loss: 0.012567062862217426\n",
            "step: 420, loss: 0.00287185562774539\n",
            "step: 430, loss: 0.05890943482518196\n",
            "step: 440, loss: 0.05951344966888428\n",
            "step: 450, loss: 0.011608695611357689\n",
            "step: 460, loss: 0.001254475093446672\n",
            "step: 470, loss: 0.05224082991480827\n",
            "step: 480, loss: 0.0035067580174654722\n",
            "step: 490, loss: 0.0029539563693106174\n",
            "step: 500, loss: 0.017411770299077034\n",
            "step: 510, loss: 0.04000784829258919\n",
            "step: 520, loss: 0.01823197491466999\n",
            "step: 530, loss: 0.005805218126624823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8967867575462514, f1=0.8952752070141257, best_f1=0.9187935034802784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001303438562899828\n",
            "step: 10, loss: 0.0026271785609424114\n",
            "step: 20, loss: 0.09054131805896759\n",
            "step: 30, loss: 0.000871316238772124\n",
            "step: 40, loss: 0.0006168611580505967\n",
            "step: 50, loss: 0.043248821049928665\n",
            "step: 60, loss: 0.00038178646354936063\n",
            "step: 70, loss: 0.010745739564299583\n",
            "step: 80, loss: 0.0022684696596115828\n",
            "step: 90, loss: 0.003440654370933771\n",
            "step: 100, loss: 0.08381766825914383\n",
            "step: 110, loss: 0.04273701831698418\n",
            "step: 120, loss: 0.0001499260397395119\n",
            "step: 130, loss: 0.0019656959921121597\n",
            "step: 140, loss: 0.0025565146934241056\n",
            "step: 150, loss: 0.011122038587927818\n",
            "step: 160, loss: 0.010983188636600971\n",
            "step: 170, loss: 0.028684202581644058\n",
            "step: 180, loss: 0.0007835359429009259\n",
            "step: 190, loss: 0.03759799525141716\n",
            "step: 200, loss: 0.0003010336949955672\n",
            "step: 210, loss: 0.014738277532160282\n",
            "step: 220, loss: 0.0009390326449647546\n",
            "step: 230, loss: 0.0023324869107455015\n",
            "step: 240, loss: 0.024812476709485054\n",
            "step: 250, loss: 0.0028423392213881016\n",
            "step: 260, loss: 0.009309235028922558\n",
            "step: 270, loss: 0.1722600758075714\n",
            "step: 280, loss: 0.0013405239442363381\n",
            "step: 290, loss: 0.0011463250266388059\n",
            "step: 300, loss: 0.008097988553345203\n",
            "step: 310, loss: 0.026912400498986244\n",
            "step: 320, loss: 0.004690709989517927\n",
            "step: 330, loss: 0.0015001374995335937\n",
            "step: 340, loss: 0.13336491584777832\n",
            "step: 350, loss: 0.004049505572766066\n",
            "step: 360, loss: 0.0019188944716006517\n",
            "step: 370, loss: 0.0031061198096722364\n",
            "step: 380, loss: 0.002074842806905508\n",
            "step: 390, loss: 0.04850439354777336\n",
            "step: 400, loss: 0.0002571083896327764\n",
            "step: 410, loss: 0.014434152282774448\n",
            "step: 420, loss: 0.002484304364770651\n",
            "step: 430, loss: 0.00634766323491931\n",
            "step: 440, loss: 0.00010047991963801906\n",
            "step: 450, loss: 0.0002439011150272563\n",
            "step: 460, loss: 0.00042983138700947165\n",
            "step: 470, loss: 0.01407709065824747\n",
            "step: 480, loss: 0.027520904317498207\n",
            "step: 490, loss: 0.001890208455733955\n",
            "step: 500, loss: 0.0034063286148011684\n",
            "step: 510, loss: 0.03623661771416664\n",
            "step: 520, loss: 0.0035270731896162033\n",
            "step: 530, loss: 0.06521754711866379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9251510925151093, f1=0.9161290322580645, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006660466082394123\n",
            "step: 10, loss: 0.00392266595736146\n",
            "step: 20, loss: 0.0016389790689572692\n",
            "step: 30, loss: 0.010404354892671108\n",
            "step: 40, loss: 0.001154205296188593\n",
            "step: 50, loss: 0.0007050023414194584\n",
            "step: 60, loss: 0.0014506984734907746\n",
            "step: 70, loss: 0.045148082077503204\n",
            "step: 80, loss: 0.02110670506954193\n",
            "step: 90, loss: 0.02400854043662548\n",
            "step: 100, loss: 0.00016791837697383016\n",
            "step: 110, loss: 0.0019041572231799364\n",
            "step: 120, loss: 0.0006936682038940489\n",
            "step: 130, loss: 0.0064859576523303986\n",
            "step: 140, loss: 0.0005775102181360126\n",
            "step: 150, loss: 0.0006007289630360901\n",
            "step: 160, loss: 0.0006833840743638575\n",
            "step: 170, loss: 0.00032431542058475316\n",
            "step: 180, loss: 0.0002907621383201331\n",
            "step: 190, loss: 0.0002558463893365115\n",
            "step: 200, loss: 0.00011421266390243545\n",
            "step: 210, loss: 0.03459353744983673\n",
            "step: 220, loss: 0.0003069808299187571\n",
            "step: 230, loss: 0.0035799795296043158\n",
            "step: 240, loss: 0.0005277845775708556\n",
            "step: 250, loss: 0.024713050574064255\n",
            "step: 260, loss: 0.0002804416581057012\n",
            "step: 270, loss: 0.004973102826625109\n",
            "step: 280, loss: 0.000253889273153618\n",
            "step: 290, loss: 0.007655272260308266\n",
            "step: 300, loss: 0.015705937519669533\n",
            "step: 310, loss: 0.00040564429946243763\n",
            "step: 320, loss: 0.00015116625581867993\n",
            "step: 330, loss: 0.0009286087588407099\n",
            "step: 340, loss: 0.038451388478279114\n",
            "step: 350, loss: 0.0008159350836649537\n",
            "step: 360, loss: 0.0028280112892389297\n",
            "step: 370, loss: 0.0006837784894742072\n",
            "step: 380, loss: 0.0016764963511377573\n",
            "step: 390, loss: 0.0003946211945731193\n",
            "step: 400, loss: 0.004907434806227684\n",
            "step: 410, loss: 0.011858836747705936\n",
            "step: 420, loss: 0.0017632985254749656\n",
            "step: 430, loss: 0.005514903925359249\n",
            "step: 440, loss: 0.0010220559779554605\n",
            "step: 450, loss: 0.0006940469611436129\n",
            "step: 460, loss: 0.00012972527474630624\n",
            "step: 470, loss: 0.013773364014923573\n",
            "step: 480, loss: 0.0038199143018573523\n",
            "step: 490, loss: 0.001555086812004447\n",
            "step: 500, loss: 0.000590239476878196\n",
            "step: 510, loss: 0.002915311139076948\n",
            "step: 520, loss: 0.024404587224125862\n",
            "step: 530, loss: 0.001074276166036725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9180176007410837, f1=0.9160092807424594, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013401573523879051\n",
            "step: 10, loss: 0.0051147048361599445\n",
            "step: 20, loss: 0.00014262455806601793\n",
            "step: 30, loss: 0.0005918109090998769\n",
            "step: 40, loss: 0.0002980410063173622\n",
            "step: 50, loss: 0.03454600274562836\n",
            "step: 60, loss: 0.05071518197655678\n",
            "step: 70, loss: 0.00012477763812057674\n",
            "step: 80, loss: 0.0002470381441526115\n",
            "step: 90, loss: 0.00032830442069098353\n",
            "step: 100, loss: 0.00017701010801829398\n",
            "step: 110, loss: 0.0004814887070097029\n",
            "step: 120, loss: 0.000602392596192658\n",
            "step: 130, loss: 0.0002552592195570469\n",
            "step: 140, loss: 0.0005699215107597411\n",
            "step: 150, loss: 0.0004752445383928716\n",
            "step: 160, loss: 0.002305036410689354\n",
            "step: 170, loss: 0.004342278931289911\n",
            "step: 180, loss: 8.050599717535079e-05\n",
            "step: 190, loss: 0.004250559490174055\n",
            "step: 200, loss: 0.0004446472448762506\n",
            "step: 210, loss: 0.05942107364535332\n",
            "step: 220, loss: 0.0018906358163803816\n",
            "step: 230, loss: 0.00036484733573161066\n",
            "step: 240, loss: 0.0003395344829186797\n",
            "step: 250, loss: 0.000321995175909251\n",
            "step: 260, loss: 0.000379120116122067\n",
            "step: 270, loss: 0.011813885532319546\n",
            "step: 280, loss: 0.026912132278084755\n",
            "step: 290, loss: 0.0002750580315478146\n",
            "step: 300, loss: 0.0003593215951696038\n",
            "step: 310, loss: 0.005581999663263559\n",
            "step: 320, loss: 0.0015324440319091082\n",
            "step: 330, loss: 0.0036529293283820152\n",
            "step: 340, loss: 0.011417010799050331\n",
            "step: 350, loss: 0.001868985127657652\n",
            "step: 360, loss: 0.00462953420355916\n",
            "step: 370, loss: 0.00011483146954560652\n",
            "step: 380, loss: 0.0008160565630532801\n",
            "step: 390, loss: 0.0012284774566069245\n",
            "step: 400, loss: 0.00019563913519959897\n",
            "step: 410, loss: 7.622369594173506e-05\n",
            "step: 420, loss: 0.0003940134774893522\n",
            "step: 430, loss: 0.009035359136760235\n",
            "step: 440, loss: 0.0011971023632213473\n",
            "step: 450, loss: 8.761665958445519e-05\n",
            "step: 460, loss: 0.0004204483120702207\n",
            "step: 470, loss: 0.00014276454749051481\n",
            "step: 480, loss: 7.624840509379283e-05\n",
            "step: 490, loss: 0.10860516130924225\n",
            "step: 500, loss: 0.0007669830229133368\n",
            "step: 510, loss: 0.0001962861279025674\n",
            "step: 520, loss: 0.0025009089149534702\n",
            "step: 530, loss: 0.0027319223154336214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.913744075829384, f1=0.9133112269066792, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023980385158210993\n",
            "step: 10, loss: 0.00023668617359362543\n",
            "step: 20, loss: 0.0012830188497900963\n",
            "step: 30, loss: 0.0004890567506663501\n",
            "step: 40, loss: 0.0001194758660858497\n",
            "step: 50, loss: 0.00010590861347736791\n",
            "step: 60, loss: 0.0001264856255147606\n",
            "step: 70, loss: 0.11610980331897736\n",
            "step: 80, loss: 0.003671110374853015\n",
            "step: 90, loss: 0.00024385721189901233\n",
            "step: 100, loss: 0.0002390186709817499\n",
            "step: 110, loss: 0.0004004244692623615\n",
            "step: 120, loss: 0.014296798966825008\n",
            "step: 130, loss: 0.001184467226266861\n",
            "step: 140, loss: 0.00036267019459046423\n",
            "step: 150, loss: 0.0013098119525238872\n",
            "step: 160, loss: 0.00021152480621822178\n",
            "step: 170, loss: 0.00188922428060323\n",
            "step: 180, loss: 6.710253364872187e-05\n",
            "step: 190, loss: 0.00026610997156240046\n",
            "step: 200, loss: 0.000641010410618037\n",
            "step: 210, loss: 0.00015001538849901408\n",
            "step: 220, loss: 0.0005229744710959494\n",
            "step: 230, loss: 0.0005955936503596604\n",
            "step: 240, loss: 0.00013739043788518757\n",
            "step: 250, loss: 8.948751928983256e-05\n",
            "step: 260, loss: 0.007757789921015501\n",
            "step: 270, loss: 7.387929508695379e-05\n",
            "step: 280, loss: 0.04322977364063263\n",
            "step: 290, loss: 0.0015279370127245784\n",
            "step: 300, loss: 0.00010845921497093514\n",
            "step: 310, loss: 0.00025151800946332514\n",
            "step: 320, loss: 0.00040445144986733794\n",
            "step: 330, loss: 0.0001508282293798402\n",
            "step: 340, loss: 0.00010723431478254497\n",
            "step: 350, loss: 0.00010587657743599266\n",
            "step: 360, loss: 0.0006168283871375024\n",
            "step: 370, loss: 0.0003484991320874542\n",
            "step: 380, loss: 0.00010364697664044797\n",
            "step: 390, loss: 5.0102487875847146e-05\n",
            "step: 400, loss: 0.0168928150087595\n",
            "step: 410, loss: 0.00013247101742308587\n",
            "step: 420, loss: 4.749779327539727e-05\n",
            "step: 430, loss: 0.00018816784722730517\n",
            "step: 440, loss: 0.002808762714266777\n",
            "step: 450, loss: 0.00011582692968659103\n",
            "step: 460, loss: 0.0001217479511979036\n",
            "step: 470, loss: 0.00016779064026195556\n",
            "step: 480, loss: 0.0008025650167837739\n",
            "step: 490, loss: 0.0019291017670184374\n",
            "step: 500, loss: 0.00041723731555975974\n",
            "step: 510, loss: 0.0003696575586218387\n",
            "step: 520, loss: 0.00010395538265584037\n",
            "step: 530, loss: 0.000744465971365571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9192886456908345, f1=0.915085817524842, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.39518111711368e-05\n",
            "step: 10, loss: 0.0002313431177753955\n",
            "step: 20, loss: 0.000128546409541741\n",
            "step: 30, loss: 0.0002199810405727476\n",
            "step: 40, loss: 0.0013255588710308075\n",
            "step: 50, loss: 0.0019189317245036364\n",
            "step: 60, loss: 0.00018118511070497334\n",
            "step: 70, loss: 0.000464934651972726\n",
            "step: 80, loss: 7.151676254579797e-05\n",
            "step: 90, loss: 0.0006183706573210657\n",
            "step: 100, loss: 5.399406290962361e-05\n",
            "step: 110, loss: 5.39096217835322e-05\n",
            "step: 120, loss: 8.738986070966348e-05\n",
            "step: 130, loss: 6.409535126294941e-05\n",
            "step: 140, loss: 0.0016151579329743981\n",
            "step: 150, loss: 8.026765135582536e-05\n",
            "step: 160, loss: 0.00035866396501660347\n",
            "step: 170, loss: 0.01865571178495884\n",
            "step: 180, loss: 0.004299794789403677\n",
            "step: 190, loss: 0.0007758447318337858\n",
            "step: 200, loss: 0.00033166076173074543\n",
            "step: 210, loss: 0.00847176555544138\n",
            "step: 220, loss: 0.08945848047733307\n",
            "step: 230, loss: 0.0011531367199495435\n",
            "step: 240, loss: 0.0004439821350388229\n",
            "step: 250, loss: 0.01578388549387455\n",
            "step: 260, loss: 0.0036296071484684944\n",
            "step: 270, loss: 0.00012406350288074464\n",
            "step: 280, loss: 0.0001766445057000965\n",
            "step: 290, loss: 0.00019983485981356353\n",
            "step: 300, loss: 8.82840613485314e-05\n",
            "step: 310, loss: 0.00011363825615262613\n",
            "step: 320, loss: 0.0008197837742045522\n",
            "step: 330, loss: 0.0005792459705844522\n",
            "step: 340, loss: 0.0001500110374763608\n",
            "step: 350, loss: 0.0005274183349683881\n",
            "step: 360, loss: 0.00016257820243481547\n",
            "step: 370, loss: 0.0012607049429789186\n",
            "step: 380, loss: 0.0003271974273957312\n",
            "step: 390, loss: 0.009399236179888248\n",
            "step: 400, loss: 0.001438653445802629\n",
            "step: 410, loss: 0.0048252977430820465\n",
            "step: 420, loss: 0.00287920655682683\n",
            "step: 430, loss: 6.388821202563122e-05\n",
            "step: 440, loss: 0.0008165038307197392\n",
            "step: 450, loss: 0.0003140467160847038\n",
            "step: 460, loss: 0.0001670285128057003\n",
            "step: 470, loss: 0.0001284251338802278\n",
            "step: 480, loss: 4.876195453107357e-05\n",
            "step: 490, loss: 0.00012084388436051086\n",
            "step: 500, loss: 0.007599333301186562\n",
            "step: 510, loss: 0.00017207404016517103\n",
            "step: 520, loss: 0.000479098001960665\n",
            "step: 530, loss: 9.863329614745453e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9240093240093239, f1=0.9089227924179382, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001887415419332683\n",
            "step: 10, loss: 0.0002217032015323639\n",
            "step: 20, loss: 9.840155689744279e-05\n",
            "step: 30, loss: 0.0003094712446909398\n",
            "step: 40, loss: 7.682076829951257e-05\n",
            "step: 50, loss: 0.0001220640042447485\n",
            "step: 60, loss: 0.0011592821683734655\n",
            "step: 70, loss: 6.526173820020631e-05\n",
            "step: 80, loss: 3.8242233131313697e-05\n",
            "step: 90, loss: 0.0019188262522220612\n",
            "step: 100, loss: 4.536036794888787e-05\n",
            "step: 110, loss: 0.0001032659929478541\n",
            "step: 120, loss: 4.819959576707333e-05\n",
            "step: 130, loss: 0.0018791857874020934\n",
            "step: 140, loss: 4.29199353675358e-05\n",
            "step: 150, loss: 3.689395452965982e-05\n",
            "step: 160, loss: 0.0002740125637501478\n",
            "step: 170, loss: 3.673013998195529e-05\n",
            "step: 180, loss: 0.00037931549013592303\n",
            "step: 190, loss: 0.00010959277278743684\n",
            "step: 200, loss: 0.21925880014896393\n",
            "step: 210, loss: 0.0015664221718907356\n",
            "step: 220, loss: 0.0009102922631427646\n",
            "step: 230, loss: 0.00014119800471235067\n",
            "step: 240, loss: 0.0004701896687038243\n",
            "step: 250, loss: 0.0001868308027042076\n",
            "step: 260, loss: 5.048307139077224e-05\n",
            "step: 270, loss: 0.012140827253460884\n",
            "step: 280, loss: 0.003655750071629882\n",
            "step: 290, loss: 5.198937651584856e-05\n",
            "step: 300, loss: 4.607940718415193e-05\n",
            "step: 310, loss: 0.000231081634410657\n",
            "step: 320, loss: 0.0001273336965823546\n",
            "step: 330, loss: 0.00015929387882351875\n",
            "step: 340, loss: 0.002118233125656843\n",
            "step: 350, loss: 6.834908708697185e-05\n",
            "step: 360, loss: 0.00011000027734553441\n",
            "step: 370, loss: 0.0002841016976162791\n",
            "step: 380, loss: 4.506995901465416e-05\n",
            "step: 390, loss: 3.8186168239917606e-05\n",
            "step: 400, loss: 6.474372639786452e-05\n",
            "step: 410, loss: 7.640023250132799e-05\n",
            "step: 420, loss: 8.71533848112449e-05\n",
            "step: 430, loss: 0.00022111473663244396\n",
            "step: 440, loss: 8.886534487828612e-05\n",
            "step: 450, loss: 4.3457013816805556e-05\n",
            "step: 460, loss: 0.0008243880001828074\n",
            "step: 470, loss: 4.748003993881866e-05\n",
            "step: 480, loss: 9.653170127421618e-05\n",
            "step: 490, loss: 0.00025370370713062584\n",
            "step: 500, loss: 9.129189129453152e-05\n",
            "step: 510, loss: 0.00038582709385082126\n",
            "step: 520, loss: 6.521683098981157e-05\n",
            "step: 530, loss: 0.0002895719662774354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9200743494423792, f1=0.9122969837587007, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027506486512720585\n",
            "step: 10, loss: 4.5660726755158976e-05\n",
            "step: 20, loss: 8.77981074154377e-05\n",
            "step: 30, loss: 0.005845976062119007\n",
            "step: 40, loss: 0.00020839097851421684\n",
            "step: 50, loss: 0.010091094300150871\n",
            "step: 60, loss: 0.0003538182645570487\n",
            "step: 70, loss: 0.00010746590123744681\n",
            "step: 80, loss: 5.568841879721731e-05\n",
            "step: 90, loss: 3.894961628247984e-05\n",
            "step: 100, loss: 8.93782707862556e-05\n",
            "step: 110, loss: 0.00016224279534071684\n",
            "step: 120, loss: 5.6278113333974034e-05\n",
            "step: 130, loss: 7.37637747079134e-05\n",
            "step: 140, loss: 4.9596012104302645e-05\n",
            "step: 150, loss: 3.584473597584292e-05\n",
            "step: 160, loss: 2.9067054128972813e-05\n",
            "step: 170, loss: 9.475155093241483e-05\n",
            "step: 180, loss: 7.131074380595237e-05\n",
            "step: 190, loss: 0.00046693364856764674\n",
            "step: 200, loss: 3.6301837099017575e-05\n",
            "step: 210, loss: 5.889755266252905e-05\n",
            "step: 220, loss: 0.0001548451546113938\n",
            "step: 230, loss: 2.5137644115602598e-05\n",
            "step: 240, loss: 8.298030297737569e-05\n",
            "step: 250, loss: 0.00021358547382988036\n",
            "step: 260, loss: 0.029007796198129654\n",
            "step: 270, loss: 7.082497177179903e-05\n",
            "step: 280, loss: 5.5576441809535027e-05\n",
            "step: 290, loss: 5.5403470469173044e-05\n",
            "step: 300, loss: 0.00103416305501014\n",
            "step: 310, loss: 0.00010142081009689718\n",
            "step: 320, loss: 0.027041980996727943\n",
            "step: 330, loss: 3.0147426514304243e-05\n",
            "step: 340, loss: 3.139574982924387e-05\n",
            "step: 350, loss: 0.0001457954931538552\n",
            "step: 360, loss: 9.114255954045802e-05\n",
            "step: 370, loss: 0.00031529658008366823\n",
            "step: 380, loss: 0.00037083119968883693\n",
            "step: 390, loss: 0.0002743860241025686\n",
            "step: 400, loss: 6.897807179484516e-05\n",
            "step: 410, loss: 9.553177369525656e-05\n",
            "step: 420, loss: 0.0016009691171348095\n",
            "step: 430, loss: 0.0003329040773678571\n",
            "step: 440, loss: 6.855140964034945e-05\n",
            "step: 450, loss: 0.00017616462719161063\n",
            "step: 460, loss: 3.1410429073730484e-05\n",
            "step: 470, loss: 3.893806569976732e-05\n",
            "step: 480, loss: 0.00016966943803709\n",
            "step: 490, loss: 0.00011057006486225873\n",
            "step: 500, loss: 0.0005201826570555568\n",
            "step: 510, loss: 0.00475282734259963\n",
            "step: 520, loss: 4.74271691928152e-05\n",
            "step: 530, loss: 3.772848504013382e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9240622140896615, f1=0.9203459262630859, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003532032133080065\n",
            "step: 10, loss: 4.681012796936557e-05\n",
            "step: 20, loss: 0.0003000826691277325\n",
            "step: 30, loss: 0.002957836724817753\n",
            "step: 40, loss: 6.570079131051898e-05\n",
            "step: 50, loss: 0.0002389862056588754\n",
            "step: 60, loss: 6.443647725973278e-05\n",
            "step: 70, loss: 3.803774234256707e-05\n",
            "step: 80, loss: 4.897519829683006e-05\n",
            "step: 90, loss: 0.0005158839630894363\n",
            "step: 100, loss: 0.004604771733283997\n",
            "step: 110, loss: 8.270446414826438e-05\n",
            "step: 120, loss: 3.0542702006641775e-05\n",
            "step: 130, loss: 2.7274583771941252e-05\n",
            "step: 140, loss: 0.00010311775986338034\n",
            "step: 150, loss: 4.082830491825007e-05\n",
            "step: 160, loss: 4.401059413794428e-05\n",
            "step: 170, loss: 0.002458004280924797\n",
            "step: 180, loss: 3.827177351922728e-05\n",
            "step: 190, loss: 8.19078559288755e-05\n",
            "step: 200, loss: 8.79556464497e-05\n",
            "step: 210, loss: 2.8005853891954757e-05\n",
            "step: 220, loss: 2.3450167645933107e-05\n",
            "step: 230, loss: 1.976972089323681e-05\n",
            "step: 240, loss: 4.067316695000045e-05\n",
            "step: 250, loss: 2.6091322069987655e-05\n",
            "step: 260, loss: 2.631084862514399e-05\n",
            "step: 270, loss: 2.4757395294727758e-05\n",
            "step: 280, loss: 4.316139529692009e-05\n",
            "step: 290, loss: 7.615180948050693e-05\n",
            "step: 300, loss: 6.591167039005086e-05\n",
            "step: 310, loss: 0.0018230135319754481\n",
            "step: 320, loss: 0.0076302820816636086\n",
            "step: 330, loss: 0.00538630411028862\n",
            "step: 340, loss: 3.187589391018264e-05\n",
            "step: 350, loss: 3.062816176679917e-05\n",
            "step: 360, loss: 0.02947108820080757\n",
            "step: 370, loss: 3.518371158861555e-05\n",
            "step: 380, loss: 4.580415043164976e-05\n",
            "step: 390, loss: 3.669657598948106e-05\n",
            "step: 400, loss: 2.6422667360748164e-05\n",
            "step: 410, loss: 0.00012426861212588847\n",
            "step: 420, loss: 3.578529504011385e-05\n",
            "step: 430, loss: 0.007159748114645481\n",
            "step: 440, loss: 7.798551087034866e-05\n",
            "step: 450, loss: 6.180829223012552e-05\n",
            "step: 460, loss: 0.0005916255759075284\n",
            "step: 470, loss: 8.415251795668155e-05\n",
            "step: 480, loss: 1.9617084035417065e-05\n",
            "step: 490, loss: 5.03226910950616e-05\n",
            "step: 500, loss: 1.9818131477222778e-05\n",
            "step: 510, loss: 0.00018146986258216202\n",
            "step: 520, loss: 0.008449992164969444\n",
            "step: 530, loss: 3.0143117328407243e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9171428571428571, f1=0.914339801230478, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013865585788153112\n",
            "step: 10, loss: 2.1092131646582857e-05\n",
            "step: 20, loss: 6.898032006574795e-05\n",
            "step: 30, loss: 2.5133938834187575e-05\n",
            "step: 40, loss: 6.14163582213223e-05\n",
            "step: 50, loss: 0.0001560638629598543\n",
            "step: 60, loss: 4.204561628284864e-05\n",
            "step: 70, loss: 0.0020134865771979094\n",
            "step: 80, loss: 0.0014126162277534604\n",
            "step: 90, loss: 0.00010403302439954132\n",
            "step: 100, loss: 3.003950223501306e-05\n",
            "step: 110, loss: 2.470177241775673e-05\n",
            "step: 120, loss: 2.3192969820229337e-05\n",
            "step: 130, loss: 0.0008930280455388129\n",
            "step: 140, loss: 3.567373278201558e-05\n",
            "step: 150, loss: 4.797432120540179e-05\n",
            "step: 160, loss: 0.00010297936387360096\n",
            "step: 170, loss: 5.289493128657341e-05\n",
            "step: 180, loss: 2.7026135285268538e-05\n",
            "step: 190, loss: 0.0001173585987999104\n",
            "step: 200, loss: 4.991512469132431e-05\n",
            "step: 210, loss: 2.3785507437423803e-05\n",
            "step: 220, loss: 3.054941407754086e-05\n",
            "step: 230, loss: 0.00012950049131177366\n",
            "step: 240, loss: 5.028907617088407e-05\n",
            "step: 250, loss: 4.476170215639286e-05\n",
            "step: 260, loss: 0.00011719614849425852\n",
            "step: 270, loss: 8.364575478481129e-05\n",
            "step: 280, loss: 3.555903094820678e-05\n",
            "step: 290, loss: 5.905404395889491e-05\n",
            "step: 300, loss: 0.0093693183735013\n",
            "step: 310, loss: 3.5764915082836524e-05\n",
            "step: 320, loss: 8.659241575514898e-05\n",
            "step: 330, loss: 5.5020893341861665e-05\n",
            "step: 340, loss: 5.078493268229067e-05\n",
            "step: 350, loss: 4.119370350963436e-05\n",
            "step: 360, loss: 0.0005837864009663463\n",
            "step: 370, loss: 0.00010801666940096766\n",
            "step: 380, loss: 0.0006235428736545146\n",
            "step: 390, loss: 0.02498939074575901\n",
            "step: 400, loss: 0.00010978255158988759\n",
            "step: 410, loss: 1.8953915059682913e-05\n",
            "step: 420, loss: 2.3669837901252322e-05\n",
            "step: 430, loss: 3.794037183979526e-05\n",
            "step: 440, loss: 9.274420881411061e-05\n",
            "step: 450, loss: 4.5257300371304154e-05\n",
            "step: 460, loss: 2.5599380023777485e-05\n",
            "step: 470, loss: 2.8757338441209868e-05\n",
            "step: 480, loss: 2.4846875021466985e-05\n",
            "step: 490, loss: 2.80873391602654e-05\n",
            "step: 500, loss: 1.9222146875108592e-05\n",
            "step: 510, loss: 0.0003137631865683943\n",
            "step: 520, loss: 3.147298048133962e-05\n",
            "step: 530, loss: 0.0009378571412526071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9211267605633803, f1=0.9206349206349207, best_f1=0.9161290322580645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019243224523961544\n",
            "step: 10, loss: 2.530510028009303e-05\n",
            "step: 20, loss: 3.2354470022255555e-05\n",
            "step: 30, loss: 0.00010604789713397622\n",
            "step: 40, loss: 8.114125375868753e-05\n",
            "step: 50, loss: 0.001059560920111835\n",
            "step: 60, loss: 1.923690433613956e-05\n",
            "step: 70, loss: 0.0002254409482702613\n",
            "step: 80, loss: 2.4258422854472883e-05\n",
            "step: 90, loss: 0.0007244282751344144\n",
            "step: 100, loss: 6.953974661882967e-05\n",
            "step: 110, loss: 0.0005227940273471177\n",
            "step: 120, loss: 8.53989549796097e-05\n",
            "step: 130, loss: 0.0036613321863114834\n",
            "step: 140, loss: 6.251762533793226e-05\n",
            "step: 150, loss: 3.970854959334247e-05\n",
            "step: 160, loss: 8.92361713340506e-05\n",
            "step: 170, loss: 4.1762406908674166e-05\n",
            "step: 180, loss: 1.7210548321600072e-05\n",
            "step: 190, loss: 4.116906347917393e-05\n",
            "step: 200, loss: 2.4548977307858877e-05\n",
            "step: 210, loss: 8.927725139074028e-05\n",
            "step: 220, loss: 0.00012316840002313256\n",
            "step: 230, loss: 8.932689524954185e-05\n",
            "step: 240, loss: 2.0902261894661933e-05\n",
            "step: 250, loss: 5.516616874956526e-05\n",
            "step: 260, loss: 0.00013078711344860494\n",
            "step: 270, loss: 7.12586406734772e-05\n",
            "step: 280, loss: 3.7814737879671156e-05\n",
            "step: 290, loss: 0.04060002788901329\n",
            "step: 300, loss: 7.642189302714542e-05\n",
            "step: 310, loss: 0.00016089262499008328\n",
            "step: 320, loss: 1.719936881272588e-05\n",
            "step: 330, loss: 3.534551433403976e-05\n",
            "step: 340, loss: 2.3837581466068514e-05\n",
            "step: 350, loss: 2.185938683396671e-05\n",
            "step: 360, loss: 0.0007592376205138862\n",
            "step: 370, loss: 1.8246108083985746e-05\n",
            "step: 380, loss: 6.406955071724951e-05\n",
            "step: 390, loss: 3.735821883310564e-05\n",
            "step: 400, loss: 2.7487893021316268e-05\n",
            "step: 410, loss: 0.00026385675300844014\n",
            "step: 420, loss: 2.9235150577733293e-05\n",
            "step: 430, loss: 0.00012142696505179629\n",
            "step: 440, loss: 4.559612716548145e-05\n",
            "step: 450, loss: 0.00010717218538047746\n",
            "step: 460, loss: 3.1186042178887874e-05\n",
            "step: 470, loss: 0.0002379176439717412\n",
            "step: 480, loss: 1.487120425736066e-05\n",
            "step: 490, loss: 2.4228540496551432e-05\n",
            "step: 500, loss: 2.1848371034138836e-05\n",
            "step: 510, loss: 1.966915442608297e-05\n",
            "step: 520, loss: 2.6780175176099874e-05\n",
            "step: 530, loss: 0.0001834465510910377\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9230055658627088, f1=0.9207373271889401, best_f1=0.9161290322580645\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 232.92it/s]\n",
            "load_f1 = 0.9173138419369575\n",
            "real_f1 = 0.9193621867881548\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 244.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cae685d-2d5d-4729-c69c-7603ae260668"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5615615844726562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3448275862068965, f1=0.375, best_f1=0.375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5297736525535583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.43902439024390244, f1=0.43902439024390244, best_f1=0.43902439024390244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5013142228126526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5294117647058824, f1=0.43750000000000006, best_f1=0.43750000000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29447659850120544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5806451612903226, f1=0.4615384615384615, best_f1=0.4615384615384615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2740180492401123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7142857142857143, f1=0.4827586206896552, best_f1=0.4827586206896552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4226515293121338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7200000000000001, f1=0.5833333333333334, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3123817443847656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6285714285714286, f1=0.4285714285714286, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17127671837806702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7407407407407408, f1=0.5833333333333334, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00497622461989522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7407407407407408, f1=0.5384615384615384, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06995583325624466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7142857142857143, f1=0.6153846153846153, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04421799257397652\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6666666666666666, f1=0.6000000000000001, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01564813405275345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6666666666666667, f1=0.5294117647058824, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01747032441198826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6666666666666667, f1=0.5625000000000001, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025611910969018936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6875000000000001, f1=0.5625000000000001, best_f1=0.5833333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018707094714045525\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6875000000000001, f1=0.5625000000000001, best_f1=0.5833333333333334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 123123.12it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.689655172413793\n",
            "real_f1 = 0.689655172413793\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 241.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302ec91a-cd8d-4d4c-c02f-970aa7d296ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5912836194038391\n",
            "step: 10, loss: 0.6436368823051453\n",
            "step: 20, loss: 0.4494129717350006\n",
            "step: 30, loss: 0.25450095534324646\n",
            "step: 40, loss: 0.2803192138671875\n",
            "step: 50, loss: 0.05536786839365959\n",
            "step: 60, loss: 0.07025075703859329\n",
            "step: 70, loss: 0.35598334670066833\n",
            "step: 80, loss: 0.09885349124670029\n",
            "step: 90, loss: 0.14349022507667542\n",
            "step: 100, loss: 0.042209792882204056\n",
            "step: 110, loss: 0.1279958337545395\n",
            "step: 120, loss: 0.013580985367298126\n",
            "step: 130, loss: 0.008160744793713093\n",
            "step: 140, loss: 0.013018233701586723\n",
            "step: 150, loss: 0.03971901535987854\n",
            "step: 160, loss: 0.011939644813537598\n",
            "step: 170, loss: 0.045513588935136795\n",
            "step: 180, loss: 0.1468939483165741\n",
            "step: 190, loss: 0.0329338014125824\n",
            "step: 200, loss: 0.03864526003599167\n",
            "step: 210, loss: 0.05485939979553223\n",
            "step: 220, loss: 0.01706445962190628\n",
            "step: 230, loss: 0.04030580446124077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9808342728297633, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009492904879152775\n",
            "step: 10, loss: 0.010450083762407303\n",
            "step: 20, loss: 0.1378997266292572\n",
            "step: 30, loss: 0.18254120647907257\n",
            "step: 40, loss: 0.016633134335279465\n",
            "step: 50, loss: 0.014288675040006638\n",
            "step: 60, loss: 0.0026654002722352743\n",
            "step: 70, loss: 0.18263214826583862\n",
            "step: 80, loss: 0.0030327499844133854\n",
            "step: 90, loss: 0.025571485981345177\n",
            "step: 100, loss: 0.01426483504474163\n",
            "step: 110, loss: 0.03190067037940025\n",
            "step: 120, loss: 0.027163799852132797\n",
            "step: 130, loss: 0.01883687824010849\n",
            "step: 140, loss: 0.009128754958510399\n",
            "step: 150, loss: 0.051172301173210144\n",
            "step: 160, loss: 0.004097449127584696\n",
            "step: 170, loss: 0.003567297011613846\n",
            "step: 180, loss: 0.0013276553945615888\n",
            "step: 190, loss: 0.04449012875556946\n",
            "step: 200, loss: 0.0019929339177906513\n",
            "step: 210, loss: 0.005313188303261995\n",
            "step: 220, loss: 0.09516651928424835\n",
            "step: 230, loss: 0.02029440551996231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9765886287625419, f1=0.974472807991121, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004059543367475271\n",
            "step: 10, loss: 0.031813278794288635\n",
            "step: 20, loss: 0.001109903329052031\n",
            "step: 30, loss: 0.06427969038486481\n",
            "step: 40, loss: 0.2138177454471588\n",
            "step: 50, loss: 0.1278141736984253\n",
            "step: 60, loss: 0.12090211361646652\n",
            "step: 70, loss: 0.012329182587563992\n",
            "step: 80, loss: 0.029261935502290726\n",
            "step: 90, loss: 0.022010745480656624\n",
            "step: 100, loss: 0.014927313663065434\n",
            "step: 110, loss: 0.0006138274911791086\n",
            "step: 120, loss: 0.020764341577887535\n",
            "step: 130, loss: 0.0007095561595633626\n",
            "step: 140, loss: 0.01250176876783371\n",
            "step: 150, loss: 0.001024774624966085\n",
            "step: 160, loss: 0.0058945827186107635\n",
            "step: 170, loss: 0.013108275830745697\n",
            "step: 180, loss: 0.018358562141656876\n",
            "step: 190, loss: 0.05605647340416908\n",
            "step: 200, loss: 0.03110646829009056\n",
            "step: 210, loss: 0.0009866626933217049\n",
            "step: 220, loss: 0.0007571495370939374\n",
            "step: 230, loss: 0.0007868582033552229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9843400447427293, f1=0.9690265486725664, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007826826767995954\n",
            "step: 10, loss: 0.00038824439980089664\n",
            "step: 20, loss: 0.0035975913051515818\n",
            "step: 30, loss: 0.000494031875859946\n",
            "step: 40, loss: 0.0012181713245809078\n",
            "step: 50, loss: 0.0012680926593020558\n",
            "step: 60, loss: 0.001146849012002349\n",
            "step: 70, loss: 0.005112092941999435\n",
            "step: 80, loss: 0.000657114724162966\n",
            "step: 90, loss: 0.0064938426949083805\n",
            "step: 100, loss: 0.0012669179122895002\n",
            "step: 110, loss: 0.0071785179898142815\n",
            "step: 120, loss: 0.018007125705480576\n",
            "step: 130, loss: 0.001614624634385109\n",
            "step: 140, loss: 0.00022864491620566696\n",
            "step: 150, loss: 0.29455864429473877\n",
            "step: 160, loss: 0.0007761695887893438\n",
            "step: 170, loss: 0.06484705954790115\n",
            "step: 180, loss: 0.000733162509277463\n",
            "step: 190, loss: 0.019812917336821556\n",
            "step: 200, loss: 0.0009241694933734834\n",
            "step: 210, loss: 0.02187633514404297\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 220, loss: 0.0004444965743459761\n",
            "step: 230, loss: 0.003322875825688243\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9730337078651685, f1=0.9765886287625419, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006526106153614819\n",
            "step: 10, loss: 0.0007505778921768069\n",
            "step: 20, loss: 0.002874403027817607\n",
            "step: 30, loss: 0.002404535422101617\n",
            "step: 40, loss: 0.0029779451433569193\n",
            "step: 50, loss: 0.0004954394535161555\n",
            "step: 60, loss: 0.14960138499736786\n",
            "step: 70, loss: 0.0005184091860428452\n",
            "step: 80, loss: 0.0004447514656931162\n",
            "step: 90, loss: 0.00035687899799086154\n",
            "step: 100, loss: 0.0007942302036099136\n",
            "step: 110, loss: 0.0002719898766372353\n",
            "step: 120, loss: 0.0019382343161851168\n",
            "step: 130, loss: 0.00478206854313612\n",
            "step: 140, loss: 0.008591129444539547\n",
            "step: 150, loss: 0.026790466159582138\n",
            "step: 160, loss: 0.0005842240061610937\n",
            "step: 170, loss: 0.00230504316277802\n",
            "step: 180, loss: 0.0022164650727063417\n",
            "step: 190, loss: 0.12431565672159195\n",
            "step: 200, loss: 0.002274681581184268\n",
            "step: 210, loss: 0.0013904864899814129\n",
            "step: 220, loss: 0.0008734457660466433\n",
            "step: 230, loss: 0.0007714052917435765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9819819819819819, f1=0.9853768278965129, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001113216276280582\n",
            "step: 10, loss: 0.001316238776780665\n",
            "step: 20, loss: 0.00048249319661408663\n",
            "step: 30, loss: 0.0018970059463754296\n",
            "step: 40, loss: 0.0020582550205290318\n",
            "step: 50, loss: 0.0010689400369301438\n",
            "step: 60, loss: 0.0002892140473704785\n",
            "step: 70, loss: 0.02720535919070244\n",
            "step: 80, loss: 0.0025691490154713392\n",
            "step: 90, loss: 0.011655852198600769\n",
            "step: 100, loss: 0.0015656236791983247\n",
            "step: 110, loss: 0.00026328343665227294\n",
            "step: 120, loss: 0.0003592222055885941\n",
            "step: 130, loss: 0.0003426947514526546\n",
            "step: 140, loss: 0.00010641995322657749\n",
            "step: 150, loss: 0.010855337604880333\n",
            "step: 160, loss: 0.0424010343849659\n",
            "step: 170, loss: 0.010654699988663197\n",
            "step: 180, loss: 0.002277534455060959\n",
            "step: 190, loss: 0.03642309084534645\n",
            "step: 200, loss: 0.00019589520525187254\n",
            "step: 210, loss: 0.006579171866178513\n",
            "step: 220, loss: 0.00024836030206643045\n",
            "step: 230, loss: 0.007187440991401672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.984304932735426, f1=0.9810479375696767, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001482901512645185\n",
            "step: 10, loss: 0.00014107360038906336\n",
            "step: 20, loss: 0.0016257783863693476\n",
            "step: 30, loss: 0.00011860084487125278\n",
            "step: 40, loss: 0.0002066227898467332\n",
            "step: 50, loss: 0.00014287805242929608\n",
            "step: 60, loss: 0.0012920330045744777\n",
            "step: 70, loss: 0.00019354936375748366\n",
            "step: 80, loss: 0.000285964400973171\n",
            "step: 90, loss: 0.00023740832693874836\n",
            "step: 100, loss: 0.0001783588668331504\n",
            "step: 110, loss: 0.0037732466589659452\n",
            "step: 120, loss: 6.422910519177094e-05\n",
            "step: 130, loss: 0.0013329032808542252\n",
            "step: 140, loss: 0.00018848368199542165\n",
            "step: 150, loss: 0.0017741440096870065\n",
            "step: 160, loss: 0.01902649737894535\n",
            "step: 170, loss: 0.001775332959368825\n",
            "step: 180, loss: 0.0006655720062553883\n",
            "step: 190, loss: 0.015081035904586315\n",
            "step: 200, loss: 0.017793871462345123\n",
            "step: 210, loss: 0.00016178310033865273\n",
            "step: 220, loss: 0.0008959562983363867\n",
            "step: 230, loss: 0.0003360004920978099\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9819004524886877, f1=0.9796839729119639, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014973821816965938\n",
            "step: 10, loss: 0.004486340098083019\n",
            "step: 20, loss: 0.00021602939523290843\n",
            "step: 30, loss: 0.0002866130671463907\n",
            "step: 40, loss: 0.0005872789770364761\n",
            "step: 50, loss: 0.0004134218906983733\n",
            "step: 60, loss: 0.0003740238316822797\n",
            "step: 70, loss: 0.00045882558333687484\n",
            "step: 80, loss: 0.00028476782608777285\n",
            "step: 90, loss: 0.0019301505526527762\n",
            "step: 100, loss: 0.0005723671056330204\n",
            "step: 110, loss: 0.00039842340629547834\n",
            "step: 120, loss: 0.1413656771183014\n",
            "step: 130, loss: 0.00013586375280283391\n",
            "step: 140, loss: 0.0003238037752453238\n",
            "step: 150, loss: 0.00022672228806186467\n",
            "step: 160, loss: 0.0001800040336092934\n",
            "step: 170, loss: 0.00015165418153628707\n",
            "step: 180, loss: 0.00011064508726121858\n",
            "step: 190, loss: 0.02164008840918541\n",
            "step: 200, loss: 6.667581328656524e-05\n",
            "step: 210, loss: 0.005621002987027168\n",
            "step: 220, loss: 0.00040964363142848015\n",
            "step: 230, loss: 9.638178744353354e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9808342728297633, f1=0.9842696629213483, best_f1=0.9690265486725664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.468300075037405e-05\n",
            "step: 10, loss: 0.0017529526958242059\n",
            "step: 20, loss: 0.003282067831605673\n",
            "step: 30, loss: 0.00011082662240369245\n",
            "step: 40, loss: 0.010249382816255093\n",
            "step: 50, loss: 5.8719080698210746e-05\n",
            "step: 60, loss: 0.0064128851518034935\n",
            "step: 70, loss: 0.00020785628294106573\n",
            "step: 80, loss: 0.00015208873082883656\n",
            "step: 90, loss: 0.0001736625999910757\n",
            "step: 100, loss: 0.0002860598615370691\n",
            "step: 110, loss: 0.0012767110019922256\n",
            "step: 120, loss: 4.792617983184755e-05\n",
            "step: 130, loss: 4.374338095658459e-05\n",
            "step: 140, loss: 0.00015597573656123132\n",
            "step: 150, loss: 0.0010692719370126724\n",
            "step: 160, loss: 6.354537617880851e-05\n",
            "step: 170, loss: 0.0002506821183487773\n",
            "step: 180, loss: 0.0001390523830195889\n",
            "step: 190, loss: 0.00016340425645466894\n",
            "step: 200, loss: 0.00017770972044672817\n",
            "step: 210, loss: 0.07228420674800873\n",
            "step: 220, loss: 0.0025755565147846937\n",
            "step: 230, loss: 0.00098368467297405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9864559819413092, f1=0.9831649831649831, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001715741236694157\n",
            "step: 10, loss: 0.0011923820711672306\n",
            "step: 20, loss: 0.0002296712773386389\n",
            "step: 30, loss: 0.0002112572838086635\n",
            "step: 40, loss: 0.011805467307567596\n",
            "step: 50, loss: 0.0010750576620921493\n",
            "step: 60, loss: 0.013978522270917892\n",
            "step: 70, loss: 0.00028450990794226527\n",
            "step: 80, loss: 0.00013330973160918802\n",
            "step: 90, loss: 0.00015111682296264917\n",
            "step: 100, loss: 0.0015833306824788451\n",
            "step: 110, loss: 8.735875599086285e-05\n",
            "step: 120, loss: 0.00015217128384392709\n",
            "step: 130, loss: 8.345385867869481e-05\n",
            "step: 140, loss: 0.00016340722504537553\n",
            "step: 150, loss: 0.0019067659741267562\n",
            "step: 160, loss: 6.118199235061184e-05\n",
            "step: 170, loss: 3.552684574970044e-05\n",
            "step: 180, loss: 0.00043602974619716406\n",
            "step: 190, loss: 0.0020916673820465803\n",
            "step: 200, loss: 5.2157931349938735e-05\n",
            "step: 210, loss: 4.278585038264282e-05\n",
            "step: 220, loss: 8.111945498967543e-05\n",
            "step: 230, loss: 0.00018982998153660446\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9807909604519773, f1=0.9865168539325843, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005098332185298204\n",
            "step: 10, loss: 6.167472747620195e-05\n",
            "step: 20, loss: 9.891140507534146e-05\n",
            "step: 30, loss: 9.65890649240464e-05\n",
            "step: 40, loss: 0.0009112178231589496\n",
            "step: 50, loss: 8.384117245441303e-05\n",
            "step: 60, loss: 0.00017895438941195607\n",
            "step: 70, loss: 6.67557178530842e-05\n",
            "step: 80, loss: 0.04097673296928406\n",
            "step: 90, loss: 0.028731804341077805\n",
            "step: 100, loss: 5.966609751340002e-05\n",
            "step: 110, loss: 9.232599404640496e-05\n",
            "step: 120, loss: 0.00010250679770251736\n",
            "step: 130, loss: 0.0002869335003197193\n",
            "step: 140, loss: 0.00010223184654023498\n",
            "step: 150, loss: 0.005001886282116175\n",
            "step: 160, loss: 8.75632103998214e-05\n",
            "step: 170, loss: 0.0007410160033032298\n",
            "step: 180, loss: 0.0012955247657373548\n",
            "step: 190, loss: 2.8386077246977948e-05\n",
            "step: 200, loss: 0.0001418653700966388\n",
            "step: 210, loss: 0.00018661361536942422\n",
            "step: 220, loss: 4.070425347890705e-05\n",
            "step: 230, loss: 0.00015194251318462193\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9820627802690582, f1=0.984304932735426, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.379961683298461e-05\n",
            "step: 10, loss: 6.120956095401198e-05\n",
            "step: 20, loss: 4.8670644900994375e-05\n",
            "step: 30, loss: 0.00031529649277217686\n",
            "step: 40, loss: 5.863955448148772e-05\n",
            "step: 50, loss: 0.0001070927974069491\n",
            "step: 60, loss: 7.85038573667407e-05\n",
            "step: 70, loss: 5.207861977396533e-05\n",
            "step: 80, loss: 4.676470052800141e-05\n",
            "step: 90, loss: 6.359694816637784e-05\n",
            "step: 100, loss: 9.192447760142386e-05\n",
            "step: 110, loss: 0.00019542794325388968\n",
            "step: 120, loss: 7.904202357167378e-05\n",
            "step: 130, loss: 0.0002962719008792192\n",
            "step: 140, loss: 0.00012872045044787228\n",
            "step: 150, loss: 6.77174684824422e-05\n",
            "step: 160, loss: 6.470434891525656e-05\n",
            "step: 170, loss: 0.00011033289774786681\n",
            "step: 180, loss: 0.00019550370052456856\n",
            "step: 190, loss: 0.0006971173570491374\n",
            "step: 200, loss: 0.0003019634750671685\n",
            "step: 210, loss: 0.0006922603934071958\n",
            "step: 220, loss: 0.000360679579898715\n",
            "step: 230, loss: 0.04525629058480263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9808342728297633, f1=0.984304932735426, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007590592140331864\n",
            "step: 10, loss: 0.000248692580498755\n",
            "step: 20, loss: 0.0002583207096904516\n",
            "step: 30, loss: 0.00015497690765187144\n",
            "step: 40, loss: 7.97479588072747e-05\n",
            "step: 50, loss: 4.1349085222464055e-05\n",
            "step: 60, loss: 4.684122541220859e-05\n",
            "step: 70, loss: 5.802487430628389e-05\n",
            "step: 80, loss: 0.00010451512935105711\n",
            "step: 90, loss: 0.000550613971427083\n",
            "step: 100, loss: 6.178844341775402e-05\n",
            "step: 110, loss: 0.0016755074029788375\n",
            "step: 120, loss: 9.320996468886733e-05\n",
            "step: 130, loss: 7.221900159493089e-05\n",
            "step: 140, loss: 7.3314702603966e-05\n",
            "step: 150, loss: 6.391266651917249e-05\n",
            "step: 160, loss: 0.00010907967953244224\n",
            "step: 170, loss: 9.250863513443619e-05\n",
            "step: 180, loss: 6.576562736881897e-05\n",
            "step: 190, loss: 6.40438447589986e-05\n",
            "step: 200, loss: 0.00010067258699564263\n",
            "step: 210, loss: 2.39563323702896e-05\n",
            "step: 220, loss: 0.0015348652377724648\n",
            "step: 230, loss: 4.6673481847392395e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9808342728297633, f1=0.9854096520763187, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.142279016785324e-05\n",
            "step: 10, loss: 0.0005020553362555802\n",
            "step: 20, loss: 0.0002667384978849441\n",
            "step: 30, loss: 0.0008615604601800442\n",
            "step: 40, loss: 0.0007373114349320531\n",
            "step: 50, loss: 7.327970524784178e-05\n",
            "step: 60, loss: 0.00019137798517476767\n",
            "step: 70, loss: 0.0003075944259762764\n",
            "step: 80, loss: 4.783759868587367e-05\n",
            "step: 90, loss: 0.00013629972818307579\n",
            "step: 100, loss: 5.8208424889016896e-05\n",
            "step: 110, loss: 4.677263132180087e-05\n",
            "step: 120, loss: 4.873882426181808e-05\n",
            "step: 130, loss: 6.244253017939627e-05\n",
            "step: 140, loss: 9.646185208112001e-05\n",
            "step: 150, loss: 5.4032858315622434e-05\n",
            "step: 160, loss: 6.380406557582319e-05\n",
            "step: 170, loss: 2.639295780682005e-05\n",
            "step: 180, loss: 6.232778105186298e-05\n",
            "step: 190, loss: 5.717920794268139e-05\n",
            "step: 200, loss: 3.584245132515207e-05\n",
            "step: 210, loss: 6.422327714972198e-05\n",
            "step: 220, loss: 0.00010595342610031366\n",
            "step: 230, loss: 0.00018721884407568723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9807909604519773, f1=0.984304932735426, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.985803677001968e-05\n",
            "step: 10, loss: 2.6124460418941453e-05\n",
            "step: 20, loss: 7.736351108178496e-05\n",
            "step: 30, loss: 4.088234345545061e-05\n",
            "step: 40, loss: 8.077503298409283e-05\n",
            "step: 50, loss: 0.00023775709269102663\n",
            "step: 60, loss: 0.00010171208850806579\n",
            "step: 70, loss: 5.038761446485296e-05\n",
            "step: 80, loss: 8.487808372592553e-05\n",
            "step: 90, loss: 6.576386658707634e-05\n",
            "step: 100, loss: 3.665166877908632e-05\n",
            "step: 110, loss: 3.3715648896759376e-05\n",
            "step: 120, loss: 6.384847074514255e-05\n",
            "step: 130, loss: 6.718731310684234e-05\n",
            "step: 140, loss: 3.962125992984511e-05\n",
            "step: 150, loss: 0.0024699154309928417\n",
            "step: 160, loss: 0.002059583319351077\n",
            "step: 170, loss: 3.5708097129827365e-05\n",
            "step: 180, loss: 0.00034649495501071215\n",
            "step: 190, loss: 5.1423266995698214e-05\n",
            "step: 200, loss: 0.00012106999929528683\n",
            "step: 210, loss: 6.606968236155808e-05\n",
            "step: 220, loss: 0.002273876452818513\n",
            "step: 230, loss: 5.732826321036555e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9808342728297633, f1=0.984304932735426, best_f1=0.9831649831649831\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 196.06it/s]\n",
            "load_f1 = 0.9832402234636871\n",
            "real_f1 = 0.983277591973244\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 230.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b28986b-fe58-412f-b32c-782bfb44f7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6432169675827026\n",
            "step: 10, loss: 0.5794656276702881\n",
            "step: 20, loss: 0.5686971545219421\n",
            "step: 30, loss: 0.2894785702228546\n",
            "step: 40, loss: 0.26723000407218933\n",
            "step: 50, loss: 0.39409443736076355\n",
            "step: 60, loss: 0.07866831123828888\n",
            "step: 70, loss: 0.16858913004398346\n",
            "step: 80, loss: 0.06388065963983536\n",
            "step: 90, loss: 0.22042101621627808\n",
            "step: 100, loss: 0.07504530251026154\n",
            "step: 110, loss: 0.11860810220241547\n",
            "step: 120, loss: 0.07462351769208908\n",
            "step: 130, loss: 0.053010813891887665\n",
            "step: 140, loss: 0.09560681134462357\n",
            "step: 150, loss: 0.12046237289905548\n",
            "step: 160, loss: 0.05177341774106026\n",
            "step: 170, loss: 0.24975648522377014\n",
            "step: 180, loss: 0.08558251708745956\n",
            "step: 190, loss: 0.01805344596505165\n",
            "step: 200, loss: 0.1721978783607483\n",
            "step: 210, loss: 0.025900324806571007\n",
            "step: 220, loss: 0.17499904334545135\n",
            "step: 230, loss: 0.17509828507900238\n",
            "step: 240, loss: 0.05129426717758179\n",
            "step: 250, loss: 0.07659067213535309\n",
            "step: 260, loss: 0.15420085191726685\n",
            "step: 270, loss: 0.017204510048031807\n",
            "step: 280, loss: 0.04848505184054375\n",
            "step: 290, loss: 0.3285023272037506\n",
            "step: 300, loss: 0.11035525798797607\n",
            "step: 310, loss: 0.19164392352104187\n",
            "step: 320, loss: 0.07271929830312729\n",
            "step: 330, loss: 0.0873781219124794\n",
            "step: 340, loss: 0.1564044952392578\n",
            "step: 350, loss: 0.019945407286286354\n",
            "step: 360, loss: 0.0987618938088417\n",
            "step: 370, loss: 0.07613836973905563\n",
            "step: 380, loss: 0.03377092257142067\n",
            "step: 390, loss: 0.25155937671661377\n",
            "step: 400, loss: 0.2865198254585266\n",
            "step: 410, loss: 0.05861658602952957\n",
            "step: 420, loss: 0.10153212398290634\n",
            "step: 430, loss: 0.1574670374393463\n",
            "step: 440, loss: 0.028653167188167572\n",
            "step: 450, loss: 0.03249174728989601\n",
            "step: 460, loss: 0.10807108134031296\n",
            "step: 470, loss: 0.10928919911384583\n",
            "step: 480, loss: 0.10203718394041061\n",
            "step: 490, loss: 0.14547939598560333\n",
            "step: 500, loss: 0.10380413383245468\n",
            "step: 510, loss: 0.02645495906472206\n",
            "step: 520, loss: 0.039297446608543396\n",
            "step: 530, loss: 0.00389574165455997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.908921933085502, f1=0.9076492537313433, best_f1=0.9076492537313433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12372715026140213\n",
            "step: 10, loss: 0.06790507584810257\n",
            "step: 20, loss: 0.08405815064907074\n",
            "step: 30, loss: 0.03098459541797638\n",
            "step: 40, loss: 0.08674251288175583\n",
            "step: 50, loss: 0.14887656271457672\n",
            "step: 60, loss: 0.02997262217104435\n",
            "step: 70, loss: 0.08427724242210388\n",
            "step: 80, loss: 0.03831005468964577\n",
            "step: 90, loss: 0.021467776969075203\n",
            "step: 100, loss: 0.05835474655032158\n",
            "step: 110, loss: 0.14676812291145325\n",
            "step: 120, loss: 0.0968737080693245\n",
            "step: 130, loss: 0.09822472184896469\n",
            "step: 140, loss: 0.0504692979156971\n",
            "step: 150, loss: 0.0873098075389862\n",
            "step: 160, loss: 0.021702472120523453\n",
            "step: 170, loss: 0.12617754936218262\n",
            "step: 180, loss: 0.04307136312127113\n",
            "step: 190, loss: 0.061948660761117935\n",
            "step: 200, loss: 0.0300495233386755\n",
            "step: 210, loss: 0.06996268033981323\n",
            "step: 220, loss: 0.13182173669338226\n",
            "step: 230, loss: 0.016093363985419273\n",
            "step: 240, loss: 0.10938452184200287\n",
            "step: 250, loss: 0.09020396322011948\n",
            "step: 260, loss: 0.011481172405183315\n",
            "step: 270, loss: 0.16227972507476807\n",
            "step: 280, loss: 0.02999839559197426\n",
            "step: 290, loss: 0.0578247494995594\n",
            "step: 300, loss: 0.17288850247859955\n",
            "step: 310, loss: 0.010233774781227112\n",
            "step: 320, loss: 0.10799362510442734\n",
            "step: 330, loss: 0.11553701758384705\n",
            "step: 340, loss: 0.022246431559324265\n",
            "step: 350, loss: 0.0027778707444667816\n",
            "step: 360, loss: 0.0849435105919838\n",
            "step: 370, loss: 0.10982630401849747\n",
            "step: 380, loss: 0.07202284038066864\n",
            "step: 390, loss: 0.08721145987510681\n",
            "step: 400, loss: 0.06855747848749161\n",
            "step: 410, loss: 0.07379887253046036\n",
            "step: 420, loss: 0.04697937145829201\n",
            "step: 430, loss: 0.020284224301576614\n",
            "step: 440, loss: 0.017918352037668228\n",
            "step: 450, loss: 0.044734615832567215\n",
            "step: 460, loss: 0.052820056676864624\n",
            "step: 470, loss: 0.08989686518907547\n",
            "step: 480, loss: 0.1787126064300537\n",
            "step: 490, loss: 0.014756029471755028\n",
            "step: 500, loss: 0.25569868087768555\n",
            "step: 510, loss: 0.021564403548836708\n",
            "step: 520, loss: 0.10473179817199707\n",
            "step: 530, loss: 0.08317268639802933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.917397323488694, f1=0.9124594719777674, best_f1=0.9124594719777674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21491973102092743\n",
            "step: 10, loss: 0.08811965584754944\n",
            "step: 20, loss: 0.11296552419662476\n",
            "step: 30, loss: 0.3435492515563965\n",
            "step: 40, loss: 0.03693331405520439\n",
            "step: 50, loss: 0.030944766476750374\n",
            "step: 60, loss: 0.13097211718559265\n",
            "step: 70, loss: 0.03636755794286728\n",
            "step: 80, loss: 0.011376826092600822\n",
            "step: 90, loss: 0.010365281254053116\n",
            "step: 100, loss: 0.07347522675991058\n",
            "step: 110, loss: 0.04070301353931427\n",
            "step: 120, loss: 0.011416873894631863\n",
            "step: 130, loss: 0.0106012849137187\n",
            "step: 140, loss: 0.007945485413074493\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.03563738986849785\n",
            "step: 160, loss: 0.009678767062723637\n",
            "step: 170, loss: 0.13403894007205963\n",
            "step: 180, loss: 0.029295608401298523\n",
            "step: 190, loss: 0.02699006162583828\n",
            "step: 200, loss: 0.03233785927295685\n",
            "step: 210, loss: 0.03896472603082657\n",
            "step: 220, loss: 0.06872522830963135\n",
            "step: 230, loss: 0.061165954917669296\n",
            "step: 240, loss: 0.002930302172899246\n",
            "step: 250, loss: 0.014165237545967102\n",
            "step: 260, loss: 0.04662686213850975\n",
            "step: 270, loss: 0.020530162379145622\n",
            "step: 280, loss: 0.1493040919303894\n",
            "step: 290, loss: 0.006884922739118338\n",
            "step: 300, loss: 0.04535877704620361\n",
            "step: 310, loss: 0.007923120632767677\n",
            "step: 320, loss: 0.04219873994588852\n",
            "step: 330, loss: 0.007602186873555183\n",
            "step: 340, loss: 0.047734957188367844\n",
            "step: 350, loss: 0.010542590171098709\n",
            "step: 360, loss: 0.047138385474681854\n",
            "step: 370, loss: 0.03142872825264931\n",
            "step: 380, loss: 0.03950284421443939\n",
            "step: 390, loss: 0.08419828861951828\n",
            "step: 400, loss: 0.015115098096430302\n",
            "step: 410, loss: 0.004173108842223883\n",
            "step: 420, loss: 0.3515644669532776\n",
            "step: 430, loss: 0.027394477277994156\n",
            "step: 440, loss: 0.035865526646375656\n",
            "step: 450, loss: 0.06232418864965439\n",
            "step: 460, loss: 0.02362065576016903\n",
            "step: 470, loss: 0.06603717803955078\n",
            "step: 480, loss: 0.00512697221711278\n",
            "step: 490, loss: 0.023833785206079483\n",
            "step: 500, loss: 0.004552891943603754\n",
            "step: 510, loss: 0.0071605509147048\n",
            "step: 520, loss: 0.054668501019477844\n",
            "step: 530, loss: 0.1638786345720291\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9142857142857143, f1=0.915068493150685, best_f1=0.9124594719777674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0063229333609342575\n",
            "step: 10, loss: 0.04357580095529556\n",
            "step: 20, loss: 0.004959931131452322\n",
            "step: 30, loss: 0.05596844479441643\n",
            "step: 40, loss: 0.06611236184835434\n",
            "step: 50, loss: 0.03200319781899452\n",
            "step: 60, loss: 0.008341671898961067\n",
            "step: 70, loss: 0.004574760794639587\n",
            "step: 80, loss: 0.005781657062470913\n",
            "step: 90, loss: 0.03557342663407326\n",
            "step: 100, loss: 0.015723608434200287\n",
            "step: 110, loss: 0.1024933010339737\n",
            "step: 120, loss: 0.0016787794884294271\n",
            "step: 130, loss: 0.013273424468934536\n",
            "step: 140, loss: 0.006591208279132843\n",
            "step: 150, loss: 0.02018212154507637\n",
            "step: 160, loss: 0.004481967072933912\n",
            "step: 170, loss: 0.010077516548335552\n",
            "step: 180, loss: 0.023227954283356667\n",
            "step: 190, loss: 0.041533760726451874\n",
            "step: 200, loss: 0.00250938069075346\n",
            "step: 210, loss: 0.15837661921977997\n",
            "step: 220, loss: 0.020147236064076424\n",
            "step: 230, loss: 0.10637642443180084\n",
            "step: 240, loss: 0.02436026930809021\n",
            "step: 250, loss: 0.17344465851783752\n",
            "step: 260, loss: 0.0044840131886303425\n",
            "step: 270, loss: 0.03124120645225048\n",
            "step: 280, loss: 0.16565090417861938\n",
            "step: 290, loss: 0.017541727051138878\n",
            "step: 300, loss: 0.0051193260587751865\n",
            "step: 310, loss: 0.00711792754009366\n",
            "step: 320, loss: 0.014753562398254871\n",
            "step: 330, loss: 0.012768425047397614\n",
            "step: 340, loss: 0.0041920519433915615\n",
            "step: 350, loss: 0.015385298058390617\n",
            "step: 360, loss: 0.006404640153050423\n",
            "step: 370, loss: 0.0025959163904190063\n",
            "step: 380, loss: 0.00903854239732027\n",
            "step: 390, loss: 0.00356141640804708\n",
            "step: 400, loss: 0.0007490938296541572\n",
            "step: 410, loss: 0.036410290747880936\n",
            "step: 420, loss: 0.003811038099229336\n",
            "step: 430, loss: 0.0664651170372963\n",
            "step: 440, loss: 0.00946042500436306\n",
            "step: 450, loss: 0.006267940159887075\n",
            "step: 460, loss: 0.08807282149791718\n",
            "step: 470, loss: 0.003630895633250475\n",
            "step: 480, loss: 0.043030012398958206\n",
            "step: 490, loss: 0.014034615829586983\n",
            "step: 500, loss: 0.006909334100782871\n",
            "step: 510, loss: 0.012908844277262688\n",
            "step: 520, loss: 0.09783407300710678\n",
            "step: 530, loss: 0.03270401060581207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9193400549954172, f1=0.9178966789667896, best_f1=0.9178966789667896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05320234224200249\n",
            "step: 10, loss: 0.03313231095671654\n",
            "step: 20, loss: 0.005363529082387686\n",
            "step: 30, loss: 0.001266652368940413\n",
            "step: 40, loss: 0.010541656985878944\n",
            "step: 50, loss: 0.02974873036146164\n",
            "step: 60, loss: 0.029580647125840187\n",
            "step: 70, loss: 0.04263077303767204\n",
            "step: 80, loss: 0.0006083748885430396\n",
            "step: 90, loss: 0.03048071265220642\n",
            "step: 100, loss: 0.17725932598114014\n",
            "step: 110, loss: 0.01825079694390297\n",
            "step: 120, loss: 0.00465376628562808\n",
            "step: 130, loss: 0.0007920297794044018\n",
            "step: 140, loss: 0.0256983432918787\n",
            "step: 150, loss: 0.01082962192595005\n",
            "step: 160, loss: 0.002842097310349345\n",
            "step: 170, loss: 0.010495824739336967\n",
            "step: 180, loss: 0.003239026525989175\n",
            "step: 190, loss: 0.011058175936341286\n",
            "step: 200, loss: 0.011724643409252167\n",
            "step: 210, loss: 0.06432536989450455\n",
            "step: 220, loss: 0.0016907139215618372\n",
            "step: 230, loss: 0.005815912503749132\n",
            "step: 240, loss: 0.26368096470832825\n",
            "step: 250, loss: 0.0020668315701186657\n",
            "step: 260, loss: 0.001958453329280019\n",
            "step: 270, loss: 0.0010813859989866614\n",
            "step: 280, loss: 0.017363782972097397\n",
            "step: 290, loss: 0.11472629010677338\n",
            "step: 300, loss: 0.03555500507354736\n",
            "step: 310, loss: 0.0016295072855427861\n",
            "step: 320, loss: 0.021979423239827156\n",
            "step: 330, loss: 0.10333732515573502\n",
            "step: 340, loss: 0.0030905436724424362\n",
            "step: 350, loss: 0.006412309594452381\n",
            "step: 360, loss: 0.02246265672147274\n",
            "step: 370, loss: 0.055154308676719666\n",
            "step: 380, loss: 0.0017654376570135355\n",
            "step: 390, loss: 0.00031861712341196835\n",
            "step: 400, loss: 0.03653624281287193\n",
            "step: 410, loss: 0.005327066406607628\n",
            "step: 420, loss: 0.0009492857498116791\n",
            "step: 430, loss: 0.004287893418222666\n",
            "step: 440, loss: 0.057711198925971985\n",
            "step: 450, loss: 0.007742025889456272\n",
            "step: 460, loss: 0.03097725659608841\n",
            "step: 470, loss: 0.008831956423819065\n",
            "step: 480, loss: 0.009001576341688633\n",
            "step: 490, loss: 0.0008534133667126298\n",
            "step: 500, loss: 0.13545680046081543\n",
            "step: 510, loss: 0.04456082731485367\n",
            "step: 520, loss: 0.016704432666301727\n",
            "step: 530, loss: 0.04602591693401337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.914365933551708, f1=0.9121495327102803, best_f1=0.9178966789667896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12769797444343567\n",
            "step: 10, loss: 0.003997258376330137\n",
            "step: 20, loss: 0.12135347723960876\n",
            "step: 30, loss: 0.008720848709344864\n",
            "step: 40, loss: 0.004984455183148384\n",
            "step: 50, loss: 0.003978007007390261\n",
            "step: 60, loss: 0.00034319827682338655\n",
            "step: 70, loss: 0.002659222576767206\n",
            "step: 80, loss: 0.03731967881321907\n",
            "step: 90, loss: 0.0011126877507194877\n",
            "step: 100, loss: 0.023373859003186226\n",
            "step: 110, loss: 0.0004954396281391382\n",
            "step: 120, loss: 0.01719234511256218\n",
            "step: 130, loss: 0.0007505913381464779\n",
            "step: 140, loss: 0.02619159407913685\n",
            "step: 150, loss: 0.0016640652902424335\n",
            "step: 160, loss: 0.020040346309542656\n",
            "step: 170, loss: 0.017490878701210022\n",
            "step: 180, loss: 0.0021748302970081568\n",
            "step: 190, loss: 0.002928877482190728\n",
            "step: 200, loss: 0.0018086194759234786\n",
            "step: 210, loss: 0.025980006903409958\n",
            "step: 220, loss: 0.0037411621306091547\n",
            "step: 230, loss: 0.004229275975376368\n",
            "step: 240, loss: 0.010301388800144196\n",
            "step: 250, loss: 0.1579292267560959\n",
            "step: 260, loss: 0.0011781086213886738\n",
            "step: 270, loss: 0.05868798866868019\n",
            "step: 280, loss: 0.0035838214680552483\n",
            "step: 290, loss: 0.0019139035139232874\n",
            "step: 300, loss: 0.0033390759490430355\n",
            "step: 310, loss: 0.05265134572982788\n",
            "step: 320, loss: 0.000401978351874277\n",
            "step: 330, loss: 0.0009159045876003802\n",
            "step: 340, loss: 0.1482921987771988\n",
            "step: 350, loss: 0.004065051209181547\n",
            "step: 360, loss: 0.02321203052997589\n",
            "step: 370, loss: 0.03329147398471832\n",
            "step: 380, loss: 0.02397562935948372\n",
            "step: 390, loss: 0.015764113515615463\n",
            "step: 400, loss: 0.03955477848649025\n",
            "step: 410, loss: 0.029203860089182854\n",
            "step: 420, loss: 0.006945027504116297\n",
            "step: 430, loss: 0.0006569690885953605\n",
            "step: 440, loss: 0.0016032017301768064\n",
            "step: 450, loss: 0.005591391585767269\n",
            "step: 460, loss: 0.0004577552026603371\n",
            "step: 470, loss: 0.04750676825642586\n",
            "step: 480, loss: 0.010568506084382534\n",
            "step: 490, loss: 0.003995290491729975\n",
            "step: 500, loss: 0.006236369721591473\n",
            "step: 510, loss: 0.0009215911268256605\n",
            "step: 520, loss: 0.005584217142313719\n",
            "step: 530, loss: 0.008746778592467308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9217230199166282, f1=0.9157070474435745, best_f1=0.9157070474435745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008866103016771376\n",
            "step: 10, loss: 0.03541533276438713\n",
            "step: 20, loss: 0.0010589235462248325\n",
            "step: 30, loss: 0.0009383137221448123\n",
            "step: 40, loss: 0.007941137067973614\n",
            "step: 50, loss: 0.018864214420318604\n",
            "step: 60, loss: 0.008207503706216812\n",
            "step: 70, loss: 0.00019126213737763464\n",
            "step: 80, loss: 0.03145724907517433\n",
            "step: 90, loss: 0.00024961470626294613\n",
            "step: 100, loss: 0.0002034027420450002\n",
            "step: 110, loss: 0.0022424482740461826\n",
            "step: 120, loss: 0.13061772286891937\n",
            "step: 130, loss: 0.004915175028145313\n",
            "step: 140, loss: 0.003931502345949411\n",
            "step: 150, loss: 0.0025763818994164467\n",
            "step: 160, loss: 0.00016469540423713624\n",
            "step: 170, loss: 0.0008645814377814531\n",
            "step: 180, loss: 0.00029091144097037613\n",
            "step: 190, loss: 0.0003357613168191165\n",
            "step: 200, loss: 0.0007675864617340267\n",
            "step: 210, loss: 0.002037388039752841\n",
            "step: 220, loss: 0.0005657821893692017\n",
            "step: 230, loss: 0.005670848768204451\n",
            "step: 240, loss: 0.0036391434259712696\n",
            "step: 250, loss: 0.03226909786462784\n",
            "step: 260, loss: 0.00155393045861274\n",
            "step: 270, loss: 0.02800709940493107\n",
            "step: 280, loss: 0.004200147930532694\n",
            "step: 290, loss: 0.0008345990208908916\n",
            "step: 300, loss: 0.010614514350891113\n",
            "step: 310, loss: 0.03656494617462158\n",
            "step: 320, loss: 0.0006216850015334785\n",
            "step: 330, loss: 0.0003608901461120695\n",
            "step: 340, loss: 0.053733211010694504\n",
            "step: 350, loss: 0.003547732951119542\n",
            "step: 360, loss: 0.0010804266203194857\n",
            "step: 370, loss: 0.0015452009392902255\n",
            "step: 380, loss: 0.003904322860762477\n",
            "step: 390, loss: 0.0015550954267382622\n",
            "step: 400, loss: 0.005224541295319796\n",
            "step: 410, loss: 0.0011256505968049169\n",
            "step: 420, loss: 0.01394051406532526\n",
            "step: 430, loss: 0.010264958254992962\n",
            "step: 440, loss: 0.0011269109090790153\n",
            "step: 450, loss: 0.04625053331255913\n",
            "step: 460, loss: 0.019836213439702988\n",
            "step: 470, loss: 0.07629935443401337\n",
            "step: 480, loss: 0.06309514492750168\n",
            "step: 490, loss: 0.0004997917567379773\n",
            "step: 500, loss: 0.001338051282800734\n",
            "step: 510, loss: 0.0008395395125262439\n",
            "step: 520, loss: 0.04311465099453926\n",
            "step: 530, loss: 0.01219343300908804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.91728624535316, f1=0.9096804075961092, best_f1=0.9157070474435745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021484028548002243\n",
            "step: 10, loss: 0.003282646182924509\n",
            "step: 20, loss: 0.0005291769048199058\n",
            "step: 30, loss: 0.0033440992701798677\n",
            "step: 40, loss: 0.00623582536354661\n",
            "step: 50, loss: 0.007723519578576088\n",
            "step: 60, loss: 0.0003241439117118716\n",
            "step: 70, loss: 0.00017739066970534623\n",
            "step: 80, loss: 0.0005010910099372268\n",
            "step: 90, loss: 0.001334032160229981\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.0014582984149456024\n",
            "step: 110, loss: 0.035171907395124435\n",
            "step: 120, loss: 0.006415925920009613\n",
            "step: 130, loss: 0.0015159603208303452\n",
            "step: 140, loss: 0.01067729014903307\n",
            "step: 150, loss: 0.0004892240976914763\n",
            "step: 160, loss: 0.008320561610162258\n",
            "step: 170, loss: 0.003964596427977085\n",
            "step: 180, loss: 0.03204644098877907\n",
            "step: 190, loss: 0.0032287738285958767\n",
            "step: 200, loss: 0.002640477381646633\n",
            "step: 210, loss: 0.002335937228053808\n",
            "step: 220, loss: 0.00489013222977519\n",
            "step: 230, loss: 0.0014675643760710955\n",
            "step: 240, loss: 0.0006858453270979226\n",
            "step: 250, loss: 0.004329627845436335\n",
            "step: 260, loss: 0.06180361658334732\n",
            "step: 270, loss: 0.0002338849735679105\n",
            "step: 280, loss: 0.014675603248178959\n",
            "step: 290, loss: 0.004660504404455423\n",
            "step: 300, loss: 0.0022376207634806633\n",
            "step: 310, loss: 0.04093267023563385\n",
            "step: 320, loss: 0.0003302262048237026\n",
            "step: 330, loss: 0.000507361488416791\n",
            "step: 340, loss: 0.013088764622807503\n",
            "step: 350, loss: 0.01198047585785389\n",
            "step: 360, loss: 0.002794749103486538\n",
            "step: 370, loss: 0.0006022340385243297\n",
            "step: 380, loss: 0.02472503110766411\n",
            "step: 390, loss: 0.0753994956612587\n",
            "step: 400, loss: 0.06648391485214233\n",
            "step: 410, loss: 0.0020583434961736202\n",
            "step: 420, loss: 0.005995376501232386\n",
            "step: 430, loss: 0.0111875981092453\n",
            "step: 440, loss: 0.0021616367157548666\n",
            "step: 450, loss: 0.0012473904062062502\n",
            "step: 460, loss: 0.026968462392687798\n",
            "step: 470, loss: 0.0007701216964051127\n",
            "step: 480, loss: 0.002123443176969886\n",
            "step: 490, loss: 0.005948388483375311\n",
            "step: 500, loss: 0.005623057950288057\n",
            "step: 510, loss: 0.0021883402951061726\n",
            "step: 520, loss: 0.002976586576551199\n",
            "step: 530, loss: 0.0007060857024043798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9233627496516488, f1=0.9123783031988874, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006570807890966535\n",
            "step: 10, loss: 0.0037700068205595016\n",
            "step: 20, loss: 0.006749854423105717\n",
            "step: 30, loss: 0.001152255805209279\n",
            "step: 40, loss: 0.000977344112470746\n",
            "step: 50, loss: 0.00014748437388334423\n",
            "step: 60, loss: 7.818690937710926e-05\n",
            "step: 70, loss: 0.00039197897422127426\n",
            "step: 80, loss: 0.0064156921580433846\n",
            "step: 90, loss: 0.00014216022100299597\n",
            "step: 100, loss: 0.00012421095743775368\n",
            "step: 110, loss: 0.00026075897039845586\n",
            "step: 120, loss: 0.010750691406428814\n",
            "step: 130, loss: 0.015329508110880852\n",
            "step: 140, loss: 0.010573489591479301\n",
            "step: 150, loss: 0.0009126697550527751\n",
            "step: 160, loss: 0.0005626273923553526\n",
            "step: 170, loss: 0.021081330254673958\n",
            "step: 180, loss: 0.00023237497953232378\n",
            "step: 190, loss: 0.004912355914711952\n",
            "step: 200, loss: 0.0007061215583235025\n",
            "step: 210, loss: 0.0003475769190117717\n",
            "step: 220, loss: 0.01134338229894638\n",
            "step: 230, loss: 0.00026853583403863013\n",
            "step: 240, loss: 0.00945820938795805\n",
            "step: 250, loss: 0.012632842175662518\n",
            "step: 260, loss: 0.011957025155425072\n",
            "step: 270, loss: 9.201082866638899e-05\n",
            "step: 280, loss: 0.00029794833972118795\n",
            "step: 290, loss: 0.011704761534929276\n",
            "step: 300, loss: 0.00013564882101491094\n",
            "step: 310, loss: 0.01381321344524622\n",
            "step: 320, loss: 0.0003214622847735882\n",
            "step: 330, loss: 0.0002718884206842631\n",
            "step: 340, loss: 0.00010680103878257796\n",
            "step: 350, loss: 0.0014255247078835964\n",
            "step: 360, loss: 0.00014812014705967158\n",
            "step: 370, loss: 0.0007048416882753372\n",
            "step: 380, loss: 0.00016902580682653934\n",
            "step: 390, loss: 0.00040158172487281263\n",
            "step: 400, loss: 0.0002844981208909303\n",
            "step: 410, loss: 0.0024590070825070143\n",
            "step: 420, loss: 5.836074342369102e-05\n",
            "step: 430, loss: 0.00017242766625713557\n",
            "step: 440, loss: 0.0036670181434601545\n",
            "step: 450, loss: 0.00022078544134274125\n",
            "step: 460, loss: 0.0007721959846094251\n",
            "step: 470, loss: 5.1276096201036125e-05\n",
            "step: 480, loss: 0.0012391373747959733\n",
            "step: 490, loss: 0.002271660603582859\n",
            "step: 500, loss: 0.003783675841987133\n",
            "step: 510, loss: 0.0016009424580261111\n",
            "step: 520, loss: 0.0004539195797406137\n",
            "step: 530, loss: 0.00012021960719721392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.919815668202765, f1=0.909007773205304, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014526124577969313\n",
            "step: 10, loss: 7.141355308704078e-05\n",
            "step: 20, loss: 0.00012395459634717554\n",
            "step: 30, loss: 5.716820669476874e-05\n",
            "step: 40, loss: 0.014403571374714375\n",
            "step: 50, loss: 0.0001546005514683202\n",
            "step: 60, loss: 0.00016487171524204314\n",
            "step: 70, loss: 0.0001779926533345133\n",
            "step: 80, loss: 8.437834185315296e-05\n",
            "step: 90, loss: 0.0004558057407848537\n",
            "step: 100, loss: 0.00010274788655806333\n",
            "step: 110, loss: 7.834178541088477e-05\n",
            "step: 120, loss: 0.0007394906133413315\n",
            "step: 130, loss: 0.00017049828602466732\n",
            "step: 140, loss: 0.00019995101320091635\n",
            "step: 150, loss: 0.0026101444382220507\n",
            "step: 160, loss: 0.0006443235324695706\n",
            "step: 170, loss: 0.0005669977399520576\n",
            "step: 180, loss: 0.001215497963130474\n",
            "step: 190, loss: 0.00030760240042582154\n",
            "step: 200, loss: 0.002735296031460166\n",
            "step: 210, loss: 0.001392025500535965\n",
            "step: 220, loss: 0.003804252715781331\n",
            "step: 230, loss: 0.00035513032344169915\n",
            "step: 240, loss: 0.0021062667947262526\n",
            "step: 250, loss: 0.00018456151883583516\n",
            "step: 260, loss: 0.00024592041154392064\n",
            "step: 270, loss: 0.00028763458249159157\n",
            "step: 280, loss: 0.0007450900739058852\n",
            "step: 290, loss: 0.0003389817720744759\n",
            "step: 300, loss: 0.0001402539201080799\n",
            "step: 310, loss: 0.0009115709690377116\n",
            "step: 320, loss: 0.0036456366069614887\n",
            "step: 330, loss: 0.0009176344610750675\n",
            "step: 340, loss: 0.025405559688806534\n",
            "step: 350, loss: 0.004322477150708437\n",
            "step: 360, loss: 0.00024232096620835364\n",
            "step: 370, loss: 0.0005230470560491085\n",
            "step: 380, loss: 0.04495873674750328\n",
            "step: 390, loss: 0.002632845425978303\n",
            "step: 400, loss: 0.000547737639863044\n",
            "step: 410, loss: 0.0012745721032842994\n",
            "step: 420, loss: 0.004148493520915508\n",
            "step: 430, loss: 0.003611496649682522\n",
            "step: 440, loss: 0.005687338765710592\n",
            "step: 450, loss: 0.00020500893879216164\n",
            "step: 460, loss: 0.0009671714506112039\n",
            "step: 470, loss: 0.001243944396264851\n",
            "step: 480, loss: 0.0021665191743522882\n",
            "step: 490, loss: 0.0013511236757040024\n",
            "step: 500, loss: 0.015312404371798038\n",
            "step: 510, loss: 0.000706340535543859\n",
            "step: 520, loss: 0.0014085295842960477\n",
            "step: 530, loss: 0.001508255722001195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9165103189493434, f1=0.9115456238361266, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006769376341253519\n",
            "step: 10, loss: 0.007076040375977755\n",
            "step: 20, loss: 0.0011369726853445172\n",
            "step: 30, loss: 0.0016552356537431479\n",
            "step: 40, loss: 0.0015684545505791903\n",
            "step: 50, loss: 0.002679994096979499\n",
            "step: 60, loss: 0.009369200095534325\n",
            "step: 70, loss: 0.00015245706890709698\n",
            "step: 80, loss: 0.00012929273361805826\n",
            "step: 90, loss: 0.00022265853476710618\n",
            "step: 100, loss: 0.0012252939632162452\n",
            "step: 110, loss: 0.0002645125496201217\n",
            "step: 120, loss: 0.002573452889919281\n",
            "step: 130, loss: 0.00041808185051195323\n",
            "step: 140, loss: 0.0024208223912864923\n",
            "step: 150, loss: 0.0001882688229670748\n",
            "step: 160, loss: 0.00013466094969771802\n",
            "step: 170, loss: 0.0011592558585107327\n",
            "step: 180, loss: 0.0005455777863971889\n",
            "step: 190, loss: 0.0009196480386890471\n",
            "step: 200, loss: 0.004846976138651371\n",
            "step: 210, loss: 0.0016713475342839956\n",
            "step: 220, loss: 0.0007246572058647871\n",
            "step: 230, loss: 0.00014087455929256976\n",
            "step: 240, loss: 0.00024051731452345848\n",
            "step: 250, loss: 4.507824996835552e-05\n",
            "step: 260, loss: 7.00874297763221e-05\n",
            "step: 270, loss: 0.002262780675664544\n",
            "step: 280, loss: 0.001268831198103726\n",
            "step: 290, loss: 0.005287983454763889\n",
            "step: 300, loss: 0.00012603425420820713\n",
            "step: 310, loss: 0.0002156123227905482\n",
            "step: 320, loss: 0.0006061005406081676\n",
            "step: 330, loss: 0.0004350713570602238\n",
            "step: 340, loss: 0.00010532131273066625\n",
            "step: 350, loss: 0.0015183438081294298\n",
            "step: 360, loss: 0.00019962441001553088\n",
            "step: 370, loss: 0.0004090456059202552\n",
            "step: 380, loss: 6.761395343346521e-05\n",
            "step: 390, loss: 0.00017333627329207957\n",
            "step: 400, loss: 0.00014783187361899763\n",
            "step: 410, loss: 0.006086017936468124\n",
            "step: 420, loss: 0.00019714314839802682\n",
            "step: 430, loss: 6.73909526085481e-05\n",
            "step: 440, loss: 0.0004028283292427659\n",
            "step: 450, loss: 0.00023967173183336854\n",
            "step: 460, loss: 0.0009789586765691638\n",
            "step: 470, loss: 5.949044498265721e-05\n",
            "step: 480, loss: 0.0004894300363957882\n",
            "step: 490, loss: 0.00035989625030197203\n",
            "step: 500, loss: 0.0016888212412595749\n",
            "step: 510, loss: 0.0001776448916643858\n",
            "step: 520, loss: 0.0005586285260505974\n",
            "step: 530, loss: 4.7826069931034e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9201307800093415, f1=0.915270018621974, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001668635755777359\n",
            "step: 10, loss: 9.601422789273784e-05\n",
            "step: 20, loss: 0.00011722083581844345\n",
            "step: 30, loss: 0.0002559900749474764\n",
            "step: 40, loss: 0.00010899812332354486\n",
            "step: 50, loss: 0.0037270488683134317\n",
            "step: 60, loss: 0.0002242066984763369\n",
            "step: 70, loss: 0.0010435784934088588\n",
            "step: 80, loss: 0.00015406383317895234\n",
            "step: 90, loss: 0.00015598170284647495\n",
            "step: 100, loss: 0.0012697571655735373\n",
            "step: 110, loss: 0.0060674757696688175\n",
            "step: 120, loss: 0.0002429101150482893\n",
            "step: 130, loss: 0.00013263044820632786\n",
            "step: 140, loss: 5.328568659024313e-05\n",
            "step: 150, loss: 4.959966099704616e-05\n",
            "step: 160, loss: 6.878014391986653e-05\n",
            "step: 170, loss: 0.0002351402654312551\n",
            "step: 180, loss: 0.0003163288638461381\n",
            "step: 190, loss: 0.00020670056983362883\n",
            "step: 200, loss: 0.00010789995576487854\n",
            "step: 210, loss: 0.001180600724183023\n",
            "step: 220, loss: 8.17626787465997e-05\n",
            "step: 230, loss: 0.00016491721908096224\n",
            "step: 240, loss: 0.02004065178334713\n",
            "step: 250, loss: 7.284760795300826e-05\n",
            "step: 260, loss: 8.48357449285686e-05\n",
            "step: 270, loss: 0.00011025104322470725\n",
            "step: 280, loss: 0.0001815672731027007\n",
            "step: 290, loss: 0.00040681182872503996\n",
            "step: 300, loss: 0.0007071828586049378\n",
            "step: 310, loss: 8.006983989616856e-05\n",
            "step: 320, loss: 0.03172234818339348\n",
            "step: 330, loss: 0.0013416141737252474\n",
            "step: 340, loss: 0.0005816711927764118\n",
            "step: 350, loss: 0.00014779796765651554\n",
            "step: 360, loss: 0.02997656911611557\n",
            "step: 370, loss: 0.00029991412884555757\n",
            "step: 380, loss: 6.245491385925561e-05\n",
            "step: 390, loss: 0.0008959879050962627\n",
            "step: 400, loss: 0.000689466018229723\n",
            "step: 410, loss: 0.0019390406087040901\n",
            "step: 420, loss: 0.013543483801186085\n",
            "step: 430, loss: 0.0007323969039134681\n",
            "step: 440, loss: 0.0002677678712643683\n",
            "step: 450, loss: 0.09057087451219559\n",
            "step: 460, loss: 0.00018184339569415897\n",
            "step: 470, loss: 0.0005838802317157388\n",
            "step: 480, loss: 0.0037541273050010204\n",
            "step: 490, loss: 0.0004530463193077594\n",
            "step: 500, loss: 0.00011814069875981659\n",
            "step: 510, loss: 0.0008226241334341466\n",
            "step: 520, loss: 8.933801291277632e-05\n",
            "step: 530, loss: 0.0004185196303296834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.922077922077922, f1=0.9158964879852127, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000635494536254555\n",
            "step: 10, loss: 0.0005558306002058089\n",
            "step: 20, loss: 0.0006653271266259253\n",
            "step: 30, loss: 0.00011362097575329244\n",
            "step: 40, loss: 7.71367849665694e-05\n",
            "step: 50, loss: 0.0001423661014996469\n",
            "step: 60, loss: 0.0001117146312026307\n",
            "step: 70, loss: 0.00021158054005354643\n",
            "step: 80, loss: 0.0006761016556993127\n",
            "step: 90, loss: 0.005123398266732693\n",
            "step: 100, loss: 0.00045350275468081236\n",
            "step: 110, loss: 9.313018381362781e-05\n",
            "step: 120, loss: 0.0006336967344395816\n",
            "step: 130, loss: 8.268420788226649e-05\n",
            "step: 140, loss: 0.00026145210722461343\n",
            "step: 150, loss: 8.401733794016764e-05\n",
            "step: 160, loss: 0.0001823462371248752\n",
            "step: 170, loss: 0.00011808936687884852\n",
            "step: 180, loss: 4.023152359877713e-05\n",
            "step: 190, loss: 0.00016359976143576205\n",
            "step: 200, loss: 5.586137922364287e-05\n",
            "step: 210, loss: 0.0005271054105833173\n",
            "step: 220, loss: 4.663785875891335e-05\n",
            "step: 230, loss: 0.00022744160378351808\n",
            "step: 240, loss: 0.0008395200711674988\n",
            "step: 250, loss: 8.997876284411177e-05\n",
            "step: 260, loss: 5.294408038025722e-05\n",
            "step: 270, loss: 3.562020356184803e-05\n",
            "step: 280, loss: 0.0006562484195455909\n",
            "step: 290, loss: 0.007479868363589048\n",
            "step: 300, loss: 0.00012616832100320607\n",
            "step: 310, loss: 0.0009010815410874784\n",
            "step: 320, loss: 0.000309580413158983\n",
            "step: 330, loss: 0.0003290224994998425\n",
            "step: 340, loss: 0.0003337975940667093\n",
            "step: 350, loss: 7.566367276012897e-05\n",
            "step: 360, loss: 4.2107807530555874e-05\n",
            "step: 370, loss: 0.00034325855085626245\n",
            "step: 380, loss: 9.540552855469286e-05\n",
            "step: 390, loss: 0.0006570870755240321\n",
            "step: 400, loss: 7.716954132774845e-05\n",
            "step: 410, loss: 0.0008609858923591673\n",
            "step: 420, loss: 6.481459422502667e-05\n",
            "step: 430, loss: 0.0005121629219502211\n",
            "step: 440, loss: 0.07560984790325165\n",
            "step: 450, loss: 0.0011715992586687207\n",
            "step: 460, loss: 0.0008879017550498247\n",
            "step: 470, loss: 0.00011436655768193305\n",
            "step: 480, loss: 5.5688622524030507e-05\n",
            "step: 490, loss: 0.0006861565634608269\n",
            "step: 500, loss: 6.882134039187804e-05\n",
            "step: 510, loss: 0.00010412819392513484\n",
            "step: 520, loss: 0.008197702467441559\n",
            "step: 530, loss: 0.001788068562746048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9206496519721578, f1=0.9131238447319778, best_f1=0.9123783031988874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.203404725762084e-05\n",
            "step: 10, loss: 4.099066791241057e-05\n",
            "step: 20, loss: 0.0002334219025215134\n",
            "step: 30, loss: 5.1569371862569824e-05\n",
            "step: 40, loss: 0.00010371125972596928\n",
            "step: 50, loss: 0.00014137380640022457\n",
            "step: 60, loss: 4.458108378457837e-05\n",
            "step: 70, loss: 0.0004566086863633245\n",
            "step: 80, loss: 0.00011357384937582538\n",
            "step: 90, loss: 7.824871863704175e-05\n",
            "step: 100, loss: 0.0006471999222412705\n",
            "step: 110, loss: 6.441182631533593e-05\n",
            "step: 120, loss: 7.201216067187488e-05\n",
            "step: 130, loss: 0.000551077420823276\n",
            "step: 140, loss: 0.00011486210132716224\n",
            "step: 150, loss: 0.01001254003494978\n",
            "step: 160, loss: 0.0001550538290757686\n",
            "step: 170, loss: 8.660005551064387e-05\n",
            "step: 180, loss: 0.000693858484737575\n",
            "step: 190, loss: 0.0002414926275378093\n",
            "step: 200, loss: 0.0032146754674613476\n",
            "step: 210, loss: 5.199792212806642e-05\n",
            "step: 220, loss: 6.773033237550408e-05\n",
            "step: 230, loss: 0.000227677752263844\n",
            "step: 240, loss: 0.00015030261420179158\n",
            "step: 250, loss: 0.00025461838231422007\n",
            "step: 260, loss: 0.0014768311521038413\n",
            "step: 270, loss: 0.0009630474378354847\n",
            "step: 280, loss: 0.013576949946582317\n",
            "step: 290, loss: 0.00039297007606364787\n",
            "step: 300, loss: 0.0001411733974236995\n",
            "step: 310, loss: 0.0004454281006474048\n",
            "step: 320, loss: 0.0001259307173313573\n",
            "step: 330, loss: 0.0002775034517981112\n",
            "step: 340, loss: 0.00013378339644987136\n",
            "step: 350, loss: 0.0002469460596330464\n",
            "step: 360, loss: 0.0020715745631605387\n",
            "step: 370, loss: 6.215545727172866e-05\n",
            "step: 380, loss: 0.0025776776019483805\n",
            "step: 390, loss: 0.0026483305264264345\n",
            "step: 400, loss: 0.000712643435690552\n",
            "step: 410, loss: 0.0012287645367905498\n",
            "step: 420, loss: 8.840048394631594e-05\n",
            "step: 430, loss: 0.01243277546018362\n",
            "step: 440, loss: 0.0006866753683425486\n",
            "step: 450, loss: 0.0004922955413348973\n",
            "step: 460, loss: 8.177180279744789e-05\n",
            "step: 470, loss: 7.203661516541615e-05\n",
            "step: 480, loss: 7.298558921320364e-05\n",
            "step: 490, loss: 0.0001625900185899809\n",
            "step: 500, loss: 7.350657688220963e-05\n",
            "step: 510, loss: 0.0002855872444342822\n",
            "step: 520, loss: 0.00031426240457221866\n",
            "step: 530, loss: 0.00012277478526812047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9237209302325581, f1=0.917941585535466, best_f1=0.917941585535466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010532478336244822\n",
            "step: 10, loss: 0.00034633479663170874\n",
            "step: 20, loss: 0.004244769457727671\n",
            "step: 30, loss: 0.0011896772775799036\n",
            "step: 40, loss: 0.062770776450634\n",
            "step: 50, loss: 0.03267700597643852\n",
            "step: 60, loss: 7.535697659477592e-05\n",
            "step: 70, loss: 0.0008974058437161148\n",
            "step: 80, loss: 0.0005324697121977806\n",
            "step: 90, loss: 0.0001245082530658692\n",
            "step: 100, loss: 0.00043774733785539865\n",
            "step: 110, loss: 0.0002854544436559081\n",
            "step: 120, loss: 0.00035022301017306745\n",
            "step: 130, loss: 0.01297916378825903\n",
            "step: 140, loss: 0.00021239528723526746\n",
            "step: 150, loss: 0.00020546908490359783\n",
            "step: 160, loss: 0.0006706533022224903\n",
            "step: 170, loss: 0.003662377130240202\n",
            "step: 180, loss: 0.00022868625819683075\n",
            "step: 190, loss: 0.003206589026376605\n",
            "step: 200, loss: 0.00015848140174057335\n",
            "step: 210, loss: 0.0013659356627613306\n",
            "step: 220, loss: 0.00021475725225172937\n",
            "step: 230, loss: 0.0009188464027829468\n",
            "step: 240, loss: 0.00013192091137170792\n",
            "step: 250, loss: 7.947587437229231e-05\n",
            "step: 260, loss: 0.0001990810123970732\n",
            "step: 270, loss: 7.086074037943035e-05\n",
            "step: 280, loss: 0.00012221535143908113\n",
            "step: 290, loss: 5.598340430879034e-05\n",
            "step: 300, loss: 7.618069503223523e-05\n",
            "step: 310, loss: 0.0006058196304365993\n",
            "step: 320, loss: 0.00031114002922549844\n",
            "step: 330, loss: 9.128131932811812e-05\n",
            "step: 340, loss: 0.00030869824695400894\n",
            "step: 350, loss: 0.00024088901409413666\n",
            "step: 360, loss: 0.00015273159078788012\n",
            "step: 370, loss: 0.0001511833688709885\n",
            "step: 380, loss: 0.00036148412618786097\n",
            "step: 390, loss: 0.0008466944564133883\n",
            "step: 400, loss: 0.0030352266039699316\n",
            "step: 410, loss: 0.0005577703705057502\n",
            "step: 420, loss: 0.00012029075878672302\n",
            "step: 430, loss: 0.00027130270609632134\n",
            "step: 440, loss: 0.0027798719238489866\n",
            "step: 450, loss: 3.993383143097162e-05\n",
            "step: 460, loss: 8.820155926514417e-05\n",
            "step: 470, loss: 0.006896992679685354\n",
            "step: 480, loss: 5.4895626817597076e-05\n",
            "step: 490, loss: 0.00622172188013792\n",
            "step: 500, loss: 7.773871038807556e-05\n",
            "step: 510, loss: 0.00014765124069526792\n",
            "step: 520, loss: 0.001406090217642486\n",
            "step: 530, loss: 9.08069487195462e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.921999065857076, f1=0.9191449814126395, best_f1=0.917941585535466\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 193.43it/s]\n",
            "load_f1 = 0.9191449814126395\n",
            "real_f1 = 0.9190938511326862\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 191.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac63953-346c-4433-9800-4b4f727d4659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.561932384967804\n",
            "step: 10, loss: 0.38196662068367004\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.37541916966438293\n",
            "step: 30, loss: 0.35598787665367126\n",
            "step: 40, loss: 0.17340539395809174\n",
            "step: 50, loss: 0.45658838748931885\n",
            "step: 60, loss: 0.3421810567378998\n",
            "step: 70, loss: 0.2333104908466339\n",
            "step: 80, loss: 0.1929652988910675\n",
            "step: 90, loss: 0.44718387722969055\n",
            "step: 100, loss: 0.5071896910667419\n",
            "step: 110, loss: 0.2838827073574066\n",
            "step: 120, loss: 0.2185257375240326\n",
            "step: 130, loss: 0.2789377272129059\n",
            "step: 140, loss: 0.31536340713500977\n",
            "step: 150, loss: 0.31696629524230957\n",
            "step: 160, loss: 0.27377820014953613\n",
            "step: 170, loss: 0.3022902011871338\n",
            "step: 180, loss: 0.0815606489777565\n",
            "step: 190, loss: 0.28965145349502563\n",
            "step: 200, loss: 0.29116880893707275\n",
            "step: 210, loss: 0.20195452868938446\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5304518664047151, f1=0.5251396648044692, best_f1=0.5251396648044692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14377640187740326\n",
            "step: 10, loss: 0.2467467188835144\n",
            "step: 20, loss: 0.16409209370613098\n",
            "step: 30, loss: 0.32453668117523193\n",
            "step: 40, loss: 0.21309612691402435\n",
            "step: 50, loss: 0.12727226316928864\n",
            "step: 60, loss: 0.3512710928916931\n",
            "step: 70, loss: 0.14912433922290802\n",
            "step: 80, loss: 0.2837107479572296\n",
            "step: 90, loss: 0.11995261907577515\n",
            "step: 100, loss: 0.07547318935394287\n",
            "step: 110, loss: 0.17647330462932587\n",
            "step: 120, loss: 0.18774248659610748\n",
            "step: 130, loss: 0.04876979440450668\n",
            "step: 140, loss: 0.22882550954818726\n",
            "step: 150, loss: 0.2446465790271759\n",
            "step: 160, loss: 0.21690145134925842\n",
            "step: 170, loss: 0.14461781084537506\n",
            "step: 180, loss: 0.2507364749908447\n",
            "step: 190, loss: 0.2718774974346161\n",
            "step: 200, loss: 0.07781342417001724\n",
            "step: 210, loss: 0.17921504378318787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5394321766561515, f1=0.5681818181818182, best_f1=0.5681818181818182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10968358814716339\n",
            "step: 10, loss: 0.16632553935050964\n",
            "step: 20, loss: 0.19120532274246216\n",
            "step: 30, loss: 0.31611472368240356\n",
            "step: 40, loss: 0.06593797355890274\n",
            "step: 50, loss: 0.07002276182174683\n",
            "step: 60, loss: 0.2627519965171814\n",
            "step: 70, loss: 0.20173591375350952\n",
            "step: 80, loss: 0.17694003880023956\n",
            "step: 90, loss: 0.10377304255962372\n",
            "step: 100, loss: 0.12604191899299622\n",
            "step: 110, loss: 0.08811713010072708\n",
            "step: 120, loss: 0.24525734782218933\n",
            "step: 130, loss: 0.15299075841903687\n",
            "step: 140, loss: 0.20891696214675903\n",
            "step: 150, loss: 0.22955092787742615\n",
            "step: 160, loss: 0.062040429562330246\n",
            "step: 170, loss: 0.1881375014781952\n",
            "step: 180, loss: 0.10262472182512283\n",
            "step: 190, loss: 0.2185002714395523\n",
            "step: 200, loss: 0.06665173172950745\n",
            "step: 210, loss: 0.1326162964105606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5613382899628253, f1=0.5813528336380257, best_f1=0.5813528336380257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23238737881183624\n",
            "step: 10, loss: 0.10036902874708176\n",
            "step: 20, loss: 0.26490330696105957\n",
            "step: 30, loss: 0.3516039550304413\n",
            "step: 40, loss: 0.08425695449113846\n",
            "step: 50, loss: 0.1862778514623642\n",
            "step: 60, loss: 0.23209835588932037\n",
            "step: 70, loss: 0.35358524322509766\n",
            "step: 80, loss: 0.09305718541145325\n",
            "step: 90, loss: 0.07219663262367249\n",
            "step: 100, loss: 0.30651605129241943\n",
            "step: 110, loss: 0.2245236188173294\n",
            "step: 120, loss: 0.15707901120185852\n",
            "step: 130, loss: 0.1405888795852661\n",
            "step: 140, loss: 0.19224271178245544\n",
            "step: 150, loss: 0.15436334908008575\n",
            "step: 160, loss: 0.044449228793382645\n",
            "step: 170, loss: 0.11533539742231369\n",
            "step: 180, loss: 0.42306676506996155\n",
            "step: 190, loss: 0.134471595287323\n",
            "step: 200, loss: 0.16183386743068695\n",
            "step: 210, loss: 0.27163609862327576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5720164609053499, f1=0.5778688524590164, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10064142942428589\n",
            "step: 10, loss: 0.06308963149785995\n",
            "step: 20, loss: 0.35051581263542175\n",
            "step: 30, loss: 0.06231073662638664\n",
            "step: 40, loss: 0.16383615136146545\n",
            "step: 50, loss: 0.16960714757442474\n",
            "step: 60, loss: 0.1653696447610855\n",
            "step: 70, loss: 0.22835950553417206\n",
            "step: 80, loss: 0.07332412898540497\n",
            "step: 90, loss: 0.10657387226819992\n",
            "step: 100, loss: 0.009353180415928364\n",
            "step: 110, loss: 0.3601433336734772\n",
            "step: 120, loss: 0.06742251664400101\n",
            "step: 130, loss: 0.14123821258544922\n",
            "step: 140, loss: 0.13999442756175995\n",
            "step: 150, loss: 0.07046673446893692\n",
            "step: 160, loss: 0.17797496914863586\n",
            "step: 170, loss: 0.0947832465171814\n",
            "step: 180, loss: 0.11231426894664764\n",
            "step: 190, loss: 0.08896876126527786\n",
            "step: 200, loss: 0.05526620149612427\n",
            "step: 210, loss: 0.0389108844101429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5458089668615984, f1=0.5632183908045977, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05238144472241402\n",
            "step: 10, loss: 0.07270846515893936\n",
            "step: 20, loss: 0.0771159827709198\n",
            "step: 30, loss: 0.004129712004214525\n",
            "step: 40, loss: 0.07813098281621933\n",
            "step: 50, loss: 0.016708573326468468\n",
            "step: 60, loss: 0.186320960521698\n",
            "step: 70, loss: 0.01720019057393074\n",
            "step: 80, loss: 0.07310400158166885\n",
            "step: 90, loss: 0.22228090465068817\n",
            "step: 100, loss: 0.012423837557435036\n",
            "step: 110, loss: 0.047253429889678955\n",
            "step: 120, loss: 0.0178973451256752\n",
            "step: 130, loss: 0.05443910136818886\n",
            "step: 140, loss: 0.027176018804311752\n",
            "step: 150, loss: 0.030436711385846138\n",
            "step: 160, loss: 0.012936200946569443\n",
            "step: 170, loss: 0.1433609426021576\n",
            "step: 180, loss: 0.019354302436113358\n",
            "step: 190, loss: 0.22401508688926697\n",
            "step: 200, loss: 0.009117942303419113\n",
            "step: 210, loss: 0.047167591750621796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5378787878787878, f1=0.540952380952381, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05575336515903473\n",
            "step: 10, loss: 0.03062710538506508\n",
            "step: 20, loss: 0.02046326920390129\n",
            "step: 30, loss: 0.141821026802063\n",
            "step: 40, loss: 0.06264271587133408\n",
            "step: 50, loss: 0.09894751012325287\n",
            "step: 60, loss: 0.10437338799238205\n",
            "step: 70, loss: 0.032828912138938904\n",
            "step: 80, loss: 0.06011096015572548\n",
            "step: 90, loss: 0.11172237247228622\n",
            "step: 100, loss: 0.013100172393023968\n",
            "step: 110, loss: 0.17753493785858154\n",
            "step: 120, loss: 0.3318089544773102\n",
            "step: 130, loss: 0.0778261125087738\n",
            "step: 140, loss: 0.033866189420223236\n",
            "step: 150, loss: 0.13273563981056213\n",
            "step: 160, loss: 0.08648640662431717\n",
            "step: 170, loss: 0.0255570225417614\n",
            "step: 180, loss: 0.04886248707771301\n",
            "step: 190, loss: 0.029400870203971863\n",
            "step: 200, loss: 0.03392895311117172\n",
            "step: 210, loss: 0.20769844949245453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5381526104417671, f1=0.511340206185567, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028750427067279816\n",
            "step: 10, loss: 0.15804320573806763\n",
            "step: 20, loss: 0.02598012425005436\n",
            "step: 30, loss: 0.06585128605365753\n",
            "step: 40, loss: 0.06588435918092728\n",
            "step: 50, loss: 0.10918564349412918\n",
            "step: 60, loss: 0.13216708600521088\n",
            "step: 70, loss: 0.11248582601547241\n",
            "step: 80, loss: 0.08318179100751877\n",
            "step: 90, loss: 0.06125306710600853\n",
            "step: 100, loss: 0.12831173837184906\n",
            "step: 110, loss: 0.08659306913614273\n",
            "step: 120, loss: 0.004709347151219845\n",
            "step: 130, loss: 0.02683163620531559\n",
            "step: 140, loss: 0.16099798679351807\n",
            "step: 150, loss: 0.12807050347328186\n",
            "step: 160, loss: 0.05142957717180252\n",
            "step: 170, loss: 0.016219468787312508\n",
            "step: 180, loss: 0.10748087614774704\n",
            "step: 190, loss: 0.06535530835390091\n",
            "step: 200, loss: 0.0026355963200330734\n",
            "step: 210, loss: 0.04204375296831131\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5571142284569138, f1=0.5295315682281059, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02137613482773304\n",
            "step: 10, loss: 0.14480091631412506\n",
            "step: 20, loss: 0.008939669467508793\n",
            "step: 30, loss: 0.13524125516414642\n",
            "step: 40, loss: 0.05369008705019951\n",
            "step: 50, loss: 0.05098594352602959\n",
            "step: 60, loss: 0.12680085003376007\n",
            "step: 70, loss: 0.11355870962142944\n",
            "step: 80, loss: 0.01467288937419653\n",
            "step: 90, loss: 0.0031887292861938477\n",
            "step: 100, loss: 0.06404727697372437\n",
            "step: 110, loss: 0.08183549344539642\n",
            "step: 120, loss: 0.06538332253694534\n",
            "step: 130, loss: 0.01797512359917164\n",
            "step: 140, loss: 0.13163688778877258\n",
            "step: 150, loss: 0.23596611618995667\n",
            "step: 160, loss: 0.008830592036247253\n",
            "step: 170, loss: 0.04893770441412926\n",
            "step: 180, loss: 0.036039821803569794\n",
            "step: 190, loss: 0.00711718900129199\n",
            "step: 200, loss: 0.12601400911808014\n",
            "step: 210, loss: 0.05112858861684799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5390946502057613, f1=0.5274261603375527, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.057605382055044174\n",
            "step: 10, loss: 0.051808472722768784\n",
            "step: 20, loss: 0.004706875886768103\n",
            "step: 30, loss: 0.35790812969207764\n",
            "step: 40, loss: 0.0074671427719295025\n",
            "step: 50, loss: 0.005264155566692352\n",
            "step: 60, loss: 0.12026776373386383\n",
            "step: 70, loss: 0.03628760203719139\n",
            "step: 80, loss: 0.06649953126907349\n",
            "step: 90, loss: 0.06965833902359009\n",
            "step: 100, loss: 0.012888294644653797\n",
            "step: 110, loss: 0.00446836743503809\n",
            "step: 120, loss: 0.0019691421184688807\n",
            "step: 130, loss: 0.11453203856945038\n",
            "step: 140, loss: 0.0032230087090283632\n",
            "step: 150, loss: 0.028772303834557533\n",
            "step: 160, loss: 0.02403487265110016\n",
            "step: 170, loss: 0.0013290190836414695\n",
            "step: 180, loss: 0.10206001996994019\n",
            "step: 190, loss: 0.13138803839683533\n",
            "step: 200, loss: 0.023523278534412384\n",
            "step: 210, loss: 0.0836658626794815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.532, f1=0.4968152866242039, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0549805723130703\n",
            "step: 10, loss: 0.10940007865428925\n",
            "step: 20, loss: 0.08297421038150787\n",
            "step: 30, loss: 0.00503826467320323\n",
            "step: 40, loss: 0.012411331757903099\n",
            "step: 50, loss: 0.14142844080924988\n",
            "step: 60, loss: 0.05044317618012428\n",
            "step: 70, loss: 0.027226148173213005\n",
            "step: 80, loss: 0.14131534099578857\n",
            "step: 90, loss: 0.02245078794658184\n",
            "step: 100, loss: 0.0032529335003346205\n",
            "step: 110, loss: 0.016533832997083664\n",
            "step: 120, loss: 0.07380200177431107\n",
            "step: 130, loss: 0.013233800418674946\n",
            "step: 140, loss: 0.19027279317378998\n",
            "step: 150, loss: 0.00043458197615109384\n",
            "step: 160, loss: 0.002324250526726246\n",
            "step: 170, loss: 0.05941487476229668\n",
            "step: 180, loss: 0.004099602811038494\n",
            "step: 190, loss: 0.027812723070383072\n",
            "step: 200, loss: 0.07288870960474014\n",
            "step: 210, loss: 0.0035410395357757807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5476673427991886, f1=0.526530612244898, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04492562636733055\n",
            "step: 10, loss: 0.0032380041666328907\n",
            "step: 20, loss: 0.21624456346035004\n",
            "step: 30, loss: 0.026094721630215645\n",
            "step: 40, loss: 0.06622488796710968\n",
            "step: 50, loss: 0.0036453185603022575\n",
            "step: 60, loss: 0.010420555248856544\n",
            "step: 70, loss: 0.002351746428757906\n",
            "step: 80, loss: 0.04536993429064751\n",
            "step: 90, loss: 0.00308625097386539\n",
            "step: 100, loss: 0.009059763513505459\n",
            "step: 110, loss: 0.002124052494764328\n",
            "step: 120, loss: 0.018663981929421425\n",
            "step: 130, loss: 0.02559361420571804\n",
            "step: 140, loss: 0.07646898180246353\n",
            "step: 150, loss: 0.016304928809404373\n",
            "step: 160, loss: 0.0027268726844340563\n",
            "step: 170, loss: 0.030342960730195045\n",
            "step: 180, loss: 0.033858492970466614\n",
            "step: 190, loss: 0.04353882744908333\n",
            "step: 200, loss: 0.002245499985292554\n",
            "step: 210, loss: 0.01419768575578928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5348314606741573, f1=0.5342465753424658, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07868249714374542\n",
            "step: 10, loss: 0.0003514767449814826\n",
            "step: 20, loss: 0.012256370857357979\n",
            "step: 30, loss: 0.06384357810020447\n",
            "step: 40, loss: 0.08444519340991974\n",
            "step: 50, loss: 0.008383489213883877\n",
            "step: 60, loss: 0.017166731879115105\n",
            "step: 70, loss: 0.03905710577964783\n",
            "step: 80, loss: 0.02983442135155201\n",
            "step: 90, loss: 0.0019172948086634278\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.0012057790299877524\n",
            "step: 110, loss: 0.0031551369465887547\n",
            "step: 120, loss: 0.0028723953291773796\n",
            "step: 130, loss: 0.0075820498168468475\n",
            "step: 140, loss: 0.0012145208893343806\n",
            "step: 150, loss: 0.006989024579524994\n",
            "step: 160, loss: 0.010570296086370945\n",
            "step: 170, loss: 0.00286588235758245\n",
            "step: 180, loss: 0.03306988254189491\n",
            "step: 190, loss: 0.0010389761300757527\n",
            "step: 200, loss: 0.011803828179836273\n",
            "step: 210, loss: 0.03821675479412079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5272331154684096, f1=0.5067873303167422, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034852023236453533\n",
            "step: 10, loss: 0.033530380576848984\n",
            "step: 20, loss: 0.001821668352931738\n",
            "step: 30, loss: 0.0022065567318350077\n",
            "step: 40, loss: 0.002641289494931698\n",
            "step: 50, loss: 0.0497792512178421\n",
            "step: 60, loss: 0.0015497613931074739\n",
            "step: 70, loss: 0.0033847633749246597\n",
            "step: 80, loss: 0.009441709145903587\n",
            "step: 90, loss: 0.011456570588052273\n",
            "step: 100, loss: 0.012943138368427753\n",
            "step: 110, loss: 0.0012560974573716521\n",
            "step: 120, loss: 0.051594335585832596\n",
            "step: 130, loss: 0.15184339880943298\n",
            "step: 140, loss: 0.02382732555270195\n",
            "step: 150, loss: 0.002482305746525526\n",
            "step: 160, loss: 0.009214404970407486\n",
            "step: 170, loss: 0.0006162370555102825\n",
            "step: 180, loss: 0.05037083849310875\n",
            "step: 190, loss: 0.05044228583574295\n",
            "step: 200, loss: 0.005049329251050949\n",
            "step: 210, loss: 0.04710579290986061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5188284518828452, f1=0.524945770065076, best_f1=0.5778688524590164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019311351934447885\n",
            "step: 10, loss: 0.0011363789672031999\n",
            "step: 20, loss: 0.01021649967879057\n",
            "step: 30, loss: 0.19968253374099731\n",
            "step: 40, loss: 0.035196613520383835\n",
            "step: 50, loss: 0.02234460413455963\n",
            "step: 60, loss: 0.013750078156590462\n",
            "step: 70, loss: 0.000797578482888639\n",
            "step: 80, loss: 0.01576909050345421\n",
            "step: 90, loss: 0.007913421839475632\n",
            "step: 100, loss: 0.2132837474346161\n",
            "step: 110, loss: 0.0007412023842334747\n",
            "step: 120, loss: 0.002454939531162381\n",
            "step: 130, loss: 0.04487837851047516\n",
            "step: 140, loss: 0.0011521333362907171\n",
            "step: 150, loss: 0.0007193986675702035\n",
            "step: 160, loss: 0.007811408489942551\n",
            "step: 170, loss: 0.002057329285889864\n",
            "step: 180, loss: 0.003477703081443906\n",
            "step: 190, loss: 0.013397534377872944\n",
            "step: 200, loss: 0.018091943114995956\n",
            "step: 210, loss: 0.023180348798632622\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5176991150442478, f1=0.5034642032332564, best_f1=0.5778688524590164\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 271.49it/s]\n",
            "load_f1 = 0.592436974789916\n",
            "real_f1 = 0.5889830508474577\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 186.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2ba3c9-5fe4-477b-c26f-acd74e507377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5842219591140747\n",
            "step: 10, loss: 0.36847999691963196\n",
            "step: 20, loss: 0.28939950466156006\n",
            "step: 30, loss: 0.4428635537624359\n",
            "step: 40, loss: 0.4337776303291321\n",
            "step: 50, loss: 0.2991374731063843\n",
            "step: 60, loss: 0.28736555576324463\n",
            "step: 70, loss: 0.2878355085849762\n",
            "step: 80, loss: 0.20732273161411285\n",
            "step: 90, loss: 0.29357412457466125\n",
            "step: 100, loss: 0.3222982585430145\n",
            "step: 110, loss: 0.45145344734191895\n",
            "step: 120, loss: 0.1955007165670395\n",
            "step: 130, loss: 0.12506334483623505\n",
            "step: 140, loss: 0.13794009387493134\n",
            "step: 150, loss: 0.20704123377799988\n",
            "step: 160, loss: 0.14964592456817627\n",
            "step: 170, loss: 0.2752370536327362\n",
            "step: 180, loss: 0.018978780135512352\n",
            "step: 190, loss: 0.21886348724365234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6432160804020101, f1=0.6255924170616114, best_f1=0.6255924170616114\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24262726306915283\n",
            "step: 10, loss: 0.08228439837694168\n",
            "step: 20, loss: 0.18907266855239868\n",
            "step: 30, loss: 0.15480133891105652\n",
            "step: 40, loss: 0.22331440448760986\n",
            "step: 50, loss: 0.4466925859451294\n",
            "step: 60, loss: 0.40300533175468445\n",
            "step: 70, loss: 0.3136136829853058\n",
            "step: 80, loss: 0.22930724918842316\n",
            "step: 90, loss: 0.22922837734222412\n",
            "step: 100, loss: 0.05980762094259262\n",
            "step: 110, loss: 0.13692079484462738\n",
            "step: 120, loss: 0.26930996775627136\n",
            "step: 130, loss: 0.11059590429067612\n",
            "step: 140, loss: 0.09162820875644684\n",
            "step: 150, loss: 0.16995376348495483\n",
            "step: 160, loss: 0.04909989610314369\n",
            "step: 170, loss: 0.1814032346010208\n",
            "step: 180, loss: 0.16269414126873016\n",
            "step: 190, loss: 0.05534593015909195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6666666666666666, f1=0.721763085399449, best_f1=0.721763085399449\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.061973292380571365\n",
            "step: 10, loss: 0.23479090631008148\n",
            "step: 20, loss: 0.08651861548423767\n",
            "step: 30, loss: 0.11929559707641602\n",
            "step: 40, loss: 0.11002923548221588\n",
            "step: 50, loss: 0.17901761829853058\n",
            "step: 60, loss: 0.05008174479007721\n",
            "step: 70, loss: 0.22240538895130157\n",
            "step: 80, loss: 0.07178807258605957\n",
            "step: 90, loss: 0.15054000914096832\n",
            "step: 100, loss: 0.09888331592082977\n",
            "step: 110, loss: 0.01598057523369789\n",
            "step: 120, loss: 0.01882929727435112\n",
            "step: 130, loss: 0.03407708555459976\n",
            "step: 140, loss: 0.025503644719719887\n",
            "step: 150, loss: 0.10935740172863007\n",
            "step: 160, loss: 0.062068331986665726\n",
            "step: 170, loss: 0.15642358362674713\n",
            "step: 180, loss: 0.06892015784978867\n",
            "step: 190, loss: 0.1699335277080536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7192982456140352, f1=0.7422096317280454, best_f1=0.7422096317280454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06078649312257767\n",
            "step: 10, loss: 0.10660126060247421\n",
            "step: 20, loss: 0.34233203530311584\n",
            "step: 30, loss: 0.11531727015972137\n",
            "step: 40, loss: 0.03834228590130806\n",
            "step: 50, loss: 0.030437367036938667\n",
            "step: 60, loss: 0.02929580584168434\n",
            "step: 70, loss: 0.09800583869218826\n",
            "step: 80, loss: 0.32523155212402344\n",
            "step: 90, loss: 0.034941595047712326\n",
            "step: 100, loss: 0.13637705147266388\n",
            "step: 110, loss: 0.005847234278917313\n",
            "step: 120, loss: 0.13901157677173615\n",
            "step: 130, loss: 0.10536915808916092\n",
            "step: 140, loss: 0.03124515525996685\n",
            "step: 150, loss: 0.24811163544654846\n",
            "step: 160, loss: 0.02259371429681778\n",
            "step: 170, loss: 0.3188821077346802\n",
            "step: 180, loss: 0.04741441458463669\n",
            "step: 190, loss: 0.25282737612724304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7219251336898397, f1=0.7454068241469817, best_f1=0.7454068241469817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05002223700284958\n",
            "step: 10, loss: 0.04142037406563759\n",
            "step: 20, loss: 0.04652230814099312\n",
            "step: 30, loss: 0.007321993820369244\n",
            "step: 40, loss: 0.052224498242139816\n",
            "step: 50, loss: 0.08021646738052368\n",
            "step: 60, loss: 0.0635162740945816\n",
            "step: 70, loss: 0.005802797619253397\n",
            "step: 80, loss: 0.014369065873324871\n",
            "step: 90, loss: 0.023137178272008896\n",
            "step: 100, loss: 0.009145675227046013\n",
            "step: 110, loss: 0.05020295828580856\n",
            "step: 120, loss: 0.01263856329023838\n",
            "step: 130, loss: 0.07527519017457962\n",
            "step: 140, loss: 0.029449939727783203\n",
            "step: 150, loss: 0.010344588197767735\n",
            "step: 160, loss: 0.0069102211855351925\n",
            "step: 170, loss: 0.009993291459977627\n",
            "step: 180, loss: 0.09381107985973358\n",
            "step: 190, loss: 0.15801554918289185\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7309644670050761, f1=0.7525773195876289, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02652456983923912\n",
            "step: 10, loss: 0.1244487315416336\n",
            "step: 20, loss: 0.019713932648301125\n",
            "step: 30, loss: 0.0048160008154809475\n",
            "step: 40, loss: 0.11472184211015701\n",
            "step: 50, loss: 0.006632114760577679\n",
            "step: 60, loss: 0.06360176205635071\n",
            "step: 70, loss: 0.018893934786319733\n",
            "step: 80, loss: 0.029050210490822792\n",
            "step: 90, loss: 0.007375611457973719\n",
            "step: 100, loss: 0.0007608954329043627\n",
            "step: 110, loss: 0.14535605907440186\n",
            "step: 120, loss: 0.11936677992343903\n",
            "step: 130, loss: 0.042954813688993454\n",
            "step: 140, loss: 0.042214635759592056\n",
            "step: 150, loss: 0.01761053130030632\n",
            "step: 160, loss: 0.01991255208849907\n",
            "step: 170, loss: 0.10522729158401489\n",
            "step: 180, loss: 0.03163396567106247\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.010433298535645008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7188264058679708, f1=0.7188264058679708, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004013307858258486\n",
            "step: 10, loss: 0.004004590213298798\n",
            "step: 20, loss: 0.0915115550160408\n",
            "step: 30, loss: 0.0701693668961525\n",
            "step: 40, loss: 0.006555613595992327\n",
            "step: 50, loss: 0.02709578163921833\n",
            "step: 60, loss: 0.004409497138112783\n",
            "step: 70, loss: 0.0012708770809695125\n",
            "step: 80, loss: 0.02708190120756626\n",
            "step: 90, loss: 0.02570854313671589\n",
            "step: 100, loss: 0.0006754864589311182\n",
            "step: 110, loss: 0.013391310349106789\n",
            "step: 120, loss: 0.004923022352159023\n",
            "step: 130, loss: 0.1274271458387375\n",
            "step: 140, loss: 0.01817917264997959\n",
            "step: 150, loss: 0.05206039175391197\n",
            "step: 160, loss: 0.03810686245560646\n",
            "step: 170, loss: 0.006371780764311552\n",
            "step: 180, loss: 0.10602252930402756\n",
            "step: 190, loss: 0.008367011323571205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7272727272727273, f1=0.7506849315068493, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.032893478870391846\n",
            "step: 10, loss: 0.004135212395340204\n",
            "step: 20, loss: 0.00301779480651021\n",
            "step: 30, loss: 0.07128602266311646\n",
            "step: 40, loss: 0.00094379618531093\n",
            "step: 50, loss: 0.003765490371733904\n",
            "step: 60, loss: 0.0442904569208622\n",
            "step: 70, loss: 0.024987680837512016\n",
            "step: 80, loss: 0.0007194837089627981\n",
            "step: 90, loss: 0.0012883262243121862\n",
            "step: 100, loss: 0.0033797668293118477\n",
            "step: 110, loss: 0.008999977260828018\n",
            "step: 120, loss: 0.0012834318913519382\n",
            "step: 130, loss: 0.0011711913393810391\n",
            "step: 140, loss: 0.010537133552134037\n",
            "step: 150, loss: 0.007273709401488304\n",
            "step: 160, loss: 0.041374243795871735\n",
            "step: 170, loss: 0.0010800486197695136\n",
            "step: 180, loss: 0.006139586213976145\n",
            "step: 190, loss: 0.005316128488630056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7164948453608248, f1=0.7586206896551724, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008875209605321288\n",
            "step: 10, loss: 0.0029812732245773077\n",
            "step: 20, loss: 0.006567132193595171\n",
            "step: 30, loss: 0.0010532742599025369\n",
            "step: 40, loss: 0.0029186769388616085\n",
            "step: 50, loss: 0.0017488928278908134\n",
            "step: 60, loss: 0.0012593941064551473\n",
            "step: 70, loss: 0.0003454705874901265\n",
            "step: 80, loss: 0.0002724747755564749\n",
            "step: 90, loss: 0.14147116243839264\n",
            "step: 100, loss: 0.0022323860321193933\n",
            "step: 110, loss: 0.0011916274670511484\n",
            "step: 120, loss: 0.00929082278162241\n",
            "step: 130, loss: 0.005604181904345751\n",
            "step: 140, loss: 0.003962044138461351\n",
            "step: 150, loss: 0.0026193028315901756\n",
            "step: 160, loss: 0.00019176244677510113\n",
            "step: 170, loss: 0.24863331019878387\n",
            "step: 180, loss: 0.013925851322710514\n",
            "step: 190, loss: 0.0032847432885318995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7197943444730078, f1=0.760705289672544, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018202968640252948\n",
            "step: 10, loss: 0.016125250607728958\n",
            "step: 20, loss: 0.03736049309372902\n",
            "step: 30, loss: 0.0008090147166512907\n",
            "step: 40, loss: 0.013895638287067413\n",
            "step: 50, loss: 0.049289681017398834\n",
            "step: 60, loss: 0.01821596547961235\n",
            "step: 70, loss: 0.0034383805468678474\n",
            "step: 80, loss: 0.005032310727983713\n",
            "step: 90, loss: 0.13039320707321167\n",
            "step: 100, loss: 0.04087036848068237\n",
            "step: 110, loss: 0.00824898760765791\n",
            "step: 120, loss: 0.023278020322322845\n",
            "step: 130, loss: 0.06823936104774475\n",
            "step: 140, loss: 0.009879096411168575\n",
            "step: 150, loss: 0.006940861232578754\n",
            "step: 160, loss: 0.0014305270742624998\n",
            "step: 170, loss: 0.0014280332252383232\n",
            "step: 180, loss: 0.013269484043121338\n",
            "step: 190, loss: 0.0016775712138041854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7297297297297297, f1=0.7750677506775067, best_f1=0.7525773195876289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011303828796371818\n",
            "step: 10, loss: 0.002975541865453124\n",
            "step: 20, loss: 0.00646353280171752\n",
            "step: 30, loss: 0.002516915090382099\n",
            "step: 40, loss: 0.003225281136110425\n",
            "step: 50, loss: 0.0010289564961567521\n",
            "step: 60, loss: 0.0008987553883343935\n",
            "step: 70, loss: 0.00048029227764345706\n",
            "step: 80, loss: 0.000474491564091295\n",
            "step: 90, loss: 0.0018364833667874336\n",
            "step: 100, loss: 0.0042603760957717896\n",
            "step: 110, loss: 0.0011585673782974482\n",
            "step: 120, loss: 0.0004426841624081135\n",
            "step: 130, loss: 0.02793942391872406\n",
            "step: 140, loss: 0.0008371319272555411\n",
            "step: 150, loss: 0.0005295657319948077\n",
            "step: 160, loss: 0.005217129830271006\n",
            "step: 170, loss: 0.0005173876998014748\n",
            "step: 180, loss: 0.010893654078245163\n",
            "step: 190, loss: 0.0005830498994328082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7455919395465995, f1=0.7712082262210797, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038446293910965323\n",
            "step: 10, loss: 0.005321075674146414\n",
            "step: 20, loss: 0.0004817423177883029\n",
            "step: 30, loss: 0.019917072728276253\n",
            "step: 40, loss: 0.04446624964475632\n",
            "step: 50, loss: 0.004777122754603624\n",
            "step: 60, loss: 0.0002709657419472933\n",
            "step: 70, loss: 0.0004416780429892242\n",
            "step: 80, loss: 0.0009198879706673324\n",
            "step: 90, loss: 0.0008404715335927904\n",
            "step: 100, loss: 0.0008363716769963503\n",
            "step: 110, loss: 0.0006229060236364603\n",
            "step: 120, loss: 0.0003042138123419136\n",
            "step: 130, loss: 0.0010626381263136864\n",
            "step: 140, loss: 0.14669805765151978\n",
            "step: 150, loss: 0.003723922185599804\n",
            "step: 160, loss: 0.0006832635845057666\n",
            "step: 170, loss: 0.001122854184359312\n",
            "step: 180, loss: 0.0003621895448304713\n",
            "step: 190, loss: 0.019213151186704636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7309644670050761, f1=0.770408163265306, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001198428450152278\n",
            "step: 10, loss: 0.018642915412783623\n",
            "step: 20, loss: 0.0015223586233332753\n",
            "step: 30, loss: 0.005174004007130861\n",
            "step: 40, loss: 0.0004530933510977775\n",
            "step: 50, loss: 0.02105294167995453\n",
            "step: 60, loss: 0.0007948490092530847\n",
            "step: 70, loss: 0.01676395907998085\n",
            "step: 80, loss: 0.0003706621646415442\n",
            "step: 90, loss: 0.0022773374803364277\n",
            "step: 100, loss: 0.004760591313242912\n",
            "step: 110, loss: 0.0005205973866395652\n",
            "step: 120, loss: 0.012955055572092533\n",
            "step: 130, loss: 0.00039067125180736184\n",
            "step: 140, loss: 0.00046967979869805276\n",
            "step: 150, loss: 0.00036756761255674064\n",
            "step: 160, loss: 0.0015203303191810846\n",
            "step: 170, loss: 0.0025559512432664633\n",
            "step: 180, loss: 0.0009458505664952099\n",
            "step: 190, loss: 0.012894047424197197\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7329842931937172, f1=0.779220779220779, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004217700334265828\n",
            "step: 10, loss: 0.0006394494557753205\n",
            "step: 20, loss: 0.007199837360531092\n",
            "step: 30, loss: 0.002015022560954094\n",
            "step: 40, loss: 0.006342803128063679\n",
            "step: 50, loss: 0.0014864943223074079\n",
            "step: 60, loss: 0.0005027608713135123\n",
            "step: 70, loss: 0.0011527311289682984\n",
            "step: 80, loss: 0.0012031053192913532\n",
            "step: 90, loss: 0.0009310831665061414\n",
            "step: 100, loss: 0.05004461482167244\n",
            "step: 110, loss: 0.0637269839644432\n",
            "step: 120, loss: 0.0032173858489841223\n",
            "step: 130, loss: 0.008002044633030891\n",
            "step: 140, loss: 0.000668852124363184\n",
            "step: 150, loss: 0.0004434005240909755\n",
            "step: 160, loss: 0.002938844496384263\n",
            "step: 170, loss: 0.0996476337313652\n",
            "step: 180, loss: 0.0018570468528196216\n",
            "step: 190, loss: 0.0003995060396846384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7204301075268816, f1=0.7745358090185677, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006386613240465522\n",
            "step: 10, loss: 0.03164894878864288\n",
            "step: 20, loss: 0.0016746881883591413\n",
            "step: 30, loss: 0.0005516742239706218\n",
            "step: 40, loss: 0.00033620939939282835\n",
            "step: 50, loss: 0.019755320623517036\n",
            "step: 60, loss: 0.0005956003442406654\n",
            "step: 70, loss: 0.00484848627820611\n",
            "step: 80, loss: 0.01500722672790289\n",
            "step: 90, loss: 0.0006225433899089694\n",
            "step: 100, loss: 0.0013655164511874318\n",
            "step: 110, loss: 0.001173274009488523\n",
            "step: 120, loss: 0.0003721210523508489\n",
            "step: 130, loss: 0.006096632685512304\n",
            "step: 140, loss: 0.0022540786303579807\n",
            "step: 150, loss: 0.000972093956079334\n",
            "step: 160, loss: 0.0005715059814974666\n",
            "step: 170, loss: 0.000813574471976608\n",
            "step: 180, loss: 0.004087143111974001\n",
            "step: 190, loss: 0.0005889696185477078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7277777777777779, f1=0.7704918032786885, best_f1=0.7712082262210797\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 166.77it/s]\n",
            "load_f1 = 0.6812652068126521\n",
            "real_f1 = 0.6526806526806527\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5f0c3d-00d1-48ff-f5a9-3e99745ae160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 298kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.61MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 65.6MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6282248497009277\n",
            "step: 10, loss: 0.3820571303367615\n",
            "step: 20, loss: 0.30688369274139404\n",
            "step: 30, loss: 0.38028424978256226\n",
            "step: 40, loss: 0.2719513773918152\n",
            "step: 50, loss: 0.2762562334537506\n",
            "step: 60, loss: 0.2582228183746338\n",
            "step: 70, loss: 0.372784823179245\n",
            "step: 80, loss: 0.3567630648612976\n",
            "step: 90, loss: 0.25024470686912537\n",
            "step: 100, loss: 0.2606011629104614\n",
            "step: 110, loss: 0.19002613425254822\n",
            "step: 120, loss: 0.24546071887016296\n",
            "step: 130, loss: 0.03158487379550934\n",
            "step: 140, loss: 0.1507294476032257\n",
            "step: 150, loss: 0.29522082209587097\n",
            "step: 160, loss: 0.11442463845014572\n",
            "step: 170, loss: 0.28355932235717773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7240506329113925, f1=0.7315914489311163, best_f1=0.7315914489311163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17454248666763306\n",
            "step: 10, loss: 0.24831520020961761\n",
            "step: 20, loss: 0.16947151720523834\n",
            "step: 30, loss: 0.20299795269966125\n",
            "step: 40, loss: 0.11483648419380188\n",
            "step: 50, loss: 0.11814901232719421\n",
            "step: 60, loss: 0.07312045246362686\n",
            "step: 70, loss: 0.18052725493907928\n",
            "step: 80, loss: 0.05518693849444389\n",
            "step: 90, loss: 0.12992316484451294\n",
            "step: 100, loss: 0.23396968841552734\n",
            "step: 110, loss: 0.048873912543058395\n",
            "step: 120, loss: 0.09295643121004105\n",
            "step: 130, loss: 0.07698088139295578\n",
            "step: 140, loss: 0.25190284848213196\n",
            "step: 150, loss: 0.2572435140609741\n",
            "step: 160, loss: 0.2055722326040268\n",
            "step: 170, loss: 0.10633544623851776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.7163461538461539, f1=0.7522123893805309, best_f1=0.7315914489311163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2124290019273758\n",
            "step: 10, loss: 0.2885635793209076\n",
            "step: 20, loss: 0.040316201746463776\n",
            "step: 30, loss: 0.20482584834098816\n",
            "step: 40, loss: 0.12995412945747375\n",
            "step: 50, loss: 0.11626067757606506\n",
            "step: 60, loss: 0.08616426587104797\n",
            "step: 70, loss: 0.08773665875196457\n",
            "step: 80, loss: 0.05445127561688423\n",
            "step: 90, loss: 0.23199620842933655\n",
            "step: 100, loss: 0.06067676097154617\n",
            "step: 110, loss: 0.1368797868490219\n",
            "step: 120, loss: 0.06145568937063217\n",
            "step: 130, loss: 0.2062867432832718\n",
            "step: 140, loss: 0.16674454510211945\n",
            "step: 150, loss: 0.04860633611679077\n",
            "step: 160, loss: 0.07929766178131104\n",
            "step: 170, loss: 0.09600520879030228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.753623188405797, f1=0.7788018433179723, best_f1=0.7788018433179723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017457270994782448\n",
            "step: 10, loss: 0.242533877491951\n",
            "step: 20, loss: 0.016158897429704666\n",
            "step: 30, loss: 0.12285533547401428\n",
            "step: 40, loss: 0.007781409192830324\n",
            "step: 50, loss: 0.07659394294023514\n",
            "step: 60, loss: 0.058008063584566116\n",
            "step: 70, loss: 0.023321984335780144\n",
            "step: 80, loss: 0.10575054585933685\n",
            "step: 90, loss: 0.19329820573329926\n",
            "step: 100, loss: 0.12179625034332275\n",
            "step: 110, loss: 0.10604274272918701\n",
            "step: 120, loss: 0.02266981080174446\n",
            "step: 130, loss: 0.1538831740617752\n",
            "step: 140, loss: 0.02549937553703785\n",
            "step: 150, loss: 0.08062820881605148\n",
            "step: 160, loss: 0.13847512006759644\n",
            "step: 170, loss: 0.08746159821748734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7731958762886598, f1=0.7804878048780487, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025319840759038925\n",
            "step: 10, loss: 0.01712062954902649\n",
            "step: 20, loss: 0.029358241707086563\n",
            "step: 30, loss: 0.16358892619609833\n",
            "step: 40, loss: 0.0197112038731575\n",
            "step: 50, loss: 0.12407731264829636\n",
            "step: 60, loss: 0.03649076074361801\n",
            "step: 70, loss: 0.045820072293281555\n",
            "step: 80, loss: 0.1337999701499939\n",
            "step: 90, loss: 0.1934937834739685\n",
            "step: 100, loss: 0.013751097023487091\n",
            "step: 110, loss: 0.08402741700410843\n",
            "step: 120, loss: 0.14835092425346375\n",
            "step: 130, loss: 0.0713755264878273\n",
            "step: 140, loss: 0.010899960063397884\n",
            "step: 150, loss: 0.047431580722332\n",
            "step: 160, loss: 0.24122317135334015\n",
            "step: 170, loss: 0.00779977859929204\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.772972972972973, f1=0.775, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030111026018857956\n",
            "step: 10, loss: 0.00853356160223484\n",
            "step: 20, loss: 0.026149030774831772\n",
            "step: 30, loss: 0.07743509113788605\n",
            "step: 40, loss: 0.010131125338375568\n",
            "step: 50, loss: 0.13515496253967285\n",
            "step: 60, loss: 0.12510401010513306\n",
            "step: 70, loss: 0.08180789649486542\n",
            "step: 80, loss: 0.02499382384121418\n",
            "step: 90, loss: 0.004628363065421581\n",
            "step: 100, loss: 0.006642083637416363\n",
            "step: 110, loss: 0.00195550755597651\n",
            "step: 120, loss: 0.0161668099462986\n",
            "step: 130, loss: 0.011173772625625134\n",
            "step: 140, loss: 0.015179879032075405\n",
            "step: 150, loss: 0.07868611812591553\n",
            "step: 160, loss: 0.11577768623828888\n",
            "step: 170, loss: 0.011886738240718842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7608695652173912, f1=0.7552083333333334, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030600279569625854\n",
            "step: 10, loss: 0.002759203314781189\n",
            "step: 20, loss: 0.0026660196017473936\n",
            "step: 30, loss: 0.02776496484875679\n",
            "step: 40, loss: 0.004751617554575205\n",
            "step: 50, loss: 0.03879593685269356\n",
            "step: 60, loss: 0.006356813479214907\n",
            "step: 70, loss: 0.0061910077929496765\n",
            "step: 80, loss: 0.008898979984223843\n",
            "step: 90, loss: 0.00041196070378646255\n",
            "step: 100, loss: 0.028146175667643547\n",
            "step: 110, loss: 0.03133145347237587\n",
            "step: 120, loss: 0.0027599127497524023\n",
            "step: 130, loss: 0.3163331151008606\n",
            "step: 140, loss: 0.0028377140406519175\n",
            "step: 150, loss: 0.013117795810103416\n",
            "step: 160, loss: 0.003439904423430562\n",
            "step: 170, loss: 0.08863722532987595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7447916666666667, f1=0.7611940298507462, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010725390166044235\n",
            "step: 10, loss: 0.008487322367727757\n",
            "step: 20, loss: 0.001621211995370686\n",
            "step: 30, loss: 0.027261173352599144\n",
            "step: 40, loss: 0.0008816521149128675\n",
            "step: 50, loss: 0.007194503676146269\n",
            "step: 60, loss: 0.030436914414167404\n",
            "step: 70, loss: 0.011250643990933895\n",
            "step: 80, loss: 0.005541532766073942\n",
            "step: 90, loss: 0.02672652155160904\n",
            "step: 100, loss: 0.09522170573472977\n",
            "step: 110, loss: 0.045716818422079086\n",
            "step: 120, loss: 0.03921923786401749\n",
            "step: 130, loss: 0.045025069266557693\n",
            "step: 140, loss: 0.011015730910003185\n",
            "step: 150, loss: 0.12641146779060364\n",
            "step: 160, loss: 0.054140664637088776\n",
            "step: 170, loss: 0.003097235457971692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7422680412371134, f1=0.7609756097560975, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003318576840683818\n",
            "step: 10, loss: 0.010014811530709267\n",
            "step: 20, loss: 0.0015953043475747108\n",
            "step: 30, loss: 0.07498791813850403\n",
            "step: 40, loss: 0.08021169155836105\n",
            "step: 50, loss: 0.002198456786572933\n",
            "step: 60, loss: 0.007459302432835102\n",
            "step: 70, loss: 0.06645450741052628\n",
            "step: 80, loss: 0.00947557482868433\n",
            "step: 90, loss: 0.07866921275854111\n",
            "step: 100, loss: 0.013034412637352943\n",
            "step: 110, loss: 0.0027232717256993055\n",
            "step: 120, loss: 0.033557746559381485\n",
            "step: 130, loss: 0.11538013070821762\n",
            "step: 140, loss: 0.004078416153788567\n",
            "step: 150, loss: 0.017846936360001564\n",
            "step: 160, loss: 0.02682776376605034\n",
            "step: 170, loss: 0.06308139860630035\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7611940298507462, f1=0.7692307692307692, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11821132153272629\n",
            "step: 10, loss: 0.001950261415913701\n",
            "step: 20, loss: 0.024629101157188416\n",
            "step: 30, loss: 0.008167391642928123\n",
            "step: 40, loss: 0.10482414066791534\n",
            "step: 50, loss: 0.08426757156848907\n",
            "step: 60, loss: 0.00446031428873539\n",
            "step: 70, loss: 0.010079209692776203\n",
            "step: 80, loss: 0.00243981066159904\n",
            "step: 90, loss: 0.0033648137468844652\n",
            "step: 100, loss: 0.0004062523366883397\n",
            "step: 110, loss: 0.021684372797608376\n",
            "step: 120, loss: 0.0014780573546886444\n",
            "step: 130, loss: 0.0018274747999385\n",
            "step: 140, loss: 0.026687216013669968\n",
            "step: 150, loss: 0.010769705288112164\n",
            "step: 160, loss: 0.0013454610016196966\n",
            "step: 170, loss: 0.000859213003423065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.75, f1=0.7673860911270983, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05263297259807587\n",
            "step: 10, loss: 0.001451238989830017\n",
            "step: 20, loss: 0.04501296579837799\n",
            "step: 30, loss: 0.00026734848506748676\n",
            "step: 40, loss: 0.0024263744708150625\n",
            "step: 50, loss: 0.0024828764144331217\n",
            "step: 60, loss: 0.031704291701316833\n",
            "step: 70, loss: 0.004000208806246519\n",
            "step: 80, loss: 0.0006643740925937891\n",
            "step: 90, loss: 0.0009838631376624107\n",
            "step: 100, loss: 0.005930631421506405\n",
            "step: 110, loss: 0.03520653396844864\n",
            "step: 120, loss: 0.027372922748327255\n",
            "step: 130, loss: 0.000644715444650501\n",
            "step: 140, loss: 0.0003416293766349554\n",
            "step: 150, loss: 0.04957769066095352\n",
            "step: 160, loss: 0.008984719403088093\n",
            "step: 170, loss: 0.007320203818380833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7453083109919573, f1=0.7700258397932818, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009798840619623661\n",
            "step: 10, loss: 0.001533341477625072\n",
            "step: 20, loss: 0.00036513348459266126\n",
            "step: 30, loss: 0.005877891555428505\n",
            "step: 40, loss: 0.003430369310081005\n",
            "step: 50, loss: 0.0009122680057771504\n",
            "step: 60, loss: 0.0653616338968277\n",
            "step: 70, loss: 0.12522801756858826\n",
            "step: 80, loss: 0.0004261146823409945\n",
            "step: 90, loss: 0.00917025189846754\n",
            "step: 100, loss: 0.001363275689072907\n",
            "step: 110, loss: 0.006026356481015682\n",
            "step: 120, loss: 0.06634782999753952\n",
            "step: 130, loss: 0.06155085191130638\n",
            "step: 140, loss: 0.015953801572322845\n",
            "step: 150, loss: 0.009153414517641068\n",
            "step: 160, loss: 0.006962120532989502\n",
            "step: 170, loss: 0.0006493252003565431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7616580310880828, f1=0.7808564231738037, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004735400900244713\n",
            "step: 10, loss: 0.0026789596304297447\n",
            "step: 20, loss: 0.0038326061330735683\n",
            "step: 30, loss: 0.0009733536280691624\n",
            "step: 40, loss: 0.14263497292995453\n",
            "step: 50, loss: 0.004585574381053448\n",
            "step: 60, loss: 0.0055251410230994225\n",
            "step: 70, loss: 0.0016049528494477272\n",
            "step: 80, loss: 0.08292330056428909\n",
            "step: 90, loss: 0.0008202476892620325\n",
            "step: 100, loss: 0.04888594523072243\n",
            "step: 110, loss: 0.006725529674440622\n",
            "step: 120, loss: 0.017615996301174164\n",
            "step: 130, loss: 0.0007776060956530273\n",
            "step: 140, loss: 0.0016999561339616776\n",
            "step: 150, loss: 0.0013985224068164825\n",
            "step: 160, loss: 0.0036377795040607452\n",
            "step: 170, loss: 0.012030177749693394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7538461538461538, f1=0.7669172932330827, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004305372480303049\n",
            "step: 10, loss: 0.0006036568083800375\n",
            "step: 20, loss: 0.0005120151326991618\n",
            "step: 30, loss: 0.0021379271056503057\n",
            "step: 40, loss: 0.002469244645908475\n",
            "step: 50, loss: 0.24916177988052368\n",
            "step: 60, loss: 0.01537253800779581\n",
            "step: 70, loss: 0.006324264220893383\n",
            "step: 80, loss: 0.0016414436977356672\n",
            "step: 90, loss: 0.0006632033619098365\n",
            "step: 100, loss: 0.018117636442184448\n",
            "step: 110, loss: 0.002440334064885974\n",
            "step: 120, loss: 0.00960470549762249\n",
            "step: 130, loss: 0.0006607882678508759\n",
            "step: 140, loss: 0.0009716842323541641\n",
            "step: 150, loss: 0.0005770110874436796\n",
            "step: 160, loss: 0.004161078017205\n",
            "step: 170, loss: 0.0015841309214010835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7567567567567567, f1=0.7700258397932818, best_f1=0.7804878048780487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002805921249091625\n",
            "step: 10, loss: 0.003237007884308696\n",
            "step: 20, loss: 0.1292945295572281\n",
            "step: 30, loss: 0.0004776118730660528\n",
            "step: 40, loss: 0.005232604686170816\n",
            "step: 50, loss: 0.007661826908588409\n",
            "step: 60, loss: 0.11941757053136826\n",
            "step: 70, loss: 0.0007561847451142967\n",
            "step: 80, loss: 0.003127575619146228\n",
            "step: 90, loss: 0.0006350639159791172\n",
            "step: 100, loss: 0.0018022560980170965\n",
            "step: 110, loss: 0.0003326215664856136\n",
            "step: 120, loss: 0.002977433381602168\n",
            "step: 130, loss: 0.001593229710124433\n",
            "step: 140, loss: 0.0062127625569701195\n",
            "step: 150, loss: 0.00033644732320681214\n",
            "step: 160, loss: 0.050782959908246994\n",
            "step: 170, loss: 0.004557653795927763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7613941018766757, f1=0.7712082262210797, best_f1=0.7804878048780487\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 275.85it/s]\n",
            "load_f1 = 0.4511627906976744\n",
            "real_f1 = 0.448512585812357\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 248.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50343888-a531-4484-8916-4a1726d76bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6059665679931641\n",
            "step: 10, loss: 0.6404613852500916\n",
            "step: 20, loss: 0.4676644206047058\n",
            "step: 30, loss: 0.26009389758110046\n",
            "step: 40, loss: 0.1380099654197693\n",
            "step: 50, loss: 0.07081162929534912\n",
            "step: 60, loss: 0.12009942531585693\n",
            "step: 70, loss: 0.14359523355960846\n",
            "step: 80, loss: 0.08826525509357452\n",
            "step: 90, loss: 0.15043465793132782\n",
            "step: 100, loss: 0.003984111826866865\n",
            "step: 110, loss: 0.2046249806880951\n",
            "step: 120, loss: 0.021998027339577675\n",
            "step: 130, loss: 0.26383280754089355\n",
            "step: 140, loss: 0.016354447230696678\n",
            "step: 150, loss: 0.11166511476039886\n",
            "step: 160, loss: 0.00678722420707345\n",
            "step: 170, loss: 0.04700412228703499\n",
            "step: 180, loss: 0.018539119511842728\n",
            "step: 190, loss: 0.01659712754189968\n",
            "step: 200, loss: 0.08994735777378082\n",
            "step: 210, loss: 0.016111386939883232\n",
            "step: 220, loss: 0.024780653417110443\n",
            "step: 230, loss: 0.11063911020755768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9576837416481068, f1=0.962962962962963, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01464009564369917\n",
            "step: 10, loss: 0.007374242413789034\n",
            "step: 20, loss: 0.2673022449016571\n",
            "step: 30, loss: 0.11102664470672607\n",
            "step: 40, loss: 0.10980721563100815\n",
            "step: 50, loss: 0.008010907098650932\n",
            "step: 60, loss: 0.0030019194819033146\n",
            "step: 70, loss: 0.0338609479367733\n",
            "step: 80, loss: 0.0016365477349609137\n",
            "step: 90, loss: 0.11316360533237457\n",
            "step: 100, loss: 0.00510804820805788\n",
            "step: 110, loss: 0.17442892491817474\n",
            "step: 120, loss: 0.13757480680942535\n",
            "step: 130, loss: 0.029067570343613625\n",
            "step: 140, loss: 0.004009446129202843\n",
            "step: 150, loss: 0.010338693857192993\n",
            "step: 160, loss: 0.02354883775115013\n",
            "step: 170, loss: 0.028716951608657837\n",
            "step: 180, loss: 0.024133404716849327\n",
            "step: 190, loss: 0.005715614650398493\n",
            "step: 200, loss: 0.002703494392335415\n",
            "step: 210, loss: 0.007546085398644209\n",
            "step: 220, loss: 0.17336145043373108\n",
            "step: 230, loss: 0.03036997653543949\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9700996677740864, f1=0.9634551495016611, best_f1=0.9634551495016611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01562969572842121\n",
            "step: 10, loss: 0.010025249794125557\n",
            "step: 20, loss: 0.034901730716228485\n",
            "step: 30, loss: 0.002894091885536909\n",
            "step: 40, loss: 0.02577958069741726\n",
            "step: 50, loss: 0.02769869565963745\n",
            "step: 60, loss: 0.03202149271965027\n",
            "step: 70, loss: 0.0036791888996958733\n",
            "step: 80, loss: 0.0029583412688225508\n",
            "step: 90, loss: 0.11965889483690262\n",
            "step: 100, loss: 0.0023448735009878874\n",
            "step: 110, loss: 0.0010150948073714972\n",
            "step: 120, loss: 0.008106199093163013\n",
            "step: 130, loss: 0.0010735881514847279\n",
            "step: 140, loss: 0.016672976315021515\n",
            "step: 150, loss: 0.09489310532808304\n",
            "step: 160, loss: 0.14365561306476593\n",
            "step: 170, loss: 0.026025768369436264\n",
            "step: 180, loss: 0.03778831660747528\n",
            "step: 190, loss: 0.024897294119000435\n",
            "step: 200, loss: 0.04490743204951286\n",
            "step: 210, loss: 0.08929906040430069\n",
            "step: 220, loss: 0.05385292321443558\n",
            "step: 230, loss: 0.000660375808365643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9695603156708005, f1=0.96045197740113, best_f1=0.9634551495016611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002160192234441638\n",
            "step: 10, loss: 0.005252293311059475\n",
            "step: 20, loss: 0.0014124322915449739\n",
            "step: 30, loss: 0.0008963234140537679\n",
            "step: 40, loss: 0.030405765399336815\n",
            "step: 50, loss: 0.00039758719503879547\n",
            "step: 60, loss: 0.002599171129986644\n",
            "step: 70, loss: 0.015827715396881104\n",
            "step: 80, loss: 0.004400173202157021\n",
            "step: 90, loss: 0.012289407663047314\n",
            "step: 100, loss: 0.0019511538557708263\n",
            "step: 110, loss: 0.0070632100105285645\n",
            "step: 120, loss: 0.02369796670973301\n",
            "step: 130, loss: 0.009322659112513065\n",
            "step: 140, loss: 0.07659043371677399\n",
            "step: 150, loss: 0.018541615456342697\n",
            "step: 160, loss: 0.006357137113809586\n",
            "step: 170, loss: 0.007736450992524624\n",
            "step: 180, loss: 0.0009204774396494031\n",
            "step: 190, loss: 0.013496620580554008\n",
            "step: 200, loss: 0.05411477014422417\n",
            "step: 210, loss: 0.08828277885913849\n",
            "step: 220, loss: 0.0055483500473201275\n",
            "step: 230, loss: 0.012469667010009289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9720670391061451, f1=0.9675977653631285, best_f1=0.9675977653631285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009438898414373398\n",
            "step: 10, loss: 0.003041165880858898\n",
            "step: 20, loss: 0.01492415089160204\n",
            "step: 30, loss: 0.0003634895256254822\n",
            "step: 40, loss: 0.0007453294238075614\n",
            "step: 50, loss: 0.0005653211264871061\n",
            "step: 60, loss: 0.15008406341075897\n",
            "step: 70, loss: 0.008538118563592434\n",
            "step: 80, loss: 0.002334153512492776\n",
            "step: 90, loss: 0.0013679706025868654\n",
            "step: 100, loss: 0.0005269358516670763\n",
            "step: 110, loss: 0.0003310497268103063\n",
            "step: 120, loss: 0.0002329212293261662\n",
            "step: 130, loss: 0.03271283954381943\n",
            "step: 140, loss: 0.01712079532444477\n",
            "step: 150, loss: 0.0011333232978358865\n",
            "step: 160, loss: 0.000377634190954268\n",
            "step: 170, loss: 0.046493012458086014\n",
            "step: 180, loss: 0.0009630703716538846\n",
            "step: 190, loss: 0.12122143059968948\n",
            "step: 200, loss: 0.07485862076282501\n",
            "step: 210, loss: 0.01831107586622238\n",
            "step: 220, loss: 0.060126811265945435\n",
            "step: 230, loss: 0.011954915709793568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9665924276169264, f1=0.953125, best_f1=0.9675977653631285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.033643919974565506\n",
            "step: 10, loss: 0.0015716498019173741\n",
            "step: 20, loss: 0.004672353155910969\n",
            "step: 30, loss: 0.004044719971716404\n",
            "step: 40, loss: 0.0011746634263545275\n",
            "step: 50, loss: 0.02020263671875\n",
            "step: 60, loss: 0.06234704330563545\n",
            "step: 70, loss: 0.015422842465341091\n",
            "step: 80, loss: 0.005512411240488291\n",
            "step: 90, loss: 0.016706688329577446\n",
            "step: 100, loss: 0.03583085909485817\n",
            "step: 110, loss: 0.036874864250421524\n",
            "step: 120, loss: 0.009359107352793217\n",
            "step: 130, loss: 0.0004215784720145166\n",
            "step: 140, loss: 0.0010565277189016342\n",
            "step: 150, loss: 0.004057268146425486\n",
            "step: 160, loss: 0.0035414351150393486\n",
            "step: 170, loss: 0.00030566996429115534\n",
            "step: 180, loss: 0.03699813783168793\n",
            "step: 190, loss: 0.0012059350265190005\n",
            "step: 200, loss: 0.005312207154929638\n",
            "step: 210, loss: 0.00014439577353186905\n",
            "step: 220, loss: 0.00019234228238929063\n",
            "step: 230, loss: 0.029930923134088516\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9678135405105438, f1=0.9664429530201343, best_f1=0.9675977653631285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023730508983135223\n",
            "step: 10, loss: 0.0026272027753293514\n",
            "step: 20, loss: 0.0018624268705025315\n",
            "step: 30, loss: 0.030708065256476402\n",
            "step: 40, loss: 0.00489537138491869\n",
            "step: 50, loss: 0.00042081845458596945\n",
            "step: 60, loss: 0.002824316034093499\n",
            "step: 70, loss: 0.0017781464848667383\n",
            "step: 80, loss: 0.001766740926541388\n",
            "step: 90, loss: 0.00040264346171170473\n",
            "step: 100, loss: 0.001479164231568575\n",
            "step: 110, loss: 0.0013787953648716211\n",
            "step: 120, loss: 0.0010553153697401285\n",
            "step: 130, loss: 0.06271138787269592\n",
            "step: 140, loss: 0.0001714264799375087\n",
            "step: 150, loss: 0.003046442288905382\n",
            "step: 160, loss: 0.06339448690414429\n",
            "step: 170, loss: 0.0033410692121833563\n",
            "step: 180, loss: 0.004350162576884031\n",
            "step: 190, loss: 0.0005433941842056811\n",
            "step: 200, loss: 0.03151823952794075\n",
            "step: 210, loss: 0.00013163995754439384\n",
            "step: 220, loss: 0.03537433221936226\n",
            "step: 230, loss: 0.0015197627944871783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9744160177975528, f1=0.9594594594594594, best_f1=0.9594594594594594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004009914118796587\n",
            "step: 10, loss: 0.00018978433217853308\n",
            "step: 20, loss: 0.00747944600880146\n",
            "step: 30, loss: 0.0009242534288205206\n",
            "step: 40, loss: 0.06910267472267151\n",
            "step: 50, loss: 0.0005853584152646363\n",
            "step: 60, loss: 0.0001173363853013143\n",
            "step: 70, loss: 0.0010681254789233208\n",
            "step: 80, loss: 0.0009250364964827895\n",
            "step: 90, loss: 0.0001890470739454031\n",
            "step: 100, loss: 0.008949533104896545\n",
            "step: 110, loss: 0.0033564630430191755\n",
            "step: 120, loss: 0.04808538034558296\n",
            "step: 130, loss: 0.001680124318227172\n",
            "step: 140, loss: 0.00021497583657037467\n",
            "step: 150, loss: 0.00019692082423716784\n",
            "step: 160, loss: 0.00010224981815554202\n",
            "step: 170, loss: 0.007910053245723248\n",
            "step: 180, loss: 0.0006775293732061982\n",
            "step: 190, loss: 0.0015575801953673363\n",
            "step: 200, loss: 0.0012878201669082046\n",
            "step: 210, loss: 0.002923607360571623\n",
            "step: 220, loss: 0.0003843313897959888\n",
            "step: 230, loss: 0.00034746824530884624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.972972972972973, f1=0.9627118644067796, best_f1=0.9594594594594594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000133520967210643\n",
            "step: 10, loss: 0.00031968214898370206\n",
            "step: 20, loss: 0.005085027310997248\n",
            "step: 30, loss: 8.979460108093917e-05\n",
            "step: 40, loss: 0.004925978370010853\n",
            "step: 50, loss: 9.756039071362466e-05\n",
            "step: 60, loss: 0.00011222836474189535\n",
            "step: 70, loss: 0.0026877750642597675\n",
            "step: 80, loss: 0.0003419673885218799\n",
            "step: 90, loss: 0.0003066405770368874\n",
            "step: 100, loss: 0.0005112490034662187\n",
            "step: 110, loss: 7.679578266106546e-05\n",
            "step: 120, loss: 6.78139113006182e-05\n",
            "step: 130, loss: 0.00013889594993088394\n",
            "step: 140, loss: 5.4140771680977196e-05\n",
            "step: 150, loss: 0.0007886136882007122\n",
            "step: 160, loss: 7.312455272767693e-05\n",
            "step: 170, loss: 6.867134652566165e-05\n",
            "step: 180, loss: 0.0031838323920965195\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 190, loss: 0.00017814284365158528\n",
            "step: 200, loss: 0.000639508361928165\n",
            "step: 210, loss: 0.0020414958707988262\n",
            "step: 220, loss: 9.686785051599145e-05\n",
            "step: 230, loss: 0.0001606520381756127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9704545454545453, f1=0.954233409610984, best_f1=0.9594594594594594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013268926704768091\n",
            "step: 10, loss: 8.840108057484031e-05\n",
            "step: 20, loss: 7.30559549992904e-05\n",
            "step: 30, loss: 6.0211717936908826e-05\n",
            "step: 40, loss: 0.005582879763096571\n",
            "step: 50, loss: 0.013344711624085903\n",
            "step: 60, loss: 0.0007766852504573762\n",
            "step: 70, loss: 0.001950457226485014\n",
            "step: 80, loss: 5.0787108193617314e-05\n",
            "step: 90, loss: 0.0001129716620198451\n",
            "step: 100, loss: 7.048757834127173e-05\n",
            "step: 110, loss: 0.006471714470535517\n",
            "step: 120, loss: 5.371864972403273e-05\n",
            "step: 130, loss: 0.0005583566380664706\n",
            "step: 140, loss: 0.0477508120238781\n",
            "step: 150, loss: 0.027626564726233482\n",
            "step: 160, loss: 0.00013078092888463289\n",
            "step: 170, loss: 4.155694477958605e-05\n",
            "step: 180, loss: 0.00464747054502368\n",
            "step: 190, loss: 0.005307209677994251\n",
            "step: 200, loss: 3.385449235793203e-05\n",
            "step: 210, loss: 0.0013157343491911888\n",
            "step: 220, loss: 4.961088052368723e-05\n",
            "step: 230, loss: 0.00028239141101948917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9764837625979844, f1=0.9606299212598425, best_f1=0.9606299212598425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004844742361456156\n",
            "step: 10, loss: 0.0001295085676247254\n",
            "step: 20, loss: 0.0009309934102930129\n",
            "step: 30, loss: 0.058469660580158234\n",
            "step: 40, loss: 6.216600013431162e-05\n",
            "step: 50, loss: 0.00011094992805738002\n",
            "step: 60, loss: 0.0006607670220546424\n",
            "step: 70, loss: 7.172777986852452e-05\n",
            "step: 80, loss: 3.9805912820156664e-05\n",
            "step: 90, loss: 0.04817216470837593\n",
            "step: 100, loss: 0.00023055882775224745\n",
            "step: 110, loss: 0.0011924298014491796\n",
            "step: 120, loss: 0.0025864322669804096\n",
            "step: 130, loss: 8.969767077360302e-05\n",
            "step: 140, loss: 0.000369323039194569\n",
            "step: 150, loss: 0.016088994219899178\n",
            "step: 160, loss: 0.0002556624822318554\n",
            "step: 170, loss: 0.016479896381497383\n",
            "step: 180, loss: 0.004941195715218782\n",
            "step: 190, loss: 0.00012007569603156298\n",
            "step: 200, loss: 0.0022529461421072483\n",
            "step: 210, loss: 0.00015553658886346966\n",
            "step: 220, loss: 5.567157131736167e-05\n",
            "step: 230, loss: 5.821090599056333e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9741863075196409, f1=0.9638009049773756, best_f1=0.9606299212598425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.953742508310825e-05\n",
            "step: 10, loss: 0.00012363515270408243\n",
            "step: 20, loss: 0.0048257820308208466\n",
            "step: 30, loss: 0.0003420225693844259\n",
            "step: 40, loss: 0.000914964999537915\n",
            "step: 50, loss: 0.0034008612856268883\n",
            "step: 60, loss: 0.0001946217380464077\n",
            "step: 70, loss: 0.0039076171815395355\n",
            "step: 80, loss: 0.0001765187334967777\n",
            "step: 90, loss: 3.92262518289499e-05\n",
            "step: 100, loss: 5.565587343880907e-05\n",
            "step: 110, loss: 0.00011285937944194302\n",
            "step: 120, loss: 5.867262007086538e-05\n",
            "step: 130, loss: 0.0001426342932973057\n",
            "step: 140, loss: 0.00016118933854158968\n",
            "step: 150, loss: 0.00023972959024831653\n",
            "step: 160, loss: 0.00018672940495889634\n",
            "step: 170, loss: 0.00023758895986247808\n",
            "step: 180, loss: 0.0015851451316848397\n",
            "step: 190, loss: 0.00010999779624398798\n",
            "step: 200, loss: 8.693575364304706e-05\n",
            "step: 210, loss: 7.476213068002835e-05\n",
            "step: 220, loss: 0.0035878769122064114\n",
            "step: 230, loss: 0.0066480315290391445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.968609865470852, f1=0.9581920903954803, best_f1=0.9606299212598425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001435736776329577\n",
            "step: 10, loss: 0.001577515620738268\n",
            "step: 20, loss: 8.064211579039693e-05\n",
            "step: 30, loss: 0.00022122752852737904\n",
            "step: 40, loss: 9.904876060318202e-05\n",
            "step: 50, loss: 0.000190642531379126\n",
            "step: 60, loss: 0.002532264683395624\n",
            "step: 70, loss: 0.0003399299457669258\n",
            "step: 80, loss: 0.004071770701557398\n",
            "step: 90, loss: 0.000709510175511241\n",
            "step: 100, loss: 8.803256059763953e-05\n",
            "step: 110, loss: 0.002418093616142869\n",
            "step: 120, loss: 0.02491261251270771\n",
            "step: 130, loss: 0.00018116930732503533\n",
            "step: 140, loss: 0.0002108907065121457\n",
            "step: 150, loss: 5.563598097069189e-05\n",
            "step: 160, loss: 0.00017684789781924337\n",
            "step: 170, loss: 0.007233056239783764\n",
            "step: 180, loss: 6.16183242527768e-05\n",
            "step: 190, loss: 8.479761891067028e-05\n",
            "step: 200, loss: 0.0008639158331789076\n",
            "step: 210, loss: 3.745682624867186e-05\n",
            "step: 220, loss: 0.0002802997187245637\n",
            "step: 230, loss: 0.0070289866998791695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9753914988814317, f1=0.963882618510158, best_f1=0.9606299212598425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011892592010553926\n",
            "step: 10, loss: 0.0038785110227763653\n",
            "step: 20, loss: 0.0031633591279387474\n",
            "step: 30, loss: 0.001306579913944006\n",
            "step: 40, loss: 0.00015858470578677952\n",
            "step: 50, loss: 0.0002133284433512017\n",
            "step: 60, loss: 7.776783604640514e-05\n",
            "step: 70, loss: 0.00027437068638391793\n",
            "step: 80, loss: 6.646446126978844e-05\n",
            "step: 90, loss: 0.0001817553857108578\n",
            "step: 100, loss: 9.511989628663287e-05\n",
            "step: 110, loss: 0.10258117318153381\n",
            "step: 120, loss: 3.43090869137086e-05\n",
            "step: 130, loss: 8.786627586232498e-05\n",
            "step: 140, loss: 9.552011761115864e-05\n",
            "step: 150, loss: 5.917467569815926e-05\n",
            "step: 160, loss: 0.0021105040796101093\n",
            "step: 170, loss: 6.198737537488341e-05\n",
            "step: 180, loss: 0.00012047730706399307\n",
            "step: 190, loss: 6.495473644463345e-05\n",
            "step: 200, loss: 3.517682489473373e-05\n",
            "step: 210, loss: 0.0018499696161597967\n",
            "step: 220, loss: 0.00041394151048734784\n",
            "step: 230, loss: 0.003433352569118142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9752252252252253, f1=0.9603624009060023, best_f1=0.9606299212598425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.477695908164605e-05\n",
            "step: 10, loss: 7.884634396759793e-05\n",
            "step: 20, loss: 6.377466343110427e-05\n",
            "step: 30, loss: 0.00015348794113378972\n",
            "step: 40, loss: 0.00012688404240179807\n",
            "step: 50, loss: 0.0326264388859272\n",
            "step: 60, loss: 0.0001559123193146661\n",
            "step: 70, loss: 0.0008347387774847448\n",
            "step: 80, loss: 0.01020828913897276\n",
            "step: 90, loss: 0.0003663110837806016\n",
            "step: 100, loss: 7.735345570836216e-05\n",
            "step: 110, loss: 3.610464045777917e-05\n",
            "step: 120, loss: 0.0003101910697296262\n",
            "step: 130, loss: 6.442405720008537e-05\n",
            "step: 140, loss: 6.720339297316968e-05\n",
            "step: 150, loss: 0.00015907138003967702\n",
            "step: 160, loss: 5.80556588829495e-05\n",
            "step: 170, loss: 4.347569483797997e-05\n",
            "step: 180, loss: 0.0033641294576227665\n",
            "step: 190, loss: 0.0011643711477518082\n",
            "step: 200, loss: 0.00017331539129372686\n",
            "step: 210, loss: 9.8819138656836e-05\n",
            "step: 220, loss: 0.0006090162205509841\n",
            "step: 230, loss: 9.00161248864606e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9741863075196409, f1=0.96045197740113, best_f1=0.9606299212598425\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 204.99it/s]\n",
            "load_f1 = 0.9732739420935412\n",
            "real_f1 = 0.9732142857142857\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 249.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc8bdd5-09c2-4771-afa6-dc2bd3c137b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.61080002784729\n",
            "step: 10, loss: 0.5383345484733582\n",
            "step: 20, loss: 0.5422026515007019\n",
            "step: 30, loss: 0.24897408485412598\n",
            "step: 40, loss: 0.30064016580581665\n",
            "step: 50, loss: 0.15027444064617157\n",
            "step: 60, loss: 0.11627935618162155\n",
            "step: 70, loss: 0.11498306691646576\n",
            "step: 80, loss: 0.07837162166833878\n",
            "step: 90, loss: 0.35957029461860657\n",
            "step: 100, loss: 0.07693329453468323\n",
            "step: 110, loss: 0.15692129731178284\n",
            "step: 120, loss: 0.18612031638622284\n",
            "step: 130, loss: 0.11197390407323837\n",
            "step: 140, loss: 0.2053571343421936\n",
            "step: 150, loss: 0.09475703537464142\n",
            "step: 160, loss: 0.023135747760534286\n",
            "step: 170, loss: 0.27847379446029663\n",
            "step: 180, loss: 0.1729660928249359\n",
            "step: 190, loss: 0.013136213645339012\n",
            "step: 200, loss: 0.1757017970085144\n",
            "step: 210, loss: 0.07172787934541702\n",
            "step: 220, loss: 0.20855997502803802\n",
            "step: 230, loss: 0.13573120534420013\n",
            "step: 240, loss: 0.10399205982685089\n",
            "step: 250, loss: 0.08755986392498016\n",
            "step: 260, loss: 0.1352640688419342\n",
            "step: 270, loss: 0.02718198671936989\n",
            "step: 280, loss: 0.0708976536989212\n",
            "step: 290, loss: 0.2047608494758606\n",
            "step: 300, loss: 0.06678146123886108\n",
            "step: 310, loss: 0.2699015438556671\n",
            "step: 320, loss: 0.12573407590389252\n",
            "step: 330, loss: 0.12598513066768646\n",
            "step: 340, loss: 0.13168513774871826\n",
            "step: 350, loss: 0.15232130885124207\n",
            "step: 360, loss: 0.09870448708534241\n",
            "step: 370, loss: 0.15023164451122284\n",
            "step: 380, loss: 0.01960039511322975\n",
            "step: 390, loss: 0.17255382239818573\n",
            "step: 400, loss: 0.25480735301971436\n",
            "step: 410, loss: 0.043844617903232574\n",
            "step: 420, loss: 0.08254163712263107\n",
            "step: 430, loss: 0.16303107142448425\n",
            "step: 440, loss: 0.03942703828215599\n",
            "step: 450, loss: 0.019924890249967575\n",
            "step: 460, loss: 0.029005005955696106\n",
            "step: 470, loss: 0.12382644414901733\n",
            "step: 480, loss: 0.036348674446344376\n",
            "step: 490, loss: 0.1035243347287178\n",
            "step: 500, loss: 0.08613231033086777\n",
            "step: 510, loss: 0.08271219581365585\n",
            "step: 520, loss: 0.05813751742243767\n",
            "step: 530, loss: 0.009291013702750206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9163972286374134, f1=0.917516218721038, best_f1=0.917516218721038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20204319059848785\n",
            "step: 10, loss: 0.1262097954750061\n",
            "step: 20, loss: 0.0921221598982811\n",
            "step: 30, loss: 0.15858756005764008\n",
            "step: 40, loss: 0.10254906117916107\n",
            "step: 50, loss: 0.19348230957984924\n",
            "step: 60, loss: 0.051150087267160416\n",
            "step: 70, loss: 0.09015528112649918\n",
            "step: 80, loss: 0.1060400903224945\n",
            "step: 90, loss: 0.021531781181693077\n",
            "step: 100, loss: 0.0878034383058548\n",
            "step: 110, loss: 0.02726954221725464\n",
            "step: 120, loss: 0.24273303151130676\n",
            "step: 130, loss: 0.08173400908708572\n",
            "step: 140, loss: 0.051366906613111496\n",
            "step: 150, loss: 0.0993482768535614\n",
            "step: 160, loss: 0.1513783037662506\n",
            "step: 170, loss: 0.06704898923635483\n",
            "step: 180, loss: 0.054891109466552734\n",
            "step: 190, loss: 0.07453974336385727\n",
            "step: 200, loss: 0.03488297760486603\n",
            "step: 210, loss: 0.05707955360412598\n",
            "step: 220, loss: 0.07237949967384338\n",
            "step: 230, loss: 0.038091715425252914\n",
            "step: 240, loss: 0.06202524155378342\n",
            "step: 250, loss: 0.01808134652674198\n",
            "step: 260, loss: 0.008136662654578686\n",
            "step: 270, loss: 0.27872347831726074\n",
            "step: 280, loss: 0.07715898007154465\n",
            "step: 290, loss: 0.07250301539897919\n",
            "step: 300, loss: 0.1378878355026245\n",
            "step: 310, loss: 0.013047021813690662\n",
            "step: 320, loss: 0.2227075695991516\n",
            "step: 330, loss: 0.10023956745862961\n",
            "step: 340, loss: 0.04766424745321274\n",
            "step: 350, loss: 0.0017953314818441868\n",
            "step: 360, loss: 0.04951445385813713\n",
            "step: 370, loss: 0.1576216220855713\n",
            "step: 380, loss: 0.04148713871836662\n",
            "step: 390, loss: 0.14564065635204315\n",
            "step: 400, loss: 0.014297065325081348\n",
            "step: 410, loss: 0.04907683655619621\n",
            "step: 420, loss: 0.09385883063077927\n",
            "step: 430, loss: 0.020085565745830536\n",
            "step: 440, loss: 0.0867990255355835\n",
            "step: 450, loss: 0.06615625321865082\n",
            "step: 460, loss: 0.018882635980844498\n",
            "step: 470, loss: 0.013921651057898998\n",
            "step: 480, loss: 0.19423410296440125\n",
            "step: 490, loss: 0.04777717962861061\n",
            "step: 500, loss: 0.26497894525527954\n",
            "step: 510, loss: 0.03894959017634392\n",
            "step: 520, loss: 0.1290690153837204\n",
            "step: 530, loss: 0.08920522779226303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9197012138188609, f1=0.9212233549582948, best_f1=0.9212233549582948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09963775426149368\n",
            "step: 10, loss: 0.06658117473125458\n",
            "step: 20, loss: 0.06964298337697983\n",
            "step: 30, loss: 0.04161316901445389\n",
            "step: 40, loss: 0.015757402405142784\n",
            "step: 50, loss: 0.16876521706581116\n",
            "step: 60, loss: 0.04282680153846741\n",
            "step: 70, loss: 0.0025942493230104446\n",
            "step: 80, loss: 0.002261054702103138\n",
            "step: 90, loss: 0.008065884932875633\n",
            "step: 100, loss: 0.019018882885575294\n",
            "step: 110, loss: 0.0031959409825503826\n",
            "step: 120, loss: 0.008375609293580055\n",
            "step: 130, loss: 0.008090573363006115\n",
            "step: 140, loss: 0.007735856808722019\n",
            "step: 150, loss: 0.018580669537186623\n",
            "step: 160, loss: 0.026295211166143417\n",
            "step: 170, loss: 0.13152463734149933\n",
            "step: 180, loss: 0.0281673576682806\n",
            "step: 190, loss: 0.012579384259879589\n",
            "step: 200, loss: 0.01760731264948845\n",
            "step: 210, loss: 0.031072836369276047\n",
            "step: 220, loss: 0.0322861485183239\n",
            "step: 230, loss: 0.09881209582090378\n",
            "step: 240, loss: 0.017911860719323158\n",
            "step: 250, loss: 0.05792003870010376\n",
            "step: 260, loss: 0.028834166005253792\n",
            "step: 270, loss: 0.006576668005436659\n",
            "step: 280, loss: 0.04272131621837616\n",
            "step: 290, loss: 0.006678635720163584\n",
            "step: 300, loss: 0.006574713625013828\n",
            "step: 310, loss: 0.09028933197259903\n",
            "step: 320, loss: 0.01748690754175186\n",
            "step: 330, loss: 0.02259197272360325\n",
            "step: 340, loss: 0.007899956777691841\n",
            "step: 350, loss: 0.20731481909751892\n",
            "step: 360, loss: 0.06606235355138779\n",
            "step: 370, loss: 0.03412339463829994\n",
            "step: 380, loss: 0.009790303185582161\n",
            "step: 390, loss: 0.02077101729810238\n",
            "step: 400, loss: 0.03901120275259018\n",
            "step: 410, loss: 0.018152492120862007\n",
            "step: 420, loss: 0.29863956570625305\n",
            "step: 430, loss: 0.18568851053714752\n",
            "step: 440, loss: 0.02233223058283329\n",
            "step: 450, loss: 0.09131369739770889\n",
            "step: 460, loss: 0.07400676608085632\n",
            "step: 470, loss: 0.13872259855270386\n",
            "step: 480, loss: 0.006677550729364157\n",
            "step: 490, loss: 0.006065468303859234\n",
            "step: 500, loss: 0.1174003928899765\n",
            "step: 510, loss: 0.010079119354486465\n",
            "step: 520, loss: 0.10296302288770676\n",
            "step: 530, loss: 0.2978144586086273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9231468849477036, f1=0.9223785746709033, best_f1=0.9223785746709033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013866585679352283\n",
            "step: 10, loss: 0.0033226991072297096\n",
            "step: 20, loss: 0.005799619946628809\n",
            "step: 30, loss: 0.01471307035535574\n",
            "step: 40, loss: 0.042139846831560135\n",
            "step: 50, loss: 0.045392412692308426\n",
            "step: 60, loss: 0.019685864448547363\n",
            "step: 70, loss: 0.010254059918224812\n",
            "step: 80, loss: 0.012907734140753746\n",
            "step: 90, loss: 0.14884167909622192\n",
            "step: 100, loss: 0.008325847797095776\n",
            "step: 110, loss: 0.16734108328819275\n",
            "step: 120, loss: 0.003086691489443183\n",
            "step: 130, loss: 0.00670077558606863\n",
            "step: 140, loss: 0.00750345503911376\n",
            "step: 150, loss: 0.07432851940393448\n",
            "step: 160, loss: 0.07939630001783371\n",
            "step: 170, loss: 0.0044620041735470295\n",
            "step: 180, loss: 0.10577023029327393\n",
            "step: 190, loss: 0.04224696755409241\n",
            "step: 200, loss: 0.046906013041734695\n",
            "step: 210, loss: 0.09913991391658783\n",
            "step: 220, loss: 0.04878959804773331\n",
            "step: 230, loss: 0.190564826130867\n",
            "step: 240, loss: 0.016304923221468925\n",
            "step: 250, loss: 0.04683208838105202\n",
            "step: 260, loss: 0.010102048516273499\n",
            "step: 270, loss: 0.011525842361152172\n",
            "step: 280, loss: 0.03712131455540657\n",
            "step: 290, loss: 0.02570868283510208\n",
            "step: 300, loss: 0.0012380675179883838\n",
            "step: 310, loss: 0.0034196788910776377\n",
            "step: 320, loss: 0.014552201144397259\n",
            "step: 330, loss: 0.0279754176735878\n",
            "step: 340, loss: 0.1538609117269516\n",
            "step: 350, loss: 0.08477845788002014\n",
            "step: 360, loss: 0.02728867344558239\n",
            "step: 370, loss: 0.006539309863001108\n",
            "step: 380, loss: 0.00952373631298542\n",
            "step: 390, loss: 0.025081831961870193\n",
            "step: 400, loss: 0.007101984228938818\n",
            "step: 410, loss: 0.0015666221734136343\n",
            "step: 420, loss: 0.020426135510206223\n",
            "step: 430, loss: 0.10110122710466385\n",
            "step: 440, loss: 0.017600443214178085\n",
            "step: 450, loss: 0.008612890727818012\n",
            "step: 460, loss: 0.11631982028484344\n",
            "step: 470, loss: 0.08710470050573349\n",
            "step: 480, loss: 0.13474102318286896\n",
            "step: 490, loss: 0.026089996099472046\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 500, loss: 0.005323675461113453\n",
            "step: 510, loss: 0.04967121034860611\n",
            "step: 520, loss: 0.06416371464729309\n",
            "step: 530, loss: 0.05516371130943298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9232209737827715, f1=0.9215777262180975, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19147244095802307\n",
            "step: 10, loss: 0.04553268477320671\n",
            "step: 20, loss: 0.02001550793647766\n",
            "step: 30, loss: 0.0027934922836720943\n",
            "step: 40, loss: 0.348354697227478\n",
            "step: 50, loss: 0.003283468075096607\n",
            "step: 60, loss: 0.09874971956014633\n",
            "step: 70, loss: 0.00728023424744606\n",
            "step: 80, loss: 0.0057103391736745834\n",
            "step: 90, loss: 0.0012580023612827063\n",
            "step: 100, loss: 0.005308025516569614\n",
            "step: 110, loss: 0.0017291426192969084\n",
            "step: 120, loss: 0.01208504382520914\n",
            "step: 130, loss: 0.04489850252866745\n",
            "step: 140, loss: 0.07966072112321854\n",
            "step: 150, loss: 0.020449042320251465\n",
            "step: 160, loss: 0.0010528727434575558\n",
            "step: 170, loss: 0.034018248319625854\n",
            "step: 180, loss: 0.0009106052457354963\n",
            "step: 190, loss: 0.0007215795340016484\n",
            "step: 200, loss: 0.004692615941166878\n",
            "step: 210, loss: 0.019158659502863884\n",
            "step: 220, loss: 0.017539892345666885\n",
            "step: 230, loss: 0.009370116516947746\n",
            "step: 240, loss: 0.03490918129682541\n",
            "step: 250, loss: 0.012273614294826984\n",
            "step: 260, loss: 0.012461233884096146\n",
            "step: 270, loss: 0.008918561972677708\n",
            "step: 280, loss: 0.007471226621419191\n",
            "step: 290, loss: 0.13334642350673676\n",
            "step: 300, loss: 0.004853709600865841\n",
            "step: 310, loss: 0.0003717902582138777\n",
            "step: 320, loss: 0.1339874565601349\n",
            "step: 330, loss: 0.10030553489923477\n",
            "step: 340, loss: 0.010979698039591312\n",
            "step: 350, loss: 0.005454869940876961\n",
            "step: 360, loss: 0.0097414031624794\n",
            "step: 370, loss: 0.06029887497425079\n",
            "step: 380, loss: 0.0010281488066539168\n",
            "step: 390, loss: 0.0016109523130580783\n",
            "step: 400, loss: 0.04843376576900482\n",
            "step: 410, loss: 0.004589509218931198\n",
            "step: 420, loss: 0.0033689034171402454\n",
            "step: 430, loss: 0.09239795804023743\n",
            "step: 440, loss: 0.11264947056770325\n",
            "step: 450, loss: 0.08354994654655457\n",
            "step: 460, loss: 0.22766190767288208\n",
            "step: 470, loss: 0.09306012839078903\n",
            "step: 480, loss: 0.11272281408309937\n",
            "step: 490, loss: 0.0020758535247296095\n",
            "step: 500, loss: 0.09462425112724304\n",
            "step: 510, loss: 0.013783513568341732\n",
            "step: 520, loss: 0.0024662932846695185\n",
            "step: 530, loss: 0.09182481467723846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9156398104265402, f1=0.9121813031161473, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006694080657325685\n",
            "step: 10, loss: 0.08799700438976288\n",
            "step: 20, loss: 0.0847768783569336\n",
            "step: 30, loss: 0.0008465080754831433\n",
            "step: 40, loss: 0.01912321150302887\n",
            "step: 50, loss: 0.005272553768008947\n",
            "step: 60, loss: 0.00043224546243436635\n",
            "step: 70, loss: 0.005195829551666975\n",
            "step: 80, loss: 0.005484360735863447\n",
            "step: 90, loss: 0.027921605855226517\n",
            "step: 100, loss: 0.11614281684160233\n",
            "step: 110, loss: 0.010088891722261906\n",
            "step: 120, loss: 0.005826218519359827\n",
            "step: 130, loss: 0.1323533058166504\n",
            "step: 140, loss: 0.032076939940452576\n",
            "step: 150, loss: 0.010544191114604473\n",
            "step: 160, loss: 0.004344886168837547\n",
            "step: 170, loss: 0.005307772662490606\n",
            "step: 180, loss: 0.006589924916625023\n",
            "step: 190, loss: 0.003623527940362692\n",
            "step: 200, loss: 0.0007314891554415226\n",
            "step: 210, loss: 0.01744513213634491\n",
            "step: 220, loss: 0.0033021203707903624\n",
            "step: 230, loss: 0.004522337578237057\n",
            "step: 240, loss: 0.016802135854959488\n",
            "step: 250, loss: 0.003082931274548173\n",
            "step: 260, loss: 0.00034006620990112424\n",
            "step: 270, loss: 0.1717025190591812\n",
            "step: 280, loss: 0.010088647715747356\n",
            "step: 290, loss: 0.00073367974255234\n",
            "step: 300, loss: 0.0024367249570786953\n",
            "step: 310, loss: 0.013496045023202896\n",
            "step: 320, loss: 0.0017502796836197376\n",
            "step: 330, loss: 0.0005428731674328446\n",
            "step: 340, loss: 0.028255749493837357\n",
            "step: 350, loss: 0.0007223719730973244\n",
            "step: 360, loss: 0.0055933184921741486\n",
            "step: 370, loss: 0.025650236755609512\n",
            "step: 380, loss: 0.0016168570145964622\n",
            "step: 390, loss: 0.026640158146619797\n",
            "step: 400, loss: 0.12957216799259186\n",
            "step: 410, loss: 0.0016910588601604104\n",
            "step: 420, loss: 0.0036215982399880886\n",
            "step: 430, loss: 0.0016898212488740683\n",
            "step: 440, loss: 0.02370426058769226\n",
            "step: 450, loss: 0.0029501451645046473\n",
            "step: 460, loss: 0.007050669752061367\n",
            "step: 470, loss: 0.0012032013619318604\n",
            "step: 480, loss: 0.0010656840167939663\n",
            "step: 490, loss: 0.002080545062199235\n",
            "step: 500, loss: 0.0003974305291194469\n",
            "step: 510, loss: 0.007140572648495436\n",
            "step: 520, loss: 0.006157559342682362\n",
            "step: 530, loss: 0.005808741319924593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9179869524697111, f1=0.9188191881918819, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033956810366362333\n",
            "step: 10, loss: 0.008805890567600727\n",
            "step: 20, loss: 0.001622610492631793\n",
            "step: 30, loss: 0.0032335345167666674\n",
            "step: 40, loss: 0.003191061085090041\n",
            "step: 50, loss: 0.0408569797873497\n",
            "step: 60, loss: 0.030292915180325508\n",
            "step: 70, loss: 0.007339250296354294\n",
            "step: 80, loss: 0.008245952427387238\n",
            "step: 90, loss: 0.0008941049454733729\n",
            "step: 100, loss: 0.0012442806037142873\n",
            "step: 110, loss: 0.0022866916842758656\n",
            "step: 120, loss: 0.0026555813383311033\n",
            "step: 130, loss: 0.010252964682877064\n",
            "step: 140, loss: 0.00023607250477652997\n",
            "step: 150, loss: 0.0018414624501019716\n",
            "step: 160, loss: 0.0004129499429836869\n",
            "step: 170, loss: 0.00016585973207838833\n",
            "step: 180, loss: 0.0008858799701556563\n",
            "step: 190, loss: 0.008328258991241455\n",
            "step: 200, loss: 0.00031627362477593124\n",
            "step: 210, loss: 0.1316833645105362\n",
            "step: 220, loss: 0.0009344404097646475\n",
            "step: 230, loss: 0.0010100039653480053\n",
            "step: 240, loss: 0.001596852787770331\n",
            "step: 250, loss: 0.012612021528184414\n",
            "step: 260, loss: 0.0002410964953014627\n",
            "step: 270, loss: 0.0010259223636239767\n",
            "step: 280, loss: 0.0008819462964311242\n",
            "step: 290, loss: 0.00021817948436364532\n",
            "step: 300, loss: 0.003135227831080556\n",
            "step: 310, loss: 0.00012282362149562687\n",
            "step: 320, loss: 0.0003854755777865648\n",
            "step: 330, loss: 0.0007413364946842194\n",
            "step: 340, loss: 0.041635047644376755\n",
            "step: 350, loss: 0.05031201243400574\n",
            "step: 360, loss: 0.008572510443627834\n",
            "step: 370, loss: 0.005722240079194307\n",
            "step: 380, loss: 0.06869616359472275\n",
            "step: 390, loss: 0.004710833076387644\n",
            "step: 400, loss: 0.0009402727009728551\n",
            "step: 410, loss: 0.002043582499027252\n",
            "step: 420, loss: 0.0009896145202219486\n",
            "step: 430, loss: 0.0004466413811314851\n",
            "step: 440, loss: 0.0007364929188042879\n",
            "step: 450, loss: 0.0005073898937553167\n",
            "step: 460, loss: 0.0009749869350343943\n",
            "step: 470, loss: 0.033625878393650055\n",
            "step: 480, loss: 0.06148655340075493\n",
            "step: 490, loss: 0.004627833608537912\n",
            "step: 500, loss: 0.0019301821012049913\n",
            "step: 510, loss: 0.0030717544723302126\n",
            "step: 520, loss: 0.03718307986855507\n",
            "step: 530, loss: 0.0020051097963005304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9206927985414768, f1=0.9190172884440401, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015243538655340672\n",
            "step: 10, loss: 0.021935539320111275\n",
            "step: 20, loss: 0.00033364040427841246\n",
            "step: 30, loss: 0.1490696519613266\n",
            "step: 40, loss: 0.0004163348057772964\n",
            "step: 50, loss: 0.004737258423119783\n",
            "step: 60, loss: 0.0005375686450861394\n",
            "step: 70, loss: 0.017557473853230476\n",
            "step: 80, loss: 0.006235751789063215\n",
            "step: 90, loss: 0.0006075531127862632\n",
            "step: 100, loss: 0.0001418153551639989\n",
            "step: 110, loss: 0.0026327345985919237\n",
            "step: 120, loss: 0.003116433508694172\n",
            "step: 130, loss: 0.0008631575619801879\n",
            "step: 140, loss: 0.008319554850459099\n",
            "step: 150, loss: 0.024342363700270653\n",
            "step: 160, loss: 0.006006488110870123\n",
            "step: 170, loss: 0.007557553704828024\n",
            "step: 180, loss: 0.002085980260744691\n",
            "step: 190, loss: 0.0035200335551053286\n",
            "step: 200, loss: 0.0067378925159573555\n",
            "step: 210, loss: 0.0023958224337548018\n",
            "step: 220, loss: 0.002016672631725669\n",
            "step: 230, loss: 0.0007380839088000357\n",
            "step: 240, loss: 0.0034803643357008696\n",
            "step: 250, loss: 0.0358218215405941\n",
            "step: 260, loss: 0.014104806818068027\n",
            "step: 270, loss: 0.0031639854423701763\n",
            "step: 280, loss: 0.001823317026719451\n",
            "step: 290, loss: 0.03380144014954567\n",
            "step: 300, loss: 0.00015479527064599097\n",
            "step: 310, loss: 0.0011821351945400238\n",
            "step: 320, loss: 0.001139625790528953\n",
            "step: 330, loss: 0.005863376893103123\n",
            "step: 340, loss: 0.0006096737924963236\n",
            "step: 350, loss: 0.020136820152401924\n",
            "step: 360, loss: 0.005018983036279678\n",
            "step: 370, loss: 0.0004056965699419379\n",
            "step: 380, loss: 0.0022599624935537577\n",
            "step: 390, loss: 0.05368902534246445\n",
            "step: 400, loss: 0.002450767904520035\n",
            "step: 410, loss: 0.0006499304436147213\n",
            "step: 420, loss: 0.0017620909493416548\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 430, loss: 0.035330381244421005\n",
            "step: 440, loss: 0.014528223313391209\n",
            "step: 450, loss: 0.003125741146504879\n",
            "step: 460, loss: 0.004184287041425705\n",
            "step: 470, loss: 0.004488442558795214\n",
            "step: 480, loss: 0.00262534455396235\n",
            "step: 490, loss: 0.006643014494329691\n",
            "step: 500, loss: 0.0017820637440308928\n",
            "step: 510, loss: 0.0035839430056512356\n",
            "step: 520, loss: 0.0011852115858346224\n",
            "step: 530, loss: 0.0007443996146321297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9162790697674419, f1=0.9242843951985227, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021525609772652388\n",
            "step: 10, loss: 0.0001997842191485688\n",
            "step: 20, loss: 0.017205040901899338\n",
            "step: 30, loss: 0.0005792456795461476\n",
            "step: 40, loss: 0.0002062551211565733\n",
            "step: 50, loss: 7.239510887302458e-05\n",
            "step: 60, loss: 0.0001725324837025255\n",
            "step: 70, loss: 0.0001371382677461952\n",
            "step: 80, loss: 0.0008196740527637303\n",
            "step: 90, loss: 6.956225115573034e-05\n",
            "step: 100, loss: 0.0013100190553814173\n",
            "step: 110, loss: 7.26299622328952e-05\n",
            "step: 120, loss: 0.0014849354047328234\n",
            "step: 130, loss: 0.11121488362550735\n",
            "step: 140, loss: 0.001695145620033145\n",
            "step: 150, loss: 6.136237789178267e-05\n",
            "step: 160, loss: 0.0014976896345615387\n",
            "step: 170, loss: 0.01946667768061161\n",
            "step: 180, loss: 9.834948286879808e-05\n",
            "step: 190, loss: 0.11007821559906006\n",
            "step: 200, loss: 0.013028758578002453\n",
            "step: 210, loss: 8.327340765390545e-05\n",
            "step: 220, loss: 0.012062161229550838\n",
            "step: 230, loss: 0.007641766685992479\n",
            "step: 240, loss: 0.0005839609657414258\n",
            "step: 250, loss: 0.0017550501506775618\n",
            "step: 260, loss: 0.003742692992091179\n",
            "step: 270, loss: 0.0013317072298377752\n",
            "step: 280, loss: 0.00010087835835292935\n",
            "step: 290, loss: 0.02664172649383545\n",
            "step: 300, loss: 6.699084042338654e-05\n",
            "step: 310, loss: 0.008692469447851181\n",
            "step: 320, loss: 0.0007810689858160913\n",
            "step: 330, loss: 0.000710830558091402\n",
            "step: 340, loss: 0.0006137603195384145\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 350, loss: 0.01568085141479969\n",
            "step: 360, loss: 4.780217204825021e-05\n",
            "step: 370, loss: 0.00044553185580298305\n",
            "step: 380, loss: 6.348468014039099e-05\n",
            "step: 390, loss: 0.00032833300065249205\n",
            "step: 400, loss: 0.0009033515816554427\n",
            "step: 410, loss: 0.00021522477618418634\n",
            "step: 420, loss: 0.0001234511291841045\n",
            "step: 430, loss: 0.0002709035179577768\n",
            "step: 440, loss: 0.0009226789698004723\n",
            "step: 450, loss: 0.0009364402503706515\n",
            "step: 460, loss: 0.0009379658149555326\n",
            "step: 470, loss: 0.016788868233561516\n",
            "step: 480, loss: 0.01588280498981476\n",
            "step: 490, loss: 0.004245594143867493\n",
            "step: 500, loss: 0.0025770440697669983\n",
            "step: 510, loss: 0.00022311195789370686\n",
            "step: 520, loss: 0.00039774354081600904\n",
            "step: 530, loss: 0.00019451286061666906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9164743885556068, f1=0.9202566452795601, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027291924925521016\n",
            "step: 10, loss: 0.00019193108892068267\n",
            "step: 20, loss: 0.00011109344632131979\n",
            "step: 30, loss: 4.44293582404498e-05\n",
            "step: 40, loss: 0.0002961991704069078\n",
            "step: 50, loss: 0.0019619239028543234\n",
            "step: 60, loss: 0.0068040881305933\n",
            "step: 70, loss: 0.00014185119653120637\n",
            "step: 80, loss: 0.0022264255676418543\n",
            "step: 90, loss: 0.0005015474162064493\n",
            "step: 100, loss: 0.0007721108268015087\n",
            "step: 110, loss: 0.001803336781449616\n",
            "step: 120, loss: 0.0011198935098946095\n",
            "step: 130, loss: 0.0002930292976088822\n",
            "step: 140, loss: 0.005612341687083244\n",
            "step: 150, loss: 0.25187948346138\n",
            "step: 160, loss: 0.0007090115104801953\n",
            "step: 170, loss: 0.012580416165292263\n",
            "step: 180, loss: 0.001482407096773386\n",
            "step: 190, loss: 0.002418265212327242\n",
            "step: 200, loss: 0.00044505661935545504\n",
            "step: 210, loss: 0.00018751119205262512\n",
            "step: 220, loss: 0.0034412285313010216\n",
            "step: 230, loss: 0.004826510325074196\n",
            "step: 240, loss: 0.0020293141715228558\n",
            "step: 250, loss: 0.00012497109128162265\n",
            "step: 260, loss: 0.001707238145172596\n",
            "step: 270, loss: 0.0025163600221276283\n",
            "step: 280, loss: 0.0006627935217693448\n",
            "step: 290, loss: 0.0003648278070613742\n",
            "step: 300, loss: 0.00013216091610956937\n",
            "step: 310, loss: 0.0001964621478691697\n",
            "step: 320, loss: 0.0014955932274460793\n",
            "step: 330, loss: 0.000331792893121019\n",
            "step: 340, loss: 0.0008968464098870754\n",
            "step: 350, loss: 0.0006174652953632176\n",
            "step: 360, loss: 0.005673261359333992\n",
            "step: 370, loss: 0.0007976102060638368\n",
            "step: 380, loss: 0.005921169184148312\n",
            "step: 390, loss: 0.002032096032053232\n",
            "step: 400, loss: 0.0006449001957662404\n",
            "step: 410, loss: 0.002782230731099844\n",
            "step: 420, loss: 0.0006281029782257974\n",
            "step: 430, loss: 0.00010919332999037579\n",
            "step: 440, loss: 0.00020162694272585213\n",
            "step: 450, loss: 0.024738803505897522\n",
            "step: 460, loss: 0.008463197387754917\n",
            "step: 470, loss: 0.000278828723821789\n",
            "step: 480, loss: 0.0004085532855242491\n",
            "step: 490, loss: 0.0009796854574233294\n",
            "step: 500, loss: 0.033432673662900925\n",
            "step: 510, loss: 5.948938269284554e-05\n",
            "step: 520, loss: 0.0004933146410621703\n",
            "step: 530, loss: 0.0022473011631518602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9172033118675251, f1=0.9155963302752294, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004086287517566234\n",
            "step: 10, loss: 0.01301321666687727\n",
            "step: 20, loss: 0.001207307679578662\n",
            "step: 30, loss: 0.0012507513165473938\n",
            "step: 40, loss: 0.0006665317341685295\n",
            "step: 50, loss: 0.0022278791293501854\n",
            "step: 60, loss: 0.011998087167739868\n",
            "step: 70, loss: 0.0727248266339302\n",
            "step: 80, loss: 0.002681497484445572\n",
            "step: 90, loss: 0.00044941456872038543\n",
            "step: 100, loss: 0.00036087504122406244\n",
            "step: 110, loss: 0.0011035241186618805\n",
            "step: 120, loss: 0.002143182558938861\n",
            "step: 130, loss: 0.0004358085279818624\n",
            "step: 140, loss: 0.07179993391036987\n",
            "step: 150, loss: 0.0001383048656862229\n",
            "step: 160, loss: 0.0002239429159089923\n",
            "step: 170, loss: 0.00030307675478979945\n",
            "step: 180, loss: 0.0011674920096993446\n",
            "step: 190, loss: 0.0006041631568223238\n",
            "step: 200, loss: 0.010846195742487907\n",
            "step: 210, loss: 0.0002193587861256674\n",
            "step: 220, loss: 0.0037538865581154823\n",
            "step: 230, loss: 0.00039608628139831126\n",
            "step: 240, loss: 0.0003263847902417183\n",
            "step: 250, loss: 6.14236414548941e-05\n",
            "step: 260, loss: 4.122109749005176e-05\n",
            "step: 270, loss: 0.0015926264459267259\n",
            "step: 280, loss: 0.000324654538417235\n",
            "step: 290, loss: 0.007876588962972164\n",
            "step: 300, loss: 0.0012890022480860353\n",
            "step: 310, loss: 0.00040184918907471\n",
            "step: 320, loss: 4.7400255425600335e-05\n",
            "step: 330, loss: 8.432790491497144e-05\n",
            "step: 340, loss: 0.0001989943120861426\n",
            "step: 350, loss: 0.0008296450832858682\n",
            "step: 360, loss: 0.00012427335605025291\n",
            "step: 370, loss: 0.00032122741686180234\n",
            "step: 380, loss: 0.00022537099721375853\n",
            "step: 390, loss: 0.00011750697740353644\n",
            "step: 400, loss: 0.0001074769752449356\n",
            "step: 410, loss: 0.0060130637139081955\n",
            "step: 420, loss: 0.0004319415311329067\n",
            "step: 430, loss: 7.387444202322513e-05\n",
            "step: 440, loss: 0.000959812372457236\n",
            "step: 450, loss: 0.00010046276292996481\n",
            "step: 460, loss: 0.13782931864261627\n",
            "step: 470, loss: 0.00022170953161548823\n",
            "step: 480, loss: 0.002544717863202095\n",
            "step: 490, loss: 0.002089774701744318\n",
            "step: 500, loss: 0.00017467650468461215\n",
            "step: 510, loss: 0.03620464727282524\n",
            "step: 520, loss: 0.00014531788474414498\n",
            "step: 530, loss: 0.0437837578356266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9222846441947565, f1=0.9133271202236719, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014268382219597697\n",
            "step: 10, loss: 0.00029826484387740493\n",
            "step: 20, loss: 0.00016141746891662478\n",
            "step: 30, loss: 0.043890777975320816\n",
            "step: 40, loss: 0.00019799704023171216\n",
            "step: 50, loss: 0.19533772766590118\n",
            "step: 60, loss: 0.001383391791023314\n",
            "step: 70, loss: 0.0014540111878886819\n",
            "step: 80, loss: 0.00011512244236655533\n",
            "step: 90, loss: 0.0003890385269187391\n",
            "step: 100, loss: 0.01821916177868843\n",
            "step: 110, loss: 7.366965292021632e-05\n",
            "step: 120, loss: 0.11653763055801392\n",
            "step: 130, loss: 7.041706703603268e-05\n",
            "step: 140, loss: 9.579704055795446e-05\n",
            "step: 150, loss: 2.1535497580771334e-05\n",
            "step: 160, loss: 6.49096182314679e-05\n",
            "step: 170, loss: 0.030184239149093628\n",
            "step: 180, loss: 0.00013230566401034594\n",
            "step: 190, loss: 0.00013988104183226824\n",
            "step: 200, loss: 0.00040043049375526607\n",
            "step: 210, loss: 0.0018766806460916996\n",
            "step: 220, loss: 8.192887617042288e-05\n",
            "step: 230, loss: 4.5742886868538335e-05\n",
            "step: 240, loss: 0.0012015430256724358\n",
            "step: 250, loss: 0.00034254734055139124\n",
            "step: 260, loss: 4.6560351620428264e-05\n",
            "step: 270, loss: 0.0010338915744796395\n",
            "step: 280, loss: 0.00031812043744139373\n",
            "step: 290, loss: 0.0009428983321413398\n",
            "step: 300, loss: 0.05172441154718399\n",
            "step: 310, loss: 0.00010797494906000793\n",
            "step: 320, loss: 0.00011194371472811326\n",
            "step: 330, loss: 0.0001569471787661314\n",
            "step: 340, loss: 0.00026301087928004563\n",
            "step: 350, loss: 0.0007987364660948515\n",
            "step: 360, loss: 0.005952878855168819\n",
            "step: 370, loss: 0.012306177988648415\n",
            "step: 380, loss: 0.004447312094271183\n",
            "step: 390, loss: 0.0011310795089229941\n",
            "step: 400, loss: 0.030268950387835503\n",
            "step: 410, loss: 7.199820538517088e-05\n",
            "step: 420, loss: 0.006682489067316055\n",
            "step: 430, loss: 0.004289600532501936\n",
            "step: 440, loss: 0.07079697400331497\n",
            "step: 450, loss: 0.0234861858189106\n",
            "step: 460, loss: 0.00035255946568213403\n",
            "step: 470, loss: 0.00150804512668401\n",
            "step: 480, loss: 0.000640198471955955\n",
            "step: 490, loss: 0.00030720746144652367\n",
            "step: 500, loss: 3.2258740247925743e-05\n",
            "step: 510, loss: 0.028986793011426926\n",
            "step: 520, loss: 4.450900451047346e-05\n",
            "step: 530, loss: 0.00013659559772349894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9211147850732168, f1=0.9201127819548872, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004196262918412685\n",
            "step: 10, loss: 0.0022130985744297504\n",
            "step: 20, loss: 0.004879768006503582\n",
            "step: 30, loss: 0.00040026201168075204\n",
            "step: 40, loss: 0.0001554218033561483\n",
            "step: 50, loss: 0.003561090910807252\n",
            "step: 60, loss: 9.703898831503466e-05\n",
            "step: 70, loss: 2.6410316422698088e-05\n",
            "step: 80, loss: 6.66006890241988e-05\n",
            "step: 90, loss: 0.0033032670617103577\n",
            "step: 100, loss: 0.0002315793390152976\n",
            "step: 110, loss: 9.371408668812364e-05\n",
            "step: 120, loss: 0.00018974865088239312\n",
            "step: 130, loss: 1.6655469153192826e-05\n",
            "step: 140, loss: 0.00010915447637671605\n",
            "step: 150, loss: 0.001132722944021225\n",
            "step: 160, loss: 7.220112456707284e-05\n",
            "step: 170, loss: 0.0006825723103247583\n",
            "step: 180, loss: 9.828728070715442e-05\n",
            "step: 190, loss: 0.0002075399534078315\n",
            "step: 200, loss: 0.0002408798027317971\n",
            "step: 210, loss: 7.989844743860886e-05\n",
            "step: 220, loss: 0.0001278335985261947\n",
            "step: 230, loss: 8.819713548291475e-05\n",
            "step: 240, loss: 0.00017561562708579004\n",
            "step: 250, loss: 4.607881419360638e-05\n",
            "step: 260, loss: 8.677581354277208e-05\n",
            "step: 270, loss: 3.9751270378474146e-05\n",
            "step: 280, loss: 0.00027854906511493027\n",
            "step: 290, loss: 0.0005282895872369409\n",
            "step: 300, loss: 5.213259646552615e-05\n",
            "step: 310, loss: 0.0020554526709020138\n",
            "step: 320, loss: 0.010212940163910389\n",
            "step: 330, loss: 0.01681152544915676\n",
            "step: 340, loss: 0.00018630531849339604\n",
            "step: 350, loss: 3.647457924671471e-05\n",
            "step: 360, loss: 3.373375875526108e-05\n",
            "step: 370, loss: 7.047411781968549e-05\n",
            "step: 380, loss: 0.00018511329835746437\n",
            "step: 390, loss: 0.0003141034103464335\n",
            "step: 400, loss: 0.0009638765477575362\n",
            "step: 410, loss: 0.0018941254820674658\n",
            "step: 420, loss: 9.431014768779278e-05\n",
            "step: 430, loss: 0.0026656801346689463\n",
            "step: 440, loss: 0.00031635872437618673\n",
            "step: 450, loss: 0.0001567101280670613\n",
            "step: 460, loss: 0.00032226971234194934\n",
            "step: 470, loss: 0.0008274338906630874\n",
            "step: 480, loss: 8.476414950564504e-05\n",
            "step: 490, loss: 0.0006805384182371199\n",
            "step: 500, loss: 3.08065464196261e-05\n",
            "step: 510, loss: 0.00011966231249971315\n",
            "step: 520, loss: 0.0009427727782167494\n",
            "step: 530, loss: 0.0016200292157009244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.918918918918919, f1=0.919815668202765, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016592119936831295\n",
            "step: 10, loss: 1.6230851542786695e-05\n",
            "step: 20, loss: 0.0003213815507479012\n",
            "step: 30, loss: 1.7147209291579202e-05\n",
            "step: 40, loss: 0.03551679477095604\n",
            "step: 50, loss: 0.0045738425105810165\n",
            "step: 60, loss: 1.2270999832253437e-05\n",
            "step: 70, loss: 5.6733813835307956e-05\n",
            "step: 80, loss: 4.6462479076581076e-05\n",
            "step: 90, loss: 0.0019668268505483866\n",
            "step: 100, loss: 0.0006074834964238107\n",
            "step: 110, loss: 7.35408320906572e-05\n",
            "step: 120, loss: 0.0002833390317391604\n",
            "step: 130, loss: 0.007994208484888077\n",
            "step: 140, loss: 0.000162361073307693\n",
            "step: 150, loss: 0.00024154283164534718\n",
            "step: 160, loss: 8.194385009119287e-05\n",
            "step: 170, loss: 0.0032390733249485493\n",
            "step: 180, loss: 0.03219957277178764\n",
            "step: 190, loss: 0.00034632149618119\n",
            "step: 200, loss: 0.0033102366141974926\n",
            "step: 210, loss: 0.0001889999839477241\n",
            "step: 220, loss: 0.002996159950271249\n",
            "step: 230, loss: 5.9321853768778965e-05\n",
            "step: 240, loss: 0.014077380299568176\n",
            "step: 250, loss: 4.0136750612873584e-05\n",
            "step: 260, loss: 3.915916022378951e-05\n",
            "step: 270, loss: 9.373675129609182e-05\n",
            "step: 280, loss: 1.6770927686593495e-05\n",
            "step: 290, loss: 0.0001812939008232206\n",
            "step: 300, loss: 0.0004922301159240305\n",
            "step: 310, loss: 0.022495390847325325\n",
            "step: 320, loss: 0.00020793756993953139\n",
            "step: 330, loss: 0.0012545189820230007\n",
            "step: 340, loss: 0.0001407687523169443\n",
            "step: 350, loss: 0.0014742386993020773\n",
            "step: 360, loss: 0.012394745834171772\n",
            "step: 370, loss: 3.211012881365605e-05\n",
            "step: 380, loss: 0.0005555456154979765\n",
            "step: 390, loss: 0.003884383710101247\n",
            "step: 400, loss: 0.00011171957885380834\n",
            "step: 410, loss: 1.6916272215894423e-05\n",
            "step: 420, loss: 7.483080844394863e-05\n",
            "step: 430, loss: 0.0001581687101861462\n",
            "step: 440, loss: 0.0024400034453719854\n",
            "step: 450, loss: 9.980182949220762e-05\n",
            "step: 460, loss: 4.942353552905843e-05\n",
            "step: 470, loss: 0.0002464594435878098\n",
            "step: 480, loss: 0.00012375987716950476\n",
            "step: 490, loss: 0.00010846122313523665\n",
            "step: 500, loss: 9.97531897155568e-05\n",
            "step: 510, loss: 5.5101227189879864e-05\n",
            "step: 520, loss: 3.872405068250373e-05\n",
            "step: 530, loss: 0.00015742173127364367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9209171736078615, f1=0.9215048769159313, best_f1=0.9215777262180975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.019068051595241e-05\n",
            "step: 10, loss: 0.00015174297732301056\n",
            "step: 20, loss: 2.9819242627127096e-05\n",
            "step: 30, loss: 8.307072130264714e-05\n",
            "step: 40, loss: 0.02194751799106598\n",
            "step: 50, loss: 0.03407355770468712\n",
            "step: 60, loss: 1.989269549085293e-05\n",
            "step: 70, loss: 0.0002259107568534091\n",
            "step: 80, loss: 0.0001035901004797779\n",
            "step: 90, loss: 2.411613240838051e-05\n",
            "step: 100, loss: 0.0007764214533381164\n",
            "step: 110, loss: 0.00022583885584026575\n",
            "step: 120, loss: 6.842219590907916e-05\n",
            "step: 130, loss: 0.010267670266330242\n",
            "step: 140, loss: 6.922987085999921e-05\n",
            "step: 150, loss: 2.5330979042337276e-05\n",
            "step: 160, loss: 0.0001748408831190318\n",
            "step: 170, loss: 1.802617589419242e-05\n",
            "step: 180, loss: 0.00011215040285605937\n",
            "step: 190, loss: 3.998599640908651e-05\n",
            "step: 200, loss: 5.815970871481113e-05\n",
            "step: 210, loss: 0.0007958183414302766\n",
            "step: 220, loss: 0.0006948591908439994\n",
            "step: 230, loss: 0.001014119596220553\n",
            "step: 240, loss: 0.00010182345431530848\n",
            "step: 250, loss: 0.000220369067392312\n",
            "step: 260, loss: 8.643844921607524e-05\n",
            "step: 270, loss: 1.3757147826254368e-05\n",
            "step: 280, loss: 1.3083067642583046e-05\n",
            "step: 290, loss: 1.3165048585506156e-05\n",
            "step: 300, loss: 4.5721848437096924e-05\n",
            "step: 310, loss: 0.0001700740249361843\n",
            "step: 320, loss: 2.635735654621385e-05\n",
            "step: 330, loss: 2.3846889234846458e-05\n",
            "step: 340, loss: 2.8111679057474248e-05\n",
            "step: 350, loss: 0.0002890195173677057\n",
            "step: 360, loss: 0.00023661630984861404\n",
            "step: 370, loss: 5.4895092034712434e-05\n",
            "step: 380, loss: 4.2946809116983786e-05\n",
            "step: 390, loss: 0.0085041094571352\n",
            "step: 400, loss: 0.0001134208869189024\n",
            "step: 410, loss: 3.98781594412867e-05\n",
            "step: 420, loss: 0.00016078643966466188\n",
            "step: 430, loss: 0.0009365401929244399\n",
            "step: 440, loss: 6.489369843620807e-05\n",
            "step: 450, loss: 1.4174458556226455e-05\n",
            "step: 460, loss: 1.2375242476991843e-05\n",
            "step: 470, loss: 0.0002497888926882297\n",
            "step: 480, loss: 1.8592174455989152e-05\n",
            "step: 490, loss: 0.0001104616021621041\n",
            "step: 500, loss: 2.041393963736482e-05\n",
            "step: 510, loss: 3.0944040190661326e-05\n",
            "step: 520, loss: 0.002612848998978734\n",
            "step: 530, loss: 1.348166279058205e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9209302325581395, f1=0.917397323488694, best_f1=0.9215777262180975\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 253.94it/s]\n",
            "load_f1 = 0.9222222222222223\n",
            "real_f1 = 0.9223616922361693\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 257.81it/s]\n"
          ]
        }
      ]
    }
  ]
}