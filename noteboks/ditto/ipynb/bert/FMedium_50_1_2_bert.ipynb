{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FMedium_50_1_2_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "5HZE1zMQgw8F",
        "pnXzXaaYhstq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0fd7faa5-9e6c-4a34-8aa3-858f7a8a969c"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 8.64 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "Checking out files: 100% (128/128), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 13.3 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 23.9 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 8.47 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 3.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.9 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 38.1 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 55.6 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 26.3 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449926 sha256=9a4ec656142c26492480bd0d2598d97ae9fd8e16291a92f66fd6e88bf9463ab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=cdab4d41c25e32758ec7f054c6977dbc9a49debdfe91257cc0435a3a99610245\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe31c67f-e40c-46f8-9b3c-c8c06979addf"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10117, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 10117 (delta 118), reused 152 (delta 62), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10117/10117), 14.96 MiB | 12.44 MiB/s, done.\n",
            "Resolving deltas: 100% (6922/6922), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-7p22dgpv\n",
            "Created temporary directory: /tmp/pip-req-tracker-7_oqwpxi\n",
            "Initialized build tracking at /tmp/pip-req-tracker-7_oqwpxi\n",
            "Created build tracker: /tmp/pip-req-tracker-7_oqwpxi\n",
            "Entered build tracker: /tmp/pip-req-tracker-7_oqwpxi\n",
            "Created temporary directory: /tmp/pip-install-16x4nzkv\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-e3egxi5z\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-7_oqwpxi'\n",
            "    Running setup.py (path:/tmp/pip-req-build-e3egxi5z/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-l29pz39a\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-l29pz39a/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-l29pz39a/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-l29pz39a/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-l29pz39a/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-l29pz39a/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-l29pz39a/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-e3egxi5z has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-7_oqwpxi'\n",
            "Created temporary directory: /tmp/pip-unpack-2nkwys05\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-da5dr70s\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-da5dr70s\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-e3egxi5z/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-e3egxi5z/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-da5dr70s\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-da5dr70s/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298244 sha256=bb0104eaef6af8b7216f320f47a46bfcf4491c2e5f4fa157b6a308d52dd66fb3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7p22dgpv/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-7_oqwpxi'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1044b30c-a612-4258-c584-edcd292a8ee4"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.49-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 44.8 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.49\n",
            "  Downloading botocore-1.27.49-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 21.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.49->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.49->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.49 botocore-1.27.49 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2611b171-d0d9-4d62-cb81-4b1a08fa3354"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "18d84b66-5e36-45ad-a49d-7cb1c7e811ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 1054, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 1054 (delta 51), reused 46 (delta 21), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (1054/1054), 257.86 MiB | 30.15 MiB/s, done.\n",
            "Resolving deltas: 100% (635/635), done.\n",
            "Checking out files: 100% (1304/1304), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813ce36a-4fa8-4fe4-9616-a6474a53c723"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/FMedium_50_1_2/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a0bcb1-22d9-43ed-c7f5-ac58cea3675e"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 276kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 354kB/s] \n",
            "Downloading: 100% 440M/440M [00:06<00:00, 70.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5525826811790466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.41379310344827586, f1=0.32352941176470584, best_f1=0.32352941176470584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48055073618888855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4150943396226415, f1=0.36666666666666664, best_f1=0.36666666666666664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5682058334350586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.55, f1=0.44897959183673464, best_f1=0.44897959183673464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2739851772785187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5714285714285714, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2758714258670807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8275862068965518, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3109860122203827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8333333333333333, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14555539190769196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.888888888888889, f1=0.6250000000000001, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06927061825990677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.88, f1=0.6666666666666667, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009115774184465408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8333333333333333, f1=0.689655172413793, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02912858873605728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8148148148148148, f1=0.6666666666666666, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005667021498084068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8, f1=0.689655172413793, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0062766363844275475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8148148148148148, f1=0.7096774193548386, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005656743887811899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8148148148148148, f1=0.7096774193548386, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00964810699224472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8148148148148148, f1=0.7096774193548386, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010872578248381615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8148148148148148, f1=0.7096774193548386, best_f1=0.6250000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 140789.99it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8666666666666666\n",
            "real_f1 = 0.742857142857143\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 195.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171454a1-9b9b-4178-d93d-39773d878114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6057059168815613\n",
            "step: 10, loss: 0.6249390840530396\n",
            "step: 20, loss: 0.28991952538490295\n",
            "step: 30, loss: 0.14765149354934692\n",
            "step: 40, loss: 0.2902633547782898\n",
            "step: 50, loss: 0.005038924049586058\n",
            "step: 60, loss: 0.034717198461294174\n",
            "step: 70, loss: 0.01313649583607912\n",
            "step: 80, loss: 0.10573048144578934\n",
            "step: 90, loss: 0.16196073591709137\n",
            "step: 100, loss: 0.006738121621310711\n",
            "step: 110, loss: 0.14653350412845612\n",
            "step: 120, loss: 0.026369348168373108\n",
            "step: 130, loss: 0.0037982745561748743\n",
            "step: 140, loss: 0.004080262966454029\n",
            "step: 150, loss: 0.023118143901228905\n",
            "step: 160, loss: 0.0037553850561380386\n",
            "step: 170, loss: 0.08718909323215485\n",
            "step: 180, loss: 0.005529792048037052\n",
            "step: 190, loss: 0.003203820902854204\n",
            "step: 200, loss: 0.004391388967633247\n",
            "step: 210, loss: 0.005485840141773224\n",
            "step: 220, loss: 0.010846731252968311\n",
            "step: 230, loss: 0.04444658011198044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9808773903262092, f1=0.9681093394077448, best_f1=0.9681093394077448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011231360025703907\n",
            "step: 10, loss: 0.0016194150084629655\n",
            "step: 20, loss: 0.07608634233474731\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.19865767657756805\n",
            "step: 40, loss: 0.03128484636545181\n",
            "step: 50, loss: 0.007728795520961285\n",
            "step: 60, loss: 0.007805082947015762\n",
            "step: 70, loss: 0.20388484001159668\n",
            "step: 80, loss: 0.002426909049972892\n",
            "step: 90, loss: 0.008195821195840836\n",
            "step: 100, loss: 0.019979454576969147\n",
            "step: 110, loss: 0.12144960463047028\n",
            "step: 120, loss: 0.014371611177921295\n",
            "step: 130, loss: 0.017746593803167343\n",
            "step: 140, loss: 0.004192421678453684\n",
            "step: 150, loss: 0.006578702479600906\n",
            "step: 160, loss: 0.003945607226341963\n",
            "step: 170, loss: 0.005896531976759434\n",
            "step: 180, loss: 0.0016473091673105955\n",
            "step: 190, loss: 0.0017146453028544784\n",
            "step: 200, loss: 0.0015600663609802723\n",
            "step: 210, loss: 0.00038352221599780023\n",
            "step: 220, loss: 0.09271343797445297\n",
            "step: 230, loss: 0.023005111142992973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9887640449438202, f1=0.9841628959276018, best_f1=0.9841628959276018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0083121657371521\n",
            "step: 10, loss: 0.00120362953748554\n",
            "step: 20, loss: 0.0465824268758297\n",
            "step: 30, loss: 0.12634100019931793\n",
            "step: 40, loss: 0.15222521126270294\n",
            "step: 50, loss: 0.013537850230932236\n",
            "step: 60, loss: 0.009103160351514816\n",
            "step: 70, loss: 0.002818470122292638\n",
            "step: 80, loss: 0.000633150979410857\n",
            "step: 90, loss: 0.09471793472766876\n",
            "step: 100, loss: 0.0008514162036590278\n",
            "step: 110, loss: 0.0029253032989799976\n",
            "step: 120, loss: 0.01143038272857666\n",
            "step: 130, loss: 0.010810190811753273\n",
            "step: 140, loss: 0.01233943086117506\n",
            "step: 150, loss: 0.004980152938514948\n",
            "step: 160, loss: 0.01807701401412487\n",
            "step: 170, loss: 0.004523350391536951\n",
            "step: 180, loss: 0.0036915522068738937\n",
            "step: 190, loss: 0.0691310241818428\n",
            "step: 200, loss: 0.0032355226576328278\n",
            "step: 210, loss: 0.002362532541155815\n",
            "step: 220, loss: 0.0006761363474652171\n",
            "step: 230, loss: 0.0005713510327041149\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9920903954802259, f1=0.9842342342342343, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00154530955478549\n",
            "step: 10, loss: 0.0006352519849315286\n",
            "step: 20, loss: 0.002088829642161727\n",
            "step: 30, loss: 0.0011944733560085297\n",
            "step: 40, loss: 0.002070953603833914\n",
            "step: 50, loss: 0.019170274958014488\n",
            "step: 60, loss: 0.048142191022634506\n",
            "step: 70, loss: 0.0025307773612439632\n",
            "step: 80, loss: 0.006638424005359411\n",
            "step: 90, loss: 0.007923578843474388\n",
            "step: 100, loss: 0.0010657634120434523\n",
            "step: 110, loss: 0.0006325837457552552\n",
            "step: 120, loss: 0.01906394027173519\n",
            "step: 130, loss: 0.007944339886307716\n",
            "step: 140, loss: 0.001523657701909542\n",
            "step: 150, loss: 0.1168130487203598\n",
            "step: 160, loss: 0.0006912490352988243\n",
            "step: 170, loss: 0.00301772216334939\n",
            "step: 180, loss: 0.0003150982956867665\n",
            "step: 190, loss: 0.00682927155867219\n",
            "step: 200, loss: 0.00859244167804718\n",
            "step: 210, loss: 0.0950300320982933\n",
            "step: 220, loss: 0.03738139942288399\n",
            "step: 230, loss: 0.0310013797134161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9852440408626559, f1=0.984090909090909, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004127363208681345\n",
            "step: 10, loss: 0.0014917385997250676\n",
            "step: 20, loss: 0.0023576575331389904\n",
            "step: 30, loss: 0.0020636345725506544\n",
            "step: 40, loss: 0.00022460424224846065\n",
            "step: 50, loss: 0.0008198187570087612\n",
            "step: 60, loss: 0.0007956565823405981\n",
            "step: 70, loss: 0.00047991276369430125\n",
            "step: 80, loss: 0.00036450501647777855\n",
            "step: 90, loss: 0.00034964020596817136\n",
            "step: 100, loss: 0.00037072718259878457\n",
            "step: 110, loss: 0.00012508657528087497\n",
            "step: 120, loss: 0.00012466726184356958\n",
            "step: 130, loss: 0.001520377118140459\n",
            "step: 140, loss: 0.007332806009799242\n",
            "step: 150, loss: 0.00016348606732208282\n",
            "step: 160, loss: 0.029484108090400696\n",
            "step: 170, loss: 0.01515639666467905\n",
            "step: 180, loss: 0.0011841440573334694\n",
            "step: 190, loss: 0.00432923948392272\n",
            "step: 200, loss: 0.11108305305242538\n",
            "step: 210, loss: 0.0004914669552817941\n",
            "step: 220, loss: 0.0033489540219306946\n",
            "step: 230, loss: 0.0002269752585561946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9819819819819819, f1=0.9751131221719457, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005546605098061264\n",
            "step: 10, loss: 0.0010469482513144612\n",
            "step: 20, loss: 0.0014178830897435546\n",
            "step: 30, loss: 0.0003746331203728914\n",
            "step: 40, loss: 0.000528948032297194\n",
            "step: 50, loss: 0.0003120514447800815\n",
            "step: 60, loss: 0.003598648589104414\n",
            "step: 70, loss: 0.0006545070209540427\n",
            "step: 80, loss: 0.0002841294335667044\n",
            "step: 90, loss: 0.000216399144846946\n",
            "step: 100, loss: 0.046288590878248215\n",
            "step: 110, loss: 0.0022864241618663073\n",
            "step: 120, loss: 0.0002638286678120494\n",
            "step: 130, loss: 0.004741283133625984\n",
            "step: 140, loss: 0.0002561297733336687\n",
            "step: 150, loss: 0.0006042369641363621\n",
            "step: 160, loss: 0.0852193683385849\n",
            "step: 170, loss: 0.0038555297069251537\n",
            "step: 180, loss: 0.04376240074634552\n",
            "step: 190, loss: 0.06216825172305107\n",
            "step: 200, loss: 0.0003576189046725631\n",
            "step: 210, loss: 0.0005697086453437805\n",
            "step: 220, loss: 0.00023488979786634445\n",
            "step: 230, loss: 0.07067608833312988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.992108229988726, f1=0.9853438556933484, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014946447918191552\n",
            "step: 10, loss: 0.00012247555423527956\n",
            "step: 20, loss: 0.0037459498271346092\n",
            "step: 30, loss: 0.0001590004685567692\n",
            "step: 40, loss: 8.765744132688269e-05\n",
            "step: 50, loss: 8.50267824716866e-05\n",
            "step: 60, loss: 0.006480115465819836\n",
            "step: 70, loss: 0.0003957234730478376\n",
            "step: 80, loss: 0.0004370519600342959\n",
            "step: 90, loss: 3.991100311395712e-05\n",
            "step: 100, loss: 7.512666343245655e-05\n",
            "step: 110, loss: 0.0008904412970878184\n",
            "step: 120, loss: 6.799174298066646e-05\n",
            "step: 130, loss: 0.0006839009001851082\n",
            "step: 140, loss: 0.00010598020162433386\n",
            "step: 150, loss: 0.010932608507573605\n",
            "step: 160, loss: 0.06640282273292542\n",
            "step: 170, loss: 0.0003601073694881052\n",
            "step: 180, loss: 0.002761821262538433\n",
            "step: 190, loss: 0.00010552666935836896\n",
            "step: 200, loss: 0.09103026986122131\n",
            "step: 210, loss: 5.567813423112966e-05\n",
            "step: 220, loss: 0.00021817309607286006\n",
            "step: 230, loss: 8.755864837439731e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9898305084745763, f1=0.9806598407281, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.891528316074982e-05\n",
            "step: 10, loss: 9.021285950439051e-05\n",
            "step: 20, loss: 0.00011075612565036863\n",
            "step: 30, loss: 0.00018946710042655468\n",
            "step: 40, loss: 0.0006802670541219413\n",
            "step: 50, loss: 0.0021134186536073685\n",
            "step: 60, loss: 0.00010406603541923687\n",
            "step: 70, loss: 0.000690205954015255\n",
            "step: 80, loss: 0.0016496527241542935\n",
            "step: 90, loss: 4.2230905819451436e-05\n",
            "step: 100, loss: 0.00035338691668584943\n",
            "step: 110, loss: 0.04899315536022186\n",
            "step: 120, loss: 0.0027607029769569635\n",
            "step: 130, loss: 0.0017212384846061468\n",
            "step: 140, loss: 6.996034790063277e-05\n",
            "step: 150, loss: 0.00038462321390397847\n",
            "step: 160, loss: 0.0122507493942976\n",
            "step: 170, loss: 0.000147269107401371\n",
            "step: 180, loss: 0.00039888842729851604\n",
            "step: 190, loss: 0.002456110902130604\n",
            "step: 200, loss: 0.00013678644609171897\n",
            "step: 210, loss: 0.00020591482461895794\n",
            "step: 220, loss: 0.0005590745713561773\n",
            "step: 230, loss: 0.00020331834093667567\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9898534385569334, f1=0.9830124575311437, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.786637120763771e-05\n",
            "step: 10, loss: 0.0012221195502206683\n",
            "step: 20, loss: 0.002725811442360282\n",
            "step: 30, loss: 6.084338019718416e-05\n",
            "step: 40, loss: 0.017018133774399757\n",
            "step: 50, loss: 4.089051435585134e-05\n",
            "step: 60, loss: 6.272690370678902e-05\n",
            "step: 70, loss: 0.0020300333853811026\n",
            "step: 80, loss: 0.0003874697722494602\n",
            "step: 90, loss: 0.0006469295476563275\n",
            "step: 100, loss: 0.0005936332745477557\n",
            "step: 110, loss: 3.384244701010175e-05\n",
            "step: 120, loss: 6.925741763552651e-05\n",
            "step: 130, loss: 3.042712341994047e-05\n",
            "step: 140, loss: 0.00014440379163715988\n",
            "step: 150, loss: 0.00111182383261621\n",
            "step: 160, loss: 3.2718031434342265e-05\n",
            "step: 170, loss: 9.163053618976846e-05\n",
            "step: 180, loss: 0.0048002018593251705\n",
            "step: 190, loss: 0.00015609081310685724\n",
            "step: 200, loss: 4.8630405217409134e-05\n",
            "step: 210, loss: 0.00019308380433358252\n",
            "step: 220, loss: 3.684161129058339e-05\n",
            "step: 230, loss: 0.0035415790043771267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9898762654668166, f1=0.9841628959276018, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016358328866772354\n",
            "step: 10, loss: 3.566339364624582e-05\n",
            "step: 20, loss: 0.00014491780893877149\n",
            "step: 30, loss: 0.00016608837177045643\n",
            "step: 40, loss: 0.00011464438284747303\n",
            "step: 50, loss: 3.095982174272649e-05\n",
            "step: 60, loss: 0.005237302277237177\n",
            "step: 70, loss: 0.004400918260216713\n",
            "step: 80, loss: 4.6809444029349834e-05\n",
            "step: 90, loss: 0.0001818091404857114\n",
            "step: 100, loss: 8.369374700123444e-05\n",
            "step: 110, loss: 6.977207522140816e-05\n",
            "step: 120, loss: 0.0001705655304249376\n",
            "step: 130, loss: 4.132243702770211e-05\n",
            "step: 140, loss: 0.044467344880104065\n",
            "step: 150, loss: 0.02464514970779419\n",
            "step: 160, loss: 5.1789393182843924e-05\n",
            "step: 170, loss: 0.0002329035778529942\n",
            "step: 180, loss: 6.553027196787298e-05\n",
            "step: 190, loss: 0.017616933211684227\n",
            "step: 200, loss: 4.6820212446618825e-05\n",
            "step: 210, loss: 0.000251006189500913\n",
            "step: 220, loss: 6.502419273601845e-05\n",
            "step: 230, loss: 0.0003036778944078833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.987598647125141, f1=0.975, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00032149412436410785\n",
            "step: 10, loss: 5.5901116866152734e-05\n",
            "step: 20, loss: 0.007959450595080853\n",
            "step: 30, loss: 0.02049371600151062\n",
            "step: 40, loss: 0.0007188209565356374\n",
            "step: 50, loss: 6.258417124627158e-05\n",
            "step: 60, loss: 0.0004886292736046016\n",
            "step: 70, loss: 0.0006528326193802059\n",
            "step: 80, loss: 2.9998607715242542e-05\n",
            "step: 90, loss: 0.0017944384599104524\n",
            "step: 100, loss: 5.889625754207373e-05\n",
            "step: 110, loss: 0.005618568509817123\n",
            "step: 120, loss: 2.7789817977463827e-05\n",
            "step: 130, loss: 3.7377325497800484e-05\n",
            "step: 140, loss: 4.4755637645721436e-05\n",
            "step: 150, loss: 0.01762053743004799\n",
            "step: 160, loss: 3.52061542798765e-05\n",
            "step: 170, loss: 0.025459593161940575\n",
            "step: 180, loss: 5.503280772245489e-05\n",
            "step: 190, loss: 3.121280678897165e-05\n",
            "step: 200, loss: 0.0007312167435884476\n",
            "step: 210, loss: 2.617705104057677e-05\n",
            "step: 220, loss: 3.357860987307504e-05\n",
            "step: 230, loss: 0.0009161244961433113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9909706546275394, f1=0.9819004524886877, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.372753053554334e-05\n",
            "step: 10, loss: 0.00014477786317002028\n",
            "step: 20, loss: 3.8401398342102766e-05\n",
            "step: 30, loss: 0.0002574701502453536\n",
            "step: 40, loss: 7.11414177203551e-05\n",
            "step: 50, loss: 0.0004443468351382762\n",
            "step: 60, loss: 0.013171347789466381\n",
            "step: 70, loss: 8.61724402057007e-05\n",
            "step: 80, loss: 0.00011765037197619677\n",
            "step: 90, loss: 3.4043572668451816e-05\n",
            "step: 100, loss: 3.555212606443092e-05\n",
            "step: 110, loss: 0.0005288183456286788\n",
            "step: 120, loss: 0.00011737873137462884\n",
            "step: 130, loss: 3.101218317169696e-05\n",
            "step: 140, loss: 0.017637133598327637\n",
            "step: 150, loss: 4.233918298268691e-05\n",
            "step: 160, loss: 3.383566581760533e-05\n",
            "step: 170, loss: 8.337311737705022e-05\n",
            "step: 180, loss: 0.03416004404425621\n",
            "step: 190, loss: 0.0004055165045429021\n",
            "step: 200, loss: 1.8428687326377258e-05\n",
            "step: 210, loss: 3.19693899655249e-05\n",
            "step: 220, loss: 6.229530845303088e-05\n",
            "step: 230, loss: 0.049131207168102264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9909706546275394, f1=0.9841269841269841, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015808780153747648\n",
            "step: 10, loss: 0.008654342964291573\n",
            "step: 20, loss: 0.00034182227682322264\n",
            "step: 30, loss: 5.1990635256515816e-05\n",
            "step: 40, loss: 0.002756014931946993\n",
            "step: 50, loss: 0.00045580396545119584\n",
            "step: 60, loss: 0.00018185898079536855\n",
            "step: 70, loss: 2.2142545276437886e-05\n",
            "step: 80, loss: 8.097309910226613e-05\n",
            "step: 90, loss: 0.00010949160059681162\n",
            "step: 100, loss: 2.761375071713701e-05\n",
            "step: 110, loss: 0.023051882162690163\n",
            "step: 120, loss: 0.022922184318304062\n",
            "step: 130, loss: 3.407763870200142e-05\n",
            "step: 140, loss: 2.6840038117370568e-05\n",
            "step: 150, loss: 3.589885091059841e-05\n",
            "step: 160, loss: 0.0009256793418899179\n",
            "step: 170, loss: 9.865371976047754e-05\n",
            "step: 180, loss: 4.8937723477138206e-05\n",
            "step: 190, loss: 2.0388128177728504e-05\n",
            "step: 200, loss: 0.00023606230388395488\n",
            "step: 210, loss: 1.9881208572769538e-05\n",
            "step: 220, loss: 2.344268432352692e-05\n",
            "step: 230, loss: 4.206991434330121e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9920724801812004, f1=0.9829738933030647, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3468810468330048e-05\n",
            "step: 10, loss: 0.011653910391032696\n",
            "step: 20, loss: 0.002485346281901002\n",
            "step: 30, loss: 6.027347990311682e-05\n",
            "step: 40, loss: 0.003054731758311391\n",
            "step: 50, loss: 3.8196638342924416e-05\n",
            "step: 60, loss: 3.0240493288147263e-05\n",
            "step: 70, loss: 4.905212699668482e-05\n",
            "step: 80, loss: 4.383278064779006e-05\n",
            "step: 90, loss: 4.4149441237095743e-05\n",
            "step: 100, loss: 4.526317570707761e-05\n",
            "step: 110, loss: 0.0004844176000915468\n",
            "step: 120, loss: 4.6455537813017145e-05\n",
            "step: 130, loss: 9.166204108623788e-05\n",
            "step: 140, loss: 8.368404814973474e-05\n",
            "step: 150, loss: 2.51149704126874e-05\n",
            "step: 160, loss: 0.014173641800880432\n",
            "step: 170, loss: 1.5213924598356243e-05\n",
            "step: 180, loss: 3.394281884538941e-05\n",
            "step: 190, loss: 0.000622395658865571\n",
            "step: 200, loss: 2.4217375539592467e-05\n",
            "step: 210, loss: 4.011925193481147e-05\n",
            "step: 220, loss: 0.00023634248645976186\n",
            "step: 230, loss: 0.00010765439947135746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9909706546275394, f1=0.9796839729119639, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9698776415898465e-05\n",
            "step: 10, loss: 1.522880484117195e-05\n",
            "step: 20, loss: 3.686303170979954e-05\n",
            "step: 30, loss: 2.8128510166425258e-05\n",
            "step: 40, loss: 6.333569763228297e-05\n",
            "step: 50, loss: 0.03994724154472351\n",
            "step: 60, loss: 4.159579475526698e-05\n",
            "step: 70, loss: 3.2289273804053664e-05\n",
            "step: 80, loss: 2.7476553441374563e-05\n",
            "step: 90, loss: 0.00010964448301820084\n",
            "step: 100, loss: 2.800216680043377e-05\n",
            "step: 110, loss: 3.416200343053788e-05\n",
            "step: 120, loss: 4.098230419913307e-05\n",
            "step: 130, loss: 0.023003071546554565\n",
            "step: 140, loss: 2.369965659454465e-05\n",
            "step: 150, loss: 0.00025210625608451664\n",
            "step: 160, loss: 2.336433135496918e-05\n",
            "step: 170, loss: 1.8450991774443537e-05\n",
            "step: 180, loss: 9.640929783927277e-05\n",
            "step: 190, loss: 0.0005735497106797993\n",
            "step: 200, loss: 4.709024142357521e-05\n",
            "step: 210, loss: 2.6288675144314766e-05\n",
            "step: 220, loss: 4.059861385030672e-05\n",
            "step: 230, loss: 2.659029996721074e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9920724801812004, f1=0.9807037457434733, best_f1=0.9853438556933484\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 160.74it/s]\n",
            "load_f1 = 0.9932279909706545\n",
            "real_f1 = 0.992108229988726\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 182.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8340f06-087b-42c6-c17c-27319c76946e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 490kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
            "Downloading: 100% 440M/440M [00:13<00:00, 32.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6112139225006104\n",
            "step: 10, loss: 0.5761849880218506\n",
            "step: 20, loss: 0.4996607005596161\n",
            "step: 30, loss: 0.09265869855880737\n",
            "step: 40, loss: 0.13024279475212097\n",
            "step: 50, loss: 0.13644194602966309\n",
            "step: 60, loss: 0.05535965785384178\n",
            "step: 70, loss: 0.12267633527517319\n",
            "step: 80, loss: 0.04027430713176727\n",
            "step: 90, loss: 0.3843410015106201\n",
            "step: 100, loss: 0.030194351449608803\n",
            "step: 110, loss: 0.07422686368227005\n",
            "step: 120, loss: 0.1277758777141571\n",
            "step: 130, loss: 0.15399722754955292\n",
            "step: 140, loss: 0.17723512649536133\n",
            "step: 150, loss: 0.06345702707767487\n",
            "step: 160, loss: 0.01942627690732479\n",
            "step: 170, loss: 0.26898932456970215\n",
            "step: 180, loss: 0.04384520277380943\n",
            "step: 190, loss: 0.009539599530398846\n",
            "step: 200, loss: 0.1833096444606781\n",
            "step: 210, loss: 0.06889040768146515\n",
            "step: 220, loss: 0.25833678245544434\n",
            "step: 230, loss: 0.14003635942935944\n",
            "step: 240, loss: 0.08440699428319931\n",
            "step: 250, loss: 0.015164528042078018\n",
            "step: 260, loss: 0.10453590005636215\n",
            "step: 270, loss: 0.007850325666368008\n",
            "step: 280, loss: 0.06237122416496277\n",
            "step: 290, loss: 0.1456483155488968\n",
            "step: 300, loss: 0.038811955600976944\n",
            "step: 310, loss: 0.2561967372894287\n",
            "step: 320, loss: 0.07986073940992355\n",
            "step: 330, loss: 0.08630380779504776\n",
            "step: 340, loss: 0.1208423376083374\n",
            "step: 350, loss: 0.02691720798611641\n",
            "step: 360, loss: 0.03054262325167656\n",
            "step: 370, loss: 0.08073750883340836\n",
            "step: 380, loss: 0.05517308786511421\n",
            "step: 390, loss: 0.20947179198265076\n",
            "step: 400, loss: 0.27701395750045776\n",
            "step: 410, loss: 0.03809661418199539\n",
            "step: 420, loss: 0.031067606061697006\n",
            "step: 430, loss: 0.19620299339294434\n",
            "step: 440, loss: 0.009951609186828136\n",
            "step: 450, loss: 0.007634905166924\n",
            "step: 460, loss: 0.005333814769983292\n",
            "step: 470, loss: 0.09421862661838531\n",
            "step: 480, loss: 0.0677407756447792\n",
            "step: 490, loss: 0.0405094139277935\n",
            "step: 500, loss: 0.14885996282100677\n",
            "step: 510, loss: 0.05720456689596176\n",
            "step: 520, loss: 0.07231215387582779\n",
            "step: 530, loss: 0.005859813652932644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9310504396112911, f1=0.9259944495837188, best_f1=0.9259944495837188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.141981840133667\n",
            "step: 10, loss: 0.07232413440942764\n",
            "step: 20, loss: 0.0224031712859869\n",
            "step: 30, loss: 0.08984833210706711\n",
            "step: 40, loss: 0.04487764835357666\n",
            "step: 50, loss: 0.21352972090244293\n",
            "step: 60, loss: 0.014138501137495041\n",
            "step: 70, loss: 0.028903719037771225\n",
            "step: 80, loss: 0.060970861464738846\n",
            "step: 90, loss: 0.01994410715997219\n",
            "step: 100, loss: 0.012419278733432293\n",
            "step: 110, loss: 0.03905189782381058\n",
            "step: 120, loss: 0.06537910550832748\n",
            "step: 130, loss: 0.0625346302986145\n",
            "step: 140, loss: 0.04938112199306488\n",
            "step: 150, loss: 0.07006359100341797\n",
            "step: 160, loss: 0.03457433730363846\n",
            "step: 170, loss: 0.02773771621286869\n",
            "step: 180, loss: 0.028659095987677574\n",
            "step: 190, loss: 0.05763339623808861\n",
            "step: 200, loss: 0.016431353986263275\n",
            "step: 210, loss: 0.09919317066669464\n",
            "step: 220, loss: 0.13107454776763916\n",
            "step: 230, loss: 0.011858991347253323\n",
            "step: 240, loss: 0.05689900740981102\n",
            "step: 250, loss: 0.06110097095370293\n",
            "step: 260, loss: 0.0016691400669515133\n",
            "step: 270, loss: 0.27483686804771423\n",
            "step: 280, loss: 0.02203219011425972\n",
            "step: 290, loss: 0.0263876561075449\n",
            "step: 300, loss: 0.17664818465709686\n",
            "step: 310, loss: 0.007993442006409168\n",
            "step: 320, loss: 0.12953436374664307\n",
            "step: 330, loss: 0.028426317498087883\n",
            "step: 340, loss: 0.011795945465564728\n",
            "step: 350, loss: 0.003035982372239232\n",
            "step: 360, loss: 0.034445907920598984\n",
            "step: 370, loss: 0.1223817765712738\n",
            "step: 380, loss: 0.02888069674372673\n",
            "step: 390, loss: 0.06237058341503143\n",
            "step: 400, loss: 0.08173598349094391\n",
            "step: 410, loss: 0.025683265179395676\n",
            "step: 420, loss: 0.039788100868463516\n",
            "step: 430, loss: 0.006068327929824591\n",
            "step: 440, loss: 0.08100096881389618\n",
            "step: 450, loss: 0.01855398900806904\n",
            "step: 460, loss: 0.05996312201023102\n",
            "step: 470, loss: 0.05318459868431091\n",
            "step: 480, loss: 0.18412968516349792\n",
            "step: 490, loss: 0.013300981372594833\n",
            "step: 500, loss: 0.34288927912712097\n",
            "step: 510, loss: 0.0510108508169651\n",
            "step: 520, loss: 0.07898642122745514\n",
            "step: 530, loss: 0.009891252964735031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9381443298969072, f1=0.9301675977653632, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025555390864610672\n",
            "step: 10, loss: 0.06895753741264343\n",
            "step: 20, loss: 0.08603094518184662\n",
            "step: 30, loss: 0.06780559569597244\n",
            "step: 40, loss: 0.004980756901204586\n",
            "step: 50, loss: 0.055251915007829666\n",
            "step: 60, loss: 0.030890772119164467\n",
            "step: 70, loss: 0.005589495878666639\n",
            "step: 80, loss: 0.0011702229967340827\n",
            "step: 90, loss: 0.009253707714378834\n",
            "step: 100, loss: 0.13839030265808105\n",
            "step: 110, loss: 0.009344966150820255\n",
            "step: 120, loss: 0.03169875219464302\n",
            "step: 130, loss: 0.009429548867046833\n",
            "step: 140, loss: 0.006286904215812683\n",
            "step: 150, loss: 0.017478933557868004\n",
            "step: 160, loss: 0.020672444254159927\n",
            "step: 170, loss: 0.04122679680585861\n",
            "step: 180, loss: 0.014993567019701004\n",
            "step: 190, loss: 0.002923975232988596\n",
            "step: 200, loss: 0.022484615445137024\n",
            "step: 210, loss: 0.0598364882171154\n",
            "step: 220, loss: 0.08822594583034515\n",
            "step: 230, loss: 0.06066465377807617\n",
            "step: 240, loss: 0.005726207513362169\n",
            "step: 250, loss: 0.03481512889266014\n",
            "step: 260, loss: 0.02843300625681877\n",
            "step: 270, loss: 0.005818720441311598\n",
            "step: 280, loss: 0.11451701074838638\n",
            "step: 290, loss: 0.004633158445358276\n",
            "step: 300, loss: 0.01479597669094801\n",
            "step: 310, loss: 0.031523630023002625\n",
            "step: 320, loss: 0.023586519062519073\n",
            "step: 330, loss: 0.008591785095632076\n",
            "step: 340, loss: 0.014665552414953709\n",
            "step: 350, loss: 0.013567444868385792\n",
            "step: 360, loss: 0.09898122400045395\n",
            "step: 370, loss: 0.006869898177683353\n",
            "step: 380, loss: 0.040383633226156235\n",
            "step: 390, loss: 0.03836481645703316\n",
            "step: 400, loss: 0.013278207741677761\n",
            "step: 410, loss: 0.005783507600426674\n",
            "step: 420, loss: 0.42717841267585754\n",
            "step: 430, loss: 0.008371870033442974\n",
            "step: 440, loss: 0.0016620998503640294\n",
            "step: 450, loss: 0.1072002723813057\n",
            "step: 460, loss: 0.02065696381032467\n",
            "step: 470, loss: 0.027829818427562714\n",
            "step: 480, loss: 0.007155397441238165\n",
            "step: 490, loss: 0.007289810571819544\n",
            "step: 500, loss: 0.05713459849357605\n",
            "step: 510, loss: 0.010871251113712788\n",
            "step: 520, loss: 0.036254484206438065\n",
            "step: 530, loss: 0.1272401064634323\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9260991580916744, f1=0.9257518796992481, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014480584301054478\n",
            "step: 10, loss: 0.01570880226790905\n",
            "step: 20, loss: 0.0008807792910374701\n",
            "step: 30, loss: 0.01195620372891426\n",
            "step: 40, loss: 0.003613423788920045\n",
            "step: 50, loss: 0.004415246658027172\n",
            "step: 60, loss: 0.009148640558123589\n",
            "step: 70, loss: 0.0012905426556244493\n",
            "step: 80, loss: 0.029701698571443558\n",
            "step: 90, loss: 0.12247774749994278\n",
            "step: 100, loss: 0.006987255532294512\n",
            "step: 110, loss: 0.015978483483195305\n",
            "step: 120, loss: 0.00078507763100788\n",
            "step: 130, loss: 0.0007180646061897278\n",
            "step: 140, loss: 0.0056296768598258495\n",
            "step: 150, loss: 0.018458906561136246\n",
            "step: 160, loss: 0.015689000487327576\n",
            "step: 170, loss: 0.012396097183227539\n",
            "step: 180, loss: 0.005440453067421913\n",
            "step: 190, loss: 0.07545750588178635\n",
            "step: 200, loss: 0.0035252892412245274\n",
            "step: 210, loss: 0.02827952615916729\n",
            "step: 220, loss: 0.0011132658692076802\n",
            "step: 230, loss: 0.04367717728018761\n",
            "step: 240, loss: 0.0032131406478583813\n",
            "step: 250, loss: 0.04976093769073486\n",
            "step: 260, loss: 0.002312254626303911\n",
            "step: 270, loss: 0.005895612761378288\n",
            "step: 280, loss: 0.04715967923402786\n",
            "step: 290, loss: 0.017230607569217682\n",
            "step: 300, loss: 0.02445465885102749\n",
            "step: 310, loss: 0.04071354120969772\n",
            "step: 320, loss: 0.0940735787153244\n",
            "step: 330, loss: 0.0838962271809578\n",
            "step: 340, loss: 0.002465995028614998\n",
            "step: 350, loss: 0.022304782643914223\n",
            "step: 360, loss: 0.0714198499917984\n",
            "step: 370, loss: 0.0038349321112036705\n",
            "step: 380, loss: 0.0017808957491070032\n",
            "step: 390, loss: 0.0005586573970504105\n",
            "step: 400, loss: 0.07651019841432571\n",
            "step: 410, loss: 0.01435515470802784\n",
            "step: 420, loss: 0.0074419667944312096\n",
            "step: 430, loss: 0.17335233092308044\n",
            "step: 440, loss: 0.02608521282672882\n",
            "step: 450, loss: 0.0014638088177889585\n",
            "step: 460, loss: 0.0003831057110801339\n",
            "step: 470, loss: 0.011126210913062096\n",
            "step: 480, loss: 0.043039388954639435\n",
            "step: 490, loss: 0.002927497960627079\n",
            "step: 500, loss: 0.0587361603975296\n",
            "step: 510, loss: 0.012861293740570545\n",
            "step: 520, loss: 0.03427661955356598\n",
            "step: 530, loss: 0.005666946992278099\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9197215777262181, f1=0.9242843951985227, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0309713464230299\n",
            "step: 10, loss: 0.02404714748263359\n",
            "step: 20, loss: 0.013725989498198032\n",
            "step: 30, loss: 0.00954394694417715\n",
            "step: 40, loss: 0.0006006183102726936\n",
            "step: 50, loss: 0.00387799134477973\n",
            "step: 60, loss: 0.1535295695066452\n",
            "step: 70, loss: 0.004822482820600271\n",
            "step: 80, loss: 0.0009048462379723787\n",
            "step: 90, loss: 0.006046958267688751\n",
            "step: 100, loss: 0.02312321960926056\n",
            "step: 110, loss: 0.00017570155614521354\n",
            "step: 120, loss: 0.0006383223808370531\n",
            "step: 130, loss: 0.0002824361727107316\n",
            "step: 140, loss: 0.0013256791280582547\n",
            "step: 150, loss: 0.0007331583765335381\n",
            "step: 160, loss: 0.0005676455330103636\n",
            "step: 170, loss: 0.05011812224984169\n",
            "step: 180, loss: 0.0009732440230436623\n",
            "step: 190, loss: 0.014293577522039413\n",
            "step: 200, loss: 0.0011947464663535357\n",
            "step: 210, loss: 0.0019188327714800835\n",
            "step: 220, loss: 0.0003954910789616406\n",
            "step: 230, loss: 0.002116000046953559\n",
            "step: 240, loss: 0.000468316808110103\n",
            "step: 250, loss: 0.02699863724410534\n",
            "step: 260, loss: 0.01286966260522604\n",
            "step: 270, loss: 0.001986290328204632\n",
            "step: 280, loss: 0.02963411621749401\n",
            "step: 290, loss: 0.13662870228290558\n",
            "step: 300, loss: 0.007060873322188854\n",
            "step: 310, loss: 0.00795788038522005\n",
            "step: 320, loss: 0.06898710131645203\n",
            "step: 330, loss: 0.009382061660289764\n",
            "step: 340, loss: 0.0027646382804960012\n",
            "step: 350, loss: 0.0012505385093390942\n",
            "step: 360, loss: 0.009228934533894062\n",
            "step: 370, loss: 0.03927990049123764\n",
            "step: 380, loss: 0.0025073334109038115\n",
            "step: 390, loss: 0.00022539231576956809\n",
            "step: 400, loss: 0.01904618926346302\n",
            "step: 410, loss: 0.003931358456611633\n",
            "step: 420, loss: 0.014294499531388283\n",
            "step: 430, loss: 0.0009824020089581609\n",
            "step: 440, loss: 0.10423880070447922\n",
            "step: 450, loss: 0.001957148080691695\n",
            "step: 460, loss: 0.0008735321462154388\n",
            "step: 470, loss: 0.017923589795827866\n",
            "step: 480, loss: 0.017003824934363365\n",
            "step: 490, loss: 0.0006280196830630302\n",
            "step: 500, loss: 0.0017913796473294497\n",
            "step: 510, loss: 0.08773525804281235\n",
            "step: 520, loss: 0.01060349028557539\n",
            "step: 530, loss: 0.00016940332716330886\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9227941176470589, f1=0.9227230910763569, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.055101603269577026\n",
            "step: 10, loss: 0.0009454225655645132\n",
            "step: 20, loss: 0.13402430713176727\n",
            "step: 30, loss: 0.0024280562065541744\n",
            "step: 40, loss: 0.011862417683005333\n",
            "step: 50, loss: 0.047061316668987274\n",
            "step: 60, loss: 0.008964655920863152\n",
            "step: 70, loss: 0.0030879685655236244\n",
            "step: 80, loss: 0.005744440481066704\n",
            "step: 90, loss: 0.0028458177112042904\n",
            "step: 100, loss: 0.00030726855038665235\n",
            "step: 110, loss: 0.0009588910033926368\n",
            "step: 120, loss: 0.00242018373683095\n",
            "step: 130, loss: 0.0015287597198039293\n",
            "step: 140, loss: 0.000966014398727566\n",
            "step: 150, loss: 0.0004241330025251955\n",
            "step: 160, loss: 0.0002569913922343403\n",
            "step: 170, loss: 0.005830035079270601\n",
            "step: 180, loss: 0.00039082023431546986\n",
            "step: 190, loss: 0.0679408386349678\n",
            "step: 200, loss: 0.00024391130136791617\n",
            "step: 210, loss: 0.0005913501954637468\n",
            "step: 220, loss: 0.000399152806494385\n",
            "step: 230, loss: 0.0005877743824385107\n",
            "step: 240, loss: 0.0002456612419337034\n",
            "step: 250, loss: 0.05057136341929436\n",
            "step: 260, loss: 9.742511610966176e-05\n",
            "step: 270, loss: 0.0009171712445095181\n",
            "step: 280, loss: 0.0004912580479867756\n",
            "step: 290, loss: 0.0001287950435653329\n",
            "step: 300, loss: 0.001994804944843054\n",
            "step: 310, loss: 0.010777272284030914\n",
            "step: 320, loss: 0.000281800574157387\n",
            "step: 330, loss: 0.0010980108054354787\n",
            "step: 340, loss: 0.08408397436141968\n",
            "step: 350, loss: 0.0006561963236890733\n",
            "step: 360, loss: 0.01449219137430191\n",
            "step: 370, loss: 0.17815320193767548\n",
            "step: 380, loss: 0.00039543944876641035\n",
            "step: 390, loss: 0.002338081831112504\n",
            "step: 400, loss: 0.00664733350276947\n",
            "step: 410, loss: 0.006909026298671961\n",
            "step: 420, loss: 0.0029447844717651606\n",
            "step: 430, loss: 0.0008453644113615155\n",
            "step: 440, loss: 0.0001506960397819057\n",
            "step: 450, loss: 0.0002687560918275267\n",
            "step: 460, loss: 0.0005530666094273329\n",
            "step: 470, loss: 0.0021866776514798403\n",
            "step: 480, loss: 0.00981609895825386\n",
            "step: 490, loss: 0.008113800548017025\n",
            "step: 500, loss: 0.0016185024287551641\n",
            "step: 510, loss: 0.011861443519592285\n",
            "step: 520, loss: 0.001848089974373579\n",
            "step: 530, loss: 0.023339437320828438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9206349206349207, f1=0.9171736078614879, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004695162642747164\n",
            "step: 10, loss: 0.009927350096404552\n",
            "step: 20, loss: 0.012538718059659004\n",
            "step: 30, loss: 0.0022897240705788136\n",
            "step: 40, loss: 0.001601825701072812\n",
            "step: 50, loss: 0.0007968173595145345\n",
            "step: 60, loss: 0.004262837581336498\n",
            "step: 70, loss: 0.026606515049934387\n",
            "step: 80, loss: 0.0033444419968873262\n",
            "step: 90, loss: 0.00044489200809039176\n",
            "step: 100, loss: 7.559629011666402e-05\n",
            "step: 110, loss: 0.001394249964505434\n",
            "step: 120, loss: 0.009921983815729618\n",
            "step: 130, loss: 0.004778003785759211\n",
            "step: 140, loss: 0.0001034564629662782\n",
            "step: 150, loss: 0.00048337061889469624\n",
            "step: 160, loss: 0.00010973892494803295\n",
            "step: 170, loss: 0.00012068434443790466\n",
            "step: 180, loss: 0.001365081174299121\n",
            "step: 190, loss: 0.004476046189665794\n",
            "step: 200, loss: 0.00014024759002495557\n",
            "step: 210, loss: 0.0004119114892091602\n",
            "step: 220, loss: 0.00020024435070808977\n",
            "step: 230, loss: 0.10160300880670547\n",
            "step: 240, loss: 0.0046157557517290115\n",
            "step: 250, loss: 0.0007817660225555301\n",
            "step: 260, loss: 0.00016086321556940675\n",
            "step: 270, loss: 0.0011260728351771832\n",
            "step: 280, loss: 0.0012583109783008695\n",
            "step: 290, loss: 9.229940042132512e-05\n",
            "step: 300, loss: 0.0035436474718153477\n",
            "step: 310, loss: 0.001122034853324294\n",
            "step: 320, loss: 0.0012480239383876324\n",
            "step: 330, loss: 0.0009961823234334588\n",
            "step: 340, loss: 0.00794603955000639\n",
            "step: 350, loss: 0.0006650958093814552\n",
            "step: 360, loss: 0.0022141719236969948\n",
            "step: 370, loss: 0.00031907297670841217\n",
            "step: 380, loss: 0.00017419569485355169\n",
            "step: 390, loss: 0.000146760226925835\n",
            "step: 400, loss: 0.008396129123866558\n",
            "step: 410, loss: 0.002393783302977681\n",
            "step: 420, loss: 0.010930193588137627\n",
            "step: 430, loss: 0.00020432836026884615\n",
            "step: 440, loss: 0.0019443127093836665\n",
            "step: 450, loss: 0.007447691168636084\n",
            "step: 460, loss: 0.0015222755027934909\n",
            "step: 470, loss: 0.0014924912247806787\n",
            "step: 480, loss: 0.11942704021930695\n",
            "step: 490, loss: 0.0012120319297537208\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 500, loss: 0.002719215117394924\n",
            "step: 510, loss: 0.0037601543590426445\n",
            "step: 520, loss: 0.018899012356996536\n",
            "step: 530, loss: 0.036000411957502365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9271523178807948, f1=0.9201700519603213, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003187562106177211\n",
            "step: 10, loss: 0.000205141375772655\n",
            "step: 20, loss: 9.316526120528579e-05\n",
            "step: 30, loss: 0.0004025712551083416\n",
            "step: 40, loss: 0.004275472369045019\n",
            "step: 50, loss: 0.04144607484340668\n",
            "step: 60, loss: 0.004131444729864597\n",
            "step: 70, loss: 0.00011599927529459819\n",
            "step: 80, loss: 0.0010925009846687317\n",
            "step: 90, loss: 0.00018968664517160505\n",
            "step: 100, loss: 8.836847700877115e-05\n",
            "step: 110, loss: 5.848565342603251e-05\n",
            "step: 120, loss: 0.003974487539380789\n",
            "step: 130, loss: 0.004000482149422169\n",
            "step: 140, loss: 0.0038218870759010315\n",
            "step: 150, loss: 0.003543214173987508\n",
            "step: 160, loss: 0.009168018586933613\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.00138648203574121\n",
            "step: 180, loss: 6.692620081594214e-05\n",
            "step: 190, loss: 0.00268665817566216\n",
            "step: 200, loss: 0.0007229113834910095\n",
            "step: 210, loss: 0.0007010906119830906\n",
            "step: 220, loss: 0.0029607012402266264\n",
            "step: 230, loss: 0.00015119636373128742\n",
            "step: 240, loss: 0.000309529947116971\n",
            "step: 250, loss: 6.677309283986688e-05\n",
            "step: 260, loss: 0.00010210338223259896\n",
            "step: 270, loss: 0.08152545243501663\n",
            "step: 280, loss: 0.04011954367160797\n",
            "step: 290, loss: 0.0008828580612316728\n",
            "step: 300, loss: 0.0017619171412661672\n",
            "step: 310, loss: 0.00044675092794932425\n",
            "step: 320, loss: 0.002345845801755786\n",
            "step: 330, loss: 0.007890750654041767\n",
            "step: 340, loss: 0.000564802554436028\n",
            "step: 350, loss: 0.004381836857646704\n",
            "step: 360, loss: 0.0002429976884741336\n",
            "step: 370, loss: 4.035781603306532e-05\n",
            "step: 380, loss: 0.009253866970539093\n",
            "step: 390, loss: 0.16533003747463226\n",
            "step: 400, loss: 0.008340463042259216\n",
            "step: 410, loss: 7.015430310275406e-05\n",
            "step: 420, loss: 0.015119336545467377\n",
            "step: 430, loss: 0.013087012805044651\n",
            "step: 440, loss: 0.0029018244240432978\n",
            "step: 450, loss: 0.00013819111336488277\n",
            "step: 460, loss: 0.00035257430863566697\n",
            "step: 470, loss: 0.02077815867960453\n",
            "step: 480, loss: 0.00012171010894235224\n",
            "step: 490, loss: 0.004175127483904362\n",
            "step: 500, loss: 0.00019030302064493299\n",
            "step: 510, loss: 0.00023389095440506935\n",
            "step: 520, loss: 0.00328422081656754\n",
            "step: 530, loss: 7.562293467344716e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9304964539007093, f1=0.9234382339126351, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018530647503212094\n",
            "step: 10, loss: 9.378222603118047e-05\n",
            "step: 20, loss: 0.0006397615070454776\n",
            "step: 30, loss: 0.000168783706612885\n",
            "step: 40, loss: 0.00027663842774927616\n",
            "step: 50, loss: 8.079451799858361e-05\n",
            "step: 60, loss: 0.00010421354090794921\n",
            "step: 70, loss: 0.0026718818116933107\n",
            "step: 80, loss: 0.00039502454455941916\n",
            "step: 90, loss: 0.0008229714003391564\n",
            "step: 100, loss: 0.00016831573157105595\n",
            "step: 110, loss: 0.00011153090599691495\n",
            "step: 120, loss: 0.0002445206919219345\n",
            "step: 130, loss: 0.03248611465096474\n",
            "step: 140, loss: 0.00010292528168065473\n",
            "step: 150, loss: 5.0329239456914365e-05\n",
            "step: 160, loss: 0.0011988675687462091\n",
            "step: 170, loss: 0.00679492624476552\n",
            "step: 180, loss: 9.072176908375695e-05\n",
            "step: 190, loss: 8.56980259413831e-05\n",
            "step: 200, loss: 0.00011449181329226121\n",
            "step: 210, loss: 0.00018526165513321757\n",
            "step: 220, loss: 0.0028199797961860895\n",
            "step: 230, loss: 4.249593257554807e-05\n",
            "step: 240, loss: 0.04522290453314781\n",
            "step: 250, loss: 0.007519814185798168\n",
            "step: 260, loss: 0.010651445016264915\n",
            "step: 270, loss: 5.440631139208563e-05\n",
            "step: 280, loss: 0.0866638720035553\n",
            "step: 290, loss: 0.0062584057450294495\n",
            "step: 300, loss: 6.24194581178017e-05\n",
            "step: 310, loss: 0.009206963703036308\n",
            "step: 320, loss: 0.0005285089137032628\n",
            "step: 330, loss: 0.014420008286833763\n",
            "step: 340, loss: 0.0010769875952973962\n",
            "step: 350, loss: 0.0010720244608819485\n",
            "step: 360, loss: 5.935695662628859e-05\n",
            "step: 370, loss: 0.004094521515071392\n",
            "step: 380, loss: 0.00010272636427544057\n",
            "step: 390, loss: 6.621023931074888e-05\n",
            "step: 400, loss: 0.009515371173620224\n",
            "step: 410, loss: 0.00014990394993219525\n",
            "step: 420, loss: 4.746398553834297e-05\n",
            "step: 430, loss: 0.0006556441658176482\n",
            "step: 440, loss: 0.0030421356204897165\n",
            "step: 450, loss: 8.290470577776432e-05\n",
            "step: 460, loss: 0.0013807103969156742\n",
            "step: 470, loss: 2.9533362976508215e-05\n",
            "step: 480, loss: 0.00010115477198269218\n",
            "step: 490, loss: 0.001002933015115559\n",
            "step: 500, loss: 0.00042985714389942586\n",
            "step: 510, loss: 0.005186126101762056\n",
            "step: 520, loss: 0.0002182797179557383\n",
            "step: 530, loss: 7.715677202213556e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.927348449791763, f1=0.9271889400921659, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016552578017581254\n",
            "step: 10, loss: 3.618170376284979e-05\n",
            "step: 20, loss: 5.45143338968046e-05\n",
            "step: 30, loss: 8.025570423342288e-05\n",
            "step: 40, loss: 0.0015513449907302856\n",
            "step: 50, loss: 0.0002445452264510095\n",
            "step: 60, loss: 6.410440983017907e-05\n",
            "step: 70, loss: 7.009419641690329e-05\n",
            "step: 80, loss: 3.289704545750283e-05\n",
            "step: 90, loss: 7.993663166416809e-05\n",
            "step: 100, loss: 7.07968429196626e-05\n",
            "step: 110, loss: 8.732483547646552e-05\n",
            "step: 120, loss: 8.596004045102745e-05\n",
            "step: 130, loss: 0.00018424341396894306\n",
            "step: 140, loss: 0.00016023797797970474\n",
            "step: 150, loss: 5.479163155541755e-05\n",
            "step: 160, loss: 4.714962051366456e-05\n",
            "step: 170, loss: 5.3077492339070886e-05\n",
            "step: 180, loss: 0.0003256300697103143\n",
            "step: 190, loss: 0.00023009728465694934\n",
            "step: 200, loss: 0.00012197933392599225\n",
            "step: 210, loss: 0.0017719839233905077\n",
            "step: 220, loss: 0.005943921394646168\n",
            "step: 230, loss: 0.0003646414843387902\n",
            "step: 240, loss: 0.00010495405149413273\n",
            "step: 250, loss: 0.01319038961082697\n",
            "step: 260, loss: 0.0025964598171412945\n",
            "step: 270, loss: 0.0007657698588445783\n",
            "step: 280, loss: 6.903061148477718e-05\n",
            "step: 290, loss: 8.918100502341986e-05\n",
            "step: 300, loss: 2.994639908138197e-05\n",
            "step: 310, loss: 0.00013613175542559475\n",
            "step: 320, loss: 0.001313585788011551\n",
            "step: 330, loss: 3.666717020678334e-05\n",
            "step: 340, loss: 0.0021475041285157204\n",
            "step: 350, loss: 6.923313048901036e-05\n",
            "step: 360, loss: 0.00022033015557099134\n",
            "step: 370, loss: 0.000682466255966574\n",
            "step: 380, loss: 0.0006798045942559838\n",
            "step: 390, loss: 0.0007704552263021469\n",
            "step: 400, loss: 0.00019797119603026658\n",
            "step: 410, loss: 0.001344421529211104\n",
            "step: 420, loss: 3.590175765566528e-05\n",
            "step: 430, loss: 2.776286783046089e-05\n",
            "step: 440, loss: 0.00029067654395475984\n",
            "step: 450, loss: 4.597817314788699e-05\n",
            "step: 460, loss: 4.1038412746274844e-05\n",
            "step: 470, loss: 0.00133479421492666\n",
            "step: 480, loss: 3.269232911407016e-05\n",
            "step: 490, loss: 6.364154251059517e-05\n",
            "step: 500, loss: 0.004135034047067165\n",
            "step: 510, loss: 8.991961658466607e-05\n",
            "step: 520, loss: 0.0002095382078550756\n",
            "step: 530, loss: 0.0007657863316126168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9280677009873061, f1=0.924092409240924, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004474273882806301\n",
            "step: 10, loss: 0.00017074205970857292\n",
            "step: 20, loss: 0.00010291920625604689\n",
            "step: 30, loss: 0.0008478441741317511\n",
            "step: 40, loss: 0.00036078336415812373\n",
            "step: 50, loss: 0.000507600954733789\n",
            "step: 60, loss: 0.004728947300463915\n",
            "step: 70, loss: 7.565146370325238e-05\n",
            "step: 80, loss: 0.00028225427377037704\n",
            "step: 90, loss: 6.374937947839499e-05\n",
            "step: 100, loss: 0.00026159427943639457\n",
            "step: 110, loss: 0.00016110032447613776\n",
            "step: 120, loss: 4.6159686462488025e-05\n",
            "step: 130, loss: 4.396999065647833e-05\n",
            "step: 140, loss: 9.766151197254658e-05\n",
            "step: 150, loss: 3.933841435355134e-05\n",
            "step: 160, loss: 4.39617142546922e-05\n",
            "step: 170, loss: 0.00011027992877643555\n",
            "step: 180, loss: 0.0014293218264356256\n",
            "step: 190, loss: 0.003264656523242593\n",
            "step: 200, loss: 0.005125797353684902\n",
            "step: 210, loss: 0.00044947597780264914\n",
            "step: 220, loss: 0.00019214858184568584\n",
            "step: 230, loss: 0.0020950704347342253\n",
            "step: 240, loss: 0.00023266905918717384\n",
            "step: 250, loss: 2.0503368432400748e-05\n",
            "step: 260, loss: 0.0021576951257884502\n",
            "step: 270, loss: 5.4526452004211023e-05\n",
            "step: 280, loss: 4.114330295124091e-05\n",
            "step: 290, loss: 0.00017048648442141712\n",
            "step: 300, loss: 3.6687841202365234e-05\n",
            "step: 310, loss: 0.00010200667747994885\n",
            "step: 320, loss: 0.00019051373237743974\n",
            "step: 330, loss: 0.0006264232797548175\n",
            "step: 340, loss: 0.0005371414008550346\n",
            "step: 350, loss: 4.280672874301672e-05\n",
            "step: 360, loss: 3.8507165299961343e-05\n",
            "step: 370, loss: 0.0012326156720519066\n",
            "step: 380, loss: 3.912351530743763e-05\n",
            "step: 390, loss: 0.00010664344154065475\n",
            "step: 400, loss: 2.4131701138685457e-05\n",
            "step: 410, loss: 2.1952711904305033e-05\n",
            "step: 420, loss: 7.203753921203315e-05\n",
            "step: 430, loss: 2.3077669538906775e-05\n",
            "step: 440, loss: 0.0006725882994942367\n",
            "step: 450, loss: 3.556214505806565e-05\n",
            "step: 460, loss: 0.0020412991289049387\n",
            "step: 470, loss: 2.949561530840583e-05\n",
            "step: 480, loss: 7.251830538734794e-05\n",
            "step: 490, loss: 0.0030329707078635693\n",
            "step: 500, loss: 0.0011423625983297825\n",
            "step: 510, loss: 0.0002847050200216472\n",
            "step: 520, loss: 5.1397997594904155e-05\n",
            "step: 530, loss: 0.0018529309891164303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9263256687001408, f1=0.9258215962441314, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001378189626848325\n",
            "step: 10, loss: 4.6443954488495365e-05\n",
            "step: 20, loss: 0.006621797569096088\n",
            "step: 30, loss: 7.466043462045491e-05\n",
            "step: 40, loss: 0.0009522641776129603\n",
            "step: 50, loss: 0.004660172387957573\n",
            "step: 60, loss: 0.001177601283416152\n",
            "step: 70, loss: 8.177241397788748e-05\n",
            "step: 80, loss: 0.008519290946424007\n",
            "step: 90, loss: 0.0001515152835054323\n",
            "step: 100, loss: 0.00033832225017249584\n",
            "step: 110, loss: 6.465181650128216e-05\n",
            "step: 120, loss: 3.264708357164636e-05\n",
            "step: 130, loss: 0.0011853532632812858\n",
            "step: 140, loss: 9.842684812610969e-05\n",
            "step: 150, loss: 2.119281271006912e-05\n",
            "step: 160, loss: 3.1804673199076205e-05\n",
            "step: 170, loss: 2.978267366415821e-05\n",
            "step: 180, loss: 4.3723921407945454e-05\n",
            "step: 190, loss: 3.732811092049815e-05\n",
            "step: 200, loss: 0.00014235172420740128\n",
            "step: 210, loss: 0.018962424248456955\n",
            "step: 220, loss: 4.188150342088193e-05\n",
            "step: 230, loss: 2.130814755219035e-05\n",
            "step: 240, loss: 0.0002943847212009132\n",
            "step: 250, loss: 2.6906895072897896e-05\n",
            "step: 260, loss: 0.0016019648173823953\n",
            "step: 270, loss: 5.462495028041303e-05\n",
            "step: 280, loss: 0.0007261126884259284\n",
            "step: 290, loss: 0.0001615135552128777\n",
            "step: 300, loss: 0.00010822966578416526\n",
            "step: 310, loss: 0.00020385670359246433\n",
            "step: 320, loss: 2.2589138097828254e-05\n",
            "step: 330, loss: 0.0008751269779168069\n",
            "step: 340, loss: 3.0378401788766496e-05\n",
            "step: 350, loss: 5.815256736241281e-05\n",
            "step: 360, loss: 0.012267555110156536\n",
            "step: 370, loss: 3.745492722373456e-05\n",
            "step: 380, loss: 9.121902985498309e-05\n",
            "step: 390, loss: 0.0009852616349235177\n",
            "step: 400, loss: 0.004419515375047922\n",
            "step: 410, loss: 0.00010554204345680773\n",
            "step: 420, loss: 0.002274012193083763\n",
            "step: 430, loss: 3.911148814950138e-05\n",
            "step: 440, loss: 0.0001002444259938784\n",
            "step: 450, loss: 5.17457774549257e-05\n",
            "step: 460, loss: 0.0004456797323655337\n",
            "step: 470, loss: 6.288695294642821e-05\n",
            "step: 480, loss: 0.00012362045526970178\n",
            "step: 490, loss: 0.0010467636166140437\n",
            "step: 500, loss: 3.0072797017055564e-05\n",
            "step: 510, loss: 0.002017635153606534\n",
            "step: 520, loss: 0.000219856490730308\n",
            "step: 530, loss: 0.0036740568466484547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.923728813559322, f1=0.9177693761814745, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006747301667928696\n",
            "step: 10, loss: 7.01288809068501e-05\n",
            "step: 20, loss: 5.972134385956451e-05\n",
            "step: 30, loss: 3.495650889817625e-05\n",
            "step: 40, loss: 3.8159701944096014e-05\n",
            "step: 50, loss: 4.7122641262831166e-05\n",
            "step: 60, loss: 8.649878873256966e-05\n",
            "step: 70, loss: 0.00020153749210294336\n",
            "step: 80, loss: 3.603811637731269e-05\n",
            "step: 90, loss: 0.0013704978628084064\n",
            "step: 100, loss: 3.146562448819168e-05\n",
            "step: 110, loss: 2.7912423320231028e-05\n",
            "step: 120, loss: 0.00011165066825924441\n",
            "step: 130, loss: 2.2619406081503257e-05\n",
            "step: 140, loss: 0.00022991005971562117\n",
            "step: 150, loss: 0.00013647427840624005\n",
            "step: 160, loss: 2.2146034098113887e-05\n",
            "step: 170, loss: 4.880688356934115e-05\n",
            "step: 180, loss: 0.0002629792725201696\n",
            "step: 190, loss: 0.0003456133999861777\n",
            "step: 200, loss: 2.6385348974145018e-05\n",
            "step: 210, loss: 4.47713682660833e-05\n",
            "step: 220, loss: 0.0004976309137418866\n",
            "step: 230, loss: 2.8312750146142207e-05\n",
            "step: 240, loss: 0.006287263706326485\n",
            "step: 250, loss: 2.199730442953296e-05\n",
            "step: 260, loss: 2.194490298279561e-05\n",
            "step: 270, loss: 2.1560786990448833e-05\n",
            "step: 280, loss: 2.0559507902362384e-05\n",
            "step: 290, loss: 2.37437998293899e-05\n",
            "step: 300, loss: 0.00013353521353565156\n",
            "step: 310, loss: 0.005166496615856886\n",
            "step: 320, loss: 0.0022634451743215322\n",
            "step: 330, loss: 0.00120248319581151\n",
            "step: 340, loss: 3.072402978432365e-05\n",
            "step: 350, loss: 3.25188048009295e-05\n",
            "step: 360, loss: 2.385589868936222e-05\n",
            "step: 370, loss: 6.895154365338385e-05\n",
            "step: 380, loss: 5.355978282750584e-05\n",
            "step: 390, loss: 0.013077208772301674\n",
            "step: 400, loss: 0.0036656649317592382\n",
            "step: 410, loss: 0.00032640955760143697\n",
            "step: 420, loss: 1.800407153496053e-05\n",
            "step: 430, loss: 0.014620697125792503\n",
            "step: 440, loss: 0.00017805893730837852\n",
            "step: 450, loss: 9.40446843742393e-05\n",
            "step: 460, loss: 0.0005873360205441713\n",
            "step: 470, loss: 7.810159877408296e-05\n",
            "step: 480, loss: 0.001256524003110826\n",
            "step: 490, loss: 0.000527491036336869\n",
            "step: 500, loss: 2.9155949960113503e-05\n",
            "step: 510, loss: 0.00022757890110369772\n",
            "step: 520, loss: 0.0042352089658379555\n",
            "step: 530, loss: 1.8164069842896424e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9254571026722925, f1=0.9255419415645617, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.246965494938195e-05\n",
            "step: 10, loss: 1.1671240827126894e-05\n",
            "step: 20, loss: 2.496096567483619e-05\n",
            "step: 30, loss: 4.8196041461778805e-05\n",
            "step: 40, loss: 3.375884625711478e-05\n",
            "step: 50, loss: 0.00012317353684920818\n",
            "step: 60, loss: 1.1593017006816808e-05\n",
            "step: 70, loss: 3.514117997838184e-05\n",
            "step: 80, loss: 1.574660745973233e-05\n",
            "step: 90, loss: 3.819811172434129e-05\n",
            "step: 100, loss: 0.0004016718012280762\n",
            "step: 110, loss: 2.714803304115776e-05\n",
            "step: 120, loss: 4.0968225221149623e-05\n",
            "step: 130, loss: 0.0003177060862071812\n",
            "step: 140, loss: 3.013790410477668e-05\n",
            "step: 150, loss: 3.348951577208936e-05\n",
            "step: 160, loss: 0.00030003051506355405\n",
            "step: 170, loss: 2.3099615646060556e-05\n",
            "step: 180, loss: 1.50052692333702e-05\n",
            "step: 190, loss: 0.00025733900838531554\n",
            "step: 200, loss: 2.7375761419534683e-05\n",
            "step: 210, loss: 4.68579055450391e-05\n",
            "step: 220, loss: 1.5146812074817717e-05\n",
            "step: 230, loss: 2.382268212386407e-05\n",
            "step: 240, loss: 6.968898378545418e-05\n",
            "step: 250, loss: 3.254602779634297e-05\n",
            "step: 260, loss: 2.222808325313963e-05\n",
            "step: 270, loss: 3.84837003366556e-05\n",
            "step: 280, loss: 1.4889814337948337e-05\n",
            "step: 290, loss: 0.008959325961768627\n",
            "step: 300, loss: 4.7663656005170196e-05\n",
            "step: 310, loss: 0.006964335683733225\n",
            "step: 320, loss: 0.00046088328235782683\n",
            "step: 330, loss: 5.382368544815108e-05\n",
            "step: 340, loss: 7.563051622128114e-05\n",
            "step: 350, loss: 0.0001143874615081586\n",
            "step: 360, loss: 0.0366409495472908\n",
            "step: 370, loss: 3.0279197744675912e-05\n",
            "step: 380, loss: 5.1900860853493214e-05\n",
            "step: 390, loss: 0.006678550038486719\n",
            "step: 400, loss: 0.0030640612822026014\n",
            "step: 410, loss: 2.6876259653363377e-05\n",
            "step: 420, loss: 0.00925316009670496\n",
            "step: 430, loss: 0.00015686865663155913\n",
            "step: 440, loss: 6.515980930998921e-05\n",
            "step: 450, loss: 7.254620140884072e-05\n",
            "step: 460, loss: 4.306982009438798e-05\n",
            "step: 470, loss: 0.00020247881184332073\n",
            "step: 480, loss: 1.817515294533223e-05\n",
            "step: 490, loss: 1.2393935321597382e-05\n",
            "step: 500, loss: 1.6104178939713165e-05\n",
            "step: 510, loss: 4.070586510351859e-05\n",
            "step: 520, loss: 1.592160333530046e-05\n",
            "step: 530, loss: 7.49628379708156e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9240093240093239, f1=0.9256661991584852, best_f1=0.9301675977653632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021986755018588156\n",
            "step: 10, loss: 1.728108327370137e-05\n",
            "step: 20, loss: 1.9903836800949648e-05\n",
            "step: 30, loss: 0.0003042147436644882\n",
            "step: 40, loss: 0.0387883298099041\n",
            "step: 50, loss: 4.194149005343206e-05\n",
            "step: 60, loss: 2.2361076844390482e-05\n",
            "step: 70, loss: 0.0035732053220272064\n",
            "step: 80, loss: 2.8835651392000727e-05\n",
            "step: 90, loss: 1.5500758308917284e-05\n",
            "step: 100, loss: 9.497452265350148e-05\n",
            "step: 110, loss: 4.304706089897081e-05\n",
            "step: 120, loss: 4.5328251871978864e-05\n",
            "step: 130, loss: 0.023808741942048073\n",
            "step: 140, loss: 1.5221266949083656e-05\n",
            "step: 150, loss: 2.691666486498434e-05\n",
            "step: 160, loss: 5.575110481004231e-05\n",
            "step: 170, loss: 5.871309622307308e-05\n",
            "step: 180, loss: 1.3001111256016884e-05\n",
            "step: 190, loss: 3.092388215009123e-05\n",
            "step: 200, loss: 2.7014413717552088e-05\n",
            "step: 210, loss: 0.0002583371242508292\n",
            "step: 220, loss: 2.705748556763865e-05\n",
            "step: 230, loss: 0.0016530497232452035\n",
            "step: 240, loss: 2.87727452814579e-05\n",
            "step: 250, loss: 1.614516440895386e-05\n",
            "step: 260, loss: 4.093044844921678e-05\n",
            "step: 270, loss: 1.4260249372455291e-05\n",
            "step: 280, loss: 1.6543779565836303e-05\n",
            "step: 290, loss: 1.100070949178189e-05\n",
            "step: 300, loss: 1.247959335159976e-05\n",
            "step: 310, loss: 2.6715410058386624e-05\n",
            "step: 320, loss: 1.235294621437788e-05\n",
            "step: 330, loss: 2.1345151253626682e-05\n",
            "step: 340, loss: 1.7232843674719334e-05\n",
            "step: 350, loss: 1.8688608179218136e-05\n",
            "step: 360, loss: 0.00021546060452237725\n",
            "step: 370, loss: 1.1958080904150847e-05\n",
            "step: 380, loss: 1.670770870987326e-05\n",
            "step: 390, loss: 2.620868690428324e-05\n",
            "step: 400, loss: 6.020576984155923e-05\n",
            "step: 410, loss: 3.328904858790338e-05\n",
            "step: 420, loss: 2.5215103960363194e-05\n",
            "step: 430, loss: 7.446280505973846e-05\n",
            "step: 440, loss: 2.060402766801417e-05\n",
            "step: 450, loss: 1.2449797395674977e-05\n",
            "step: 460, loss: 1.744522523949854e-05\n",
            "step: 470, loss: 0.009204726666212082\n",
            "step: 480, loss: 1.2211364264658187e-05\n",
            "step: 490, loss: 1.1037958756787702e-05\n",
            "step: 500, loss: 0.00010896598541876301\n",
            "step: 510, loss: 1.535172668809537e-05\n",
            "step: 520, loss: 1.591052750882227e-05\n",
            "step: 530, loss: 4.3820979044539854e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9250116441546343, f1=0.9245107176141659, best_f1=0.9301675977653632\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 247.79it/s]\n",
            "load_f1 = 0.933826931975937\n",
            "real_f1 = 0.933579335793358\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 242.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e274c0-e905-4d05-ddc7-9169d739ba1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5598036050796509\n",
            "step: 10, loss: 0.3724798858165741\n",
            "step: 20, loss: 0.3711826205253601\n",
            "step: 30, loss: 0.32394054532051086\n",
            "step: 40, loss: 0.15782466530799866\n",
            "step: 50, loss: 0.4257924556732178\n",
            "step: 60, loss: 0.2819114923477173\n",
            "step: 70, loss: 0.1687166690826416\n",
            "step: 80, loss: 0.19220884144306183\n",
            "step: 90, loss: 0.32432931661605835\n",
            "step: 100, loss: 0.29886823892593384\n",
            "step: 110, loss: 0.2283511608839035\n",
            "step: 120, loss: 0.20441561937332153\n",
            "step: 130, loss: 0.1765952706336975\n",
            "step: 140, loss: 0.2474832385778427\n",
            "step: 150, loss: 0.24614039063453674\n",
            "step: 160, loss: 0.23412856459617615\n",
            "step: 170, loss: 0.2832360863685608\n",
            "step: 180, loss: 0.10937520861625671\n",
            "step: 190, loss: 0.23019027709960938\n",
            "step: 200, loss: 0.2768842875957489\n",
            "step: 210, loss: 0.1954365372657776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5577689243027888, f1=0.6029106029106029, best_f1=0.6029106029106029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1887277513742447\n",
            "step: 10, loss: 0.2917536199092865\n",
            "step: 20, loss: 0.1877974420785904\n",
            "step: 30, loss: 0.2271943837404251\n",
            "step: 40, loss: 0.3054088354110718\n",
            "step: 50, loss: 0.13405056297779083\n",
            "step: 60, loss: 0.312446653842926\n",
            "step: 70, loss: 0.15952517092227936\n",
            "step: 80, loss: 0.15314051508903503\n",
            "step: 90, loss: 0.11414064466953278\n",
            "step: 100, loss: 0.016651280224323273\n",
            "step: 110, loss: 0.1845061033964157\n",
            "step: 120, loss: 0.14541210234165192\n",
            "step: 130, loss: 0.03561607748270035\n",
            "step: 140, loss: 0.14115968346595764\n",
            "step: 150, loss: 0.2504834532737732\n",
            "step: 160, loss: 0.18280285596847534\n",
            "step: 170, loss: 0.10971717536449432\n",
            "step: 180, loss: 0.18671655654907227\n",
            "step: 190, loss: 0.27811598777770996\n",
            "step: 200, loss: 0.07039676606655121\n",
            "step: 210, loss: 0.14356347918510437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5807692307692308, f1=0.5964912280701755, best_f1=0.5964912280701755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05592883750796318\n",
            "step: 10, loss: 0.1395963430404663\n",
            "step: 20, loss: 0.1440947949886322\n",
            "step: 30, loss: 0.06494157016277313\n",
            "step: 40, loss: 0.1443946808576584\n",
            "step: 50, loss: 0.10042446106672287\n",
            "step: 60, loss: 0.15180723369121552\n",
            "step: 70, loss: 0.07649175077676773\n",
            "step: 80, loss: 0.19419313967227936\n",
            "step: 90, loss: 0.05536219850182533\n",
            "step: 100, loss: 0.1487531065940857\n",
            "step: 110, loss: 0.26447027921676636\n",
            "step: 120, loss: 0.14968241751194\n",
            "step: 130, loss: 0.2001883089542389\n",
            "step: 140, loss: 0.1021946594119072\n",
            "step: 150, loss: 0.1743331253528595\n",
            "step: 160, loss: 0.03898724168539047\n",
            "step: 170, loss: 0.2157101333141327\n",
            "step: 180, loss: 0.09947267919778824\n",
            "step: 190, loss: 0.188408762216568\n",
            "step: 200, loss: 0.0311172716319561\n",
            "step: 210, loss: 0.13408252596855164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.5841392649903289, f1=0.6330097087378641, best_f1=0.6330097087378641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1761915683746338\n",
            "step: 10, loss: 0.07003211975097656\n",
            "step: 20, loss: 0.041822370141744614\n",
            "step: 30, loss: 0.12127557396888733\n",
            "step: 40, loss: 0.018630629405379295\n",
            "step: 50, loss: 0.3050701320171356\n",
            "step: 60, loss: 0.19458192586898804\n",
            "step: 70, loss: 0.22916020452976227\n",
            "step: 80, loss: 0.08853716403245926\n",
            "step: 90, loss: 0.05604640394449234\n",
            "step: 100, loss: 0.23611435294151306\n",
            "step: 110, loss: 0.09689179807901382\n",
            "step: 120, loss: 0.14567790925502777\n",
            "step: 130, loss: 0.21805283427238464\n",
            "step: 140, loss: 0.19200602173805237\n",
            "step: 150, loss: 0.03883733972907066\n",
            "step: 160, loss: 0.05303743854165077\n",
            "step: 170, loss: 0.15159285068511963\n",
            "step: 180, loss: 0.3295295238494873\n",
            "step: 190, loss: 0.06136636435985565\n",
            "step: 200, loss: 0.15663041174411774\n",
            "step: 210, loss: 0.19279645383358002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6131386861313869, f1=0.6233766233766235, best_f1=0.6233766233766235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1358800083398819\n",
            "step: 10, loss: 0.3075452148914337\n",
            "step: 20, loss: 0.37591758370399475\n",
            "step: 30, loss: 0.08910229057073593\n",
            "step: 40, loss: 0.0341729111969471\n",
            "step: 50, loss: 0.06093299388885498\n",
            "step: 60, loss: 0.09123119711875916\n",
            "step: 70, loss: 0.10841576755046844\n",
            "step: 80, loss: 0.08981432020664215\n",
            "step: 90, loss: 0.1467059850692749\n",
            "step: 100, loss: 0.0020013817120343447\n",
            "step: 110, loss: 0.1012880727648735\n",
            "step: 120, loss: 0.22137020528316498\n",
            "step: 130, loss: 0.08975016325712204\n",
            "step: 140, loss: 0.10814248770475388\n",
            "step: 150, loss: 0.06598053127527237\n",
            "step: 160, loss: 0.2073296308517456\n",
            "step: 170, loss: 0.06348360329866409\n",
            "step: 180, loss: 0.03568857163190842\n",
            "step: 190, loss: 0.050461504608392715\n",
            "step: 200, loss: 0.08577824383974075\n",
            "step: 210, loss: 0.05240341275930405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6134453781512605, f1=0.6150627615062761, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06791884452104568\n",
            "step: 10, loss: 0.056246429681777954\n",
            "step: 20, loss: 0.013278260827064514\n",
            "step: 30, loss: 0.005311715882271528\n",
            "step: 40, loss: 0.03950103372335434\n",
            "step: 50, loss: 0.035311561077833176\n",
            "step: 60, loss: 0.12717430293560028\n",
            "step: 70, loss: 0.03900650888681412\n",
            "step: 80, loss: 0.2401895523071289\n",
            "step: 90, loss: 0.1556638926267624\n",
            "step: 100, loss: 0.0034073875285685062\n",
            "step: 110, loss: 0.05151527374982834\n",
            "step: 120, loss: 0.12677089869976044\n",
            "step: 130, loss: 0.35700175166130066\n",
            "step: 140, loss: 0.09051655232906342\n",
            "step: 150, loss: 0.044989585876464844\n",
            "step: 160, loss: 0.056572359055280685\n",
            "step: 170, loss: 0.17216870188713074\n",
            "step: 180, loss: 0.04845832288265228\n",
            "step: 190, loss: 0.06726638972759247\n",
            "step: 200, loss: 0.01941925659775734\n",
            "step: 210, loss: 0.03244544565677643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5783132530120482, f1=0.6223091976516635, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04500409960746765\n",
            "step: 10, loss: 0.18394100666046143\n",
            "step: 20, loss: 0.012024104595184326\n",
            "step: 30, loss: 0.044502127915620804\n",
            "step: 40, loss: 0.021453870460391045\n",
            "step: 50, loss: 0.1144186332821846\n",
            "step: 60, loss: 0.04227632284164429\n",
            "step: 70, loss: 0.01750374399125576\n",
            "step: 80, loss: 0.10415192693471909\n",
            "step: 90, loss: 0.03452767804265022\n",
            "step: 100, loss: 0.051370769739151\n",
            "step: 110, loss: 0.11434818804264069\n",
            "step: 120, loss: 0.04931601881980896\n",
            "step: 130, loss: 0.1058887243270874\n",
            "step: 140, loss: 0.021735114976763725\n",
            "step: 150, loss: 0.06571725010871887\n",
            "step: 160, loss: 0.06278405338525772\n",
            "step: 170, loss: 0.006753807421773672\n",
            "step: 180, loss: 0.12606970965862274\n",
            "step: 190, loss: 0.03840116038918495\n",
            "step: 200, loss: 0.019184496253728867\n",
            "step: 210, loss: 0.012241905555129051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5911708253358925, f1=0.6118546845124283, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03551917150616646\n",
            "step: 10, loss: 0.04278747737407684\n",
            "step: 20, loss: 0.014411550015211105\n",
            "step: 30, loss: 0.001838164054788649\n",
            "step: 40, loss: 0.06521841883659363\n",
            "step: 50, loss: 0.10279514640569687\n",
            "step: 60, loss: 0.14683392643928528\n",
            "step: 70, loss: 0.01333583053201437\n",
            "step: 80, loss: 0.08009949326515198\n",
            "step: 90, loss: 0.010130398906767368\n",
            "step: 100, loss: 0.09462790936231613\n",
            "step: 110, loss: 0.04997800290584564\n",
            "step: 120, loss: 0.019794659689068794\n",
            "step: 130, loss: 0.006079118698835373\n",
            "step: 140, loss: 0.07365964353084564\n",
            "step: 150, loss: 0.13887859880924225\n",
            "step: 160, loss: 0.04023422673344612\n",
            "step: 170, loss: 0.04524162411689758\n",
            "step: 180, loss: 0.05215667560696602\n",
            "step: 190, loss: 0.021766401827335358\n",
            "step: 200, loss: 0.00527178542688489\n",
            "step: 210, loss: 0.1349562704563141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5833333333333333, f1=0.5838779956427015, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0400240458548069\n",
            "step: 10, loss: 0.07427675276994705\n",
            "step: 20, loss: 0.013974439352750778\n",
            "step: 30, loss: 0.016434194520115852\n",
            "step: 40, loss: 0.07276245206594467\n",
            "step: 50, loss: 0.2664376497268677\n",
            "step: 60, loss: 0.07128296047449112\n",
            "step: 70, loss: 0.03620029613375664\n",
            "step: 80, loss: 0.037850793451070786\n",
            "step: 90, loss: 0.050959475338459015\n",
            "step: 100, loss: 0.07600944489240646\n",
            "step: 110, loss: 0.04962853342294693\n",
            "step: 120, loss: 0.00402718223631382\n",
            "step: 130, loss: 0.015062617138028145\n",
            "step: 140, loss: 0.015807582065463066\n",
            "step: 150, loss: 0.04623973369598389\n",
            "step: 160, loss: 0.005137254949659109\n",
            "step: 170, loss: 0.007451658137142658\n",
            "step: 180, loss: 0.04835083708167076\n",
            "step: 190, loss: 0.00463412469252944\n",
            "step: 200, loss: 0.08840131759643555\n",
            "step: 210, loss: 0.05357997119426727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.576923076923077, f1=0.603415559772296, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02842884138226509\n",
            "step: 10, loss: 0.054489586502313614\n",
            "step: 20, loss: 0.0049280510284006596\n",
            "step: 30, loss: 0.11490694433450699\n",
            "step: 40, loss: 0.0054540918208658695\n",
            "step: 50, loss: 0.007783061359077692\n",
            "step: 60, loss: 0.0632421150803566\n",
            "step: 70, loss: 0.039779532700777054\n",
            "step: 80, loss: 0.08183122426271439\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 90, loss: 0.10375906527042389\n",
            "step: 100, loss: 0.025289155542850494\n",
            "step: 110, loss: 0.01363386120647192\n",
            "step: 120, loss: 0.023928984999656677\n",
            "step: 130, loss: 0.07000978291034698\n",
            "step: 140, loss: 0.1137956753373146\n",
            "step: 150, loss: 0.0553729347884655\n",
            "step: 160, loss: 0.07058236747980118\n",
            "step: 170, loss: 0.008585772477090359\n",
            "step: 180, loss: 0.040805261582136154\n",
            "step: 190, loss: 0.26701271533966064\n",
            "step: 200, loss: 0.016408493742346764\n",
            "step: 210, loss: 0.11495840549468994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.583170254403131, f1=0.5877862595419848, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022121572867035866\n",
            "step: 10, loss: 0.012404796667397022\n",
            "step: 20, loss: 0.03696329519152641\n",
            "step: 30, loss: 0.008150286041200161\n",
            "step: 40, loss: 0.012056923471391201\n",
            "step: 50, loss: 0.0024900222197175026\n",
            "step: 60, loss: 0.07579828053712845\n",
            "step: 70, loss: 0.008149194531142712\n",
            "step: 80, loss: 0.10163899511098862\n",
            "step: 90, loss: 0.014450409449636936\n",
            "step: 100, loss: 0.05545773729681969\n",
            "step: 110, loss: 0.043803371489048004\n",
            "step: 120, loss: 0.0460045300424099\n",
            "step: 130, loss: 0.012823894619941711\n",
            "step: 140, loss: 0.013694755733013153\n",
            "step: 150, loss: 0.0012600196059793234\n",
            "step: 160, loss: 0.005689549725502729\n",
            "step: 170, loss: 0.045832559466362\n",
            "step: 180, loss: 0.005027105566114187\n",
            "step: 190, loss: 0.012184408493340015\n",
            "step: 200, loss: 0.0023869189899414778\n",
            "step: 210, loss: 0.0023014270700514317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5708582834331337, f1=0.5776031434184675, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02461201697587967\n",
            "step: 10, loss: 0.003672220977023244\n",
            "step: 20, loss: 0.03645826503634453\n",
            "step: 30, loss: 0.06274808943271637\n",
            "step: 40, loss: 0.08967742323875427\n",
            "step: 50, loss: 0.002330037532374263\n",
            "step: 60, loss: 0.07918699085712433\n",
            "step: 70, loss: 0.015210231766104698\n",
            "step: 80, loss: 0.0033182334154844284\n",
            "step: 90, loss: 0.025355264544487\n",
            "step: 100, loss: 0.042204126715660095\n",
            "step: 110, loss: 0.003922413568943739\n",
            "step: 120, loss: 0.04869980737566948\n",
            "step: 130, loss: 0.004273551050573587\n",
            "step: 140, loss: 0.0012670510914176702\n",
            "step: 150, loss: 0.08631602674722672\n",
            "step: 160, loss: 0.020300183445215225\n",
            "step: 170, loss: 0.010893880389630795\n",
            "step: 180, loss: 0.016984963789582253\n",
            "step: 190, loss: 0.016049016267061234\n",
            "step: 200, loss: 0.012290786020457745\n",
            "step: 210, loss: 0.0020288312807679176\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5666666666666667, f1=0.5802469135802469, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013363828882575035\n",
            "step: 10, loss: 0.00038452379521913826\n",
            "step: 20, loss: 0.08373091369867325\n",
            "step: 30, loss: 0.19294269382953644\n",
            "step: 40, loss: 0.014437548816204071\n",
            "step: 50, loss: 0.005449514836072922\n",
            "step: 60, loss: 0.001352564780972898\n",
            "step: 70, loss: 0.02738841623067856\n",
            "step: 80, loss: 0.00727388309314847\n",
            "step: 90, loss: 0.0005470588803291321\n",
            "step: 100, loss: 0.002218283014371991\n",
            "step: 110, loss: 0.012102843262255192\n",
            "step: 120, loss: 0.049525875598192215\n",
            "step: 130, loss: 0.0560002401471138\n",
            "step: 140, loss: 0.013239783234894276\n",
            "step: 150, loss: 0.017505327239632607\n",
            "step: 160, loss: 0.02651493065059185\n",
            "step: 170, loss: 0.07348839938640594\n",
            "step: 180, loss: 0.18401531875133514\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.0053323195315897465\n",
            "step: 200, loss: 0.002662640530616045\n",
            "step: 210, loss: 0.005958002991974354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5625000000000001, f1=0.5627376425855514, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0063949753530323505\n",
            "step: 10, loss: 0.037266697734594345\n",
            "step: 20, loss: 0.005555859301239252\n",
            "step: 30, loss: 0.038072213530540466\n",
            "step: 40, loss: 0.01370995119214058\n",
            "step: 50, loss: 0.03592122346162796\n",
            "step: 60, loss: 0.006484565325081348\n",
            "step: 70, loss: 0.02848643809556961\n",
            "step: 80, loss: 0.006734847091138363\n",
            "step: 90, loss: 0.0069362446665763855\n",
            "step: 100, loss: 0.03720000013709068\n",
            "step: 110, loss: 0.11054122447967529\n",
            "step: 120, loss: 0.007277941331267357\n",
            "step: 130, loss: 0.005659548565745354\n",
            "step: 140, loss: 0.04811783507466316\n",
            "step: 150, loss: 0.013542412780225277\n",
            "step: 160, loss: 0.01634407229721546\n",
            "step: 170, loss: 0.018499698489904404\n",
            "step: 180, loss: 0.0014622207963839173\n",
            "step: 190, loss: 0.004960612393915653\n",
            "step: 200, loss: 0.027959268540143967\n",
            "step: 210, loss: 0.004700921941548586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5689655172413792, f1=0.5842217484008528, best_f1=0.6150627615062761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005664885509759188\n",
            "step: 10, loss: 0.01443135179579258\n",
            "step: 20, loss: 0.011330030858516693\n",
            "step: 30, loss: 0.027116132900118828\n",
            "step: 40, loss: 0.012379948981106281\n",
            "step: 50, loss: 0.0017625009641051292\n",
            "step: 60, loss: 0.053304076194763184\n",
            "step: 70, loss: 0.005041378550231457\n",
            "step: 80, loss: 0.0017813952872529626\n",
            "step: 90, loss: 0.007368307560682297\n",
            "step: 100, loss: 0.013345913961529732\n",
            "step: 110, loss: 0.006330669391900301\n",
            "step: 120, loss: 0.009726883843541145\n",
            "step: 130, loss: 0.13836796581745148\n",
            "step: 140, loss: 0.0014850231818854809\n",
            "step: 150, loss: 0.0008521999116055667\n",
            "step: 160, loss: 0.022389844059944153\n",
            "step: 170, loss: 0.00043754972284659743\n",
            "step: 180, loss: 0.0014113446231931448\n",
            "step: 190, loss: 0.004579621367156506\n",
            "step: 200, loss: 0.008729131892323494\n",
            "step: 210, loss: 0.02431696653366089\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5695652173913044, f1=0.5824411134903641, best_f1=0.6150627615062761\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 279.33it/s]\n",
            "load_f1 = 0.5984555984555985\n",
            "real_f1 = 0.5984555984555985\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9fff91-c9ce-47e1-e93e-819c5afbdcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5860747694969177\n",
            "step: 10, loss: 0.3699457049369812\n",
            "step: 20, loss: 0.29852885007858276\n",
            "step: 30, loss: 0.43563222885131836\n",
            "step: 40, loss: 0.4283563494682312\n",
            "step: 50, loss: 0.28739842772483826\n",
            "step: 60, loss: 0.26915502548217773\n",
            "step: 70, loss: 0.2721223533153534\n",
            "step: 80, loss: 0.3803044855594635\n",
            "step: 90, loss: 0.2921803593635559\n",
            "step: 100, loss: 0.3123225271701813\n",
            "step: 110, loss: 0.37398913502693176\n",
            "step: 120, loss: 0.07097073644399643\n",
            "step: 130, loss: 0.16173309087753296\n",
            "step: 140, loss: 0.12701542675495148\n",
            "step: 150, loss: 0.1784985214471817\n",
            "step: 160, loss: 0.12768356502056122\n",
            "step: 170, loss: 0.21265456080436707\n",
            "step: 180, loss: 0.037735968828201294\n",
            "step: 190, loss: 0.19754242897033691\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.689655172413793, f1=0.7200000000000001, best_f1=0.7200000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41063982248306274\n",
            "step: 10, loss: 0.13873662054538727\n",
            "step: 20, loss: 0.14243528246879578\n",
            "step: 30, loss: 0.09658415615558624\n",
            "step: 40, loss: 0.09155689924955368\n",
            "step: 50, loss: 0.1715438961982727\n",
            "step: 60, loss: 0.2296360433101654\n",
            "step: 70, loss: 0.18812036514282227\n",
            "step: 80, loss: 0.11694139242172241\n",
            "step: 90, loss: 0.174770787358284\n",
            "step: 100, loss: 0.01404343731701374\n",
            "step: 110, loss: 0.08454569429159164\n",
            "step: 120, loss: 0.23567049205303192\n",
            "step: 130, loss: 0.025750569999217987\n",
            "step: 140, loss: 0.03462538868188858\n",
            "step: 150, loss: 0.11624882370233536\n",
            "step: 160, loss: 0.06506526470184326\n",
            "step: 170, loss: 0.08335327357053757\n",
            "step: 180, loss: 0.2164388746023178\n",
            "step: 190, loss: 0.057422738522291183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6983240223463687, f1=0.7138964577656676, best_f1=0.7138964577656676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05833734944462776\n",
            "step: 10, loss: 0.215668722987175\n",
            "step: 20, loss: 0.018682807683944702\n",
            "step: 30, loss: 0.07680007070302963\n",
            "step: 40, loss: 0.031001750379800797\n",
            "step: 50, loss: 0.03426777198910713\n",
            "step: 60, loss: 0.030910154804587364\n",
            "step: 70, loss: 0.20360040664672852\n",
            "step: 80, loss: 0.0737101137638092\n",
            "step: 90, loss: 0.06317804008722305\n",
            "step: 100, loss: 0.08043084293603897\n",
            "step: 110, loss: 0.00754470145329833\n",
            "step: 120, loss: 0.009222608990967274\n",
            "step: 130, loss: 0.004132918547838926\n",
            "step: 140, loss: 0.060433946549892426\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.1384047120809555\n",
            "step: 160, loss: 0.08051764219999313\n",
            "step: 170, loss: 0.05051286518573761\n",
            "step: 180, loss: 0.03823181614279747\n",
            "step: 190, loss: 0.1117570698261261\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7616580310880829, f1=0.7708333333333334, best_f1=0.7708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01942298375070095\n",
            "step: 10, loss: 0.1487920582294464\n",
            "step: 20, loss: 0.01561746746301651\n",
            "step: 30, loss: 0.06942711025476456\n",
            "step: 40, loss: 0.033753324300050735\n",
            "step: 50, loss: 0.07632255554199219\n",
            "step: 60, loss: 0.013407742604613304\n",
            "step: 70, loss: 0.06238960102200508\n",
            "step: 80, loss: 0.09247173368930817\n",
            "step: 90, loss: 0.0668671727180481\n",
            "step: 100, loss: 0.1416480988264084\n",
            "step: 110, loss: 0.004992746748030186\n",
            "step: 120, loss: 0.016437828540802002\n",
            "step: 130, loss: 0.20305849611759186\n",
            "step: 140, loss: 0.021932493895292282\n",
            "step: 150, loss: 0.1256922334432602\n",
            "step: 160, loss: 0.022877370938658714\n",
            "step: 170, loss: 0.08009079098701477\n",
            "step: 180, loss: 0.021593105047941208\n",
            "step: 190, loss: 0.13653624057769775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7819148936170213, f1=0.7431693989071039, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.046867743134498596\n",
            "step: 10, loss: 0.008822151459753513\n",
            "step: 20, loss: 0.062441933900117874\n",
            "step: 30, loss: 0.00897202454507351\n",
            "step: 40, loss: 0.1870037317276001\n",
            "step: 50, loss: 0.040516551584005356\n",
            "step: 60, loss: 0.20056934654712677\n",
            "step: 70, loss: 0.028555743396282196\n",
            "step: 80, loss: 0.014274933375418186\n",
            "step: 90, loss: 0.007344079669564962\n",
            "step: 100, loss: 0.003958349581807852\n",
            "step: 110, loss: 0.012727775610983372\n",
            "step: 120, loss: 0.027578184381127357\n",
            "step: 130, loss: 0.030793873593211174\n",
            "step: 140, loss: 0.03615182638168335\n",
            "step: 150, loss: 0.018960123881697655\n",
            "step: 160, loss: 0.003627296071499586\n",
            "step: 170, loss: 0.0017280913889408112\n",
            "step: 180, loss: 0.17828652262687683\n",
            "step: 190, loss: 0.08885205537080765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7679558011049724, f1=0.7818696883852692, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006504178512841463\n",
            "step: 10, loss: 0.02468474954366684\n",
            "step: 20, loss: 0.009791286662220955\n",
            "step: 30, loss: 0.0027160763274878263\n",
            "step: 40, loss: 0.08111745864152908\n",
            "step: 50, loss: 0.02369275689125061\n",
            "step: 60, loss: 0.0021611680276691914\n",
            "step: 70, loss: 0.009452786296606064\n",
            "step: 80, loss: 0.03934541717171669\n",
            "step: 90, loss: 0.004988270346075296\n",
            "step: 100, loss: 0.0012097805738449097\n",
            "step: 110, loss: 0.015886541455984116\n",
            "step: 120, loss: 0.1272667497396469\n",
            "step: 130, loss: 0.0294302087277174\n",
            "step: 140, loss: 0.0959126204252243\n",
            "step: 150, loss: 0.001304954057559371\n",
            "step: 160, loss: 0.033749669790267944\n",
            "step: 170, loss: 0.014543374069035053\n",
            "step: 180, loss: 0.006596689578145742\n",
            "step: 190, loss: 0.04346676915884018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7801047120418847, f1=0.779291553133515, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022686293814331293\n",
            "step: 10, loss: 0.002287331037223339\n",
            "step: 20, loss: 0.0457039438188076\n",
            "step: 30, loss: 0.04504550248384476\n",
            "step: 40, loss: 0.004516434855759144\n",
            "step: 50, loss: 0.06984587758779526\n",
            "step: 60, loss: 0.0845959410071373\n",
            "step: 70, loss: 0.0014829599531367421\n",
            "step: 80, loss: 0.006699014455080032\n",
            "step: 90, loss: 0.01141088455915451\n",
            "step: 100, loss: 0.0010171085596084595\n",
            "step: 110, loss: 0.005318324547261\n",
            "step: 120, loss: 0.0007899105548858643\n",
            "step: 130, loss: 0.0012393315555527806\n",
            "step: 140, loss: 0.0039890725165605545\n",
            "step: 150, loss: 0.008620476350188255\n",
            "step: 160, loss: 0.09691125154495239\n",
            "step: 170, loss: 0.009142562747001648\n",
            "step: 180, loss: 0.003379608504474163\n",
            "step: 190, loss: 0.07783334702253342\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7519582245430808, f1=0.7637362637362638, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003052895423024893\n",
            "step: 10, loss: 0.0039247204549610615\n",
            "step: 20, loss: 0.005507759749889374\n",
            "step: 30, loss: 0.026896072551608086\n",
            "step: 40, loss: 0.004243208095431328\n",
            "step: 50, loss: 0.004308126866817474\n",
            "step: 60, loss: 0.017608920112252235\n",
            "step: 70, loss: 0.19182553887367249\n",
            "step: 80, loss: 0.0008633402176201344\n",
            "step: 90, loss: 0.003412820165976882\n",
            "step: 100, loss: 0.049580566585063934\n",
            "step: 110, loss: 0.0031211257446557283\n",
            "step: 120, loss: 0.002461896976456046\n",
            "step: 130, loss: 0.0028555605094879866\n",
            "step: 140, loss: 0.005869029089808464\n",
            "step: 150, loss: 0.025579817593097687\n",
            "step: 160, loss: 0.005839771591126919\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.0064052981324493885\n",
            "step: 180, loss: 0.11745210736989975\n",
            "step: 190, loss: 0.0669386237859726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7601078167115903, f1=0.7555555555555554, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008186392486095428\n",
            "step: 10, loss: 0.05752887576818466\n",
            "step: 20, loss: 0.027306215837597847\n",
            "step: 30, loss: 0.007338060066103935\n",
            "step: 40, loss: 0.0050886753015220165\n",
            "step: 50, loss: 0.0018842354184016585\n",
            "step: 60, loss: 0.018373854458332062\n",
            "step: 70, loss: 0.0009727111901156604\n",
            "step: 80, loss: 0.005839458666741848\n",
            "step: 90, loss: 0.005159558262676001\n",
            "step: 100, loss: 0.02568749152123928\n",
            "step: 110, loss: 0.10586817562580109\n",
            "step: 120, loss: 0.0018366453005000949\n",
            "step: 130, loss: 0.0029012365266680717\n",
            "step: 140, loss: 0.022274741902947426\n",
            "step: 150, loss: 0.16560621559619904\n",
            "step: 160, loss: 0.0005013669142499566\n",
            "step: 170, loss: 0.0015891434159129858\n",
            "step: 180, loss: 0.011437413282692432\n",
            "step: 190, loss: 0.001739352010190487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7631578947368421, f1=0.7431693989071039, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16636165976524353\n",
            "step: 10, loss: 0.017492400482296944\n",
            "step: 20, loss: 0.0657305121421814\n",
            "step: 30, loss: 0.0016672409838065505\n",
            "step: 40, loss: 0.07065758854150772\n",
            "step: 50, loss: 0.052130743861198425\n",
            "step: 60, loss: 0.0014792007859796286\n",
            "step: 70, loss: 0.0009025081526488066\n",
            "step: 80, loss: 0.013024871237576008\n",
            "step: 90, loss: 0.0019767179619520903\n",
            "step: 100, loss: 0.002733079483732581\n",
            "step: 110, loss: 0.0037193787284195423\n",
            "step: 120, loss: 0.11143873631954193\n",
            "step: 130, loss: 0.0005871022003702819\n",
            "step: 140, loss: 0.0009244380635209382\n",
            "step: 150, loss: 0.0015112493420019746\n",
            "step: 160, loss: 0.19680407643318176\n",
            "step: 170, loss: 0.0008870505262166262\n",
            "step: 180, loss: 0.0014651117380708456\n",
            "step: 190, loss: 0.0013852189294993877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.765498652291105, f1=0.7415730337078652, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002255016705021262\n",
            "step: 10, loss: 0.003128194948658347\n",
            "step: 20, loss: 0.29143571853637695\n",
            "step: 30, loss: 0.002280828543007374\n",
            "step: 40, loss: 0.002542814938351512\n",
            "step: 50, loss: 0.0030016798991709948\n",
            "step: 60, loss: 0.0022001040633767843\n",
            "step: 70, loss: 0.002009608317166567\n",
            "step: 80, loss: 0.001601871568709612\n",
            "step: 90, loss: 0.0034694448113441467\n",
            "step: 100, loss: 0.0004064622044097632\n",
            "step: 110, loss: 0.0010587325086817145\n",
            "step: 120, loss: 0.0019393155816942453\n",
            "step: 130, loss: 0.04219285398721695\n",
            "step: 140, loss: 0.002045291243121028\n",
            "step: 150, loss: 0.0009356517693959177\n",
            "step: 160, loss: 0.0017966885352507234\n",
            "step: 170, loss: 0.0007658274844288826\n",
            "step: 180, loss: 0.007486349903047085\n",
            "step: 190, loss: 0.004081686958670616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7567567567567567, f1=0.752808988764045, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009429504163563251\n",
            "step: 10, loss: 0.0026659017894417048\n",
            "step: 20, loss: 0.0006783903809264302\n",
            "step: 30, loss: 0.0009217248880304396\n",
            "step: 40, loss: 0.0008160057477653027\n",
            "step: 50, loss: 0.0019954906310886145\n",
            "step: 60, loss: 0.0009332456975243986\n",
            "step: 70, loss: 0.0031424607150256634\n",
            "step: 80, loss: 0.003221542574465275\n",
            "step: 90, loss: 0.004189217463135719\n",
            "step: 100, loss: 0.0008922258857637644\n",
            "step: 110, loss: 0.004527769051492214\n",
            "step: 120, loss: 0.012177322059869766\n",
            "step: 130, loss: 0.0210725087672472\n",
            "step: 140, loss: 0.00783412903547287\n",
            "step: 150, loss: 0.00268439669162035\n",
            "step: 160, loss: 0.00044219029950909317\n",
            "step: 170, loss: 0.0020610217470675707\n",
            "step: 180, loss: 0.004302562214434147\n",
            "step: 190, loss: 0.003243881743401289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7724867724867726, f1=0.7771739130434783, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017627837136387825\n",
            "step: 10, loss: 0.003664088435471058\n",
            "step: 20, loss: 0.0008972655050456524\n",
            "step: 30, loss: 0.0059359995648264885\n",
            "step: 40, loss: 0.0065207090228796005\n",
            "step: 50, loss: 0.03792504966259003\n",
            "step: 60, loss: 0.0008346408721990883\n",
            "step: 70, loss: 0.00223959656432271\n",
            "step: 80, loss: 0.001666333177126944\n",
            "step: 90, loss: 0.008031199686229229\n",
            "step: 100, loss: 0.029031898826360703\n",
            "step: 110, loss: 0.004958544857800007\n",
            "step: 120, loss: 0.0022442257031798363\n",
            "step: 130, loss: 0.005948720965534449\n",
            "step: 140, loss: 0.0004100005317013711\n",
            "step: 150, loss: 0.0004884991212747991\n",
            "step: 160, loss: 0.0018079282017424703\n",
            "step: 170, loss: 0.00043439221917651594\n",
            "step: 180, loss: 0.0006128231179900467\n",
            "step: 190, loss: 0.029949422925710678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7624020887728459, f1=0.7472527472527473, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011455550789833069\n",
            "step: 10, loss: 0.00039734956226311624\n",
            "step: 20, loss: 0.0006368986214511096\n",
            "step: 30, loss: 0.0006286200950853527\n",
            "step: 40, loss: 0.0010392883559688926\n",
            "step: 50, loss: 0.0009436543332412839\n",
            "step: 60, loss: 0.12271852046251297\n",
            "step: 70, loss: 0.01099249254912138\n",
            "step: 80, loss: 0.0026731824036687613\n",
            "step: 90, loss: 0.003856358351185918\n",
            "step: 100, loss: 0.021517563611268997\n",
            "step: 110, loss: 0.0017451152671128511\n",
            "step: 120, loss: 0.00627576420083642\n",
            "step: 130, loss: 0.0011697817826643586\n",
            "step: 140, loss: 0.0008434601477347314\n",
            "step: 150, loss: 0.0007123261457309127\n",
            "step: 160, loss: 0.0031684606801718473\n",
            "step: 170, loss: 0.0009381055715493858\n",
            "step: 180, loss: 0.005667706951498985\n",
            "step: 190, loss: 0.0010379317682236433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7567567567567567, f1=0.7605633802816902, best_f1=0.7431693989071039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008934746496379375\n",
            "step: 10, loss: 0.0006673657917417586\n",
            "step: 20, loss: 0.05470908433198929\n",
            "step: 30, loss: 0.0013476377353072166\n",
            "step: 40, loss: 0.0011005504056811333\n",
            "step: 50, loss: 0.00351284584030509\n",
            "step: 60, loss: 0.002062340034171939\n",
            "step: 70, loss: 0.0008644303889013827\n",
            "step: 80, loss: 0.0026942032855004072\n",
            "step: 90, loss: 0.001687712618149817\n",
            "step: 100, loss: 0.001342430361546576\n",
            "step: 110, loss: 0.001384460600093007\n",
            "step: 120, loss: 0.0005211810348555446\n",
            "step: 130, loss: 0.00021263725648168474\n",
            "step: 140, loss: 0.005351543892174959\n",
            "step: 150, loss: 0.00047860541963018477\n",
            "step: 160, loss: 0.005418866407126188\n",
            "step: 170, loss: 0.001593775232322514\n",
            "step: 180, loss: 0.015473474748432636\n",
            "step: 190, loss: 0.0003995979204773903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7573333333333333, f1=0.7423822714681441, best_f1=0.7431693989071039\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 170.28it/s]\n",
            "load_f1 = 0.6574585635359116\n",
            "real_f1 = 0.646900269541779\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 193.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152469a0-c91b-4518-9cc9-50c17e630606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6208709478378296\n",
            "step: 10, loss: 0.3753649890422821\n",
            "step: 20, loss: 0.3145114779472351\n",
            "step: 30, loss: 0.37894412875175476\n",
            "step: 40, loss: 0.2788117825984955\n",
            "step: 50, loss: 0.2894257605075836\n",
            "step: 60, loss: 0.2890865206718445\n",
            "step: 70, loss: 0.36827483773231506\n",
            "step: 80, loss: 0.35561051964759827\n",
            "step: 90, loss: 0.2311665117740631\n",
            "step: 100, loss: 0.22713589668273926\n",
            "step: 110, loss: 0.290507048368454\n",
            "step: 120, loss: 0.20087595283985138\n",
            "step: 130, loss: 0.023152196779847145\n",
            "step: 140, loss: 0.1857108175754547\n",
            "step: 150, loss: 0.2593490779399872\n",
            "step: 160, loss: 0.10743484646081924\n",
            "step: 170, loss: 0.26644158363342285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7142857142857143, f1=0.6965174129353233, best_f1=0.6965174129353233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11205919086933136\n",
            "step: 10, loss: 0.23630967736244202\n",
            "step: 20, loss: 0.08145750313997269\n",
            "step: 30, loss: 0.13673678040504456\n",
            "step: 40, loss: 0.08426313102245331\n",
            "step: 50, loss: 0.10212019830942154\n",
            "step: 60, loss: 0.10292063653469086\n",
            "step: 70, loss: 0.09505636990070343\n",
            "step: 80, loss: 0.11131690442562103\n",
            "step: 90, loss: 0.11657392978668213\n",
            "step: 100, loss: 0.14209921658039093\n",
            "step: 110, loss: 0.029399225488305092\n",
            "step: 120, loss: 0.04871276393532753\n",
            "step: 130, loss: 0.08565958589315414\n",
            "step: 140, loss: 0.3038383424282074\n",
            "step: 150, loss: 0.15978612005710602\n",
            "step: 160, loss: 0.14384225010871887\n",
            "step: 170, loss: 0.0740838423371315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7676767676767677, f1=0.7628361858190709, best_f1=0.7628361858190709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04589603841304779\n",
            "step: 10, loss: 0.027856187894940376\n",
            "step: 20, loss: 0.13310706615447998\n",
            "step: 30, loss: 0.22038309276103973\n",
            "step: 40, loss: 0.14459441602230072\n",
            "step: 50, loss: 0.07948023825883865\n",
            "step: 60, loss: 0.12298727035522461\n",
            "step: 70, loss: 0.06199325621128082\n",
            "step: 80, loss: 0.07122794538736343\n",
            "step: 90, loss: 0.18245084583759308\n",
            "step: 100, loss: 0.021113421767950058\n",
            "step: 110, loss: 0.07818453758955002\n",
            "step: 120, loss: 0.054652128368616104\n",
            "step: 130, loss: 0.04032561182975769\n",
            "step: 140, loss: 0.14661964774131775\n",
            "step: 150, loss: 0.11500699073076248\n",
            "step: 160, loss: 0.07093897461891174\n",
            "step: 170, loss: 0.14762355387210846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7786666666666666, f1=0.772378516624041, best_f1=0.772378516624041\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019822267815470695\n",
            "step: 10, loss: 0.027099167928099632\n",
            "step: 20, loss: 0.018666738644242287\n",
            "step: 30, loss: 0.05891574174165726\n",
            "step: 40, loss: 0.00870172493159771\n",
            "step: 50, loss: 0.01765969954431057\n",
            "step: 60, loss: 0.2933269143104553\n",
            "step: 70, loss: 0.014387513510882854\n",
            "step: 80, loss: 0.07886816561222076\n",
            "step: 90, loss: 0.030948594212532043\n",
            "step: 100, loss: 0.1963164508342743\n",
            "step: 110, loss: 0.04092586785554886\n",
            "step: 120, loss: 0.01880648173391819\n",
            "step: 130, loss: 0.029308408498764038\n",
            "step: 140, loss: 0.03623213618993759\n",
            "step: 150, loss: 0.03920881822705269\n",
            "step: 160, loss: 0.03964788094162941\n",
            "step: 170, loss: 0.0043827807530760765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.824742268041237, f1=0.7938931297709925, best_f1=0.7938931297709925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03050174191594124\n",
            "step: 10, loss: 0.0404733307659626\n",
            "step: 20, loss: 0.01805218495428562\n",
            "step: 30, loss: 0.03225826099514961\n",
            "step: 40, loss: 0.01861579157412052\n",
            "step: 50, loss: 0.09967617690563202\n",
            "step: 60, loss: 0.029553476721048355\n",
            "step: 70, loss: 0.3470028340816498\n",
            "step: 80, loss: 0.0931498259305954\n",
            "step: 90, loss: 0.13062599301338196\n",
            "step: 100, loss: 0.08470024913549423\n",
            "step: 110, loss: 0.2846479117870331\n",
            "step: 120, loss: 0.00516888964921236\n",
            "step: 130, loss: 0.11958368867635727\n",
            "step: 140, loss: 0.033746588975191116\n",
            "step: 150, loss: 0.07854551821947098\n",
            "step: 160, loss: 0.030839405953884125\n",
            "step: 170, loss: 0.006890848278999329\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8303797468354431, f1=0.7651331719128328, best_f1=0.7651331719128328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003591670421883464\n",
            "step: 10, loss: 0.00746597396209836\n",
            "step: 20, loss: 0.030872199684381485\n",
            "step: 30, loss: 0.0010925120441243052\n",
            "step: 40, loss: 0.048856835812330246\n",
            "step: 50, loss: 0.11108041554689407\n",
            "step: 60, loss: 0.07734636962413788\n",
            "step: 70, loss: 0.09026351571083069\n",
            "step: 80, loss: 0.022172143682837486\n",
            "step: 90, loss: 0.035639479756355286\n",
            "step: 100, loss: 0.006063130684196949\n",
            "step: 110, loss: 0.0019254995277151465\n",
            "step: 120, loss: 0.003400791436433792\n",
            "step: 130, loss: 0.01103877928107977\n",
            "step: 140, loss: 0.006603463087230921\n",
            "step: 150, loss: 0.05243257060647011\n",
            "step: 160, loss: 0.047264859080314636\n",
            "step: 170, loss: 0.0027761098463088274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8132992327365729, f1=0.7857142857142857, best_f1=0.7651331719128328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015197602333500981\n",
            "step: 10, loss: 0.012026231735944748\n",
            "step: 20, loss: 0.008188840001821518\n",
            "step: 30, loss: 0.013475471176207066\n",
            "step: 40, loss: 0.010450302623212337\n",
            "step: 50, loss: 0.07607221603393555\n",
            "step: 60, loss: 0.0015976082067936659\n",
            "step: 70, loss: 0.017284374684095383\n",
            "step: 80, loss: 0.0439736470580101\n",
            "step: 90, loss: 0.000997377559542656\n",
            "step: 100, loss: 0.002947795670479536\n",
            "step: 110, loss: 0.025622103363275528\n",
            "step: 120, loss: 0.002751907566562295\n",
            "step: 130, loss: 0.1393931359052658\n",
            "step: 140, loss: 0.0078069837763905525\n",
            "step: 150, loss: 0.011723031289875507\n",
            "step: 160, loss: 0.005478072911500931\n",
            "step: 170, loss: 0.03501461446285248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8, f1=0.7715736040609138, best_f1=0.7651331719128328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01175597496330738\n",
            "step: 10, loss: 0.005597907584160566\n",
            "step: 20, loss: 0.00037359673297032714\n",
            "step: 30, loss: 0.00561732379719615\n",
            "step: 40, loss: 0.000769125996157527\n",
            "step: 50, loss: 0.00405000289902091\n",
            "step: 60, loss: 0.00143570511136204\n",
            "step: 70, loss: 0.0008483501151204109\n",
            "step: 80, loss: 0.001615081331692636\n",
            "step: 90, loss: 0.006854633800685406\n",
            "step: 100, loss: 0.029842497780919075\n",
            "step: 110, loss: 0.09849821776151657\n",
            "step: 120, loss: 0.0004921084619127214\n",
            "step: 130, loss: 0.012810804881155491\n",
            "step: 140, loss: 0.007701690308749676\n",
            "step: 150, loss: 0.14436163008213043\n",
            "step: 160, loss: 0.019262254238128662\n",
            "step: 170, loss: 0.002525411080569029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8426395939086295, f1=0.7909319899244334, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003716642677318305\n",
            "step: 10, loss: 0.0011317559983581305\n",
            "step: 20, loss: 0.002686471212655306\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.0023182793520390987\n",
            "step: 40, loss: 0.0005480466643348336\n",
            "step: 50, loss: 0.00043408581404946744\n",
            "step: 60, loss: 0.01959356479346752\n",
            "step: 70, loss: 0.00823761522769928\n",
            "step: 80, loss: 0.0004151341854594648\n",
            "step: 90, loss: 0.01595686934888363\n",
            "step: 100, loss: 0.0032221514265984297\n",
            "step: 110, loss: 0.0013594586635008454\n",
            "step: 120, loss: 0.012378249317407608\n",
            "step: 130, loss: 0.09747273474931717\n",
            "step: 140, loss: 0.008356461301445961\n",
            "step: 150, loss: 0.015401050448417664\n",
            "step: 160, loss: 0.03812716156244278\n",
            "step: 170, loss: 0.0005294220754876733\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8404255319148938, f1=0.806282722513089, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021366257220506668\n",
            "step: 10, loss: 0.0002270653931191191\n",
            "step: 20, loss: 0.18337631225585938\n",
            "step: 30, loss: 0.0004061675281263888\n",
            "step: 40, loss: 0.0003267905267421156\n",
            "step: 50, loss: 0.0815521627664566\n",
            "step: 60, loss: 0.00017530554032418877\n",
            "step: 70, loss: 0.009048041887581348\n",
            "step: 80, loss: 0.0021777863148599863\n",
            "step: 90, loss: 0.013215244747698307\n",
            "step: 100, loss: 0.0004860087647102773\n",
            "step: 110, loss: 0.0026094221975654364\n",
            "step: 120, loss: 0.0006409049965441227\n",
            "step: 130, loss: 0.0013032922288402915\n",
            "step: 140, loss: 0.13275951147079468\n",
            "step: 150, loss: 0.04247702658176422\n",
            "step: 160, loss: 0.02069278247654438\n",
            "step: 170, loss: 0.00020562621648423374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8232445520581113, f1=0.7846889952153109, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.047709252685308456\n",
            "step: 10, loss: 0.02832961082458496\n",
            "step: 20, loss: 0.001509928610175848\n",
            "step: 30, loss: 0.0001845131191657856\n",
            "step: 40, loss: 0.0007578147924505174\n",
            "step: 50, loss: 0.0024931272491812706\n",
            "step: 60, loss: 0.03443383425474167\n",
            "step: 70, loss: 0.0018471510848030448\n",
            "step: 80, loss: 0.00034488795790821314\n",
            "step: 90, loss: 0.0019309193594381213\n",
            "step: 100, loss: 0.012068388052284718\n",
            "step: 110, loss: 0.008913157507777214\n",
            "step: 120, loss: 0.0009526594076305628\n",
            "step: 130, loss: 0.00025050260592252016\n",
            "step: 140, loss: 0.04953882843255997\n",
            "step: 150, loss: 0.03150627762079239\n",
            "step: 160, loss: 0.05462475121021271\n",
            "step: 170, loss: 0.0005708563257940114\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8238341968911919, f1=0.7786259541984734, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01417714636772871\n",
            "step: 10, loss: 0.0005116310785524547\n",
            "step: 20, loss: 0.0003728649753611535\n",
            "step: 30, loss: 0.0008686677319929004\n",
            "step: 40, loss: 0.00019269609765615314\n",
            "step: 50, loss: 0.0002417021314613521\n",
            "step: 60, loss: 0.00024845157167874277\n",
            "step: 70, loss: 0.06670600920915604\n",
            "step: 80, loss: 8.931061893235892e-05\n",
            "step: 90, loss: 0.0005216005374677479\n",
            "step: 100, loss: 0.0010434574214741588\n",
            "step: 110, loss: 0.0002787198463920504\n",
            "step: 120, loss: 0.009932305663824081\n",
            "step: 130, loss: 0.0038695475086569786\n",
            "step: 140, loss: 0.0045863487757742405\n",
            "step: 150, loss: 0.007277729921042919\n",
            "step: 160, loss: 0.00016721864813007414\n",
            "step: 170, loss: 0.0003010371292475611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8125, f1=0.7575757575757577, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00549383694306016\n",
            "step: 10, loss: 0.00047069863649085164\n",
            "step: 20, loss: 0.00032019056379795074\n",
            "step: 30, loss: 0.00010373349505243823\n",
            "step: 40, loss: 0.0007931647705845535\n",
            "step: 50, loss: 0.0002398498763795942\n",
            "step: 60, loss: 0.0008482017437927425\n",
            "step: 70, loss: 0.0027497424744069576\n",
            "step: 80, loss: 0.0012505070772022009\n",
            "step: 90, loss: 0.00025433924747630954\n",
            "step: 100, loss: 0.0007609547465108335\n",
            "step: 110, loss: 0.00019516705651767552\n",
            "step: 120, loss: 0.031779058277606964\n",
            "step: 130, loss: 0.0001375235733576119\n",
            "step: 140, loss: 0.0002653195697348565\n",
            "step: 150, loss: 0.0002600336738396436\n",
            "step: 160, loss: 0.00017939312965609133\n",
            "step: 170, loss: 0.0006164719234220684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8043478260869564, f1=0.7745358090185676, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007303419406525791\n",
            "step: 10, loss: 0.00012085065827704966\n",
            "step: 20, loss: 0.0005406226846389472\n",
            "step: 30, loss: 0.00018069315410684794\n",
            "step: 40, loss: 0.0001838218013290316\n",
            "step: 50, loss: 0.00022516574244946241\n",
            "step: 60, loss: 9.111643157666549e-05\n",
            "step: 70, loss: 0.00117797392886132\n",
            "step: 80, loss: 0.007795966230332851\n",
            "step: 90, loss: 0.00022445245122071356\n",
            "step: 100, loss: 0.03776091709733009\n",
            "step: 110, loss: 0.0005791764706373215\n",
            "step: 120, loss: 0.0280525591224432\n",
            "step: 130, loss: 0.0015535939019173384\n",
            "step: 140, loss: 0.020424626767635345\n",
            "step: 150, loss: 0.00025025333161465824\n",
            "step: 160, loss: 0.00010495173773961142\n",
            "step: 170, loss: 0.00037377458647824824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8132992327365729, f1=0.7676767676767677, best_f1=0.7909319899244334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007463428191840649\n",
            "step: 10, loss: 0.004169286694377661\n",
            "step: 20, loss: 0.06057625263929367\n",
            "step: 30, loss: 0.0012093756813555956\n",
            "step: 40, loss: 0.0004732007801067084\n",
            "step: 50, loss: 0.00016581623640377074\n",
            "step: 60, loss: 0.017158757895231247\n",
            "step: 70, loss: 0.0003416991967242211\n",
            "step: 80, loss: 0.02809653989970684\n",
            "step: 90, loss: 0.0009789559990167618\n",
            "step: 100, loss: 0.0015559608582407236\n",
            "step: 110, loss: 0.0002416410279693082\n",
            "step: 120, loss: 0.0006464757025241852\n",
            "step: 130, loss: 0.0040021250024437904\n",
            "step: 140, loss: 0.00036808487493544817\n",
            "step: 150, loss: 0.0005902992561459541\n",
            "step: 160, loss: 0.00016263349971268326\n",
            "step: 170, loss: 0.00028634440968744457\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8142493638676845, f1=0.7611940298507462, best_f1=0.7909319899244334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:09, 202.25it/s]\n",
            "load_f1 = 0.5353846153846153\n",
            "real_f1 = 0.524390243902439\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 184.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befc30ef-09f6-4ac6-90b6-799ed23cba08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 352kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 255kB/s] \n",
            "Downloading: 100% 440M/440M [00:10<00:00, 41.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6313666105270386\n",
            "step: 10, loss: 0.6160402894020081\n",
            "step: 20, loss: 0.4453106224536896\n",
            "step: 30, loss: 0.18677982687950134\n",
            "step: 40, loss: 0.1482085883617401\n",
            "step: 50, loss: 0.07969135046005249\n",
            "step: 60, loss: 0.11881394684314728\n",
            "step: 70, loss: 0.13898612558841705\n",
            "step: 80, loss: 0.037479083985090256\n",
            "step: 90, loss: 0.1138644739985466\n",
            "step: 100, loss: 0.03148370236158371\n",
            "step: 110, loss: 0.18885385990142822\n",
            "step: 120, loss: 0.009103581309318542\n",
            "step: 130, loss: 0.007923022843897343\n",
            "step: 140, loss: 0.0045296926982700825\n",
            "step: 150, loss: 0.01730893738567829\n",
            "step: 160, loss: 0.013848387636244297\n",
            "step: 170, loss: 0.06609407812356949\n",
            "step: 180, loss: 0.05616101250052452\n",
            "step: 190, loss: 0.09819313883781433\n",
            "step: 200, loss: 0.1145409643650055\n",
            "step: 210, loss: 0.007917859591543674\n",
            "step: 220, loss: 0.0559203214943409\n",
            "step: 230, loss: 0.020429883152246475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9665924276169264, f1=0.9718785151856018, best_f1=0.9718785151856018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009322626516222954\n",
            "step: 10, loss: 0.016781968995928764\n",
            "step: 20, loss: 0.048843588680028915\n",
            "step: 30, loss: 0.2023492455482483\n",
            "step: 40, loss: 0.08794018626213074\n",
            "step: 50, loss: 0.035454485565423965\n",
            "step: 60, loss: 0.007601082790642977\n",
            "step: 70, loss: 0.024727115407586098\n",
            "step: 80, loss: 0.0077914223074913025\n",
            "step: 90, loss: 0.1364997923374176\n",
            "step: 100, loss: 0.049655769020318985\n",
            "step: 110, loss: 0.16063852608203888\n",
            "step: 120, loss: 0.038879454135894775\n",
            "step: 130, loss: 0.05648957937955856\n",
            "step: 140, loss: 0.001224212464876473\n",
            "step: 150, loss: 0.020025847479701042\n",
            "step: 160, loss: 0.026653872802853584\n",
            "step: 170, loss: 0.00235636904835701\n",
            "step: 180, loss: 0.004518745467066765\n",
            "step: 190, loss: 0.0032607463654130697\n",
            "step: 200, loss: 0.002789855469018221\n",
            "step: 210, loss: 0.03461331129074097\n",
            "step: 220, loss: 0.12110108882188797\n",
            "step: 230, loss: 0.01125344354659319\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9753363228699552, f1=0.976271186440678, best_f1=0.976271186440678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007171829231083393\n",
            "step: 10, loss: 0.006613289937376976\n",
            "step: 20, loss: 0.12185335159301758\n",
            "step: 30, loss: 0.023588968440890312\n",
            "step: 40, loss: 0.16844624280929565\n",
            "step: 50, loss: 0.0038270633667707443\n",
            "step: 60, loss: 0.0055871088989079\n",
            "step: 70, loss: 0.005180764943361282\n",
            "step: 80, loss: 0.0007737440173514187\n",
            "step: 90, loss: 0.0036055578384548426\n",
            "step: 100, loss: 0.0006038368446752429\n",
            "step: 110, loss: 0.0009110231185331941\n",
            "step: 120, loss: 0.003765536705031991\n",
            "step: 130, loss: 0.00036347273271530867\n",
            "step: 140, loss: 0.0009221050422638655\n",
            "step: 150, loss: 0.009423513896763325\n",
            "step: 160, loss: 0.10802282392978668\n",
            "step: 170, loss: 0.009301080368459225\n",
            "step: 180, loss: 0.006033124402165413\n",
            "step: 190, loss: 0.004456007853150368\n",
            "step: 200, loss: 0.08064481616020203\n",
            "step: 210, loss: 0.010812790133059025\n",
            "step: 220, loss: 0.0005625163903459907\n",
            "step: 230, loss: 0.000646439497359097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9776785714285714, f1=0.9730941704035874, best_f1=0.9730941704035874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017781708447728306\n",
            "step: 10, loss: 0.001118619809858501\n",
            "step: 20, loss: 0.006761047523468733\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.0020740488544106483\n",
            "step: 40, loss: 0.004742722027003765\n",
            "step: 50, loss: 0.0003202592197339982\n",
            "step: 60, loss: 0.003143803682178259\n",
            "step: 70, loss: 0.000878685328643769\n",
            "step: 80, loss: 0.0008694067364558578\n",
            "step: 90, loss: 0.0021464533638209105\n",
            "step: 100, loss: 0.0018053387757390738\n",
            "step: 110, loss: 0.001844258513301611\n",
            "step: 120, loss: 0.00542720640078187\n",
            "step: 130, loss: 0.005393777042627335\n",
            "step: 140, loss: 0.0033724645618349314\n",
            "step: 150, loss: 0.17186100780963898\n",
            "step: 160, loss: 0.004763667471706867\n",
            "step: 170, loss: 0.003150732023641467\n",
            "step: 180, loss: 0.0013604884734377265\n",
            "step: 190, loss: 0.00291770719923079\n",
            "step: 200, loss: 0.0016682051355019212\n",
            "step: 210, loss: 0.0515204481780529\n",
            "step: 220, loss: 0.000471106031909585\n",
            "step: 230, loss: 0.003650605445727706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9832402234636871, f1=0.9788182831661093, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009844383457675576\n",
            "step: 10, loss: 0.008379202336072922\n",
            "step: 20, loss: 0.011639807373285294\n",
            "step: 30, loss: 0.0002290497359354049\n",
            "step: 40, loss: 0.0003848338674288243\n",
            "step: 50, loss: 0.0008529890328645706\n",
            "step: 60, loss: 0.20988187193870544\n",
            "step: 70, loss: 0.00041096104541793466\n",
            "step: 80, loss: 0.0007606280851177871\n",
            "step: 90, loss: 0.001187873538583517\n",
            "step: 100, loss: 0.0007678103866055608\n",
            "step: 110, loss: 0.0005572218797169626\n",
            "step: 120, loss: 0.00012593316205311567\n",
            "step: 130, loss: 0.004529277794063091\n",
            "step: 140, loss: 0.00030592118855565786\n",
            "step: 150, loss: 0.00028016246506012976\n",
            "step: 160, loss: 0.000293567223707214\n",
            "step: 170, loss: 0.019679438322782516\n",
            "step: 180, loss: 0.0011461434187367558\n",
            "step: 190, loss: 0.018029630184173584\n",
            "step: 200, loss: 0.006423930637538433\n",
            "step: 210, loss: 0.0007857974851503968\n",
            "step: 220, loss: 0.001188797876238823\n",
            "step: 230, loss: 0.0013285784516483545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9733924611973392, f1=0.9720044792833147, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0062195416539907455\n",
            "step: 10, loss: 0.0007956397021189332\n",
            "step: 20, loss: 0.004060571547597647\n",
            "step: 30, loss: 0.00033373889164067805\n",
            "step: 40, loss: 0.0002919159014709294\n",
            "step: 50, loss: 0.00031014109845273197\n",
            "step: 60, loss: 0.0002920467231888324\n",
            "step: 70, loss: 0.003011043183505535\n",
            "step: 80, loss: 0.0048033567145466805\n",
            "step: 90, loss: 0.0005145580507814884\n",
            "step: 100, loss: 0.06768209487199783\n",
            "step: 110, loss: 0.004651864059269428\n",
            "step: 120, loss: 0.0019059175392612815\n",
            "step: 130, loss: 0.0006140762707218528\n",
            "step: 140, loss: 0.0026206846814602613\n",
            "step: 150, loss: 0.00169295200612396\n",
            "step: 160, loss: 0.002045018831267953\n",
            "step: 170, loss: 0.00044337729923427105\n",
            "step: 180, loss: 0.07206191122531891\n",
            "step: 190, loss: 0.07733612507581711\n",
            "step: 200, loss: 0.01807139627635479\n",
            "step: 210, loss: 0.0010269093327224255\n",
            "step: 220, loss: 0.002383536659181118\n",
            "step: 230, loss: 0.090727299451828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9707207207207207, f1=0.9660633484162895, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0048971399664878845\n",
            "step: 10, loss: 0.00024032837245613337\n",
            "step: 20, loss: 0.0021800464019179344\n",
            "step: 30, loss: 0.016430726274847984\n",
            "step: 40, loss: 0.0013684442965313792\n",
            "step: 50, loss: 0.00041244574822485447\n",
            "step: 60, loss: 0.002898563165217638\n",
            "step: 70, loss: 0.017623094841837883\n",
            "step: 80, loss: 0.00021868146723136306\n",
            "step: 90, loss: 0.0002984184247907251\n",
            "step: 100, loss: 0.00040385080501437187\n",
            "step: 110, loss: 0.00044822331983596087\n",
            "step: 120, loss: 0.00019920409249607474\n",
            "step: 130, loss: 0.00043693528277799487\n",
            "step: 140, loss: 0.0004216132510919124\n",
            "step: 150, loss: 0.007178385276347399\n",
            "step: 160, loss: 0.048633139580488205\n",
            "step: 170, loss: 0.0018870962085202336\n",
            "step: 180, loss: 0.04201589524745941\n",
            "step: 190, loss: 0.03671328350901604\n",
            "step: 200, loss: 0.035639889538288116\n",
            "step: 210, loss: 0.00023776183661539108\n",
            "step: 220, loss: 0.011402975767850876\n",
            "step: 230, loss: 0.0006328829913400114\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9820627802690582, f1=0.9797752808988766, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005014449125155807\n",
            "step: 10, loss: 0.0011985261226072907\n",
            "step: 20, loss: 0.0015206370735540986\n",
            "step: 30, loss: 0.00042068917537108064\n",
            "step: 40, loss: 0.0028364837635308504\n",
            "step: 50, loss: 0.0003247592831030488\n",
            "step: 60, loss: 0.0010255769593641162\n",
            "step: 70, loss: 0.0005445001879706979\n",
            "step: 80, loss: 0.0007236028322950006\n",
            "step: 90, loss: 0.00011035819625249133\n",
            "step: 100, loss: 0.0004248509940225631\n",
            "step: 110, loss: 0.00024639375624246895\n",
            "step: 120, loss: 0.0027348354924470186\n",
            "step: 130, loss: 0.0007153471233323216\n",
            "step: 140, loss: 0.00012782312114723027\n",
            "step: 150, loss: 0.00026213799719698727\n",
            "step: 160, loss: 0.00044960726518183947\n",
            "step: 170, loss: 0.00013201228284742683\n",
            "step: 180, loss: 0.04124615341424942\n",
            "step: 190, loss: 0.0007343311444856226\n",
            "step: 200, loss: 0.0017126353923231363\n",
            "step: 210, loss: 0.0005677210283465683\n",
            "step: 220, loss: 0.00020683559705503285\n",
            "step: 230, loss: 0.0007354849367402494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9776286353467561, f1=0.9753363228699552, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030860621482133865\n",
            "step: 10, loss: 0.0012312246253713965\n",
            "step: 20, loss: 0.00024086461053229868\n",
            "step: 30, loss: 0.0001487628760514781\n",
            "step: 40, loss: 0.015636755153536797\n",
            "step: 50, loss: 7.094251486705616e-05\n",
            "step: 60, loss: 0.00023654797405470163\n",
            "step: 70, loss: 8.370511932298541e-05\n",
            "step: 80, loss: 6.808075704611838e-05\n",
            "step: 90, loss: 0.002329841023311019\n",
            "step: 100, loss: 0.000330111215589568\n",
            "step: 110, loss: 0.0031149580609053373\n",
            "step: 120, loss: 4.049178824061528e-05\n",
            "step: 130, loss: 9.718483488541096e-05\n",
            "step: 140, loss: 0.006441451609134674\n",
            "step: 150, loss: 0.00047514791367575526\n",
            "step: 160, loss: 0.0014697522856295109\n",
            "step: 170, loss: 0.023571917787194252\n",
            "step: 180, loss: 0.0015559792518615723\n",
            "step: 190, loss: 0.00019159408111590892\n",
            "step: 200, loss: 0.00023734445858281106\n",
            "step: 210, loss: 0.0002505175652913749\n",
            "step: 220, loss: 7.328364154091105e-05\n",
            "step: 230, loss: 0.00017466458666604012\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9820224719101124, f1=0.9763779527559054, best_f1=0.9788182831661093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021487149933818728\n",
            "step: 10, loss: 0.0003837365366052836\n",
            "step: 20, loss: 0.0002708900428842753\n",
            "step: 30, loss: 0.00013476524327415973\n",
            "step: 40, loss: 5.68471041333396e-05\n",
            "step: 50, loss: 0.00018210116832051426\n",
            "step: 60, loss: 0.0017164844321087003\n",
            "step: 70, loss: 0.00020430804579518735\n",
            "step: 80, loss: 0.0001601203257450834\n",
            "step: 90, loss: 0.00021539928275160491\n",
            "step: 100, loss: 7.372339314315468e-05\n",
            "step: 110, loss: 0.00024919735733419657\n",
            "step: 120, loss: 0.0004546447889879346\n",
            "step: 130, loss: 0.00010388806549599394\n",
            "step: 140, loss: 0.02138170599937439\n",
            "step: 150, loss: 0.04196241497993469\n",
            "step: 160, loss: 0.00017778182518668473\n",
            "step: 170, loss: 3.992929487139918e-05\n",
            "step: 180, loss: 0.00031823391327634454\n",
            "step: 190, loss: 0.04870704188942909\n",
            "step: 200, loss: 5.982339644106105e-05\n",
            "step: 210, loss: 0.0003956783621106297\n",
            "step: 220, loss: 0.00030150319798849523\n",
            "step: 230, loss: 0.00013670866610482335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9842696629213483, f1=0.9784335981838819, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018043439195025712\n",
            "step: 10, loss: 0.00011102335702162236\n",
            "step: 20, loss: 0.00013091655273456126\n",
            "step: 30, loss: 0.0026319127064198256\n",
            "step: 40, loss: 0.00014548587205354124\n",
            "step: 50, loss: 0.0001791502145351842\n",
            "step: 60, loss: 0.08259917050600052\n",
            "step: 70, loss: 0.0002777956542558968\n",
            "step: 80, loss: 6.303645932348445e-05\n",
            "step: 90, loss: 0.0002525286690797657\n",
            "step: 100, loss: 0.00016962597146630287\n",
            "step: 110, loss: 0.00013901609054300934\n",
            "step: 120, loss: 8.846498531056568e-05\n",
            "step: 130, loss: 0.00020769701222889125\n",
            "step: 140, loss: 4.764707773574628e-05\n",
            "step: 150, loss: 0.001293735345825553\n",
            "step: 160, loss: 8.999722194857895e-05\n",
            "step: 170, loss: 0.007131515070796013\n",
            "step: 180, loss: 0.001636813278310001\n",
            "step: 190, loss: 4.9789712647907436e-05\n",
            "step: 200, loss: 0.00016186882567126304\n",
            "step: 210, loss: 8.151022484526038e-05\n",
            "step: 220, loss: 6.385167216649279e-05\n",
            "step: 230, loss: 0.00011343829100951552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9776286353467561, f1=0.9730941704035874, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.053914254764095e-05\n",
            "step: 10, loss: 0.004032640717923641\n",
            "step: 20, loss: 4.498757334658876e-05\n",
            "step: 30, loss: 0.0001996853679884225\n",
            "step: 40, loss: 0.0002986134495586157\n",
            "step: 50, loss: 0.01539567019790411\n",
            "step: 60, loss: 0.0008547070901840925\n",
            "step: 70, loss: 6.536010914715007e-05\n",
            "step: 80, loss: 0.0005130688077770174\n",
            "step: 90, loss: 8.371299918508157e-05\n",
            "step: 100, loss: 3.855476461467333e-05\n",
            "step: 110, loss: 5.3725147154182196e-05\n",
            "step: 120, loss: 7.248753536259755e-05\n",
            "step: 130, loss: 4.370680107967928e-05\n",
            "step: 140, loss: 6.02427389821969e-05\n",
            "step: 150, loss: 0.00027363255503587425\n",
            "step: 160, loss: 0.0012194202281534672\n",
            "step: 170, loss: 5.398325447458774e-05\n",
            "step: 180, loss: 0.0017191523220390081\n",
            "step: 190, loss: 0.0001020272247842513\n",
            "step: 200, loss: 7.554941112175584e-05\n",
            "step: 210, loss: 5.214969860389829e-05\n",
            "step: 220, loss: 3.750035830307752e-05\n",
            "step: 230, loss: 0.0031827285420149565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9809203142536477, f1=0.9751131221719457, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.69718356220983e-05\n",
            "step: 10, loss: 7.471938442904502e-05\n",
            "step: 20, loss: 0.00018874014494940639\n",
            "step: 30, loss: 7.768873911118135e-05\n",
            "step: 40, loss: 4.623085987986997e-05\n",
            "step: 50, loss: 0.00017239006410818547\n",
            "step: 60, loss: 0.0211234912276268\n",
            "step: 70, loss: 2.997619958478026e-05\n",
            "step: 80, loss: 6.284801929723471e-05\n",
            "step: 90, loss: 0.0414714515209198\n",
            "step: 100, loss: 4.1761930333450437e-05\n",
            "step: 110, loss: 0.01311770360916853\n",
            "step: 120, loss: 0.0009371191845275462\n",
            "step: 130, loss: 4.947231718688272e-05\n",
            "step: 140, loss: 5.723924550693482e-05\n",
            "step: 150, loss: 5.993645027047023e-05\n",
            "step: 160, loss: 0.00011904364509973675\n",
            "step: 170, loss: 5.34715800313279e-05\n",
            "step: 180, loss: 8.748114487389103e-05\n",
            "step: 190, loss: 3.803321305895224e-05\n",
            "step: 200, loss: 0.001654560910537839\n",
            "step: 210, loss: 2.1013880541431718e-05\n",
            "step: 220, loss: 0.00037385107134468853\n",
            "step: 230, loss: 2.910474722739309e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9798206278026906, f1=0.9753363228699552, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003558448515832424\n",
            "step: 10, loss: 0.00014171349175740033\n",
            "step: 20, loss: 4.973264003638178e-05\n",
            "step: 30, loss: 4.6481607569148764e-05\n",
            "step: 40, loss: 0.022921158000826836\n",
            "step: 50, loss: 9.960120951291174e-05\n",
            "step: 60, loss: 3.3254393201787025e-05\n",
            "step: 70, loss: 8.044371497817338e-05\n",
            "step: 80, loss: 8.741364581510425e-05\n",
            "step: 90, loss: 0.00042992105591110885\n",
            "step: 100, loss: 4.969057044945657e-05\n",
            "step: 110, loss: 0.0001878091279650107\n",
            "step: 120, loss: 2.6377760150353424e-05\n",
            "step: 130, loss: 3.5023927921429276e-05\n",
            "step: 140, loss: 4.308854477130808e-05\n",
            "step: 150, loss: 5.774123565061018e-05\n",
            "step: 160, loss: 0.0007394130807369947\n",
            "step: 170, loss: 4.17032242694404e-05\n",
            "step: 180, loss: 0.00024534022668376565\n",
            "step: 190, loss: 5.474625140777789e-05\n",
            "step: 200, loss: 8.318566688103601e-05\n",
            "step: 210, loss: 0.00011000559607055038\n",
            "step: 220, loss: 3.9179562008939683e-05\n",
            "step: 230, loss: 7.62989729992114e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9820627802690582, f1=0.9774774774774775, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.7815929570351727e-05\n",
            "step: 10, loss: 2.143846904800739e-05\n",
            "step: 20, loss: 3.95906281482894e-05\n",
            "step: 30, loss: 6.142690108390525e-05\n",
            "step: 40, loss: 0.0001945612020790577\n",
            "step: 50, loss: 0.0036008029710501432\n",
            "step: 60, loss: 5.645798592013307e-05\n",
            "step: 70, loss: 9.488880459684879e-05\n",
            "step: 80, loss: 0.00012688004062511027\n",
            "step: 90, loss: 9.682605741545558e-05\n",
            "step: 100, loss: 0.00013741773727815598\n",
            "step: 110, loss: 3.132472193101421e-05\n",
            "step: 120, loss: 5.070889164926484e-05\n",
            "step: 130, loss: 7.779243605909869e-05\n",
            "step: 140, loss: 7.175366044975817e-05\n",
            "step: 150, loss: 0.0001907076803036034\n",
            "step: 160, loss: 3.701768218888901e-05\n",
            "step: 170, loss: 2.72604502242757e-05\n",
            "step: 180, loss: 0.0001508156128693372\n",
            "step: 190, loss: 0.0001229803019668907\n",
            "step: 200, loss: 0.000103120764833875\n",
            "step: 210, loss: 0.0006455379771068692\n",
            "step: 220, loss: 0.0006382195861078799\n",
            "step: 230, loss: 4.207074016449042e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9820627802690582, f1=0.9774774774774775, best_f1=0.9784335981838819\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 165.11it/s]\n",
            "load_f1 = 0.9831271091113611\n",
            "real_f1 = 0.9808773903262092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 184.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ff46a2-4ddc-4fc7-a94a-5d963415ae43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.636089026927948\n",
            "step: 10, loss: 0.564772367477417\n",
            "step: 20, loss: 0.4878000020980835\n",
            "step: 30, loss: 0.17334645986557007\n",
            "step: 40, loss: 0.21001403033733368\n",
            "step: 50, loss: 0.25118738412857056\n",
            "step: 60, loss: 0.103816919028759\n",
            "step: 70, loss: 0.21679669618606567\n",
            "step: 80, loss: 0.09102486073970795\n",
            "step: 90, loss: 0.36530813574790955\n",
            "step: 100, loss: 0.07199956476688385\n",
            "step: 110, loss: 0.1492275446653366\n",
            "step: 120, loss: 0.17446886003017426\n",
            "step: 130, loss: 0.117298923432827\n",
            "step: 140, loss: 0.11727374792098999\n",
            "step: 150, loss: 0.060932599008083344\n",
            "step: 160, loss: 0.029064994305372238\n",
            "step: 170, loss: 0.24856676161289215\n",
            "step: 180, loss: 0.17126227915287018\n",
            "step: 190, loss: 0.021905740723013878\n",
            "step: 200, loss: 0.15603764355182648\n",
            "step: 210, loss: 0.02232777327299118\n",
            "step: 220, loss: 0.27868038415908813\n",
            "step: 230, loss: 0.14336155354976654\n",
            "step: 240, loss: 0.019814414903521538\n",
            "step: 250, loss: 0.009108795784413815\n",
            "step: 260, loss: 0.07697809487581253\n",
            "step: 270, loss: 0.0073485528118908405\n",
            "step: 280, loss: 0.017690328881144524\n",
            "step: 290, loss: 0.16017784178256989\n",
            "step: 300, loss: 0.07453105598688126\n",
            "step: 310, loss: 0.107426218688488\n",
            "step: 320, loss: 0.13322660326957703\n",
            "step: 330, loss: 0.03586847707629204\n",
            "step: 340, loss: 0.026646055281162262\n",
            "step: 350, loss: 0.013399352319538593\n",
            "step: 360, loss: 0.01540173590183258\n",
            "step: 370, loss: 0.1029503121972084\n",
            "step: 380, loss: 0.01941845566034317\n",
            "step: 390, loss: 0.1902616024017334\n",
            "step: 400, loss: 0.2711779475212097\n",
            "step: 410, loss: 0.04569073021411896\n",
            "step: 420, loss: 0.05222201719880104\n",
            "step: 430, loss: 0.17697520554065704\n",
            "step: 440, loss: 0.020459400489926338\n",
            "step: 450, loss: 0.02494249865412712\n",
            "step: 460, loss: 0.007925021462142467\n",
            "step: 470, loss: 0.15620636940002441\n",
            "step: 480, loss: 0.10177326202392578\n",
            "step: 490, loss: 0.08203937858343124\n",
            "step: 500, loss: 0.0400925949215889\n",
            "step: 510, loss: 0.09227853268384933\n",
            "step: 520, loss: 0.025872599333524704\n",
            "step: 530, loss: 0.01300827506929636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9284725426857406, f1=0.929784304726939, best_f1=0.929784304726939\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2304660975933075\n",
            "step: 10, loss: 0.0685761570930481\n",
            "step: 20, loss: 0.01558742206543684\n",
            "step: 30, loss: 0.11445004492998123\n",
            "step: 40, loss: 0.26235324144363403\n",
            "step: 50, loss: 0.14523017406463623\n",
            "step: 60, loss: 0.012049800716340542\n",
            "step: 70, loss: 0.015875011682510376\n",
            "step: 80, loss: 0.060030169785022736\n",
            "step: 90, loss: 0.01950003392994404\n",
            "step: 100, loss: 0.03551284968852997\n",
            "step: 110, loss: 0.11991959810256958\n",
            "step: 120, loss: 0.26115697622299194\n",
            "step: 130, loss: 0.11757072061300278\n",
            "step: 140, loss: 0.032736774533987045\n",
            "step: 150, loss: 0.10768896341323853\n",
            "step: 160, loss: 0.04979144409298897\n",
            "step: 170, loss: 0.010789585299789906\n",
            "step: 180, loss: 0.031076889485120773\n",
            "step: 190, loss: 0.042071886360645294\n",
            "step: 200, loss: 0.00913113635033369\n",
            "step: 210, loss: 0.07719338685274124\n",
            "step: 220, loss: 0.05973225459456444\n",
            "step: 230, loss: 0.01339353434741497\n",
            "step: 240, loss: 0.008515733294188976\n",
            "step: 250, loss: 0.050369344651699066\n",
            "step: 260, loss: 0.0059548174031078815\n",
            "step: 270, loss: 0.24851825833320618\n",
            "step: 280, loss: 0.04491310939192772\n",
            "step: 290, loss: 0.046347733587026596\n",
            "step: 300, loss: 0.205389603972435\n",
            "step: 310, loss: 0.02338734269142151\n",
            "step: 320, loss: 0.10450409352779388\n",
            "step: 330, loss: 0.03120938315987587\n",
            "step: 340, loss: 0.16689518094062805\n",
            "step: 350, loss: 0.012854074127972126\n",
            "step: 360, loss: 0.10328326374292374\n",
            "step: 370, loss: 0.11577589064836502\n",
            "step: 380, loss: 0.0662461370229721\n",
            "step: 390, loss: 0.08985821902751923\n",
            "step: 400, loss: 0.023299764841794968\n",
            "step: 410, loss: 0.011578240431845188\n",
            "step: 420, loss: 0.22805839776992798\n",
            "step: 430, loss: 0.01588333211839199\n",
            "step: 440, loss: 0.19641900062561035\n",
            "step: 450, loss: 0.025574522092938423\n",
            "step: 460, loss: 0.03587519749999046\n",
            "step: 470, loss: 0.03884318843483925\n",
            "step: 480, loss: 0.22643645107746124\n",
            "step: 490, loss: 0.02299177274107933\n",
            "step: 500, loss: 0.21494153141975403\n",
            "step: 510, loss: 0.04664485156536102\n",
            "step: 520, loss: 0.05500565096735954\n",
            "step: 530, loss: 0.03050341084599495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.931985294117647, f1=0.9251824817518249, best_f1=0.9251824817518249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03127370774745941\n",
            "step: 10, loss: 0.06586199253797531\n",
            "step: 20, loss: 0.0591057650744915\n",
            "step: 30, loss: 0.022323554381728172\n",
            "step: 40, loss: 0.010250689461827278\n",
            "step: 50, loss: 0.11781751364469528\n",
            "step: 60, loss: 0.03153245896100998\n",
            "step: 70, loss: 0.01131503190845251\n",
            "step: 80, loss: 0.0023442304227501154\n",
            "step: 90, loss: 0.0032426936086267233\n",
            "step: 100, loss: 0.023057634010910988\n",
            "step: 110, loss: 0.11101873964071274\n",
            "step: 120, loss: 0.0027141496539115906\n",
            "step: 130, loss: 0.018843434751033783\n",
            "step: 140, loss: 0.03356128931045532\n",
            "step: 150, loss: 0.045808009803295135\n",
            "step: 160, loss: 0.006419194396585226\n",
            "step: 170, loss: 0.16395801305770874\n",
            "step: 180, loss: 0.07537841796875\n",
            "step: 190, loss: 0.08285857737064362\n",
            "step: 200, loss: 0.06487192958593369\n",
            "step: 210, loss: 0.08746829628944397\n",
            "step: 220, loss: 0.060369718819856644\n",
            "step: 230, loss: 0.03586912900209427\n",
            "step: 240, loss: 0.0006958636804483831\n",
            "step: 250, loss: 0.07518967986106873\n",
            "step: 260, loss: 0.028020240366458893\n",
            "step: 270, loss: 0.02511218748986721\n",
            "step: 280, loss: 0.05930168554186821\n",
            "step: 290, loss: 0.003947815857827663\n",
            "step: 300, loss: 0.08592071384191513\n",
            "step: 310, loss: 0.02940445765852928\n",
            "step: 320, loss: 0.01096279639750719\n",
            "step: 330, loss: 0.0011476161889731884\n",
            "step: 340, loss: 0.013209376484155655\n",
            "step: 350, loss: 0.004478665068745613\n",
            "step: 360, loss: 0.014042233116924763\n",
            "step: 370, loss: 0.017180873081088066\n",
            "step: 380, loss: 0.010549076832830906\n",
            "step: 390, loss: 0.004014675971120596\n",
            "step: 400, loss: 0.02376568503677845\n",
            "step: 410, loss: 0.004887694958597422\n",
            "step: 420, loss: 0.12086282670497894\n",
            "step: 430, loss: 0.013867199420928955\n",
            "step: 440, loss: 0.023243283852934837\n",
            "step: 450, loss: 0.0998539924621582\n",
            "step: 460, loss: 0.033529628068208694\n",
            "step: 470, loss: 0.010360165499150753\n",
            "step: 480, loss: 0.0010671771597117186\n",
            "step: 490, loss: 0.0009369294857606292\n",
            "step: 500, loss: 0.014125674031674862\n",
            "step: 510, loss: 0.0036030272021889687\n",
            "step: 520, loss: 0.12428172677755356\n",
            "step: 530, loss: 0.09726033359766006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9319029850746268, f1=0.9215686274509803, best_f1=0.9251824817518249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0151384761556983\n",
            "step: 10, loss: 0.007702060975134373\n",
            "step: 20, loss: 0.005878378637135029\n",
            "step: 30, loss: 0.09944991767406464\n",
            "step: 40, loss: 0.0015625189989805222\n",
            "step: 50, loss: 0.07439842820167542\n",
            "step: 60, loss: 0.07015606015920639\n",
            "step: 70, loss: 0.0064657642506062984\n",
            "step: 80, loss: 0.03256262466311455\n",
            "step: 90, loss: 0.03556828573346138\n",
            "step: 100, loss: 0.002672976115718484\n",
            "step: 110, loss: 0.038312073796987534\n",
            "step: 120, loss: 0.008631890639662743\n",
            "step: 130, loss: 0.07048112899065018\n",
            "step: 140, loss: 0.022393863648176193\n",
            "step: 150, loss: 0.011579958721995354\n",
            "step: 160, loss: 0.171139657497406\n",
            "step: 170, loss: 0.008119868114590645\n",
            "step: 180, loss: 0.004284133668988943\n",
            "step: 190, loss: 0.021005189046263695\n",
            "step: 200, loss: 0.003066832199692726\n",
            "step: 210, loss: 0.12918400764465332\n",
            "step: 220, loss: 0.003497564233839512\n",
            "step: 230, loss: 0.0312797911465168\n",
            "step: 240, loss: 0.008324399590492249\n",
            "step: 250, loss: 0.013371913693845272\n",
            "step: 260, loss: 0.06334331631660461\n",
            "step: 270, loss: 0.015165876597166061\n",
            "step: 280, loss: 0.0263985525816679\n",
            "step: 290, loss: 0.03423236683011055\n",
            "step: 300, loss: 0.0006211677100509405\n",
            "step: 310, loss: 0.009418885223567486\n",
            "step: 320, loss: 0.016956400126218796\n",
            "step: 330, loss: 0.024582847952842712\n",
            "step: 340, loss: 0.0011382800294086337\n",
            "step: 350, loss: 0.002214202657341957\n",
            "step: 360, loss: 0.017338665202260017\n",
            "step: 370, loss: 0.003677277360111475\n",
            "step: 380, loss: 0.010829341597855091\n",
            "step: 390, loss: 0.00303968065418303\n",
            "step: 400, loss: 0.04593709111213684\n",
            "step: 410, loss: 0.004616255406290293\n",
            "step: 420, loss: 0.00920820888131857\n",
            "step: 430, loss: 0.05265449732542038\n",
            "step: 440, loss: 0.0009293221519328654\n",
            "step: 450, loss: 0.01039355993270874\n",
            "step: 460, loss: 0.023498520255088806\n",
            "step: 470, loss: 0.048531673848629\n",
            "step: 480, loss: 0.009802197106182575\n",
            "step: 490, loss: 0.006529544945806265\n",
            "step: 500, loss: 0.014859583228826523\n",
            "step: 510, loss: 0.10338962823152542\n",
            "step: 520, loss: 0.09349282085895538\n",
            "step: 530, loss: 0.0568724162876606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.935831381733021, f1=0.9289719626168225, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.036127980798482895\n",
            "step: 10, loss: 0.041045181453228\n",
            "step: 20, loss: 0.017612973228096962\n",
            "step: 30, loss: 0.0012366812443360686\n",
            "step: 40, loss: 0.07302258908748627\n",
            "step: 50, loss: 0.002075514057651162\n",
            "step: 60, loss: 0.01941888965666294\n",
            "step: 70, loss: 0.0046392339281737804\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 80, loss: 0.000600749219302088\n",
            "step: 90, loss: 0.12851563096046448\n",
            "step: 100, loss: 0.00496671162545681\n",
            "step: 110, loss: 0.0008080188999883831\n",
            "step: 120, loss: 0.02621176280081272\n",
            "step: 130, loss: 0.0012267122510820627\n",
            "step: 140, loss: 0.009052220731973648\n",
            "step: 150, loss: 0.030998162925243378\n",
            "step: 160, loss: 0.0009712736937217414\n",
            "step: 170, loss: 0.15267926454544067\n",
            "step: 180, loss: 0.006268912926316261\n",
            "step: 190, loss: 0.028393536806106567\n",
            "step: 200, loss: 0.0023196435067802668\n",
            "step: 210, loss: 0.012746225111186504\n",
            "step: 220, loss: 0.0008689396199770272\n",
            "step: 230, loss: 0.04277164116501808\n",
            "step: 240, loss: 0.002788479905575514\n",
            "step: 250, loss: 0.0027782537508755922\n",
            "step: 260, loss: 0.013525744900107384\n",
            "step: 270, loss: 0.000798641296569258\n",
            "step: 280, loss: 0.019712507724761963\n",
            "step: 290, loss: 0.1439356803894043\n",
            "step: 300, loss: 0.006884533446282148\n",
            "step: 310, loss: 0.01662001572549343\n",
            "step: 320, loss: 0.10645108669996262\n",
            "step: 330, loss: 0.0905006155371666\n",
            "step: 340, loss: 0.004987109452486038\n",
            "step: 350, loss: 0.0009866395266726613\n",
            "step: 360, loss: 0.009240495041012764\n",
            "step: 370, loss: 0.02035694010555744\n",
            "step: 380, loss: 0.008872589096426964\n",
            "step: 390, loss: 0.00032016393379308283\n",
            "step: 400, loss: 0.002475622110068798\n",
            "step: 410, loss: 0.03702891618013382\n",
            "step: 420, loss: 0.00997175369411707\n",
            "step: 430, loss: 0.015687275677919388\n",
            "step: 440, loss: 0.006769683212041855\n",
            "step: 450, loss: 0.0316438190639019\n",
            "step: 460, loss: 0.00872889719903469\n",
            "step: 470, loss: 0.0238113384693861\n",
            "step: 480, loss: 0.015395567752420902\n",
            "step: 490, loss: 0.0004327462229412049\n",
            "step: 500, loss: 0.00835703406482935\n",
            "step: 510, loss: 0.004693460650742054\n",
            "step: 520, loss: 0.039681147783994675\n",
            "step: 530, loss: 0.01890701614320278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9340761374187557, f1=0.9312906220984216, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002111943205818534\n",
            "step: 10, loss: 0.0032453483436256647\n",
            "step: 20, loss: 0.06285421550273895\n",
            "step: 30, loss: 0.0031635835766792297\n",
            "step: 40, loss: 0.0030857441015541553\n",
            "step: 50, loss: 0.02152368612587452\n",
            "step: 60, loss: 0.00016834333655424416\n",
            "step: 70, loss: 0.0007830673130229115\n",
            "step: 80, loss: 0.028236357495188713\n",
            "step: 90, loss: 0.013763239607214928\n",
            "step: 100, loss: 0.023113617673516273\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 110, loss: 0.001759466715157032\n",
            "step: 120, loss: 0.28391000628471375\n",
            "step: 130, loss: 0.00803354661911726\n",
            "step: 140, loss: 0.013879876583814621\n",
            "step: 150, loss: 0.035170093178749084\n",
            "step: 160, loss: 0.014795220457017422\n",
            "step: 170, loss: 0.0011932094348594546\n",
            "step: 180, loss: 0.0008611801313236356\n",
            "step: 190, loss: 0.0018343385308980942\n",
            "step: 200, loss: 0.005719199310988188\n",
            "step: 210, loss: 0.011809086427092552\n",
            "step: 220, loss: 0.001907320343889296\n",
            "step: 230, loss: 0.008516494184732437\n",
            "step: 240, loss: 0.13321948051452637\n",
            "step: 250, loss: 0.008891466073691845\n",
            "step: 260, loss: 0.0032072202302515507\n",
            "step: 270, loss: 0.09594354033470154\n",
            "step: 280, loss: 0.0017021503299474716\n",
            "step: 290, loss: 0.00021630791889037937\n",
            "step: 300, loss: 0.06994830071926117\n",
            "step: 310, loss: 0.005402350332587957\n",
            "step: 320, loss: 0.001685724244453013\n",
            "step: 330, loss: 0.0021728326100856066\n",
            "step: 340, loss: 0.02288939617574215\n",
            "step: 350, loss: 0.001553216832689941\n",
            "step: 360, loss: 0.022073956206440926\n",
            "step: 370, loss: 0.004527789540588856\n",
            "step: 380, loss: 0.031337980180978775\n",
            "step: 390, loss: 0.01721198670566082\n",
            "step: 400, loss: 0.040384821593761444\n",
            "step: 410, loss: 0.0010553275933489203\n",
            "step: 420, loss: 0.036183856427669525\n",
            "step: 430, loss: 0.0009403012227267027\n",
            "step: 440, loss: 0.00030457269167527556\n",
            "step: 450, loss: 0.0002022529224632308\n",
            "step: 460, loss: 0.00047686105244793\n",
            "step: 470, loss: 0.006437177304178476\n",
            "step: 480, loss: 0.02414359711110592\n",
            "step: 490, loss: 0.025370554998517036\n",
            "step: 500, loss: 0.016739496961236\n",
            "step: 510, loss: 0.015799010172486305\n",
            "step: 520, loss: 0.0012194555019959807\n",
            "step: 530, loss: 0.10229156911373138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9263157894736842, f1=0.9223439733206289, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004820366739295423\n",
            "step: 10, loss: 0.0016399382147938013\n",
            "step: 20, loss: 0.007792723830789328\n",
            "step: 30, loss: 0.009257500059902668\n",
            "step: 40, loss: 0.06218644976615906\n",
            "step: 50, loss: 0.026070410385727882\n",
            "step: 60, loss: 0.03670414537191391\n",
            "step: 70, loss: 0.02879066951572895\n",
            "step: 80, loss: 0.02874823473393917\n",
            "step: 90, loss: 0.002738968003541231\n",
            "step: 100, loss: 0.00030260684434324503\n",
            "step: 110, loss: 0.002068639500066638\n",
            "step: 120, loss: 0.00661817193031311\n",
            "step: 130, loss: 0.017142046242952347\n",
            "step: 140, loss: 0.00010271491919411346\n",
            "step: 150, loss: 0.10353396087884903\n",
            "step: 160, loss: 0.0010418395977467299\n",
            "step: 170, loss: 0.007615128066390753\n",
            "step: 180, loss: 0.0012831324711441994\n",
            "step: 190, loss: 0.0014570921193808317\n",
            "step: 200, loss: 0.0011252970434725285\n",
            "step: 210, loss: 0.0012988217640668154\n",
            "step: 220, loss: 0.0003597074537537992\n",
            "step: 230, loss: 0.008504454977810383\n",
            "step: 240, loss: 0.000674059905577451\n",
            "step: 250, loss: 0.0008131968206726015\n",
            "step: 260, loss: 0.002367795677855611\n",
            "step: 270, loss: 0.0022442527115345\n",
            "step: 280, loss: 0.004333206452429295\n",
            "step: 290, loss: 0.0028088660910725594\n",
            "step: 300, loss: 0.005901915021240711\n",
            "step: 310, loss: 0.000751662184484303\n",
            "step: 320, loss: 0.0014129654737189412\n",
            "step: 330, loss: 0.03173191472887993\n",
            "step: 340, loss: 0.04084257408976555\n",
            "step: 350, loss: 0.010305325500667095\n",
            "step: 360, loss: 0.0067139589227736\n",
            "step: 370, loss: 0.0048877340741455555\n",
            "step: 380, loss: 0.0006414662930183113\n",
            "step: 390, loss: 0.013727892190217972\n",
            "step: 400, loss: 0.0004764254263136536\n",
            "step: 410, loss: 0.0019989819265902042\n",
            "step: 420, loss: 0.011847683228552341\n",
            "step: 430, loss: 0.0018291858723387122\n",
            "step: 440, loss: 0.07546535134315491\n",
            "step: 450, loss: 0.0004071047296747565\n",
            "step: 460, loss: 0.00032459417707286775\n",
            "step: 470, loss: 0.004285715986043215\n",
            "step: 480, loss: 0.10548784583806992\n",
            "step: 490, loss: 0.003764424240216613\n",
            "step: 500, loss: 0.013078613206744194\n",
            "step: 510, loss: 0.13456493616104126\n",
            "step: 520, loss: 0.004147139377892017\n",
            "step: 530, loss: 0.004466884769499302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9308755760368663, f1=0.9337042188224385, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013968441635370255\n",
            "step: 10, loss: 0.0012740673264488578\n",
            "step: 20, loss: 0.0005568547639995813\n",
            "step: 30, loss: 0.2843917906284332\n",
            "step: 40, loss: 0.0001212691277032718\n",
            "step: 50, loss: 0.009244505316019058\n",
            "step: 60, loss: 0.007254383526742458\n",
            "step: 70, loss: 0.0006638478953391314\n",
            "step: 80, loss: 0.002205916214734316\n",
            "step: 90, loss: 0.09376157820224762\n",
            "step: 100, loss: 0.0014609259087592363\n",
            "step: 110, loss: 0.0048766653053462505\n",
            "step: 120, loss: 0.0006663592648692429\n",
            "step: 130, loss: 0.0063848369754850864\n",
            "step: 140, loss: 0.0008597875130362809\n",
            "step: 150, loss: 0.0013957148184999824\n",
            "step: 160, loss: 0.04443887621164322\n",
            "step: 170, loss: 0.009083995595574379\n",
            "step: 180, loss: 0.00011369732237653807\n",
            "step: 190, loss: 0.0011295417789369822\n",
            "step: 200, loss: 0.002174106426537037\n",
            "step: 210, loss: 0.0015002256259322166\n",
            "step: 220, loss: 0.0007287145708687603\n",
            "step: 230, loss: 0.001157433376647532\n",
            "step: 240, loss: 0.02515220455825329\n",
            "step: 250, loss: 0.0005110138445161283\n",
            "step: 260, loss: 0.0004191832267679274\n",
            "step: 270, loss: 0.0011972261127084494\n",
            "step: 280, loss: 0.28585585951805115\n",
            "step: 290, loss: 0.0008106542518362403\n",
            "step: 300, loss: 0.009808046743273735\n",
            "step: 310, loss: 0.08076801151037216\n",
            "step: 320, loss: 0.016513720154762268\n",
            "step: 330, loss: 0.001455010031349957\n",
            "step: 340, loss: 0.00010849506361410022\n",
            "step: 350, loss: 0.0001848995016189292\n",
            "step: 360, loss: 0.0029007818084210157\n",
            "step: 370, loss: 0.0009999481262639165\n",
            "step: 380, loss: 0.06760293990373611\n",
            "step: 390, loss: 0.16116182506084442\n",
            "step: 400, loss: 0.0013475967571139336\n",
            "step: 410, loss: 0.04638671875\n",
            "step: 420, loss: 0.005316935479640961\n",
            "step: 430, loss: 0.05575443059206009\n",
            "step: 440, loss: 0.031225116923451424\n",
            "step: 450, loss: 0.000613710202742368\n",
            "step: 460, loss: 0.0011444339761510491\n",
            "step: 470, loss: 0.0008887294679880142\n",
            "step: 480, loss: 0.00024813925847411156\n",
            "step: 490, loss: 0.031403373926877975\n",
            "step: 500, loss: 0.001649389392696321\n",
            "step: 510, loss: 0.005562176462262869\n",
            "step: 520, loss: 0.010710427537560463\n",
            "step: 530, loss: 0.0005508763715624809\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9350046425255338, f1=0.9330855018587362, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006608982221223414\n",
            "step: 10, loss: 9.753812628332525e-05\n",
            "step: 20, loss: 0.012566258199512959\n",
            "step: 30, loss: 0.00031160638900473714\n",
            "step: 40, loss: 0.00013254015357233584\n",
            "step: 50, loss: 0.003708675503730774\n",
            "step: 60, loss: 0.0001488519337726757\n",
            "step: 70, loss: 0.003983202390372753\n",
            "step: 80, loss: 0.0025048176757991314\n",
            "step: 90, loss: 3.211131479474716e-05\n",
            "step: 100, loss: 0.0033437085803598166\n",
            "step: 110, loss: 0.00011017084761988372\n",
            "step: 120, loss: 0.00030919822165742517\n",
            "step: 130, loss: 0.043866414576768875\n",
            "step: 140, loss: 0.0010142731480300426\n",
            "step: 150, loss: 0.005809554364532232\n",
            "step: 160, loss: 0.00033392017940059304\n",
            "step: 170, loss: 0.013425000011920929\n",
            "step: 180, loss: 0.00042993968236260116\n",
            "step: 190, loss: 0.001063808100298047\n",
            "step: 200, loss: 0.0003611628490034491\n",
            "step: 210, loss: 0.00029011606238782406\n",
            "step: 220, loss: 0.00026979329413734376\n",
            "step: 230, loss: 0.0017583256121724844\n",
            "step: 240, loss: 0.0005154876853339374\n",
            "step: 250, loss: 0.0017658810829743743\n",
            "step: 260, loss: 0.00991087406873703\n",
            "step: 270, loss: 5.5047534260666e-05\n",
            "step: 280, loss: 0.00041709517245180905\n",
            "step: 290, loss: 0.027341941371560097\n",
            "step: 300, loss: 0.00028129780548624694\n",
            "step: 310, loss: 0.0008242553449235857\n",
            "step: 320, loss: 0.0003146980598103255\n",
            "step: 330, loss: 0.0006854363018646836\n",
            "step: 340, loss: 0.0019205005373805761\n",
            "step: 350, loss: 0.008824613876640797\n",
            "step: 360, loss: 7.910741987871006e-05\n",
            "step: 370, loss: 0.005943265277892351\n",
            "step: 380, loss: 0.00030514583340846\n",
            "step: 390, loss: 0.0001663332513999194\n",
            "step: 400, loss: 0.002585864858701825\n",
            "step: 410, loss: 0.00021202386415097862\n",
            "step: 420, loss: 0.0006569541292265058\n",
            "step: 430, loss: 0.0004294740210752934\n",
            "step: 440, loss: 0.0026977595407515764\n",
            "step: 450, loss: 0.0006005557952448726\n",
            "step: 460, loss: 0.00012950006930623204\n",
            "step: 470, loss: 0.00013715428940486163\n",
            "step: 480, loss: 0.00016636328655295074\n",
            "step: 490, loss: 0.0010840811301022768\n",
            "step: 500, loss: 0.003165985457599163\n",
            "step: 510, loss: 0.0038095058407634497\n",
            "step: 520, loss: 0.0010932504665106535\n",
            "step: 530, loss: 0.00011255505523877218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9250936329588015, f1=0.9225715626466449, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003396144602447748\n",
            "step: 10, loss: 0.00011992715735686943\n",
            "step: 20, loss: 0.00017847606795839965\n",
            "step: 30, loss: 0.0011532288044691086\n",
            "step: 40, loss: 0.00023766969388816506\n",
            "step: 50, loss: 5.7765519159147516e-05\n",
            "step: 60, loss: 0.0004696470859926194\n",
            "step: 70, loss: 5.878507363377139e-05\n",
            "step: 80, loss: 0.0001011321583064273\n",
            "step: 90, loss: 0.00043970445403829217\n",
            "step: 100, loss: 0.001943996292538941\n",
            "step: 110, loss: 0.00014093273784965277\n",
            "step: 120, loss: 5.2691364544443786e-05\n",
            "step: 130, loss: 0.0005068217869848013\n",
            "step: 140, loss: 0.0018053660169243813\n",
            "step: 150, loss: 0.0003145506198052317\n",
            "step: 160, loss: 0.0007515937322750688\n",
            "step: 170, loss: 0.00010977408237522468\n",
            "step: 180, loss: 8.23492300696671e-05\n",
            "step: 190, loss: 0.0011733429273590446\n",
            "step: 200, loss: 0.0004922430962324142\n",
            "step: 210, loss: 0.0011706511722877622\n",
            "step: 220, loss: 0.001898410846479237\n",
            "step: 230, loss: 0.000531876168679446\n",
            "step: 240, loss: 0.00021527787612285465\n",
            "step: 250, loss: 0.00023305483045987785\n",
            "step: 260, loss: 0.004223083145916462\n",
            "step: 270, loss: 0.0015812565106898546\n",
            "step: 280, loss: 0.028550706803798676\n",
            "step: 290, loss: 0.00011967978934990242\n",
            "step: 300, loss: 0.00010587961878627539\n",
            "step: 310, loss: 0.000290554336970672\n",
            "step: 320, loss: 0.0007926473044790328\n",
            "step: 330, loss: 0.008853787556290627\n",
            "step: 340, loss: 0.0002529959019739181\n",
            "step: 350, loss: 0.05144836753606796\n",
            "step: 360, loss: 0.0003444428148213774\n",
            "step: 370, loss: 0.004371405579149723\n",
            "step: 380, loss: 0.002275161212310195\n",
            "step: 390, loss: 0.00013050738198217005\n",
            "step: 400, loss: 0.0015995497815310955\n",
            "step: 410, loss: 0.0017273633275181055\n",
            "step: 420, loss: 0.001176983118057251\n",
            "step: 430, loss: 0.00024424551520496607\n",
            "step: 440, loss: 0.0007434709114022553\n",
            "step: 450, loss: 0.0004207190650049597\n",
            "step: 460, loss: 0.0002295111771672964\n",
            "step: 470, loss: 0.00018963150796480477\n",
            "step: 480, loss: 0.041733141988515854\n",
            "step: 490, loss: 0.000375897012418136\n",
            "step: 500, loss: 0.0496344156563282\n",
            "step: 510, loss: 0.002978340955451131\n",
            "step: 520, loss: 0.00138323032297194\n",
            "step: 530, loss: 0.0008227488724514842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9257683215130024, f1=0.9161904761904762, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006183511111885309\n",
            "step: 10, loss: 0.009700002148747444\n",
            "step: 20, loss: 0.0004322330350987613\n",
            "step: 30, loss: 0.0003445993934292346\n",
            "step: 40, loss: 0.0011188415810465813\n",
            "step: 50, loss: 0.022155988961458206\n",
            "step: 60, loss: 0.0860411748290062\n",
            "step: 70, loss: 0.00015604142390657216\n",
            "step: 80, loss: 0.000468239450128749\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.000697278999723494\n",
            "step: 100, loss: 0.03396039083600044\n",
            "step: 110, loss: 0.015075547620654106\n",
            "step: 120, loss: 0.0032000967767089605\n",
            "step: 130, loss: 0.001421774853952229\n",
            "step: 140, loss: 0.0007689374033361673\n",
            "step: 150, loss: 0.00011166747572133318\n",
            "step: 160, loss: 4.380721293273382e-05\n",
            "step: 170, loss: 0.00012784163118340075\n",
            "step: 180, loss: 0.00016433930431958288\n",
            "step: 190, loss: 7.215381629066542e-05\n",
            "step: 200, loss: 0.020556455478072166\n",
            "step: 210, loss: 0.00043162042857147753\n",
            "step: 220, loss: 0.02331605926156044\n",
            "step: 230, loss: 0.0001416872109984979\n",
            "step: 240, loss: 0.0008677492733113468\n",
            "step: 250, loss: 0.001906550140120089\n",
            "step: 260, loss: 0.0001671548088779673\n",
            "step: 270, loss: 0.0007331918459385633\n",
            "step: 280, loss: 0.00023865429102443159\n",
            "step: 290, loss: 0.0005053990753367543\n",
            "step: 300, loss: 7.844743231544271e-05\n",
            "step: 310, loss: 0.00018134366837330163\n",
            "step: 320, loss: 0.001343687530606985\n",
            "step: 330, loss: 0.00031486907391808927\n",
            "step: 340, loss: 5.546441752812825e-05\n",
            "step: 350, loss: 5.879807940800674e-05\n",
            "step: 360, loss: 5.198052531341091e-05\n",
            "step: 370, loss: 0.0011367269326001406\n",
            "step: 380, loss: 5.51946104678791e-05\n",
            "step: 390, loss: 5.511147537617944e-05\n",
            "step: 400, loss: 0.0009758605156093836\n",
            "step: 410, loss: 0.0038225825410336256\n",
            "step: 420, loss: 0.00014743870997335762\n",
            "step: 430, loss: 8.37927291286178e-05\n",
            "step: 440, loss: 0.000311432930175215\n",
            "step: 450, loss: 0.00018823324353434145\n",
            "step: 460, loss: 0.00018719356739893556\n",
            "step: 470, loss: 3.715714774443768e-05\n",
            "step: 480, loss: 0.0010311445221304893\n",
            "step: 490, loss: 9.226264955941588e-05\n",
            "step: 500, loss: 6.753928028047085e-05\n",
            "step: 510, loss: 0.0002789102145470679\n",
            "step: 520, loss: 0.0008386140689253807\n",
            "step: 530, loss: 0.0026630021166056395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9292076887013595, f1=0.9235100891600189, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004140136763453484\n",
            "step: 10, loss: 3.693836333695799e-05\n",
            "step: 20, loss: 0.012024296447634697\n",
            "step: 30, loss: 0.0001530235167592764\n",
            "step: 40, loss: 0.0008427543216384947\n",
            "step: 50, loss: 0.0064666904509067535\n",
            "step: 60, loss: 0.002674748655408621\n",
            "step: 70, loss: 0.00011237413855269551\n",
            "step: 80, loss: 0.00016834016423672438\n",
            "step: 90, loss: 0.001277645817026496\n",
            "step: 100, loss: 0.00022549911227542907\n",
            "step: 110, loss: 8.112001523841172e-05\n",
            "step: 120, loss: 8.962200081441551e-05\n",
            "step: 130, loss: 0.0017346425447613\n",
            "step: 140, loss: 0.02009095624089241\n",
            "step: 150, loss: 0.0002722779172472656\n",
            "step: 160, loss: 0.00028993390151299536\n",
            "step: 170, loss: 0.0003728807205334306\n",
            "step: 180, loss: 0.00010088331328006461\n",
            "step: 190, loss: 0.0005703077185899019\n",
            "step: 200, loss: 0.0005429785815067589\n",
            "step: 210, loss: 0.00044286984484642744\n",
            "step: 220, loss: 6.166548701003194e-05\n",
            "step: 230, loss: 6.280246452661231e-05\n",
            "step: 240, loss: 0.0009836372919380665\n",
            "step: 250, loss: 0.0017469319282099605\n",
            "step: 260, loss: 0.0001132977704401128\n",
            "step: 270, loss: 9.156251326203346e-05\n",
            "step: 280, loss: 0.027523312717676163\n",
            "step: 290, loss: 0.001828458858653903\n",
            "step: 300, loss: 0.005202615633606911\n",
            "step: 310, loss: 0.00036504241870716214\n",
            "step: 320, loss: 5.8888603234663606e-05\n",
            "step: 330, loss: 0.0001548353029647842\n",
            "step: 340, loss: 0.00032244520843960345\n",
            "step: 350, loss: 0.0002857298240996897\n",
            "step: 360, loss: 0.00016083900118246675\n",
            "step: 370, loss: 0.0004252833023201674\n",
            "step: 380, loss: 0.0004267265321686864\n",
            "step: 390, loss: 0.0022220283281058073\n",
            "step: 400, loss: 0.00029382313368842006\n",
            "step: 410, loss: 0.005697300657629967\n",
            "step: 420, loss: 0.03493368625640869\n",
            "step: 430, loss: 0.002300616819411516\n",
            "step: 440, loss: 0.004414400551468134\n",
            "step: 450, loss: 0.0007699363050051033\n",
            "step: 460, loss: 9.502691682428122e-05\n",
            "step: 470, loss: 9.541920735500753e-05\n",
            "step: 480, loss: 0.0021321100648492575\n",
            "step: 490, loss: 0.00035032874438911676\n",
            "step: 500, loss: 0.00015108443039935082\n",
            "step: 510, loss: 0.0018416864331811666\n",
            "step: 520, loss: 3.665813710540533e-05\n",
            "step: 530, loss: 0.055099062621593475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9284712482468443, f1=0.9256820319849483, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002093459013849497\n",
            "step: 10, loss: 0.0002484804717823863\n",
            "step: 20, loss: 0.003931957762688398\n",
            "step: 30, loss: 0.004539126064628363\n",
            "step: 40, loss: 0.00015848023758735508\n",
            "step: 50, loss: 0.0004795166023541242\n",
            "step: 60, loss: 0.0002486702287569642\n",
            "step: 70, loss: 8.403797255596146e-05\n",
            "step: 80, loss: 0.00023611244978383183\n",
            "step: 90, loss: 0.012485766783356667\n",
            "step: 100, loss: 0.0006204357487149537\n",
            "step: 110, loss: 0.002663047518581152\n",
            "step: 120, loss: 0.005804837681353092\n",
            "step: 130, loss: 0.0009138752357102931\n",
            "step: 140, loss: 0.03051638789474964\n",
            "step: 150, loss: 0.0016841395990923047\n",
            "step: 160, loss: 4.235333108226769e-05\n",
            "step: 170, loss: 0.00022314822126645595\n",
            "step: 180, loss: 8.979316771728918e-05\n",
            "step: 190, loss: 0.0002728084218688309\n",
            "step: 200, loss: 9.350771142635494e-05\n",
            "step: 210, loss: 0.0007927849073894322\n",
            "step: 220, loss: 9.051791857928038e-05\n",
            "step: 230, loss: 0.005404164083302021\n",
            "step: 240, loss: 0.0007669439655728638\n",
            "step: 250, loss: 0.0004247694159857929\n",
            "step: 260, loss: 0.0012273866450414062\n",
            "step: 270, loss: 0.00011459668894531205\n",
            "step: 280, loss: 0.00027331410092301667\n",
            "step: 290, loss: 0.001853934838436544\n",
            "step: 300, loss: 6.405235035344958e-05\n",
            "step: 310, loss: 0.0037577508483082056\n",
            "step: 320, loss: 0.006621803622692823\n",
            "step: 330, loss: 0.0007742154411971569\n",
            "step: 340, loss: 0.0003310991742182523\n",
            "step: 350, loss: 0.0006882966263219714\n",
            "step: 360, loss: 5.040665564592928e-05\n",
            "step: 370, loss: 0.002101023681461811\n",
            "step: 380, loss: 0.000152743115904741\n",
            "step: 390, loss: 0.0062352982349693775\n",
            "step: 400, loss: 0.000329487316776067\n",
            "step: 410, loss: 0.0014702138723805547\n",
            "step: 420, loss: 0.00024486170150339603\n",
            "step: 430, loss: 0.0003811752831097692\n",
            "step: 440, loss: 0.00017332231800537556\n",
            "step: 450, loss: 9.668127313489094e-05\n",
            "step: 460, loss: 0.0005924534052610397\n",
            "step: 470, loss: 0.00014015917258802801\n",
            "step: 480, loss: 0.00010788116924231872\n",
            "step: 490, loss: 0.003309716237708926\n",
            "step: 500, loss: 0.00037394592072814703\n",
            "step: 510, loss: 0.004057477228343487\n",
            "step: 520, loss: 0.0003109322569798678\n",
            "step: 530, loss: 0.0002447979059070349\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9301895515487749, f1=0.9264229523368811, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008828625432215631\n",
            "step: 10, loss: 3.681459202198312e-05\n",
            "step: 20, loss: 0.00013902199862059206\n",
            "step: 30, loss: 6.671086885035038e-05\n",
            "step: 40, loss: 0.00019689188047777861\n",
            "step: 50, loss: 0.00862960983067751\n",
            "step: 60, loss: 0.00044714572140946984\n",
            "step: 70, loss: 0.0002905844885390252\n",
            "step: 80, loss: 2.9506347345886752e-05\n",
            "step: 90, loss: 0.0007768190116621554\n",
            "step: 100, loss: 0.00017259051674045622\n",
            "step: 110, loss: 9.704477997729555e-05\n",
            "step: 120, loss: 0.00033363429247401655\n",
            "step: 130, loss: 0.006469429470598698\n",
            "step: 140, loss: 0.0003493014955893159\n",
            "step: 150, loss: 0.00047237929538823664\n",
            "step: 160, loss: 0.0004237140528857708\n",
            "step: 170, loss: 0.00041165947914123535\n",
            "step: 180, loss: 5.441347093437798e-05\n",
            "step: 190, loss: 0.00036625683424063027\n",
            "step: 200, loss: 0.0001330904196947813\n",
            "step: 210, loss: 4.0920036553870887e-05\n",
            "step: 220, loss: 0.0025351541116833687\n",
            "step: 230, loss: 0.000347140128724277\n",
            "step: 240, loss: 0.00017475885397288948\n",
            "step: 250, loss: 0.0034296808298677206\n",
            "step: 260, loss: 0.0016037391033023596\n",
            "step: 270, loss: 0.00017077724623959512\n",
            "step: 280, loss: 0.00019122511730529368\n",
            "step: 290, loss: 0.007198140025138855\n",
            "step: 300, loss: 0.002270712750032544\n",
            "step: 310, loss: 0.000772113271523267\n",
            "step: 320, loss: 0.0005500062834471464\n",
            "step: 330, loss: 0.0020467224530875683\n",
            "step: 340, loss: 0.00019904789223801345\n",
            "step: 350, loss: 0.00033528023050166667\n",
            "step: 360, loss: 0.005053210072219372\n",
            "step: 370, loss: 9.535243589198217e-05\n",
            "step: 380, loss: 0.0006410346832126379\n",
            "step: 390, loss: 0.035266581922769547\n",
            "step: 400, loss: 0.00027669465634971857\n",
            "step: 410, loss: 0.002117678988724947\n",
            "step: 420, loss: 0.23341630399227142\n",
            "step: 430, loss: 4.383133273222484e-05\n",
            "step: 440, loss: 0.006542154587805271\n",
            "step: 450, loss: 0.007428754586726427\n",
            "step: 460, loss: 0.0003629977290984243\n",
            "step: 470, loss: 0.00032317920704372227\n",
            "step: 480, loss: 0.0001225252781296149\n",
            "step: 490, loss: 3.832978109130636e-05\n",
            "step: 500, loss: 0.0003525686333887279\n",
            "step: 510, loss: 0.008421283215284348\n",
            "step: 520, loss: 0.0004905915702693164\n",
            "step: 530, loss: 0.00812464114278555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9329556185080263, f1=0.9248937175247992, best_f1=0.9289719626168225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.638316411525011e-05\n",
            "step: 10, loss: 0.00013386573118623346\n",
            "step: 20, loss: 0.000956860720179975\n",
            "step: 30, loss: 0.00015131205145735294\n",
            "step: 40, loss: 0.02402821183204651\n",
            "step: 50, loss: 8.263030758826062e-05\n",
            "step: 60, loss: 0.000137067268951796\n",
            "step: 70, loss: 0.0014191848458722234\n",
            "step: 80, loss: 7.646105950698256e-05\n",
            "step: 90, loss: 0.00010268065670970827\n",
            "step: 100, loss: 0.0003870357759296894\n",
            "step: 110, loss: 0.0003044807817786932\n",
            "step: 120, loss: 0.0005781413055956364\n",
            "step: 130, loss: 0.132158562541008\n",
            "step: 140, loss: 0.00040255545172840357\n",
            "step: 150, loss: 6.30418217042461e-05\n",
            "step: 160, loss: 0.0001228264154633507\n",
            "step: 170, loss: 0.0001469750568503514\n",
            "step: 180, loss: 6.435428804252297e-05\n",
            "step: 190, loss: 0.00012690799485426396\n",
            "step: 200, loss: 0.00013557725469581783\n",
            "step: 210, loss: 0.01124142948538065\n",
            "step: 220, loss: 3.606607424444519e-05\n",
            "step: 230, loss: 0.10171453654766083\n",
            "step: 240, loss: 5.560824502026662e-05\n",
            "step: 250, loss: 0.00024255149764940143\n",
            "step: 260, loss: 8.730378613108769e-05\n",
            "step: 270, loss: 2.540941750339698e-05\n",
            "step: 280, loss: 5.254785719444044e-05\n",
            "step: 290, loss: 2.353581294300966e-05\n",
            "step: 300, loss: 2.106990541506093e-05\n",
            "step: 310, loss: 0.0003605015517678112\n",
            "step: 320, loss: 7.199015817604959e-05\n",
            "step: 330, loss: 0.0012306615244597197\n",
            "step: 340, loss: 7.128303695935756e-05\n",
            "step: 350, loss: 8.047474693739787e-05\n",
            "step: 360, loss: 0.0005629633087664843\n",
            "step: 370, loss: 0.000157530652359128\n",
            "step: 380, loss: 9.745806164573878e-05\n",
            "step: 390, loss: 0.0031704904977232218\n",
            "step: 400, loss: 0.01190404873341322\n",
            "step: 410, loss: 0.0005819576908834279\n",
            "step: 420, loss: 0.0011317136231809855\n",
            "step: 430, loss: 0.00014207027561496943\n",
            "step: 440, loss: 0.00042443061829544604\n",
            "step: 450, loss: 4.756827183882706e-05\n",
            "step: 460, loss: 2.254839455417823e-05\n",
            "step: 470, loss: 0.0003973132697865367\n",
            "step: 480, loss: 3.802841456490569e-05\n",
            "step: 490, loss: 2.1877840481465682e-05\n",
            "step: 500, loss: 0.030920125544071198\n",
            "step: 510, loss: 0.00010846649820450693\n",
            "step: 520, loss: 3.428575291763991e-05\n",
            "step: 530, loss: 0.00018212015856988728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9328891004283675, f1=0.9235154394299286, best_f1=0.9289719626168225\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 196.70it/s]\n",
            "load_f1 = 0.9339578454332552\n",
            "real_f1 = 0.9322671683913453\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d509ffb-f06f-4447-b3d2-39b517a80992"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5628790259361267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5161290322580646, f1=0.4, best_f1=0.4\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.46800440549850464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.4615384615384615, f1=0.39285714285714285, best_f1=0.4\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4539681077003479\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6086956521739131, f1=0.41025641025641024, best_f1=0.41025641025641024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3242473602294922\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7096774193548386, f1=0.4782608695652174, best_f1=0.4782608695652174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23497581481933594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5263157894736842, f1=0.64, best_f1=0.4782608695652174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3422366976737976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6, f1=0.5833333333333334, best_f1=0.4782608695652174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19782574474811554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8, f1=0.6250000000000001, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011447827331721783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8, f1=0.6250000000000001, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026455437764525414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7692307692307692, f1=0.6250000000000001, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006094394717365503\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.782608695652174, f1=0.6666666666666666, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027552435640245676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7692307692307692, f1=0.6666666666666666, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00666341558098793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8, f1=0.6206896551724138, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006637635175138712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.782608695652174, f1=0.5714285714285714, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003047654405236244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8, f1=0.6206896551724138, best_f1=0.6250000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006738625466823578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8, f1=0.6206896551724138, best_f1=0.6250000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 139605.58it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8\n",
            "real_f1 = 0.7407407407407408\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 190.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c351f88-3740-4f93-d498-62df6f041021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6499109268188477\n",
            "step: 10, loss: 0.6404380798339844\n",
            "step: 20, loss: 0.33818355202674866\n",
            "step: 30, loss: 0.10740054398775101\n",
            "step: 40, loss: 0.43571341037750244\n",
            "step: 50, loss: 0.0387084074318409\n",
            "step: 60, loss: 0.0304944459348917\n",
            "step: 70, loss: 0.0627317875623703\n",
            "step: 80, loss: 0.07253143936395645\n",
            "step: 90, loss: 0.24622781574726105\n",
            "step: 100, loss: 0.005992227699607611\n",
            "step: 110, loss: 0.2653926610946655\n",
            "step: 120, loss: 0.01825672574341297\n",
            "step: 130, loss: 0.006490346975624561\n",
            "step: 140, loss: 0.004942466039210558\n",
            "step: 150, loss: 0.007207461632788181\n",
            "step: 160, loss: 0.006130199879407883\n",
            "step: 170, loss: 0.10741986334323883\n",
            "step: 180, loss: 0.019619911909103394\n",
            "step: 190, loss: 0.0998012125492096\n",
            "step: 200, loss: 0.019376881420612335\n",
            "step: 210, loss: 0.0052696396596729755\n",
            "step: 220, loss: 0.002717965515330434\n",
            "step: 230, loss: 0.2562961280345917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9820627802690582, f1=0.9750566893424036, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028902951162308455\n",
            "step: 10, loss: 0.0030420708935707808\n",
            "step: 20, loss: 0.08488193899393082\n",
            "step: 30, loss: 0.17557646334171295\n",
            "step: 40, loss: 0.05593681335449219\n",
            "step: 50, loss: 0.042870309203863144\n",
            "step: 60, loss: 0.0028662518598139286\n",
            "step: 70, loss: 0.11992425471544266\n",
            "step: 80, loss: 0.0074059502221643925\n",
            "step: 90, loss: 0.14768856763839722\n",
            "step: 100, loss: 0.16069245338439941\n",
            "step: 110, loss: 0.027385877445340157\n",
            "step: 120, loss: 0.13472767174243927\n",
            "step: 130, loss: 0.025348249822854996\n",
            "step: 140, loss: 0.005405582953244448\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.005307094659656286\n",
            "step: 160, loss: 0.0020937644876539707\n",
            "step: 170, loss: 0.001976720057427883\n",
            "step: 180, loss: 0.001152555807493627\n",
            "step: 190, loss: 0.007001284975558519\n",
            "step: 200, loss: 0.005872141104191542\n",
            "step: 210, loss: 0.0005663957563228905\n",
            "step: 220, loss: 0.04687467962503433\n",
            "step: 230, loss: 0.16096359491348267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9730941704035874, f1=0.9761092150170648, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026318546384572983\n",
            "step: 10, loss: 0.013838320970535278\n",
            "step: 20, loss: 0.0017308200476691127\n",
            "step: 30, loss: 0.013464020565152168\n",
            "step: 40, loss: 0.04790053516626358\n",
            "step: 50, loss: 0.00860634446144104\n",
            "step: 60, loss: 0.03948524594306946\n",
            "step: 70, loss: 0.010546554811298847\n",
            "step: 80, loss: 0.000841242668684572\n",
            "step: 90, loss: 0.1957041621208191\n",
            "step: 100, loss: 0.005007543601095676\n",
            "step: 110, loss: 0.0007771142991259694\n",
            "step: 120, loss: 0.051020704209804535\n",
            "step: 130, loss: 0.006505885161459446\n",
            "step: 140, loss: 0.008034494705498219\n",
            "step: 150, loss: 0.003458187449723482\n",
            "step: 160, loss: 0.004795626271516085\n",
            "step: 170, loss: 0.010964491404592991\n",
            "step: 180, loss: 0.0062615820206701756\n",
            "step: 190, loss: 0.002107849810272455\n",
            "step: 200, loss: 0.016472330316901207\n",
            "step: 210, loss: 0.02824213169515133\n",
            "step: 220, loss: 0.0005137564148753881\n",
            "step: 230, loss: 0.001352141727693379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9853768278965129, f1=0.9764309764309763, best_f1=0.9764309764309763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006390249473042786\n",
            "step: 10, loss: 0.0002836911880876869\n",
            "step: 20, loss: 0.002010715426877141\n",
            "step: 30, loss: 0.0005628179642371833\n",
            "step: 40, loss: 0.003220637096092105\n",
            "step: 50, loss: 0.0007250572089105844\n",
            "step: 60, loss: 0.030013099312782288\n",
            "step: 70, loss: 0.00834976602345705\n",
            "step: 80, loss: 0.0010333952959626913\n",
            "step: 90, loss: 0.0012954360572621226\n",
            "step: 100, loss: 0.000581770611461252\n",
            "step: 110, loss: 0.0007268697954714298\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.08803177624940872\n",
            "step: 130, loss: 0.033187732100486755\n",
            "step: 140, loss: 0.001997441751882434\n",
            "step: 150, loss: 0.004102783277630806\n",
            "step: 160, loss: 0.0002737884351518005\n",
            "step: 170, loss: 0.0027118981815874577\n",
            "step: 180, loss: 0.0009003353770822287\n",
            "step: 190, loss: 0.0008849338046275079\n",
            "step: 200, loss: 0.0024761618115007877\n",
            "step: 210, loss: 0.0018983418121933937\n",
            "step: 220, loss: 0.0002587438502814621\n",
            "step: 230, loss: 0.005022658035159111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9887387387387387, f1=0.9819413092550789, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00034429033985361457\n",
            "step: 10, loss: 0.0003005805774591863\n",
            "step: 20, loss: 0.0009440059075132012\n",
            "step: 30, loss: 0.00022006318613421172\n",
            "step: 40, loss: 0.0004901050706394017\n",
            "step: 50, loss: 0.0005131374346092343\n",
            "step: 60, loss: 0.00132584641687572\n",
            "step: 70, loss: 0.001550248940475285\n",
            "step: 80, loss: 0.0007385800709016621\n",
            "step: 90, loss: 0.000344013242283836\n",
            "step: 100, loss: 0.00021505825861822814\n",
            "step: 110, loss: 0.0005877810763195157\n",
            "step: 120, loss: 8.567688928451389e-05\n",
            "step: 130, loss: 0.00031277057132683694\n",
            "step: 140, loss: 0.0036069871857762337\n",
            "step: 150, loss: 0.0011335948947817087\n",
            "step: 160, loss: 0.0006685375119559467\n",
            "step: 170, loss: 0.051312293857336044\n",
            "step: 180, loss: 0.00833399873226881\n",
            "step: 190, loss: 0.0076180389150977135\n",
            "step: 200, loss: 0.022601531818509102\n",
            "step: 210, loss: 0.001622588373720646\n",
            "step: 220, loss: 0.0029344195500016212\n",
            "step: 230, loss: 0.0006067244685254991\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9887387387387387, f1=0.9831649831649831, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001258359756320715\n",
            "step: 10, loss: 0.0005805341061204672\n",
            "step: 20, loss: 0.0035291300155222416\n",
            "step: 30, loss: 0.0007948352722451091\n",
            "step: 40, loss: 0.00026177382096648216\n",
            "step: 50, loss: 0.00023698896984569728\n",
            "step: 60, loss: 0.00014981160347815603\n",
            "step: 70, loss: 0.00038707340718246996\n",
            "step: 80, loss: 0.0013755386462435126\n",
            "step: 90, loss: 0.10804078727960587\n",
            "step: 100, loss: 0.032018497586250305\n",
            "step: 110, loss: 0.0025057268794625998\n",
            "step: 120, loss: 0.0030430436599999666\n",
            "step: 130, loss: 0.0011317646130919456\n",
            "step: 140, loss: 0.00014395129983313382\n",
            "step: 150, loss: 0.00031024092459119856\n",
            "step: 160, loss: 0.005494014360010624\n",
            "step: 170, loss: 0.00015672114386688918\n",
            "step: 180, loss: 0.002560992958024144\n",
            "step: 190, loss: 0.03642645478248596\n",
            "step: 200, loss: 0.0002681954065337777\n",
            "step: 210, loss: 0.0007197900558821857\n",
            "step: 220, loss: 0.0003659986541606486\n",
            "step: 230, loss: 0.044792819768190384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9854096520763187, f1=0.9785794813979707, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015267760027199984\n",
            "step: 10, loss: 0.0001697596162557602\n",
            "step: 20, loss: 0.001133991521783173\n",
            "step: 30, loss: 0.016385924071073532\n",
            "step: 40, loss: 0.0006883738678880036\n",
            "step: 50, loss: 0.0023778830654919147\n",
            "step: 60, loss: 0.00089441635645926\n",
            "step: 70, loss: 0.01443660631775856\n",
            "step: 80, loss: 0.00043627448030747473\n",
            "step: 90, loss: 0.00010141950770048425\n",
            "step: 100, loss: 0.000171036968822591\n",
            "step: 110, loss: 0.0004819768655579537\n",
            "step: 120, loss: 0.00025616015773266554\n",
            "step: 130, loss: 0.0006762737757526338\n",
            "step: 140, loss: 0.0002930292102973908\n",
            "step: 150, loss: 0.005617765709757805\n",
            "step: 160, loss: 0.051642756909132004\n",
            "step: 170, loss: 0.0006658969214186072\n",
            "step: 180, loss: 0.0017408514395356178\n",
            "step: 190, loss: 0.0003176049212925136\n",
            "step: 200, loss: 0.031610533595085144\n",
            "step: 210, loss: 0.00014886818826198578\n",
            "step: 220, loss: 0.004541992675513029\n",
            "step: 230, loss: 0.0035705030895769596\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9863636363636363, f1=0.9806598407281, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013374965637922287\n",
            "step: 10, loss: 0.00047512276796624064\n",
            "step: 20, loss: 0.00011524009460117668\n",
            "step: 30, loss: 0.00016173605399671942\n",
            "step: 40, loss: 0.000538735359441489\n",
            "step: 50, loss: 0.00023987094755284488\n",
            "step: 60, loss: 0.0005943485884927213\n",
            "step: 70, loss: 0.00023697350115980953\n",
            "step: 80, loss: 0.0021772868931293488\n",
            "step: 90, loss: 0.00018048447964247316\n",
            "step: 100, loss: 0.0008017154177650809\n",
            "step: 110, loss: 0.0013392710825428367\n",
            "step: 120, loss: 0.00022335625544656068\n",
            "step: 130, loss: 0.007440399844199419\n",
            "step: 140, loss: 0.0001825473882490769\n",
            "step: 150, loss: 0.00030576949939131737\n",
            "step: 160, loss: 0.0009877369739115238\n",
            "step: 170, loss: 0.00026091362815350294\n",
            "step: 180, loss: 9.741173562360927e-05\n",
            "step: 190, loss: 0.00018503218598198146\n",
            "step: 200, loss: 0.00017073056369554251\n",
            "step: 210, loss: 0.00012707685527857393\n",
            "step: 220, loss: 0.0003257939824834466\n",
            "step: 230, loss: 0.00017310735711362213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9875424688561721, f1=0.9772727272727272, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011777962208725512\n",
            "step: 10, loss: 0.0006153172580525279\n",
            "step: 20, loss: 0.0007745450711809099\n",
            "step: 30, loss: 6.51234295219183e-05\n",
            "step: 40, loss: 0.03178218752145767\n",
            "step: 50, loss: 0.00012249067367520183\n",
            "step: 60, loss: 0.00011851927411044016\n",
            "step: 70, loss: 0.001919787609949708\n",
            "step: 80, loss: 0.0010732188820838928\n",
            "step: 90, loss: 0.00020442645472940058\n",
            "step: 100, loss: 0.00026922710821963847\n",
            "step: 110, loss: 0.001838106196373701\n",
            "step: 120, loss: 9.186736861011013e-05\n",
            "step: 130, loss: 4.312100281822495e-05\n",
            "step: 140, loss: 4.839895336772315e-05\n",
            "step: 150, loss: 0.00024794391356408596\n",
            "step: 160, loss: 7.037647446850315e-05\n",
            "step: 170, loss: 0.00010992435272783041\n",
            "step: 180, loss: 0.0018144132336601615\n",
            "step: 190, loss: 0.00010586456482997164\n",
            "step: 200, loss: 6.798967660870403e-05\n",
            "step: 210, loss: 0.0007703670999035239\n",
            "step: 220, loss: 4.9479414883535355e-05\n",
            "step: 230, loss: 0.0001025608362397179\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9864253393665158, f1=0.9807037457434733, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.266698569059372e-05\n",
            "step: 10, loss: 2.7864369258168153e-05\n",
            "step: 20, loss: 7.076525798765942e-05\n",
            "step: 30, loss: 0.0001288107450818643\n",
            "step: 40, loss: 0.0017209912184625864\n",
            "step: 50, loss: 0.0001353437837678939\n",
            "step: 60, loss: 0.001457109465263784\n",
            "step: 70, loss: 0.0026412836741656065\n",
            "step: 80, loss: 9.239283826900646e-05\n",
            "step: 90, loss: 0.0003521083854138851\n",
            "step: 100, loss: 0.00017213723913300782\n",
            "step: 110, loss: 8.267957309726626e-05\n",
            "step: 120, loss: 8.474523929180577e-05\n",
            "step: 130, loss: 7.986660784808919e-05\n",
            "step: 140, loss: 0.06927884370088577\n",
            "step: 150, loss: 0.009639851748943329\n",
            "step: 160, loss: 5.55875594727695e-05\n",
            "step: 170, loss: 0.0008785890531726182\n",
            "step: 180, loss: 7.435668521793559e-05\n",
            "step: 190, loss: 0.014198931865394115\n",
            "step: 200, loss: 0.004320110194385052\n",
            "step: 210, loss: 5.6249515182571486e-05\n",
            "step: 220, loss: 0.0001812630071071908\n",
            "step: 230, loss: 0.002798826899379492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9865470852017937, f1=0.9831271091113611, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.470128523185849e-05\n",
            "step: 10, loss: 0.00023333601711783558\n",
            "step: 20, loss: 0.0008139958954416215\n",
            "step: 30, loss: 0.025641456246376038\n",
            "step: 40, loss: 0.00018291579908691347\n",
            "step: 50, loss: 0.00011145273310830817\n",
            "step: 60, loss: 9.51944530243054e-05\n",
            "step: 70, loss: 0.0004295269900467247\n",
            "step: 80, loss: 4.322562017478049e-05\n",
            "step: 90, loss: 6.999829201959074e-05\n",
            "step: 100, loss: 6.21319777565077e-05\n",
            "step: 110, loss: 0.014245384372770786\n",
            "step: 120, loss: 4.8328387492801994e-05\n",
            "step: 130, loss: 3.3045304007828236e-05\n",
            "step: 140, loss: 5.17582411703188e-05\n",
            "step: 150, loss: 0.024614961817860603\n",
            "step: 160, loss: 4.5561155275208876e-05\n",
            "step: 170, loss: 0.023466214537620544\n",
            "step: 180, loss: 6.803734868299216e-05\n",
            "step: 190, loss: 6.0815702454419807e-05\n",
            "step: 200, loss: 9.645529644330963e-05\n",
            "step: 210, loss: 4.0472830733051524e-05\n",
            "step: 220, loss: 6.148131069494411e-05\n",
            "step: 230, loss: 0.00012699011131189764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9875706214689265, f1=0.9806598407281, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.726773088099435e-05\n",
            "step: 10, loss: 4.803081537829712e-05\n",
            "step: 20, loss: 4.558289219858125e-05\n",
            "step: 30, loss: 7.004579674685374e-05\n",
            "step: 40, loss: 9.123428026214242e-05\n",
            "step: 50, loss: 0.00020978820975869894\n",
            "step: 60, loss: 0.0002706437953747809\n",
            "step: 70, loss: 6.016105544404127e-05\n",
            "step: 80, loss: 8.107251778710634e-05\n",
            "step: 90, loss: 5.808902278658934e-05\n",
            "step: 100, loss: 0.0001347788784187287\n",
            "step: 110, loss: 8.746454113861546e-05\n",
            "step: 120, loss: 6.600791675737128e-05\n",
            "step: 130, loss: 4.970364534528926e-05\n",
            "step: 140, loss: 0.013006370514631271\n",
            "step: 150, loss: 0.00010073917655972764\n",
            "step: 160, loss: 6.310873868642375e-05\n",
            "step: 170, loss: 6.0834918258478865e-05\n",
            "step: 180, loss: 0.024975404143333435\n",
            "step: 190, loss: 3.769757677218877e-05\n",
            "step: 200, loss: 2.7897362087969668e-05\n",
            "step: 210, loss: 5.3642950661014766e-05\n",
            "step: 220, loss: 5.592870365944691e-05\n",
            "step: 230, loss: 0.07343403995037079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9864253393665158, f1=0.979591836734694, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.765325426589698e-05\n",
            "step: 10, loss: 0.00021181110059842467\n",
            "step: 20, loss: 0.0004999861703254282\n",
            "step: 30, loss: 7.976012420840561e-05\n",
            "step: 40, loss: 0.0001394561113556847\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.0001325777411693707\n",
            "step: 60, loss: 5.178566061658785e-05\n",
            "step: 70, loss: 3.0419440008699894e-05\n",
            "step: 80, loss: 4.4566153519554064e-05\n",
            "step: 90, loss: 3.996642408310436e-05\n",
            "step: 100, loss: 4.187223385088146e-05\n",
            "step: 110, loss: 0.0262893158942461\n",
            "step: 120, loss: 0.0443478487432003\n",
            "step: 130, loss: 6.082091204007156e-05\n",
            "step: 140, loss: 4.501776857068762e-05\n",
            "step: 150, loss: 6.026994378771633e-05\n",
            "step: 160, loss: 9.220552601618692e-05\n",
            "step: 170, loss: 6.262618990149349e-05\n",
            "step: 180, loss: 5.088022953714244e-05\n",
            "step: 190, loss: 3.386514072190039e-05\n",
            "step: 200, loss: 6.828460027463734e-05\n",
            "step: 210, loss: 2.78157622233266e-05\n",
            "step: 220, loss: 5.151160803507082e-05\n",
            "step: 230, loss: 4.9019119614968076e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9875141884222476, f1=0.9794520547945206, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.025319867650978e-05\n",
            "step: 10, loss: 0.00981968268752098\n",
            "step: 20, loss: 5.0379465392325073e-05\n",
            "step: 30, loss: 7.64135256758891e-05\n",
            "step: 40, loss: 0.00016172986943274736\n",
            "step: 50, loss: 4.4712269300362095e-05\n",
            "step: 60, loss: 6.944490451132879e-05\n",
            "step: 70, loss: 6.178942567203194e-05\n",
            "step: 80, loss: 5.0298905989620835e-05\n",
            "step: 90, loss: 0.00011610796354943886\n",
            "step: 100, loss: 4.775624620378949e-05\n",
            "step: 110, loss: 5.1423041441012174e-05\n",
            "step: 120, loss: 2.6928828447125852e-05\n",
            "step: 130, loss: 5.040309770265594e-05\n",
            "step: 140, loss: 7.106881821528077e-05\n",
            "step: 150, loss: 4.130293382331729e-05\n",
            "step: 160, loss: 0.003184210741892457\n",
            "step: 170, loss: 1.9140275981044397e-05\n",
            "step: 180, loss: 3.430474316701293e-05\n",
            "step: 190, loss: 0.00010257851681672037\n",
            "step: 200, loss: 3.27958186971955e-05\n",
            "step: 210, loss: 4.8828420403879136e-05\n",
            "step: 220, loss: 9.502867760602385e-05\n",
            "step: 230, loss: 9.120386675931513e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9875706214689265, f1=0.9830124575311437, best_f1=0.9819413092550789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.199849743396044e-05\n",
            "step: 10, loss: 2.1829562683706172e-05\n",
            "step: 20, loss: 4.5341712393565103e-05\n",
            "step: 30, loss: 4.117372373002581e-05\n",
            "step: 40, loss: 5.9097168559674174e-05\n",
            "step: 50, loss: 0.05776557698845863\n",
            "step: 60, loss: 4.2954226955771446e-05\n",
            "step: 70, loss: 0.0021005484741181135\n",
            "step: 80, loss: 0.0009846960892900825\n",
            "step: 90, loss: 0.0002371605223743245\n",
            "step: 100, loss: 5.030544343753718e-05\n",
            "step: 110, loss: 4.235721644363366e-05\n",
            "step: 120, loss: 6.512441905215383e-05\n",
            "step: 130, loss: 0.018132973462343216\n",
            "step: 140, loss: 3.27550878864713e-05\n",
            "step: 150, loss: 0.00024897779803723097\n",
            "step: 160, loss: 4.168817395111546e-05\n",
            "step: 170, loss: 8.500654075760394e-05\n",
            "step: 180, loss: 0.0005866176798008382\n",
            "step: 190, loss: 0.0009046823251992464\n",
            "step: 200, loss: 5.7410226872889325e-05\n",
            "step: 210, loss: 3.333642234792933e-05\n",
            "step: 220, loss: 3.152180215693079e-05\n",
            "step: 230, loss: 3.5973102058051154e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9875706214689265, f1=0.9830124575311437, best_f1=0.9819413092550789\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 160.90it/s]\n",
            "load_f1 = 0.9876265466816648\n",
            "real_f1 = 0.9865168539325843\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 183.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dbebd5-4518-4a2a-e966-05adede0f33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6259139180183411\n",
            "step: 10, loss: 0.5429301261901855\n",
            "step: 20, loss: 0.5519333481788635\n",
            "step: 30, loss: 0.2263692170381546\n",
            "step: 40, loss: 0.2085190862417221\n",
            "step: 50, loss: 0.2415197640657425\n",
            "step: 60, loss: 0.09252595156431198\n",
            "step: 70, loss: 0.10910136997699738\n",
            "step: 80, loss: 0.12536096572875977\n",
            "step: 90, loss: 0.4425000250339508\n",
            "step: 100, loss: 0.045878905802965164\n",
            "step: 110, loss: 0.07744130492210388\n",
            "step: 120, loss: 0.11383363604545593\n",
            "step: 130, loss: 0.15734638273715973\n",
            "step: 140, loss: 0.15088671445846558\n",
            "step: 150, loss: 0.057592831552028656\n",
            "step: 160, loss: 0.03142508491873741\n",
            "step: 170, loss: 0.25414931774139404\n",
            "step: 180, loss: 0.020348068326711655\n",
            "step: 190, loss: 0.02034829370677471\n",
            "step: 200, loss: 0.1573685258626938\n",
            "step: 210, loss: 0.08803804218769073\n",
            "step: 220, loss: 0.36695489287376404\n",
            "step: 230, loss: 0.1698952466249466\n",
            "step: 240, loss: 0.06353004276752472\n",
            "step: 250, loss: 0.021902363747358322\n",
            "step: 260, loss: 0.10909101366996765\n",
            "step: 270, loss: 0.005421613808721304\n",
            "step: 280, loss: 0.029140204191207886\n",
            "step: 290, loss: 0.24269813299179077\n",
            "step: 300, loss: 0.08072756975889206\n",
            "step: 310, loss: 0.35138827562332153\n",
            "step: 320, loss: 0.056562453508377075\n",
            "step: 330, loss: 0.1376778930425644\n",
            "step: 340, loss: 0.07603613287210464\n",
            "step: 350, loss: 0.04859049245715141\n",
            "step: 360, loss: 0.03328530862927437\n",
            "step: 370, loss: 0.1221434473991394\n",
            "step: 380, loss: 0.015347711741924286\n",
            "step: 390, loss: 0.20221586525440216\n",
            "step: 400, loss: 0.3347606658935547\n",
            "step: 410, loss: 0.03526876121759415\n",
            "step: 420, loss: 0.03415589779615402\n",
            "step: 430, loss: 0.17614834010601044\n",
            "step: 440, loss: 0.013476529158651829\n",
            "step: 450, loss: 0.009422343224287033\n",
            "step: 460, loss: 0.009144806303083897\n",
            "step: 470, loss: 0.08354010432958603\n",
            "step: 480, loss: 0.041020315140485764\n",
            "step: 490, loss: 0.07238859683275223\n",
            "step: 500, loss: 0.2213204950094223\n",
            "step: 510, loss: 0.04803695157170296\n",
            "step: 520, loss: 0.07377597689628601\n",
            "step: 530, loss: 0.003504390362650156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.922166590805644, f1=0.9253731343283582, best_f1=0.9253731343283582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11322378367185593\n",
            "step: 10, loss: 0.07562016695737839\n",
            "step: 20, loss: 0.04994134604930878\n",
            "step: 30, loss: 0.03877311572432518\n",
            "step: 40, loss: 0.08610009402036667\n",
            "step: 50, loss: 0.2055002599954605\n",
            "step: 60, loss: 0.023130353540182114\n",
            "step: 70, loss: 0.02336384542286396\n",
            "step: 80, loss: 0.05858834832906723\n",
            "step: 90, loss: 0.04307268559932709\n",
            "step: 100, loss: 0.02351577766239643\n",
            "step: 110, loss: 0.05777204409241676\n",
            "step: 120, loss: 0.06714864075183868\n",
            "step: 130, loss: 0.12024373561143875\n",
            "step: 140, loss: 0.05106078088283539\n",
            "step: 150, loss: 0.06278771162033081\n",
            "step: 160, loss: 0.020358623936772346\n",
            "step: 170, loss: 0.04242348298430443\n",
            "step: 180, loss: 0.038977138698101044\n",
            "step: 190, loss: 0.13501864671707153\n",
            "step: 200, loss: 0.01954326033592224\n",
            "step: 210, loss: 0.05230044201016426\n",
            "step: 220, loss: 0.07767936587333679\n",
            "step: 230, loss: 0.009143305942416191\n",
            "step: 240, loss: 0.0353456474840641\n",
            "step: 250, loss: 0.09594839811325073\n",
            "step: 260, loss: 0.0032147967722266912\n",
            "step: 270, loss: 0.24113714694976807\n",
            "step: 280, loss: 0.03043900430202484\n",
            "step: 290, loss: 0.0479779876768589\n",
            "step: 300, loss: 0.12261504679918289\n",
            "step: 310, loss: 0.006869862787425518\n",
            "step: 320, loss: 0.06610658764839172\n",
            "step: 330, loss: 0.015379974618554115\n",
            "step: 340, loss: 0.04359659552574158\n",
            "step: 350, loss: 0.0010628130985423923\n",
            "step: 360, loss: 0.08340112864971161\n",
            "step: 370, loss: 0.18925416469573975\n",
            "step: 380, loss: 0.028822094202041626\n",
            "step: 390, loss: 0.12665413320064545\n",
            "step: 400, loss: 0.05976913869380951\n",
            "step: 410, loss: 0.049646731466054916\n",
            "step: 420, loss: 0.012062128633260727\n",
            "step: 430, loss: 0.014955226331949234\n",
            "step: 440, loss: 0.055414535105228424\n",
            "step: 450, loss: 0.022723300382494926\n",
            "step: 460, loss: 0.032877158373594284\n",
            "step: 470, loss: 0.039479807019233704\n",
            "step: 480, loss: 0.2068672478199005\n",
            "step: 490, loss: 0.01643744297325611\n",
            "step: 500, loss: 0.1612488329410553\n",
            "step: 510, loss: 0.015514673665165901\n",
            "step: 520, loss: 0.04855310544371605\n",
            "step: 530, loss: 0.00866367481648922\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9300373134328358, f1=0.9218604651162791, best_f1=0.9218604651162791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014008976519107819\n",
            "step: 10, loss: 0.014228101819753647\n",
            "step: 20, loss: 0.11687395721673965\n",
            "step: 30, loss: 0.017438432201743126\n",
            "step: 40, loss: 0.018703745678067207\n",
            "step: 50, loss: 0.05675927549600601\n",
            "step: 60, loss: 0.013127898797392845\n",
            "step: 70, loss: 0.002595438389107585\n",
            "step: 80, loss: 0.001638023997657001\n",
            "step: 90, loss: 0.016976656392216682\n",
            "step: 100, loss: 0.07019287347793579\n",
            "step: 110, loss: 0.004538485314697027\n",
            "step: 120, loss: 0.00883584376424551\n",
            "step: 130, loss: 0.004203883931040764\n",
            "step: 140, loss: 0.0375799685716629\n",
            "step: 150, loss: 0.06245533376932144\n",
            "step: 160, loss: 0.03258798271417618\n",
            "step: 170, loss: 0.1295512467622757\n",
            "step: 180, loss: 0.022077038884162903\n",
            "step: 190, loss: 0.001423089997842908\n",
            "step: 200, loss: 0.013720179907977581\n",
            "step: 210, loss: 0.07613172382116318\n",
            "step: 220, loss: 0.05771968886256218\n",
            "step: 230, loss: 0.09643890708684921\n",
            "step: 240, loss: 0.0035572443157434464\n",
            "step: 250, loss: 0.022076735273003578\n",
            "step: 260, loss: 0.01774178259074688\n",
            "step: 270, loss: 0.008652810007333755\n",
            "step: 280, loss: 0.16737355291843414\n",
            "step: 290, loss: 0.0019097456242889166\n",
            "step: 300, loss: 0.020807037129998207\n",
            "step: 310, loss: 0.008133841678500175\n",
            "step: 320, loss: 0.02434352971613407\n",
            "step: 330, loss: 0.002115862211212516\n",
            "step: 340, loss: 0.0048689087852835655\n",
            "step: 350, loss: 0.013193079270422459\n",
            "step: 360, loss: 0.02832004986703396\n",
            "step: 370, loss: 0.0042067524045705795\n",
            "step: 380, loss: 0.01066708192229271\n",
            "step: 390, loss: 0.004934691824018955\n",
            "step: 400, loss: 0.009654135443270206\n",
            "step: 410, loss: 0.0039640069007873535\n",
            "step: 420, loss: 0.16462276875972748\n",
            "step: 430, loss: 0.00950285978615284\n",
            "step: 440, loss: 0.0021667887922376394\n",
            "step: 450, loss: 0.08079519867897034\n",
            "step: 460, loss: 0.012232815846800804\n",
            "step: 470, loss: 0.013028601184487343\n",
            "step: 480, loss: 0.01121152751147747\n",
            "step: 490, loss: 0.005641681607812643\n",
            "step: 500, loss: 0.02716829814016819\n",
            "step: 510, loss: 0.0153029290959239\n",
            "step: 520, loss: 0.18531550467014313\n",
            "step: 530, loss: 0.10548549145460129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9296947271045328, f1=0.912882298424467, best_f1=0.9218604651162791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03593999519944191\n",
            "step: 10, loss: 0.015007133595645428\n",
            "step: 20, loss: 0.0016690518241375685\n",
            "step: 30, loss: 0.012942738831043243\n",
            "step: 40, loss: 0.03770671784877777\n",
            "step: 50, loss: 0.005144147202372551\n",
            "step: 60, loss: 0.03470725938677788\n",
            "step: 70, loss: 0.09062179177999496\n",
            "step: 80, loss: 0.12140249460935593\n",
            "step: 90, loss: 0.08369696140289307\n",
            "step: 100, loss: 0.014008603058755398\n",
            "step: 110, loss: 0.06227251887321472\n",
            "step: 120, loss: 0.003128688782453537\n",
            "step: 130, loss: 0.0013119817012920976\n",
            "step: 140, loss: 0.004930784460157156\n",
            "step: 150, loss: 0.013052236288785934\n",
            "step: 160, loss: 0.0029952065087854862\n",
            "step: 170, loss: 0.017514554783701897\n",
            "step: 180, loss: 0.001692574005573988\n",
            "step: 190, loss: 0.11332236230373383\n",
            "step: 200, loss: 0.001557356328703463\n",
            "step: 210, loss: 0.04353763163089752\n",
            "step: 220, loss: 0.02754310704767704\n",
            "step: 230, loss: 0.029778163880109787\n",
            "step: 240, loss: 0.012866639532148838\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 250, loss: 0.02143000066280365\n",
            "step: 260, loss: 0.0024012948852032423\n",
            "step: 270, loss: 0.006164020858705044\n",
            "step: 280, loss: 0.005797902587801218\n",
            "step: 290, loss: 0.07706480473279953\n",
            "step: 300, loss: 0.025053834542632103\n",
            "step: 310, loss: 0.027207596227526665\n",
            "step: 320, loss: 0.08889579027891159\n",
            "step: 330, loss: 0.022207560017704964\n",
            "step: 340, loss: 0.005076949018985033\n",
            "step: 350, loss: 0.011877452954649925\n",
            "step: 360, loss: 0.056753601878881454\n",
            "step: 370, loss: 0.004360363353043795\n",
            "step: 380, loss: 0.042703233659267426\n",
            "step: 390, loss: 0.0017762985080480576\n",
            "step: 400, loss: 0.002753408858552575\n",
            "step: 410, loss: 0.03506613150238991\n",
            "step: 420, loss: 0.004656198434531689\n",
            "step: 430, loss: 0.0837777853012085\n",
            "step: 440, loss: 0.0014668430667370558\n",
            "step: 450, loss: 0.0013045119121670723\n",
            "step: 460, loss: 0.004161639604717493\n",
            "step: 470, loss: 0.0036744140088558197\n",
            "step: 480, loss: 0.012732578441500664\n",
            "step: 490, loss: 0.006430018227547407\n",
            "step: 500, loss: 0.16251318156719208\n",
            "step: 510, loss: 0.09729927778244019\n",
            "step: 520, loss: 0.08309108763933182\n",
            "step: 530, loss: 0.0035566387232393026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9232941176470588, f1=0.9155261915998112, best_f1=0.9218604651162791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03275645151734352\n",
            "step: 10, loss: 0.14850065112113953\n",
            "step: 20, loss: 0.0034662929829210043\n",
            "step: 30, loss: 0.0020458283834159374\n",
            "step: 40, loss: 0.013448450714349747\n",
            "step: 50, loss: 0.02759713865816593\n",
            "step: 60, loss: 0.09527038037776947\n",
            "step: 70, loss: 0.0066460915841162205\n",
            "step: 80, loss: 0.005780573468655348\n",
            "step: 90, loss: 0.0015320121310651302\n",
            "step: 100, loss: 0.011220293119549751\n",
            "step: 110, loss: 0.0004185680008959025\n",
            "step: 120, loss: 0.04959959164261818\n",
            "step: 130, loss: 0.001428509596735239\n",
            "step: 140, loss: 0.0012334228958934546\n",
            "step: 150, loss: 0.04435708746314049\n",
            "step: 160, loss: 0.00043886867933906615\n",
            "step: 170, loss: 0.021128058433532715\n",
            "step: 180, loss: 0.0043291086331009865\n",
            "step: 190, loss: 0.005187429022043943\n",
            "step: 200, loss: 0.0008620452717877924\n",
            "step: 210, loss: 0.009548197500407696\n",
            "step: 220, loss: 0.0015005712630227208\n",
            "step: 230, loss: 0.002317834412679076\n",
            "step: 240, loss: 0.0016847428632900119\n",
            "step: 250, loss: 0.0003422212030272931\n",
            "step: 260, loss: 0.0012573704589158297\n",
            "step: 270, loss: 0.00914551317691803\n",
            "step: 280, loss: 0.007277489174157381\n",
            "step: 290, loss: 0.15715013444423676\n",
            "step: 300, loss: 0.0019153961911797523\n",
            "step: 310, loss: 0.004313752055168152\n",
            "step: 320, loss: 0.05333560332655907\n",
            "step: 330, loss: 0.004124656319618225\n",
            "step: 340, loss: 0.0035360995680093765\n",
            "step: 350, loss: 0.004136071074754\n",
            "step: 360, loss: 0.011964897625148296\n",
            "step: 370, loss: 0.013600331731140614\n",
            "step: 380, loss: 0.0012085438938811421\n",
            "step: 390, loss: 0.00020390893041621894\n",
            "step: 400, loss: 0.004129997454583645\n",
            "step: 410, loss: 0.002939188852906227\n",
            "step: 420, loss: 0.002509195590391755\n",
            "step: 430, loss: 0.024034107103943825\n",
            "step: 440, loss: 0.014702090062201023\n",
            "step: 450, loss: 0.0019079884514212608\n",
            "step: 460, loss: 0.005943169817328453\n",
            "step: 470, loss: 0.004094367381185293\n",
            "step: 480, loss: 0.08388858288526535\n",
            "step: 490, loss: 0.0019085491076111794\n",
            "step: 500, loss: 0.004026051610708237\n",
            "step: 510, loss: 0.11381292343139648\n",
            "step: 520, loss: 0.0006899121799506247\n",
            "step: 530, loss: 0.003952185623347759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9112709832134291, f1=0.9012464046021093, best_f1=0.9218604651162791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009717652574181557\n",
            "step: 10, loss: 0.0006255880580283701\n",
            "step: 20, loss: 0.09690696746110916\n",
            "step: 30, loss: 0.0005412728060036898\n",
            "step: 40, loss: 0.006090562324970961\n",
            "step: 50, loss: 0.008341302163898945\n",
            "step: 60, loss: 0.0007399183814413846\n",
            "step: 70, loss: 0.0010859956964850426\n",
            "step: 80, loss: 0.0011331363348290324\n",
            "step: 90, loss: 0.000214208266697824\n",
            "step: 100, loss: 0.0012064214097335935\n",
            "step: 110, loss: 0.0002619070000946522\n",
            "step: 120, loss: 0.0022474525030702353\n",
            "step: 130, loss: 0.00027654547011479735\n",
            "step: 140, loss: 0.002372988034039736\n",
            "step: 150, loss: 0.01943478174507618\n",
            "step: 160, loss: 0.004082463216036558\n",
            "step: 170, loss: 0.0001357924338662997\n",
            "step: 180, loss: 0.003515472635626793\n",
            "step: 190, loss: 0.02279866859316826\n",
            "step: 200, loss: 0.0005285681108944118\n",
            "step: 210, loss: 0.13318997621536255\n",
            "step: 220, loss: 0.0027345577254891396\n",
            "step: 230, loss: 0.012179802171885967\n",
            "step: 240, loss: 0.030613616108894348\n",
            "step: 250, loss: 0.003968267235904932\n",
            "step: 260, loss: 0.0006256437627598643\n",
            "step: 270, loss: 0.024270372465252876\n",
            "step: 280, loss: 0.0015473227249458432\n",
            "step: 290, loss: 0.00016761515871621668\n",
            "step: 300, loss: 0.0031451350077986717\n",
            "step: 310, loss: 0.007111575920134783\n",
            "step: 320, loss: 0.0009512068936601281\n",
            "step: 330, loss: 0.004690965637564659\n",
            "step: 340, loss: 0.08184810727834702\n",
            "step: 350, loss: 0.00113730039447546\n",
            "step: 360, loss: 0.04919832572340965\n",
            "step: 370, loss: 0.0028224620036780834\n",
            "step: 380, loss: 0.00034310147748328745\n",
            "step: 390, loss: 0.01899496465921402\n",
            "step: 400, loss: 0.002446339698508382\n",
            "step: 410, loss: 0.006222357973456383\n",
            "step: 420, loss: 0.007608650717884302\n",
            "step: 430, loss: 0.0026178492698818445\n",
            "step: 440, loss: 8.382948726648465e-05\n",
            "step: 450, loss: 0.0001219103651237674\n",
            "step: 460, loss: 7.160489622037858e-05\n",
            "step: 470, loss: 0.0006084393244236708\n",
            "step: 480, loss: 0.070460245013237\n",
            "step: 490, loss: 0.013174567371606827\n",
            "step: 500, loss: 0.003753578057512641\n",
            "step: 510, loss: 0.002600171137601137\n",
            "step: 520, loss: 0.0010049634147435427\n",
            "step: 530, loss: 0.1311797797679901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9348623853211009, f1=0.9278256922378576, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029702650499530137\n",
            "step: 10, loss: 0.0031468886882066727\n",
            "step: 20, loss: 0.00042638974264264107\n",
            "step: 30, loss: 0.00805203802883625\n",
            "step: 40, loss: 0.019255202263593674\n",
            "step: 50, loss: 0.00030918308766558766\n",
            "step: 60, loss: 0.007502499967813492\n",
            "step: 70, loss: 0.000845936534460634\n",
            "step: 80, loss: 0.0009930958040058613\n",
            "step: 90, loss: 0.002477996051311493\n",
            "step: 100, loss: 7.53315252950415e-05\n",
            "step: 110, loss: 0.00026377104222774506\n",
            "step: 120, loss: 0.0006676369230262935\n",
            "step: 130, loss: 0.011751458048820496\n",
            "step: 140, loss: 6.240420771064237e-05\n",
            "step: 150, loss: 0.0005458152736537158\n",
            "step: 160, loss: 0.0004928645212203264\n",
            "step: 170, loss: 0.0009287514840252697\n",
            "step: 180, loss: 0.0006313444464467466\n",
            "step: 190, loss: 0.0008152038208208978\n",
            "step: 200, loss: 8.769274427322671e-05\n",
            "step: 210, loss: 0.00010922650835709646\n",
            "step: 220, loss: 7.711737271165475e-05\n",
            "step: 230, loss: 0.09495517611503601\n",
            "step: 240, loss: 0.0008268069941550493\n",
            "step: 250, loss: 0.0005990591598674655\n",
            "step: 260, loss: 0.00026893592439591885\n",
            "step: 270, loss: 0.0004867773677688092\n",
            "step: 280, loss: 0.00029208199703134596\n",
            "step: 290, loss: 0.0008317812462337315\n",
            "step: 300, loss: 0.018860487267374992\n",
            "step: 310, loss: 0.00017523285350762308\n",
            "step: 320, loss: 0.0002883501583710313\n",
            "step: 330, loss: 0.001998076681047678\n",
            "step: 340, loss: 0.0036317245103418827\n",
            "step: 350, loss: 0.0007358689908869565\n",
            "step: 360, loss: 0.002224421827122569\n",
            "step: 370, loss: 0.0006291468744166195\n",
            "step: 380, loss: 0.0012155374279245734\n",
            "step: 390, loss: 0.001196654629893601\n",
            "step: 400, loss: 0.003173787146806717\n",
            "step: 410, loss: 0.0021389981266111135\n",
            "step: 420, loss: 0.006606516428291798\n",
            "step: 430, loss: 0.00017134027439169586\n",
            "step: 440, loss: 0.0007808374939486384\n",
            "step: 450, loss: 0.0009281456004828215\n",
            "step: 460, loss: 0.0007187701994553208\n",
            "step: 470, loss: 0.0021278050262480974\n",
            "step: 480, loss: 0.12227635830640793\n",
            "step: 490, loss: 0.0009297503856942058\n",
            "step: 500, loss: 0.0003088205121457577\n",
            "step: 510, loss: 0.0011131507344543934\n",
            "step: 520, loss: 0.0007617485825903714\n",
            "step: 530, loss: 0.02731630951166153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9252767527675276, f1=0.9227230910763569, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021567470394074917\n",
            "step: 10, loss: 0.0325743593275547\n",
            "step: 20, loss: 0.0003908572834916413\n",
            "step: 30, loss: 0.12153588980436325\n",
            "step: 40, loss: 0.00010392047261120752\n",
            "step: 50, loss: 0.006427675951272249\n",
            "step: 60, loss: 0.04063229262828827\n",
            "step: 70, loss: 0.0003957074659410864\n",
            "step: 80, loss: 0.014473183080554008\n",
            "step: 90, loss: 0.0009001547005027533\n",
            "step: 100, loss: 0.00038270457298494875\n",
            "step: 110, loss: 0.00022471662668976933\n",
            "step: 120, loss: 0.0003132352139800787\n",
            "step: 130, loss: 0.00012956908904016018\n",
            "step: 140, loss: 0.004699037875980139\n",
            "step: 150, loss: 0.010922005400061607\n",
            "step: 160, loss: 0.0314205177128315\n",
            "step: 170, loss: 0.0004045121022500098\n",
            "step: 180, loss: 0.0001597420050529763\n",
            "step: 190, loss: 0.0017349711852148175\n",
            "step: 200, loss: 0.0011247514048591256\n",
            "step: 210, loss: 0.002519201021641493\n",
            "step: 220, loss: 0.002089668298140168\n",
            "step: 230, loss: 0.00014393069432117045\n",
            "step: 240, loss: 0.002849096432328224\n",
            "step: 250, loss: 0.0010960143990814686\n",
            "step: 260, loss: 0.0011174783576279879\n",
            "step: 270, loss: 0.18505483865737915\n",
            "step: 280, loss: 0.09660976380109787\n",
            "step: 290, loss: 0.008944257162511349\n",
            "step: 300, loss: 0.0007000002660788596\n",
            "step: 310, loss: 0.0015785802388563752\n",
            "step: 320, loss: 0.006594059988856316\n",
            "step: 330, loss: 0.00031924102222546935\n",
            "step: 340, loss: 0.0035907926503568888\n",
            "step: 350, loss: 0.00020890971063636243\n",
            "step: 360, loss: 0.00015910038200672716\n",
            "step: 370, loss: 0.02411090023815632\n",
            "step: 380, loss: 0.07510209828615189\n",
            "step: 390, loss: 0.10067986696958542\n",
            "step: 400, loss: 0.0010486847022548318\n",
            "step: 410, loss: 0.00010114223550772294\n",
            "step: 420, loss: 0.0019670359324663877\n",
            "step: 430, loss: 0.03044169209897518\n",
            "step: 440, loss: 0.029670586809515953\n",
            "step: 450, loss: 0.005527024623006582\n",
            "step: 460, loss: 4.961053855367936e-05\n",
            "step: 470, loss: 0.0003416603140067309\n",
            "step: 480, loss: 7.052377623040229e-05\n",
            "step: 490, loss: 0.0003253297763876617\n",
            "step: 500, loss: 0.0004679708508774638\n",
            "step: 510, loss: 0.004676567390561104\n",
            "step: 520, loss: 0.00397426076233387\n",
            "step: 530, loss: 3.443885361775756e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9237918215613383, f1=0.9068150208623087, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010518726194277406\n",
            "step: 10, loss: 0.0004215231165289879\n",
            "step: 20, loss: 0.0011014258489012718\n",
            "step: 30, loss: 0.00020159172709099948\n",
            "step: 40, loss: 4.903268563793972e-05\n",
            "step: 50, loss: 7.94461157056503e-05\n",
            "step: 60, loss: 3.9262584323296323e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.003044678131118417\n",
            "step: 80, loss: 0.00028899224707856774\n",
            "step: 90, loss: 8.15197272459045e-05\n",
            "step: 100, loss: 3.188003756804392e-05\n",
            "step: 110, loss: 0.0006740832468494773\n",
            "step: 120, loss: 0.00037787805194966495\n",
            "step: 130, loss: 9.256205521523952e-05\n",
            "step: 140, loss: 3.673361425171606e-05\n",
            "step: 150, loss: 6.111735274316743e-05\n",
            "step: 160, loss: 0.002722078002989292\n",
            "step: 170, loss: 0.005953626707196236\n",
            "step: 180, loss: 2.8165943149360828e-05\n",
            "step: 190, loss: 4.217287278152071e-05\n",
            "step: 200, loss: 0.00018328483565710485\n",
            "step: 210, loss: 3.6404599086381495e-05\n",
            "step: 220, loss: 0.0028994856402277946\n",
            "step: 230, loss: 4.230867853038944e-05\n",
            "step: 240, loss: 8.259883179562166e-05\n",
            "step: 250, loss: 6.376235978677869e-05\n",
            "step: 260, loss: 0.04552552476525307\n",
            "step: 270, loss: 2.204218617407605e-05\n",
            "step: 280, loss: 3.338500391691923e-05\n",
            "step: 290, loss: 0.058656591922044754\n",
            "step: 300, loss: 3.866892075166106e-05\n",
            "step: 310, loss: 0.012051098980009556\n",
            "step: 320, loss: 0.03243112564086914\n",
            "step: 330, loss: 0.0002309717092430219\n",
            "step: 340, loss: 6.27697809250094e-05\n",
            "step: 350, loss: 0.0027693151496350765\n",
            "step: 360, loss: 2.7611231416813098e-05\n",
            "step: 370, loss: 0.00047974183689802885\n",
            "step: 380, loss: 0.0003628809645306319\n",
            "step: 390, loss: 0.003485799068585038\n",
            "step: 400, loss: 0.0010947661940008402\n",
            "step: 410, loss: 0.0029835153836756945\n",
            "step: 420, loss: 6.733314512530342e-05\n",
            "step: 430, loss: 0.0023202192969620228\n",
            "step: 440, loss: 0.0044007194228470325\n",
            "step: 450, loss: 0.0002662469632923603\n",
            "step: 460, loss: 0.00016189305461011827\n",
            "step: 470, loss: 0.00015188861289061606\n",
            "step: 480, loss: 5.715875158784911e-05\n",
            "step: 490, loss: 0.0013444741489365697\n",
            "step: 500, loss: 0.0015115300193428993\n",
            "step: 510, loss: 0.016797633841633797\n",
            "step: 520, loss: 0.0011844616383314133\n",
            "step: 530, loss: 5.134392631589435e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9325210871602625, f1=0.9282700421940927, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.953997894423082e-05\n",
            "step: 10, loss: 5.516310557140969e-05\n",
            "step: 20, loss: 2.7067086193710566e-05\n",
            "step: 30, loss: 4.673732109949924e-05\n",
            "step: 40, loss: 6.644726090598851e-05\n",
            "step: 50, loss: 9.025391045724973e-05\n",
            "step: 60, loss: 7.321356679312885e-05\n",
            "step: 70, loss: 0.0066113220527768135\n",
            "step: 80, loss: 5.1788578275591135e-05\n",
            "step: 90, loss: 0.00014795952301938087\n",
            "step: 100, loss: 0.0005385371041484177\n",
            "step: 110, loss: 2.6314826754969545e-05\n",
            "step: 120, loss: 0.037015125155448914\n",
            "step: 130, loss: 3.522102633723989e-05\n",
            "step: 140, loss: 0.00011627024650806561\n",
            "step: 150, loss: 5.334442175808363e-05\n",
            "step: 160, loss: 5.938001049798913e-05\n",
            "step: 170, loss: 4.180668111075647e-05\n",
            "step: 180, loss: 4.136320058023557e-05\n",
            "step: 190, loss: 0.00010666754678823054\n",
            "step: 200, loss: 0.0031523555517196655\n",
            "step: 210, loss: 0.00018304250261280686\n",
            "step: 220, loss: 0.00033248536055907607\n",
            "step: 230, loss: 0.0035157182719558477\n",
            "step: 240, loss: 7.717379776295274e-05\n",
            "step: 250, loss: 4.5187800424173474e-05\n",
            "step: 260, loss: 0.0019404874183237553\n",
            "step: 270, loss: 0.000981162185780704\n",
            "step: 280, loss: 0.00012357922969385982\n",
            "step: 290, loss: 0.00019831440295092762\n",
            "step: 300, loss: 2.401644815108739e-05\n",
            "step: 310, loss: 3.506418943288736e-05\n",
            "step: 320, loss: 0.0014711221447214484\n",
            "step: 330, loss: 0.00010687114991014823\n",
            "step: 340, loss: 0.0001610025210538879\n",
            "step: 350, loss: 0.00020606584439519793\n",
            "step: 360, loss: 0.0006825198652222753\n",
            "step: 370, loss: 0.0003624722012318671\n",
            "step: 380, loss: 0.0008317400352098048\n",
            "step: 390, loss: 0.0002059179823845625\n",
            "step: 400, loss: 0.06050065532326698\n",
            "step: 410, loss: 0.0005206117639318109\n",
            "step: 420, loss: 0.20837639272212982\n",
            "step: 430, loss: 3.551000190782361e-05\n",
            "step: 440, loss: 0.006998415570706129\n",
            "step: 450, loss: 6.10796341788955e-05\n",
            "step: 460, loss: 0.0017823355738073587\n",
            "step: 470, loss: 0.00019142328528687358\n",
            "step: 480, loss: 5.7366218243259937e-05\n",
            "step: 490, loss: 0.0012123020133003592\n",
            "step: 500, loss: 0.0037286588922142982\n",
            "step: 510, loss: 2.9536851798184216e-05\n",
            "step: 520, loss: 0.00010933510202448815\n",
            "step: 530, loss: 0.00042793998727574944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.930276087973795, f1=0.9185882352941176, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019798825087491423\n",
            "step: 10, loss: 0.000497575500048697\n",
            "step: 20, loss: 0.00011290224210824817\n",
            "step: 30, loss: 5.0497612392064184e-05\n",
            "step: 40, loss: 8.503944263793528e-05\n",
            "step: 50, loss: 0.00013377676077652723\n",
            "step: 60, loss: 0.038123685866594315\n",
            "step: 70, loss: 2.4213823053287342e-05\n",
            "step: 80, loss: 3.296724389656447e-05\n",
            "step: 90, loss: 3.4430588129907846e-05\n",
            "step: 100, loss: 0.0106945326551795\n",
            "step: 110, loss: 8.502614946337417e-05\n",
            "step: 120, loss: 0.00016156914352905005\n",
            "step: 130, loss: 0.0019384657498449087\n",
            "step: 140, loss: 0.0005384988035075366\n",
            "step: 150, loss: 1.3980839867144823e-05\n",
            "step: 160, loss: 0.00014076569641474634\n",
            "step: 170, loss: 2.908230817411095e-05\n",
            "step: 180, loss: 5.073121064924635e-05\n",
            "step: 190, loss: 0.0028150649741292\n",
            "step: 200, loss: 0.006271859630942345\n",
            "step: 210, loss: 0.0001491977454861626\n",
            "step: 220, loss: 0.0009648512350395322\n",
            "step: 230, loss: 7.799485319992527e-05\n",
            "step: 240, loss: 0.00039094293606467545\n",
            "step: 250, loss: 2.1591253243968822e-05\n",
            "step: 260, loss: 2.1911677322350442e-05\n",
            "step: 270, loss: 0.0001110882731154561\n",
            "step: 280, loss: 9.582141501596197e-05\n",
            "step: 290, loss: 2.0753101125592366e-05\n",
            "step: 300, loss: 8.176191477105021e-05\n",
            "step: 310, loss: 0.00020126021991018206\n",
            "step: 320, loss: 0.00010498863412067294\n",
            "step: 330, loss: 0.0003830725618172437\n",
            "step: 340, loss: 0.0004551328020170331\n",
            "step: 350, loss: 0.0003259831282775849\n",
            "step: 360, loss: 9.633307490730658e-05\n",
            "step: 370, loss: 0.012995137833058834\n",
            "step: 380, loss: 0.0004256577230989933\n",
            "step: 390, loss: 0.0002032285847235471\n",
            "step: 400, loss: 0.0002917663659900427\n",
            "step: 410, loss: 5.5115062423283234e-05\n",
            "step: 420, loss: 0.0030569378286600113\n",
            "step: 430, loss: 3.6503806768450886e-05\n",
            "step: 440, loss: 0.0005455637583509088\n",
            "step: 450, loss: 0.00020357989706099033\n",
            "step: 460, loss: 0.00013785772898700088\n",
            "step: 470, loss: 6.215825851541013e-05\n",
            "step: 480, loss: 0.0009581110207363963\n",
            "step: 490, loss: 5.574045280809514e-05\n",
            "step: 500, loss: 0.002352561568841338\n",
            "step: 510, loss: 0.0003878124407492578\n",
            "step: 520, loss: 4.4074673496652395e-05\n",
            "step: 530, loss: 0.00040188629645854235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9295904279797514, f1=0.9220055710306406, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017259805463254452\n",
            "step: 10, loss: 2.2001144316163845e-05\n",
            "step: 20, loss: 0.00012793521455023438\n",
            "step: 30, loss: 5.604374746326357e-05\n",
            "step: 40, loss: 0.0001089470533770509\n",
            "step: 50, loss: 0.019839158281683922\n",
            "step: 60, loss: 0.002913211937993765\n",
            "step: 70, loss: 7.374497363343835e-05\n",
            "step: 80, loss: 0.0003047978680115193\n",
            "step: 90, loss: 5.3755156841361895e-05\n",
            "step: 100, loss: 0.00015990981773938984\n",
            "step: 110, loss: 0.0016918821493163705\n",
            "step: 120, loss: 1.5388974134111777e-05\n",
            "step: 130, loss: 0.00010909508273471147\n",
            "step: 140, loss: 2.6354833607911132e-05\n",
            "step: 150, loss: 3.578404721338302e-05\n",
            "step: 160, loss: 7.0178386522457e-05\n",
            "step: 170, loss: 0.013829309493303299\n",
            "step: 180, loss: 0.00025598282809369266\n",
            "step: 190, loss: 5.471969780046493e-05\n",
            "step: 200, loss: 3.2069827284431085e-05\n",
            "step: 210, loss: 5.825484186061658e-05\n",
            "step: 220, loss: 7.138562068575993e-05\n",
            "step: 230, loss: 1.5735397028038278e-05\n",
            "step: 240, loss: 0.0005922638811171055\n",
            "step: 250, loss: 2.0991594283259474e-05\n",
            "step: 260, loss: 1.742294625728391e-05\n",
            "step: 270, loss: 6.011364530422725e-05\n",
            "step: 280, loss: 0.0007435836014337838\n",
            "step: 290, loss: 2.7953630706178956e-05\n",
            "step: 300, loss: 0.0004296534461900592\n",
            "step: 310, loss: 2.45488863583887e-05\n",
            "step: 320, loss: 1.8629856640473008e-05\n",
            "step: 330, loss: 7.879081385908648e-05\n",
            "step: 340, loss: 3.287247091066092e-05\n",
            "step: 350, loss: 3.950058453483507e-05\n",
            "step: 360, loss: 5.872672772966325e-05\n",
            "step: 370, loss: 0.0005725902155973017\n",
            "step: 380, loss: 2.9074790290906094e-05\n",
            "step: 390, loss: 2.7711468646884896e-05\n",
            "step: 400, loss: 0.0001516190532129258\n",
            "step: 410, loss: 0.00019252007768955082\n",
            "step: 420, loss: 0.0011221329914405942\n",
            "step: 430, loss: 2.1989924789522775e-05\n",
            "step: 440, loss: 3.642441879492253e-05\n",
            "step: 450, loss: 0.0023591527715325356\n",
            "step: 460, loss: 1.3813236364512704e-05\n",
            "step: 470, loss: 1.7631484297453426e-05\n",
            "step: 480, loss: 0.06006309762597084\n",
            "step: 490, loss: 0.00037615405744872987\n",
            "step: 500, loss: 0.0007923663943074644\n",
            "step: 510, loss: 0.0006404114537872374\n",
            "step: 520, loss: 0.00014365841343533248\n",
            "step: 530, loss: 3.097115404671058e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9306569343065694, f1=0.9191919191919192, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038992136251181364\n",
            "step: 10, loss: 6.575549195986241e-05\n",
            "step: 20, loss: 0.00032533545163460076\n",
            "step: 30, loss: 9.603805665392429e-05\n",
            "step: 40, loss: 1.910670289362315e-05\n",
            "step: 50, loss: 0.0004989627050235868\n",
            "step: 60, loss: 0.0001712052326183766\n",
            "step: 70, loss: 3.462704989942722e-05\n",
            "step: 80, loss: 0.00011092858039774\n",
            "step: 90, loss: 0.0003025145852006972\n",
            "step: 100, loss: 0.0006104277563281357\n",
            "step: 110, loss: 5.740196502301842e-05\n",
            "step: 120, loss: 0.004268335644155741\n",
            "step: 130, loss: 2.2917369278729893e-05\n",
            "step: 140, loss: 8.703325875103474e-05\n",
            "step: 150, loss: 0.0003484599874354899\n",
            "step: 160, loss: 3.3379008527845144e-05\n",
            "step: 170, loss: 0.00031466654036194086\n",
            "step: 180, loss: 3.3938922570087016e-05\n",
            "step: 190, loss: 0.0012479603756219149\n",
            "step: 200, loss: 7.65460790717043e-05\n",
            "step: 210, loss: 4.048154369229451e-05\n",
            "step: 220, loss: 1.4580617971660104e-05\n",
            "step: 230, loss: 0.0010647507151588798\n",
            "step: 240, loss: 0.00012575041910167783\n",
            "step: 250, loss: 1.4860031114949379e-05\n",
            "step: 260, loss: 1.548584805277642e-05\n",
            "step: 270, loss: 1.381693255098071e-05\n",
            "step: 280, loss: 0.0002099053090205416\n",
            "step: 290, loss: 0.0004025642410852015\n",
            "step: 300, loss: 1.8443633962306194e-05\n",
            "step: 310, loss: 0.0021895423997193575\n",
            "step: 320, loss: 0.0067748394794762135\n",
            "step: 330, loss: 0.0015096277929842472\n",
            "step: 340, loss: 2.9665487090824172e-05\n",
            "step: 350, loss: 3.0149181839078665e-05\n",
            "step: 360, loss: 4.2676641896832734e-05\n",
            "step: 370, loss: 0.00015289433940779418\n",
            "step: 380, loss: 1.687530311755836e-05\n",
            "step: 390, loss: 0.00011453829938545823\n",
            "step: 400, loss: 9.149884135695174e-05\n",
            "step: 410, loss: 0.0003411297802813351\n",
            "step: 420, loss: 1.6867836166056804e-05\n",
            "step: 430, loss: 7.16036229277961e-05\n",
            "step: 440, loss: 5.916254667681642e-05\n",
            "step: 450, loss: 9.083474287763238e-05\n",
            "step: 460, loss: 0.0006909787771292031\n",
            "step: 470, loss: 0.00014346376701723784\n",
            "step: 480, loss: 9.374086221214384e-05\n",
            "step: 490, loss: 0.00011633334361249581\n",
            "step: 500, loss: 1.1473794984340202e-05\n",
            "step: 510, loss: 3.402333095436916e-05\n",
            "step: 520, loss: 0.004052670672535896\n",
            "step: 530, loss: 1.3094268979330081e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9197707736389685, f1=0.9108433734939759, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002784297685138881\n",
            "step: 10, loss: 1.1082654964411631e-05\n",
            "step: 20, loss: 1.6726133253541775e-05\n",
            "step: 30, loss: 3.476894198684022e-05\n",
            "step: 40, loss: 2.727501305344049e-05\n",
            "step: 50, loss: 4.884713780484162e-05\n",
            "step: 60, loss: 1.1533405995578505e-05\n",
            "step: 70, loss: 0.004368986003100872\n",
            "step: 80, loss: 2.946895983768627e-05\n",
            "step: 90, loss: 1.878251350717619e-05\n",
            "step: 100, loss: 0.0003913629916496575\n",
            "step: 110, loss: 2.2626658392255194e-05\n",
            "step: 120, loss: 1.5139382412598934e-05\n",
            "step: 130, loss: 0.00019934881129302084\n",
            "step: 140, loss: 3.9127011405071244e-05\n",
            "step: 150, loss: 1.27180001072702e-05\n",
            "step: 160, loss: 8.767870895098895e-05\n",
            "step: 170, loss: 1.9050681657972746e-05\n",
            "step: 180, loss: 2.8138227207819e-05\n",
            "step: 190, loss: 6.162712816148996e-05\n",
            "step: 200, loss: 0.008522801101207733\n",
            "step: 210, loss: 1.5564066416118294e-05\n",
            "step: 220, loss: 1.543366306577809e-05\n",
            "step: 230, loss: 3.0277384212240577e-05\n",
            "step: 240, loss: 3.1778556149220094e-05\n",
            "step: 250, loss: 1.8387738236924633e-05\n",
            "step: 260, loss: 0.00011496499791974202\n",
            "step: 270, loss: 7.537949568359181e-05\n",
            "step: 280, loss: 1.4979223124100827e-05\n",
            "step: 290, loss: 5.538707773666829e-05\n",
            "step: 300, loss: 3.445563925197348e-05\n",
            "step: 310, loss: 1.912532388814725e-05\n",
            "step: 320, loss: 0.001325904275290668\n",
            "step: 330, loss: 3.885431942762807e-05\n",
            "step: 340, loss: 2.2224501663004048e-05\n",
            "step: 350, loss: 2.4220671548391692e-05\n",
            "step: 360, loss: 0.0016949607525020838\n",
            "step: 370, loss: 4.504420212469995e-05\n",
            "step: 380, loss: 0.00039932687650434673\n",
            "step: 390, loss: 0.01378073263913393\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 0.00028342296718619764\n",
            "step: 410, loss: 2.2823789549875073e-05\n",
            "step: 420, loss: 0.0002697133750189096\n",
            "step: 430, loss: 2.539728484407533e-05\n",
            "step: 440, loss: 3.030016341654118e-05\n",
            "step: 450, loss: 0.0002515902160666883\n",
            "step: 460, loss: 4.651767449104227e-05\n",
            "step: 470, loss: 0.0010555799817666411\n",
            "step: 480, loss: 1.6841791875776835e-05\n",
            "step: 490, loss: 2.1594481950160116e-05\n",
            "step: 500, loss: 1.3328954082680866e-05\n",
            "step: 510, loss: 0.0007344963378272951\n",
            "step: 520, loss: 7.102434028638527e-05\n",
            "step: 530, loss: 4.8899197281571105e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9296413980935089, f1=0.920562868815252, best_f1=0.9278256922378576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.5230412575183436e-05\n",
            "step: 10, loss: 2.6357338356319815e-05\n",
            "step: 20, loss: 9.680917719379067e-05\n",
            "step: 30, loss: 0.004280685912817717\n",
            "step: 40, loss: 0.07986417412757874\n",
            "step: 50, loss: 1.726274422253482e-05\n",
            "step: 60, loss: 2.4352761101908982e-05\n",
            "step: 70, loss: 0.0007425005314871669\n",
            "step: 80, loss: 1.9777127818088047e-05\n",
            "step: 90, loss: 1.9654218704090454e-05\n",
            "step: 100, loss: 6.840448622824624e-05\n",
            "step: 110, loss: 6.459037831518799e-05\n",
            "step: 120, loss: 0.005219500977545977\n",
            "step: 130, loss: 0.0021991031244397163\n",
            "step: 140, loss: 1.6186158973141573e-05\n",
            "step: 150, loss: 1.6260695701930672e-05\n",
            "step: 160, loss: 5.457469160319306e-05\n",
            "step: 170, loss: 3.5270819353172556e-05\n",
            "step: 180, loss: 1.3444415344565641e-05\n",
            "step: 190, loss: 1.7091433619498275e-05\n",
            "step: 200, loss: 3.832177753793076e-05\n",
            "step: 210, loss: 9.147867240244523e-05\n",
            "step: 220, loss: 1.287448230868904e-05\n",
            "step: 230, loss: 1.4480055142485071e-05\n",
            "step: 240, loss: 4.3180392822250724e-05\n",
            "step: 250, loss: 2.169928848161362e-05\n",
            "step: 260, loss: 0.00020952941849827766\n",
            "step: 270, loss: 1.3656783266924322e-05\n",
            "step: 280, loss: 2.241042056994047e-05\n",
            "step: 290, loss: 1.4774324881727807e-05\n",
            "step: 300, loss: 1.6998121282085776e-05\n",
            "step: 310, loss: 3.171491334796883e-05\n",
            "step: 320, loss: 5.151388904778287e-05\n",
            "step: 330, loss: 2.094288902299013e-05\n",
            "step: 340, loss: 2.043182212219108e-05\n",
            "step: 350, loss: 1.7902815670822747e-05\n",
            "step: 360, loss: 0.0019007368246093392\n",
            "step: 370, loss: 2.3533852072432637e-05\n",
            "step: 380, loss: 1.4815322174399626e-05\n",
            "step: 390, loss: 2.4179929823731072e-05\n",
            "step: 400, loss: 1.8152963093598373e-05\n",
            "step: 410, loss: 1.733340286591556e-05\n",
            "step: 420, loss: 2.8999387723160908e-05\n",
            "step: 430, loss: 3.46120104950387e-05\n",
            "step: 440, loss: 2.802681592584122e-05\n",
            "step: 450, loss: 9.258893987862393e-05\n",
            "step: 460, loss: 2.083113940898329e-05\n",
            "step: 470, loss: 0.0013442394556477666\n",
            "step: 480, loss: 1.1365776117600035e-05\n",
            "step: 490, loss: 1.2766455256496556e-05\n",
            "step: 500, loss: 1.8488050045561977e-05\n",
            "step: 510, loss: 5.6456003221683204e-05\n",
            "step: 520, loss: 1.3247007700556424e-05\n",
            "step: 530, loss: 1.6260657503153197e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9291994447015272, f1=0.919944470152707, best_f1=0.9278256922378576\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 192.83it/s]\n",
            "load_f1 = 0.9348025711662074\n",
            "real_f1 = 0.9337626494940202\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6ac570-0113-457c-ba49-85eb1f83a983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5425687432289124\n",
            "step: 10, loss: 0.36731481552124023\n",
            "step: 20, loss: 0.36463871598243713\n",
            "step: 30, loss: 0.3450835049152374\n",
            "step: 40, loss: 0.1637304127216339\n",
            "step: 50, loss: 0.4170503616333008\n",
            "step: 60, loss: 0.2843277156352997\n",
            "step: 70, loss: 0.18189765512943268\n",
            "step: 80, loss: 0.18742412328720093\n",
            "step: 90, loss: 0.3964230418205261\n",
            "step: 100, loss: 0.34028610587120056\n",
            "step: 110, loss: 0.21414773166179657\n",
            "step: 120, loss: 0.20328186452388763\n",
            "step: 130, loss: 0.1896314173936844\n",
            "step: 140, loss: 0.19417986273765564\n",
            "step: 150, loss: 0.18473398685455322\n",
            "step: 160, loss: 0.34342649579048157\n",
            "step: 170, loss: 0.2860686779022217\n",
            "step: 180, loss: 0.14511999487876892\n",
            "step: 190, loss: 0.2608819901943207\n",
            "step: 200, loss: 0.2991541624069214\n",
            "step: 210, loss: 0.25575709342956543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.564755838641189, f1=0.6171171171171171, best_f1=0.6171171171171171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19796344637870789\n",
            "step: 10, loss: 0.23283670842647552\n",
            "step: 20, loss: 0.1668589860200882\n",
            "step: 30, loss: 0.14008890092372894\n",
            "step: 40, loss: 0.2721114754676819\n",
            "step: 50, loss: 0.07193109393119812\n",
            "step: 60, loss: 0.3217650055885315\n",
            "step: 70, loss: 0.09714290499687195\n",
            "step: 80, loss: 0.254610538482666\n",
            "step: 90, loss: 0.10286614298820496\n",
            "step: 100, loss: 0.009485863149166107\n",
            "step: 110, loss: 0.1453656554222107\n",
            "step: 120, loss: 0.1422993689775467\n",
            "step: 130, loss: 0.06980980187654495\n",
            "step: 140, loss: 0.3103395104408264\n",
            "step: 150, loss: 0.2598874270915985\n",
            "step: 160, loss: 0.20402014255523682\n",
            "step: 170, loss: 0.09425190091133118\n",
            "step: 180, loss: 0.24440698325634003\n",
            "step: 190, loss: 0.20936639606952667\n",
            "step: 200, loss: 0.04158642515540123\n",
            "step: 210, loss: 0.15540482103824615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.605427974947808, f1=0.6311300639658849, best_f1=0.6311300639658849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.049943145364522934\n",
            "step: 10, loss: 0.10010017454624176\n",
            "step: 20, loss: 0.189909428358078\n",
            "step: 30, loss: 0.18689729273319244\n",
            "step: 40, loss: 0.07045457512140274\n",
            "step: 50, loss: 0.08209437876939774\n",
            "step: 60, loss: 0.2106439173221588\n",
            "step: 70, loss: 0.05555292218923569\n",
            "step: 80, loss: 0.2028937041759491\n",
            "step: 90, loss: 0.05825386568903923\n",
            "step: 100, loss: 0.13041092455387115\n",
            "step: 110, loss: 0.16435886919498444\n",
            "step: 120, loss: 0.1538999378681183\n",
            "step: 130, loss: 0.2184806615114212\n",
            "step: 140, loss: 0.130941703915596\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.19806480407714844\n",
            "step: 160, loss: 0.012422798201441765\n",
            "step: 170, loss: 0.12907518446445465\n",
            "step: 180, loss: 0.06325237452983856\n",
            "step: 190, loss: 0.22762931883335114\n",
            "step: 200, loss: 0.049790892750024796\n",
            "step: 210, loss: 0.24946291744709015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6197183098591549, f1=0.6452905811623246, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08901810646057129\n",
            "step: 10, loss: 0.05527091026306152\n",
            "step: 20, loss: 0.05913827195763588\n",
            "step: 30, loss: 0.1267441213130951\n",
            "step: 40, loss: 0.02113049291074276\n",
            "step: 50, loss: 0.20276795327663422\n",
            "step: 60, loss: 0.07627850025892258\n",
            "step: 70, loss: 0.23380520939826965\n",
            "step: 80, loss: 0.03699542582035065\n",
            "step: 90, loss: 0.008605497889220715\n",
            "step: 100, loss: 0.17128701508045197\n",
            "step: 110, loss: 0.05548013746738434\n",
            "step: 120, loss: 0.06757694482803345\n",
            "step: 130, loss: 0.16498441994190216\n",
            "step: 140, loss: 0.25511959195137024\n",
            "step: 150, loss: 0.030442196875810623\n",
            "step: 160, loss: 0.04993711784482002\n",
            "step: 170, loss: 0.09241853654384613\n",
            "step: 180, loss: 0.25081866979599\n",
            "step: 190, loss: 0.04420215263962746\n",
            "step: 200, loss: 0.12231796979904175\n",
            "step: 210, loss: 0.18145586550235748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6191446028513239, f1=0.6398305084745763, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16056528687477112\n",
            "step: 10, loss: 0.03527902811765671\n",
            "step: 20, loss: 0.05844531208276749\n",
            "step: 30, loss: 0.0321698933839798\n",
            "step: 40, loss: 0.03394206985831261\n",
            "step: 50, loss: 0.087093286216259\n",
            "step: 60, loss: 0.1676856428384781\n",
            "step: 70, loss: 0.06808336824178696\n",
            "step: 80, loss: 0.12461848556995392\n",
            "step: 90, loss: 0.18976108729839325\n",
            "step: 100, loss: 0.004807612858712673\n",
            "step: 110, loss: 0.16880223155021667\n",
            "step: 120, loss: 0.053899724036455154\n",
            "step: 130, loss: 0.07762877643108368\n",
            "step: 140, loss: 0.03530817851424217\n",
            "step: 150, loss: 0.02911079302430153\n",
            "step: 160, loss: 0.13215157389640808\n",
            "step: 170, loss: 0.05948077514767647\n",
            "step: 180, loss: 0.11514181643724442\n",
            "step: 190, loss: 0.015856973826885223\n",
            "step: 200, loss: 0.2680121660232544\n",
            "step: 210, loss: 0.03144887089729309\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5754527162977867, f1=0.6198347107438017, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021839873865246773\n",
            "step: 10, loss: 0.021856199949979782\n",
            "step: 20, loss: 0.003997281193733215\n",
            "step: 30, loss: 0.0007676546229049563\n",
            "step: 40, loss: 0.0465519018471241\n",
            "step: 50, loss: 0.05598337948322296\n",
            "step: 60, loss: 0.019257213920354843\n",
            "step: 70, loss: 0.015618446283042431\n",
            "step: 80, loss: 0.24137574434280396\n",
            "step: 90, loss: 0.07017102092504501\n",
            "step: 100, loss: 0.0424807146191597\n",
            "step: 110, loss: 0.0064195310696959496\n",
            "step: 120, loss: 0.08615506440401077\n",
            "step: 130, loss: 0.07364015281200409\n",
            "step: 140, loss: 0.007018789649009705\n",
            "step: 150, loss: 0.010599980130791664\n",
            "step: 160, loss: 0.011315410025417805\n",
            "step: 170, loss: 0.11321219801902771\n",
            "step: 180, loss: 0.027111347764730453\n",
            "step: 190, loss: 0.04408985748887062\n",
            "step: 200, loss: 0.01989610306918621\n",
            "step: 210, loss: 0.038333695381879807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5973025048169557, f1=0.6215139442231076, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019642503932118416\n",
            "step: 10, loss: 0.009694666601717472\n",
            "step: 20, loss: 0.016831567510962486\n",
            "step: 30, loss: 0.010769758373498917\n",
            "step: 40, loss: 0.09108764678239822\n",
            "step: 50, loss: 0.05035245418548584\n",
            "step: 60, loss: 0.016459660604596138\n",
            "step: 70, loss: 0.007867238484323025\n",
            "step: 80, loss: 0.029674984514713287\n",
            "step: 90, loss: 0.09884826093912125\n",
            "step: 100, loss: 0.009474498219788074\n",
            "step: 110, loss: 0.04288423806428909\n",
            "step: 120, loss: 0.06136512756347656\n",
            "step: 130, loss: 0.015496695414185524\n",
            "step: 140, loss: 0.037813130766153336\n",
            "step: 150, loss: 0.0061620245687663555\n",
            "step: 160, loss: 0.02529778704047203\n",
            "step: 170, loss: 0.0016564549878239632\n",
            "step: 180, loss: 0.013467421755194664\n",
            "step: 190, loss: 0.022861571982502937\n",
            "step: 200, loss: 0.030961347743868828\n",
            "step: 210, loss: 0.0035480125807225704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5545851528384279, f1=0.6133333333333332, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03950076177716255\n",
            "step: 10, loss: 0.02443673647940159\n",
            "step: 20, loss: 0.01397599745541811\n",
            "step: 30, loss: 0.0026972410269081593\n",
            "step: 40, loss: 0.006648447830229998\n",
            "step: 50, loss: 0.004266645293682814\n",
            "step: 60, loss: 0.13876011967658997\n",
            "step: 70, loss: 0.014708274975419044\n",
            "step: 80, loss: 0.015690673142671585\n",
            "step: 90, loss: 0.02340044640004635\n",
            "step: 100, loss: 0.03151620551943779\n",
            "step: 110, loss: 0.02343909814953804\n",
            "step: 120, loss: 0.0039958711713552475\n",
            "step: 130, loss: 0.0014359727501869202\n",
            "step: 140, loss: 0.013118883594870567\n",
            "step: 150, loss: 0.008754375390708447\n",
            "step: 160, loss: 0.035399485379457474\n",
            "step: 170, loss: 0.016473809257149696\n",
            "step: 180, loss: 0.09233017265796661\n",
            "step: 190, loss: 0.0025134612806141376\n",
            "step: 200, loss: 0.03951618820428848\n",
            "step: 210, loss: 0.08708585798740387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5653104925053533, f1=0.6274509803921569, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014830072410404682\n",
            "step: 10, loss: 0.008112623356282711\n",
            "step: 20, loss: 0.020197054371237755\n",
            "step: 30, loss: 0.015750406309962273\n",
            "step: 40, loss: 0.02512260526418686\n",
            "step: 50, loss: 0.0004133554466534406\n",
            "step: 60, loss: 0.01461930200457573\n",
            "step: 70, loss: 0.0008544484153389931\n",
            "step: 80, loss: 0.002765498124063015\n",
            "step: 90, loss: 0.0014442631509155035\n",
            "step: 100, loss: 0.0006953014526516199\n",
            "step: 110, loss: 0.06084620952606201\n",
            "step: 120, loss: 0.015831809490919113\n",
            "step: 130, loss: 0.0007693234947510064\n",
            "step: 140, loss: 0.0022031632252037525\n",
            "step: 150, loss: 0.03530101105570793\n",
            "step: 160, loss: 0.0003288098960183561\n",
            "step: 170, loss: 0.0028187076095491648\n",
            "step: 180, loss: 0.04494944214820862\n",
            "step: 190, loss: 0.0004155482747592032\n",
            "step: 200, loss: 0.03662963956594467\n",
            "step: 210, loss: 0.0022158699575811625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.54337899543379, f1=0.5967365967365967, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010666750371456146\n",
            "step: 10, loss: 0.0038399246986955404\n",
            "step: 20, loss: 0.0018683267990127206\n",
            "step: 30, loss: 0.005783209577202797\n",
            "step: 40, loss: 0.0039078062400221825\n",
            "step: 50, loss: 0.0010473368456587195\n",
            "step: 60, loss: 0.047945279628038406\n",
            "step: 70, loss: 0.022387400269508362\n",
            "step: 80, loss: 0.010334371589124203\n",
            "step: 90, loss: 0.042907603085041046\n",
            "step: 100, loss: 0.01668045111000538\n",
            "step: 110, loss: 0.0005535919335670769\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.0029512846376746893\n",
            "step: 130, loss: 0.000823818554636091\n",
            "step: 140, loss: 0.004086012486368418\n",
            "step: 150, loss: 0.08183100074529648\n",
            "step: 160, loss: 0.0031296999659389257\n",
            "step: 170, loss: 0.0009000185527838767\n",
            "step: 180, loss: 0.1677587330341339\n",
            "step: 190, loss: 0.06168324872851372\n",
            "step: 200, loss: 0.0010898024775087833\n",
            "step: 210, loss: 0.05065380409359932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5688888888888889, f1=0.6031746031746031, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011139529757201672\n",
            "step: 10, loss: 0.0035929526202380657\n",
            "step: 20, loss: 0.007086622528731823\n",
            "step: 30, loss: 0.009360265918076038\n",
            "step: 40, loss: 0.02151186764240265\n",
            "step: 50, loss: 0.0010682233842089772\n",
            "step: 60, loss: 0.016486940905451775\n",
            "step: 70, loss: 0.005988935008645058\n",
            "step: 80, loss: 0.07821740210056305\n",
            "step: 90, loss: 0.039708394557237625\n",
            "step: 100, loss: 0.011555788107216358\n",
            "step: 110, loss: 0.02382788248360157\n",
            "step: 120, loss: 0.05630321800708771\n",
            "step: 130, loss: 0.021465683355927467\n",
            "step: 140, loss: 0.06567580997943878\n",
            "step: 150, loss: 0.00013107484846841544\n",
            "step: 160, loss: 0.0009294373448938131\n",
            "step: 170, loss: 0.022342130541801453\n",
            "step: 180, loss: 0.0004313152749091387\n",
            "step: 190, loss: 0.00025951897259801626\n",
            "step: 200, loss: 0.0005623206961899996\n",
            "step: 210, loss: 0.0001653011131566018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5662921348314606, f1=0.6055045871559633, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023052005097270012\n",
            "step: 10, loss: 0.00130128872115165\n",
            "step: 20, loss: 0.0002233231207355857\n",
            "step: 30, loss: 0.0001462176878703758\n",
            "step: 40, loss: 0.07762588560581207\n",
            "step: 50, loss: 0.00045536100515164435\n",
            "step: 60, loss: 0.024047518149018288\n",
            "step: 70, loss: 0.0007376528228633106\n",
            "step: 80, loss: 0.0008078819955699146\n",
            "step: 90, loss: 0.006059459410607815\n",
            "step: 100, loss: 0.0015412616776302457\n",
            "step: 110, loss: 0.0012013037921860814\n",
            "step: 120, loss: 0.00019617094949353486\n",
            "step: 130, loss: 0.002009339863434434\n",
            "step: 140, loss: 0.0001594608329469338\n",
            "step: 150, loss: 0.0007085895631462336\n",
            "step: 160, loss: 0.0006734480848535895\n",
            "step: 170, loss: 0.00042602611938491464\n",
            "step: 180, loss: 0.0025560841895639896\n",
            "step: 190, loss: 0.00017581565771251917\n",
            "step: 200, loss: 0.00018492071831133217\n",
            "step: 210, loss: 0.011802962981164455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5610859728506787, f1=0.6000000000000001, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006277641747146845\n",
            "step: 10, loss: 0.00011492678459035233\n",
            "step: 20, loss: 0.022745702415704727\n",
            "step: 30, loss: 0.020457861945033073\n",
            "step: 40, loss: 0.0013349531218409538\n",
            "step: 50, loss: 0.00047090614680200815\n",
            "step: 60, loss: 0.00042197867878712714\n",
            "step: 70, loss: 0.011088176630437374\n",
            "step: 80, loss: 0.0005682246992364526\n",
            "step: 90, loss: 0.0001010531050269492\n",
            "step: 100, loss: 0.019743507727980614\n",
            "step: 110, loss: 0.00017991459753829986\n",
            "step: 120, loss: 0.00027770333690568805\n",
            "step: 130, loss: 0.00021787370496895164\n",
            "step: 140, loss: 0.007059624418616295\n",
            "step: 150, loss: 0.0026060377713292837\n",
            "step: 160, loss: 0.029601560905575752\n",
            "step: 170, loss: 9.628834231989458e-05\n",
            "step: 180, loss: 0.0005278571625240147\n",
            "step: 190, loss: 0.0002802237286232412\n",
            "step: 200, loss: 0.029836630448698997\n",
            "step: 210, loss: 0.0005498878890648484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5583524027459954, f1=0.5995423340961099, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005759702762588859\n",
            "step: 10, loss: 0.016410116106271744\n",
            "step: 20, loss: 0.0001626913290238008\n",
            "step: 30, loss: 0.00023825163953006268\n",
            "step: 40, loss: 0.00013031532580498606\n",
            "step: 50, loss: 0.01171413529664278\n",
            "step: 60, loss: 0.00029741256730630994\n",
            "step: 70, loss: 0.0002730275155045092\n",
            "step: 80, loss: 0.00013549065624829382\n",
            "step: 90, loss: 0.0007366005447693169\n",
            "step: 100, loss: 0.00036201823968440294\n",
            "step: 110, loss: 0.00029428728157654405\n",
            "step: 120, loss: 0.0011050974717363715\n",
            "step: 130, loss: 0.00016153468459378928\n",
            "step: 140, loss: 0.039376381784677505\n",
            "step: 150, loss: 0.005427696742117405\n",
            "step: 160, loss: 0.007764906622469425\n",
            "step: 170, loss: 0.0002736827591434121\n",
            "step: 180, loss: 0.0002998471027240157\n",
            "step: 190, loss: 0.00043299520621076226\n",
            "step: 200, loss: 0.0003677497443277389\n",
            "step: 210, loss: 0.0003980435139965266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5492957746478874, f1=0.5813953488372092, best_f1=0.6452905811623246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002164265315514058\n",
            "step: 10, loss: 0.0001884384109871462\n",
            "step: 20, loss: 0.0005536008975468576\n",
            "step: 30, loss: 0.01617651991546154\n",
            "step: 40, loss: 0.009819394908845425\n",
            "step: 50, loss: 9.852898074313998e-05\n",
            "step: 60, loss: 0.018809763714671135\n",
            "step: 70, loss: 0.0026279401499778032\n",
            "step: 80, loss: 0.00017606645997148007\n",
            "step: 90, loss: 0.00046493211993947625\n",
            "step: 100, loss: 0.020348727703094482\n",
            "step: 110, loss: 0.00011077075760113075\n",
            "step: 120, loss: 0.00011666442878777161\n",
            "step: 130, loss: 0.00032041294616647065\n",
            "step: 140, loss: 0.00010679856495698914\n",
            "step: 150, loss: 0.0003619845083449036\n",
            "step: 160, loss: 0.0008952996577136219\n",
            "step: 170, loss: 0.0025686249136924744\n",
            "step: 180, loss: 0.00017520504479762167\n",
            "step: 190, loss: 0.00010419584577903152\n",
            "step: 200, loss: 0.0008669685339555144\n",
            "step: 210, loss: 0.0004135944473091513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5560747663551402, f1=0.586046511627907, best_f1=0.6452905811623246\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 281.27it/s]\n",
            "load_f1 = 0.6202783300198806\n",
            "real_f1 = 0.616\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcbc99d-0679-470a-f17f-6b5216ad4b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5702155828475952\n",
            "step: 10, loss: 0.3836224675178528\n",
            "step: 20, loss: 0.29920610785484314\n",
            "step: 30, loss: 0.416873961687088\n",
            "step: 40, loss: 0.4372500479221344\n",
            "step: 50, loss: 0.33031418919563293\n",
            "step: 60, loss: 0.274509996175766\n",
            "step: 70, loss: 0.24865178763866425\n",
            "step: 80, loss: 0.2500951290130615\n",
            "step: 90, loss: 0.311379998922348\n",
            "step: 100, loss: 0.2898648977279663\n",
            "step: 110, loss: 0.3344440758228302\n",
            "step: 120, loss: 0.09324097633361816\n",
            "step: 130, loss: 0.08567391335964203\n",
            "step: 140, loss: 0.03678925707936287\n",
            "step: 150, loss: 0.10789704322814941\n",
            "step: 160, loss: 0.14984479546546936\n",
            "step: 170, loss: 0.20682837069034576\n",
            "step: 180, loss: 0.01976926252245903\n",
            "step: 190, loss: 0.1942034661769867\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6826923076923076, f1=0.7153284671532846, best_f1=0.7153284671532846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2956336736679077\n",
            "step: 10, loss: 0.031310100108385086\n",
            "step: 20, loss: 0.12008827924728394\n",
            "step: 30, loss: 0.1635238528251648\n",
            "step: 40, loss: 0.13057096302509308\n",
            "step: 50, loss: 0.043385252356529236\n",
            "step: 60, loss: 0.24106577038764954\n",
            "step: 70, loss: 0.3683586120605469\n",
            "step: 80, loss: 0.168092280626297\n",
            "step: 90, loss: 0.21834388375282288\n",
            "step: 100, loss: 0.020674103870987892\n",
            "step: 110, loss: 0.2171851247549057\n",
            "step: 120, loss: 0.17850612103939056\n",
            "step: 130, loss: 0.06762033700942993\n",
            "step: 140, loss: 0.09340450912714005\n",
            "step: 150, loss: 0.08743634819984436\n",
            "step: 160, loss: 0.06408315151929855\n",
            "step: 170, loss: 0.1396813839673996\n",
            "step: 180, loss: 0.13979472219944\n",
            "step: 190, loss: 0.0459727980196476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7923497267759563, f1=0.8022284122562673, best_f1=0.8022284122562673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08057229965925217\n",
            "step: 10, loss: 0.16307112574577332\n",
            "step: 20, loss: 0.07401979714632034\n",
            "step: 30, loss: 0.09704585373401642\n",
            "step: 40, loss: 0.03035420924425125\n",
            "step: 50, loss: 0.12583871185779572\n",
            "step: 60, loss: 0.024735018610954285\n",
            "step: 70, loss: 0.08353149890899658\n",
            "step: 80, loss: 0.14219403266906738\n",
            "step: 90, loss: 0.151479110121727\n",
            "step: 100, loss: 0.030592795461416245\n",
            "step: 110, loss: 0.09082714468240738\n",
            "step: 120, loss: 0.07148759067058563\n",
            "step: 130, loss: 0.01106941606849432\n",
            "step: 140, loss: 0.04693306237459183\n",
            "step: 150, loss: 0.10662461072206497\n",
            "step: 160, loss: 0.015533359721302986\n",
            "step: 170, loss: 0.10966257750988007\n",
            "step: 180, loss: 0.029515083879232407\n",
            "step: 190, loss: 0.12080565840005875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7958115183246073, f1=0.7851458885941645, best_f1=0.7851458885941645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06044350191950798\n",
            "step: 10, loss: 0.05021367594599724\n",
            "step: 20, loss: 0.08799106627702713\n",
            "step: 30, loss: 0.012075376696884632\n",
            "step: 40, loss: 0.053986094892024994\n",
            "step: 50, loss: 0.035611171275377274\n",
            "step: 60, loss: 0.22459357976913452\n",
            "step: 70, loss: 0.13074442744255066\n",
            "step: 80, loss: 0.07893980294466019\n",
            "step: 90, loss: 0.01019175536930561\n",
            "step: 100, loss: 0.01481155026704073\n",
            "step: 110, loss: 0.005755351856350899\n",
            "step: 120, loss: 0.019682660698890686\n",
            "step: 130, loss: 0.05625030770897865\n",
            "step: 140, loss: 0.01275554858148098\n",
            "step: 150, loss: 0.01053585298359394\n",
            "step: 160, loss: 0.037732526659965515\n",
            "step: 170, loss: 0.1378309279680252\n",
            "step: 180, loss: 0.012998834252357483\n",
            "step: 190, loss: 0.16423946619033813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7999999999999999, f1=0.7774647887323942, best_f1=0.7774647887323942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08114514499902725\n",
            "step: 10, loss: 0.041523829102516174\n",
            "step: 20, loss: 0.0769176259636879\n",
            "step: 30, loss: 0.007329241372644901\n",
            "step: 40, loss: 0.07311122864484787\n",
            "step: 50, loss: 0.20590655505657196\n",
            "step: 60, loss: 0.1386532485485077\n",
            "step: 70, loss: 0.0037273357156664133\n",
            "step: 80, loss: 0.014444788917899132\n",
            "step: 90, loss: 0.07222390919923782\n",
            "step: 100, loss: 0.0014577771071344614\n",
            "step: 110, loss: 0.005960146430879831\n",
            "step: 120, loss: 0.005933031905442476\n",
            "step: 130, loss: 0.02347630448639393\n",
            "step: 140, loss: 0.010341061279177666\n",
            "step: 150, loss: 0.023266294971108437\n",
            "step: 160, loss: 0.0508766770362854\n",
            "step: 170, loss: 0.008281505666673183\n",
            "step: 180, loss: 0.08159155398607254\n",
            "step: 190, loss: 0.1985924243927002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7842105263157894, f1=0.7532467532467532, best_f1=0.7774647887323942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034222134854644537\n",
            "step: 10, loss: 0.04718136042356491\n",
            "step: 20, loss: 0.0130492327734828\n",
            "step: 30, loss: 0.001444653607904911\n",
            "step: 40, loss: 0.005945586133748293\n",
            "step: 50, loss: 0.015236676670610905\n",
            "step: 60, loss: 0.0013912888243794441\n",
            "step: 70, loss: 0.008568077348172665\n",
            "step: 80, loss: 0.07872714847326279\n",
            "step: 90, loss: 0.08581099659204483\n",
            "step: 100, loss: 0.002056194469332695\n",
            "step: 110, loss: 0.0033763523679226637\n",
            "step: 120, loss: 0.03185780718922615\n",
            "step: 130, loss: 0.07693180441856384\n",
            "step: 140, loss: 0.023033302277326584\n",
            "step: 150, loss: 0.0023629572242498398\n",
            "step: 160, loss: 0.004261190537363291\n",
            "step: 170, loss: 0.09295755624771118\n",
            "step: 180, loss: 0.006278639659285545\n",
            "step: 190, loss: 0.014068116433918476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7833333333333333, f1=0.7999999999999999, best_f1=0.7774647887323942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002032299293205142\n",
            "step: 10, loss: 0.008091830648481846\n",
            "step: 20, loss: 0.005830393638461828\n",
            "step: 30, loss: 0.0023516362998634577\n",
            "step: 40, loss: 0.0016847988590598106\n",
            "step: 50, loss: 0.001602275762706995\n",
            "step: 60, loss: 0.009783570654690266\n",
            "step: 70, loss: 0.001672651618719101\n",
            "step: 80, loss: 0.05229629576206207\n",
            "step: 90, loss: 0.006804086733609438\n",
            "step: 100, loss: 0.0008148199412971735\n",
            "step: 110, loss: 0.001307718106545508\n",
            "step: 120, loss: 0.0069845011457800865\n",
            "step: 130, loss: 0.001879932009615004\n",
            "step: 140, loss: 0.0043743629939854145\n",
            "step: 150, loss: 0.0014881929382681847\n",
            "step: 160, loss: 0.15961001813411713\n",
            "step: 170, loss: 0.003599154995754361\n",
            "step: 180, loss: 0.00361996004357934\n",
            "step: 190, loss: 0.0012383178109303117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7946666666666666, f1=0.7956989247311829, best_f1=0.7774647887323942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010801676660776138\n",
            "step: 10, loss: 0.0027001709677278996\n",
            "step: 20, loss: 0.0011746302479878068\n",
            "step: 30, loss: 0.0024364925920963287\n",
            "step: 40, loss: 0.0009644146775826812\n",
            "step: 50, loss: 0.0008555327076464891\n",
            "step: 60, loss: 0.00562919769436121\n",
            "step: 70, loss: 0.016477515920996666\n",
            "step: 80, loss: 0.00019424264610279351\n",
            "step: 90, loss: 0.015790782868862152\n",
            "step: 100, loss: 0.08704125136137009\n",
            "step: 110, loss: 0.0012244138633832335\n",
            "step: 120, loss: 0.00041784142376855016\n",
            "step: 130, loss: 0.0005228723748587072\n",
            "step: 140, loss: 0.0024311027955263853\n",
            "step: 150, loss: 0.0008802110678516328\n",
            "step: 160, loss: 0.0007375647546723485\n",
            "step: 170, loss: 0.0019694501534104347\n",
            "step: 180, loss: 0.00491564953699708\n",
            "step: 190, loss: 0.00269674276933074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7795698924731183, f1=0.7765957446808511, best_f1=0.7774647887323942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003129478543996811\n",
            "step: 10, loss: 0.028208276256918907\n",
            "step: 20, loss: 0.001205503474920988\n",
            "step: 30, loss: 0.00232895789667964\n",
            "step: 40, loss: 0.000269347830908373\n",
            "step: 50, loss: 0.009156467393040657\n",
            "step: 60, loss: 0.004984751343727112\n",
            "step: 70, loss: 0.0004167104489170015\n",
            "step: 80, loss: 0.0005010993336327374\n",
            "step: 90, loss: 0.0019458180759102106\n",
            "step: 100, loss: 0.0013845915673300624\n",
            "step: 110, loss: 0.0005960778798907995\n",
            "step: 120, loss: 0.007905753329396248\n",
            "step: 130, loss: 0.002773771295323968\n",
            "step: 140, loss: 0.18314428627490997\n",
            "step: 150, loss: 0.0026812897995114326\n",
            "step: 160, loss: 0.006079643499106169\n",
            "step: 170, loss: 0.016497069969773293\n",
            "step: 180, loss: 0.0010350544471293688\n",
            "step: 190, loss: 0.0011524721048772335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8041775456919059, f1=0.7917737789203083, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033793009351938963\n",
            "step: 10, loss: 0.0033078899141401052\n",
            "step: 20, loss: 0.0013476446038112044\n",
            "step: 30, loss: 0.0002863405679818243\n",
            "step: 40, loss: 0.018343951553106308\n",
            "step: 50, loss: 0.0007257536053657532\n",
            "step: 60, loss: 0.0003480835584923625\n",
            "step: 70, loss: 0.0004372376424726099\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.0005128409247845411\n",
            "step: 90, loss: 0.001565228565596044\n",
            "step: 100, loss: 0.0007517257472500205\n",
            "step: 110, loss: 0.008999078534543514\n",
            "step: 120, loss: 0.12250525504350662\n",
            "step: 130, loss: 0.0002805425028782338\n",
            "step: 140, loss: 0.0033096643164753914\n",
            "step: 150, loss: 0.0011196006089448929\n",
            "step: 160, loss: 0.0005305794766172767\n",
            "step: 170, loss: 0.00022910661937203258\n",
            "step: 180, loss: 0.000128921601572074\n",
            "step: 190, loss: 0.0003266753046773374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7894736842105263, f1=0.7947368421052631, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011483923299238086\n",
            "step: 10, loss: 0.00031188817229121923\n",
            "step: 20, loss: 0.0017020761733874679\n",
            "step: 30, loss: 0.0013549355790019035\n",
            "step: 40, loss: 0.00012311217142269015\n",
            "step: 50, loss: 0.0003417113039176911\n",
            "step: 60, loss: 0.00037323770811781287\n",
            "step: 70, loss: 0.0007866558735258877\n",
            "step: 80, loss: 0.0005814250325784087\n",
            "step: 90, loss: 0.00010504276724532247\n",
            "step: 100, loss: 0.00017000663501676172\n",
            "step: 110, loss: 0.00015951784735079855\n",
            "step: 120, loss: 0.0005004038102924824\n",
            "step: 130, loss: 0.0002187734644394368\n",
            "step: 140, loss: 0.0004936994519084692\n",
            "step: 150, loss: 0.00014902283146511763\n",
            "step: 160, loss: 0.0006982521736063063\n",
            "step: 170, loss: 0.0003652212326414883\n",
            "step: 180, loss: 0.00022904294019099325\n",
            "step: 190, loss: 0.0021641463972628117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7868852459016394, f1=0.8130081300813009, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014890571765135974\n",
            "step: 10, loss: 0.005604640115052462\n",
            "step: 20, loss: 0.00013102895172778517\n",
            "step: 30, loss: 0.0004554119077511132\n",
            "step: 40, loss: 0.00017773172294255346\n",
            "step: 50, loss: 0.005759811028838158\n",
            "step: 60, loss: 0.00032786044175736606\n",
            "step: 70, loss: 9.898316784529015e-05\n",
            "step: 80, loss: 0.00010955319157801569\n",
            "step: 90, loss: 0.00027169616078026593\n",
            "step: 100, loss: 0.0001742802414810285\n",
            "step: 110, loss: 0.00018857847317121923\n",
            "step: 120, loss: 0.0005595466936938465\n",
            "step: 130, loss: 0.0003359736583661288\n",
            "step: 140, loss: 0.0009906981140375137\n",
            "step: 150, loss: 0.0004599730600602925\n",
            "step: 160, loss: 0.0002258413878735155\n",
            "step: 170, loss: 0.028764907270669937\n",
            "step: 180, loss: 5.382017116062343e-05\n",
            "step: 190, loss: 0.00010667372407624498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7878787878787877, f1=0.8199445983379501, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006206862162798643\n",
            "step: 10, loss: 9.190778655465692e-05\n",
            "step: 20, loss: 0.0005552738439291716\n",
            "step: 30, loss: 0.0005100017879158258\n",
            "step: 40, loss: 0.0004875875893048942\n",
            "step: 50, loss: 0.0009482991881668568\n",
            "step: 60, loss: 8.689275273354724e-05\n",
            "step: 70, loss: 0.004376382566988468\n",
            "step: 80, loss: 0.05556127056479454\n",
            "step: 90, loss: 0.0004548032011371106\n",
            "step: 100, loss: 0.019768504425883293\n",
            "step: 110, loss: 0.000327251706039533\n",
            "step: 120, loss: 0.002163359895348549\n",
            "step: 130, loss: 8.758237527217716e-05\n",
            "step: 140, loss: 0.0001855078589869663\n",
            "step: 150, loss: 0.00030200841138139367\n",
            "step: 160, loss: 0.0002241660258732736\n",
            "step: 170, loss: 0.0006248675053939223\n",
            "step: 180, loss: 0.00012566331133712083\n",
            "step: 190, loss: 0.08425241708755493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7845303867403315, f1=0.8044077134986225, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013595340715255588\n",
            "step: 10, loss: 0.00017399652278982103\n",
            "step: 20, loss: 0.00019687198800966144\n",
            "step: 30, loss: 0.00011040570097975433\n",
            "step: 40, loss: 0.0002890509204007685\n",
            "step: 50, loss: 0.00045038352254778147\n",
            "step: 60, loss: 0.0003403876326046884\n",
            "step: 70, loss: 0.00020084828429389745\n",
            "step: 80, loss: 0.00011068554886151105\n",
            "step: 90, loss: 0.00035833672154694796\n",
            "step: 100, loss: 0.0018613828578963876\n",
            "step: 110, loss: 0.00016717828111723065\n",
            "step: 120, loss: 0.00023862652597017586\n",
            "step: 130, loss: 0.0001663265866227448\n",
            "step: 140, loss: 0.00014619404100812972\n",
            "step: 150, loss: 0.00016459856124129146\n",
            "step: 160, loss: 0.0005644951015710831\n",
            "step: 170, loss: 0.00019694012007676065\n",
            "step: 180, loss: 0.00020923679403495044\n",
            "step: 190, loss: 0.0011075170477852225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7927461139896373, f1=0.7948051948051948, best_f1=0.7917737789203083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006905566435307264\n",
            "step: 10, loss: 8.728742250241339e-05\n",
            "step: 20, loss: 0.00016162953397724777\n",
            "step: 30, loss: 0.00011198624997632578\n",
            "step: 40, loss: 0.00039611064130440354\n",
            "step: 50, loss: 0.00047178970999084413\n",
            "step: 60, loss: 0.00019262643763795495\n",
            "step: 70, loss: 0.0012662302469834685\n",
            "step: 80, loss: 0.0016822739271447062\n",
            "step: 90, loss: 0.00010025635856436566\n",
            "step: 100, loss: 0.002405765699222684\n",
            "step: 110, loss: 0.00010481294157216325\n",
            "step: 120, loss: 0.00021389633184298873\n",
            "step: 130, loss: 0.0002198223810410127\n",
            "step: 140, loss: 0.0008918584208004177\n",
            "step: 150, loss: 0.00015531318786088377\n",
            "step: 160, loss: 0.00014494686911348253\n",
            "step: 170, loss: 0.00026655549299903214\n",
            "step: 180, loss: 0.0004924798267893493\n",
            "step: 190, loss: 0.0002608966315165162\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7916666666666666, f1=0.7905759162303665, best_f1=0.7917737789203083\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 167.51it/s]\n",
            "load_f1 = 0.8053333333333333\n",
            "real_f1 = 0.7989417989417988\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 183.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cc8c63-e9a3-4750-efe1-dc7abc5b3e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6196228861808777\n",
            "step: 10, loss: 0.39933598041534424\n",
            "step: 20, loss: 0.30724263191223145\n",
            "step: 30, loss: 0.38219794631004333\n",
            "step: 40, loss: 0.2735332250595093\n",
            "step: 50, loss: 0.2742752134799957\n",
            "step: 60, loss: 0.2130870223045349\n",
            "step: 70, loss: 0.3485220968723297\n",
            "step: 80, loss: 0.29464730620384216\n",
            "step: 90, loss: 0.2597172260284424\n",
            "step: 100, loss: 0.21661250293254852\n",
            "step: 110, loss: 0.2502395510673523\n",
            "step: 120, loss: 0.17414219677448273\n",
            "step: 130, loss: 0.022846849635243416\n",
            "step: 140, loss: 0.18572096526622772\n",
            "step: 150, loss: 0.28571823239326477\n",
            "step: 160, loss: 0.09379509091377258\n",
            "step: 170, loss: 0.2237543910741806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7276887871853546, f1=0.7136363636363636, best_f1=0.7136363636363636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06043141707777977\n",
            "step: 10, loss: 0.3401145339012146\n",
            "step: 20, loss: 0.1467931866645813\n",
            "step: 30, loss: 0.17829269170761108\n",
            "step: 40, loss: 0.13352158665657043\n",
            "step: 50, loss: 0.10286816209554672\n",
            "step: 60, loss: 0.07268332690000534\n",
            "step: 70, loss: 0.084718719124794\n",
            "step: 80, loss: 0.13715822994709015\n",
            "step: 90, loss: 0.14804226160049438\n",
            "step: 100, loss: 0.07689683884382248\n",
            "step: 110, loss: 0.04126761108636856\n",
            "step: 120, loss: 0.056172922253608704\n",
            "step: 130, loss: 0.12318399548530579\n",
            "step: 140, loss: 0.36141064763069153\n",
            "step: 150, loss: 0.17943806946277618\n",
            "step: 160, loss: 0.1454332023859024\n",
            "step: 170, loss: 0.04997464269399643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7461139896373058, f1=0.7860696517412936, best_f1=0.7860696517412936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07172627002000809\n",
            "step: 10, loss: 0.05240214616060257\n",
            "step: 20, loss: 0.08119945973157883\n",
            "step: 30, loss: 0.15223455429077148\n",
            "step: 40, loss: 0.17177042365074158\n",
            "step: 50, loss: 0.1756497323513031\n",
            "step: 60, loss: 0.03955001384019852\n",
            "step: 70, loss: 0.0593859925866127\n",
            "step: 80, loss: 0.07488259673118591\n",
            "step: 90, loss: 0.16890372335910797\n",
            "step: 100, loss: 0.0034857902210205793\n",
            "step: 110, loss: 0.13568605482578278\n",
            "step: 120, loss: 0.05946598947048187\n",
            "step: 130, loss: 0.1291753202676773\n",
            "step: 140, loss: 0.05205842852592468\n",
            "step: 150, loss: 0.010964316315948963\n",
            "step: 160, loss: 0.012530962936580181\n",
            "step: 170, loss: 0.032856933772563934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7669902912621359, f1=0.7848699763593381, best_f1=0.7848699763593381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01890941895544529\n",
            "step: 10, loss: 0.03478655591607094\n",
            "step: 20, loss: 0.049054067581892014\n",
            "step: 30, loss: 0.020390208810567856\n",
            "step: 40, loss: 0.0011237503495067358\n",
            "step: 50, loss: 0.08822202682495117\n",
            "step: 60, loss: 0.1810912787914276\n",
            "step: 70, loss: 0.003396302228793502\n",
            "step: 80, loss: 0.05588885769248009\n",
            "step: 90, loss: 0.03993607684969902\n",
            "step: 100, loss: 0.05111761391162872\n",
            "step: 110, loss: 0.03405589237809181\n",
            "step: 120, loss: 0.01167586725205183\n",
            "step: 130, loss: 0.0208131093531847\n",
            "step: 140, loss: 0.11245584487915039\n",
            "step: 150, loss: 0.22941400110721588\n",
            "step: 160, loss: 0.06553412973880768\n",
            "step: 170, loss: 0.0037049211096018553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7917737789203084, f1=0.7951219512195121, best_f1=0.7951219512195121\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019348373636603355\n",
            "step: 10, loss: 0.0047509875148534775\n",
            "step: 20, loss: 0.06311140954494476\n",
            "step: 30, loss: 0.05799786373972893\n",
            "step: 40, loss: 0.006668620742857456\n",
            "step: 50, loss: 0.08698318898677826\n",
            "step: 60, loss: 0.026570696383714676\n",
            "step: 70, loss: 0.07476731389760971\n",
            "step: 80, loss: 0.04302487149834633\n",
            "step: 90, loss: 0.1427266150712967\n",
            "step: 100, loss: 0.03599507734179497\n",
            "step: 110, loss: 0.00778178870677948\n",
            "step: 120, loss: 0.045794423669576645\n",
            "step: 130, loss: 0.031062986701726913\n",
            "step: 140, loss: 0.0029855507891625166\n",
            "step: 150, loss: 0.043681707233190536\n",
            "step: 160, loss: 0.009345878846943378\n",
            "step: 170, loss: 0.012876022607088089\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8163265306122449, f1=0.801007556675063, best_f1=0.801007556675063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024349349550902843\n",
            "step: 10, loss: 0.01246707420796156\n",
            "step: 20, loss: 0.008436968550086021\n",
            "step: 30, loss: 0.00366060808300972\n",
            "step: 40, loss: 0.05145813897252083\n",
            "step: 50, loss: 0.33885252475738525\n",
            "step: 60, loss: 0.014850914478302002\n",
            "step: 70, loss: 0.10497765243053436\n",
            "step: 80, loss: 0.019096823409199715\n",
            "step: 90, loss: 0.03112824447453022\n",
            "step: 100, loss: 0.016238398849964142\n",
            "step: 110, loss: 0.0008717470336705446\n",
            "step: 120, loss: 0.003292483976110816\n",
            "step: 130, loss: 0.0006960314349271357\n",
            "step: 140, loss: 0.0009502628818154335\n",
            "step: 150, loss: 0.13097599148750305\n",
            "step: 160, loss: 0.08523133397102356\n",
            "step: 170, loss: 0.009932897053658962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7979539641943735, f1=0.8040201005025125, best_f1=0.801007556675063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019195096101611853\n",
            "step: 10, loss: 0.013225359842181206\n",
            "step: 20, loss: 0.019429627805948257\n",
            "step: 30, loss: 0.02416081354022026\n",
            "step: 40, loss: 0.0013128925347700715\n",
            "step: 50, loss: 0.022173311561346054\n",
            "step: 60, loss: 0.00229705311357975\n",
            "step: 70, loss: 0.0003211093135178089\n",
            "step: 80, loss: 0.00779046630486846\n",
            "step: 90, loss: 0.0002247638622066006\n",
            "step: 100, loss: 0.0009710636222735047\n",
            "step: 110, loss: 0.004248865880072117\n",
            "step: 120, loss: 0.005733162630349398\n",
            "step: 130, loss: 0.10137295722961426\n",
            "step: 140, loss: 0.0012844894081354141\n",
            "step: 150, loss: 0.0025098284240812063\n",
            "step: 160, loss: 0.0007781382300890982\n",
            "step: 170, loss: 0.032245833426713943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7882037533512065, f1=0.8148148148148149, best_f1=0.801007556675063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01392060425132513\n",
            "step: 10, loss: 0.0003236115735489875\n",
            "step: 20, loss: 0.12130532413721085\n",
            "step: 30, loss: 0.011273130774497986\n",
            "step: 40, loss: 0.00019934013835154474\n",
            "step: 50, loss: 0.03867615759372711\n",
            "step: 60, loss: 0.0005625950288958848\n",
            "step: 70, loss: 0.00039867445593699813\n",
            "step: 80, loss: 0.0030711544677615166\n",
            "step: 90, loss: 0.002966040512546897\n",
            "step: 100, loss: 0.014893629588186741\n",
            "step: 110, loss: 0.10648414492607117\n",
            "step: 120, loss: 0.000481956172734499\n",
            "step: 130, loss: 0.04728879779577255\n",
            "step: 140, loss: 0.010034782811999321\n",
            "step: 150, loss: 0.058347709476947784\n",
            "step: 160, loss: 0.10581134259700775\n",
            "step: 170, loss: 0.00539632560685277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7929292929292929, f1=0.7821782178217821, best_f1=0.801007556675063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005739156622439623\n",
            "step: 10, loss: 0.0008634804398752749\n",
            "step: 20, loss: 0.00031753050279803574\n",
            "step: 30, loss: 0.01611943170428276\n",
            "step: 40, loss: 0.013466579839587212\n",
            "step: 50, loss: 0.00023319771571550518\n",
            "step: 60, loss: 0.0007529794238507748\n",
            "step: 70, loss: 0.032060205936431885\n",
            "step: 80, loss: 0.001007425133138895\n",
            "step: 90, loss: 0.03324209526181221\n",
            "step: 100, loss: 0.004220642615109682\n",
            "step: 110, loss: 0.0004282856243662536\n",
            "step: 120, loss: 0.03254341706633568\n",
            "step: 130, loss: 0.042230598628520966\n",
            "step: 140, loss: 0.00020105308794882149\n",
            "step: 150, loss: 0.005991777870804071\n",
            "step: 160, loss: 0.021100565791130066\n",
            "step: 170, loss: 0.02218502201139927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8217054263565892, f1=0.7917737789203084, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022024255245923996\n",
            "step: 10, loss: 0.00018385794828645885\n",
            "step: 20, loss: 0.14623376727104187\n",
            "step: 30, loss: 0.0063887485302984715\n",
            "step: 40, loss: 0.00018589307728689164\n",
            "step: 50, loss: 0.06394912302494049\n",
            "step: 60, loss: 0.00015468805213458836\n",
            "step: 70, loss: 0.004066022578626871\n",
            "step: 80, loss: 0.0014900639653205872\n",
            "step: 90, loss: 0.004190411418676376\n",
            "step: 100, loss: 0.00043010921217501163\n",
            "step: 110, loss: 0.00033321548835374415\n",
            "step: 120, loss: 0.00024392161867581308\n",
            "step: 130, loss: 0.010018400847911835\n",
            "step: 140, loss: 0.004200671333819628\n",
            "step: 150, loss: 0.0038411584682762623\n",
            "step: 160, loss: 0.010998778976500034\n",
            "step: 170, loss: 0.00015542608161922544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7979274611398963, f1=0.7897435897435897, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04774194583296776\n",
            "step: 10, loss: 0.005075046326965094\n",
            "step: 20, loss: 0.00019204682030249387\n",
            "step: 30, loss: 0.00010329913493478671\n",
            "step: 40, loss: 0.00041238105040974915\n",
            "step: 50, loss: 0.006517794914543629\n",
            "step: 60, loss: 0.01490430161356926\n",
            "step: 70, loss: 0.00011733171413652599\n",
            "step: 80, loss: 0.0001263357262359932\n",
            "step: 90, loss: 0.00014533119974657893\n",
            "step: 100, loss: 0.0001305321929976344\n",
            "step: 110, loss: 0.013237877748906612\n",
            "step: 120, loss: 0.0005859804805368185\n",
            "step: 130, loss: 0.00013198982924222946\n",
            "step: 140, loss: 0.00012130050163250417\n",
            "step: 150, loss: 0.056329019367694855\n",
            "step: 160, loss: 0.010028978809714317\n",
            "step: 170, loss: 0.0004444525111466646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7912621359223301, f1=0.7761904761904763, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014674892649054527\n",
            "step: 10, loss: 0.00038714526453986764\n",
            "step: 20, loss: 0.00010915351595031098\n",
            "step: 30, loss: 0.00011508540774229914\n",
            "step: 40, loss: 0.00026142876595258713\n",
            "step: 50, loss: 0.00011893634655280039\n",
            "step: 60, loss: 0.0005742667708545923\n",
            "step: 70, loss: 0.06378734111785889\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 80, loss: 9.830964700086042e-05\n",
            "step: 90, loss: 0.00013999029761180282\n",
            "step: 100, loss: 0.00023496380890719593\n",
            "step: 110, loss: 0.0002464156423229724\n",
            "step: 120, loss: 0.0002303333458257839\n",
            "step: 130, loss: 0.02107195369899273\n",
            "step: 140, loss: 0.029963472858071327\n",
            "step: 150, loss: 0.0037411802913993597\n",
            "step: 160, loss: 0.00014745022053830326\n",
            "step: 170, loss: 9.574176510795951e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.801007556675063, f1=0.7654320987654322, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022442620247602463\n",
            "step: 10, loss: 0.0001006435850285925\n",
            "step: 20, loss: 0.00017724369536153972\n",
            "step: 30, loss: 6.731012399541214e-05\n",
            "step: 40, loss: 0.00015562308544758707\n",
            "step: 50, loss: 0.00010967948037432507\n",
            "step: 60, loss: 0.0008317791507579386\n",
            "step: 70, loss: 0.00023919122759252787\n",
            "step: 80, loss: 0.0003892956010531634\n",
            "step: 90, loss: 7.414602441713214e-05\n",
            "step: 100, loss: 0.00023653706011828035\n",
            "step: 110, loss: 7.376277062576264e-05\n",
            "step: 120, loss: 0.03564397245645523\n",
            "step: 130, loss: 0.00011880516831297427\n",
            "step: 140, loss: 0.00039788498543202877\n",
            "step: 150, loss: 0.0032705513294786215\n",
            "step: 160, loss: 0.0008479305543005466\n",
            "step: 170, loss: 0.00042022368870675564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8064516129032259, f1=0.8020833333333334, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014902418479323387\n",
            "step: 10, loss: 7.680935959797353e-05\n",
            "step: 20, loss: 0.00027412292547523975\n",
            "step: 30, loss: 0.0002554076199885458\n",
            "step: 40, loss: 9.886805491987616e-05\n",
            "step: 50, loss: 0.00015462710871361196\n",
            "step: 60, loss: 7.534863834735006e-05\n",
            "step: 70, loss: 0.0006323034758679569\n",
            "step: 80, loss: 0.0017808134434744716\n",
            "step: 90, loss: 7.289768109330907e-05\n",
            "step: 100, loss: 0.00027334579499438405\n",
            "step: 110, loss: 0.00013981306983623654\n",
            "step: 120, loss: 0.02641107887029648\n",
            "step: 130, loss: 0.052360907196998596\n",
            "step: 140, loss: 0.00010293009836459532\n",
            "step: 150, loss: 0.0001853188732638955\n",
            "step: 160, loss: 4.654255462810397e-05\n",
            "step: 170, loss: 6.961715553188697e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8071979434447302, f1=0.7938931297709925, best_f1=0.7917737789203084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010869545076275244\n",
            "step: 10, loss: 0.0003566346422303468\n",
            "step: 20, loss: 0.00019642870756797493\n",
            "step: 30, loss: 8.898483065422624e-05\n",
            "step: 40, loss: 0.00015089927182998508\n",
            "step: 50, loss: 6.512270920211449e-05\n",
            "step: 60, loss: 0.00018356842338107526\n",
            "step: 70, loss: 6.777067028451711e-05\n",
            "step: 80, loss: 0.004825887270271778\n",
            "step: 90, loss: 0.0017568438779562712\n",
            "step: 100, loss: 0.0007957497728057206\n",
            "step: 110, loss: 9.62048870860599e-05\n",
            "step: 120, loss: 0.0005524765001609921\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.00012035373947583139\n",
            "step: 140, loss: 0.00010769845539471135\n",
            "step: 150, loss: 0.000209121557418257\n",
            "step: 160, loss: 7.086442928994074e-05\n",
            "step: 170, loss: 0.00011826871923403814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7946666666666666, f1=0.8020833333333334, best_f1=0.7917737789203084\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 236.69it/s]\n",
            "load_f1 = 0.8207792207792208\n",
            "real_f1 = 0.8155844155844156\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 194.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb20e29-4cd8-482d-98b1-f775fe05f0cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 649kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 4.07MB/s]\n",
            "Downloading: 100% 440M/440M [00:07<00:00, 56.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6203136444091797\n",
            "step: 10, loss: 0.6090444922447205\n",
            "step: 20, loss: 0.49729305505752563\n",
            "step: 30, loss: 0.26589420437812805\n",
            "step: 40, loss: 0.2327870875597\n",
            "step: 50, loss: 0.07982302457094193\n",
            "step: 60, loss: 0.10154690593481064\n",
            "step: 70, loss: 0.10197766125202179\n",
            "step: 80, loss: 0.10242507606744766\n",
            "step: 90, loss: 0.1576164960861206\n",
            "step: 100, loss: 0.023660091683268547\n",
            "step: 110, loss: 0.11278767138719559\n",
            "step: 120, loss: 0.00947610568255186\n",
            "step: 130, loss: 0.012313169427216053\n",
            "step: 140, loss: 0.008549846708774567\n",
            "step: 150, loss: 0.027086645364761353\n",
            "step: 160, loss: 0.005439393687993288\n",
            "step: 170, loss: 0.18384641408920288\n",
            "step: 180, loss: 0.012912136502563953\n",
            "step: 190, loss: 0.10595869272947311\n",
            "step: 200, loss: 0.10133808851242065\n",
            "step: 210, loss: 0.007792278658598661\n",
            "step: 220, loss: 0.038231950253248215\n",
            "step: 230, loss: 0.025000110268592834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9663677130044843, f1=0.9695603156708005, best_f1=0.9695603156708005\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006685005035251379\n",
            "step: 10, loss: 0.06122604385018349\n",
            "step: 20, loss: 0.04237993806600571\n",
            "step: 30, loss: 0.20470644533634186\n",
            "step: 40, loss: 0.07428956776857376\n",
            "step: 50, loss: 0.007970769889652729\n",
            "step: 60, loss: 0.006369869224727154\n",
            "step: 70, loss: 0.06876637041568756\n",
            "step: 80, loss: 0.017426123842597008\n",
            "step: 90, loss: 0.0028628273867070675\n",
            "step: 100, loss: 0.10388855636119843\n",
            "step: 110, loss: 0.16532394289970398\n",
            "step: 120, loss: 0.12045326828956604\n",
            "step: 130, loss: 0.0095136109739542\n",
            "step: 140, loss: 0.005261925049126148\n",
            "step: 150, loss: 0.025200532749295235\n",
            "step: 160, loss: 0.014332551509141922\n",
            "step: 170, loss: 0.0029257687274366617\n",
            "step: 180, loss: 0.031022600829601288\n",
            "step: 190, loss: 0.002776074456050992\n",
            "step: 200, loss: 0.007127845194190741\n",
            "step: 210, loss: 0.007954521104693413\n",
            "step: 220, loss: 0.06550294160842896\n",
            "step: 230, loss: 0.0045699626207351685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9732739420935412, f1=0.9696969696969697, best_f1=0.9696969696969697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01039542444050312\n",
            "step: 10, loss: 0.005203761626034975\n",
            "step: 20, loss: 0.0027406783774495125\n",
            "step: 30, loss: 0.001264840946532786\n",
            "step: 40, loss: 0.13681627810001373\n",
            "step: 50, loss: 0.0025718004908412695\n",
            "step: 60, loss: 0.005658379755914211\n",
            "step: 70, loss: 0.09106779098510742\n",
            "step: 80, loss: 0.0011980487033724785\n",
            "step: 90, loss: 0.003239715239033103\n",
            "step: 100, loss: 0.001729230978526175\n",
            "step: 110, loss: 0.0016505997627973557\n",
            "step: 120, loss: 0.00333068217150867\n",
            "step: 130, loss: 0.0023095218930393457\n",
            "step: 140, loss: 0.0011402072850614786\n",
            "step: 150, loss: 0.010353309102356434\n",
            "step: 160, loss: 0.052606381475925446\n",
            "step: 170, loss: 0.012831867672502995\n",
            "step: 180, loss: 0.052789632230997086\n",
            "step: 190, loss: 0.0054565249010920525\n",
            "step: 200, loss: 0.005081301089376211\n",
            "step: 210, loss: 0.00681639788672328\n",
            "step: 220, loss: 0.0013100765645503998\n",
            "step: 230, loss: 0.0009961072355508804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.975609756097561, f1=0.9742441209406495, best_f1=0.9742441209406495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007054973975755274\n",
            "step: 10, loss: 0.0012748984154313803\n",
            "step: 20, loss: 0.0017985348822548985\n",
            "step: 30, loss: 0.0008376381592825055\n",
            "step: 40, loss: 0.0016268648905679584\n",
            "step: 50, loss: 0.0003530631074681878\n",
            "step: 60, loss: 0.0015560067258775234\n",
            "step: 70, loss: 0.0014265121426433325\n",
            "step: 80, loss: 0.004743081517517567\n",
            "step: 90, loss: 0.0017458280781283975\n",
            "step: 100, loss: 0.00036667747190222144\n",
            "step: 110, loss: 0.00036624615313485265\n",
            "step: 120, loss: 0.028861388564109802\n",
            "step: 130, loss: 0.000830171920824796\n",
            "step: 140, loss: 0.004536097403615713\n",
            "step: 150, loss: 0.06854332238435745\n",
            "step: 160, loss: 0.03915450721979141\n",
            "step: 170, loss: 0.006158101838082075\n",
            "step: 180, loss: 0.0004778072179760784\n",
            "step: 190, loss: 0.0020205934997648\n",
            "step: 200, loss: 0.004174020141363144\n",
            "step: 210, loss: 0.14743784070014954\n",
            "step: 220, loss: 0.0007484497618861496\n",
            "step: 230, loss: 0.006114515941590071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9709821428571428, f1=0.968609865470852, best_f1=0.9742441209406495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011089156614616513\n",
            "step: 10, loss: 0.030930452048778534\n",
            "step: 20, loss: 0.0013225848088040948\n",
            "step: 30, loss: 0.0009122778428718448\n",
            "step: 40, loss: 0.0008043685229495168\n",
            "step: 50, loss: 0.04201795160770416\n",
            "step: 60, loss: 0.0017717125592753291\n",
            "step: 70, loss: 0.0006277997163124382\n",
            "step: 80, loss: 0.0029925189446657896\n",
            "step: 90, loss: 0.0025745525490492582\n",
            "step: 100, loss: 0.00028829669463448226\n",
            "step: 110, loss: 0.0005890181055292487\n",
            "step: 120, loss: 0.0002635337586980313\n",
            "step: 130, loss: 0.11814319342374802\n",
            "step: 140, loss: 0.025061678141355515\n",
            "step: 150, loss: 0.0024781005922704935\n",
            "step: 160, loss: 0.012722463347017765\n",
            "step: 170, loss: 0.0026803649961948395\n",
            "step: 180, loss: 0.00022198984515853226\n",
            "step: 190, loss: 0.04063091799616814\n",
            "step: 200, loss: 0.0008623615140095353\n",
            "step: 210, loss: 0.00038147493614815176\n",
            "step: 220, loss: 0.09630849957466125\n",
            "step: 230, loss: 0.00050015514716506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9745293466223698, f1=0.9776286353467561, best_f1=0.9742441209406495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010858844965696335\n",
            "step: 10, loss: 0.005254772491753101\n",
            "step: 20, loss: 0.013450803235173225\n",
            "step: 30, loss: 0.00027963309548795223\n",
            "step: 40, loss: 0.00047587064909748733\n",
            "step: 50, loss: 0.0007280459976755083\n",
            "step: 60, loss: 0.00021749631559941918\n",
            "step: 70, loss: 0.0154136186465621\n",
            "step: 80, loss: 0.0007688934565521777\n",
            "step: 90, loss: 0.0004805296193808317\n",
            "step: 100, loss: 0.01650143787264824\n",
            "step: 110, loss: 0.0011751118581742048\n",
            "step: 120, loss: 0.0031102111097425222\n",
            "step: 130, loss: 0.000861030537635088\n",
            "step: 140, loss: 0.00040979532059282064\n",
            "step: 150, loss: 0.005370547994971275\n",
            "step: 160, loss: 0.0010284302989020944\n",
            "step: 170, loss: 0.0004058251215610653\n",
            "step: 180, loss: 0.07008382678031921\n",
            "step: 190, loss: 0.002156592905521393\n",
            "step: 200, loss: 0.00018153547716792673\n",
            "step: 210, loss: 0.00025688466848805547\n",
            "step: 220, loss: 0.0001417480962118134\n",
            "step: 230, loss: 0.029992982745170593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9766925638179801, f1=0.9764837625979844, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03792852535843849\n",
            "step: 10, loss: 0.00013172619219403714\n",
            "step: 20, loss: 0.00022896949667483568\n",
            "step: 30, loss: 0.00016714679077267647\n",
            "step: 40, loss: 0.001472764415666461\n",
            "step: 50, loss: 0.0003106906951870769\n",
            "step: 60, loss: 0.007000145502388477\n",
            "step: 70, loss: 0.0024141264148056507\n",
            "step: 80, loss: 0.0036908115725964308\n",
            "step: 90, loss: 0.001114372629672289\n",
            "step: 100, loss: 0.00044169844477437437\n",
            "step: 110, loss: 0.00022299677948467433\n",
            "step: 120, loss: 0.0009932825341820717\n",
            "step: 130, loss: 0.00046735629439353943\n",
            "step: 140, loss: 0.006286637857556343\n",
            "step: 150, loss: 0.0015910181682556868\n",
            "step: 160, loss: 0.11693967133760452\n",
            "step: 170, loss: 0.007845073938369751\n",
            "step: 180, loss: 0.0014428726863116026\n",
            "step: 190, loss: 0.0004969812580384314\n",
            "step: 200, loss: 0.0519932359457016\n",
            "step: 210, loss: 0.00025646568974480033\n",
            "step: 220, loss: 0.0007134417537599802\n",
            "step: 230, loss: 0.0008424681727774441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9788182831661093, f1=0.9755011135857461, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011436459608376026\n",
            "step: 10, loss: 0.0018933292012661695\n",
            "step: 20, loss: 0.0009468463831581175\n",
            "step: 30, loss: 0.0001476348115829751\n",
            "step: 40, loss: 0.0018967631040140986\n",
            "step: 50, loss: 0.0013764846371486783\n",
            "step: 60, loss: 0.00017890507297124714\n",
            "step: 70, loss: 0.0002432514593238011\n",
            "step: 80, loss: 0.0015375993680208921\n",
            "step: 90, loss: 9.572722774464637e-05\n",
            "step: 100, loss: 0.00026531759067438543\n",
            "step: 110, loss: 0.0035335770808160305\n",
            "step: 120, loss: 0.0005013828631490469\n",
            "step: 130, loss: 0.001065199845470488\n",
            "step: 140, loss: 0.0001143014378612861\n",
            "step: 150, loss: 9.473761019762605e-05\n",
            "step: 160, loss: 0.00013876651064492762\n",
            "step: 170, loss: 8.66696864250116e-05\n",
            "step: 180, loss: 0.00015495880506932735\n",
            "step: 190, loss: 0.00020435960323084146\n",
            "step: 200, loss: 0.002072185045108199\n",
            "step: 210, loss: 0.0006966587388888001\n",
            "step: 220, loss: 0.0011385338148102164\n",
            "step: 230, loss: 0.0021629626862704754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9732142857142857, f1=0.9776785714285714, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007118204375728965\n",
            "step: 10, loss: 0.00014521428965963423\n",
            "step: 20, loss: 0.0005988253396935761\n",
            "step: 30, loss: 0.00019733711087610573\n",
            "step: 40, loss: 0.008111165836453438\n",
            "step: 50, loss: 0.00013735094398725778\n",
            "step: 60, loss: 0.00016871883417479694\n",
            "step: 70, loss: 0.0003814975207205862\n",
            "step: 80, loss: 0.00011552208889042959\n",
            "step: 90, loss: 0.06812690198421478\n",
            "step: 100, loss: 0.00040233368054032326\n",
            "step: 110, loss: 8.909563621273264e-05\n",
            "step: 120, loss: 0.00010414614371256903\n",
            "step: 130, loss: 0.00016074338054750115\n",
            "step: 140, loss: 0.00013115581532474607\n",
            "step: 150, loss: 0.0003232079034205526\n",
            "step: 160, loss: 0.0001597989903530106\n",
            "step: 170, loss: 0.0003171678108628839\n",
            "step: 180, loss: 0.004128125496208668\n",
            "step: 190, loss: 5.4761905630584806e-05\n",
            "step: 200, loss: 0.0007729026256129146\n",
            "step: 210, loss: 0.00011584136518649757\n",
            "step: 220, loss: 4.944383181282319e-05\n",
            "step: 230, loss: 0.00015114012057892978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9751693002257337, f1=0.9785310734463276, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011174352403031662\n",
            "step: 10, loss: 9.101275645662099e-05\n",
            "step: 20, loss: 6.252714956644922e-05\n",
            "step: 30, loss: 4.320436710258946e-05\n",
            "step: 40, loss: 0.0005581480800174177\n",
            "step: 50, loss: 6.20713472017087e-05\n",
            "step: 60, loss: 4.084174724994227e-05\n",
            "step: 70, loss: 0.00024602163466624916\n",
            "step: 80, loss: 9.52604241319932e-05\n",
            "step: 90, loss: 0.00011723016359610483\n",
            "step: 100, loss: 0.00011928498861379921\n",
            "step: 110, loss: 0.00014941337576601654\n",
            "step: 120, loss: 0.0003884203324560076\n",
            "step: 130, loss: 4.882452412857674e-05\n",
            "step: 140, loss: 0.036131247878074646\n",
            "step: 150, loss: 0.021745938807725906\n",
            "step: 160, loss: 8.465252176392823e-05\n",
            "step: 170, loss: 0.00018687530246097594\n",
            "step: 180, loss: 0.00011865893611684442\n",
            "step: 190, loss: 0.00011729911784641445\n",
            "step: 200, loss: 3.245012339903042e-05\n",
            "step: 210, loss: 7.958280184539035e-05\n",
            "step: 220, loss: 0.00014964950969442725\n",
            "step: 230, loss: 0.0001668405457166955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9719416386083053, f1=0.9832402234636871, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.822204497642815e-05\n",
            "step: 10, loss: 0.0001755817502271384\n",
            "step: 20, loss: 0.0003405751776881516\n",
            "step: 30, loss: 0.02094799093902111\n",
            "step: 40, loss: 0.001842131488956511\n",
            "step: 50, loss: 0.001225697691552341\n",
            "step: 60, loss: 0.0004038875922560692\n",
            "step: 70, loss: 0.0008352554286830127\n",
            "step: 80, loss: 5.6601933465572074e-05\n",
            "step: 90, loss: 8.610571967437863e-05\n",
            "step: 100, loss: 0.0001233528892043978\n",
            "step: 110, loss: 0.00010024051152868196\n",
            "step: 120, loss: 5.664722266374156e-05\n",
            "step: 130, loss: 0.00012681575026363134\n",
            "step: 140, loss: 0.0001598795352037996\n",
            "step: 150, loss: 0.0003007844206877053\n",
            "step: 160, loss: 5.1115879614371806e-05\n",
            "step: 170, loss: 0.00014309374091681093\n",
            "step: 180, loss: 8.768062252784148e-05\n",
            "step: 190, loss: 0.00014711881522089243\n",
            "step: 200, loss: 0.002121708123013377\n",
            "step: 210, loss: 8.027219155337662e-05\n",
            "step: 220, loss: 5.2252347813919187e-05\n",
            "step: 230, loss: 6.773220957256854e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9753914988814317, f1=0.9775784753363228, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001236708485521376\n",
            "step: 10, loss: 0.00012253034219611436\n",
            "step: 20, loss: 0.0003580800839699805\n",
            "step: 30, loss: 0.00021873302466701716\n",
            "step: 40, loss: 0.00016614010382909328\n",
            "step: 50, loss: 0.00041336423601023853\n",
            "step: 60, loss: 0.00012046777555951849\n",
            "step: 70, loss: 0.029461340978741646\n",
            "step: 80, loss: 0.00020788866095244884\n",
            "step: 90, loss: 8.683915075380355e-05\n",
            "step: 100, loss: 4.3475880374899134e-05\n",
            "step: 110, loss: 7.773165998514742e-05\n",
            "step: 120, loss: 0.0011307810200378299\n",
            "step: 130, loss: 3.9605940401088446e-05\n",
            "step: 140, loss: 5.9409929235698655e-05\n",
            "step: 150, loss: 6.316474173218012e-05\n",
            "step: 160, loss: 9.545715874992311e-05\n",
            "step: 170, loss: 8.329687261721119e-05\n",
            "step: 180, loss: 0.00018651440041139722\n",
            "step: 190, loss: 0.000190298305824399\n",
            "step: 200, loss: 3.116479638265446e-05\n",
            "step: 210, loss: 6.193658919073641e-05\n",
            "step: 220, loss: 4.6521661715814844e-05\n",
            "step: 230, loss: 0.00016051175771281123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9765363128491621, f1=0.977728285077951, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018988600641023368\n",
            "step: 10, loss: 0.00031651632161810994\n",
            "step: 20, loss: 0.0025193304754793644\n",
            "step: 30, loss: 0.00016825592319946736\n",
            "step: 40, loss: 0.00010818723239935935\n",
            "step: 50, loss: 4.988238652003929e-05\n",
            "step: 60, loss: 0.000404448714107275\n",
            "step: 70, loss: 3.5172550269635394e-05\n",
            "step: 80, loss: 5.771326323156245e-05\n",
            "step: 90, loss: 4.408655877341516e-05\n",
            "step: 100, loss: 4.472574801184237e-05\n",
            "step: 110, loss: 0.014342390932142735\n",
            "step: 120, loss: 0.00022486248053610325\n",
            "step: 130, loss: 5.048907769378275e-05\n",
            "step: 140, loss: 3.543753336998634e-05\n",
            "step: 150, loss: 3.8164391298778355e-05\n",
            "step: 160, loss: 8.172391972038895e-05\n",
            "step: 170, loss: 6.562525959452614e-05\n",
            "step: 180, loss: 4.742372766486369e-05\n",
            "step: 190, loss: 0.0006974944844841957\n",
            "step: 200, loss: 0.0005744736990891397\n",
            "step: 210, loss: 2.653845695022028e-05\n",
            "step: 220, loss: 6.0619011492235586e-05\n",
            "step: 230, loss: 3.6338788049761206e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9765363128491621, f1=0.9755555555555556, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.407795520615764e-05\n",
            "step: 10, loss: 0.001464334549382329\n",
            "step: 20, loss: 3.921422830899246e-05\n",
            "step: 30, loss: 3.8551719626411796e-05\n",
            "step: 40, loss: 0.02265482395887375\n",
            "step: 50, loss: 5.515987504622899e-05\n",
            "step: 60, loss: 0.00010535735782468691\n",
            "step: 70, loss: 5.580452852882445e-05\n",
            "step: 80, loss: 0.00011145668395329267\n",
            "step: 90, loss: 7.1633534389548e-05\n",
            "step: 100, loss: 4.155731949140318e-05\n",
            "step: 110, loss: 0.00015889416681602597\n",
            "step: 120, loss: 2.3532116756541654e-05\n",
            "step: 130, loss: 3.0651030101580545e-05\n",
            "step: 140, loss: 4.31146145274397e-05\n",
            "step: 150, loss: 0.00013450399274006486\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 160, loss: 0.0024484579917043447\n",
            "step: 170, loss: 5.105702075525187e-05\n",
            "step: 180, loss: 9.469208453083411e-05\n",
            "step: 190, loss: 0.0012054521357640624\n",
            "step: 200, loss: 5.295656228554435e-05\n",
            "step: 210, loss: 4.8377009079558775e-05\n",
            "step: 220, loss: 3.374631342012435e-05\n",
            "step: 230, loss: 0.000719711824785918\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9765363128491621, f1=0.9810055865921787, best_f1=0.9755011135857461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.755207944777794e-05\n",
            "step: 10, loss: 2.0999104890506715e-05\n",
            "step: 20, loss: 0.00025159993674606085\n",
            "step: 30, loss: 0.00012576566950883716\n",
            "step: 40, loss: 0.00027998481527902186\n",
            "step: 50, loss: 0.005368425976485014\n",
            "step: 60, loss: 0.00010171304893447086\n",
            "step: 70, loss: 6.76778145134449e-05\n",
            "step: 80, loss: 7.416880544042215e-05\n",
            "step: 90, loss: 8.682261977810413e-05\n",
            "step: 100, loss: 0.0008303593494929373\n",
            "step: 110, loss: 7.441522757289931e-05\n",
            "step: 120, loss: 9.091122046811506e-05\n",
            "step: 130, loss: 3.8294485420919955e-05\n",
            "step: 140, loss: 4.842838097829372e-05\n",
            "step: 150, loss: 0.00033934460952878\n",
            "step: 160, loss: 3.228205241612159e-05\n",
            "step: 170, loss: 2.703748941712547e-05\n",
            "step: 180, loss: 0.0003549768007360399\n",
            "step: 190, loss: 5.751214484917e-05\n",
            "step: 200, loss: 5.563685408560559e-05\n",
            "step: 210, loss: 0.00032665932667441666\n",
            "step: 220, loss: 0.000331385585013777\n",
            "step: 230, loss: 4.4147564040031284e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9775280898876404, f1=0.9808773903262092, best_f1=0.9755011135857461\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 210.71it/s]\n",
            "load_f1 = 0.9787709497206705\n",
            "real_f1 = 0.9788182831661093\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 253.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ee5a84-b029-4e8e-dc83-f9043cfcfc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6307729482650757\n",
            "step: 10, loss: 0.5116791129112244\n",
            "step: 20, loss: 0.5376427173614502\n",
            "step: 30, loss: 0.2225271612405777\n",
            "step: 40, loss: 0.20386184751987457\n",
            "step: 50, loss: 0.28114327788352966\n",
            "step: 60, loss: 0.07726120203733444\n",
            "step: 70, loss: 0.15436582267284393\n",
            "step: 80, loss: 0.07224278897047043\n",
            "step: 90, loss: 0.04765675961971283\n",
            "step: 100, loss: 0.036880847066640854\n",
            "step: 110, loss: 0.049555402249097824\n",
            "step: 120, loss: 0.1729259192943573\n",
            "step: 130, loss: 0.09313459694385529\n",
            "step: 140, loss: 0.15318775177001953\n",
            "step: 150, loss: 0.045988183468580246\n",
            "step: 160, loss: 0.02520507574081421\n",
            "step: 170, loss: 0.23363670706748962\n",
            "step: 180, loss: 0.2048174887895584\n",
            "step: 190, loss: 0.009783057495951653\n",
            "step: 200, loss: 0.13948902487754822\n",
            "step: 210, loss: 0.014135531149804592\n",
            "step: 220, loss: 0.21009758114814758\n",
            "step: 230, loss: 0.11048240214586258\n",
            "step: 240, loss: 0.09313053637742996\n",
            "step: 250, loss: 0.018163319677114487\n",
            "step: 260, loss: 0.1410256177186966\n",
            "step: 270, loss: 0.008299284614622593\n",
            "step: 280, loss: 0.02369680255651474\n",
            "step: 290, loss: 0.16910487413406372\n",
            "step: 300, loss: 0.03308264538645744\n",
            "step: 310, loss: 0.17405621707439423\n",
            "step: 320, loss: 0.13624723255634308\n",
            "step: 330, loss: 0.04177641496062279\n",
            "step: 340, loss: 0.019859984517097473\n",
            "step: 350, loss: 0.06292828917503357\n",
            "step: 360, loss: 0.02557295747101307\n",
            "step: 370, loss: 0.04449380189180374\n",
            "step: 380, loss: 0.024509411305189133\n",
            "step: 390, loss: 0.19033773243427277\n",
            "step: 400, loss: 0.22776901721954346\n",
            "step: 410, loss: 0.07091238349676132\n",
            "step: 420, loss: 0.06574337184429169\n",
            "step: 430, loss: 0.14066411554813385\n",
            "step: 440, loss: 0.025944625958800316\n",
            "step: 450, loss: 0.006461089476943016\n",
            "step: 460, loss: 0.06049305200576782\n",
            "step: 470, loss: 0.110963374376297\n",
            "step: 480, loss: 0.04995698481798172\n",
            "step: 490, loss: 0.22725415229797363\n",
            "step: 500, loss: 0.12741875648498535\n",
            "step: 510, loss: 0.05776817351579666\n",
            "step: 520, loss: 0.12494073808193207\n",
            "step: 530, loss: 0.004357177298516035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9307479224376731, f1=0.9322820037105751, best_f1=0.9322820037105751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1956339329481125\n",
            "step: 10, loss: 0.05569889396429062\n",
            "step: 20, loss: 0.018854748457670212\n",
            "step: 30, loss: 0.05282512307167053\n",
            "step: 40, loss: 0.21686111390590668\n",
            "step: 50, loss: 0.1638326197862625\n",
            "step: 60, loss: 0.008924935013055801\n",
            "step: 70, loss: 0.027572283521294594\n",
            "step: 80, loss: 0.04969339072704315\n",
            "step: 90, loss: 0.024523308500647545\n",
            "step: 100, loss: 0.013679927214980125\n",
            "step: 110, loss: 0.008336649276316166\n",
            "step: 120, loss: 0.1148570105433464\n",
            "step: 130, loss: 0.21637113392353058\n",
            "step: 140, loss: 0.017508935183286667\n",
            "step: 150, loss: 0.052965156733989716\n",
            "step: 160, loss: 0.030412796884775162\n",
            "step: 170, loss: 0.02481074631214142\n",
            "step: 180, loss: 0.06470821052789688\n",
            "step: 190, loss: 0.08755320310592651\n",
            "step: 200, loss: 0.004415736999362707\n",
            "step: 210, loss: 0.015501068904995918\n",
            "step: 220, loss: 0.05861566215753555\n",
            "step: 230, loss: 0.002212373074144125\n",
            "step: 240, loss: 0.023143306374549866\n",
            "step: 250, loss: 0.07691628485918045\n",
            "step: 260, loss: 0.002741017146036029\n",
            "step: 270, loss: 0.3594549298286438\n",
            "step: 280, loss: 0.056399811059236526\n",
            "step: 290, loss: 0.025861982256174088\n",
            "step: 300, loss: 0.18497507274150848\n",
            "step: 310, loss: 0.04451332241296768\n",
            "step: 320, loss: 0.03668956458568573\n",
            "step: 330, loss: 0.025185072794556618\n",
            "step: 340, loss: 0.03676968067884445\n",
            "step: 350, loss: 0.0032595591619610786\n",
            "step: 360, loss: 0.12251301109790802\n",
            "step: 370, loss: 0.21622014045715332\n",
            "step: 380, loss: 0.06104037165641785\n",
            "step: 390, loss: 0.06023208051919937\n",
            "step: 400, loss: 0.026824550703167915\n",
            "step: 410, loss: 0.040040142834186554\n",
            "step: 420, loss: 0.18377253413200378\n",
            "step: 430, loss: 0.02355162799358368\n",
            "step: 440, loss: 0.1538930982351303\n",
            "step: 450, loss: 0.00979082752019167\n",
            "step: 460, loss: 0.10711950808763504\n",
            "step: 470, loss: 0.1068413034081459\n",
            "step: 480, loss: 0.2521912157535553\n",
            "step: 490, loss: 0.03982939198613167\n",
            "step: 500, loss: 0.1801994889974594\n",
            "step: 510, loss: 0.010388778522610664\n",
            "step: 520, loss: 0.10765866190195084\n",
            "step: 530, loss: 0.018529901280999184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9340245051837888, f1=0.9316360207449317, best_f1=0.9316360207449317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029775364324450493\n",
            "step: 10, loss: 0.028834201395511627\n",
            "step: 20, loss: 0.031453195959329605\n",
            "step: 30, loss: 0.015082649886608124\n",
            "step: 40, loss: 0.007488580886274576\n",
            "step: 50, loss: 0.09476970881223679\n",
            "step: 60, loss: 0.006936758290976286\n",
            "step: 70, loss: 0.001819196972064674\n",
            "step: 80, loss: 0.0014931465266272426\n",
            "step: 90, loss: 0.007065081503242254\n",
            "step: 100, loss: 0.0387478843331337\n",
            "step: 110, loss: 0.0034156087785959244\n",
            "step: 120, loss: 0.007899652235209942\n",
            "step: 130, loss: 0.014267493970692158\n",
            "step: 140, loss: 0.0029627024196088314\n",
            "step: 150, loss: 0.04744500666856766\n",
            "step: 160, loss: 0.022084074094891548\n",
            "step: 170, loss: 0.08321832120418549\n",
            "step: 180, loss: 0.026838865131139755\n",
            "step: 190, loss: 0.00465679494664073\n",
            "step: 200, loss: 0.10299994051456451\n",
            "step: 210, loss: 0.04167623072862625\n",
            "step: 220, loss: 0.027236230671405792\n",
            "step: 230, loss: 0.02235463447868824\n",
            "step: 240, loss: 0.0017375557217746973\n",
            "step: 250, loss: 0.07403434813022614\n",
            "step: 260, loss: 0.020068775862455368\n",
            "step: 270, loss: 0.007868170738220215\n",
            "step: 280, loss: 0.08289359509944916\n",
            "step: 290, loss: 0.001590369618497789\n",
            "step: 300, loss: 0.013612971641123295\n",
            "step: 310, loss: 0.0032947801519185305\n",
            "step: 320, loss: 0.07334110885858536\n",
            "step: 330, loss: 0.0042465864680707455\n",
            "step: 340, loss: 0.0052255019545555115\n",
            "step: 350, loss: 0.02037326991558075\n",
            "step: 360, loss: 0.05066382884979248\n",
            "step: 370, loss: 0.005190294235944748\n",
            "step: 380, loss: 0.03956708684563637\n",
            "step: 390, loss: 0.02733069285750389\n",
            "step: 400, loss: 0.017077479511499405\n",
            "step: 410, loss: 0.002491460181772709\n",
            "step: 420, loss: 0.05378052964806557\n",
            "step: 430, loss: 0.005687069147825241\n",
            "step: 440, loss: 0.010639277286827564\n",
            "step: 450, loss: 0.08338530361652374\n",
            "step: 460, loss: 0.04191947355866432\n",
            "step: 470, loss: 0.015210014767944813\n",
            "step: 480, loss: 0.0035969321615993977\n",
            "step: 490, loss: 0.034034617245197296\n",
            "step: 500, loss: 0.02031429298222065\n",
            "step: 510, loss: 0.013829110190272331\n",
            "step: 520, loss: 0.16164766252040863\n",
            "step: 530, loss: 0.12363824993371964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9344490934449092, f1=0.9278642149929278, best_f1=0.9278642149929278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025446906685829163\n",
            "step: 10, loss: 0.0034773030783981085\n",
            "step: 20, loss: 0.004976049065589905\n",
            "step: 30, loss: 0.002295037033036351\n",
            "step: 40, loss: 0.01177167147397995\n",
            "step: 50, loss: 0.001592107000760734\n",
            "step: 60, loss: 0.0022238451056182384\n",
            "step: 70, loss: 0.0007301029399968684\n",
            "step: 80, loss: 0.023928072303533554\n",
            "step: 90, loss: 0.017999669536948204\n",
            "step: 100, loss: 0.015635516494512558\n",
            "step: 110, loss: 0.016163045540452003\n",
            "step: 120, loss: 0.00526600843295455\n",
            "step: 130, loss: 0.002318625571206212\n",
            "step: 140, loss: 0.002793633146211505\n",
            "step: 150, loss: 0.02687184140086174\n",
            "step: 160, loss: 0.0070847938768565655\n",
            "step: 170, loss: 0.006022121757268906\n",
            "step: 180, loss: 0.002460871357470751\n",
            "step: 190, loss: 0.17507243156433105\n",
            "step: 200, loss: 0.001833907444961369\n",
            "step: 210, loss: 0.037873730063438416\n",
            "step: 220, loss: 0.0010107691632583737\n",
            "step: 230, loss: 0.009784498251974583\n",
            "step: 240, loss: 0.003431156277656555\n",
            "step: 250, loss: 0.008718075230717659\n",
            "step: 260, loss: 0.0018110109958797693\n",
            "step: 270, loss: 0.007736311759799719\n",
            "step: 280, loss: 0.07104682922363281\n",
            "step: 290, loss: 0.0526873916387558\n",
            "step: 300, loss: 0.0006233923486433923\n",
            "step: 310, loss: 0.06339360773563385\n",
            "step: 320, loss: 0.03729701414704323\n",
            "step: 330, loss: 0.0073302993550896645\n",
            "step: 340, loss: 0.0036254168953746557\n",
            "step: 350, loss: 0.025122124701738358\n",
            "step: 360, loss: 0.029902102425694466\n",
            "step: 370, loss: 0.003875532653182745\n",
            "step: 380, loss: 0.05260513722896576\n",
            "step: 390, loss: 0.0005119703128002584\n",
            "step: 400, loss: 0.00099133828189224\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 410, loss: 0.009816375561058521\n",
            "step: 420, loss: 0.0010612853802740574\n",
            "step: 430, loss: 0.09651830792427063\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 440, loss: 0.0006519126472994685\n",
            "step: 450, loss: 0.031518466770648956\n",
            "step: 460, loss: 0.03149603679776192\n",
            "step: 470, loss: 0.06135253980755806\n",
            "step: 480, loss: 0.01583290658891201\n",
            "step: 490, loss: 0.010550244711339474\n",
            "step: 500, loss: 0.010369845665991306\n",
            "step: 510, loss: 0.004077381454408169\n",
            "step: 520, loss: 0.009896657429635525\n",
            "step: 530, loss: 0.012936703860759735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9246823956442831, f1=0.9229374433363553, best_f1=0.9278642149929278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1770845651626587\n",
            "step: 10, loss: 0.025627033784985542\n",
            "step: 20, loss: 0.07036726921796799\n",
            "step: 30, loss: 0.0009378149406984448\n",
            "step: 40, loss: 0.008883361704647541\n",
            "step: 50, loss: 0.00034383885213173926\n",
            "step: 60, loss: 0.0019190324237570167\n",
            "step: 70, loss: 0.001175232115201652\n",
            "step: 80, loss: 0.00019710586639121175\n",
            "step: 90, loss: 0.011595739051699638\n",
            "step: 100, loss: 0.0033333541359752417\n",
            "step: 110, loss: 0.0010935108875855803\n",
            "step: 120, loss: 0.0037058107554912567\n",
            "step: 130, loss: 0.0015794640639796853\n",
            "step: 140, loss: 0.0010975380428135395\n",
            "step: 150, loss: 0.0006514160195365548\n",
            "step: 160, loss: 0.00033538261777721345\n",
            "step: 170, loss: 0.027575943619012833\n",
            "step: 180, loss: 0.0011541841086000204\n",
            "step: 190, loss: 0.002087540691718459\n",
            "step: 200, loss: 0.0007022308418527246\n",
            "step: 210, loss: 0.04400760307908058\n",
            "step: 220, loss: 0.0023507673759013414\n",
            "step: 230, loss: 0.023345032706856728\n",
            "step: 240, loss: 0.004242179915308952\n",
            "step: 250, loss: 0.006210905034095049\n",
            "step: 260, loss: 0.045130763202905655\n",
            "step: 270, loss: 0.0005774853634648025\n",
            "step: 280, loss: 0.003703601658344269\n",
            "step: 290, loss: 0.08529368042945862\n",
            "step: 300, loss: 0.020729145035147667\n",
            "step: 310, loss: 0.005008715204894543\n",
            "step: 320, loss: 0.09313465654850006\n",
            "step: 330, loss: 0.2164238542318344\n",
            "step: 340, loss: 0.003742827335372567\n",
            "step: 350, loss: 0.00026872940361499786\n",
            "step: 360, loss: 0.0031054699793457985\n",
            "step: 370, loss: 0.0032636860851198435\n",
            "step: 380, loss: 0.0021992006804794073\n",
            "step: 390, loss: 0.00011899645323865116\n",
            "step: 400, loss: 0.0037751246709376574\n",
            "step: 410, loss: 0.01963956654071808\n",
            "step: 420, loss: 0.001938435947522521\n",
            "step: 430, loss: 0.009836063720285892\n",
            "step: 440, loss: 0.018099313601851463\n",
            "step: 450, loss: 0.002753540640696883\n",
            "step: 460, loss: 0.037813298404216766\n",
            "step: 470, loss: 0.047347813844680786\n",
            "step: 480, loss: 0.0647284984588623\n",
            "step: 490, loss: 0.00023391087597701699\n",
            "step: 500, loss: 0.0064266640692949295\n",
            "step: 510, loss: 0.0028315503150224686\n",
            "step: 520, loss: 0.0006852172664366663\n",
            "step: 530, loss: 0.0033944682218134403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9289772727272728, f1=0.9221957040572791, best_f1=0.9278642149929278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007899457705207169\n",
            "step: 10, loss: 0.0018356702057644725\n",
            "step: 20, loss: 0.07526040822267532\n",
            "step: 30, loss: 0.001271457877010107\n",
            "step: 40, loss: 0.0012050011428073049\n",
            "step: 50, loss: 0.029327359050512314\n",
            "step: 60, loss: 0.00012865409371443093\n",
            "step: 70, loss: 0.0005679072928614914\n",
            "step: 80, loss: 0.004402863793075085\n",
            "step: 90, loss: 0.0005045935395173728\n",
            "step: 100, loss: 0.06424573063850403\n",
            "step: 110, loss: 0.0023204535245895386\n",
            "step: 120, loss: 0.001719460473395884\n",
            "step: 130, loss: 0.0005792190786451101\n",
            "step: 140, loss: 0.0018747327849268913\n",
            "step: 150, loss: 0.015484537929296494\n",
            "step: 160, loss: 0.0008379866485483944\n",
            "step: 170, loss: 0.001902903662994504\n",
            "step: 180, loss: 0.000547240546438843\n",
            "step: 190, loss: 0.00024769920855760574\n",
            "step: 200, loss: 0.005001958925276995\n",
            "step: 210, loss: 0.004075324162840843\n",
            "step: 220, loss: 0.0007850155234336853\n",
            "step: 230, loss: 0.00034833510289900005\n",
            "step: 240, loss: 0.029256654903292656\n",
            "step: 250, loss: 0.0003252677561249584\n",
            "step: 260, loss: 0.000594722107052803\n",
            "step: 270, loss: 0.17969726026058197\n",
            "step: 280, loss: 0.002310763578861952\n",
            "step: 290, loss: 0.0006011488731019199\n",
            "step: 300, loss: 0.0024828847963362932\n",
            "step: 310, loss: 0.0036665424704551697\n",
            "step: 320, loss: 0.002756949979811907\n",
            "step: 330, loss: 0.0018946018535643816\n",
            "step: 340, loss: 0.0388069786131382\n",
            "step: 350, loss: 0.0007401690818369389\n",
            "step: 360, loss: 0.004737881012260914\n",
            "step: 370, loss: 0.007484528701752424\n",
            "step: 380, loss: 0.00019527898984961212\n",
            "step: 390, loss: 0.0012342870468273759\n",
            "step: 400, loss: 0.09845340251922607\n",
            "step: 410, loss: 0.001962654758244753\n",
            "step: 420, loss: 0.00164921919349581\n",
            "step: 430, loss: 0.0020695501007139683\n",
            "step: 440, loss: 0.0002024698187597096\n",
            "step: 450, loss: 0.0022931837011128664\n",
            "step: 460, loss: 0.000529334181919694\n",
            "step: 470, loss: 0.006963694468140602\n",
            "step: 480, loss: 0.0004548417928162962\n",
            "step: 490, loss: 0.0011620430741459131\n",
            "step: 500, loss: 0.002469703322276473\n",
            "step: 510, loss: 0.0015499430010095239\n",
            "step: 520, loss: 0.02110685035586357\n",
            "step: 530, loss: 0.013853080570697784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9317647058823528, f1=0.9272388059701492, best_f1=0.9278642149929278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008518265676684678\n",
            "step: 10, loss: 0.0003188895061612129\n",
            "step: 20, loss: 0.0026177912950515747\n",
            "step: 30, loss: 0.0009447474731132388\n",
            "step: 40, loss: 0.14101262390613556\n",
            "step: 50, loss: 0.006388816516846418\n",
            "step: 60, loss: 0.0007804523920640349\n",
            "step: 70, loss: 0.0015211841091513634\n",
            "step: 80, loss: 0.0007228325121104717\n",
            "step: 90, loss: 0.0005229872767813504\n",
            "step: 100, loss: 0.00039039907278493047\n",
            "step: 110, loss: 0.004269953817129135\n",
            "step: 120, loss: 5.672975385095924e-05\n",
            "step: 130, loss: 0.002223587827757001\n",
            "step: 140, loss: 0.009562644176185131\n",
            "step: 150, loss: 0.00025823930627666414\n",
            "step: 160, loss: 0.00012100509775336832\n",
            "step: 170, loss: 0.0001103436152334325\n",
            "step: 180, loss: 0.000130826752865687\n",
            "step: 190, loss: 0.0008795917965471745\n",
            "step: 200, loss: 0.00031367811607196927\n",
            "step: 210, loss: 0.0014301524497568607\n",
            "step: 220, loss: 0.00011602530867094174\n",
            "step: 230, loss: 0.0015497241402044892\n",
            "step: 240, loss: 0.0009624743834137917\n",
            "step: 250, loss: 0.0005483580171130598\n",
            "step: 260, loss: 0.0007152835023589432\n",
            "step: 270, loss: 0.000675368937663734\n",
            "step: 280, loss: 0.0008957774261943996\n",
            "step: 290, loss: 0.0011441516689956188\n",
            "step: 300, loss: 0.02453889325261116\n",
            "step: 310, loss: 0.0007801610045135021\n",
            "step: 320, loss: 0.0013250806368887424\n",
            "step: 330, loss: 0.00764043815433979\n",
            "step: 340, loss: 0.07312183082103729\n",
            "step: 350, loss: 0.0005495835794135928\n",
            "step: 360, loss: 0.08074939250946045\n",
            "step: 370, loss: 0.0014101593988016248\n",
            "step: 380, loss: 0.002382292877882719\n",
            "step: 390, loss: 0.012848705984652042\n",
            "step: 400, loss: 0.016806039959192276\n",
            "step: 410, loss: 0.0018978673033416271\n",
            "step: 420, loss: 0.0003778671962209046\n",
            "step: 430, loss: 0.00447367038577795\n",
            "step: 440, loss: 0.0005564495804719627\n",
            "step: 450, loss: 0.0018483992898836732\n",
            "step: 460, loss: 0.00011390688450774178\n",
            "step: 470, loss: 0.002887047128751874\n",
            "step: 480, loss: 0.006683684419840574\n",
            "step: 490, loss: 0.029385792091488838\n",
            "step: 500, loss: 0.00039817712968215346\n",
            "step: 510, loss: 0.0012541778851300478\n",
            "step: 520, loss: 0.022135842591524124\n",
            "step: 530, loss: 0.0020877665374428034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.933271547729379, f1=0.9314942528735632, best_f1=0.9278642149929278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02819335274398327\n",
            "step: 10, loss: 0.00921015813946724\n",
            "step: 20, loss: 0.00010754097456810996\n",
            "step: 30, loss: 0.0023496488574892282\n",
            "step: 40, loss: 8.397880446864292e-05\n",
            "step: 50, loss: 0.0013359682634472847\n",
            "step: 60, loss: 0.00670336838811636\n",
            "step: 70, loss: 0.00011419531074352562\n",
            "step: 80, loss: 0.000854643527418375\n",
            "step: 90, loss: 0.00023014402540866286\n",
            "step: 100, loss: 5.600736039923504e-05\n",
            "step: 110, loss: 0.00037078381865285337\n",
            "step: 120, loss: 0.0001882117794593796\n",
            "step: 130, loss: 0.0015722776297479868\n",
            "step: 140, loss: 0.0008303174399770796\n",
            "step: 150, loss: 0.00020872097229585052\n",
            "step: 160, loss: 0.000175420573214069\n",
            "step: 170, loss: 0.08047032356262207\n",
            "step: 180, loss: 6.60930818412453e-05\n",
            "step: 190, loss: 0.00033548372448422015\n",
            "step: 200, loss: 0.00015680260548833758\n",
            "step: 210, loss: 0.00010210260370513424\n",
            "step: 220, loss: 0.00011879372323164716\n",
            "step: 230, loss: 4.6413493691943586e-05\n",
            "step: 240, loss: 0.014237958006560802\n",
            "step: 250, loss: 0.00013228955504018813\n",
            "step: 260, loss: 0.00018935339176096022\n",
            "step: 270, loss: 0.020259737968444824\n",
            "step: 280, loss: 0.011237739585340023\n",
            "step: 290, loss: 0.00018161864136345685\n",
            "step: 300, loss: 0.0002089074841933325\n",
            "step: 310, loss: 0.013971388339996338\n",
            "step: 320, loss: 0.0018894897075369954\n",
            "step: 330, loss: 0.00010498808114789426\n",
            "step: 340, loss: 0.00016658162348903716\n",
            "step: 350, loss: 0.00022842099133413285\n",
            "step: 360, loss: 0.00016613276966381818\n",
            "step: 370, loss: 4.042047658003867e-05\n",
            "step: 380, loss: 0.0005590923829004169\n",
            "step: 390, loss: 0.22995564341545105\n",
            "step: 400, loss: 0.00022745836758986115\n",
            "step: 410, loss: 0.0005369687569327652\n",
            "step: 420, loss: 0.0011793707963079214\n",
            "step: 430, loss: 0.003494466422125697\n",
            "step: 440, loss: 0.0009923817124217749\n",
            "step: 450, loss: 0.0001133238329202868\n",
            "step: 460, loss: 6.975134601816535e-05\n",
            "step: 470, loss: 0.00028412960818968713\n",
            "step: 480, loss: 6.597091851290315e-05\n",
            "step: 490, loss: 0.16673380136489868\n",
            "step: 500, loss: 0.0037178401835262775\n",
            "step: 510, loss: 0.002176918089389801\n",
            "step: 520, loss: 0.007154872175306082\n",
            "step: 530, loss: 0.00027233900618739426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9354988399071927, f1=0.9323447636700649, best_f1=0.9323447636700649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011794296093285084\n",
            "step: 10, loss: 5.05396346852649e-05\n",
            "step: 20, loss: 0.010764137841761112\n",
            "step: 30, loss: 0.001084189279936254\n",
            "step: 40, loss: 4.7878125769784674e-05\n",
            "step: 50, loss: 0.000449511397164315\n",
            "step: 60, loss: 5.519023761735298e-05\n",
            "step: 70, loss: 0.0011251728283241391\n",
            "step: 80, loss: 0.002999397926032543\n",
            "step: 90, loss: 0.00046814512461423874\n",
            "step: 100, loss: 8.853139297571033e-05\n",
            "step: 110, loss: 0.00012606017116922885\n",
            "step: 120, loss: 0.00021971823298372328\n",
            "step: 130, loss: 0.0004398797173053026\n",
            "step: 140, loss: 0.0001219765908899717\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.00032053174800239503\n",
            "step: 160, loss: 0.0110640162602067\n",
            "step: 170, loss: 0.012584148906171322\n",
            "step: 180, loss: 6.0380516515579075e-05\n",
            "step: 190, loss: 7.734390237601474e-05\n",
            "step: 200, loss: 8.477398660033941e-05\n",
            "step: 210, loss: 2.751038846326992e-05\n",
            "step: 220, loss: 9.316517389379442e-05\n",
            "step: 230, loss: 3.405856841709465e-05\n",
            "step: 240, loss: 0.0001894822926260531\n",
            "step: 250, loss: 0.017288431525230408\n",
            "step: 260, loss: 0.0009583849459886551\n",
            "step: 270, loss: 9.02475367183797e-05\n",
            "step: 280, loss: 0.00024916199618019164\n",
            "step: 290, loss: 0.009605584666132927\n",
            "step: 300, loss: 3.022590681212023e-05\n",
            "step: 310, loss: 0.00036428726161830127\n",
            "step: 320, loss: 0.03768037632107735\n",
            "step: 330, loss: 6.540554750245064e-05\n",
            "step: 340, loss: 0.00021719495998695493\n",
            "step: 350, loss: 0.00028201600071042776\n",
            "step: 360, loss: 3.837106123683043e-05\n",
            "step: 370, loss: 0.008725748397409916\n",
            "step: 380, loss: 0.00015865886234678328\n",
            "step: 390, loss: 0.007047031540423632\n",
            "step: 400, loss: 0.0006710326415486634\n",
            "step: 410, loss: 4.742529563372955e-05\n",
            "step: 420, loss: 0.0007305934559553862\n",
            "step: 430, loss: 0.00019007727678399533\n",
            "step: 440, loss: 0.0013610003516077995\n",
            "step: 450, loss: 6.0645532357739285e-05\n",
            "step: 460, loss: 0.00036710576387122273\n",
            "step: 470, loss: 3.962566188420169e-05\n",
            "step: 480, loss: 0.000815010629594326\n",
            "step: 490, loss: 0.00023561548732686788\n",
            "step: 500, loss: 0.001152914366684854\n",
            "step: 510, loss: 0.00016412307741120458\n",
            "step: 520, loss: 5.971561768092215e-05\n",
            "step: 530, loss: 0.00010333766113035381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9351851851851852, f1=0.9317231769623782, best_f1=0.9323447636700649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045040753320790827\n",
            "step: 10, loss: 4.882032590103336e-05\n",
            "step: 20, loss: 2.012351615121588e-05\n",
            "step: 30, loss: 3.0531326046912e-05\n",
            "step: 40, loss: 8.663292828714475e-05\n",
            "step: 50, loss: 3.230785659980029e-05\n",
            "step: 60, loss: 7.664111763006076e-05\n",
            "step: 70, loss: 0.00012109129602322355\n",
            "step: 80, loss: 2.3483646145905368e-05\n",
            "step: 90, loss: 9.598115138942376e-05\n",
            "step: 100, loss: 2.0347139070509e-05\n",
            "step: 110, loss: 1.801889084163122e-05\n",
            "step: 120, loss: 0.0001325858465861529\n",
            "step: 130, loss: 0.00021922063024248928\n",
            "step: 140, loss: 0.0006396176177076995\n",
            "step: 150, loss: 0.0016113659366965294\n",
            "step: 160, loss: 8.945615991251543e-05\n",
            "step: 170, loss: 0.002193088410422206\n",
            "step: 180, loss: 5.818776844535023e-05\n",
            "step: 190, loss: 4.715450995718129e-05\n",
            "step: 200, loss: 0.03992244228720665\n",
            "step: 210, loss: 0.003543152706697583\n",
            "step: 220, loss: 0.0011759598273783922\n",
            "step: 230, loss: 0.001029586885124445\n",
            "step: 240, loss: 0.01734039932489395\n",
            "step: 250, loss: 3.491854658932425e-05\n",
            "step: 260, loss: 0.0008076640078797936\n",
            "step: 270, loss: 0.00015066830383148044\n",
            "step: 280, loss: 0.0005538144032470882\n",
            "step: 290, loss: 0.007172219455242157\n",
            "step: 300, loss: 0.0003077126166317612\n",
            "step: 310, loss: 7.263291627168655e-05\n",
            "step: 320, loss: 0.0038240987341850996\n",
            "step: 330, loss: 7.311560329981148e-05\n",
            "step: 340, loss: 0.00013772047532256693\n",
            "step: 350, loss: 7.59853792260401e-05\n",
            "step: 360, loss: 5.759613850386813e-05\n",
            "step: 370, loss: 0.024976136162877083\n",
            "step: 380, loss: 0.0003714078338816762\n",
            "step: 390, loss: 0.00011582868319237605\n",
            "step: 400, loss: 0.00040050214738585055\n",
            "step: 410, loss: 0.0007818436715751886\n",
            "step: 420, loss: 0.00018921987793873996\n",
            "step: 430, loss: 0.0001504958199802786\n",
            "step: 440, loss: 0.00035373884020373225\n",
            "step: 450, loss: 0.00192429986782372\n",
            "step: 460, loss: 8.00935085862875e-05\n",
            "step: 470, loss: 0.00023510016035288572\n",
            "step: 480, loss: 0.00038721159216947854\n",
            "step: 490, loss: 0.00028851875686086714\n",
            "step: 500, loss: 0.013136486522853374\n",
            "step: 510, loss: 0.0003679335059132427\n",
            "step: 520, loss: 0.0004474966845009476\n",
            "step: 530, loss: 0.0006667079287581146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9314685314685315, f1=0.9309056956115779, best_f1=0.9323447636700649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012203966616652906\n",
            "step: 10, loss: 0.00010197170922765508\n",
            "step: 20, loss: 0.0003499736776575446\n",
            "step: 30, loss: 0.0003781066625379026\n",
            "step: 40, loss: 0.00020680668239947408\n",
            "step: 50, loss: 0.000877965590916574\n",
            "step: 60, loss: 0.00019878779130522162\n",
            "step: 70, loss: 0.00012980842438992113\n",
            "step: 80, loss: 2.376264637860004e-05\n",
            "step: 90, loss: 5.655417044181377e-05\n",
            "step: 100, loss: 0.0011397427879273891\n",
            "step: 110, loss: 6.330014730338007e-05\n",
            "step: 120, loss: 0.0022487116511911154\n",
            "step: 130, loss: 2.519971167203039e-05\n",
            "step: 140, loss: 0.00013433660205919296\n",
            "step: 150, loss: 1.9158618670189753e-05\n",
            "step: 160, loss: 3.197908154106699e-05\n",
            "step: 170, loss: 4.4718297431245446e-05\n",
            "step: 180, loss: 6.418812699848786e-05\n",
            "step: 190, loss: 0.0003919703303836286\n",
            "step: 200, loss: 0.0009653124725446105\n",
            "step: 210, loss: 4.9953105190070346e-05\n",
            "step: 220, loss: 7.826452929293737e-05\n",
            "step: 230, loss: 1.2174131370557006e-05\n",
            "step: 240, loss: 6.37678531347774e-05\n",
            "step: 250, loss: 1.4662481589766685e-05\n",
            "step: 260, loss: 2.2887430532136932e-05\n",
            "step: 270, loss: 7.505503162974492e-05\n",
            "step: 280, loss: 3.1689403840573505e-05\n",
            "step: 290, loss: 0.0009540563332848251\n",
            "step: 300, loss: 1.9482748029986396e-05\n",
            "step: 310, loss: 2.3401427824865095e-05\n",
            "step: 320, loss: 4.6731798647670075e-05\n",
            "step: 330, loss: 3.1305138691095635e-05\n",
            "step: 340, loss: 3.284812191850506e-05\n",
            "step: 350, loss: 0.0011516917729750276\n",
            "step: 360, loss: 1.92479037650628e-05\n",
            "step: 370, loss: 3.504452251945622e-05\n",
            "step: 380, loss: 2.1114477931405418e-05\n",
            "step: 390, loss: 1.73297303263098e-05\n",
            "step: 400, loss: 2.1468158593052067e-05\n",
            "step: 410, loss: 0.0012728903675451875\n",
            "step: 420, loss: 3.309731255285442e-05\n",
            "step: 430, loss: 9.097748989006504e-05\n",
            "step: 440, loss: 3.341200135764666e-05\n",
            "step: 450, loss: 3.4290915209567174e-05\n",
            "step: 460, loss: 0.027045512571930885\n",
            "step: 470, loss: 3.288740117568523e-05\n",
            "step: 480, loss: 0.00014010470476932824\n",
            "step: 490, loss: 1.6651796613587067e-05\n",
            "step: 500, loss: 4.47414604423102e-05\n",
            "step: 510, loss: 3.343435673741624e-05\n",
            "step: 520, loss: 4.869022450293414e-05\n",
            "step: 530, loss: 0.002835912862792611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9359742054352833, f1=0.9354838709677419, best_f1=0.9354838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.593479050323367e-05\n",
            "step: 10, loss: 1.904706186905969e-05\n",
            "step: 20, loss: 0.005009418353438377\n",
            "step: 30, loss: 3.4545999369584024e-05\n",
            "step: 40, loss: 0.00011539211118360981\n",
            "step: 50, loss: 0.0035985936410725117\n",
            "step: 60, loss: 0.001712589175440371\n",
            "step: 70, loss: 4.228106263326481e-05\n",
            "step: 80, loss: 0.00010726010077632964\n",
            "step: 90, loss: 3.5661691072164103e-05\n",
            "step: 100, loss: 0.00036333134630694985\n",
            "step: 110, loss: 2.6761765184346586e-05\n",
            "step: 120, loss: 4.925368193653412e-05\n",
            "step: 130, loss: 0.0007238658727146685\n",
            "step: 140, loss: 2.860103086277377e-05\n",
            "step: 150, loss: 1.889796658360865e-05\n",
            "step: 160, loss: 1.2654670172196347e-05\n",
            "step: 170, loss: 5.051032349001616e-05\n",
            "step: 180, loss: 2.357276389375329e-05\n",
            "step: 190, loss: 5.038263043388724e-05\n",
            "step: 200, loss: 2.027997834375128e-05\n",
            "step: 210, loss: 0.0006255400949157774\n",
            "step: 220, loss: 0.00013506195682566613\n",
            "step: 230, loss: 4.6988159738248214e-05\n",
            "step: 240, loss: 0.00012234764290042222\n",
            "step: 250, loss: 8.568148041376844e-05\n",
            "step: 260, loss: 2.7405802029534243e-05\n",
            "step: 270, loss: 2.6784135116031393e-05\n",
            "step: 280, loss: 0.0003642719238996506\n",
            "step: 290, loss: 2.5491262931609526e-05\n",
            "step: 300, loss: 0.005116807762533426\n",
            "step: 310, loss: 7.478307088604197e-05\n",
            "step: 320, loss: 2.7204023353988305e-05\n",
            "step: 330, loss: 3.5756806028075516e-05\n",
            "step: 340, loss: 5.160036016604863e-05\n",
            "step: 350, loss: 0.0003018690913449973\n",
            "step: 360, loss: 0.0014240018790587783\n",
            "step: 370, loss: 6.595631566597149e-05\n",
            "step: 380, loss: 5.339215204003267e-05\n",
            "step: 390, loss: 0.00012374285142868757\n",
            "step: 400, loss: 3.117927917628549e-05\n",
            "step: 410, loss: 0.0045042079873383045\n",
            "step: 420, loss: 0.01587119698524475\n",
            "step: 430, loss: 0.0005446573486551642\n",
            "step: 440, loss: 0.00015018266276456416\n",
            "step: 450, loss: 0.0028901146724820137\n",
            "step: 460, loss: 1.522130514786113e-05\n",
            "step: 470, loss: 2.6071309548569843e-05\n",
            "step: 480, loss: 6.50013898848556e-05\n",
            "step: 490, loss: 0.0010173522168770432\n",
            "step: 500, loss: 2.4734006728976965e-05\n",
            "step: 510, loss: 0.00017343151557724923\n",
            "step: 520, loss: 6.931564712431282e-05\n",
            "step: 530, loss: 3.608088081819005e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9341486359360301, f1=0.922859830667921, best_f1=0.9354838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019528381526470184\n",
            "step: 10, loss: 2.105097155435942e-05\n",
            "step: 20, loss: 6.889783253427595e-05\n",
            "step: 30, loss: 3.65345949830953e-05\n",
            "step: 40, loss: 2.3036251150188036e-05\n",
            "step: 50, loss: 0.0006609720294363797\n",
            "step: 60, loss: 1.9907409296138212e-05\n",
            "step: 70, loss: 1.8678189007914625e-05\n",
            "step: 80, loss: 1.2948968105774838e-05\n",
            "step: 90, loss: 0.010899974964559078\n",
            "step: 100, loss: 2.5732817448442802e-05\n",
            "step: 110, loss: 1.6554815374547616e-05\n",
            "step: 120, loss: 5.584873724728823e-05\n",
            "step: 130, loss: 1.6309037164319307e-05\n",
            "step: 140, loss: 5.861766112502664e-05\n",
            "step: 150, loss: 2.469018909323495e-05\n",
            "step: 160, loss: 2.5841345632215962e-05\n",
            "step: 170, loss: 2.2276502932072617e-05\n",
            "step: 180, loss: 7.951445149956271e-05\n",
            "step: 190, loss: 1.5642277503502555e-05\n",
            "step: 200, loss: 1.694607090030331e-05\n",
            "step: 210, loss: 1.566824175824877e-05\n",
            "step: 220, loss: 1.3015981494390871e-05\n",
            "step: 230, loss: 1.5489462384721264e-05\n",
            "step: 240, loss: 0.002197554800659418\n",
            "step: 250, loss: 1.5444822565768845e-05\n",
            "step: 260, loss: 1.6439233149867505e-05\n",
            "step: 270, loss: 1.1358306437614374e-05\n",
            "step: 280, loss: 1.9646458895294927e-05\n",
            "step: 290, loss: 1.8949929653899744e-05\n",
            "step: 300, loss: 5.940763367107138e-05\n",
            "step: 310, loss: 2.2276572053669952e-05\n",
            "step: 320, loss: 6.1005554016446695e-05\n",
            "step: 330, loss: 3.8266651245066896e-05\n",
            "step: 340, loss: 3.06825568259228e-05\n",
            "step: 350, loss: 0.004696923308074474\n",
            "step: 360, loss: 1.8674501916393638e-05\n",
            "step: 370, loss: 2.3017910280032083e-05\n",
            "step: 380, loss: 1.7363281585858203e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 390, loss: 0.0020458605140447617\n",
            "step: 400, loss: 2.5292931240983307e-05\n",
            "step: 410, loss: 0.0001926858676597476\n",
            "step: 420, loss: 1.743404209264554e-05\n",
            "step: 430, loss: 5.093269282951951e-05\n",
            "step: 440, loss: 0.0001545355044072494\n",
            "step: 450, loss: 2.1375015421654098e-05\n",
            "step: 460, loss: 2.791984297800809e-05\n",
            "step: 470, loss: 1.8885708414018154e-05\n",
            "step: 480, loss: 1.4330988960864488e-05\n",
            "step: 490, loss: 0.001458386075682938\n",
            "step: 500, loss: 1.306435569858877e-05\n",
            "step: 510, loss: 2.972466973005794e-05\n",
            "step: 520, loss: 0.00036372223985381424\n",
            "step: 530, loss: 1.9702010831679218e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9339578454332552, f1=0.9243776420854861, best_f1=0.9354838709677419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003139447420835495\n",
            "step: 10, loss: 1.1097524293290917e-05\n",
            "step: 20, loss: 1.3928666703577619e-05\n",
            "step: 30, loss: 1.4547067621606402e-05\n",
            "step: 40, loss: 3.7640678783645853e-05\n",
            "step: 50, loss: 4.647160676540807e-05\n",
            "step: 60, loss: 1.79290564119583e-05\n",
            "step: 70, loss: 0.0001485950779169798\n",
            "step: 80, loss: 2.6254376280121505e-05\n",
            "step: 90, loss: 2.2644870114163496e-05\n",
            "step: 100, loss: 0.00032973987981677055\n",
            "step: 110, loss: 1.4677393664896954e-05\n",
            "step: 120, loss: 7.221426494652405e-05\n",
            "step: 130, loss: 0.005680023226886988\n",
            "step: 140, loss: 3.9392161852447316e-05\n",
            "step: 150, loss: 2.090810812660493e-05\n",
            "step: 160, loss: 3.451183874858543e-05\n",
            "step: 170, loss: 1.6860389223438688e-05\n",
            "step: 180, loss: 1.6774714822531678e-05\n",
            "step: 190, loss: 1.852528475865256e-05\n",
            "step: 200, loss: 9.627616236684844e-05\n",
            "step: 210, loss: 1.4535898117173929e-05\n",
            "step: 220, loss: 2.0015231712022796e-05\n",
            "step: 230, loss: 6.427543121390045e-05\n",
            "step: 240, loss: 2.194105582020711e-05\n",
            "step: 250, loss: 3.0148736186674796e-05\n",
            "step: 260, loss: 8.888455340638757e-05\n",
            "step: 270, loss: 1.361579415970482e-05\n",
            "step: 280, loss: 1.3898868928663433e-05\n",
            "step: 290, loss: 4.499302667682059e-05\n",
            "step: 300, loss: 0.00017721332551445812\n",
            "step: 310, loss: 0.0006483061006292701\n",
            "step: 320, loss: 5.655493805534206e-05\n",
            "step: 330, loss: 2.8828262657043524e-05\n",
            "step: 340, loss: 4.0683255065232515e-05\n",
            "step: 350, loss: 2.773676715150941e-05\n",
            "step: 360, loss: 6.044032488716766e-05\n",
            "step: 370, loss: 0.001720789703540504\n",
            "step: 380, loss: 7.164126145653427e-05\n",
            "step: 390, loss: 0.0013887012610211968\n",
            "step: 400, loss: 0.0011823853710666299\n",
            "step: 410, loss: 1.4740718143002596e-05\n",
            "step: 420, loss: 2.6298712327843532e-05\n",
            "step: 430, loss: 1.8775139324134216e-05\n",
            "step: 440, loss: 4.751302913064137e-05\n",
            "step: 450, loss: 5.3235708037391305e-05\n",
            "step: 460, loss: 2.1311660020728596e-05\n",
            "step: 470, loss: 0.00012996207806281745\n",
            "step: 480, loss: 1.4226691746443976e-05\n",
            "step: 490, loss: 0.00042156269773840904\n",
            "step: 500, loss: 1.4308651770988945e-05\n",
            "step: 510, loss: 0.00010535962064750493\n",
            "step: 520, loss: 2.182919706683606e-05\n",
            "step: 530, loss: 4.179935422143899e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9419414770088249, f1=0.9309225776541493, best_f1=0.9309225776541493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.3965953257866204e-05\n",
            "step: 10, loss: 0.0006544888601638377\n",
            "step: 20, loss: 1.6830448657856323e-05\n",
            "step: 30, loss: 2.4845530788297765e-05\n",
            "step: 40, loss: 3.806559107033536e-05\n",
            "step: 50, loss: 3.9495276723755524e-05\n",
            "step: 60, loss: 1.3213392776378896e-05\n",
            "step: 70, loss: 0.0003243098617531359\n",
            "step: 80, loss: 6.641867366852239e-05\n",
            "step: 90, loss: 1.9285429516457953e-05\n",
            "step: 100, loss: 4.21962104155682e-05\n",
            "step: 110, loss: 4.878882100456394e-05\n",
            "step: 120, loss: 0.00015363722923211753\n",
            "step: 130, loss: 0.07381158322095871\n",
            "step: 140, loss: 1.657727079873439e-05\n",
            "step: 150, loss: 2.0607712940545753e-05\n",
            "step: 160, loss: 4.4449581764638424e-05\n",
            "step: 170, loss: 5.41058907401748e-05\n",
            "step: 180, loss: 1.1194388207513839e-05\n",
            "step: 190, loss: 1.888319820864126e-05\n",
            "step: 200, loss: 1.7247619325644337e-05\n",
            "step: 210, loss: 0.00010816749272635207\n",
            "step: 220, loss: 1.3161282367946114e-05\n",
            "step: 230, loss: 3.768680107896216e-05\n",
            "step: 240, loss: 1.491584498580778e-05\n",
            "step: 250, loss: 1.9612971300375648e-05\n",
            "step: 260, loss: 1.8510401787352748e-05\n",
            "step: 270, loss: 1.6245732695097104e-05\n",
            "step: 280, loss: 1.3772241800324991e-05\n",
            "step: 290, loss: 1.1492402336443774e-05\n",
            "step: 300, loss: 1.4442732208408415e-05\n",
            "step: 310, loss: 4.887758768745698e-05\n",
            "step: 320, loss: 1.546699604659807e-05\n",
            "step: 330, loss: 1.8372484191786498e-05\n",
            "step: 340, loss: 1.119812804972753e-05\n",
            "step: 350, loss: 1.0512685548746958e-05\n",
            "step: 360, loss: 0.000441272568423301\n",
            "step: 370, loss: 1.148119099525502e-05\n",
            "step: 380, loss: 3.0771981982979923e-05\n",
            "step: 390, loss: 3.807251778198406e-05\n",
            "step: 400, loss: 1.6826843420858495e-05\n",
            "step: 410, loss: 0.00014150737843010575\n",
            "step: 420, loss: 3.047790414711926e-05\n",
            "step: 430, loss: 0.000406336213927716\n",
            "step: 440, loss: 6.802468851674348e-05\n",
            "step: 450, loss: 1.2077267456334084e-05\n",
            "step: 460, loss: 1.4163354535412509e-05\n",
            "step: 470, loss: 0.0001894988090498373\n",
            "step: 480, loss: 1.0751075024018064e-05\n",
            "step: 490, loss: 1.3749858226219658e-05\n",
            "step: 500, loss: 1.7366832253173925e-05\n",
            "step: 510, loss: 3.68289474863559e-05\n",
            "step: 520, loss: 1.4155902135826182e-05\n",
            "step: 530, loss: 1.8421036656945944e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9414519906323185, f1=0.9293785310734464, best_f1=0.9309225776541493\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 194.24it/s]\n",
            "load_f1 = 0.9410672853828307\n",
            "real_f1 = 0.9410672853828307\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17bf4b6-b565-4ed1-8e5d-1231b816bcef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=94b98f69d9e610b44b6b9aa31bb203a2d6837a74388976384972c2a7dfb62195\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-89n5fuka/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa22bd0-5d82-4826-da52-3722dd82df0e"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5620827674865723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.45161290322580644, f1=0.3829787234042553, best_f1=0.3829787234042553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4846442937850952\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6086956521739131, f1=0.4736842105263159, best_f1=0.4736842105263159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5578250288963318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6666666666666666, f1=0.4571428571428571, best_f1=0.4571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2720649242401123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6666666666666666, f1=0.5000000000000001, best_f1=0.4571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28687548637390137\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6666666666666666, f1=0.4615384615384615, best_f1=0.4571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23377825319766998\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7586206896551724, f1=0.5555555555555556, best_f1=0.5555555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09038093686103821\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7878787878787878, f1=0.5454545454545455, best_f1=0.5454545454545455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008914544247090816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7857142857142857, f1=0.6250000000000001, best_f1=0.5454545454545455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002683384343981743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7999999999999999, f1=0.588235294117647, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014428733848035336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7741935483870968, f1=0.5714285714285714, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005240441299974918\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7999999999999999, f1=0.6285714285714286, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0054726446978747845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.75, f1=0.6000000000000001, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00855493638664484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7692307692307692, f1=0.6000000000000001, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001994606340304017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7692307692307692, f1=0.6000000000000001, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015845805406570435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7692307692307692, f1=0.6000000000000001, best_f1=0.588235294117647\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 120025.68it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7272727272727273\n",
            "real_f1 = 0.6666666666666665\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 236.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312f8b27-a6c1-49e6-ebe1-186d281d2df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6059117317199707\n",
            "step: 10, loss: 0.611181914806366\n",
            "step: 20, loss: 0.33527982234954834\n",
            "step: 30, loss: 0.1488988846540451\n",
            "step: 40, loss: 0.3733484745025635\n",
            "step: 50, loss: 0.11643394082784653\n",
            "step: 60, loss: 0.16398367285728455\n",
            "step: 70, loss: 0.02333519421517849\n",
            "step: 80, loss: 0.020267415791749954\n",
            "step: 90, loss: 0.3227471113204956\n",
            "step: 100, loss: 0.019575025886297226\n",
            "step: 110, loss: 0.15943145751953125\n",
            "step: 120, loss: 0.006536425091326237\n",
            "step: 130, loss: 0.003829285502433777\n",
            "step: 140, loss: 0.0038007094990462065\n",
            "step: 150, loss: 0.11099487543106079\n",
            "step: 160, loss: 0.005907314829528332\n",
            "step: 170, loss: 0.06413731724023819\n",
            "step: 180, loss: 0.004143578466027975\n",
            "step: 190, loss: 0.06911870092153549\n",
            "step: 200, loss: 0.011374126188457012\n",
            "step: 210, loss: 0.0057992469519376755\n",
            "step: 220, loss: 0.004002366214990616\n",
            "step: 230, loss: 0.08286087214946747\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9796839729119639, f1=0.9784335981838819, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003988240379840136\n",
            "step: 10, loss: 0.007441353984177113\n",
            "step: 20, loss: 0.1463269740343094\n",
            "step: 30, loss: 0.17317107319831848\n",
            "step: 40, loss: 0.10396994650363922\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.009604064747691154\n",
            "step: 60, loss: 0.0038037814665585756\n",
            "step: 70, loss: 0.21021059155464172\n",
            "step: 80, loss: 0.004350540228188038\n",
            "step: 90, loss: 0.011855541728436947\n",
            "step: 100, loss: 0.023767389357089996\n",
            "step: 110, loss: 0.08303207904100418\n",
            "step: 120, loss: 0.004572061821818352\n",
            "step: 130, loss: 0.007278771605342627\n",
            "step: 140, loss: 0.0024646075908094645\n",
            "step: 150, loss: 0.0628281831741333\n",
            "step: 160, loss: 0.025913668796420097\n",
            "step: 170, loss: 0.0009532858384773135\n",
            "step: 180, loss: 0.058723628520965576\n",
            "step: 190, loss: 0.0017961438279598951\n",
            "step: 200, loss: 0.0026363919023424387\n",
            "step: 210, loss: 0.0013825084315612912\n",
            "step: 220, loss: 0.1254175305366516\n",
            "step: 230, loss: 0.039316289126873016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9875424688561721, f1=0.9853438556933484, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04970972612500191\n",
            "step: 10, loss: 0.012848138809204102\n",
            "step: 20, loss: 0.0012175787705928087\n",
            "step: 30, loss: 0.003349388251081109\n",
            "step: 40, loss: 0.026896502822637558\n",
            "step: 50, loss: 0.01195070892572403\n",
            "step: 60, loss: 0.0015384380239993334\n",
            "step: 70, loss: 0.001769556780345738\n",
            "step: 80, loss: 0.0006559052271768451\n",
            "step: 90, loss: 0.19075515866279602\n",
            "step: 100, loss: 0.0010318945860490203\n",
            "step: 110, loss: 0.0008078203536570072\n",
            "step: 120, loss: 0.046550195664167404\n",
            "step: 130, loss: 0.0016172362957149744\n",
            "step: 140, loss: 0.08001568913459778\n",
            "step: 150, loss: 0.005265025421977043\n",
            "step: 160, loss: 0.013753416016697884\n",
            "step: 170, loss: 0.004979969467967749\n",
            "step: 180, loss: 0.002972103189677\n",
            "step: 190, loss: 0.0030539524741470814\n",
            "step: 200, loss: 0.07336866110563278\n",
            "step: 210, loss: 0.0023304440546780825\n",
            "step: 220, loss: 0.0008848590077832341\n",
            "step: 230, loss: 0.0036448698956519365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9821826280623607, f1=0.9788182831661093, best_f1=0.9853438556933484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001709171338006854\n",
            "step: 10, loss: 0.0003246757260058075\n",
            "step: 20, loss: 0.00035910168662667274\n",
            "step: 30, loss: 0.0003473885590210557\n",
            "step: 40, loss: 0.03729569539427757\n",
            "step: 50, loss: 0.0005903543205931783\n",
            "step: 60, loss: 0.0038875846657902002\n",
            "step: 70, loss: 0.0011626447085291147\n",
            "step: 80, loss: 0.0051305051892995834\n",
            "step: 90, loss: 0.0004760025185532868\n",
            "step: 100, loss: 0.000518286949954927\n",
            "step: 110, loss: 0.0006857928237877786\n",
            "step: 120, loss: 0.07203167676925659\n",
            "step: 130, loss: 0.001813827664591372\n",
            "step: 140, loss: 0.0005909205647185445\n",
            "step: 150, loss: 0.007443417329341173\n",
            "step: 160, loss: 0.006245717406272888\n",
            "step: 170, loss: 0.007255596108734608\n",
            "step: 180, loss: 0.0003812692884821445\n",
            "step: 190, loss: 0.0019119977951049805\n",
            "step: 200, loss: 0.0015609340043738484\n",
            "step: 210, loss: 0.05116428807377815\n",
            "step: 220, loss: 0.0004838423046749085\n",
            "step: 230, loss: 0.02201245352625847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.990990990990991, f1=0.9842342342342343, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008141954895108938\n",
            "step: 10, loss: 0.0006636914913542569\n",
            "step: 20, loss: 0.0033733383752405643\n",
            "step: 30, loss: 0.00018284606630913913\n",
            "step: 40, loss: 0.0025158273056149483\n",
            "step: 50, loss: 0.0003786934830714017\n",
            "step: 60, loss: 0.0297209732234478\n",
            "step: 70, loss: 0.0006489738589152694\n",
            "step: 80, loss: 0.002148926490917802\n",
            "step: 90, loss: 0.0023620750289410353\n",
            "step: 100, loss: 0.0004112029855605215\n",
            "step: 110, loss: 0.0002463505952619016\n",
            "step: 120, loss: 0.0001233329821843654\n",
            "step: 130, loss: 0.0014551416970789433\n",
            "step: 140, loss: 0.005858397576957941\n",
            "step: 150, loss: 0.00046098302118480206\n",
            "step: 160, loss: 0.00035481381928548217\n",
            "step: 170, loss: 0.008092770352959633\n",
            "step: 180, loss: 0.015315856784582138\n",
            "step: 190, loss: 0.09543842077255249\n",
            "step: 200, loss: 0.02776476740837097\n",
            "step: 210, loss: 0.0007673182990401983\n",
            "step: 220, loss: 0.0011368737323209643\n",
            "step: 230, loss: 0.0003057481371797621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9909502262443439, f1=0.9841628959276018, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005893422639928758\n",
            "step: 10, loss: 0.0005842798855155706\n",
            "step: 20, loss: 0.002685253042727709\n",
            "step: 30, loss: 0.0005435132188722491\n",
            "step: 40, loss: 0.00025670730974525213\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.00020609331841114908\n",
            "step: 60, loss: 0.00015941115270834416\n",
            "step: 70, loss: 0.020734811201691628\n",
            "step: 80, loss: 0.0019428739324212074\n",
            "step: 90, loss: 0.000609216804150492\n",
            "step: 100, loss: 0.04423752427101135\n",
            "step: 110, loss: 0.0003832468646578491\n",
            "step: 120, loss: 0.00025113398442044854\n",
            "step: 130, loss: 0.0011454051127657294\n",
            "step: 140, loss: 0.00017147361359093338\n",
            "step: 150, loss: 0.011167222633957863\n",
            "step: 160, loss: 0.00021917707636021078\n",
            "step: 170, loss: 0.00013773301907349378\n",
            "step: 180, loss: 0.026753835380077362\n",
            "step: 190, loss: 0.0002797854831442237\n",
            "step: 200, loss: 0.0003968550590798259\n",
            "step: 210, loss: 0.01548443827778101\n",
            "step: 220, loss: 9.444540773984045e-05\n",
            "step: 230, loss: 0.1650511920452118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9887892376681614, f1=0.9819819819819819, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010745205217972398\n",
            "step: 10, loss: 0.0001707050541881472\n",
            "step: 20, loss: 0.006048272363841534\n",
            "step: 30, loss: 0.00037031190004199743\n",
            "step: 40, loss: 0.00036385853309184313\n",
            "step: 50, loss: 0.00034542905632406473\n",
            "step: 60, loss: 0.00021317480423022062\n",
            "step: 70, loss: 0.014782070182263851\n",
            "step: 80, loss: 0.00032083812402561307\n",
            "step: 90, loss: 0.00014653820835519582\n",
            "step: 100, loss: 0.0004341880267020315\n",
            "step: 110, loss: 0.00010795980779221281\n",
            "step: 120, loss: 8.580723078921437e-05\n",
            "step: 130, loss: 0.00016396892897319049\n",
            "step: 140, loss: 0.0007789547671563923\n",
            "step: 150, loss: 0.014049806632101536\n",
            "step: 160, loss: 0.04862329736351967\n",
            "step: 170, loss: 0.00045240053441375494\n",
            "step: 180, loss: 0.001250546076335013\n",
            "step: 190, loss: 0.00015863694716244936\n",
            "step: 200, loss: 0.08243868499994278\n",
            "step: 210, loss: 5.639364826492965e-05\n",
            "step: 220, loss: 0.0033671052660793066\n",
            "step: 230, loss: 0.00012422299187164754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9898074745186863, f1=0.9829738933030647, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.274467836599797e-05\n",
            "step: 10, loss: 0.00011986812751274556\n",
            "step: 20, loss: 0.00011410965089453384\n",
            "step: 30, loss: 0.00016938730550464243\n",
            "step: 40, loss: 0.0002330265415366739\n",
            "step: 50, loss: 0.0005765188834629953\n",
            "step: 60, loss: 0.0001495241594966501\n",
            "step: 70, loss: 0.00025532825384289026\n",
            "step: 80, loss: 0.00021628881222568452\n",
            "step: 90, loss: 4.124809129280038e-05\n",
            "step: 100, loss: 0.00015629148401785642\n",
            "step: 110, loss: 0.02799215540289879\n",
            "step: 120, loss: 0.00022441506735049188\n",
            "step: 130, loss: 0.006068977061659098\n",
            "step: 140, loss: 7.979785732459277e-05\n",
            "step: 150, loss: 0.0003779448161367327\n",
            "step: 160, loss: 0.00010077144543174654\n",
            "step: 170, loss: 5.3719308198196813e-05\n",
            "step: 180, loss: 6.460365693783388e-05\n",
            "step: 190, loss: 0.00011368478590156883\n",
            "step: 200, loss: 5.719358887290582e-05\n",
            "step: 210, loss: 6.769879109924659e-05\n",
            "step: 220, loss: 0.00024374874192290008\n",
            "step: 230, loss: 7.641538104508072e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9887640449438202, f1=0.9808773903262092, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.53499664622359e-05\n",
            "step: 10, loss: 0.00015581565094180405\n",
            "step: 20, loss: 0.00021632153948303312\n",
            "step: 30, loss: 4.9274993216386065e-05\n",
            "step: 40, loss: 0.030559314414858818\n",
            "step: 50, loss: 7.219148392323405e-05\n",
            "step: 60, loss: 0.0002764719829428941\n",
            "step: 70, loss: 0.025525685399770737\n",
            "step: 80, loss: 0.0013233562931418419\n",
            "step: 90, loss: 0.00018368272867519408\n",
            "step: 100, loss: 0.007010356057435274\n",
            "step: 110, loss: 5.4474712669616565e-05\n",
            "step: 120, loss: 6.81498131598346e-05\n",
            "step: 130, loss: 0.11074821650981903\n",
            "step: 140, loss: 0.010614493861794472\n",
            "step: 150, loss: 0.0014064532006159425\n",
            "step: 160, loss: 0.00011158398410771042\n",
            "step: 170, loss: 0.0002899171377066523\n",
            "step: 180, loss: 0.002971445210278034\n",
            "step: 190, loss: 0.00010279826528858393\n",
            "step: 200, loss: 9.382270945934579e-05\n",
            "step: 210, loss: 0.0003665972617454827\n",
            "step: 220, loss: 6.749962631147355e-05\n",
            "step: 230, loss: 9.279977530241013e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9854096520763187, f1=0.9796839729119639, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020971556659787893\n",
            "step: 10, loss: 0.00012665620306506753\n",
            "step: 20, loss: 9.000847057905048e-05\n",
            "step: 30, loss: 0.00012679870997089893\n",
            "step: 40, loss: 9.51701367739588e-05\n",
            "step: 50, loss: 0.00035950797609984875\n",
            "step: 60, loss: 0.0036881454288959503\n",
            "step: 70, loss: 0.001616881461814046\n",
            "step: 80, loss: 7.505427493015304e-05\n",
            "step: 90, loss: 0.0016781880985945463\n",
            "step: 100, loss: 0.0001268952473765239\n",
            "step: 110, loss: 0.00010481038771104068\n",
            "step: 120, loss: 9.25728163565509e-05\n",
            "step: 130, loss: 6.985836807871237e-05\n",
            "step: 140, loss: 0.05796099454164505\n",
            "step: 150, loss: 0.008026476949453354\n",
            "step: 160, loss: 3.6272067518439144e-05\n",
            "step: 170, loss: 0.00010077381011797115\n",
            "step: 180, loss: 9.308586595579982e-05\n",
            "step: 190, loss: 0.01228022575378418\n",
            "step: 200, loss: 0.00011065325088566169\n",
            "step: 210, loss: 9.6011339337565e-05\n",
            "step: 220, loss: 6.099817619542591e-05\n",
            "step: 230, loss: 0.0001882334763649851\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9843400447427293, f1=0.978675645342312, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001258408883586526\n",
            "step: 10, loss: 0.01137105468660593\n",
            "step: 20, loss: 0.002546819858253002\n",
            "step: 30, loss: 0.020896660163998604\n",
            "step: 40, loss: 0.0001037332767737098\n",
            "step: 50, loss: 0.00010070372809423134\n",
            "step: 60, loss: 7.822015322744846e-05\n",
            "step: 70, loss: 0.0004056546895299107\n",
            "step: 80, loss: 0.00010807038052007556\n",
            "step: 90, loss: 3.595529778976925e-05\n",
            "step: 100, loss: 4.429497494129464e-05\n",
            "step: 110, loss: 0.029341477900743484\n",
            "step: 120, loss: 4.278232154319994e-05\n",
            "step: 130, loss: 6.635719182668254e-05\n",
            "step: 140, loss: 5.0155365897808224e-05\n",
            "step: 150, loss: 0.030899394303560257\n",
            "step: 160, loss: 3.65476735169068e-05\n",
            "step: 170, loss: 0.028475528582930565\n",
            "step: 180, loss: 5.4377167543862015e-05\n",
            "step: 190, loss: 5.5573491408722475e-05\n",
            "step: 200, loss: 0.0011739627225324512\n",
            "step: 210, loss: 3.838403426925652e-05\n",
            "step: 220, loss: 3.6581277527147904e-05\n",
            "step: 230, loss: 0.00015026355686131865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9887387387387387, f1=0.9807474518686297, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.226173405186273e-05\n",
            "step: 10, loss: 3.5996228689327836e-05\n",
            "step: 20, loss: 3.379099871381186e-05\n",
            "step: 30, loss: 4.6184606617316604e-05\n",
            "step: 40, loss: 0.0004856008163187653\n",
            "step: 50, loss: 0.00010237043898086995\n",
            "step: 60, loss: 0.00013591408787760884\n",
            "step: 70, loss: 0.000504637835547328\n",
            "step: 80, loss: 7.81681010266766e-05\n",
            "step: 90, loss: 5.63038483960554e-05\n",
            "step: 100, loss: 8.089582115644589e-05\n",
            "step: 110, loss: 4.77645626233425e-05\n",
            "step: 120, loss: 5.1924489525845274e-05\n",
            "step: 130, loss: 3.676011328934692e-05\n",
            "step: 140, loss: 0.012259208597242832\n",
            "step: 150, loss: 0.0006830113707110286\n",
            "step: 160, loss: 4.470878775464371e-05\n",
            "step: 170, loss: 4.1107021388597786e-05\n",
            "step: 180, loss: 0.02040359377861023\n",
            "step: 190, loss: 5.1823139074258506e-05\n",
            "step: 200, loss: 3.5275148547952995e-05\n",
            "step: 210, loss: 5.8294401242164895e-05\n",
            "step: 220, loss: 4.212279600324109e-05\n",
            "step: 230, loss: 0.03963029012084007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9886877828054299, f1=0.9829738933030647, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.55151525279507e-05\n",
            "step: 10, loss: 0.0008782543009147048\n",
            "step: 20, loss: 0.0001243498845724389\n",
            "step: 30, loss: 0.00030429053003899753\n",
            "step: 40, loss: 8.296417217934504e-05\n",
            "step: 50, loss: 8.671992691233754e-05\n",
            "step: 60, loss: 5.97627367824316e-05\n",
            "step: 70, loss: 2.6519788661971688e-05\n",
            "step: 80, loss: 4.7082765377126634e-05\n",
            "step: 90, loss: 7.379040471278131e-05\n",
            "step: 100, loss: 3.2170111808227375e-05\n",
            "step: 110, loss: 0.033507172018289566\n",
            "step: 120, loss: 0.025960084050893784\n",
            "step: 130, loss: 5.6332413805648685e-05\n",
            "step: 140, loss: 3.760171966860071e-05\n",
            "step: 150, loss: 4.605573121807538e-05\n",
            "step: 160, loss: 0.00026769412215799093\n",
            "step: 170, loss: 0.0003948290541302413\n",
            "step: 180, loss: 5.1348310080356896e-05\n",
            "step: 190, loss: 3.594799272832461e-05\n",
            "step: 200, loss: 4.7451041609747335e-05\n",
            "step: 210, loss: 4.792941399500705e-05\n",
            "step: 220, loss: 0.00017468795704189688\n",
            "step: 230, loss: 0.00013524311361834407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9898534385569334, f1=0.9797752808988766, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.613052467699163e-05\n",
            "step: 10, loss: 0.010059130378067493\n",
            "step: 20, loss: 0.0008100527920760214\n",
            "step: 30, loss: 0.00018335790082346648\n",
            "step: 40, loss: 0.0002461541153024882\n",
            "step: 50, loss: 4.7122579417191446e-05\n",
            "step: 60, loss: 8.842068200465292e-05\n",
            "step: 70, loss: 5.8242338127456605e-05\n",
            "step: 80, loss: 4.604601417668164e-05\n",
            "step: 90, loss: 8.223658369388431e-05\n",
            "step: 100, loss: 0.00010739926801761612\n",
            "step: 110, loss: 6.222187221283093e-05\n",
            "step: 120, loss: 2.5647992515587248e-05\n",
            "step: 130, loss: 5.602025703410618e-05\n",
            "step: 140, loss: 7.963574898894876e-05\n",
            "step: 150, loss: 4.72661922685802e-05\n",
            "step: 160, loss: 0.009751527570188046\n",
            "step: 170, loss: 4.3671032472047955e-05\n",
            "step: 180, loss: 4.259903289494105e-05\n",
            "step: 190, loss: 0.0001116966232075356\n",
            "step: 200, loss: 4.605402864399366e-05\n",
            "step: 210, loss: 7.239599653985351e-05\n",
            "step: 220, loss: 0.00014574134547729045\n",
            "step: 230, loss: 0.014314772561192513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9909502262443439, f1=0.9807909604519773, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.269200897193514e-05\n",
            "step: 10, loss: 2.5823113901424222e-05\n",
            "step: 20, loss: 4.82327195641119e-05\n",
            "step: 30, loss: 6.0782491345889866e-05\n",
            "step: 40, loss: 8.010570309124887e-05\n",
            "step: 50, loss: 0.05141625553369522\n",
            "step: 60, loss: 0.00010812245454872027\n",
            "step: 70, loss: 7.4500021582935e-05\n",
            "step: 80, loss: 4.281180736143142e-05\n",
            "step: 90, loss: 4.8434474592795596e-05\n",
            "step: 100, loss: 3.9720875065540895e-05\n",
            "step: 110, loss: 0.00016169129230547696\n",
            "step: 120, loss: 6.315074278973043e-05\n",
            "step: 130, loss: 0.013024765066802502\n",
            "step: 140, loss: 3.1321487767854705e-05\n",
            "step: 150, loss: 0.0003681635425891727\n",
            "step: 160, loss: 3.1239265808835626e-05\n",
            "step: 170, loss: 2.832644895534031e-05\n",
            "step: 180, loss: 0.00018100447778124362\n",
            "step: 190, loss: 0.002522439928725362\n",
            "step: 200, loss: 0.00011340812488924712\n",
            "step: 210, loss: 4.514778993325308e-05\n",
            "step: 220, loss: 3.93967711715959e-05\n",
            "step: 230, loss: 2.987979314639233e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9909502262443439, f1=0.9807909604519773, best_f1=0.9842342342342343\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 184.50it/s]\n",
            "load_f1 = 0.9887892376681614\n",
            "real_f1 = 0.9865771812080537\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 224.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b2ab81-7239-4a2b-f072-868500a39b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.618728518486023\n",
            "step: 10, loss: 0.515981912612915\n",
            "step: 20, loss: 0.5366967916488647\n",
            "step: 30, loss: 0.21571069955825806\n",
            "step: 40, loss: 0.2407843917608261\n",
            "step: 50, loss: 0.13492625951766968\n",
            "step: 60, loss: 0.04225018247961998\n",
            "step: 70, loss: 0.06414006650447845\n",
            "step: 80, loss: 0.12169679999351501\n",
            "step: 90, loss: 0.48164230585098267\n",
            "step: 100, loss: 0.04567006975412369\n",
            "step: 110, loss: 0.08598248660564423\n",
            "step: 120, loss: 0.127407044172287\n",
            "step: 130, loss: 0.11097394675016403\n",
            "step: 140, loss: 0.18651317059993744\n",
            "step: 150, loss: 0.07540961354970932\n",
            "step: 160, loss: 0.022495217621326447\n",
            "step: 170, loss: 0.3126569092273712\n",
            "step: 180, loss: 0.04699435830116272\n",
            "step: 190, loss: 0.010290445759892464\n",
            "step: 200, loss: 0.1928737908601761\n",
            "step: 210, loss: 0.07692752778530121\n",
            "step: 220, loss: 0.2660588324069977\n",
            "step: 230, loss: 0.14805693924427032\n",
            "step: 240, loss: 0.023193135857582092\n",
            "step: 250, loss: 0.029829569160938263\n",
            "step: 260, loss: 0.16553150117397308\n",
            "step: 270, loss: 0.005201591178774834\n",
            "step: 280, loss: 0.040971092879772186\n",
            "step: 290, loss: 0.17219054698944092\n",
            "step: 300, loss: 0.06331434845924377\n",
            "step: 310, loss: 0.2297128587961197\n",
            "step: 320, loss: 0.045154646039009094\n",
            "step: 330, loss: 0.07781630009412766\n",
            "step: 340, loss: 0.07056573033332825\n",
            "step: 350, loss: 0.10790145397186279\n",
            "step: 360, loss: 0.027928847819566727\n",
            "step: 370, loss: 0.055861204862594604\n",
            "step: 380, loss: 0.012857172638177872\n",
            "step: 390, loss: 0.22029952704906464\n",
            "step: 400, loss: 0.36013442277908325\n",
            "step: 410, loss: 0.09282002598047256\n",
            "step: 420, loss: 0.05812486261129379\n",
            "step: 430, loss: 0.2337077409029007\n",
            "step: 440, loss: 0.009490055963397026\n",
            "step: 450, loss: 0.022324757650494576\n",
            "step: 460, loss: 0.009175543673336506\n",
            "step: 470, loss: 0.16353251039981842\n",
            "step: 480, loss: 0.08413874357938766\n",
            "step: 490, loss: 0.052150908857584\n",
            "step: 500, loss: 0.12615440785884857\n",
            "step: 510, loss: 0.03333437442779541\n",
            "step: 520, loss: 0.1266438513994217\n",
            "step: 530, loss: 0.0035739082377403975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9317972350230416, f1=0.9323515876668201, best_f1=0.9323515876668201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06924884021282196\n",
            "step: 10, loss: 0.0393051914870739\n",
            "step: 20, loss: 0.036855049431324005\n",
            "step: 30, loss: 0.033584114164114\n",
            "step: 40, loss: 0.06314270943403244\n",
            "step: 50, loss: 0.2168661504983902\n",
            "step: 60, loss: 0.0333644300699234\n",
            "step: 70, loss: 0.03298493102192879\n",
            "step: 80, loss: 0.02077917754650116\n",
            "step: 90, loss: 0.10082071274518967\n",
            "step: 100, loss: 0.014607016928493977\n",
            "step: 110, loss: 0.06842157244682312\n",
            "step: 120, loss: 0.053143154829740524\n",
            "step: 130, loss: 0.07677986472845078\n",
            "step: 140, loss: 0.014691661112010479\n",
            "step: 150, loss: 0.11031849682331085\n",
            "step: 160, loss: 0.02405460551381111\n",
            "step: 170, loss: 0.0505279116332531\n",
            "step: 180, loss: 0.016228802502155304\n",
            "step: 190, loss: 0.12627500295639038\n",
            "step: 200, loss: 0.013167462311685085\n",
            "step: 210, loss: 0.11771532148122787\n",
            "step: 220, loss: 0.06581374257802963\n",
            "step: 230, loss: 0.017326481640338898\n",
            "step: 240, loss: 0.053097914904356\n",
            "step: 250, loss: 0.06455294787883759\n",
            "step: 260, loss: 0.014858880080282688\n",
            "step: 270, loss: 0.2529503107070923\n",
            "step: 280, loss: 0.04549906775355339\n",
            "step: 290, loss: 0.01780070923268795\n",
            "step: 300, loss: 0.17117686569690704\n",
            "step: 310, loss: 0.005995849147439003\n",
            "step: 320, loss: 0.08937830477952957\n",
            "step: 330, loss: 0.017817141488194466\n",
            "step: 340, loss: 0.037942223250865936\n",
            "step: 350, loss: 0.004653653595596552\n",
            "step: 360, loss: 0.023059561848640442\n",
            "step: 370, loss: 0.17343777418136597\n",
            "step: 380, loss: 0.05995073541998863\n",
            "step: 390, loss: 0.11288518458604813\n",
            "step: 400, loss: 0.08719757944345474\n",
            "step: 410, loss: 0.07190967351198196\n",
            "step: 420, loss: 0.01557641290128231\n",
            "step: 430, loss: 0.010183734819293022\n",
            "step: 440, loss: 0.06735852360725403\n",
            "step: 450, loss: 0.01872827485203743\n",
            "step: 460, loss: 0.11252516508102417\n",
            "step: 470, loss: 0.01583481766283512\n",
            "step: 480, loss: 0.19921506941318512\n",
            "step: 490, loss: 0.02389358915388584\n",
            "step: 500, loss: 0.1790066659450531\n",
            "step: 510, loss: 0.05235062912106514\n",
            "step: 520, loss: 0.04379424825310707\n",
            "step: 530, loss: 0.025343814864754677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9345182413470534, f1=0.9272137227630968, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03808059170842171\n",
            "step: 10, loss: 0.012615501880645752\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.07439304143190384\n",
            "step: 30, loss: 0.03520280495285988\n",
            "step: 40, loss: 0.0064519718289375305\n",
            "step: 50, loss: 0.07310600578784943\n",
            "step: 60, loss: 0.010614286176860332\n",
            "step: 70, loss: 0.004157107789069414\n",
            "step: 80, loss: 0.0010762747842818499\n",
            "step: 90, loss: 0.002474491950124502\n",
            "step: 100, loss: 0.14437077939510345\n",
            "step: 110, loss: 0.007583060767501593\n",
            "step: 120, loss: 0.005649389233440161\n",
            "step: 130, loss: 0.019664710387587547\n",
            "step: 140, loss: 0.03150424361228943\n",
            "step: 150, loss: 0.008362935855984688\n",
            "step: 160, loss: 0.015260039828717709\n",
            "step: 170, loss: 0.04539187625050545\n",
            "step: 180, loss: 0.010830646380782127\n",
            "step: 190, loss: 0.012899565510451794\n",
            "step: 200, loss: 0.008999058045446873\n",
            "step: 210, loss: 0.015198147855699062\n",
            "step: 220, loss: 0.02604391612112522\n",
            "step: 230, loss: 0.06317012012004852\n",
            "step: 240, loss: 0.0015216391766443849\n",
            "step: 250, loss: 0.006632383447140455\n",
            "step: 260, loss: 0.01637723110616207\n",
            "step: 270, loss: 0.011745711788535118\n",
            "step: 280, loss: 0.2106398195028305\n",
            "step: 290, loss: 0.003980844747275114\n",
            "step: 300, loss: 0.08755360543727875\n",
            "step: 310, loss: 0.016442691907286644\n",
            "step: 320, loss: 0.009631489403545856\n",
            "step: 330, loss: 0.0029171414207667112\n",
            "step: 340, loss: 0.00916792917996645\n",
            "step: 350, loss: 0.0032869947608560324\n",
            "step: 360, loss: 0.06576641649007797\n",
            "step: 370, loss: 0.011902594938874245\n",
            "step: 380, loss: 0.01596098765730858\n",
            "step: 390, loss: 0.04861151799559593\n",
            "step: 400, loss: 0.01756276749074459\n",
            "step: 410, loss: 0.008160505443811417\n",
            "step: 420, loss: 0.12525708973407745\n",
            "step: 430, loss: 0.003640108974650502\n",
            "step: 440, loss: 0.0028777415864169598\n",
            "step: 450, loss: 0.06696987897157669\n",
            "step: 460, loss: 0.013824112713336945\n",
            "step: 470, loss: 0.03724226355552673\n",
            "step: 480, loss: 0.004180482123047113\n",
            "step: 490, loss: 0.0032680071890354156\n",
            "step: 500, loss: 0.11082301288843155\n",
            "step: 510, loss: 0.042007558047771454\n",
            "step: 520, loss: 0.035006940364837646\n",
            "step: 530, loss: 0.11177602410316467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9327808471454879, f1=0.9217311233885819, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010976339690387249\n",
            "step: 10, loss: 0.006677596364170313\n",
            "step: 20, loss: 0.0021080428268760443\n",
            "step: 30, loss: 0.007920732721686363\n",
            "step: 40, loss: 0.10763366520404816\n",
            "step: 50, loss: 0.0201860424131155\n",
            "step: 60, loss: 0.006347812246531248\n",
            "step: 70, loss: 0.03340178728103638\n",
            "step: 80, loss: 0.003394537139683962\n",
            "step: 90, loss: 0.04741382226347923\n",
            "step: 100, loss: 0.002090269699692726\n",
            "step: 110, loss: 0.021722150966525078\n",
            "step: 120, loss: 0.0005662609473802149\n",
            "step: 130, loss: 0.023779893293976784\n",
            "step: 140, loss: 0.09869711101055145\n",
            "step: 150, loss: 0.00780758960172534\n",
            "step: 160, loss: 0.06011227145791054\n",
            "step: 170, loss: 0.004733314272016287\n",
            "step: 180, loss: 0.029182568192481995\n",
            "step: 190, loss: 0.02283570170402527\n",
            "step: 200, loss: 0.007789176423102617\n",
            "step: 210, loss: 0.018967753276228905\n",
            "step: 220, loss: 0.015121948905289173\n",
            "step: 230, loss: 0.01139984279870987\n",
            "step: 240, loss: 0.005227621644735336\n",
            "step: 250, loss: 0.14548179507255554\n",
            "step: 260, loss: 0.0021902977023273706\n",
            "step: 270, loss: 0.0380401611328125\n",
            "step: 280, loss: 0.05910724401473999\n",
            "step: 290, loss: 0.020545054227113724\n",
            "step: 300, loss: 0.0024234207812696695\n",
            "step: 310, loss: 0.041709404438734055\n",
            "step: 320, loss: 0.0808263048529625\n",
            "step: 330, loss: 0.045657578855752945\n",
            "step: 340, loss: 0.005275183357298374\n",
            "step: 350, loss: 0.0226899404078722\n",
            "step: 360, loss: 0.05096912384033203\n",
            "step: 370, loss: 0.0017130578635260463\n",
            "step: 380, loss: 0.008446352556347847\n",
            "step: 390, loss: 0.019888920709490776\n",
            "step: 400, loss: 0.011690815910696983\n",
            "step: 410, loss: 0.0028103606309741735\n",
            "step: 420, loss: 0.01021298486739397\n",
            "step: 430, loss: 0.18208596110343933\n",
            "step: 440, loss: 0.0031709212344139814\n",
            "step: 450, loss: 0.0015954433474689722\n",
            "step: 460, loss: 0.0023771722335368395\n",
            "step: 470, loss: 0.0149462278932333\n",
            "step: 480, loss: 0.02222374640405178\n",
            "step: 490, loss: 0.005124672316014767\n",
            "step: 500, loss: 0.011940614320337772\n",
            "step: 510, loss: 0.004713071510195732\n",
            "step: 520, loss: 0.07507845759391785\n",
            "step: 530, loss: 0.0027739207725971937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9314814814814815, f1=0.9271889400921659, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011368962936103344\n",
            "step: 10, loss: 0.03258872404694557\n",
            "step: 20, loss: 0.010979508981108665\n",
            "step: 30, loss: 0.008422796614468098\n",
            "step: 40, loss: 0.06580749154090881\n",
            "step: 50, loss: 0.013733502477407455\n",
            "step: 60, loss: 0.13756795227527618\n",
            "step: 70, loss: 0.00159462948795408\n",
            "step: 80, loss: 0.0003334791399538517\n",
            "step: 90, loss: 0.031070811673998833\n",
            "step: 100, loss: 0.11044403910636902\n",
            "step: 110, loss: 0.0017502681585028768\n",
            "step: 120, loss: 0.013588091358542442\n",
            "step: 130, loss: 0.0014546706806868315\n",
            "step: 140, loss: 0.0004966126289218664\n",
            "step: 150, loss: 0.0026454529725015163\n",
            "step: 160, loss: 0.0003865393518935889\n",
            "step: 170, loss: 0.10669172555208206\n",
            "step: 180, loss: 0.002379161538556218\n",
            "step: 190, loss: 0.007829147391021252\n",
            "step: 200, loss: 0.002733712550252676\n",
            "step: 210, loss: 0.0013530271826311946\n",
            "step: 220, loss: 0.0012457524426281452\n",
            "step: 230, loss: 0.0061980560421943665\n",
            "step: 240, loss: 0.003778153331950307\n",
            "step: 250, loss: 0.021101979538798332\n",
            "step: 260, loss: 0.00194891809951514\n",
            "step: 270, loss: 0.0014987303875386715\n",
            "step: 280, loss: 0.003239057259634137\n",
            "step: 290, loss: 0.12224928289651871\n",
            "step: 300, loss: 0.004826186690479517\n",
            "step: 310, loss: 0.010258536785840988\n",
            "step: 320, loss: 0.18420174717903137\n",
            "step: 330, loss: 0.011600874364376068\n",
            "step: 340, loss: 0.0045065986923873425\n",
            "step: 350, loss: 0.0009025849867612123\n",
            "step: 360, loss: 0.012427293695509434\n",
            "step: 370, loss: 0.04348345845937729\n",
            "step: 380, loss: 0.003230612026527524\n",
            "step: 390, loss: 0.00023009035794530064\n",
            "step: 400, loss: 0.0017212703824043274\n",
            "step: 410, loss: 0.0010567621793597937\n",
            "step: 420, loss: 0.003495460608974099\n",
            "step: 430, loss: 0.001293838256970048\n",
            "step: 440, loss: 0.012865149416029453\n",
            "step: 450, loss: 0.005949912127107382\n",
            "step: 460, loss: 0.0008740106713958085\n",
            "step: 470, loss: 0.0010083181550726295\n",
            "step: 480, loss: 0.003879395080730319\n",
            "step: 490, loss: 0.0006579341716133058\n",
            "step: 500, loss: 0.005530611611902714\n",
            "step: 510, loss: 0.15052059292793274\n",
            "step: 520, loss: 0.010439681820571423\n",
            "step: 530, loss: 0.013736148364841938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9319129226493746, f1=0.9233593391463975, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00702802324667573\n",
            "step: 10, loss: 0.00046530886902473867\n",
            "step: 20, loss: 0.10398077219724655\n",
            "step: 30, loss: 0.009261524304747581\n",
            "step: 40, loss: 0.11607876420021057\n",
            "step: 50, loss: 0.021127792075276375\n",
            "step: 60, loss: 0.052770182490348816\n",
            "step: 70, loss: 0.0008609380456618965\n",
            "step: 80, loss: 0.001721388311125338\n",
            "step: 90, loss: 0.0036636090371757746\n",
            "step: 100, loss: 0.00036452373024076223\n",
            "step: 110, loss: 0.000707692583091557\n",
            "step: 120, loss: 0.003209654474630952\n",
            "step: 130, loss: 0.00024813375785015523\n",
            "step: 140, loss: 0.0061063747853040695\n",
            "step: 150, loss: 0.0009065705817192793\n",
            "step: 160, loss: 0.001689845696091652\n",
            "step: 170, loss: 0.006255008280277252\n",
            "step: 180, loss: 0.00031491630943492055\n",
            "step: 190, loss: 0.0007160500390455127\n",
            "step: 200, loss: 0.0024839637335389853\n",
            "step: 210, loss: 0.012732835486531258\n",
            "step: 220, loss: 0.0005652702529914677\n",
            "step: 230, loss: 0.00045395284541882575\n",
            "step: 240, loss: 0.0010749832727015018\n",
            "step: 250, loss: 0.000698892108630389\n",
            "step: 260, loss: 0.00011744433868443593\n",
            "step: 270, loss: 0.011293946765363216\n",
            "step: 280, loss: 0.0014424154069274664\n",
            "step: 290, loss: 0.0017396352486684918\n",
            "step: 300, loss: 0.0009681288502179086\n",
            "step: 310, loss: 0.0004578132356982678\n",
            "step: 320, loss: 0.00024547247448936105\n",
            "step: 330, loss: 0.00023475805937778205\n",
            "step: 340, loss: 0.2152595967054367\n",
            "step: 350, loss: 0.0012219587806612253\n",
            "step: 360, loss: 0.0052760932594537735\n",
            "step: 370, loss: 0.0050185490399599075\n",
            "step: 380, loss: 0.00204872852191329\n",
            "step: 390, loss: 0.03831702843308449\n",
            "step: 400, loss: 0.00029731480753980577\n",
            "step: 410, loss: 0.004156865645200014\n",
            "step: 420, loss: 0.0033292374573647976\n",
            "step: 430, loss: 0.001921538612805307\n",
            "step: 440, loss: 7.239123806357384e-05\n",
            "step: 450, loss: 0.0022078058682382107\n",
            "step: 460, loss: 0.0019408121006563306\n",
            "step: 470, loss: 0.0001475482276873663\n",
            "step: 480, loss: 0.003043810836970806\n",
            "step: 490, loss: 0.006051321048289537\n",
            "step: 500, loss: 0.000978070660494268\n",
            "step: 510, loss: 0.01049328688532114\n",
            "step: 520, loss: 0.006560326553881168\n",
            "step: 530, loss: 0.04426606744527817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9292740046838408, f1=0.9197012138188609, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016936320753302425\n",
            "step: 10, loss: 0.0017768391408026218\n",
            "step: 20, loss: 0.0011564097367227077\n",
            "step: 30, loss: 0.0003067585057578981\n",
            "step: 40, loss: 0.027487298473715782\n",
            "step: 50, loss: 0.00012846343452110887\n",
            "step: 60, loss: 0.0018352663610130548\n",
            "step: 70, loss: 0.0010446715168654919\n",
            "step: 80, loss: 0.0004950986476615071\n",
            "step: 90, loss: 0.00042814022162929177\n",
            "step: 100, loss: 0.00012238803901709616\n",
            "step: 110, loss: 0.0036210371181368828\n",
            "step: 120, loss: 0.00028730204212479293\n",
            "step: 130, loss: 0.0003740586689673364\n",
            "step: 140, loss: 0.00016972301818896085\n",
            "step: 150, loss: 0.00023170662461780012\n",
            "step: 160, loss: 7.270366768352687e-05\n",
            "step: 170, loss: 0.0001479435886722058\n",
            "step: 180, loss: 0.0005932722124271095\n",
            "step: 190, loss: 0.001290147309191525\n",
            "step: 200, loss: 0.0003284814883954823\n",
            "step: 210, loss: 0.0003127541858702898\n",
            "step: 220, loss: 7.783529144944623e-05\n",
            "step: 230, loss: 0.00038276996929198503\n",
            "step: 240, loss: 9.365245932713151e-05\n",
            "step: 250, loss: 0.001259015523828566\n",
            "step: 260, loss: 0.00015692257147748023\n",
            "step: 270, loss: 0.07226871699094772\n",
            "step: 280, loss: 0.06425037235021591\n",
            "step: 290, loss: 0.0015931092202663422\n",
            "step: 300, loss: 0.0020855963230133057\n",
            "step: 310, loss: 0.004794291220605373\n",
            "step: 320, loss: 0.017461709678173065\n",
            "step: 330, loss: 0.004231874365359545\n",
            "step: 340, loss: 0.01350106205791235\n",
            "step: 350, loss: 0.000209203120903112\n",
            "step: 360, loss: 0.0007482976652681828\n",
            "step: 370, loss: 0.00044116380740888417\n",
            "step: 380, loss: 0.006304113194346428\n",
            "step: 390, loss: 0.00045896461233496666\n",
            "step: 400, loss: 0.0005511585623025894\n",
            "step: 410, loss: 0.007429542951285839\n",
            "step: 420, loss: 0.0023654478136450052\n",
            "step: 430, loss: 0.0002146952028851956\n",
            "step: 440, loss: 0.015528281219303608\n",
            "step: 450, loss: 0.022986235097050667\n",
            "step: 460, loss: 0.0003616886679083109\n",
            "step: 470, loss: 0.001961720874533057\n",
            "step: 480, loss: 0.037142314016819\n",
            "step: 490, loss: 0.004637132398784161\n",
            "step: 500, loss: 0.000760572322178632\n",
            "step: 510, loss: 0.0009669058490544558\n",
            "step: 520, loss: 0.002510347403585911\n",
            "step: 530, loss: 0.0006889255600981414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9344569288389513, f1=0.9273743016759776, best_f1=0.9272137227630968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040390403592027724\n",
            "step: 10, loss: 0.0012508867075666785\n",
            "step: 20, loss: 0.0003066679055336863\n",
            "step: 30, loss: 0.0008388845017179847\n",
            "step: 40, loss: 0.00034899963065981865\n",
            "step: 50, loss: 0.012862395495176315\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.00043983079376630485\n",
            "step: 70, loss: 0.0019150185398757458\n",
            "step: 80, loss: 0.005262686405330896\n",
            "step: 90, loss: 0.0015686313854530454\n",
            "step: 100, loss: 0.00012585727381519973\n",
            "step: 110, loss: 0.0002930688497144729\n",
            "step: 120, loss: 0.00022717878164257854\n",
            "step: 130, loss: 0.04022463038563728\n",
            "step: 140, loss: 0.0006436927942559123\n",
            "step: 150, loss: 0.007982085458934307\n",
            "step: 160, loss: 0.0007033630390651524\n",
            "step: 170, loss: 0.011321288533508778\n",
            "step: 180, loss: 0.00018276067567057908\n",
            "step: 190, loss: 0.003436551196500659\n",
            "step: 200, loss: 0.0010426456574350595\n",
            "step: 210, loss: 0.0016661732224747539\n",
            "step: 220, loss: 0.002261901507154107\n",
            "step: 230, loss: 5.906377555220388e-05\n",
            "step: 240, loss: 7.716403342783451e-05\n",
            "step: 250, loss: 0.00018538469157647341\n",
            "step: 260, loss: 0.00015389721374958754\n",
            "step: 270, loss: 0.0007102079107426107\n",
            "step: 280, loss: 0.15041133761405945\n",
            "step: 290, loss: 8.635130507173017e-05\n",
            "step: 300, loss: 0.0007915357127785683\n",
            "step: 310, loss: 0.0008638535509817302\n",
            "step: 320, loss: 0.005249197129160166\n",
            "step: 330, loss: 0.00026694100233726203\n",
            "step: 340, loss: 8.421535312663764e-05\n",
            "step: 350, loss: 8.863227412803099e-05\n",
            "step: 360, loss: 4.762049138662405e-05\n",
            "step: 370, loss: 0.009148281998932362\n",
            "step: 380, loss: 0.0008901027613319457\n",
            "step: 390, loss: 0.1392165571451187\n",
            "step: 400, loss: 0.0037792143411934376\n",
            "step: 410, loss: 0.00013547635171562433\n",
            "step: 420, loss: 0.0009475798578932881\n",
            "step: 430, loss: 0.006875597406178713\n",
            "step: 440, loss: 0.010061689652502537\n",
            "step: 450, loss: 0.0715487077832222\n",
            "step: 460, loss: 0.002626709872856736\n",
            "step: 470, loss: 0.003130408236756921\n",
            "step: 480, loss: 0.000501624250318855\n",
            "step: 490, loss: 0.007215987890958786\n",
            "step: 500, loss: 0.00040152863948605955\n",
            "step: 510, loss: 0.0002896194637287408\n",
            "step: 520, loss: 0.008976859971880913\n",
            "step: 530, loss: 4.169771636952646e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9365760610395804, f1=0.9182990922121358, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011186351766809821\n",
            "step: 10, loss: 5.2770723414141685e-05\n",
            "step: 20, loss: 0.00017828747513704002\n",
            "step: 30, loss: 0.0023996697273105383\n",
            "step: 40, loss: 6.215899338712916e-05\n",
            "step: 50, loss: 0.00014181082951836288\n",
            "step: 60, loss: 5.381164373829961e-05\n",
            "step: 70, loss: 0.0072986832819879055\n",
            "step: 80, loss: 0.0004186795558780432\n",
            "step: 90, loss: 9.923920879373327e-05\n",
            "step: 100, loss: 0.00012999399041291326\n",
            "step: 110, loss: 9.73199712461792e-05\n",
            "step: 120, loss: 0.00036376024945639074\n",
            "step: 130, loss: 0.03690587356686592\n",
            "step: 140, loss: 0.0008125274325720966\n",
            "step: 150, loss: 0.0024481110740453005\n",
            "step: 160, loss: 0.0024127590004354715\n",
            "step: 170, loss: 0.008649058640003204\n",
            "step: 180, loss: 7.39467068342492e-05\n",
            "step: 190, loss: 7.719209679635242e-05\n",
            "step: 200, loss: 0.007457563187927008\n",
            "step: 210, loss: 0.00010988880967488512\n",
            "step: 220, loss: 0.00015618535690009594\n",
            "step: 230, loss: 7.355539128184319e-05\n",
            "step: 240, loss: 0.02644377388060093\n",
            "step: 250, loss: 0.002143742283806205\n",
            "step: 260, loss: 0.010439714416861534\n",
            "step: 270, loss: 5.131130092195235e-05\n",
            "step: 280, loss: 0.0013499314663931727\n",
            "step: 290, loss: 0.005442557856440544\n",
            "step: 300, loss: 0.00020636389672290534\n",
            "step: 310, loss: 0.012571691535413265\n",
            "step: 320, loss: 0.0008294770959764719\n",
            "step: 330, loss: 0.0004383068298920989\n",
            "step: 340, loss: 0.00023078659432940185\n",
            "step: 350, loss: 0.00014859951625112444\n",
            "step: 360, loss: 4.52155145467259e-05\n",
            "step: 370, loss: 0.00020609673811122775\n",
            "step: 380, loss: 0.0005565829342231154\n",
            "step: 390, loss: 0.0001414047583239153\n",
            "step: 400, loss: 0.002213717671111226\n",
            "step: 410, loss: 0.00012286726268939674\n",
            "step: 420, loss: 0.00011076685041189194\n",
            "step: 430, loss: 0.0007056518807075918\n",
            "step: 440, loss: 0.00033520793658681214\n",
            "step: 450, loss: 0.00013420585310086608\n",
            "step: 460, loss: 0.00010155110794585198\n",
            "step: 470, loss: 6.63664031890221e-05\n",
            "step: 480, loss: 5.751186836278066e-05\n",
            "step: 490, loss: 4.837114829570055e-05\n",
            "step: 500, loss: 0.001280138036236167\n",
            "step: 510, loss: 0.0001685680908849463\n",
            "step: 520, loss: 5.622846947517246e-05\n",
            "step: 530, loss: 3.723657573573291e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9333954354913834, f1=0.9190697674418604, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017394457245245576\n",
            "step: 10, loss: 4.08478663302958e-05\n",
            "step: 20, loss: 2.9246441044961102e-05\n",
            "step: 30, loss: 4.510571307037026e-05\n",
            "step: 40, loss: 0.004033718258142471\n",
            "step: 50, loss: 0.0002250542165711522\n",
            "step: 60, loss: 4.76178975077346e-05\n",
            "step: 70, loss: 5.504829823621549e-05\n",
            "step: 80, loss: 6.963252963032573e-05\n",
            "step: 90, loss: 7.713835657341406e-05\n",
            "step: 100, loss: 3.12656193273142e-05\n",
            "step: 110, loss: 0.00026002994854934514\n",
            "step: 120, loss: 5.08709235873539e-05\n",
            "step: 130, loss: 0.0001313033135375008\n",
            "step: 140, loss: 0.00014214224938768893\n",
            "step: 150, loss: 0.0005620788433589041\n",
            "step: 160, loss: 8.671922114444897e-05\n",
            "step: 170, loss: 0.00040948003879748285\n",
            "step: 180, loss: 6.718875374644995e-05\n",
            "step: 190, loss: 6.0241163737373427e-05\n",
            "step: 200, loss: 0.00031455254065804183\n",
            "step: 210, loss: 0.00015773935592733324\n",
            "step: 220, loss: 0.0007074982859194279\n",
            "step: 230, loss: 5.62215136596933e-05\n",
            "step: 240, loss: 7.278079283423722e-05\n",
            "step: 250, loss: 3.849815402645618e-05\n",
            "step: 260, loss: 0.0005067503661848605\n",
            "step: 270, loss: 0.0003540661418810487\n",
            "step: 280, loss: 0.0004428609390743077\n",
            "step: 290, loss: 5.4140058637131006e-05\n",
            "step: 300, loss: 4.237448229105212e-05\n",
            "step: 310, loss: 2.9201879442553036e-05\n",
            "step: 320, loss: 0.006374639458954334\n",
            "step: 330, loss: 2.4422441128990613e-05\n",
            "step: 340, loss: 0.003272698726505041\n",
            "step: 350, loss: 0.00017164355085697025\n",
            "step: 360, loss: 0.00023345084628090262\n",
            "step: 370, loss: 0.000378690951038152\n",
            "step: 380, loss: 8.865267591318116e-05\n",
            "step: 390, loss: 0.013853328302502632\n",
            "step: 400, loss: 0.0002288996911374852\n",
            "step: 410, loss: 0.0062864674255251884\n",
            "step: 420, loss: 0.000358732882887125\n",
            "step: 430, loss: 6.661711813649163e-05\n",
            "step: 440, loss: 0.0009681205847300589\n",
            "step: 450, loss: 4.37651397078298e-05\n",
            "step: 460, loss: 7.874359289417043e-05\n",
            "step: 470, loss: 9.780227992450818e-05\n",
            "step: 480, loss: 5.1004113629460335e-05\n",
            "step: 490, loss: 8.027883450267836e-05\n",
            "step: 500, loss: 0.0002152318338630721\n",
            "step: 510, loss: 5.5209253332577646e-05\n",
            "step: 520, loss: 0.00017012089665513486\n",
            "step: 530, loss: 0.0012972523691132665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9340866290018832, f1=0.9222065063649223, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027342357207089663\n",
            "step: 10, loss: 5.3870127885602415e-05\n",
            "step: 20, loss: 4.317060665925965e-05\n",
            "step: 30, loss: 4.412170164869167e-05\n",
            "step: 40, loss: 7.977334462339059e-05\n",
            "step: 50, loss: 8.056686783675104e-05\n",
            "step: 60, loss: 0.0012600923655554652\n",
            "step: 70, loss: 0.00019471465202514082\n",
            "step: 80, loss: 4.542526221484877e-05\n",
            "step: 90, loss: 0.0001430870033800602\n",
            "step: 100, loss: 0.0005008465377613902\n",
            "step: 110, loss: 7.836832810426131e-05\n",
            "step: 120, loss: 0.0002217122819274664\n",
            "step: 130, loss: 0.00011003485269611701\n",
            "step: 140, loss: 7.543046376667917e-05\n",
            "step: 150, loss: 4.332789467298426e-05\n",
            "step: 160, loss: 2.9231345251901075e-05\n",
            "step: 170, loss: 2.9052936952211894e-05\n",
            "step: 180, loss: 0.005519400350749493\n",
            "step: 190, loss: 0.004197920206934214\n",
            "step: 200, loss: 0.0014616854023188353\n",
            "step: 210, loss: 0.0002201780880568549\n",
            "step: 220, loss: 0.001126280054450035\n",
            "step: 230, loss: 5.869208325748332e-05\n",
            "step: 240, loss: 0.00028988183476030827\n",
            "step: 250, loss: 2.2254469513427466e-05\n",
            "step: 260, loss: 4.587876901496202e-05\n",
            "step: 270, loss: 6.746464350726455e-05\n",
            "step: 280, loss: 4.105461266590282e-05\n",
            "step: 290, loss: 0.0004727281047962606\n",
            "step: 300, loss: 7.211006595753133e-05\n",
            "step: 310, loss: 3.818251934717409e-05\n",
            "step: 320, loss: 7.657417882001027e-05\n",
            "step: 330, loss: 5.571676592808217e-05\n",
            "step: 340, loss: 0.005880813580006361\n",
            "step: 350, loss: 6.49440917186439e-05\n",
            "step: 360, loss: 0.00011049892054870725\n",
            "step: 370, loss: 0.0010352933313697577\n",
            "step: 380, loss: 6.191047577885911e-05\n",
            "step: 390, loss: 8.686474757269025e-05\n",
            "step: 400, loss: 5.803240492241457e-05\n",
            "step: 410, loss: 3.8302114262478426e-05\n",
            "step: 420, loss: 6.446677434723824e-05\n",
            "step: 430, loss: 0.0001432519347872585\n",
            "step: 440, loss: 0.00025766450562514365\n",
            "step: 450, loss: 7.09254527464509e-05\n",
            "step: 460, loss: 0.030401429161429405\n",
            "step: 470, loss: 3.125400689896196e-05\n",
            "step: 480, loss: 0.00011766709212679416\n",
            "step: 490, loss: 0.0005571133806370199\n",
            "step: 500, loss: 0.00043077568989247084\n",
            "step: 510, loss: 8.485165744787082e-05\n",
            "step: 520, loss: 5.752086144639179e-05\n",
            "step: 530, loss: 0.010567037388682365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9311320754716981, f1=0.913556920170052, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.1879753805696964e-05\n",
            "step: 10, loss: 2.0403138478286564e-05\n",
            "step: 20, loss: 0.00041309266816824675\n",
            "step: 30, loss: 0.0013160220114514232\n",
            "step: 40, loss: 7.709916826570407e-05\n",
            "step: 50, loss: 5.0484224630054086e-05\n",
            "step: 60, loss: 0.00029329099925234914\n",
            "step: 70, loss: 0.0002452540211379528\n",
            "step: 80, loss: 0.002330988412722945\n",
            "step: 90, loss: 0.00014332152204588056\n",
            "step: 100, loss: 6.334352656267583e-05\n",
            "step: 110, loss: 0.00018050607468467206\n",
            "step: 120, loss: 2.3580225388286635e-05\n",
            "step: 130, loss: 4.5169963414082304e-05\n",
            "step: 140, loss: 3.7663881812477484e-05\n",
            "step: 150, loss: 7.265512977028266e-05\n",
            "step: 160, loss: 2.4701899747014977e-05\n",
            "step: 170, loss: 2.9697246645810083e-05\n",
            "step: 180, loss: 3.4625256375875324e-05\n",
            "step: 190, loss: 4.7287863708334044e-05\n",
            "step: 200, loss: 6.000322900945321e-05\n",
            "step: 210, loss: 0.00016706521273590624\n",
            "step: 220, loss: 3.281503086327575e-05\n",
            "step: 230, loss: 2.0637806301238015e-05\n",
            "step: 240, loss: 0.00018433821969665587\n",
            "step: 250, loss: 2.2403424736694433e-05\n",
            "step: 260, loss: 3.104880670434795e-05\n",
            "step: 270, loss: 0.0033879110123962164\n",
            "step: 280, loss: 0.00026323599740862846\n",
            "step: 290, loss: 6.518009467981756e-05\n",
            "step: 300, loss: 7.866616942919791e-05\n",
            "step: 310, loss: 3.44087638950441e-05\n",
            "step: 320, loss: 1.9006149159395136e-05\n",
            "step: 330, loss: 0.00020936888176947832\n",
            "step: 340, loss: 2.937238423328381e-05\n",
            "step: 350, loss: 5.925962977926247e-05\n",
            "step: 360, loss: 0.00012058016000082716\n",
            "step: 370, loss: 3.403632581466809e-05\n",
            "step: 380, loss: 2.564808528404683e-05\n",
            "step: 390, loss: 0.06684788316488266\n",
            "step: 400, loss: 5.1820639782818034e-05\n",
            "step: 410, loss: 2.450409738230519e-05\n",
            "step: 420, loss: 3.8755715650040656e-05\n",
            "step: 430, loss: 3.336540248710662e-05\n",
            "step: 440, loss: 3.286932042101398e-05\n",
            "step: 450, loss: 3.126497540506534e-05\n",
            "step: 460, loss: 1.8022719814325683e-05\n",
            "step: 470, loss: 2.35354746109806e-05\n",
            "step: 480, loss: 7.581948739243671e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 490, loss: 0.00015606862143613398\n",
            "step: 500, loss: 0.00014785297389607877\n",
            "step: 510, loss: 0.004302083980292082\n",
            "step: 520, loss: 2.629238042572979e-05\n",
            "step: 530, loss: 0.0527227520942688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9332706766917293, f1=0.9194756554307116, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019448879174888134\n",
            "step: 10, loss: 5.273127317195758e-05\n",
            "step: 20, loss: 5.725146547774784e-05\n",
            "step: 30, loss: 2.9712047762586735e-05\n",
            "step: 40, loss: 3.659473077277653e-05\n",
            "step: 50, loss: 0.00011259539314778522\n",
            "step: 60, loss: 4.321780215832405e-05\n",
            "step: 70, loss: 0.00012042629532516003\n",
            "step: 80, loss: 2.8195829145261087e-05\n",
            "step: 90, loss: 0.0002210403181379661\n",
            "step: 100, loss: 3.051640851481352e-05\n",
            "step: 110, loss: 0.00018499558791518211\n",
            "step: 120, loss: 5.44606227776967e-05\n",
            "step: 130, loss: 2.5875122446450405e-05\n",
            "step: 140, loss: 0.0017187207704409957\n",
            "step: 150, loss: 6.000500434311107e-05\n",
            "step: 160, loss: 3.8930043956497684e-05\n",
            "step: 170, loss: 0.00011927523883059621\n",
            "step: 180, loss: 5.129589408170432e-05\n",
            "step: 190, loss: 0.0006644694949500263\n",
            "step: 200, loss: 0.00031220263917930424\n",
            "step: 210, loss: 2.9436341719701886e-05\n",
            "step: 220, loss: 2.3487429643864743e-05\n",
            "step: 230, loss: 8.778498886385933e-05\n",
            "step: 240, loss: 0.025366811081767082\n",
            "step: 250, loss: 6.611204298678786e-05\n",
            "step: 260, loss: 3.353271313244477e-05\n",
            "step: 270, loss: 3.079898306168616e-05\n",
            "step: 280, loss: 3.79257726308424e-05\n",
            "step: 290, loss: 6.945554923731834e-05\n",
            "step: 300, loss: 8.277945016743615e-05\n",
            "step: 310, loss: 0.008196452632546425\n",
            "step: 320, loss: 0.0026862272061407566\n",
            "step: 330, loss: 0.001310506951995194\n",
            "step: 340, loss: 0.000143088647746481\n",
            "step: 350, loss: 6.381948333000764e-05\n",
            "step: 360, loss: 6.739344098605216e-05\n",
            "step: 370, loss: 0.00020318813039921224\n",
            "step: 380, loss: 0.0010298279812559485\n",
            "step: 390, loss: 6.380579725373536e-05\n",
            "step: 400, loss: 0.00021134479902684689\n",
            "step: 410, loss: 0.00011270544928265736\n",
            "step: 420, loss: 4.3529966205824167e-05\n",
            "step: 430, loss: 6.686478445772082e-05\n",
            "step: 440, loss: 7.803340122336522e-05\n",
            "step: 450, loss: 0.00013601151295006275\n",
            "step: 460, loss: 0.0007039060583338141\n",
            "step: 470, loss: 4.173777051619254e-05\n",
            "step: 480, loss: 2.7104437322122976e-05\n",
            "step: 490, loss: 0.00016055155720096081\n",
            "step: 500, loss: 1.786620487109758e-05\n",
            "step: 510, loss: 6.885619222884998e-05\n",
            "step: 520, loss: 0.0004826945951208472\n",
            "step: 530, loss: 3.0549243092536926e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9319271332694151, f1=0.9091787439613526, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.234426786657423e-05\n",
            "step: 10, loss: 2.002284054469783e-05\n",
            "step: 20, loss: 2.7722559025278315e-05\n",
            "step: 30, loss: 0.0001440558407921344\n",
            "step: 40, loss: 5.1083708967780694e-05\n",
            "step: 50, loss: 0.00024026080791372806\n",
            "step: 60, loss: 2.752127511485014e-05\n",
            "step: 70, loss: 4.0378225094173104e-05\n",
            "step: 80, loss: 5.906034493818879e-05\n",
            "step: 90, loss: 7.763072790112346e-05\n",
            "step: 100, loss: 0.00014803174417465925\n",
            "step: 110, loss: 7.033316796878353e-05\n",
            "step: 120, loss: 8.73956159921363e-05\n",
            "step: 130, loss: 0.00016287424659822136\n",
            "step: 140, loss: 2.7126432541990653e-05\n",
            "step: 150, loss: 8.625135524198413e-05\n",
            "step: 160, loss: 4.191449988866225e-05\n",
            "step: 170, loss: 4.357858415460214e-05\n",
            "step: 180, loss: 2.819169094436802e-05\n",
            "step: 190, loss: 0.00010242173448204994\n",
            "step: 200, loss: 0.00017756350280251354\n",
            "step: 210, loss: 2.8702308554784395e-05\n",
            "step: 220, loss: 2.400515768385958e-05\n",
            "step: 230, loss: 3.5533848858904094e-05\n",
            "step: 240, loss: 2.9629623895743862e-05\n",
            "step: 250, loss: 0.00038724884507246315\n",
            "step: 260, loss: 2.9760236429865472e-05\n",
            "step: 270, loss: 2.559942367952317e-05\n",
            "step: 280, loss: 2.0034256522194482e-05\n",
            "step: 290, loss: 0.00013978421338833869\n",
            "step: 300, loss: 4.3723444832721725e-05\n",
            "step: 310, loss: 0.0006004479364491999\n",
            "step: 320, loss: 0.00015254398749675602\n",
            "step: 330, loss: 6.364625005517155e-05\n",
            "step: 340, loss: 3.342596028232947e-05\n",
            "step: 350, loss: 7.049906707834452e-05\n",
            "step: 360, loss: 0.00014691676187794656\n",
            "step: 370, loss: 0.00043972552521154284\n",
            "step: 380, loss: 4.381880353321321e-05\n",
            "step: 390, loss: 0.03467344492673874\n",
            "step: 400, loss: 0.0008444760460406542\n",
            "step: 410, loss: 0.00025138119235634804\n",
            "step: 420, loss: 3.338713213452138e-05\n",
            "step: 430, loss: 0.00045610294910147786\n",
            "step: 440, loss: 7.940283830976114e-05\n",
            "step: 450, loss: 0.00011491176701383665\n",
            "step: 460, loss: 8.377804624615237e-05\n",
            "step: 470, loss: 5.7138080592267215e-05\n",
            "step: 480, loss: 3.8744859921280295e-05\n",
            "step: 490, loss: 4.3352665670681745e-05\n",
            "step: 500, loss: 3.8322930777212605e-05\n",
            "step: 510, loss: 0.0006206388934515417\n",
            "step: 520, loss: 3.2441657822346315e-05\n",
            "step: 530, loss: 8.565666939830408e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9338446788111218, f1=0.9165067178502879, best_f1=0.9182990922121358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.7391162802814506e-05\n",
            "step: 10, loss: 3.8738005969207734e-05\n",
            "step: 20, loss: 0.00012495825649239123\n",
            "step: 30, loss: 0.00016951403813436627\n",
            "step: 40, loss: 0.06681101024150848\n",
            "step: 50, loss: 0.00010557743371464312\n",
            "step: 60, loss: 1.579874697199557e-05\n",
            "step: 70, loss: 7.924111559987068e-05\n",
            "step: 80, loss: 7.739248394500464e-05\n",
            "step: 90, loss: 3.347011806908995e-05\n",
            "step: 100, loss: 8.664101187605411e-05\n",
            "step: 110, loss: 0.0005122966831550002\n",
            "step: 120, loss: 0.00041538619552738965\n",
            "step: 130, loss: 0.0070480480790138245\n",
            "step: 140, loss: 2.8437512810342014e-05\n",
            "step: 150, loss: 0.00023115449585020542\n",
            "step: 160, loss: 2.3230277292896062e-05\n",
            "step: 170, loss: 4.891716525889933e-05\n",
            "step: 180, loss: 1.87042878678767e-05\n",
            "step: 190, loss: 5.7657231081975624e-05\n",
            "step: 200, loss: 3.224020838388242e-05\n",
            "step: 210, loss: 0.000647236593067646\n",
            "step: 220, loss: 2.0037874492118135e-05\n",
            "step: 230, loss: 0.0011330011766403913\n",
            "step: 240, loss: 0.00019822495232801884\n",
            "step: 250, loss: 3.234888208680786e-05\n",
            "step: 260, loss: 7.335448754020035e-05\n",
            "step: 270, loss: 2.9755989089608192e-05\n",
            "step: 280, loss: 0.0003443742170929909\n",
            "step: 290, loss: 7.206398731796071e-05\n",
            "step: 300, loss: 2.1475734683917835e-05\n",
            "step: 310, loss: 3.1574083550367504e-05\n",
            "step: 320, loss: 3.829310662695207e-05\n",
            "step: 330, loss: 0.0011340021155774593\n",
            "step: 340, loss: 1.8216302123619244e-05\n",
            "step: 350, loss: 2.4528850190108642e-05\n",
            "step: 360, loss: 0.0005182178574614227\n",
            "step: 370, loss: 9.943562326952815e-05\n",
            "step: 380, loss: 0.0002598853316158056\n",
            "step: 390, loss: 0.0008547154720872641\n",
            "step: 400, loss: 3.549905159161426e-05\n",
            "step: 410, loss: 2.406461317150388e-05\n",
            "step: 420, loss: 3.0151382816256955e-05\n",
            "step: 430, loss: 0.022699952125549316\n",
            "step: 440, loss: 5.0309965445194393e-05\n",
            "step: 450, loss: 0.00012688766582868993\n",
            "step: 460, loss: 3.2382289646193385e-05\n",
            "step: 470, loss: 0.002060920000076294\n",
            "step: 480, loss: 2.1330033632693812e-05\n",
            "step: 490, loss: 2.4757573555689305e-05\n",
            "step: 500, loss: 3.428530544624664e-05\n",
            "step: 510, loss: 6.172592838993296e-05\n",
            "step: 520, loss: 2.1457293769344687e-05\n",
            "step: 530, loss: 2.8538643164210953e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9355608591885441, f1=0.9190225203641591, best_f1=0.9182990922121358\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 235.40it/s]\n",
            "load_f1 = 0.9366130558183537\n",
            "real_f1 = 0.934916864608076\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 235.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e926592e-a543-4612-b21d-fadd547d8954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.563360333442688\n",
            "step: 10, loss: 0.3503831624984741\n",
            "step: 20, loss: 0.4105105698108673\n",
            "step: 30, loss: 0.3261139690876007\n",
            "step: 40, loss: 0.20120301842689514\n",
            "step: 50, loss: 0.4789356291294098\n",
            "step: 60, loss: 0.34300094842910767\n",
            "step: 70, loss: 0.2249271273612976\n",
            "step: 80, loss: 0.18367575109004974\n",
            "step: 90, loss: 0.400393545627594\n",
            "step: 100, loss: 0.41239821910858154\n",
            "step: 110, loss: 0.2505453824996948\n",
            "step: 120, loss: 0.2032642960548401\n",
            "step: 130, loss: 0.2150827944278717\n",
            "step: 140, loss: 0.22916072607040405\n",
            "step: 150, loss: 0.22421923279762268\n",
            "step: 160, loss: 0.23917867243289948\n",
            "step: 170, loss: 0.31388622522354126\n",
            "step: 180, loss: 0.117743581533432\n",
            "step: 190, loss: 0.2402559220790863\n",
            "step: 200, loss: 0.2509887218475342\n",
            "step: 210, loss: 0.25570595264434814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.540268456375839, f1=0.5800000000000001, best_f1=0.5800000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17664921283721924\n",
            "step: 10, loss: 0.27837231755256653\n",
            "step: 20, loss: 0.22109819948673248\n",
            "step: 30, loss: 0.25786665081977844\n",
            "step: 40, loss: 0.2745864987373352\n",
            "step: 50, loss: 0.21357087790966034\n",
            "step: 60, loss: 0.3675594925880432\n",
            "step: 70, loss: 0.11966989189386368\n",
            "step: 80, loss: 0.2251892387866974\n",
            "step: 90, loss: 0.10733044892549515\n",
            "step: 100, loss: 0.013442235998809338\n",
            "step: 110, loss: 0.11826404184103012\n",
            "step: 120, loss: 0.11911533027887344\n",
            "step: 130, loss: 0.04031048342585564\n",
            "step: 140, loss: 0.2616669237613678\n",
            "step: 150, loss: 0.193214550614357\n",
            "step: 160, loss: 0.194614976644516\n",
            "step: 170, loss: 0.11580502986907959\n",
            "step: 180, loss: 0.1965094953775406\n",
            "step: 190, loss: 0.22741568088531494\n",
            "step: 200, loss: 0.04976562783122063\n",
            "step: 210, loss: 0.12227512151002884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5963636363636364, f1=0.6329588014981273, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03005756437778473\n",
            "step: 10, loss: 0.21435211598873138\n",
            "step: 20, loss: 0.22298549115657806\n",
            "step: 30, loss: 0.05987906455993652\n",
            "step: 40, loss: 0.14741632342338562\n",
            "step: 50, loss: 0.1451779454946518\n",
            "step: 60, loss: 0.05692560598254204\n",
            "step: 70, loss: 0.08794037252664566\n",
            "step: 80, loss: 0.17160925269126892\n",
            "step: 90, loss: 0.03592577949166298\n",
            "step: 100, loss: 0.11533746123313904\n",
            "step: 110, loss: 0.1730012744665146\n",
            "step: 120, loss: 0.1340017169713974\n",
            "step: 130, loss: 0.17459821701049805\n",
            "step: 140, loss: 0.1265365481376648\n",
            "step: 150, loss: 0.15194495022296906\n",
            "step: 160, loss: 0.013118579052388668\n",
            "step: 170, loss: 0.1584625244140625\n",
            "step: 180, loss: 0.049523405730724335\n",
            "step: 190, loss: 0.20823487639427185\n",
            "step: 200, loss: 0.04815278947353363\n",
            "step: 210, loss: 0.1967179775238037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.5759162303664921, f1=0.6143344709897611, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08072014898061752\n",
            "step: 10, loss: 0.07716238498687744\n",
            "step: 20, loss: 0.03119313344359398\n",
            "step: 30, loss: 0.08885606378316879\n",
            "step: 40, loss: 0.042767200618982315\n",
            "step: 50, loss: 0.10173439234495163\n",
            "step: 60, loss: 0.10029321908950806\n",
            "step: 70, loss: 0.23678088188171387\n",
            "step: 80, loss: 0.127373605966568\n",
            "step: 90, loss: 0.015954140573740005\n",
            "step: 100, loss: 0.1958421915769577\n",
            "step: 110, loss: 0.07008136063814163\n",
            "step: 120, loss: 0.07413958758115768\n",
            "step: 130, loss: 0.09217330068349838\n",
            "step: 140, loss: 0.21308274567127228\n",
            "step: 150, loss: 0.03564954921603203\n",
            "step: 160, loss: 0.02336619235575199\n",
            "step: 170, loss: 0.0968162938952446\n",
            "step: 180, loss: 0.1843322366476059\n",
            "step: 190, loss: 0.09285930544137955\n",
            "step: 200, loss: 0.15167710185050964\n",
            "step: 210, loss: 0.1945231705904007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.5870841487279843, f1=0.6363636363636365, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11068297177553177\n",
            "step: 10, loss: 0.018535101786255836\n",
            "step: 20, loss: 0.04281897097826004\n",
            "step: 30, loss: 0.022144030779600143\n",
            "step: 40, loss: 0.025472577661275864\n",
            "step: 50, loss: 0.04114553704857826\n",
            "step: 60, loss: 0.054316021502017975\n",
            "step: 70, loss: 0.06000697240233421\n",
            "step: 80, loss: 0.1430298238992691\n",
            "step: 90, loss: 0.16454939544200897\n",
            "step: 100, loss: 0.002541961148381233\n",
            "step: 110, loss: 0.10301757603883743\n",
            "step: 120, loss: 0.058145541697740555\n",
            "step: 130, loss: 0.07875803858041763\n",
            "step: 140, loss: 0.10440153628587723\n",
            "step: 150, loss: 0.050497423857450485\n",
            "step: 160, loss: 0.1507968306541443\n",
            "step: 170, loss: 0.04902178794145584\n",
            "step: 180, loss: 0.053887102752923965\n",
            "step: 190, loss: 0.006940164603292942\n",
            "step: 200, loss: 0.15851211547851562\n",
            "step: 210, loss: 0.007250573951750994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5585215605749486, f1=0.6147540983606558, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020463505759835243\n",
            "step: 10, loss: 0.05030268803238869\n",
            "step: 20, loss: 0.0022997339256107807\n",
            "step: 30, loss: 0.0006486869533546269\n",
            "step: 40, loss: 0.09302273392677307\n",
            "step: 50, loss: 0.0035654231905937195\n",
            "step: 60, loss: 0.018637992441654205\n",
            "step: 70, loss: 0.08970974385738373\n",
            "step: 80, loss: 0.08950715512037277\n",
            "step: 90, loss: 0.194692924618721\n",
            "step: 100, loss: 0.032635707408189774\n",
            "step: 110, loss: 0.021840829402208328\n",
            "step: 120, loss: 0.026198435574769974\n",
            "step: 130, loss: 0.0856182873249054\n",
            "step: 140, loss: 0.022725556045770645\n",
            "step: 150, loss: 0.016028767451643944\n",
            "step: 160, loss: 0.006460533011704683\n",
            "step: 170, loss: 0.07752082496881485\n",
            "step: 180, loss: 0.012287558056414127\n",
            "step: 190, loss: 0.04696967825293541\n",
            "step: 200, loss: 0.004480244591832161\n",
            "step: 210, loss: 0.045179061591625214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5641025641025642, f1=0.6057906458797327, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005138831678777933\n",
            "step: 10, loss: 0.008256569504737854\n",
            "step: 20, loss: 0.011069356463849545\n",
            "step: 30, loss: 0.037517037242650986\n",
            "step: 40, loss: 0.0317300409078598\n",
            "step: 50, loss: 0.04713372141122818\n",
            "step: 60, loss: 0.009419750422239304\n",
            "step: 70, loss: 0.012600899673998356\n",
            "step: 80, loss: 0.02642858773469925\n",
            "step: 90, loss: 0.007969693280756474\n",
            "step: 100, loss: 0.003431286895647645\n",
            "step: 110, loss: 0.09477470815181732\n",
            "step: 120, loss: 0.06173530966043472\n",
            "step: 130, loss: 0.017004158347845078\n",
            "step: 140, loss: 0.0296366848051548\n",
            "step: 150, loss: 0.0418255478143692\n",
            "step: 160, loss: 0.013520305044949055\n",
            "step: 170, loss: 0.0235359538346529\n",
            "step: 180, loss: 0.026575801894068718\n",
            "step: 190, loss: 0.10815361887216568\n",
            "step: 200, loss: 0.05452549457550049\n",
            "step: 210, loss: 0.003038847353309393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5763440860215053, f1=0.6313645621181263, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00435477402061224\n",
            "step: 10, loss: 0.0425267219543457\n",
            "step: 20, loss: 0.002241868991404772\n",
            "step: 30, loss: 0.03269007056951523\n",
            "step: 40, loss: 0.019471708685159683\n",
            "step: 50, loss: 0.0021740440279245377\n",
            "step: 60, loss: 0.11257948726415634\n",
            "step: 70, loss: 0.021384116262197495\n",
            "step: 80, loss: 0.015801405534148216\n",
            "step: 90, loss: 0.09447604417800903\n",
            "step: 100, loss: 0.006964368280023336\n",
            "step: 110, loss: 0.04490488022565842\n",
            "step: 120, loss: 0.0010223345598205924\n",
            "step: 130, loss: 0.003523328574374318\n",
            "step: 140, loss: 0.09991740435361862\n",
            "step: 150, loss: 0.031559739261865616\n",
            "step: 160, loss: 0.008494120091199875\n",
            "step: 170, loss: 0.002063456689938903\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.12886695563793182\n",
            "step: 190, loss: 0.17302480340003967\n",
            "step: 200, loss: 0.0025183469988405704\n",
            "step: 210, loss: 0.04165862500667572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.579520697167756, f1=0.6322580645161291, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011400529183447361\n",
            "step: 10, loss: 0.004235875327140093\n",
            "step: 20, loss: 0.0007694929954595864\n",
            "step: 30, loss: 0.001286288839764893\n",
            "step: 40, loss: 0.05214017257094383\n",
            "step: 50, loss: 0.0005088350153528154\n",
            "step: 60, loss: 0.013371741399168968\n",
            "step: 70, loss: 0.0003622269432526082\n",
            "step: 80, loss: 0.0010336491977795959\n",
            "step: 90, loss: 0.0012933410471305251\n",
            "step: 100, loss: 0.023528529331088066\n",
            "step: 110, loss: 0.03459262102842331\n",
            "step: 120, loss: 0.0013804389163851738\n",
            "step: 130, loss: 0.005207001231610775\n",
            "step: 140, loss: 0.009093528613448143\n",
            "step: 150, loss: 0.022521112114191055\n",
            "step: 160, loss: 0.0006001211586408317\n",
            "step: 170, loss: 0.0009101805626414716\n",
            "step: 180, loss: 0.07498621195554733\n",
            "step: 190, loss: 0.00026070926105603576\n",
            "step: 200, loss: 0.0824221670627594\n",
            "step: 210, loss: 0.004690409172326326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5720524017467249, f1=0.6315789473684211, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011501987464725971\n",
            "step: 10, loss: 0.011343899182975292\n",
            "step: 20, loss: 0.001848881016485393\n",
            "step: 30, loss: 0.00961315631866455\n",
            "step: 40, loss: 0.010896777734160423\n",
            "step: 50, loss: 0.0010800468735396862\n",
            "step: 60, loss: 0.038900237530469894\n",
            "step: 70, loss: 0.005916444119066\n",
            "step: 80, loss: 0.10831131041049957\n",
            "step: 90, loss: 0.013077495619654655\n",
            "step: 100, loss: 0.03601592034101486\n",
            "step: 110, loss: 0.0002706335799302906\n",
            "step: 120, loss: 0.0015133823035284877\n",
            "step: 130, loss: 0.004909981973469257\n",
            "step: 140, loss: 0.0006069550872780383\n",
            "step: 150, loss: 0.003982912749052048\n",
            "step: 160, loss: 0.0008774717571213841\n",
            "step: 170, loss: 0.0008841177332215011\n",
            "step: 180, loss: 0.018130086362361908\n",
            "step: 190, loss: 0.17782141268253326\n",
            "step: 200, loss: 0.007179387379437685\n",
            "step: 210, loss: 0.09476633369922638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5783664459161149, f1=0.6411889596602973, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01110947597771883\n",
            "step: 10, loss: 0.04825994372367859\n",
            "step: 20, loss: 0.00614469638094306\n",
            "step: 30, loss: 0.004864180460572243\n",
            "step: 40, loss: 0.021005835384130478\n",
            "step: 50, loss: 0.06448047608137131\n",
            "step: 60, loss: 0.012686456553637981\n",
            "step: 70, loss: 0.03266415372490883\n",
            "step: 80, loss: 0.026104165241122246\n",
            "step: 90, loss: 0.019069209694862366\n",
            "step: 100, loss: 0.055845096707344055\n",
            "step: 110, loss: 0.052059076726436615\n",
            "step: 120, loss: 0.014618927612900734\n",
            "step: 130, loss: 0.00560676259920001\n",
            "step: 140, loss: 0.0005820785299874842\n",
            "step: 150, loss: 0.00022644006821792573\n",
            "step: 160, loss: 0.02425299771130085\n",
            "step: 170, loss: 0.038971733301877975\n",
            "step: 180, loss: 0.0007057894836179912\n",
            "step: 190, loss: 0.0007205406436696649\n",
            "step: 200, loss: 0.006359575316309929\n",
            "step: 210, loss: 0.0010351981036365032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5663716814159292, f1=0.6458333333333333, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.053702957928180695\n",
            "step: 10, loss: 0.00017244499758817255\n",
            "step: 20, loss: 0.004252160899341106\n",
            "step: 30, loss: 0.0025366938207298517\n",
            "step: 40, loss: 0.005107346456497908\n",
            "step: 50, loss: 0.0008793707238510251\n",
            "step: 60, loss: 0.0014618179993703961\n",
            "step: 70, loss: 0.0017720786854624748\n",
            "step: 80, loss: 0.00042871941695921123\n",
            "step: 90, loss: 0.003383324481546879\n",
            "step: 100, loss: 0.007137601263821125\n",
            "step: 110, loss: 0.00034173380117863417\n",
            "step: 120, loss: 0.0023642953019589186\n",
            "step: 130, loss: 6.57185009913519e-05\n",
            "step: 140, loss: 0.00041210404015146196\n",
            "step: 150, loss: 0.0006515298737213016\n",
            "step: 160, loss: 0.004012248013168573\n",
            "step: 170, loss: 0.030026167631149292\n",
            "step: 180, loss: 0.002062223618850112\n",
            "step: 190, loss: 0.014580950140953064\n",
            "step: 200, loss: 0.0005191928939893842\n",
            "step: 210, loss: 0.008644786663353443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5581395348837209, f1=0.609271523178808, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005174342077225447\n",
            "step: 10, loss: 9.903189493343234e-05\n",
            "step: 20, loss: 0.01143078226596117\n",
            "step: 30, loss: 0.03384726494550705\n",
            "step: 40, loss: 0.0007055692258290946\n",
            "step: 50, loss: 0.0004784493939951062\n",
            "step: 60, loss: 0.00019864653586409986\n",
            "step: 70, loss: 0.0019857981242239475\n",
            "step: 80, loss: 0.0010491838911548257\n",
            "step: 90, loss: 0.0002400741068413481\n",
            "step: 100, loss: 0.0013114864705130458\n",
            "step: 110, loss: 0.00029726704815402627\n",
            "step: 120, loss: 0.00018670885765459388\n",
            "step: 130, loss: 0.014299076050519943\n",
            "step: 140, loss: 0.004616493359208107\n",
            "step: 150, loss: 0.017269032076001167\n",
            "step: 160, loss: 0.03275209292769432\n",
            "step: 170, loss: 0.0028466694056987762\n",
            "step: 180, loss: 0.012071754783391953\n",
            "step: 190, loss: 0.0003674958716146648\n",
            "step: 200, loss: 0.0007314502145163715\n",
            "step: 210, loss: 0.005525875836610794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5605381165919282, f1=0.6111111111111112, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.960497845895588e-05\n",
            "step: 10, loss: 0.0650465115904808\n",
            "step: 20, loss: 0.0001306365302298218\n",
            "step: 30, loss: 0.032783206552267075\n",
            "step: 40, loss: 0.00018337820074521005\n",
            "step: 50, loss: 0.018666144460439682\n",
            "step: 60, loss: 9.595046140020713e-05\n",
            "step: 70, loss: 0.00010438133176648989\n",
            "step: 80, loss: 0.00036616038414649665\n",
            "step: 90, loss: 0.008816235698759556\n",
            "step: 100, loss: 0.0031616908963769674\n",
            "step: 110, loss: 0.0008806888945400715\n",
            "step: 120, loss: 0.0002182106691179797\n",
            "step: 130, loss: 0.0037130825221538544\n",
            "step: 140, loss: 0.002939556958153844\n",
            "step: 150, loss: 0.00035331628168933094\n",
            "step: 160, loss: 0.00017482455587014556\n",
            "step: 170, loss: 0.0001441782369511202\n",
            "step: 180, loss: 0.0020997868850827217\n",
            "step: 190, loss: 0.0002545146271586418\n",
            "step: 200, loss: 0.00045864927233196795\n",
            "step: 210, loss: 0.0006936974823474884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5540540540540541, f1=0.6226012793176973, best_f1=0.6329588014981273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001240027922904119\n",
            "step: 10, loss: 0.0046764821745455265\n",
            "step: 20, loss: 0.0007616077200509608\n",
            "step: 30, loss: 0.005556656047701836\n",
            "step: 40, loss: 0.0006691452581435442\n",
            "step: 50, loss: 0.0002182352327508852\n",
            "step: 60, loss: 0.000779944472014904\n",
            "step: 70, loss: 0.0019263973226770759\n",
            "step: 80, loss: 0.00033050504862330854\n",
            "step: 90, loss: 0.021520795300602913\n",
            "step: 100, loss: 0.0003443534660618752\n",
            "step: 110, loss: 0.00017763575306162238\n",
            "step: 120, loss: 0.00024243525695055723\n",
            "step: 130, loss: 0.0030066706240177155\n",
            "step: 140, loss: 0.00031715244404040277\n",
            "step: 150, loss: 0.0002593204553704709\n",
            "step: 160, loss: 0.012510450556874275\n",
            "step: 170, loss: 0.0004771027306560427\n",
            "step: 180, loss: 0.00018896773690357804\n",
            "step: 190, loss: 0.00011549473856575787\n",
            "step: 200, loss: 0.003694762010127306\n",
            "step: 210, loss: 0.0008528839680366218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5585585585585586, f1=0.6268656716417911, best_f1=0.6329588014981273\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 403.38it/s]\n",
            "load_f1 = 0.582120582120582\n",
            "real_f1 = 0.5793991416309012\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 227.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1b4188-ffa3-4858-81a0-70e34750f5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5452269315719604\n",
            "step: 10, loss: 0.36817097663879395\n",
            "step: 20, loss: 0.28986942768096924\n",
            "step: 30, loss: 0.45134422183036804\n",
            "step: 40, loss: 0.4287320077419281\n",
            "step: 50, loss: 0.30328235030174255\n",
            "step: 60, loss: 0.2633335590362549\n",
            "step: 70, loss: 0.2903966009616852\n",
            "step: 80, loss: 0.23985759913921356\n",
            "step: 90, loss: 0.31227874755859375\n",
            "step: 100, loss: 0.32209303975105286\n",
            "step: 110, loss: 0.37393274903297424\n",
            "step: 120, loss: 0.2303493469953537\n",
            "step: 130, loss: 0.17246727645397186\n",
            "step: 140, loss: 0.04297035560011864\n",
            "step: 150, loss: 0.17568805813789368\n",
            "step: 160, loss: 0.15779371559619904\n",
            "step: 170, loss: 0.17679567635059357\n",
            "step: 180, loss: 0.04662597179412842\n",
            "step: 190, loss: 0.21532569825649261\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6683804627249357, f1=0.645, best_f1=0.645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2817363142967224\n",
            "step: 10, loss: 0.05807296931743622\n",
            "step: 20, loss: 0.2248144894838333\n",
            "step: 30, loss: 0.14834201335906982\n",
            "step: 40, loss: 0.22870375216007233\n",
            "step: 50, loss: 0.08230677247047424\n",
            "step: 60, loss: 0.2019953578710556\n",
            "step: 70, loss: 0.1251639574766159\n",
            "step: 80, loss: 0.2000456601381302\n",
            "step: 90, loss: 0.33416107296943665\n",
            "step: 100, loss: 0.026996886357665062\n",
            "step: 110, loss: 0.32853156328201294\n",
            "step: 120, loss: 0.21320727467536926\n",
            "step: 130, loss: 0.10905258357524872\n",
            "step: 140, loss: 0.044592246413230896\n",
            "step: 150, loss: 0.10344403982162476\n",
            "step: 160, loss: 0.0951179563999176\n",
            "step: 170, loss: 0.14392924308776855\n",
            "step: 180, loss: 0.2426450103521347\n",
            "step: 190, loss: 0.06577770411968231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7396449704142012, f1=0.7457627118644068, best_f1=0.7457627118644068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10509008169174194\n",
            "step: 10, loss: 0.040476422756910324\n",
            "step: 20, loss: 0.13620203733444214\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.04464838281273842\n",
            "step: 40, loss: 0.036566730588674545\n",
            "step: 50, loss: 0.1962156891822815\n",
            "step: 60, loss: 0.056676074862480164\n",
            "step: 70, loss: 0.11589344590902328\n",
            "step: 80, loss: 0.19252638518810272\n",
            "step: 90, loss: 0.10209353268146515\n",
            "step: 100, loss: 0.11452794075012207\n",
            "step: 110, loss: 0.05633218213915825\n",
            "step: 120, loss: 0.023028751835227013\n",
            "step: 130, loss: 0.011186822317540646\n",
            "step: 140, loss: 0.00762784481048584\n",
            "step: 150, loss: 0.11540187895298004\n",
            "step: 160, loss: 0.013530520722270012\n",
            "step: 170, loss: 0.15212246775627136\n",
            "step: 180, loss: 0.022288713604211807\n",
            "step: 190, loss: 0.12219218164682388\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7671957671957672, f1=0.7692307692307692, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012715582735836506\n",
            "step: 10, loss: 0.06311271339654922\n",
            "step: 20, loss: 0.09582587331533432\n",
            "step: 30, loss: 0.03351597487926483\n",
            "step: 40, loss: 0.02944793738424778\n",
            "step: 50, loss: 0.0509248748421669\n",
            "step: 60, loss: 0.11829254031181335\n",
            "step: 70, loss: 0.047074396163225174\n",
            "step: 80, loss: 0.05815267562866211\n",
            "step: 90, loss: 0.01310658548027277\n",
            "step: 100, loss: 0.11931125819683075\n",
            "step: 110, loss: 0.0034231669269502163\n",
            "step: 120, loss: 0.028076669201254845\n",
            "step: 130, loss: 0.24677535891532898\n",
            "step: 140, loss: 0.010788812302052975\n",
            "step: 150, loss: 0.023105179890990257\n",
            "step: 160, loss: 0.017650820314884186\n",
            "step: 170, loss: 0.039781734347343445\n",
            "step: 180, loss: 0.0015132622793316841\n",
            "step: 190, loss: 0.13045012950897217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7300771208226221, f1=0.7407407407407407, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07308498024940491\n",
            "step: 10, loss: 0.0033747439738363028\n",
            "step: 20, loss: 0.007382153999060392\n",
            "step: 30, loss: 0.004429445136338472\n",
            "step: 40, loss: 0.092521533370018\n",
            "step: 50, loss: 0.023120488971471786\n",
            "step: 60, loss: 0.15245865285396576\n",
            "step: 70, loss: 0.047948792576789856\n",
            "step: 80, loss: 0.017886372283101082\n",
            "step: 90, loss: 0.02656984142959118\n",
            "step: 100, loss: 0.0007936947513371706\n",
            "step: 110, loss: 0.0036146086640655994\n",
            "step: 120, loss: 0.001668113050982356\n",
            "step: 130, loss: 0.00793603714555502\n",
            "step: 140, loss: 0.07555221021175385\n",
            "step: 150, loss: 0.00561038963496685\n",
            "step: 160, loss: 0.048836614936590195\n",
            "step: 170, loss: 0.039414625614881516\n",
            "step: 180, loss: 0.054794829338788986\n",
            "step: 190, loss: 0.07125131785869598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7310704960835509, f1=0.7262872628726287, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031970273703336716\n",
            "step: 10, loss: 0.020742442458868027\n",
            "step: 20, loss: 0.006146089173853397\n",
            "step: 30, loss: 0.00038968416629359126\n",
            "step: 40, loss: 0.004602793604135513\n",
            "step: 50, loss: 0.002848108299076557\n",
            "step: 60, loss: 0.06474051624536514\n",
            "step: 70, loss: 0.0784892737865448\n",
            "step: 80, loss: 0.032977793365716934\n",
            "step: 90, loss: 0.005671176128089428\n",
            "step: 100, loss: 0.001222697552293539\n",
            "step: 110, loss: 0.002505709184333682\n",
            "step: 120, loss: 0.04362298548221588\n",
            "step: 130, loss: 0.016597097739577293\n",
            "step: 140, loss: 0.09415131062269211\n",
            "step: 150, loss: 0.007364785764366388\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 160, loss: 0.007079660426825285\n",
            "step: 170, loss: 0.018650414422154427\n",
            "step: 180, loss: 0.009884619154036045\n",
            "step: 190, loss: 0.011729124002158642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7311827956989247, f1=0.7374301675977653, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017322899075224996\n",
            "step: 10, loss: 0.0006093577831052244\n",
            "step: 20, loss: 0.0027315542101860046\n",
            "step: 30, loss: 0.00561560969799757\n",
            "step: 40, loss: 0.0010077308397740126\n",
            "step: 50, loss: 0.0027419214602559805\n",
            "step: 60, loss: 0.0013190245954319835\n",
            "step: 70, loss: 0.0008036476792767644\n",
            "step: 80, loss: 0.008132938295602798\n",
            "step: 90, loss: 0.0007770649972371757\n",
            "step: 100, loss: 0.00033500700374133885\n",
            "step: 110, loss: 0.0020510314498096704\n",
            "step: 120, loss: 0.0006311756442300975\n",
            "step: 130, loss: 0.00035739943268708885\n",
            "step: 140, loss: 0.000441909913206473\n",
            "step: 150, loss: 0.036979738622903824\n",
            "step: 160, loss: 0.18015483021736145\n",
            "step: 170, loss: 0.003021097742021084\n",
            "step: 180, loss: 0.0006977110751904547\n",
            "step: 190, loss: 0.002377202734351158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7354497354497355, f1=0.7582417582417582, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026396203320473433\n",
            "step: 10, loss: 0.01117573119699955\n",
            "step: 20, loss: 0.005541789345443249\n",
            "step: 30, loss: 0.005935350898653269\n",
            "step: 40, loss: 0.0024840575642883778\n",
            "step: 50, loss: 0.008471054956316948\n",
            "step: 60, loss: 0.0024205902591347694\n",
            "step: 70, loss: 0.008065887726843357\n",
            "step: 80, loss: 0.0003527947992552072\n",
            "step: 90, loss: 0.00035175096127204597\n",
            "step: 100, loss: 0.03161201253533363\n",
            "step: 110, loss: 0.000375796458683908\n",
            "step: 120, loss: 0.009610197506844997\n",
            "step: 130, loss: 0.0006875402759760618\n",
            "step: 140, loss: 0.0024981095921248198\n",
            "step: 150, loss: 0.0012344500282779336\n",
            "step: 160, loss: 0.00041528273141011596\n",
            "step: 170, loss: 0.0011643391335383058\n",
            "step: 180, loss: 0.00978743378072977\n",
            "step: 190, loss: 0.0005060415132902563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7346938775510203, f1=0.7636363636363637, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009534706478007138\n",
            "step: 10, loss: 0.04930378124117851\n",
            "step: 20, loss: 0.05107381194829941\n",
            "step: 30, loss: 0.00020598860282916576\n",
            "step: 40, loss: 0.0003552387352101505\n",
            "step: 50, loss: 0.00028842836036346853\n",
            "step: 60, loss: 0.0017900166567415\n",
            "step: 70, loss: 0.0028451166581362486\n",
            "step: 80, loss: 0.00039572990499436855\n",
            "step: 90, loss: 0.0002675779105629772\n",
            "step: 100, loss: 0.0019520105561241508\n",
            "step: 110, loss: 0.03014741837978363\n",
            "step: 120, loss: 0.0008956866804510355\n",
            "step: 130, loss: 0.0217701718211174\n",
            "step: 140, loss: 0.000519098190125078\n",
            "step: 150, loss: 0.0035363254137337208\n",
            "step: 160, loss: 0.0002697481832001358\n",
            "step: 170, loss: 0.0003260981466155499\n",
            "step: 180, loss: 0.0019500170601531863\n",
            "step: 190, loss: 0.00016161489475052804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7540106951871658, f1=0.772117962466488, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023005357070360333\n",
            "step: 10, loss: 0.00038503302494063973\n",
            "step: 20, loss: 0.01875758171081543\n",
            "step: 30, loss: 9.918970317812636e-05\n",
            "step: 40, loss: 0.010479462333023548\n",
            "step: 50, loss: 0.00028338286210782826\n",
            "step: 60, loss: 0.0010597279760986567\n",
            "step: 70, loss: 0.00023175703245215118\n",
            "step: 80, loss: 0.00013418936578091234\n",
            "step: 90, loss: 0.0004345369234215468\n",
            "step: 100, loss: 0.0014630656223744154\n",
            "step: 110, loss: 0.00038244135794229805\n",
            "step: 120, loss: 0.01770181581377983\n",
            "step: 130, loss: 0.008964535780251026\n",
            "step: 140, loss: 0.007033176254481077\n",
            "step: 150, loss: 0.1306622475385666\n",
            "step: 160, loss: 0.0009637553594075143\n",
            "step: 170, loss: 0.07725829631090164\n",
            "step: 180, loss: 0.00035228158230893314\n",
            "step: 190, loss: 0.0018447600305080414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.765498652291105, f1=0.7828418230563003, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007687179022468626\n",
            "step: 10, loss: 0.0005987334880046546\n",
            "step: 20, loss: 0.04350616782903671\n",
            "step: 30, loss: 0.06733270734548569\n",
            "step: 40, loss: 0.0005106751923449337\n",
            "step: 50, loss: 0.000783177965786308\n",
            "step: 60, loss: 0.0007641083211638033\n",
            "step: 70, loss: 0.00032885593827813864\n",
            "step: 80, loss: 0.0004727505729533732\n",
            "step: 90, loss: 0.0004547291318885982\n",
            "step: 100, loss: 0.0007202949491329491\n",
            "step: 110, loss: 0.00022641487885266542\n",
            "step: 120, loss: 0.0004152871260885149\n",
            "step: 130, loss: 0.002339035039767623\n",
            "step: 140, loss: 0.0011024392442777753\n",
            "step: 150, loss: 0.00029399959021247923\n",
            "step: 160, loss: 0.0006650452269241214\n",
            "step: 170, loss: 0.001004343619570136\n",
            "step: 180, loss: 0.0007003378123044968\n",
            "step: 190, loss: 0.00019151443848386407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7486910994764396, f1=0.768, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010649684554664418\n",
            "step: 10, loss: 0.01172295305877924\n",
            "step: 20, loss: 0.00027816148940473795\n",
            "step: 30, loss: 0.0007130308076739311\n",
            "step: 40, loss: 0.00524082500487566\n",
            "step: 50, loss: 0.0017109045293182135\n",
            "step: 60, loss: 0.016006946563720703\n",
            "step: 70, loss: 0.0001259191776625812\n",
            "step: 80, loss: 0.000165314253536053\n",
            "step: 90, loss: 0.00015187384269665927\n",
            "step: 100, loss: 0.0001957739586941898\n",
            "step: 110, loss: 0.00019068457186222076\n",
            "step: 120, loss: 0.0002171052765334025\n",
            "step: 130, loss: 0.00015991598775144666\n",
            "step: 140, loss: 0.0005177899147383869\n",
            "step: 150, loss: 0.00023508761660195887\n",
            "step: 160, loss: 0.0001504052779637277\n",
            "step: 170, loss: 0.002417949726805091\n",
            "step: 180, loss: 0.00130832742433995\n",
            "step: 190, loss: 0.00015640369383618236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.736, f1=0.7603305785123966, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029612184152938426\n",
            "step: 10, loss: 9.958762530004606e-05\n",
            "step: 20, loss: 0.005426805000752211\n",
            "step: 30, loss: 0.00017278778250329196\n",
            "step: 40, loss: 0.00018404207366984338\n",
            "step: 50, loss: 0.00044853572035208344\n",
            "step: 60, loss: 0.00017731158004608005\n",
            "step: 70, loss: 0.1745568960905075\n",
            "step: 80, loss: 0.011667163111269474\n",
            "step: 90, loss: 0.00027290545403957367\n",
            "step: 100, loss: 0.00018345267744734883\n",
            "step: 110, loss: 0.000208265075343661\n",
            "step: 120, loss: 0.0006985389045439661\n",
            "step: 130, loss: 0.0001649160694796592\n",
            "step: 140, loss: 0.00013386792852543294\n",
            "step: 150, loss: 9.396355744684115e-05\n",
            "step: 160, loss: 0.0001951050799107179\n",
            "step: 170, loss: 0.0006337066297419369\n",
            "step: 180, loss: 0.0005257653538137674\n",
            "step: 190, loss: 0.0011371728032827377\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7602905569007263, f1=0.7493917274939174, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005261329002678394\n",
            "step: 10, loss: 0.00025195899070240557\n",
            "step: 20, loss: 0.00017727434169501066\n",
            "step: 30, loss: 0.0045271990820765495\n",
            "step: 40, loss: 0.005873749498277903\n",
            "step: 50, loss: 0.00016686221351847053\n",
            "step: 60, loss: 0.019923441112041473\n",
            "step: 70, loss: 0.0001452265860280022\n",
            "step: 80, loss: 0.00012880393478553742\n",
            "step: 90, loss: 0.08874945342540741\n",
            "step: 100, loss: 0.00028692357591353357\n",
            "step: 110, loss: 0.0002707724052015692\n",
            "step: 120, loss: 0.00016176192730199546\n",
            "step: 130, loss: 0.00023900228552520275\n",
            "step: 140, loss: 0.0011958708055317402\n",
            "step: 150, loss: 0.00020032792235724628\n",
            "step: 160, loss: 0.0027678448241204023\n",
            "step: 170, loss: 0.00024268400738947093\n",
            "step: 180, loss: 0.0007056510075926781\n",
            "step: 190, loss: 0.0003931655955966562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7571801566579635, f1=0.7626666666666667, best_f1=0.7692307692307692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007518379716202617\n",
            "step: 10, loss: 0.00017904079868458211\n",
            "step: 20, loss: 0.00017243711045011878\n",
            "step: 30, loss: 0.0001055002212524414\n",
            "step: 40, loss: 0.0002810796722769737\n",
            "step: 50, loss: 0.00018935247499030083\n",
            "step: 60, loss: 0.00016862331540323794\n",
            "step: 70, loss: 0.0003353011270519346\n",
            "step: 80, loss: 0.00022666265431325883\n",
            "step: 90, loss: 0.00041081852396018803\n",
            "step: 100, loss: 0.00038401797064580023\n",
            "step: 110, loss: 0.0002597900456748903\n",
            "step: 120, loss: 0.00013960196520201862\n",
            "step: 130, loss: 0.00044668070040643215\n",
            "step: 140, loss: 0.0003846963809337467\n",
            "step: 150, loss: 0.0003018486895598471\n",
            "step: 160, loss: 0.00015293850447051227\n",
            "step: 170, loss: 0.0003429329372011125\n",
            "step: 180, loss: 0.0019687237218022346\n",
            "step: 190, loss: 0.0001735844707582146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.751269035532995, f1=0.7552083333333333, best_f1=0.7692307692307692\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:10, 204.71it/s]\n",
            "load_f1 = 0.6253521126760564\n",
            "real_f1 = 0.6022099447513812\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 224.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92dba0e-5a4e-4848-c074-d95b54132a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.632739782333374\n",
            "step: 10, loss: 0.37038320302963257\n",
            "step: 20, loss: 0.3050107955932617\n",
            "step: 30, loss: 0.38223764300346375\n",
            "step: 40, loss: 0.2891567051410675\n",
            "step: 50, loss: 0.26228275895118713\n",
            "step: 60, loss: 0.21775288879871368\n",
            "step: 70, loss: 0.3533221483230591\n",
            "step: 80, loss: 0.36923354864120483\n",
            "step: 90, loss: 0.24865815043449402\n",
            "step: 100, loss: 0.1936742216348648\n",
            "step: 110, loss: 0.24791541695594788\n",
            "step: 120, loss: 0.20225240290164948\n",
            "step: 130, loss: 0.0640677809715271\n",
            "step: 140, loss: 0.16259326040744781\n",
            "step: 150, loss: 0.42403897643089294\n",
            "step: 160, loss: 0.10192075371742249\n",
            "step: 170, loss: 0.30268722772598267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6860158311345647, f1=0.6701030927835051, best_f1=0.6701030927835051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18410290777683258\n",
            "step: 10, loss: 0.1628885716199875\n",
            "step: 20, loss: 0.19318018853664398\n",
            "step: 30, loss: 0.2092280089855194\n",
            "step: 40, loss: 0.0585927739739418\n",
            "step: 50, loss: 0.0382172092795372\n",
            "step: 60, loss: 0.12345796078443527\n",
            "step: 70, loss: 0.08629650622606277\n",
            "step: 80, loss: 0.033292241394519806\n",
            "step: 90, loss: 0.13623034954071045\n",
            "step: 100, loss: 0.307438462972641\n",
            "step: 110, loss: 0.14011813700199127\n",
            "step: 120, loss: 0.09266550093889236\n",
            "step: 130, loss: 0.11212494224309921\n",
            "step: 140, loss: 0.34575775265693665\n",
            "step: 150, loss: 0.09824033826589584\n",
            "step: 160, loss: 0.21741469204425812\n",
            "step: 170, loss: 0.12829914689064026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7669172932330827, f1=0.7559808612440191, best_f1=0.7559808612440191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14016571640968323\n",
            "step: 10, loss: 0.08018378913402557\n",
            "step: 20, loss: 0.07740788906812668\n",
            "step: 30, loss: 0.2578282058238983\n",
            "step: 40, loss: 0.08566073328256607\n",
            "step: 50, loss: 0.13336388766765594\n",
            "step: 60, loss: 0.08021154254674911\n",
            "step: 70, loss: 0.08590752631425858\n",
            "step: 80, loss: 0.047036003321409225\n",
            "step: 90, loss: 0.21178023517131805\n",
            "step: 100, loss: 0.020496198907494545\n",
            "step: 110, loss: 0.07938499003648758\n",
            "step: 120, loss: 0.06333784013986588\n",
            "step: 130, loss: 0.1425594985485077\n",
            "step: 140, loss: 0.05002064257860184\n",
            "step: 150, loss: 0.08955232799053192\n",
            "step: 160, loss: 0.0494915246963501\n",
            "step: 170, loss: 0.08185029774904251\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7728459530026109, f1=0.7669172932330827, best_f1=0.7669172932330827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05426459386944771\n",
            "step: 10, loss: 0.03372087702155113\n",
            "step: 20, loss: 0.03139173611998558\n",
            "step: 30, loss: 0.03151148930191994\n",
            "step: 40, loss: 0.0025260362308472395\n",
            "step: 50, loss: 0.05875542759895325\n",
            "step: 60, loss: 0.31397444009780884\n",
            "step: 70, loss: 0.010271042585372925\n",
            "step: 80, loss: 0.07792916148900986\n",
            "step: 90, loss: 0.05873329937458038\n",
            "step: 100, loss: 0.1789478063583374\n",
            "step: 110, loss: 0.10825372487306595\n",
            "step: 120, loss: 0.02587568387389183\n",
            "step: 130, loss: 0.062473833560943604\n",
            "step: 140, loss: 0.08705305308103561\n",
            "step: 150, loss: 0.12902668118476868\n",
            "step: 160, loss: 0.07928363978862762\n",
            "step: 170, loss: 0.014957894571125507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7609756097560975, f1=0.7703016241299303, best_f1=0.7669172932330827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017416128888726234\n",
            "step: 10, loss: 0.006490418221801519\n",
            "step: 20, loss: 0.08978545665740967\n",
            "step: 30, loss: 0.008168577216565609\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.10276439785957336\n",
            "step: 50, loss: 0.1886325180530548\n",
            "step: 60, loss: 0.02425079233944416\n",
            "step: 70, loss: 0.14308419823646545\n",
            "step: 80, loss: 0.08333475142717361\n",
            "step: 90, loss: 0.10797087848186493\n",
            "step: 100, loss: 0.1414894014596939\n",
            "step: 110, loss: 0.04984355345368385\n",
            "step: 120, loss: 0.10853646695613861\n",
            "step: 130, loss: 0.032828979194164276\n",
            "step: 140, loss: 0.026228327304124832\n",
            "step: 150, loss: 0.015511252917349339\n",
            "step: 160, loss: 0.012815392576158047\n",
            "step: 170, loss: 0.004345873836427927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7938931297709925, f1=0.7837150127226464, best_f1=0.7837150127226464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009723008843138814\n",
            "step: 10, loss: 0.019251661375164986\n",
            "step: 20, loss: 0.006137113552540541\n",
            "step: 30, loss: 0.10749093443155289\n",
            "step: 40, loss: 0.0215646643191576\n",
            "step: 50, loss: 0.06897779554128647\n",
            "step: 60, loss: 0.006100276950746775\n",
            "step: 70, loss: 0.10045921057462692\n",
            "step: 80, loss: 0.023663099855184555\n",
            "step: 90, loss: 0.0308522991836071\n",
            "step: 100, loss: 0.009221049956977367\n",
            "step: 110, loss: 0.000358502846211195\n",
            "step: 120, loss: 0.02465413138270378\n",
            "step: 130, loss: 0.009542613290250301\n",
            "step: 140, loss: 0.004911658354103565\n",
            "step: 150, loss: 0.033168941736221313\n",
            "step: 160, loss: 0.06263312697410583\n",
            "step: 170, loss: 0.09246088564395905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7989690721649484, f1=0.7951219512195121, best_f1=0.7951219512195121\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008702676277607679\n",
            "step: 10, loss: 0.0033266874961555004\n",
            "step: 20, loss: 0.012659572064876556\n",
            "step: 30, loss: 0.017646800726652145\n",
            "step: 40, loss: 0.008067849092185497\n",
            "step: 50, loss: 0.00972488708794117\n",
            "step: 60, loss: 0.00044111296301707625\n",
            "step: 70, loss: 0.022138865664601326\n",
            "step: 80, loss: 0.03609864041209221\n",
            "step: 90, loss: 0.0002526627213228494\n",
            "step: 100, loss: 0.00041741097811609507\n",
            "step: 110, loss: 0.02314937114715576\n",
            "step: 120, loss: 0.0005479935789480805\n",
            "step: 130, loss: 0.08975660800933838\n",
            "step: 140, loss: 0.0051292660646140575\n",
            "step: 150, loss: 0.0015385339502245188\n",
            "step: 160, loss: 0.01057574711740017\n",
            "step: 170, loss: 0.011522735469043255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8010610079575595, f1=0.7989690721649484, best_f1=0.7989690721649484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011453295592218637\n",
            "step: 10, loss: 0.0016012032283470035\n",
            "step: 20, loss: 0.0007159037631936371\n",
            "step: 30, loss: 0.019895954057574272\n",
            "step: 40, loss: 0.00019000381871592253\n",
            "step: 50, loss: 0.002026304369792342\n",
            "step: 60, loss: 0.016347579658031464\n",
            "step: 70, loss: 0.004774716217070818\n",
            "step: 80, loss: 0.00970317330211401\n",
            "step: 90, loss: 0.006746246013790369\n",
            "step: 100, loss: 0.07048908621072769\n",
            "step: 110, loss: 0.06073266267776489\n",
            "step: 120, loss: 0.0014939996181055903\n",
            "step: 130, loss: 0.004906741436570883\n",
            "step: 140, loss: 0.0012960669118911028\n",
            "step: 150, loss: 0.0022796031553298235\n",
            "step: 160, loss: 0.007857694290578365\n",
            "step: 170, loss: 0.009688693098723888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7513812154696132, f1=0.7417582417582418, best_f1=0.7989690721649484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005141738336533308\n",
            "step: 10, loss: 0.0011864654952660203\n",
            "step: 20, loss: 0.0011080339318141341\n",
            "step: 30, loss: 0.0016011834377422929\n",
            "step: 40, loss: 0.0005603623576462269\n",
            "step: 50, loss: 0.000211476391996257\n",
            "step: 60, loss: 0.00485726585611701\n",
            "step: 70, loss: 0.017608370631933212\n",
            "step: 80, loss: 0.0011873203329741955\n",
            "step: 90, loss: 0.020657166838645935\n",
            "step: 100, loss: 0.04819655790925026\n",
            "step: 110, loss: 0.002286008559167385\n",
            "step: 120, loss: 0.0506039559841156\n",
            "step: 130, loss: 0.1460554450750351\n",
            "step: 140, loss: 0.005550294648855925\n",
            "step: 150, loss: 0.027288077399134636\n",
            "step: 160, loss: 0.011956220492720604\n",
            "step: 170, loss: 0.0017446192214265466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7878787878787877, f1=0.7709251101321585, best_f1=0.7989690721649484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04308857023715973\n",
            "step: 10, loss: 0.00020389420387800783\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.15305674076080322\n",
            "step: 30, loss: 0.004146676044911146\n",
            "step: 40, loss: 0.0005489125032909214\n",
            "step: 50, loss: 0.061331965029239655\n",
            "step: 60, loss: 0.00415191613137722\n",
            "step: 70, loss: 0.038607459515333176\n",
            "step: 80, loss: 0.0057866573333740234\n",
            "step: 90, loss: 0.00724796112626791\n",
            "step: 100, loss: 0.0009097831207327545\n",
            "step: 110, loss: 0.0009322210098616779\n",
            "step: 120, loss: 0.0007600924582220614\n",
            "step: 130, loss: 0.00047687088954262435\n",
            "step: 140, loss: 0.004565997049212456\n",
            "step: 150, loss: 0.0007176477229222655\n",
            "step: 160, loss: 0.00022217899095267057\n",
            "step: 170, loss: 0.00024382799165323377\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7949367088607596, f1=0.7951219512195121, best_f1=0.7989690721649484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.032749444246292114\n",
            "step: 10, loss: 0.01090905349701643\n",
            "step: 20, loss: 0.0002574566169641912\n",
            "step: 30, loss: 0.0001221149432240054\n",
            "step: 40, loss: 0.0011555084493011236\n",
            "step: 50, loss: 0.0004450917767826468\n",
            "step: 60, loss: 0.006977875251322985\n",
            "step: 70, loss: 0.00029323159833438694\n",
            "step: 80, loss: 0.00018523834296502173\n",
            "step: 90, loss: 0.0002549227501731366\n",
            "step: 100, loss: 0.01542644388973713\n",
            "step: 110, loss: 0.12889285385608673\n",
            "step: 120, loss: 0.0002906818699557334\n",
            "step: 130, loss: 0.00015129851817619056\n",
            "step: 140, loss: 0.00022891572734806687\n",
            "step: 150, loss: 0.0009157481254078448\n",
            "step: 160, loss: 0.028967201709747314\n",
            "step: 170, loss: 0.0008985287859104574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7990074441687345, f1=0.7858823529411765, best_f1=0.7989690721649484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005090140271931887\n",
            "step: 10, loss: 0.00018467227346263826\n",
            "step: 20, loss: 0.0017438099021092057\n",
            "step: 30, loss: 0.0006787814782001078\n",
            "step: 40, loss: 0.001357087749056518\n",
            "step: 50, loss: 0.0003101710171904415\n",
            "step: 60, loss: 0.00032214721431955695\n",
            "step: 70, loss: 0.052136484533548355\n",
            "step: 80, loss: 0.00026439217617735267\n",
            "step: 90, loss: 0.000350871414411813\n",
            "step: 100, loss: 0.0009384783916175365\n",
            "step: 110, loss: 0.0002468094462528825\n",
            "step: 120, loss: 0.0036148815415799618\n",
            "step: 130, loss: 0.1520456075668335\n",
            "step: 140, loss: 0.0030570372473448515\n",
            "step: 150, loss: 0.001164968591183424\n",
            "step: 160, loss: 0.00015078438445925713\n",
            "step: 170, loss: 0.000629876391030848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8060453400503778, f1=0.7903614457831325, best_f1=0.7903614457831325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02551787532866001\n",
            "step: 10, loss: 0.00047631311463192105\n",
            "step: 20, loss: 0.000764504715334624\n",
            "step: 30, loss: 0.00015878125850576907\n",
            "step: 40, loss: 0.015795540064573288\n",
            "step: 50, loss: 0.00013028662942815572\n",
            "step: 60, loss: 0.0022768089547753334\n",
            "step: 70, loss: 0.0013134053442627192\n",
            "step: 80, loss: 0.00044818632886745036\n",
            "step: 90, loss: 9.665441029937938e-05\n",
            "step: 100, loss: 0.0012917056446895003\n",
            "step: 110, loss: 8.759079355513677e-05\n",
            "step: 120, loss: 0.019623521715402603\n",
            "step: 130, loss: 0.00014472912880592048\n",
            "step: 140, loss: 0.0007792628020979464\n",
            "step: 150, loss: 0.05090973153710365\n",
            "step: 160, loss: 0.0005043313140049577\n",
            "step: 170, loss: 0.00030007361783646047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8099999999999999, f1=0.7877358490566038, best_f1=0.7877358490566038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002684962237253785\n",
            "step: 10, loss: 8.374505705432966e-05\n",
            "step: 20, loss: 0.00018493911193218082\n",
            "step: 30, loss: 0.0001718347193673253\n",
            "step: 40, loss: 0.00013363097968976945\n",
            "step: 50, loss: 0.00012722726387437433\n",
            "step: 60, loss: 0.00013587449211627245\n",
            "step: 70, loss: 0.00022308457118924707\n",
            "step: 80, loss: 0.00027688153204508126\n",
            "step: 90, loss: 8.327541581820697e-05\n",
            "step: 100, loss: 0.0002260479814140126\n",
            "step: 110, loss: 0.00021223675867076963\n",
            "step: 120, loss: 0.028732536360621452\n",
            "step: 130, loss: 0.007931840606033802\n",
            "step: 140, loss: 0.0001494777825428173\n",
            "step: 150, loss: 0.0009601020137779415\n",
            "step: 160, loss: 0.00015397844254039228\n",
            "step: 170, loss: 0.00027501489967107773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8051282051282052, f1=0.8029556650246306, best_f1=0.7877358490566038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033043374423868954\n",
            "step: 10, loss: 0.0003230276342947036\n",
            "step: 20, loss: 0.01298128068447113\n",
            "step: 30, loss: 8.143199374899268e-05\n",
            "step: 40, loss: 0.00013747200137004256\n",
            "step: 50, loss: 8.302720380015671e-05\n",
            "step: 60, loss: 0.022859172895550728\n",
            "step: 70, loss: 0.0001669505872996524\n",
            "step: 80, loss: 0.03149697557091713\n",
            "step: 90, loss: 0.00019729915948119015\n",
            "step: 100, loss: 0.0002201776806032285\n",
            "step: 110, loss: 0.0006137315649539232\n",
            "step: 120, loss: 0.00035085593117401004\n",
            "step: 130, loss: 0.00011026396532543004\n",
            "step: 140, loss: 0.00011347964755259454\n",
            "step: 150, loss: 0.00022467119561042637\n",
            "step: 160, loss: 0.0003061838506255299\n",
            "step: 170, loss: 0.0012398915132507682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8062015503875968, f1=0.79, best_f1=0.7877358490566038\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 256.66it/s]\n",
            "load_f1 = 0.6651053864168619\n",
            "real_f1 = 0.6728538283062646\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 225.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ac7e51-8aa0-4f90-8a3d-91feb5a41f80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6126011610031128\n",
            "step: 10, loss: 0.6527372002601624\n",
            "step: 20, loss: 0.4880896508693695\n",
            "step: 30, loss: 0.3975338041782379\n",
            "step: 40, loss: 0.26567211747169495\n",
            "step: 50, loss: 0.06396333873271942\n",
            "step: 60, loss: 0.1458476036787033\n",
            "step: 70, loss: 0.05747273564338684\n",
            "step: 80, loss: 0.12282168865203857\n",
            "step: 90, loss: 0.10568390786647797\n",
            "step: 100, loss: 0.004571777302771807\n",
            "step: 110, loss: 0.23206840455532074\n",
            "step: 120, loss: 0.01871507614850998\n",
            "step: 130, loss: 0.008458291180431843\n",
            "step: 140, loss: 0.005634055007249117\n",
            "step: 150, loss: 0.01165102794766426\n",
            "step: 160, loss: 0.017222965136170387\n",
            "step: 170, loss: 0.13111567497253418\n",
            "step: 180, loss: 0.12118849903345108\n",
            "step: 190, loss: 0.08691630512475967\n",
            "step: 200, loss: 0.08075296878814697\n",
            "step: 210, loss: 0.013369560241699219\n",
            "step: 220, loss: 0.030246881768107414\n",
            "step: 230, loss: 0.032949332147836685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9671574178935448, f1=0.9657534246575342, best_f1=0.9657534246575342\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01253865472972393\n",
            "step: 10, loss: 0.0076027060858905315\n",
            "step: 20, loss: 0.3110095262527466\n",
            "step: 30, loss: 0.18061085045337677\n",
            "step: 40, loss: 0.05921277031302452\n",
            "step: 50, loss: 0.02766549028456211\n",
            "step: 60, loss: 0.007250667084008455\n",
            "step: 70, loss: 0.030923942103981972\n",
            "step: 80, loss: 0.20837868750095367\n",
            "step: 90, loss: 0.04754062741994858\n",
            "step: 100, loss: 0.14061880111694336\n",
            "step: 110, loss: 0.20782533288002014\n",
            "step: 120, loss: 0.11756771057844162\n",
            "step: 130, loss: 0.07157785445451736\n",
            "step: 140, loss: 0.0025072877760976553\n",
            "step: 150, loss: 0.017243552953004837\n",
            "step: 160, loss: 0.0907628983259201\n",
            "step: 170, loss: 0.0018413832876831293\n",
            "step: 180, loss: 0.01760738715529442\n",
            "step: 190, loss: 0.007904977537691593\n",
            "step: 200, loss: 0.008219534531235695\n",
            "step: 210, loss: 0.013042888604104519\n",
            "step: 220, loss: 0.020889298990368843\n",
            "step: 230, loss: 0.03653707355260849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9702315325248071, f1=0.9721913236929923, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006974378135055304\n",
            "step: 10, loss: 0.04768693447113037\n",
            "step: 20, loss: 0.0033438962418586016\n",
            "step: 30, loss: 0.002296461956575513\n",
            "step: 40, loss: 0.12155673652887344\n",
            "step: 50, loss: 0.054248493164777756\n",
            "step: 60, loss: 0.022989608347415924\n",
            "step: 70, loss: 0.0053657954558730125\n",
            "step: 80, loss: 0.0015065384795889258\n",
            "step: 90, loss: 0.05937513709068298\n",
            "step: 100, loss: 0.00046900598681531847\n",
            "step: 110, loss: 0.0010191226610913873\n",
            "step: 120, loss: 0.01518743485212326\n",
            "step: 130, loss: 0.0007851509726606309\n",
            "step: 140, loss: 0.0011057317024096847\n",
            "step: 150, loss: 0.04129253327846527\n",
            "step: 160, loss: 0.031200271099805832\n",
            "step: 170, loss: 0.02510489523410797\n",
            "step: 180, loss: 0.05049625784158707\n",
            "step: 190, loss: 0.02623898535966873\n",
            "step: 200, loss: 0.04598816856741905\n",
            "step: 210, loss: 0.0038255969993770123\n",
            "step: 220, loss: 0.0017997699324041605\n",
            "step: 230, loss: 0.0006485473713837564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9765363128491621, f1=0.967452300785634, best_f1=0.967452300785634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040577654726803303\n",
            "step: 10, loss: 0.01128549687564373\n",
            "step: 20, loss: 0.0007738117128610611\n",
            "step: 30, loss: 0.00032089813612401485\n",
            "step: 40, loss: 0.0016232941998168826\n",
            "step: 50, loss: 0.0007447195239365101\n",
            "step: 60, loss: 0.002017645863816142\n",
            "step: 70, loss: 0.0037231387104839087\n",
            "step: 80, loss: 0.004729273729026318\n",
            "step: 90, loss: 0.016417495906352997\n",
            "step: 100, loss: 0.007113283034414053\n",
            "step: 110, loss: 0.0033640225883573294\n",
            "step: 120, loss: 0.037177674472332\n",
            "step: 130, loss: 0.04056518152356148\n",
            "step: 140, loss: 0.0021210063714534044\n",
            "step: 150, loss: 0.17067913711071014\n",
            "step: 160, loss: 0.06522431969642639\n",
            "step: 170, loss: 0.024223821237683296\n",
            "step: 180, loss: 0.00037140343920327723\n",
            "step: 190, loss: 0.136310875415802\n",
            "step: 200, loss: 0.002623399253934622\n",
            "step: 210, loss: 0.06511382013559341\n",
            "step: 220, loss: 0.0005827300483360887\n",
            "step: 230, loss: 0.0009018222917802632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9774774774774775, f1=0.9775280898876404, best_f1=0.9775280898876404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011228544171899557\n",
            "step: 10, loss: 0.00043154411832802\n",
            "step: 20, loss: 0.0067100077867507935\n",
            "step: 30, loss: 0.0002708283718675375\n",
            "step: 40, loss: 0.0007661355193704367\n",
            "step: 50, loss: 0.0029982293490320444\n",
            "step: 60, loss: 0.2049357295036316\n",
            "step: 70, loss: 0.0007483907393179834\n",
            "step: 80, loss: 0.001456500613130629\n",
            "step: 90, loss: 0.004827443975955248\n",
            "step: 100, loss: 0.0004902297514490783\n",
            "step: 110, loss: 0.00041862347279675305\n",
            "step: 120, loss: 0.00010211296466877684\n",
            "step: 130, loss: 0.003991322126239538\n",
            "step: 140, loss: 0.0025546157266944647\n",
            "step: 150, loss: 0.002892864402383566\n",
            "step: 160, loss: 0.0005046247388236225\n",
            "step: 170, loss: 0.002657569944858551\n",
            "step: 180, loss: 0.004293206613510847\n",
            "step: 190, loss: 0.04632715880870819\n",
            "step: 200, loss: 0.0023071381729096174\n",
            "step: 210, loss: 0.009015209972858429\n",
            "step: 220, loss: 0.0016860940959304571\n",
            "step: 230, loss: 0.012870448641479015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9670828603859251, f1=0.9681093394077448, best_f1=0.9775280898876404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07734512537717819\n",
            "step: 10, loss: 0.00982637982815504\n",
            "step: 20, loss: 0.01048331893980503\n",
            "step: 30, loss: 0.00024192439741455019\n",
            "step: 40, loss: 0.0003339753020554781\n",
            "step: 50, loss: 0.03219113126397133\n",
            "step: 60, loss: 0.00023476807109545916\n",
            "step: 70, loss: 0.008227691985666752\n",
            "step: 80, loss: 0.001730539370328188\n",
            "step: 90, loss: 0.0052446285262703896\n",
            "step: 100, loss: 0.04189801216125488\n",
            "step: 110, loss: 0.0006238609785214067\n",
            "step: 120, loss: 0.11243776977062225\n",
            "step: 130, loss: 0.0008317552856169641\n",
            "step: 140, loss: 0.0013718227855861187\n",
            "step: 150, loss: 0.010919883847236633\n",
            "step: 160, loss: 0.0004400262259878218\n",
            "step: 170, loss: 0.0004257207619957626\n",
            "step: 180, loss: 0.03459533303976059\n",
            "step: 190, loss: 0.08390911668539047\n",
            "step: 200, loss: 0.0007020271150395274\n",
            "step: 210, loss: 0.0022760231513530016\n",
            "step: 220, loss: 0.0015270696021616459\n",
            "step: 230, loss: 0.0331089124083519\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9776286353467561, f1=0.9765363128491621, best_f1=0.9765363128491621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026623098528943956\n",
            "step: 10, loss: 0.000246195966610685\n",
            "step: 20, loss: 0.00023191932996269315\n",
            "step: 30, loss: 0.00023768364917486906\n",
            "step: 40, loss: 0.0003199357306584716\n",
            "step: 50, loss: 0.0004210088518448174\n",
            "step: 60, loss: 0.0002999085118062794\n",
            "step: 70, loss: 0.041103560477495193\n",
            "step: 80, loss: 0.0006694376352243125\n",
            "step: 90, loss: 0.00038683009915985167\n",
            "step: 100, loss: 0.00048388485447503626\n",
            "step: 110, loss: 0.00023814353335183114\n",
            "step: 120, loss: 0.00035611240309663117\n",
            "step: 130, loss: 0.0015442841686308384\n",
            "step: 140, loss: 0.0011006025597453117\n",
            "step: 150, loss: 0.00015873955271672457\n",
            "step: 160, loss: 0.06704866141080856\n",
            "step: 170, loss: 0.021408287808299065\n",
            "step: 180, loss: 0.0006800832925364375\n",
            "step: 190, loss: 0.0014242319157347083\n",
            "step: 200, loss: 0.02453378587961197\n",
            "step: 210, loss: 7.828049274394289e-05\n",
            "step: 220, loss: 0.00025916993035934865\n",
            "step: 230, loss: 0.00034151767613366246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9785310734463276, f1=0.9819004524886877, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002627172798383981\n",
            "step: 10, loss: 0.00025435510906390846\n",
            "step: 20, loss: 0.00011701216135406867\n",
            "step: 30, loss: 0.0006247369456104934\n",
            "step: 40, loss: 0.0011538820108398795\n",
            "step: 50, loss: 0.0003870457876473665\n",
            "step: 60, loss: 0.00020578672410920262\n",
            "step: 70, loss: 0.0003970218531321734\n",
            "step: 80, loss: 0.0008485792786814272\n",
            "step: 90, loss: 7.159342931117862e-05\n",
            "step: 100, loss: 0.00016901103663258255\n",
            "step: 110, loss: 0.00011736681335605681\n",
            "step: 120, loss: 0.00021281266526784748\n",
            "step: 130, loss: 0.00011421350063756108\n",
            "step: 140, loss: 7.857051241444424e-05\n",
            "step: 150, loss: 8.75258119776845e-05\n",
            "step: 160, loss: 0.00024711398873478174\n",
            "step: 170, loss: 0.00036551945959217846\n",
            "step: 180, loss: 0.00010565595584921539\n",
            "step: 190, loss: 0.000251814752118662\n",
            "step: 200, loss: 0.00010167754226131365\n",
            "step: 210, loss: 0.0008236333378590643\n",
            "step: 220, loss: 0.00012020250869682059\n",
            "step: 230, loss: 0.00014973471115808934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9808773903262092, f1=0.9785310734463276, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.104819815140218e-05\n",
            "step: 10, loss: 0.00011664468911476433\n",
            "step: 20, loss: 0.0003807905304711312\n",
            "step: 30, loss: 0.00011219624866498634\n",
            "step: 40, loss: 0.05258239060640335\n",
            "step: 50, loss: 7.527583511546254e-05\n",
            "step: 60, loss: 0.0001021713251248002\n",
            "step: 70, loss: 0.0002750180719885975\n",
            "step: 80, loss: 7.167068542912602e-05\n",
            "step: 90, loss: 0.00012131348921684548\n",
            "step: 100, loss: 7.793032273184508e-05\n",
            "step: 110, loss: 6.397895776899531e-05\n",
            "step: 120, loss: 3.518001176416874e-05\n",
            "step: 130, loss: 3.9746679249219596e-05\n",
            "step: 140, loss: 6.641366053372622e-05\n",
            "step: 150, loss: 0.0028258010279387236\n",
            "step: 160, loss: 8.307098323712125e-05\n",
            "step: 170, loss: 9.220601350534707e-05\n",
            "step: 180, loss: 0.0001132795077865012\n",
            "step: 190, loss: 6.351860065478832e-05\n",
            "step: 200, loss: 6.877210398670286e-05\n",
            "step: 210, loss: 5.679506648448296e-05\n",
            "step: 220, loss: 5.052207416156307e-05\n",
            "step: 230, loss: 4.886560418526642e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9776785714285714, f1=0.9743016759776536, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.0140406276332214e-05\n",
            "step: 10, loss: 8.395867916988209e-05\n",
            "step: 20, loss: 5.683355266228318e-05\n",
            "step: 30, loss: 6.294447666732594e-05\n",
            "step: 40, loss: 4.241314309183508e-05\n",
            "step: 50, loss: 5.9329158830223605e-05\n",
            "step: 60, loss: 6.304839916992933e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 70, loss: 8.791083382675424e-05\n",
            "step: 80, loss: 0.00010611765901558101\n",
            "step: 90, loss: 6.842521543148905e-05\n",
            "step: 100, loss: 4.783384065376595e-05\n",
            "step: 110, loss: 0.0001510446600150317\n",
            "step: 120, loss: 0.0002228591765742749\n",
            "step: 130, loss: 0.0007968784775584936\n",
            "step: 140, loss: 0.03880402445793152\n",
            "step: 150, loss: 0.020866425707936287\n",
            "step: 160, loss: 9.43236009334214e-05\n",
            "step: 170, loss: 3.2755022402852774e-05\n",
            "step: 180, loss: 0.001973672304302454\n",
            "step: 190, loss: 0.000526083167642355\n",
            "step: 200, loss: 4.6951106924097985e-05\n",
            "step: 210, loss: 0.00020079135720152408\n",
            "step: 220, loss: 4.31622720498126e-05\n",
            "step: 230, loss: 0.0002885922440327704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9787709497206705, f1=0.9764837625979844, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.845056562568061e-05\n",
            "step: 10, loss: 4.4902244553668424e-05\n",
            "step: 20, loss: 0.00011348613043082878\n",
            "step: 30, loss: 0.00031880565802566707\n",
            "step: 40, loss: 0.00010918391490122303\n",
            "step: 50, loss: 3.733322591870092e-05\n",
            "step: 60, loss: 0.00011218366853427142\n",
            "step: 70, loss: 3.802524952334352e-05\n",
            "step: 80, loss: 7.655729132238775e-05\n",
            "step: 90, loss: 3.234927135054022e-05\n",
            "step: 100, loss: 6.029678479535505e-05\n",
            "step: 110, loss: 2.6511946998653002e-05\n",
            "step: 120, loss: 9.34827548917383e-05\n",
            "step: 130, loss: 0.0002989622007589787\n",
            "step: 140, loss: 3.3511052606627345e-05\n",
            "step: 150, loss: 0.0524294339120388\n",
            "step: 160, loss: 0.00018534513947088271\n",
            "step: 170, loss: 0.011144941672682762\n",
            "step: 180, loss: 0.0005698499735444784\n",
            "step: 190, loss: 0.00011863037070725113\n",
            "step: 200, loss: 0.0015379536198452115\n",
            "step: 210, loss: 2.8743326765834354e-05\n",
            "step: 220, loss: 4.916566103929654e-05\n",
            "step: 230, loss: 3.632331208791584e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9776286353467561, f1=0.9776286353467561, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.036112558329478e-05\n",
            "step: 10, loss: 4.522918243310414e-05\n",
            "step: 20, loss: 0.00018791590991895646\n",
            "step: 30, loss: 0.00015192190767265856\n",
            "step: 40, loss: 4.5497359678847715e-05\n",
            "step: 50, loss: 0.010488727129995823\n",
            "step: 60, loss: 0.005049597471952438\n",
            "step: 70, loss: 0.020970623940229416\n",
            "step: 80, loss: 5.3875981393503025e-05\n",
            "step: 90, loss: 9.371626219945028e-05\n",
            "step: 100, loss: 2.4407365344814025e-05\n",
            "step: 110, loss: 3.344792276038788e-05\n",
            "step: 120, loss: 4.715426257462241e-05\n",
            "step: 130, loss: 0.00033395891659893095\n",
            "step: 140, loss: 8.188914944184944e-05\n",
            "step: 150, loss: 0.00024024331651162356\n",
            "step: 160, loss: 0.00016157595382537693\n",
            "step: 170, loss: 7.408853707602248e-05\n",
            "step: 180, loss: 2.7994552510790527e-05\n",
            "step: 190, loss: 3.14470635203179e-05\n",
            "step: 200, loss: 2.0246447093086317e-05\n",
            "step: 210, loss: 2.9935408747405745e-05\n",
            "step: 220, loss: 6.117126758908853e-05\n",
            "step: 230, loss: 0.00015477236593142152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9787709497206705, f1=0.9742441209406495, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016266391321551055\n",
            "step: 10, loss: 3.845337778329849e-05\n",
            "step: 20, loss: 2.7283060262561776e-05\n",
            "step: 30, loss: 0.0002217446599388495\n",
            "step: 40, loss: 3.541048863553442e-05\n",
            "step: 50, loss: 0.00014313901192508638\n",
            "step: 60, loss: 0.0002587540075182915\n",
            "step: 70, loss: 2.760253300948534e-05\n",
            "step: 80, loss: 6.31136863376014e-05\n",
            "step: 90, loss: 0.00014164933236315846\n",
            "step: 100, loss: 2.169926665374078e-05\n",
            "step: 110, loss: 0.00966478232294321\n",
            "step: 120, loss: 5.06639844388701e-05\n",
            "step: 130, loss: 3.0274473829194903e-05\n",
            "step: 140, loss: 0.00020012696040794253\n",
            "step: 150, loss: 3.9176695281639695e-05\n",
            "step: 160, loss: 2.4362027033930644e-05\n",
            "step: 170, loss: 5.7294870202895254e-05\n",
            "step: 180, loss: 2.98422783089336e-05\n",
            "step: 190, loss: 2.477236557751894e-05\n",
            "step: 200, loss: 5.889424937777221e-05\n",
            "step: 210, loss: 1.6368703654734418e-05\n",
            "step: 220, loss: 0.0003025256155524403\n",
            "step: 230, loss: 1.9237035303376615e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9776286353467561, f1=0.9742441209406495, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000351289170794189\n",
            "step: 10, loss: 3.670648220577277e-05\n",
            "step: 20, loss: 2.520096313674003e-05\n",
            "step: 30, loss: 2.6623985831975006e-05\n",
            "step: 40, loss: 0.002804488642141223\n",
            "step: 50, loss: 3.992136043962091e-05\n",
            "step: 60, loss: 2.7558813599171117e-05\n",
            "step: 70, loss: 3.102315167780034e-05\n",
            "step: 80, loss: 4.2487667087698355e-05\n",
            "step: 90, loss: 5.26755211467389e-05\n",
            "step: 100, loss: 3.166717942804098e-05\n",
            "step: 110, loss: 2.701869379961863e-05\n",
            "step: 120, loss: 2.3736549337627366e-05\n",
            "step: 130, loss: 2.9246346457512118e-05\n",
            "step: 140, loss: 3.573883805074729e-05\n",
            "step: 150, loss: 0.00030269642593339086\n",
            "step: 160, loss: 0.0022169495932757854\n",
            "step: 170, loss: 1.3492873222276103e-05\n",
            "step: 180, loss: 2.3353133656200953e-05\n",
            "step: 190, loss: 4.8975478421198204e-05\n",
            "step: 200, loss: 4.236956374370493e-05\n",
            "step: 210, loss: 0.0009549983660690486\n",
            "step: 220, loss: 1.8130560420104302e-05\n",
            "step: 230, loss: 0.0031987898983061314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9787234042553192, f1=0.9763779527559054, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.5714356524986215e-05\n",
            "step: 10, loss: 1.478171816415852e-05\n",
            "step: 20, loss: 2.857936851796694e-05\n",
            "step: 30, loss: 3.774502329179086e-05\n",
            "step: 40, loss: 4.612604243448004e-05\n",
            "step: 50, loss: 0.006188246421515942\n",
            "step: 60, loss: 3.6930588976247236e-05\n",
            "step: 70, loss: 4.206090306979604e-05\n",
            "step: 80, loss: 2.7554760890780017e-05\n",
            "step: 90, loss: 3.14032549795229e-05\n",
            "step: 100, loss: 0.00019554788013920188\n",
            "step: 110, loss: 2.6560226615401916e-05\n",
            "step: 120, loss: 3.3041840652003884e-05\n",
            "step: 130, loss: 3.796105011133477e-05\n",
            "step: 140, loss: 2.1453399313031696e-05\n",
            "step: 150, loss: 5.0999860832234845e-05\n",
            "step: 160, loss: 1.9903747670468874e-05\n",
            "step: 170, loss: 1.820141915231943e-05\n",
            "step: 180, loss: 0.00031287307501770556\n",
            "step: 190, loss: 4.23486708314158e-05\n",
            "step: 200, loss: 2.758108894340694e-05\n",
            "step: 210, loss: 6.56067204545252e-05\n",
            "step: 220, loss: 2.3237635105033405e-05\n",
            "step: 230, loss: 2.667580520210322e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9786276715410572, f1=0.9785794813979707, best_f1=0.9785310734463276\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 211.49it/s]\n",
            "load_f1 = 0.9765886287625419\n",
            "real_f1 = 0.9744160177975528\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 252.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d37164d-306d-4058-e08d-696d1502e466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6195212602615356\n",
            "step: 10, loss: 0.5765916109085083\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.5651190280914307\n",
            "step: 30, loss: 0.2706012725830078\n",
            "step: 40, loss: 0.20202946662902832\n",
            "step: 50, loss: 0.3509480953216553\n",
            "step: 60, loss: 0.10224489867687225\n",
            "step: 70, loss: 0.17626447975635529\n",
            "step: 80, loss: 0.08984517306089401\n",
            "step: 90, loss: 0.3696531653404236\n",
            "step: 100, loss: 0.028100015595555305\n",
            "step: 110, loss: 0.09497007727622986\n",
            "step: 120, loss: 0.2015731781721115\n",
            "step: 130, loss: 0.11670743674039841\n",
            "step: 140, loss: 0.18831464648246765\n",
            "step: 150, loss: 0.02651393413543701\n",
            "step: 160, loss: 0.02908453531563282\n",
            "step: 170, loss: 0.3086468279361725\n",
            "step: 180, loss: 0.1458704173564911\n",
            "step: 190, loss: 0.010649560019373894\n",
            "step: 200, loss: 0.18204112350940704\n",
            "step: 210, loss: 0.01593373902142048\n",
            "step: 220, loss: 0.1767669916152954\n",
            "step: 230, loss: 0.10715630650520325\n",
            "step: 240, loss: 0.06764689832925797\n",
            "step: 250, loss: 0.012844357639551163\n",
            "step: 260, loss: 0.08256451040506363\n",
            "step: 270, loss: 0.01834874041378498\n",
            "step: 280, loss: 0.02294364757835865\n",
            "step: 290, loss: 0.1675165593624115\n",
            "step: 300, loss: 0.04470326751470566\n",
            "step: 310, loss: 0.15878064930438995\n",
            "step: 320, loss: 0.13066436350345612\n",
            "step: 330, loss: 0.033280279487371445\n",
            "step: 340, loss: 0.025908589363098145\n",
            "step: 350, loss: 0.05665979161858559\n",
            "step: 360, loss: 0.1484445333480835\n",
            "step: 370, loss: 0.07763749361038208\n",
            "step: 380, loss: 0.04872081056237221\n",
            "step: 390, loss: 0.20568925142288208\n",
            "step: 400, loss: 0.25816407799720764\n",
            "step: 410, loss: 0.040617093443870544\n",
            "step: 420, loss: 0.05815301835536957\n",
            "step: 430, loss: 0.1224735751748085\n",
            "step: 440, loss: 0.08387583494186401\n",
            "step: 450, loss: 0.011273612268269062\n",
            "step: 460, loss: 0.0209552850574255\n",
            "step: 470, loss: 0.1320737600326538\n",
            "step: 480, loss: 0.032671984285116196\n",
            "step: 490, loss: 0.05299540236592293\n",
            "step: 500, loss: 0.08585385233163834\n",
            "step: 510, loss: 0.16160838305950165\n",
            "step: 520, loss: 0.07363683730363846\n",
            "step: 530, loss: 0.004940960556268692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9272811486799445, f1=0.925925925925926, best_f1=0.925925925925926\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13253583014011383\n",
            "step: 10, loss: 0.05691390484571457\n",
            "step: 20, loss: 0.0443231500685215\n",
            "step: 30, loss: 0.04267364367842674\n",
            "step: 40, loss: 0.09868130832910538\n",
            "step: 50, loss: 0.14161553978919983\n",
            "step: 60, loss: 0.0065262350253760815\n",
            "step: 70, loss: 0.03372414410114288\n",
            "step: 80, loss: 0.07566142082214355\n",
            "step: 90, loss: 0.02701016142964363\n",
            "step: 100, loss: 0.023448975756764412\n",
            "step: 110, loss: 0.022619172930717468\n",
            "step: 120, loss: 0.06318160891532898\n",
            "step: 130, loss: 0.21218527853488922\n",
            "step: 140, loss: 0.053803060203790665\n",
            "step: 150, loss: 0.12056922912597656\n",
            "step: 160, loss: 0.018872451037168503\n",
            "step: 170, loss: 0.03600763529539108\n",
            "step: 180, loss: 0.05060649290680885\n",
            "step: 190, loss: 0.06778207421302795\n",
            "step: 200, loss: 0.033464182168245316\n",
            "step: 210, loss: 0.06493229418992996\n",
            "step: 220, loss: 0.0512474849820137\n",
            "step: 230, loss: 0.004148425534367561\n",
            "step: 240, loss: 0.006506418343633413\n",
            "step: 250, loss: 0.08905289322137833\n",
            "step: 260, loss: 0.00426099356263876\n",
            "step: 270, loss: 0.3049338459968567\n",
            "step: 280, loss: 0.03438476473093033\n",
            "step: 290, loss: 0.05466708913445473\n",
            "step: 300, loss: 0.15193502604961395\n",
            "step: 310, loss: 0.022829484194517136\n",
            "step: 320, loss: 0.07010871171951294\n",
            "step: 330, loss: 0.039905428886413574\n",
            "step: 340, loss: 0.05101151019334793\n",
            "step: 350, loss: 0.01749131642282009\n",
            "step: 360, loss: 0.07318411022424698\n",
            "step: 370, loss: 0.09608688205480576\n",
            "step: 380, loss: 0.056131161749362946\n",
            "step: 390, loss: 0.0410529188811779\n",
            "step: 400, loss: 0.03470990061759949\n",
            "step: 410, loss: 0.004191124811768532\n",
            "step: 420, loss: 0.17803867161273956\n",
            "step: 430, loss: 0.00814494676887989\n",
            "step: 440, loss: 0.21102160215377808\n",
            "step: 450, loss: 0.024528518319129944\n",
            "step: 460, loss: 0.11545268446207047\n",
            "step: 470, loss: 0.04326992854475975\n",
            "step: 480, loss: 0.3169393837451935\n",
            "step: 490, loss: 0.026460090652108192\n",
            "step: 500, loss: 0.14582459628582\n",
            "step: 510, loss: 0.015128230676054955\n",
            "step: 520, loss: 0.04868721216917038\n",
            "step: 530, loss: 0.008245762437582016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9336448598130841, f1=0.9369453526389537, best_f1=0.9369453526389537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06975121051073074\n",
            "step: 10, loss: 0.01457066461443901\n",
            "step: 20, loss: 0.031811464577913284\n",
            "step: 30, loss: 0.01792074181139469\n",
            "step: 40, loss: 0.012826628051698208\n",
            "step: 50, loss: 0.05442037060856819\n",
            "step: 60, loss: 0.012984794564545155\n",
            "step: 70, loss: 0.0013384164776653051\n",
            "step: 80, loss: 0.0016516183968633413\n",
            "step: 90, loss: 0.009069678373634815\n",
            "step: 100, loss: 0.016295913606882095\n",
            "step: 110, loss: 0.00478900084272027\n",
            "step: 120, loss: 0.0029611103236675262\n",
            "step: 130, loss: 0.013963501900434494\n",
            "step: 140, loss: 0.10192937403917313\n",
            "step: 150, loss: 0.0150893060490489\n",
            "step: 160, loss: 0.011182018555700779\n",
            "step: 170, loss: 0.06815513223409653\n",
            "step: 180, loss: 0.08099626004695892\n",
            "step: 190, loss: 0.009581775404512882\n",
            "step: 200, loss: 0.057819169014692307\n",
            "step: 210, loss: 0.044069401919841766\n",
            "step: 220, loss: 0.03628029674291611\n",
            "step: 230, loss: 0.0315762534737587\n",
            "step: 240, loss: 0.0031131517607718706\n",
            "step: 250, loss: 0.08951357752084732\n",
            "step: 260, loss: 0.016452694311738014\n",
            "step: 270, loss: 0.004566090647131205\n",
            "step: 280, loss: 0.09791824221611023\n",
            "step: 290, loss: 0.0024751676246523857\n",
            "step: 300, loss: 0.024491287767887115\n",
            "step: 310, loss: 0.0008743611397221684\n",
            "step: 320, loss: 0.09691835194826126\n",
            "step: 330, loss: 0.013068515807390213\n",
            "step: 340, loss: 0.018561463803052902\n",
            "step: 350, loss: 0.006175433751195669\n",
            "step: 360, loss: 0.05202168598771095\n",
            "step: 370, loss: 0.012686355970799923\n",
            "step: 380, loss: 0.01161677110940218\n",
            "step: 390, loss: 0.12569865584373474\n",
            "step: 400, loss: 0.02742885798215866\n",
            "step: 410, loss: 0.005186567083001137\n",
            "step: 420, loss: 0.37090256810188293\n",
            "step: 430, loss: 0.011059198528528214\n",
            "step: 440, loss: 0.002702200785279274\n",
            "step: 450, loss: 0.06044469401240349\n",
            "step: 460, loss: 0.05031370371580124\n",
            "step: 470, loss: 0.012616841122508049\n",
            "step: 480, loss: 0.002214850392192602\n",
            "step: 490, loss: 0.007964401505887508\n",
            "step: 500, loss: 0.049142878502607346\n",
            "step: 510, loss: 0.009635289199650288\n",
            "step: 520, loss: 0.19199681282043457\n",
            "step: 530, loss: 0.16258513927459717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9365671641791044, f1=0.9285042333019755, best_f1=0.9285042333019755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007492916192859411\n",
            "step: 10, loss: 0.002759241499006748\n",
            "step: 20, loss: 0.0018770701717585325\n",
            "step: 30, loss: 0.003592473454773426\n",
            "step: 40, loss: 0.001042340649291873\n",
            "step: 50, loss: 0.012981999665498734\n",
            "step: 60, loss: 0.0038510605227202177\n",
            "step: 70, loss: 0.0014340993948280811\n",
            "step: 80, loss: 0.016506753861904144\n",
            "step: 90, loss: 0.07209735363721848\n",
            "step: 100, loss: 0.006707184482365847\n",
            "step: 110, loss: 0.004747482016682625\n",
            "step: 120, loss: 0.0006303103873506188\n",
            "step: 130, loss: 0.0005276952288113534\n",
            "step: 140, loss: 0.0009954130509868264\n",
            "step: 150, loss: 0.03559499979019165\n",
            "step: 160, loss: 0.002098396187648177\n",
            "step: 170, loss: 0.0032029643189162016\n",
            "step: 180, loss: 0.02569214440882206\n",
            "step: 190, loss: 0.005160018336027861\n",
            "step: 200, loss: 0.007016859948635101\n",
            "step: 210, loss: 0.030721500515937805\n",
            "step: 220, loss: 0.001031847088597715\n",
            "step: 230, loss: 0.027903635054826736\n",
            "step: 240, loss: 0.00390934431925416\n",
            "step: 250, loss: 0.011181836947798729\n",
            "step: 260, loss: 0.0013633670751005411\n",
            "step: 270, loss: 0.0036296045873314142\n",
            "step: 280, loss: 0.021004987880587578\n",
            "step: 290, loss: 0.018851129338145256\n",
            "step: 300, loss: 0.0013192385667935014\n",
            "step: 310, loss: 0.0266936793923378\n",
            "step: 320, loss: 0.020666565746068954\n",
            "step: 330, loss: 0.01259284932166338\n",
            "step: 340, loss: 0.00614632572978735\n",
            "step: 350, loss: 0.006644006818532944\n",
            "step: 360, loss: 0.015004978515207767\n",
            "step: 370, loss: 0.007939326576888561\n",
            "step: 380, loss: 0.0029079008381813765\n",
            "step: 390, loss: 0.001929916557855904\n",
            "step: 400, loss: 0.0020435508340597153\n",
            "step: 410, loss: 0.0042672534473240376\n",
            "step: 420, loss: 0.011159161105751991\n",
            "step: 430, loss: 0.11848296225070953\n",
            "step: 440, loss: 0.022369230166077614\n",
            "step: 450, loss: 0.08038304001092911\n",
            "step: 460, loss: 0.03354453295469284\n",
            "step: 470, loss: 0.02126815915107727\n",
            "step: 480, loss: 0.0034557355102151632\n",
            "step: 490, loss: 0.0020450856536626816\n",
            "step: 500, loss: 0.01171035598963499\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 510, loss: 0.10241449624300003\n",
            "step: 520, loss: 0.22167733311653137\n",
            "step: 530, loss: 0.01547135878354311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9320565435476517, f1=0.9324137931034482, best_f1=0.9285042333019755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10346756875514984\n",
            "step: 10, loss: 0.050809722393751144\n",
            "step: 20, loss: 0.002394131850451231\n",
            "step: 30, loss: 0.0027144707273691893\n",
            "step: 40, loss: 0.003729406278580427\n",
            "step: 50, loss: 0.0006626932881772518\n",
            "step: 60, loss: 0.007492128759622574\n",
            "step: 70, loss: 0.014752635732293129\n",
            "step: 80, loss: 0.0006109355017542839\n",
            "step: 90, loss: 0.022680558264255524\n",
            "step: 100, loss: 0.007785417605191469\n",
            "step: 110, loss: 0.0007461195345968008\n",
            "step: 120, loss: 0.012243775650858879\n",
            "step: 130, loss: 0.00045587419299408793\n",
            "step: 140, loss: 0.0007731199730187654\n",
            "step: 150, loss: 0.00953905563801527\n",
            "step: 160, loss: 0.00036853132769465446\n",
            "step: 170, loss: 0.05217902362346649\n",
            "step: 180, loss: 0.0008127790642902255\n",
            "step: 190, loss: 0.004659904167056084\n",
            "step: 200, loss: 0.0005628687213174999\n",
            "step: 210, loss: 0.006715515162795782\n",
            "step: 220, loss: 0.0037746152374893427\n",
            "step: 230, loss: 0.01240452378988266\n",
            "step: 240, loss: 0.004047929774969816\n",
            "step: 250, loss: 0.013637342490255833\n",
            "step: 260, loss: 0.012312465347349644\n",
            "step: 270, loss: 0.0015669780550524592\n",
            "step: 280, loss: 0.10568627715110779\n",
            "step: 290, loss: 0.142738938331604\n",
            "step: 300, loss: 0.014439794234931469\n",
            "step: 310, loss: 0.0065361387096345425\n",
            "step: 320, loss: 0.052671585232019424\n",
            "step: 330, loss: 0.0983051136136055\n",
            "step: 340, loss: 0.0018195408629253507\n",
            "step: 350, loss: 0.0005099042900837958\n",
            "step: 360, loss: 0.015026210807263851\n",
            "step: 370, loss: 0.025258265435695648\n",
            "step: 380, loss: 0.0004290786455385387\n",
            "step: 390, loss: 0.00019371301459614187\n",
            "step: 400, loss: 0.0017979086842387915\n",
            "step: 410, loss: 0.015757808461785316\n",
            "step: 420, loss: 0.00868980772793293\n",
            "step: 430, loss: 0.01564270630478859\n",
            "step: 440, loss: 0.00795766618102789\n",
            "step: 450, loss: 0.16484849154949188\n",
            "step: 460, loss: 0.002219551708549261\n",
            "step: 470, loss: 0.07151418924331665\n",
            "step: 480, loss: 0.009022342972457409\n",
            "step: 490, loss: 0.0022501021157950163\n",
            "step: 500, loss: 0.005111621227115393\n",
            "step: 510, loss: 0.002897809259593487\n",
            "step: 520, loss: 0.0005703885108232498\n",
            "step: 530, loss: 0.0011419864604249597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9297094657919399, f1=0.9279026217228464, best_f1=0.9285042333019755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00919105764478445\n",
            "step: 10, loss: 0.00018541251483839005\n",
            "step: 20, loss: 0.09814789146184921\n",
            "step: 30, loss: 0.0009171895799227059\n",
            "step: 40, loss: 0.0641070008277893\n",
            "step: 50, loss: 0.01043115183711052\n",
            "step: 60, loss: 0.00034362688893452287\n",
            "step: 70, loss: 0.0003771652700379491\n",
            "step: 80, loss: 0.0009246297413483262\n",
            "step: 90, loss: 0.00017960413242690265\n",
            "step: 100, loss: 0.0025795435067266226\n",
            "step: 110, loss: 0.020391609519720078\n",
            "step: 120, loss: 0.0008681142353452742\n",
            "step: 130, loss: 0.0019777962006628513\n",
            "step: 140, loss: 0.04704009369015694\n",
            "step: 150, loss: 0.0024491252843290567\n",
            "step: 160, loss: 0.004819035530090332\n",
            "step: 170, loss: 0.007534568663686514\n",
            "step: 180, loss: 0.0034055905416607857\n",
            "step: 190, loss: 0.00036079654819332063\n",
            "step: 200, loss: 0.00019711852655746043\n",
            "step: 210, loss: 0.0029485225677490234\n",
            "step: 220, loss: 0.00020589676569215953\n",
            "step: 230, loss: 0.0003828612680081278\n",
            "step: 240, loss: 0.001260321238078177\n",
            "step: 250, loss: 0.015686841681599617\n",
            "step: 260, loss: 9.636517643230036e-05\n",
            "step: 270, loss: 0.05077638477087021\n",
            "step: 280, loss: 0.0008388060377910733\n",
            "step: 290, loss: 0.00031374130048789084\n",
            "step: 300, loss: 0.0028347070328891277\n",
            "step: 310, loss: 0.0006088482914492488\n",
            "step: 320, loss: 0.0009457318228669465\n",
            "step: 330, loss: 0.00045203379704616964\n",
            "step: 340, loss: 0.0054770647548139095\n",
            "step: 350, loss: 0.0006256599444895983\n",
            "step: 360, loss: 0.0031525942031294107\n",
            "step: 370, loss: 0.006640706676989794\n",
            "step: 380, loss: 0.0001342079194728285\n",
            "step: 390, loss: 0.036135852336883545\n",
            "step: 400, loss: 0.0001995862985495478\n",
            "step: 410, loss: 0.0011306841624900699\n",
            "step: 420, loss: 0.030599620193243027\n",
            "step: 430, loss: 0.036485761404037476\n",
            "step: 440, loss: 9.907634375849739e-05\n",
            "step: 450, loss: 0.0040053571574389935\n",
            "step: 460, loss: 7.730462675681338e-05\n",
            "step: 470, loss: 0.0005005723796784878\n",
            "step: 480, loss: 0.0074946996755898\n",
            "step: 490, loss: 0.004402755293995142\n",
            "step: 500, loss: 0.0011524097062647343\n",
            "step: 510, loss: 0.019221654161810875\n",
            "step: 520, loss: 0.021230660378932953\n",
            "step: 530, loss: 0.00446250569075346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9296173830892772, f1=0.9180483183325437, best_f1=0.9285042333019755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017569514457136393\n",
            "step: 10, loss: 0.0010619444074109197\n",
            "step: 20, loss: 0.0008445741259492934\n",
            "step: 30, loss: 0.0005572860245592892\n",
            "step: 40, loss: 0.09300735592842102\n",
            "step: 50, loss: 0.01120565365999937\n",
            "step: 60, loss: 0.000998001778498292\n",
            "step: 70, loss: 0.011368019506335258\n",
            "step: 80, loss: 0.033232927322387695\n",
            "step: 90, loss: 0.0004457421600818634\n",
            "step: 100, loss: 0.0003673556202556938\n",
            "step: 110, loss: 0.002418326446786523\n",
            "step: 120, loss: 0.0009219153434969485\n",
            "step: 130, loss: 0.01245032250881195\n",
            "step: 140, loss: 0.00021481038129422814\n",
            "step: 150, loss: 0.00023374526062980294\n",
            "step: 160, loss: 9.034380491357297e-05\n",
            "step: 170, loss: 0.004382355138659477\n",
            "step: 180, loss: 0.0005790955037809908\n",
            "step: 190, loss: 0.002783124800771475\n",
            "step: 200, loss: 0.0005719860782846808\n",
            "step: 210, loss: 0.00376767385751009\n",
            "step: 220, loss: 0.000876014877576381\n",
            "step: 230, loss: 0.0027522847522050142\n",
            "step: 240, loss: 0.004722734913229942\n",
            "step: 250, loss: 0.0006051286472938955\n",
            "step: 260, loss: 0.0006143860518932343\n",
            "step: 270, loss: 0.00179457594640553\n",
            "step: 280, loss: 0.00024736931663937867\n",
            "step: 290, loss: 0.0004081601509824395\n",
            "step: 300, loss: 0.0018931833328679204\n",
            "step: 310, loss: 7.993172766873613e-05\n",
            "step: 320, loss: 5.9337289712857455e-05\n",
            "step: 330, loss: 0.00045364152174443007\n",
            "step: 340, loss: 0.0418863371014595\n",
            "step: 350, loss: 0.04335104674100876\n",
            "step: 360, loss: 0.0016367494827136397\n",
            "step: 370, loss: 0.0001066683980752714\n",
            "step: 380, loss: 0.1734425574541092\n",
            "step: 390, loss: 0.016422223299741745\n",
            "step: 400, loss: 0.001609547296538949\n",
            "step: 410, loss: 0.001306032296270132\n",
            "step: 420, loss: 0.0013005140936002135\n",
            "step: 430, loss: 0.0001222880237037316\n",
            "step: 440, loss: 0.02634650282561779\n",
            "step: 450, loss: 0.000767081102821976\n",
            "step: 460, loss: 0.0004013872821815312\n",
            "step: 470, loss: 0.003984385170042515\n",
            "step: 480, loss: 0.06565458327531815\n",
            "step: 490, loss: 0.0007386918878182769\n",
            "step: 500, loss: 0.027139397338032722\n",
            "step: 510, loss: 0.003992523066699505\n",
            "step: 520, loss: 0.0014857195783406496\n",
            "step: 530, loss: 0.0014126661699265242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9402985074626865, f1=0.9274004683840749, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005649511702358723\n",
            "step: 10, loss: 0.02952253259718418\n",
            "step: 20, loss: 0.0008161460282281041\n",
            "step: 30, loss: 0.002377950819209218\n",
            "step: 40, loss: 0.0002004967536777258\n",
            "step: 50, loss: 0.0008602454327046871\n",
            "step: 60, loss: 0.0025440293829888105\n",
            "step: 70, loss: 0.00021612024283967912\n",
            "step: 80, loss: 0.0001906217512441799\n",
            "step: 90, loss: 0.00016565168334636837\n",
            "step: 100, loss: 0.0005826420383527875\n",
            "step: 110, loss: 0.0007577879587188363\n",
            "step: 120, loss: 0.0003309389285277575\n",
            "step: 130, loss: 0.003623978467658162\n",
            "step: 140, loss: 0.0013498942134901881\n",
            "step: 150, loss: 0.020085854455828667\n",
            "step: 160, loss: 0.00012922476162202656\n",
            "step: 170, loss: 0.001724899746477604\n",
            "step: 180, loss: 4.489791172090918e-05\n",
            "step: 190, loss: 0.0002774458553176373\n",
            "step: 200, loss: 0.002653789008036256\n",
            "step: 210, loss: 0.0005975327803753316\n",
            "step: 220, loss: 0.0007945027900859714\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 230, loss: 0.01891220360994339\n",
            "step: 240, loss: 7.476117025362328e-05\n",
            "step: 250, loss: 0.04799959063529968\n",
            "step: 260, loss: 0.00019383961625862867\n",
            "step: 270, loss: 0.010745150037109852\n",
            "step: 280, loss: 0.0013496900210157037\n",
            "step: 290, loss: 0.00019909509865101427\n",
            "step: 300, loss: 0.0007668319740332663\n",
            "step: 310, loss: 0.00016569055151194334\n",
            "step: 320, loss: 0.019122913479804993\n",
            "step: 330, loss: 0.001248071901500225\n",
            "step: 340, loss: 0.0017471954924985766\n",
            "step: 350, loss: 0.0002674379211384803\n",
            "step: 360, loss: 9.704499825602397e-05\n",
            "step: 370, loss: 0.0006114374264143407\n",
            "step: 380, loss: 0.0003256397321820259\n",
            "step: 390, loss: 0.09835092723369598\n",
            "step: 400, loss: 0.0008269482059404254\n",
            "step: 410, loss: 0.0005140863941051066\n",
            "step: 420, loss: 0.0003349227481521666\n",
            "step: 430, loss: 0.014690757729113102\n",
            "step: 440, loss: 0.0004658898978959769\n",
            "step: 450, loss: 0.00036529835779219866\n",
            "step: 460, loss: 0.004712519235908985\n",
            "step: 470, loss: 4.791836909134872e-05\n",
            "step: 480, loss: 0.004296299535781145\n",
            "step: 490, loss: 0.00012910933583043516\n",
            "step: 500, loss: 9.6085415862035e-05\n",
            "step: 510, loss: 0.00012685115507338196\n",
            "step: 520, loss: 0.002945579355582595\n",
            "step: 530, loss: 2.8113478037994355e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9332087809434843, f1=0.924953095684803, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001299262628890574\n",
            "step: 10, loss: 7.051140710245818e-05\n",
            "step: 20, loss: 0.043456826359033585\n",
            "step: 30, loss: 4.146090941503644e-05\n",
            "step: 40, loss: 0.00048336584586650133\n",
            "step: 50, loss: 3.14868739224039e-05\n",
            "step: 60, loss: 8.85341432876885e-05\n",
            "step: 70, loss: 0.0013906779931858182\n",
            "step: 80, loss: 0.005386752542108297\n",
            "step: 90, loss: 0.00010001431655837223\n",
            "step: 100, loss: 6.605195812880993e-05\n",
            "step: 110, loss: 5.2356150263221934e-05\n",
            "step: 120, loss: 5.911454718443565e-05\n",
            "step: 130, loss: 0.002570055890828371\n",
            "step: 140, loss: 0.0016758347628638148\n",
            "step: 150, loss: 3.9567326894029975e-05\n",
            "step: 160, loss: 0.00020641941227950156\n",
            "step: 170, loss: 0.004567799158394337\n",
            "step: 180, loss: 7.228335016407073e-05\n",
            "step: 190, loss: 3.566546365618706e-05\n",
            "step: 200, loss: 5.2656665502581745e-05\n",
            "step: 210, loss: 2.2857735530124046e-05\n",
            "step: 220, loss: 6.460276199504733e-05\n",
            "step: 230, loss: 3.087332152063027e-05\n",
            "step: 240, loss: 6.97513751219958e-05\n",
            "step: 250, loss: 0.00035152232157997787\n",
            "step: 260, loss: 0.009947218000888824\n",
            "step: 270, loss: 0.002002053428441286\n",
            "step: 280, loss: 0.0006545149371959269\n",
            "step: 290, loss: 0.023777266964316368\n",
            "step: 300, loss: 3.691590245580301e-05\n",
            "step: 310, loss: 0.0008561965660192072\n",
            "step: 320, loss: 0.029801707714796066\n",
            "step: 330, loss: 8.99302976904437e-05\n",
            "step: 340, loss: 0.00015198663459159434\n",
            "step: 350, loss: 0.0015037282137200236\n",
            "step: 360, loss: 5.536551907425746e-05\n",
            "step: 370, loss: 0.039273813366889954\n",
            "step: 380, loss: 0.0036603461485356092\n",
            "step: 390, loss: 3.8934660551603884e-05\n",
            "step: 400, loss: 0.00015583647473249584\n",
            "step: 410, loss: 0.0007578007644042373\n",
            "step: 420, loss: 0.00013494912127498537\n",
            "step: 430, loss: 5.213002805248834e-05\n",
            "step: 440, loss: 0.001938357949256897\n",
            "step: 450, loss: 0.00010485580423846841\n",
            "step: 460, loss: 0.00016464856162201613\n",
            "step: 470, loss: 3.153271973133087e-05\n",
            "step: 480, loss: 4.082572922925465e-05\n",
            "step: 490, loss: 0.0005395914195105433\n",
            "step: 500, loss: 0.005132363177835941\n",
            "step: 510, loss: 0.0005518170073628426\n",
            "step: 520, loss: 0.002375206211581826\n",
            "step: 530, loss: 8.784775855019689e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9371215649743828, f1=0.9302107728337237, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007114681648090482\n",
            "step: 10, loss: 7.716967957094312e-05\n",
            "step: 20, loss: 5.131991201778874e-05\n",
            "step: 30, loss: 0.00043628033017739654\n",
            "step: 40, loss: 0.0003010965301655233\n",
            "step: 50, loss: 7.060759526211768e-05\n",
            "step: 60, loss: 3.8357342418748885e-05\n",
            "step: 70, loss: 2.440757270960603e-05\n",
            "step: 80, loss: 3.082543844357133e-05\n",
            "step: 90, loss: 3.119808388873935e-05\n",
            "step: 100, loss: 0.0030369299929589033\n",
            "step: 110, loss: 1.926676486618817e-05\n",
            "step: 120, loss: 0.0001581173128215596\n",
            "step: 130, loss: 0.00011900313256774098\n",
            "step: 140, loss: 0.0023177252151072025\n",
            "step: 150, loss: 3.1286901503335685e-05\n",
            "step: 160, loss: 2.681357364053838e-05\n",
            "step: 170, loss: 0.00011587983317440376\n",
            "step: 180, loss: 5.8231718867318705e-05\n",
            "step: 190, loss: 2.4910445063142106e-05\n",
            "step: 200, loss: 0.0002955813833978027\n",
            "step: 210, loss: 0.0004172890039626509\n",
            "step: 220, loss: 0.00043394090607762337\n",
            "step: 230, loss: 0.0009984101634472609\n",
            "step: 240, loss: 0.0008375747711397707\n",
            "step: 250, loss: 2.1501895389519632e-05\n",
            "step: 260, loss: 0.0019073731964454055\n",
            "step: 270, loss: 0.00010120368096977472\n",
            "step: 280, loss: 6.008896525599994e-05\n",
            "step: 290, loss: 2.1758813090855256e-05\n",
            "step: 300, loss: 2.1151585315237753e-05\n",
            "step: 310, loss: 0.0001575176283949986\n",
            "step: 320, loss: 0.0006395849050022662\n",
            "step: 330, loss: 3.642858428065665e-05\n",
            "step: 340, loss: 3.4458924346836284e-05\n",
            "step: 350, loss: 3.781215855269693e-05\n",
            "step: 360, loss: 6.036788909113966e-05\n",
            "step: 370, loss: 0.00036748923594132066\n",
            "step: 380, loss: 0.00011972471838817\n",
            "step: 390, loss: 0.002573252422735095\n",
            "step: 400, loss: 0.00020737577870022506\n",
            "step: 410, loss: 0.0007487852708436549\n",
            "step: 420, loss: 0.0003101800393778831\n",
            "step: 430, loss: 1.7497368389740586e-05\n",
            "step: 440, loss: 8.550158963771537e-05\n",
            "step: 450, loss: 3.198936246917583e-05\n",
            "step: 460, loss: 2.2335994799504988e-05\n",
            "step: 470, loss: 0.0057886624708771706\n",
            "step: 480, loss: 3.692874815897085e-05\n",
            "step: 490, loss: 0.048236824572086334\n",
            "step: 500, loss: 0.033361632376909256\n",
            "step: 510, loss: 0.006759658921509981\n",
            "step: 520, loss: 0.0003125386137980968\n",
            "step: 530, loss: 0.0001384680945193395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9345266132830901, f1=0.9243065350258581, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00041772244730964303\n",
            "step: 10, loss: 0.0015536301070824265\n",
            "step: 20, loss: 0.000367827364243567\n",
            "step: 30, loss: 0.0010490705026313663\n",
            "step: 40, loss: 2.4675597160239704e-05\n",
            "step: 50, loss: 0.0003986067895311862\n",
            "step: 60, loss: 0.03640146926045418\n",
            "step: 70, loss: 3.3316926419502124e-05\n",
            "step: 80, loss: 3.165148154948838e-05\n",
            "step: 90, loss: 7.792739052092656e-05\n",
            "step: 100, loss: 0.00013950321590527892\n",
            "step: 110, loss: 0.0029623592272400856\n",
            "step: 120, loss: 6.747170118615031e-05\n",
            "step: 130, loss: 1.9616807549027726e-05\n",
            "step: 140, loss: 0.001663219416514039\n",
            "step: 150, loss: 1.817902193579357e-05\n",
            "step: 160, loss: 0.00029436047771014273\n",
            "step: 170, loss: 2.6441397494636476e-05\n",
            "step: 180, loss: 3.981712143286131e-05\n",
            "step: 190, loss: 0.0005939934053458273\n",
            "step: 200, loss: 0.06585351377725601\n",
            "step: 210, loss: 0.00012789653555955738\n",
            "step: 220, loss: 0.005029933527112007\n",
            "step: 230, loss: 2.3460472220904194e-05\n",
            "step: 240, loss: 0.001699427724815905\n",
            "step: 250, loss: 2.6462377718416974e-05\n",
            "step: 260, loss: 8.456937939627096e-05\n",
            "step: 270, loss: 0.004472623113542795\n",
            "step: 280, loss: 8.276801236206666e-05\n",
            "step: 290, loss: 5.5196756875375286e-05\n",
            "step: 300, loss: 3.50514892488718e-05\n",
            "step: 310, loss: 2.4172342818928882e-05\n",
            "step: 320, loss: 4.2951200157403946e-05\n",
            "step: 330, loss: 6.021286390023306e-05\n",
            "step: 340, loss: 0.0015518140280619264\n",
            "step: 350, loss: 0.004899853374809027\n",
            "step: 360, loss: 2.365064574405551e-05\n",
            "step: 370, loss: 0.09552344679832458\n",
            "step: 380, loss: 2.0723340639960952e-05\n",
            "step: 390, loss: 3.827585169347003e-05\n",
            "step: 400, loss: 4.5884287828812376e-05\n",
            "step: 410, loss: 0.00036606189678423107\n",
            "step: 420, loss: 4.5309938286663964e-05\n",
            "step: 430, loss: 0.00017957072122953832\n",
            "step: 440, loss: 0.00013035045412834734\n",
            "step: 450, loss: 3.748282324522734e-05\n",
            "step: 460, loss: 0.00013507110998034477\n",
            "step: 470, loss: 0.0014946083538234234\n",
            "step: 480, loss: 0.00046258026850409806\n",
            "step: 490, loss: 4.6745171857764944e-05\n",
            "step: 500, loss: 0.00021093030227348208\n",
            "step: 510, loss: 0.00011351694411132485\n",
            "step: 520, loss: 3.0479235647362657e-05\n",
            "step: 530, loss: 0.0004043527296744287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9311969839773798, f1=0.9227144203581528, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.5705975682940334e-05\n",
            "step: 10, loss: 1.7635233234614134e-05\n",
            "step: 20, loss: 0.0019430659012869\n",
            "step: 30, loss: 3.9394915802404284e-05\n",
            "step: 40, loss: 8.253785199485719e-05\n",
            "step: 50, loss: 0.03316566348075867\n",
            "step: 60, loss: 0.0007463502115570009\n",
            "step: 70, loss: 2.8545975510496646e-05\n",
            "step: 80, loss: 3.5097546060569584e-05\n",
            "step: 90, loss: 2.6637677365215495e-05\n",
            "step: 100, loss: 2.9346099836402573e-05\n",
            "step: 110, loss: 3.214443495380692e-05\n",
            "step: 120, loss: 4.434544825926423e-05\n",
            "step: 130, loss: 5.3667557949665934e-05\n",
            "step: 140, loss: 2.038434467976913e-05\n",
            "step: 150, loss: 1.4677462786494289e-05\n",
            "step: 160, loss: 1.5906758562778123e-05\n",
            "step: 170, loss: 0.00014764147636014968\n",
            "step: 180, loss: 2.614285949675832e-05\n",
            "step: 190, loss: 3.473127435427159e-05\n",
            "step: 200, loss: 2.8027865482727066e-05\n",
            "step: 210, loss: 3.318592644063756e-05\n",
            "step: 220, loss: 0.0006201990763656795\n",
            "step: 230, loss: 1.174202134279767e-05\n",
            "step: 240, loss: 3.466839189059101e-05\n",
            "step: 250, loss: 1.9706216335180216e-05\n",
            "step: 260, loss: 5.120319110574201e-05\n",
            "step: 270, loss: 2.7197684175916947e-05\n",
            "step: 280, loss: 0.0016430671093985438\n",
            "step: 290, loss: 0.0006626711110584438\n",
            "step: 300, loss: 0.00013850329560227692\n",
            "step: 310, loss: 0.00010544699034653604\n",
            "step: 320, loss: 1.9739847630262375e-05\n",
            "step: 330, loss: 5.188323848415166e-05\n",
            "step: 340, loss: 0.020327774807810783\n",
            "step: 350, loss: 2.939539081125986e-05\n",
            "step: 360, loss: 0.0006560836918652058\n",
            "step: 370, loss: 0.0010972294257953763\n",
            "step: 380, loss: 0.00010496489994693547\n",
            "step: 390, loss: 9.683080861577764e-05\n",
            "step: 400, loss: 6.834600935690105e-05\n",
            "step: 410, loss: 0.0007855519070290029\n",
            "step: 420, loss: 0.006071730982512236\n",
            "step: 430, loss: 0.00023687808425165713\n",
            "step: 440, loss: 0.0002494214568287134\n",
            "step: 450, loss: 5.9113528550369665e-05\n",
            "step: 460, loss: 0.0003947076038457453\n",
            "step: 470, loss: 3.3747823181329295e-05\n",
            "step: 480, loss: 4.328229624661617e-05\n",
            "step: 490, loss: 0.00014057981024961919\n",
            "step: 500, loss: 0.00031248919549398124\n",
            "step: 510, loss: 0.00036529178032651544\n",
            "step: 520, loss: 0.00014420950901694596\n",
            "step: 530, loss: 6.621011561946943e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9359925788497218, f1=0.9290681502086231, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006201095529831946\n",
            "step: 10, loss: 3.187154288752936e-05\n",
            "step: 20, loss: 0.0006850226200185716\n",
            "step: 30, loss: 7.673575601074845e-05\n",
            "step: 40, loss: 2.744991797953844e-05\n",
            "step: 50, loss: 0.0007504629320465028\n",
            "step: 60, loss: 1.728871211525984e-05\n",
            "step: 70, loss: 3.12221709464211e-05\n",
            "step: 80, loss: 3.818076947936788e-05\n",
            "step: 90, loss: 0.016168231144547462\n",
            "step: 100, loss: 0.0032151085324585438\n",
            "step: 110, loss: 1.4629058568971232e-05\n",
            "step: 120, loss: 2.4105527700157836e-05\n",
            "step: 130, loss: 1.6290485291392542e-05\n",
            "step: 140, loss: 2.6943243938148953e-05\n",
            "step: 150, loss: 0.00010614572238409892\n",
            "step: 160, loss: 1.782880826795008e-05\n",
            "step: 170, loss: 9.289410809287801e-05\n",
            "step: 180, loss: 2.3397922632284462e-05\n",
            "step: 190, loss: 0.0006416430696845055\n",
            "step: 200, loss: 4.204725701129064e-05\n",
            "step: 210, loss: 3.05758039758075e-05\n",
            "step: 220, loss: 1.978429099835921e-05\n",
            "step: 230, loss: 3.62702994607389e-05\n",
            "step: 240, loss: 0.0010473011061549187\n",
            "step: 250, loss: 1.8998433006345294e-05\n",
            "step: 260, loss: 2.9415055905701593e-05\n",
            "step: 270, loss: 2.9060405722702853e-05\n",
            "step: 280, loss: 1.7869897419586778e-05\n",
            "step: 290, loss: 2.8170101359137334e-05\n",
            "step: 300, loss: 2.4478144041495398e-05\n",
            "step: 310, loss: 0.0015843127621337771\n",
            "step: 320, loss: 0.0010837140725925565\n",
            "step: 330, loss: 0.00034626887645572424\n",
            "step: 340, loss: 3.410403951420449e-05\n",
            "step: 350, loss: 2.557227708166465e-05\n",
            "step: 360, loss: 1.4658866348327138e-05\n",
            "step: 370, loss: 0.00014241307508200407\n",
            "step: 380, loss: 9.817010868573561e-05\n",
            "step: 390, loss: 0.0004302476882003248\n",
            "step: 400, loss: 4.0936625737231225e-05\n",
            "step: 410, loss: 0.0002654832205735147\n",
            "step: 420, loss: 2.2037998860469088e-05\n",
            "step: 430, loss: 2.1631929485010915e-05\n",
            "step: 440, loss: 3.0764451366849244e-05\n",
            "step: 450, loss: 1.788464214769192e-05\n",
            "step: 460, loss: 0.00021421707060653716\n",
            "step: 470, loss: 2.467705235176254e-05\n",
            "step: 480, loss: 3.349994221935049e-05\n",
            "step: 490, loss: 3.124537761323154e-05\n",
            "step: 500, loss: 2.8276388547965325e-05\n",
            "step: 510, loss: 0.0002221287868451327\n",
            "step: 520, loss: 7.448783435393125e-05\n",
            "step: 530, loss: 1.235667059518164e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9363891487371375, f1=0.9271028037383178, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007328671053983271\n",
            "step: 10, loss: 2.3744252757751383e-05\n",
            "step: 20, loss: 1.4576862668036483e-05\n",
            "step: 30, loss: 5.477751255966723e-05\n",
            "step: 40, loss: 0.0002036787336692214\n",
            "step: 50, loss: 0.0005668638623319566\n",
            "step: 60, loss: 1.4975248632254079e-05\n",
            "step: 70, loss: 2.2182746761245653e-05\n",
            "step: 80, loss: 0.0022441574838012457\n",
            "step: 90, loss: 0.00017792658763937652\n",
            "step: 100, loss: 0.0003266620624344796\n",
            "step: 110, loss: 0.0015486186603084207\n",
            "step: 120, loss: 0.0001350520469713956\n",
            "step: 130, loss: 0.021363994106650352\n",
            "step: 140, loss: 0.0006570083787664771\n",
            "step: 150, loss: 1.9582970708142966e-05\n",
            "step: 160, loss: 8.283081115223467e-05\n",
            "step: 170, loss: 1.8447224647388794e-05\n",
            "step: 180, loss: 1.2889329809695482e-05\n",
            "step: 190, loss: 1.4897223991283681e-05\n",
            "step: 200, loss: 8.817889465717599e-05\n",
            "step: 210, loss: 1.3671680790139362e-05\n",
            "step: 220, loss: 7.320961594814435e-05\n",
            "step: 230, loss: 0.0002610245137475431\n",
            "step: 240, loss: 0.00011487940355436876\n",
            "step: 250, loss: 3.771199772018008e-05\n",
            "step: 260, loss: 0.00020289747044444084\n",
            "step: 270, loss: 9.505728667136282e-05\n",
            "step: 280, loss: 1.10081527964212e-05\n",
            "step: 290, loss: 0.00010937628394458443\n",
            "step: 300, loss: 0.003583427518606186\n",
            "step: 310, loss: 2.6750083634397015e-05\n",
            "step: 320, loss: 0.0008289710385724902\n",
            "step: 330, loss: 7.503705273848027e-05\n",
            "step: 340, loss: 5.608183710137382e-05\n",
            "step: 350, loss: 6.457961717387661e-05\n",
            "step: 360, loss: 9.652004519011825e-05\n",
            "step: 370, loss: 3.640705836005509e-05\n",
            "step: 380, loss: 2.610169212857727e-05\n",
            "step: 390, loss: 0.022483158856630325\n",
            "step: 400, loss: 4.254343730281107e-05\n",
            "step: 410, loss: 2.2577956769964658e-05\n",
            "step: 420, loss: 1.3362430763663724e-05\n",
            "step: 430, loss: 2.34536528296303e-05\n",
            "step: 440, loss: 4.001690831501037e-05\n",
            "step: 450, loss: 4.2735471652122214e-05\n",
            "step: 460, loss: 0.000137261493364349\n",
            "step: 470, loss: 3.3677672035992146e-05\n",
            "step: 480, loss: 1.774636075424496e-05\n",
            "step: 490, loss: 1.3626943655253854e-05\n",
            "step: 500, loss: 1.0773443136713468e-05\n",
            "step: 510, loss: 2.412403773632832e-05\n",
            "step: 520, loss: 4.7541230742353946e-05\n",
            "step: 530, loss: 0.006902511231601238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9353932584269663, f1=0.9243697478991597, best_f1=0.9274004683840749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.2688208698818926e-05\n",
            "step: 10, loss: 0.0004908169503323734\n",
            "step: 20, loss: 5.6036049500107765e-05\n",
            "step: 30, loss: 0.0005577130359597504\n",
            "step: 40, loss: 0.0014463234692811966\n",
            "step: 50, loss: 0.00013508116535376757\n",
            "step: 60, loss: 1.1127347534056753e-05\n",
            "step: 70, loss: 0.00014487304724752903\n",
            "step: 80, loss: 2.0462033717194572e-05\n",
            "step: 90, loss: 0.00017333406140096486\n",
            "step: 100, loss: 3.130375262117013e-05\n",
            "step: 110, loss: 2.390286863374058e-05\n",
            "step: 120, loss: 3.5558263334678486e-05\n",
            "step: 130, loss: 0.08448746800422668\n",
            "step: 140, loss: 0.0002297312894370407\n",
            "step: 150, loss: 2.3009850337984972e-05\n",
            "step: 160, loss: 5.360241266316734e-05\n",
            "step: 170, loss: 1.906897705339361e-05\n",
            "step: 180, loss: 1.2688205060840119e-05\n",
            "step: 190, loss: 3.868569547194056e-05\n",
            "step: 200, loss: 3.1839299481362104e-05\n",
            "step: 210, loss: 0.00013505203241948038\n",
            "step: 220, loss: 1.4420361367228907e-05\n",
            "step: 230, loss: 8.547288598492742e-05\n",
            "step: 240, loss: 1.8528960936237127e-05\n",
            "step: 250, loss: 8.475377399008721e-05\n",
            "step: 260, loss: 1.445395355403889e-05\n",
            "step: 270, loss: 1.3567360838351306e-05\n",
            "step: 280, loss: 0.00011186140909558162\n",
            "step: 290, loss: 9.25728181755403e-06\n",
            "step: 300, loss: 1.3269265764392912e-05\n",
            "step: 310, loss: 3.081019895034842e-05\n",
            "step: 320, loss: 2.5159504730254412e-05\n",
            "step: 330, loss: 5.5133110436145216e-05\n",
            "step: 340, loss: 6.559021130669862e-05\n",
            "step: 350, loss: 2.0661782400566153e-05\n",
            "step: 360, loss: 0.0013796997955068946\n",
            "step: 370, loss: 1.8360920876148157e-05\n",
            "step: 380, loss: 1.443525525246514e-05\n",
            "step: 390, loss: 2.68652420345461e-05\n",
            "step: 400, loss: 0.0002444024430587888\n",
            "step: 410, loss: 0.00021050851501058787\n",
            "step: 420, loss: 2.253223465231713e-05\n",
            "step: 430, loss: 4.050443021696992e-05\n",
            "step: 440, loss: 4.0849812648957595e-05\n",
            "step: 450, loss: 1.3507632502296474e-05\n",
            "step: 460, loss: 3.8963127735769376e-05\n",
            "step: 470, loss: 6.549627869389951e-05\n",
            "step: 480, loss: 3.252438546041958e-05\n",
            "step: 490, loss: 1.2259786672075279e-05\n",
            "step: 500, loss: 1.4204324543243274e-05\n",
            "step: 510, loss: 3.122250200249255e-05\n",
            "step: 520, loss: 1.1622805686783977e-05\n",
            "step: 530, loss: 5.172895544092171e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9393656716417911, f1=0.9291338582677167, best_f1=0.9274004683840749\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:30, 187.81it/s]\n",
            "load_f1 = 0.9367789570835257\n",
            "real_f1 = 0.9343880874825501\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15cfab4-c8c2-4a05-9f76-175a3d0f9251"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5511002540588379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5, f1=0.4210526315789474, best_f1=0.4210526315789474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.49595898389816284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.5, f1=0.43137254901960786, best_f1=0.4210526315789474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.608761727809906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6153846153846153, f1=0.45, best_f1=0.45\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25722289085388184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.588235294117647, f1=0.46511627906976755, best_f1=0.45\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2601982355117798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7586206896551724, f1=0.5641025641025641, best_f1=0.5641025641025641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28989189863204956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6666666666666666, f1=0.6250000000000001, best_f1=0.5641025641025641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07463992387056351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8235294117647058, f1=0.6, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01574951969087124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8125000000000001, f1=0.6486486486486486, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004100667778402567\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6923076923076924, f1=0.6451612903225806, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03464384377002716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7878787878787878, f1=0.5714285714285714, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006523944437503815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7878787878787878, f1=0.5555555555555556, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022012202069163322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.75, f1=0.588235294117647, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007865325547754765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.75, f1=0.588235294117647, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01728222891688347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.75, f1=0.588235294117647, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00888928771018982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.75, f1=0.588235294117647, best_f1=0.6\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 140686.20it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7333333333333334\n",
            "real_f1 = 0.6875000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 251.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488cb556-c5df-469a-a616-d871d0f337a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6061214804649353\n",
            "step: 10, loss: 0.605292022228241\n",
            "step: 20, loss: 0.3887988030910492\n",
            "step: 30, loss: 0.16632089018821716\n",
            "step: 40, loss: 0.2980862259864807\n",
            "step: 50, loss: 0.023008793592453003\n",
            "step: 60, loss: 0.07408381253480911\n",
            "step: 70, loss: 0.043123237788677216\n",
            "step: 80, loss: 0.05447756126523018\n",
            "step: 90, loss: 0.08592504262924194\n",
            "step: 100, loss: 0.06027046963572502\n",
            "step: 110, loss: 0.1549503654241562\n",
            "step: 120, loss: 0.04348963126540184\n",
            "step: 130, loss: 0.010631104931235313\n",
            "step: 140, loss: 0.006415170151740313\n",
            "step: 150, loss: 0.005066617392003536\n",
            "step: 160, loss: 0.031125571578741074\n",
            "step: 170, loss: 0.2472187727689743\n",
            "step: 180, loss: 0.02043755166232586\n",
            "step: 190, loss: 0.05109798163175583\n",
            "step: 200, loss: 0.007822808809578419\n",
            "step: 210, loss: 0.011585514061152935\n",
            "step: 220, loss: 0.003178322222083807\n",
            "step: 230, loss: 0.06561879068613052\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9799107142857142, f1=0.9772209567198178, best_f1=0.9772209567198178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00391040463000536\n",
            "step: 10, loss: 0.0073185451328754425\n",
            "step: 20, loss: 0.1593630611896515\n",
            "step: 30, loss: 0.19522452354431152\n",
            "step: 40, loss: 0.07574471086263657\n",
            "step: 50, loss: 0.002732407534494996\n",
            "step: 60, loss: 0.005066982004791498\n",
            "step: 70, loss: 0.02658754214644432\n",
            "step: 80, loss: 0.0027237138710916042\n",
            "step: 90, loss: 0.042431578040122986\n",
            "step: 100, loss: 0.04207243397831917\n",
            "step: 110, loss: 0.08846672624349594\n",
            "step: 120, loss: 0.002397255739197135\n",
            "step: 130, loss: 0.007656731642782688\n",
            "step: 140, loss: 0.007012381684035063\n",
            "step: 150, loss: 0.002294846810400486\n",
            "step: 160, loss: 0.0028200275264680386\n",
            "step: 170, loss: 0.003613975364714861\n",
            "step: 180, loss: 0.0037960782647132874\n",
            "step: 190, loss: 0.10597337037324905\n",
            "step: 200, loss: 0.02133975736796856\n",
            "step: 210, loss: 0.0013057009782642126\n",
            "step: 220, loss: 0.10195448249578476\n",
            "step: 230, loss: 0.05652402341365814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9898989898989898, f1=0.9886877828054299, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0025497782044112682\n",
            "step: 10, loss: 0.007293289992958307\n",
            "step: 20, loss: 0.0007460599881596863\n",
            "step: 30, loss: 0.005540187004953623\n",
            "step: 40, loss: 0.061241935938596725\n",
            "step: 50, loss: 0.010928564704954624\n",
            "step: 60, loss: 0.001921224407851696\n",
            "step: 70, loss: 0.0015334610361605883\n",
            "step: 80, loss: 0.0022647767327725887\n",
            "step: 90, loss: 0.0438607819378376\n",
            "step: 100, loss: 0.01093264203518629\n",
            "step: 110, loss: 0.0022617941722273827\n",
            "step: 120, loss: 0.015088518150150776\n",
            "step: 130, loss: 0.002209986327216029\n",
            "step: 140, loss: 0.0558207631111145\n",
            "step: 150, loss: 0.004774031694978476\n",
            "step: 160, loss: 0.003969457000494003\n",
            "step: 170, loss: 0.015985697507858276\n",
            "step: 180, loss: 0.0677664577960968\n",
            "step: 190, loss: 0.034740906208753586\n",
            "step: 200, loss: 0.008106322027742863\n",
            "step: 210, loss: 0.05339689180254936\n",
            "step: 220, loss: 0.0010800090385600924\n",
            "step: 230, loss: 0.004561154637485743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9898305084745763, f1=0.9853438556933484, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012332913465797901\n",
            "step: 10, loss: 0.0004992689937353134\n",
            "step: 20, loss: 0.00168679712805897\n",
            "step: 30, loss: 0.001181506086140871\n",
            "step: 40, loss: 0.003037896938621998\n",
            "step: 50, loss: 0.0004470893763937056\n",
            "step: 60, loss: 0.0015111371176317334\n",
            "step: 70, loss: 0.023020479828119278\n",
            "step: 80, loss: 0.0032547004520893097\n",
            "step: 90, loss: 0.029678937047719955\n",
            "step: 100, loss: 0.0003864556201733649\n",
            "step: 110, loss: 0.0005500918487086892\n",
            "step: 120, loss: 0.002838586922734976\n",
            "step: 130, loss: 0.0007012824644334614\n",
            "step: 140, loss: 0.0009238818311132491\n",
            "step: 150, loss: 0.08191327005624771\n",
            "step: 160, loss: 0.0523848831653595\n",
            "step: 170, loss: 0.00799059122800827\n",
            "step: 180, loss: 0.00031192589085549116\n",
            "step: 190, loss: 0.0013671924825757742\n",
            "step: 200, loss: 0.000588251743465662\n",
            "step: 210, loss: 0.007289730943739414\n",
            "step: 220, loss: 0.0006517054280266166\n",
            "step: 230, loss: 0.02521209977567196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9853107344632768, f1=0.9841628959276018, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003307521343231201\n",
            "step: 10, loss: 0.0008297530584968626\n",
            "step: 20, loss: 0.04524804651737213\n",
            "step: 30, loss: 0.0006034502875991166\n",
            "step: 40, loss: 0.00032416798057965934\n",
            "step: 50, loss: 0.00018161571642849594\n",
            "step: 60, loss: 0.022050585597753525\n",
            "step: 70, loss: 0.00024709911667741835\n",
            "step: 80, loss: 0.0002273024438181892\n",
            "step: 90, loss: 0.0007357497233897448\n",
            "step: 100, loss: 0.0006166977109387517\n",
            "step: 110, loss: 0.00018078717403113842\n",
            "step: 120, loss: 0.00016613602929282933\n",
            "step: 130, loss: 0.005447994451969862\n",
            "step: 140, loss: 0.01188534963876009\n",
            "step: 150, loss: 0.0012088310904800892\n",
            "step: 160, loss: 0.0015741604147478938\n",
            "step: 170, loss: 0.006835451815277338\n",
            "step: 180, loss: 0.0021578832529485226\n",
            "step: 190, loss: 0.001614658278413117\n",
            "step: 200, loss: 0.024035369977355003\n",
            "step: 210, loss: 0.04846566542983055\n",
            "step: 220, loss: 0.002510604914277792\n",
            "step: 230, loss: 0.00017924320127349347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9853438556933484, f1=0.9797752808988766, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026651102234609425\n",
            "step: 10, loss: 0.0008472128538414836\n",
            "step: 20, loss: 0.016094785183668137\n",
            "step: 30, loss: 0.001243659295141697\n",
            "step: 40, loss: 0.0003955017600674182\n",
            "step: 50, loss: 0.1546647995710373\n",
            "step: 60, loss: 0.001267924322746694\n",
            "step: 70, loss: 0.007335457485169172\n",
            "step: 80, loss: 0.0066327317617833614\n",
            "step: 90, loss: 0.008627158589661121\n",
            "step: 100, loss: 0.003794071264564991\n",
            "step: 110, loss: 0.0013208953896537423\n",
            "step: 120, loss: 0.0070229824632406235\n",
            "step: 130, loss: 0.0024461434222757816\n",
            "step: 140, loss: 0.0008551275823265314\n",
            "step: 150, loss: 0.0020201208535581827\n",
            "step: 160, loss: 0.020362215116620064\n",
            "step: 170, loss: 0.06317215412855148\n",
            "step: 180, loss: 0.0048872968181967735\n",
            "step: 190, loss: 0.03498687967658043\n",
            "step: 200, loss: 0.0008415665361098945\n",
            "step: 210, loss: 0.001979719614610076\n",
            "step: 220, loss: 0.00032641933648847044\n",
            "step: 230, loss: 0.08591833710670471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.983277591973244, f1=0.9777777777777777, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0063486103899776936\n",
            "step: 10, loss: 0.00015621005150023848\n",
            "step: 20, loss: 0.0016572794411331415\n",
            "step: 30, loss: 0.00028028644737787545\n",
            "step: 40, loss: 0.0006096633733250201\n",
            "step: 50, loss: 0.00042008605669252574\n",
            "step: 60, loss: 0.002174234250560403\n",
            "step: 70, loss: 0.006069842725992203\n",
            "step: 80, loss: 0.0007989198202267289\n",
            "step: 90, loss: 6.856678373878822e-05\n",
            "step: 100, loss: 0.00013652330380864441\n",
            "step: 110, loss: 0.001472583506256342\n",
            "step: 120, loss: 0.0028884713537991047\n",
            "step: 130, loss: 0.00016076255997177213\n",
            "step: 140, loss: 0.00016602959658484906\n",
            "step: 150, loss: 0.0022138634230941534\n",
            "step: 160, loss: 0.08988130837678909\n",
            "step: 170, loss: 0.012266106903553009\n",
            "step: 180, loss: 0.0046605998650193214\n",
            "step: 190, loss: 0.0002744227822404355\n",
            "step: 200, loss: 0.15015734732151031\n",
            "step: 210, loss: 0.00016668302123434842\n",
            "step: 220, loss: 0.03487051650881767\n",
            "step: 230, loss: 0.000783730938564986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.987598647125141, f1=0.9784824462061155, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004009282565675676\n",
            "step: 10, loss: 0.013619153760373592\n",
            "step: 20, loss: 0.0006676649209111929\n",
            "step: 30, loss: 0.00018351402832195163\n",
            "step: 40, loss: 0.007190602831542492\n",
            "step: 50, loss: 0.000538757536560297\n",
            "step: 60, loss: 0.0021897819824516773\n",
            "step: 70, loss: 0.0003764888097066432\n",
            "step: 80, loss: 0.0022143975365906954\n",
            "step: 90, loss: 0.00010306006879545748\n",
            "step: 100, loss: 0.00019591959426179528\n",
            "step: 110, loss: 0.06084541603922844\n",
            "step: 120, loss: 0.03827289119362831\n",
            "step: 130, loss: 0.0006776733789592981\n",
            "step: 140, loss: 0.0002913026255555451\n",
            "step: 150, loss: 0.00017509111785329878\n",
            "step: 160, loss: 0.0003512180701363832\n",
            "step: 170, loss: 0.00012320633686613292\n",
            "step: 180, loss: 0.0048442864790558815\n",
            "step: 190, loss: 0.006660020910203457\n",
            "step: 200, loss: 0.00025401570019312203\n",
            "step: 210, loss: 0.0005727014504373074\n",
            "step: 220, loss: 0.0009546215878799558\n",
            "step: 230, loss: 0.00020079759997315705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9886363636363636, f1=0.9748283752860413, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002505742886569351\n",
            "step: 10, loss: 0.001198692829348147\n",
            "step: 20, loss: 0.03054552525281906\n",
            "step: 30, loss: 0.0003315930371172726\n",
            "step: 40, loss: 0.02607487142086029\n",
            "step: 50, loss: 0.00014509368338622153\n",
            "step: 60, loss: 0.0002333331503905356\n",
            "step: 70, loss: 0.00775337778031826\n",
            "step: 80, loss: 0.00203350349329412\n",
            "step: 90, loss: 0.00022285079467110336\n",
            "step: 100, loss: 0.000362630991730839\n",
            "step: 110, loss: 0.0004350449307821691\n",
            "step: 120, loss: 0.00017157854745164514\n",
            "step: 130, loss: 0.00011129422637168318\n",
            "step: 140, loss: 0.00010287732584401965\n",
            "step: 150, loss: 0.001124995294958353\n",
            "step: 160, loss: 5.669135498465039e-05\n",
            "step: 170, loss: 0.004693076945841312\n",
            "step: 180, loss: 0.002948983572423458\n",
            "step: 190, loss: 9.952845721272752e-05\n",
            "step: 200, loss: 7.881074998294935e-05\n",
            "step: 210, loss: 0.006957856006920338\n",
            "step: 220, loss: 5.794260141556151e-05\n",
            "step: 230, loss: 5.9679648984456435e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9886621315192743, f1=0.976054732041049, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.630556501680985e-05\n",
            "step: 10, loss: 5.3797150030732155e-05\n",
            "step: 20, loss: 8.624919428257272e-05\n",
            "step: 30, loss: 8.956748206401244e-05\n",
            "step: 40, loss: 0.0025615228805691004\n",
            "step: 50, loss: 5.0073143938789144e-05\n",
            "step: 60, loss: 0.0002981189463753253\n",
            "step: 70, loss: 0.0024106719065457582\n",
            "step: 80, loss: 7.692477811360732e-05\n",
            "step: 90, loss: 0.0002377850323682651\n",
            "step: 100, loss: 0.0001148953742813319\n",
            "step: 110, loss: 0.002005124231800437\n",
            "step: 120, loss: 0.0001029735867632553\n",
            "step: 130, loss: 0.0002428300358587876\n",
            "step: 140, loss: 0.08006144315004349\n",
            "step: 150, loss: 0.009457878768444061\n",
            "step: 160, loss: 3.8385001971619204e-05\n",
            "step: 170, loss: 0.0001840379263740033\n",
            "step: 180, loss: 0.0003892892855219543\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 190, loss: 0.027955716475844383\n",
            "step: 200, loss: 0.00010780720913317055\n",
            "step: 210, loss: 7.331249071285129e-05\n",
            "step: 220, loss: 7.889342668931931e-05\n",
            "step: 230, loss: 0.00014576272224076092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9854423292273236, f1=0.9842342342342343, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.94942825450562e-05\n",
            "step: 10, loss: 4.829626777791418e-05\n",
            "step: 20, loss: 0.005543518345803022\n",
            "step: 30, loss: 0.02466147020459175\n",
            "step: 40, loss: 7.91358106653206e-05\n",
            "step: 50, loss: 4.4656066165771335e-05\n",
            "step: 60, loss: 0.00012588189565576613\n",
            "step: 70, loss: 0.011390983127057552\n",
            "step: 80, loss: 4.043734588776715e-05\n",
            "step: 90, loss: 3.4985350794158876e-05\n",
            "step: 100, loss: 4.804369746125303e-05\n",
            "step: 110, loss: 0.00638565793633461\n",
            "step: 120, loss: 0.0003467602946329862\n",
            "step: 130, loss: 2.7941558073507622e-05\n",
            "step: 140, loss: 3.873564492096193e-05\n",
            "step: 150, loss: 0.05022372677922249\n",
            "step: 160, loss: 4.1329833038616925e-05\n",
            "step: 170, loss: 0.04627276584506035\n",
            "step: 180, loss: 0.010073814541101456\n",
            "step: 190, loss: 4.204840661259368e-05\n",
            "step: 200, loss: 0.0011993184452876449\n",
            "step: 210, loss: 9.398067049914971e-05\n",
            "step: 220, loss: 0.00020860797667410225\n",
            "step: 230, loss: 0.0002323703229194507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9854096520763187, f1=0.9831649831649831, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.550706843379885e-05\n",
            "step: 10, loss: 4.0469774830853567e-05\n",
            "step: 20, loss: 4.946575791109353e-05\n",
            "step: 30, loss: 0.00019820136367343366\n",
            "step: 40, loss: 0.0012995335273444653\n",
            "step: 50, loss: 0.0003881447482854128\n",
            "step: 60, loss: 0.042375560849905014\n",
            "step: 70, loss: 0.00010929997370112687\n",
            "step: 80, loss: 5.6391632824670523e-05\n",
            "step: 90, loss: 5.465991489472799e-05\n",
            "step: 100, loss: 3.217429912183434e-05\n",
            "step: 110, loss: 0.00010366107017034665\n",
            "step: 120, loss: 0.0012216410832479596\n",
            "step: 130, loss: 4.9161586503032595e-05\n",
            "step: 140, loss: 0.025181086733937263\n",
            "step: 150, loss: 8.936645463109016e-05\n",
            "step: 160, loss: 7.85778829595074e-05\n",
            "step: 170, loss: 4.834727224078961e-05\n",
            "step: 180, loss: 0.03303677216172218\n",
            "step: 190, loss: 0.00021505822951439768\n",
            "step: 200, loss: 0.000146169972140342\n",
            "step: 210, loss: 0.0015411594649776816\n",
            "step: 220, loss: 5.5773623898858204e-05\n",
            "step: 230, loss: 0.06061113625764847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9876265466816648, f1=0.9785794813979707, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.831220475258306e-05\n",
            "step: 10, loss: 0.00020445795962587\n",
            "step: 20, loss: 0.0006657004123553634\n",
            "step: 30, loss: 0.00020624930039048195\n",
            "step: 40, loss: 5.382890594773926e-05\n",
            "step: 50, loss: 8.791712753009051e-05\n",
            "step: 60, loss: 0.0003833768132608384\n",
            "step: 70, loss: 2.7789694286184385e-05\n",
            "step: 80, loss: 0.0004144649428781122\n",
            "step: 90, loss: 0.000751481216866523\n",
            "step: 100, loss: 8.39322674437426e-05\n",
            "step: 110, loss: 0.05114414915442467\n",
            "step: 120, loss: 0.034279439598321915\n",
            "step: 130, loss: 0.0001112412428483367\n",
            "step: 140, loss: 9.8348522442393e-05\n",
            "step: 150, loss: 5.4620191804133356e-05\n",
            "step: 160, loss: 0.0015103663317859173\n",
            "step: 170, loss: 0.00025574929895810783\n",
            "step: 180, loss: 0.00010180105891777202\n",
            "step: 190, loss: 4.761349919135682e-05\n",
            "step: 200, loss: 8.781092765275389e-05\n",
            "step: 210, loss: 0.00013125441910233349\n",
            "step: 220, loss: 4.518569039646536e-05\n",
            "step: 230, loss: 3.705236304085702e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9887387387387387, f1=0.9808342728297633, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.8713871870422736e-05\n",
            "step: 10, loss: 0.009286812506616116\n",
            "step: 20, loss: 0.001073975465260446\n",
            "step: 30, loss: 7.467172690667212e-05\n",
            "step: 40, loss: 0.00013494603626895696\n",
            "step: 50, loss: 4.706923573394306e-05\n",
            "step: 60, loss: 3.7976555177010596e-05\n",
            "step: 70, loss: 3.8518010114785284e-05\n",
            "step: 80, loss: 2.8721309718093835e-05\n",
            "step: 90, loss: 3.999659020337276e-05\n",
            "step: 100, loss: 4.948130663251504e-05\n",
            "step: 110, loss: 0.0014766567619517446\n",
            "step: 120, loss: 2.137150841008406e-05\n",
            "step: 130, loss: 4.347791036707349e-05\n",
            "step: 140, loss: 0.006006760522723198\n",
            "step: 150, loss: 6.976364238653332e-05\n",
            "step: 160, loss: 0.008893095888197422\n",
            "step: 170, loss: 4.1311068343929946e-05\n",
            "step: 180, loss: 4.365385393612087e-05\n",
            "step: 190, loss: 2.9484674087143503e-05\n",
            "step: 200, loss: 2.736872738751117e-05\n",
            "step: 210, loss: 5.688942110282369e-05\n",
            "step: 220, loss: 3.676188862300478e-05\n",
            "step: 230, loss: 7.563867984572425e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9887892376681614, f1=0.9808773903262092, best_f1=0.9886877828054299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.280575608892832e-05\n",
            "step: 10, loss: 4.610396717907861e-05\n",
            "step: 20, loss: 4.39639225078281e-05\n",
            "step: 30, loss: 0.00011784490197896957\n",
            "step: 40, loss: 9.268586291000247e-05\n",
            "step: 50, loss: 0.04390988498926163\n",
            "step: 60, loss: 3.7195131881162524e-05\n",
            "step: 70, loss: 3.9269751141546294e-05\n",
            "step: 80, loss: 7.330129301408306e-05\n",
            "step: 90, loss: 0.00023964133288245648\n",
            "step: 100, loss: 7.353709952440113e-05\n",
            "step: 110, loss: 0.005154613871127367\n",
            "step: 120, loss: 5.202697866479866e-05\n",
            "step: 130, loss: 0.023003092035651207\n",
            "step: 140, loss: 2.2686623196932487e-05\n",
            "step: 150, loss: 0.0002849488810170442\n",
            "step: 160, loss: 2.4448389012832195e-05\n",
            "step: 170, loss: 2.461601616232656e-05\n",
            "step: 180, loss: 0.0009564972715452313\n",
            "step: 190, loss: 0.0008533722721040249\n",
            "step: 200, loss: 7.669292244827375e-05\n",
            "step: 210, loss: 0.00024848085013218224\n",
            "step: 220, loss: 3.7325600715121254e-05\n",
            "step: 230, loss: 0.003706247778609395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9887892376681614, f1=0.9819819819819819, best_f1=0.9886877828054299\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 196.60it/s]\n",
            "load_f1 = 0.9898989898989898\n",
            "real_f1 = 0.9865771812080537\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 233.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dcab04-b6ab-41e4-aeee-42ccca63f14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6285241842269897\n",
            "step: 10, loss: 0.5669483542442322\n",
            "step: 20, loss: 0.5321688055992126\n",
            "step: 30, loss: 0.182898610830307\n",
            "step: 40, loss: 0.12531262636184692\n",
            "step: 50, loss: 0.2879013121128082\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.13791784644126892\n",
            "step: 70, loss: 0.24558648467063904\n",
            "step: 80, loss: 0.10841207206249237\n",
            "step: 90, loss: 0.442481130361557\n",
            "step: 100, loss: 0.041496392339468\n",
            "step: 110, loss: 0.05641983449459076\n",
            "step: 120, loss: 0.12259756028652191\n",
            "step: 130, loss: 0.10075687617063522\n",
            "step: 140, loss: 0.08051954209804535\n",
            "step: 150, loss: 0.05108888819813728\n",
            "step: 160, loss: 0.022994542494416237\n",
            "step: 170, loss: 0.286937415599823\n",
            "step: 180, loss: 0.03054133802652359\n",
            "step: 190, loss: 0.022461673244833946\n",
            "step: 200, loss: 0.19407887756824493\n",
            "step: 210, loss: 0.08699078857898712\n",
            "step: 220, loss: 0.3465065658092499\n",
            "step: 230, loss: 0.17822730541229248\n",
            "step: 240, loss: 0.04034729674458504\n",
            "step: 250, loss: 0.014584777876734734\n",
            "step: 260, loss: 0.17137333750724792\n",
            "step: 270, loss: 0.017274249345064163\n",
            "step: 280, loss: 0.04840749874711037\n",
            "step: 290, loss: 0.19885630905628204\n",
            "step: 300, loss: 0.06790231913328171\n",
            "step: 310, loss: 0.23013971745967865\n",
            "step: 320, loss: 0.032489608973264694\n",
            "step: 330, loss: 0.05417736992239952\n",
            "step: 340, loss: 0.04506872594356537\n",
            "step: 350, loss: 0.0610090047121048\n",
            "step: 360, loss: 0.03434844687581062\n",
            "step: 370, loss: 0.12791739404201508\n",
            "step: 380, loss: 0.030965063720941544\n",
            "step: 390, loss: 0.21496383845806122\n",
            "step: 400, loss: 0.28014153242111206\n",
            "step: 410, loss: 0.04271762818098068\n",
            "step: 420, loss: 0.0114183584228158\n",
            "step: 430, loss: 0.21828754246234894\n",
            "step: 440, loss: 0.01379118300974369\n",
            "step: 450, loss: 0.029481206089258194\n",
            "step: 460, loss: 0.003077238332480192\n",
            "step: 470, loss: 0.1630936563014984\n",
            "step: 480, loss: 0.07224290072917938\n",
            "step: 490, loss: 0.04520146921277046\n",
            "step: 500, loss: 0.26821866631507874\n",
            "step: 510, loss: 0.022634845227003098\n",
            "step: 520, loss: 0.054193586111068726\n",
            "step: 530, loss: 0.007056616246700287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9266943291839558, f1=0.9242843951985227, best_f1=0.9242843951985227\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21890942752361298\n",
            "step: 10, loss: 0.09178178012371063\n",
            "step: 20, loss: 0.01624663732945919\n",
            "step: 30, loss: 0.023961266502738\n",
            "step: 40, loss: 0.054806794971227646\n",
            "step: 50, loss: 0.14817525446414948\n",
            "step: 60, loss: 0.07633794844150543\n",
            "step: 70, loss: 0.040345754474401474\n",
            "step: 80, loss: 0.06655189394950867\n",
            "step: 90, loss: 0.0688859298825264\n",
            "step: 100, loss: 0.050431232899427414\n",
            "step: 110, loss: 0.04060802981257439\n",
            "step: 120, loss: 0.033867355436086655\n",
            "step: 130, loss: 0.02649732679128647\n",
            "step: 140, loss: 0.01918405294418335\n",
            "step: 150, loss: 0.10421093553304672\n",
            "step: 160, loss: 0.019495142623782158\n",
            "step: 170, loss: 0.02913626655936241\n",
            "step: 180, loss: 0.06749176234006882\n",
            "step: 190, loss: 0.18134573101997375\n",
            "step: 200, loss: 0.017483815550804138\n",
            "step: 210, loss: 0.04889163002371788\n",
            "step: 220, loss: 0.02062462829053402\n",
            "step: 230, loss: 0.0050751748494803905\n",
            "step: 240, loss: 0.11864247173070908\n",
            "step: 250, loss: 0.0904751792550087\n",
            "step: 260, loss: 0.0034614368341863155\n",
            "step: 270, loss: 0.21632522344589233\n",
            "step: 280, loss: 0.07202326506376266\n",
            "step: 290, loss: 0.019947391003370285\n",
            "step: 300, loss: 0.14072732627391815\n",
            "step: 310, loss: 0.010599079541862011\n",
            "step: 320, loss: 0.2201213538646698\n",
            "step: 330, loss: 0.016609760001301765\n",
            "step: 340, loss: 0.03205070644617081\n",
            "step: 350, loss: 0.005467991344630718\n",
            "step: 360, loss: 0.10394211858510971\n",
            "step: 370, loss: 0.18022724986076355\n",
            "step: 380, loss: 0.03132566064596176\n",
            "step: 390, loss: 0.09874425828456879\n",
            "step: 400, loss: 0.04208733141422272\n",
            "step: 410, loss: 0.05810854956507683\n",
            "step: 420, loss: 0.03329041972756386\n",
            "step: 430, loss: 0.1611957997083664\n",
            "step: 440, loss: 0.08732837438583374\n",
            "step: 450, loss: 0.06489990651607513\n",
            "step: 460, loss: 0.014789998531341553\n",
            "step: 470, loss: 0.02895546518266201\n",
            "step: 480, loss: 0.221199631690979\n",
            "step: 490, loss: 0.017964059486985207\n",
            "step: 500, loss: 0.38394248485565186\n",
            "step: 510, loss: 0.0753404051065445\n",
            "step: 520, loss: 0.08333936333656311\n",
            "step: 530, loss: 0.031297262758016586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9340866290018832, f1=0.9259606373008434, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0901007205247879\n",
            "step: 10, loss: 0.007743304595351219\n",
            "step: 20, loss: 0.1424281895160675\n",
            "step: 30, loss: 0.20885543525218964\n",
            "step: 40, loss: 0.010421739891171455\n",
            "step: 50, loss: 0.08587846904993057\n",
            "step: 60, loss: 0.04031241685152054\n",
            "step: 70, loss: 0.013980228453874588\n",
            "step: 80, loss: 0.0014190416550263762\n",
            "step: 90, loss: 0.010147071443498135\n",
            "step: 100, loss: 0.02767310105264187\n",
            "step: 110, loss: 0.007886149920523167\n",
            "step: 120, loss: 0.025072989985346794\n",
            "step: 130, loss: 0.010768481530249119\n",
            "step: 140, loss: 0.052200645208358765\n",
            "step: 150, loss: 0.03257005289196968\n",
            "step: 160, loss: 0.005490054842084646\n",
            "step: 170, loss: 0.18782250583171844\n",
            "step: 180, loss: 0.010421882383525372\n",
            "step: 190, loss: 0.06333430856466293\n",
            "step: 200, loss: 0.01690562628209591\n",
            "step: 210, loss: 0.08667097240686417\n",
            "step: 220, loss: 0.053848862648010254\n",
            "step: 230, loss: 0.10317162424325943\n",
            "step: 240, loss: 0.021438395604491234\n",
            "step: 250, loss: 0.00952671468257904\n",
            "step: 260, loss: 0.01854466088116169\n",
            "step: 270, loss: 0.0009400804410688579\n",
            "step: 280, loss: 0.10425731539726257\n",
            "step: 290, loss: 0.002325496869161725\n",
            "step: 300, loss: 0.034709278494119644\n",
            "step: 310, loss: 0.029337970539927483\n",
            "step: 320, loss: 0.04080408439040184\n",
            "step: 330, loss: 0.009390410035848618\n",
            "step: 340, loss: 0.012159531936049461\n",
            "step: 350, loss: 0.05510813742876053\n",
            "step: 360, loss: 0.05192332714796066\n",
            "step: 370, loss: 0.008980154059827328\n",
            "step: 380, loss: 0.03944670408964157\n",
            "step: 390, loss: 0.07472562044858932\n",
            "step: 400, loss: 0.030650990083813667\n",
            "step: 410, loss: 0.00524608138948679\n",
            "step: 420, loss: 0.3432892858982086\n",
            "step: 430, loss: 0.021345002576708794\n",
            "step: 440, loss: 0.0065876180306077\n",
            "step: 450, loss: 0.06876009702682495\n",
            "step: 460, loss: 0.011559983715415001\n",
            "step: 470, loss: 0.00883571244776249\n",
            "step: 480, loss: 0.006713760085403919\n",
            "step: 490, loss: 0.002779174828901887\n",
            "step: 500, loss: 0.0025358840357512236\n",
            "step: 510, loss: 0.011169695295393467\n",
            "step: 520, loss: 0.10765612125396729\n",
            "step: 530, loss: 0.18099495768547058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9330210772833724, f1=0.9224952741020794, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010597443208098412\n",
            "step: 10, loss: 0.01061151921749115\n",
            "step: 20, loss: 0.0012612675782293081\n",
            "step: 30, loss: 0.009953909553587437\n",
            "step: 40, loss: 0.022030729800462723\n",
            "step: 50, loss: 0.014665858820080757\n",
            "step: 60, loss: 0.010553190484642982\n",
            "step: 70, loss: 0.046124521642923355\n",
            "step: 80, loss: 0.02653161622583866\n",
            "step: 90, loss: 0.025320952758193016\n",
            "step: 100, loss: 0.0025408302899450064\n",
            "step: 110, loss: 0.06781080365180969\n",
            "step: 120, loss: 0.009577454067766666\n",
            "step: 130, loss: 0.0006381465937010944\n",
            "step: 140, loss: 0.0033846115693449974\n",
            "step: 150, loss: 0.017153959721326828\n",
            "step: 160, loss: 0.1195702850818634\n",
            "step: 170, loss: 0.013470826670527458\n",
            "step: 180, loss: 0.00861178245395422\n",
            "step: 190, loss: 0.036302562803030014\n",
            "step: 200, loss: 0.002886081114411354\n",
            "step: 210, loss: 0.07508957386016846\n",
            "step: 220, loss: 0.009870847687125206\n",
            "step: 230, loss: 0.10002961754798889\n",
            "step: 240, loss: 0.006746762432157993\n",
            "step: 250, loss: 0.014447485096752644\n",
            "step: 260, loss: 0.001814359100535512\n",
            "step: 270, loss: 0.01645423285663128\n",
            "step: 280, loss: 0.03876518830657005\n",
            "step: 290, loss: 0.11685206741094589\n",
            "step: 300, loss: 0.0006228065467439592\n",
            "step: 310, loss: 0.008843693882226944\n",
            "step: 320, loss: 0.05231742188334465\n",
            "step: 330, loss: 0.07296323031187057\n",
            "step: 340, loss: 0.003555557457730174\n",
            "step: 350, loss: 0.0962429940700531\n",
            "step: 360, loss: 0.10720962285995483\n",
            "step: 370, loss: 0.0014056236250326037\n",
            "step: 380, loss: 0.006823668256402016\n",
            "step: 390, loss: 0.015712542459368706\n",
            "step: 400, loss: 0.0035394199658185244\n",
            "step: 410, loss: 0.018351275473833084\n",
            "step: 420, loss: 0.01563842222094536\n",
            "step: 430, loss: 0.1520972102880478\n",
            "step: 440, loss: 0.011323941871523857\n",
            "step: 450, loss: 0.020411644130945206\n",
            "step: 460, loss: 0.00214054505340755\n",
            "step: 470, loss: 0.01915382593870163\n",
            "step: 480, loss: 0.1363624930381775\n",
            "step: 490, loss: 0.0018768602749332786\n",
            "step: 500, loss: 0.003373140702024102\n",
            "step: 510, loss: 0.009937630966305733\n",
            "step: 520, loss: 0.06129147857427597\n",
            "step: 530, loss: 0.03141492232680321\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.924020764511562, f1=0.9168241965973536, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1024293377995491\n",
            "step: 10, loss: 0.07605648785829544\n",
            "step: 20, loss: 0.004116054158657789\n",
            "step: 30, loss: 0.006748526357114315\n",
            "step: 40, loss: 0.08719682693481445\n",
            "step: 50, loss: 0.004854109138250351\n",
            "step: 60, loss: 0.025629926472902298\n",
            "step: 70, loss: 0.012865501455962658\n",
            "step: 80, loss: 0.00047477419138886034\n",
            "step: 90, loss: 0.016734343022108078\n",
            "step: 100, loss: 0.009951993823051453\n",
            "step: 110, loss: 0.0004214121145196259\n",
            "step: 120, loss: 0.011270679533481598\n",
            "step: 130, loss: 0.0008638472645543516\n",
            "step: 140, loss: 0.006962520536035299\n",
            "step: 150, loss: 0.006797345820814371\n",
            "step: 160, loss: 0.0006607159157283604\n",
            "step: 170, loss: 0.028976093977689743\n",
            "step: 180, loss: 0.003888531122356653\n",
            "step: 190, loss: 0.006451841443777084\n",
            "step: 200, loss: 0.00041757934377528727\n",
            "step: 210, loss: 0.00793563760817051\n",
            "step: 220, loss: 0.0012075494742020965\n",
            "step: 230, loss: 0.026978803798556328\n",
            "step: 240, loss: 0.0001585463178344071\n",
            "step: 250, loss: 0.0012221974320709705\n",
            "step: 260, loss: 0.0010746954940259457\n",
            "step: 270, loss: 0.003408782882615924\n",
            "step: 280, loss: 0.008576802909374237\n",
            "step: 290, loss: 0.14384891092777252\n",
            "step: 300, loss: 0.001784113934263587\n",
            "step: 310, loss: 0.027619412168860435\n",
            "step: 320, loss: 0.038975320756435394\n",
            "step: 330, loss: 0.004174084402620792\n",
            "step: 340, loss: 0.0026087448932230473\n",
            "step: 350, loss: 0.002501856070011854\n",
            "step: 360, loss: 0.0012863914016634226\n",
            "step: 370, loss: 0.07395047694444656\n",
            "step: 380, loss: 0.0010647941380739212\n",
            "step: 390, loss: 0.0005337253678590059\n",
            "step: 400, loss: 0.003887779312208295\n",
            "step: 410, loss: 0.012095445767045021\n",
            "step: 420, loss: 0.003568827174603939\n",
            "step: 430, loss: 0.007913751527667046\n",
            "step: 440, loss: 0.03764210641384125\n",
            "step: 450, loss: 0.023904047906398773\n",
            "step: 460, loss: 0.005666851531714201\n",
            "step: 470, loss: 0.019130764529109\n",
            "step: 480, loss: 0.0029463900718837976\n",
            "step: 490, loss: 0.0009380700648762286\n",
            "step: 500, loss: 0.06466781347990036\n",
            "step: 510, loss: 0.032108109444379807\n",
            "step: 520, loss: 0.03586322441697121\n",
            "step: 530, loss: 0.006639121100306511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9216809933142313, f1=0.9136276391554704, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0174103956669569\n",
            "step: 10, loss: 0.0009057187708094716\n",
            "step: 20, loss: 0.08570417761802673\n",
            "step: 30, loss: 0.001736402977257967\n",
            "step: 40, loss: 0.04374256730079651\n",
            "step: 50, loss: 0.03429804742336273\n",
            "step: 60, loss: 0.007929792627692223\n",
            "step: 70, loss: 0.002558852545917034\n",
            "step: 80, loss: 0.11404943466186523\n",
            "step: 90, loss: 0.00184503267519176\n",
            "step: 100, loss: 0.0007906618411652744\n",
            "step: 110, loss: 0.002884302055463195\n",
            "step: 120, loss: 0.02446235530078411\n",
            "step: 130, loss: 0.008159087039530277\n",
            "step: 140, loss: 0.042132988572120667\n",
            "step: 150, loss: 0.0008723121718503535\n",
            "step: 160, loss: 0.003754575038328767\n",
            "step: 170, loss: 0.0028301344718784094\n",
            "step: 180, loss: 0.0005283191567286849\n",
            "step: 190, loss: 0.0006278474465943873\n",
            "step: 200, loss: 0.0002059510734397918\n",
            "step: 210, loss: 0.04158264398574829\n",
            "step: 220, loss: 0.002106573898345232\n",
            "step: 230, loss: 0.0024091065861284733\n",
            "step: 240, loss: 0.02666548453271389\n",
            "step: 250, loss: 0.001126629300415516\n",
            "step: 260, loss: 0.023524673655629158\n",
            "step: 270, loss: 0.006907726638019085\n",
            "step: 280, loss: 0.019943049177527428\n",
            "step: 290, loss: 0.0012704723048955202\n",
            "step: 300, loss: 0.007142620626837015\n",
            "step: 310, loss: 0.0004975608899258077\n",
            "step: 320, loss: 0.00010721310536609963\n",
            "step: 330, loss: 0.0004258513217791915\n",
            "step: 340, loss: 0.0491255559027195\n",
            "step: 350, loss: 0.008708065375685692\n",
            "step: 360, loss: 0.023698318749666214\n",
            "step: 370, loss: 0.08254417032003403\n",
            "step: 380, loss: 0.0002311818825546652\n",
            "step: 390, loss: 0.016368167474865913\n",
            "step: 400, loss: 0.12295406311750412\n",
            "step: 410, loss: 0.002678483724594116\n",
            "step: 420, loss: 0.0014474482741206884\n",
            "step: 430, loss: 0.002560056746006012\n",
            "step: 440, loss: 0.00012522548786364496\n",
            "step: 450, loss: 0.00012949023221153766\n",
            "step: 460, loss: 9.927034261636436e-05\n",
            "step: 470, loss: 0.00024880500859580934\n",
            "step: 480, loss: 0.001607969286851585\n",
            "step: 490, loss: 0.006829915102571249\n",
            "step: 500, loss: 0.001741070649586618\n",
            "step: 510, loss: 0.003489800961688161\n",
            "step: 520, loss: 0.0031919165048748255\n",
            "step: 530, loss: 0.01896718703210354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.926899187768753, f1=0.9111969111969113, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010913291946053505\n",
            "step: 10, loss: 0.13358764350414276\n",
            "step: 20, loss: 0.0014406624250113964\n",
            "step: 30, loss: 0.00033813450136221945\n",
            "step: 40, loss: 0.005796332843601704\n",
            "step: 50, loss: 0.01748610846698284\n",
            "step: 60, loss: 0.01694188080728054\n",
            "step: 70, loss: 0.128782257437706\n",
            "step: 80, loss: 0.003024990437552333\n",
            "step: 90, loss: 0.0015754456399008632\n",
            "step: 100, loss: 0.0006517560686916113\n",
            "step: 110, loss: 0.008721916936337948\n",
            "step: 120, loss: 0.00985005684196949\n",
            "step: 130, loss: 0.005548038985580206\n",
            "step: 140, loss: 0.000570224947296083\n",
            "step: 150, loss: 0.002194309374317527\n",
            "step: 160, loss: 0.0004893750883638859\n",
            "step: 170, loss: 0.0029488529544323683\n",
            "step: 180, loss: 0.002283995971083641\n",
            "step: 190, loss: 0.025638682767748833\n",
            "step: 200, loss: 0.0011331313289701939\n",
            "step: 210, loss: 0.0020304929930716753\n",
            "step: 220, loss: 0.0008479313110001385\n",
            "step: 230, loss: 0.010329591110348701\n",
            "step: 240, loss: 0.0022011827677488327\n",
            "step: 250, loss: 0.0010404642671346664\n",
            "step: 260, loss: 0.000472038023872301\n",
            "step: 270, loss: 0.09571455419063568\n",
            "step: 280, loss: 0.0016380869783461094\n",
            "step: 290, loss: 0.0021948348730802536\n",
            "step: 300, loss: 0.0015808402094990015\n",
            "step: 310, loss: 0.004581376910209656\n",
            "step: 320, loss: 0.00015474454266950488\n",
            "step: 330, loss: 0.00032037432538345456\n",
            "step: 340, loss: 0.00018974218983203173\n",
            "step: 350, loss: 0.0001083182287402451\n",
            "step: 360, loss: 0.0016623842529952526\n",
            "step: 370, loss: 0.03054589405655861\n",
            "step: 380, loss: 0.000904663116671145\n",
            "step: 390, loss: 0.0003186353715136647\n",
            "step: 400, loss: 0.015137066133320332\n",
            "step: 410, loss: 0.0014563470613211393\n",
            "step: 420, loss: 0.007358611561357975\n",
            "step: 430, loss: 0.00023000950750429183\n",
            "step: 440, loss: 0.00020768145623151213\n",
            "step: 450, loss: 0.0002830904850270599\n",
            "step: 460, loss: 0.0012508329236879945\n",
            "step: 470, loss: 0.0025227274745702744\n",
            "step: 480, loss: 0.11222493648529053\n",
            "step: 490, loss: 0.00023093802155926824\n",
            "step: 500, loss: 0.0013438710011541843\n",
            "step: 510, loss: 0.006117869168519974\n",
            "step: 520, loss: 0.011779936961829662\n",
            "step: 530, loss: 0.018000874668359756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9269427640763147, f1=0.9157747789669615, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009947809157893062\n",
            "step: 10, loss: 0.0031282047275453806\n",
            "step: 20, loss: 0.00022325565805658698\n",
            "step: 30, loss: 0.14457732439041138\n",
            "step: 40, loss: 0.00021733118046540767\n",
            "step: 50, loss: 0.029642567038536072\n",
            "step: 60, loss: 0.0008593470556661487\n",
            "step: 70, loss: 6.65746774757281e-05\n",
            "step: 80, loss: 0.0037284495774656534\n",
            "step: 90, loss: 0.001791840884834528\n",
            "step: 100, loss: 0.00021240924252197146\n",
            "step: 110, loss: 0.00037680543027818203\n",
            "step: 120, loss: 0.0062010991387069225\n",
            "step: 130, loss: 0.2757044732570648\n",
            "step: 140, loss: 0.0034825284965336323\n",
            "step: 150, loss: 0.0008171197841875255\n",
            "step: 160, loss: 0.07543494552373886\n",
            "step: 170, loss: 0.013763826340436935\n",
            "step: 180, loss: 0.0010542075615376234\n",
            "step: 190, loss: 0.0028873677365481853\n",
            "step: 200, loss: 0.005004914477467537\n",
            "step: 210, loss: 0.004525922704488039\n",
            "step: 220, loss: 0.001545432605780661\n",
            "step: 230, loss: 0.0049820393323898315\n",
            "step: 240, loss: 0.013950377702713013\n",
            "step: 250, loss: 0.00021483012824319303\n",
            "step: 260, loss: 0.00017371847934555262\n",
            "step: 270, loss: 0.03850511088967323\n",
            "step: 280, loss: 0.030442040413618088\n",
            "step: 290, loss: 0.0074782343581318855\n",
            "step: 300, loss: 0.0013930853456258774\n",
            "step: 310, loss: 0.0004371624963823706\n",
            "step: 320, loss: 0.06029972806572914\n",
            "step: 330, loss: 0.05665672942996025\n",
            "step: 340, loss: 0.00011700025061145425\n",
            "step: 350, loss: 0.00018125917995348573\n",
            "step: 360, loss: 0.006680522114038467\n",
            "step: 370, loss: 0.0002860456588678062\n",
            "step: 380, loss: 0.0013105300022289157\n",
            "step: 390, loss: 0.12026407569646835\n",
            "step: 400, loss: 0.00016246414452325553\n",
            "step: 410, loss: 0.0013779107248410583\n",
            "step: 420, loss: 0.0076371910981833935\n",
            "step: 430, loss: 0.0022507179528474808\n",
            "step: 440, loss: 0.027125095948576927\n",
            "step: 450, loss: 7.159994856920093e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 460, loss: 0.0004250780330039561\n",
            "step: 470, loss: 0.00010989073780365288\n",
            "step: 480, loss: 4.932649608235806e-05\n",
            "step: 490, loss: 0.0008219322771765292\n",
            "step: 500, loss: 0.0004139734955970198\n",
            "step: 510, loss: 0.00029413672746159136\n",
            "step: 520, loss: 0.000811391684692353\n",
            "step: 530, loss: 0.046387478709220886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9257403189066059, f1=0.9195820081781009, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01727769896388054\n",
            "step: 10, loss: 0.0055893100798130035\n",
            "step: 20, loss: 0.06181266903877258\n",
            "step: 30, loss: 0.00036199475289322436\n",
            "step: 40, loss: 0.0007858893368393183\n",
            "step: 50, loss: 0.0009573093266226351\n",
            "step: 60, loss: 7.624673889949918e-05\n",
            "step: 70, loss: 0.04977573826909065\n",
            "step: 80, loss: 0.00047571983304806054\n",
            "step: 90, loss: 0.00021133161499164999\n",
            "step: 100, loss: 0.0004715489922091365\n",
            "step: 110, loss: 8.133990195346996e-05\n",
            "step: 120, loss: 0.012172365561127663\n",
            "step: 130, loss: 0.005729194264858961\n",
            "step: 140, loss: 8.37109619169496e-05\n",
            "step: 150, loss: 0.0002237223816337064\n",
            "step: 160, loss: 0.001177011290565133\n",
            "step: 170, loss: 0.0028899405151605606\n",
            "step: 180, loss: 0.00012057839921908453\n",
            "step: 190, loss: 4.5756645704386756e-05\n",
            "step: 200, loss: 0.00032746707438491285\n",
            "step: 210, loss: 0.0152514623478055\n",
            "step: 220, loss: 0.00038821931229904294\n",
            "step: 230, loss: 4.454405279830098e-05\n",
            "step: 240, loss: 0.033083848655223846\n",
            "step: 250, loss: 0.001851867069490254\n",
            "step: 260, loss: 0.00929450336843729\n",
            "step: 270, loss: 6.675560143776238e-05\n",
            "step: 280, loss: 0.0012676535407081246\n",
            "step: 290, loss: 0.010704873129725456\n",
            "step: 300, loss: 0.00014513457426801324\n",
            "step: 310, loss: 0.0062699648551642895\n",
            "step: 320, loss: 0.05494087561964989\n",
            "step: 330, loss: 0.0013933792943134904\n",
            "step: 340, loss: 0.0010654510697349906\n",
            "step: 350, loss: 0.06977984309196472\n",
            "step: 360, loss: 0.00034631136804819107\n",
            "step: 370, loss: 0.005054487381130457\n",
            "step: 380, loss: 0.0007754673133604228\n",
            "step: 390, loss: 0.00033541175071150064\n",
            "step: 400, loss: 0.0010332047240808606\n",
            "step: 410, loss: 0.0006520762108266354\n",
            "step: 420, loss: 0.0015778725501149893\n",
            "step: 430, loss: 0.0007339830044656992\n",
            "step: 440, loss: 0.0018763345433399081\n",
            "step: 450, loss: 0.00021478504640981555\n",
            "step: 460, loss: 0.0001697464322205633\n",
            "step: 470, loss: 4.770298983203247e-05\n",
            "step: 480, loss: 0.001222248189151287\n",
            "step: 490, loss: 0.0006111955735832453\n",
            "step: 500, loss: 0.0032193525694310665\n",
            "step: 510, loss: 0.0004404279461596161\n",
            "step: 520, loss: 0.000606061308644712\n",
            "step: 530, loss: 0.00011073880887124687\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9279486002753558, f1=0.9227230910763569, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012299054651521146\n",
            "step: 10, loss: 0.00027717521879822016\n",
            "step: 20, loss: 6.432789086829871e-05\n",
            "step: 30, loss: 0.00044266143231652677\n",
            "step: 40, loss: 6.700262019876391e-05\n",
            "step: 50, loss: 0.0007456768653355539\n",
            "step: 60, loss: 0.00015134115528780967\n",
            "step: 70, loss: 8.336314931511879e-05\n",
            "step: 80, loss: 0.00011965718294959515\n",
            "step: 90, loss: 0.00016202534607145935\n",
            "step: 100, loss: 0.000733321940060705\n",
            "step: 110, loss: 8.112486830214038e-05\n",
            "step: 120, loss: 0.002361917868256569\n",
            "step: 130, loss: 4.1415969462832436e-05\n",
            "step: 140, loss: 0.0005329031264409423\n",
            "step: 150, loss: 0.0019497292814776301\n",
            "step: 160, loss: 0.0006282167159952223\n",
            "step: 170, loss: 0.00014620335423387587\n",
            "step: 180, loss: 0.0002778259222395718\n",
            "step: 190, loss: 0.000772689760196954\n",
            "step: 200, loss: 0.032769475132226944\n",
            "step: 210, loss: 0.00020724930800497532\n",
            "step: 220, loss: 0.0029583116993308067\n",
            "step: 230, loss: 0.000799146480858326\n",
            "step: 240, loss: 0.00012383359717205167\n",
            "step: 250, loss: 3.6233424907550216e-05\n",
            "step: 260, loss: 0.0009759506792761385\n",
            "step: 270, loss: 0.001459151622839272\n",
            "step: 280, loss: 0.0002655299613252282\n",
            "step: 290, loss: 0.0001553100737510249\n",
            "step: 300, loss: 0.0002943686558865011\n",
            "step: 310, loss: 0.0016569035360589623\n",
            "step: 320, loss: 0.0010191358160227537\n",
            "step: 330, loss: 0.0013135515619069338\n",
            "step: 340, loss: 0.0033904362935572863\n",
            "step: 350, loss: 0.00017350148118566722\n",
            "step: 360, loss: 0.00028385280165821314\n",
            "step: 370, loss: 0.0012838492402806878\n",
            "step: 380, loss: 0.005099273752421141\n",
            "step: 390, loss: 0.00028534213197417557\n",
            "step: 400, loss: 0.0015744994161650538\n",
            "step: 410, loss: 0.0016191356116905808\n",
            "step: 420, loss: 0.00019133204477839172\n",
            "step: 430, loss: 7.736340921837837e-05\n",
            "step: 440, loss: 0.012233621440827847\n",
            "step: 450, loss: 5.876173236174509e-05\n",
            "step: 460, loss: 8.461297693429515e-05\n",
            "step: 470, loss: 0.00017706651124171913\n",
            "step: 480, loss: 8.719958714209497e-05\n",
            "step: 490, loss: 0.000270206481218338\n",
            "step: 500, loss: 0.09123064577579498\n",
            "step: 510, loss: 7.66839730204083e-05\n",
            "step: 520, loss: 0.00792189035564661\n",
            "step: 530, loss: 0.004869687370955944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.927348449791763, f1=0.9202397418165053, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022362252697348595\n",
            "step: 10, loss: 0.0038368890527635813\n",
            "step: 20, loss: 0.002061122329905629\n",
            "step: 30, loss: 0.024850623682141304\n",
            "step: 40, loss: 0.0040555899031460285\n",
            "step: 50, loss: 0.006438758689910173\n",
            "step: 60, loss: 0.006488774437457323\n",
            "step: 70, loss: 0.00012699961371254176\n",
            "step: 80, loss: 0.00015861431893426925\n",
            "step: 90, loss: 1.9751209038076922e-05\n",
            "step: 100, loss: 0.0017345501109957695\n",
            "step: 110, loss: 0.023177146911621094\n",
            "step: 120, loss: 9.876034164335579e-05\n",
            "step: 130, loss: 7.953323074616492e-05\n",
            "step: 140, loss: 0.031296633183956146\n",
            "step: 150, loss: 6.687342829536647e-05\n",
            "step: 160, loss: 0.015240604057908058\n",
            "step: 170, loss: 8.767500548856333e-05\n",
            "step: 180, loss: 0.003202693536877632\n",
            "step: 190, loss: 0.003103413851931691\n",
            "step: 200, loss: 0.012378614395856857\n",
            "step: 210, loss: 0.02392008900642395\n",
            "step: 220, loss: 0.0014102841960266232\n",
            "step: 230, loss: 0.0009753815247677267\n",
            "step: 240, loss: 0.0006366357556544244\n",
            "step: 250, loss: 1.952379898284562e-05\n",
            "step: 260, loss: 6.175140879349783e-05\n",
            "step: 270, loss: 9.930308442562819e-05\n",
            "step: 280, loss: 0.0004925283719785511\n",
            "step: 290, loss: 0.0010286273900419474\n",
            "step: 300, loss: 0.002404395956546068\n",
            "step: 310, loss: 0.0010843131458386779\n",
            "step: 320, loss: 0.0028237425722181797\n",
            "step: 330, loss: 0.0005313997389748693\n",
            "step: 340, loss: 2.0533258066279814e-05\n",
            "step: 350, loss: 0.022559231147170067\n",
            "step: 360, loss: 0.0001296534901484847\n",
            "step: 370, loss: 0.0010597355430945754\n",
            "step: 380, loss: 2.525982381484937e-05\n",
            "step: 390, loss: 0.00021720162476412952\n",
            "step: 400, loss: 0.00013073264562990516\n",
            "step: 410, loss: 0.00038403633516281843\n",
            "step: 420, loss: 0.00010721445869421586\n",
            "step: 430, loss: 1.8152988559450023e-05\n",
            "step: 440, loss: 0.0003123873902950436\n",
            "step: 450, loss: 0.00021917521371506155\n",
            "step: 460, loss: 0.000803980918135494\n",
            "step: 470, loss: 3.458313221926801e-05\n",
            "step: 480, loss: 0.0007945887628011405\n",
            "step: 490, loss: 0.00042362138628959656\n",
            "step: 500, loss: 0.007345553953200579\n",
            "step: 510, loss: 0.005693704355508089\n",
            "step: 520, loss: 1.2371579941827804e-05\n",
            "step: 530, loss: 0.00016500714991707355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9304267161410019, f1=0.9228624535315985, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.95828488864936e-05\n",
            "step: 10, loss: 4.5295979361981153e-05\n",
            "step: 20, loss: 0.00028487565577961504\n",
            "step: 30, loss: 0.0005939225666224957\n",
            "step: 40, loss: 0.0006355839432217181\n",
            "step: 50, loss: 4.322097811382264e-05\n",
            "step: 60, loss: 0.0014097323874011636\n",
            "step: 70, loss: 0.0009007210028357804\n",
            "step: 80, loss: 5.658773079630919e-05\n",
            "step: 90, loss: 2.0495255739660934e-05\n",
            "step: 100, loss: 0.00014069097233004868\n",
            "step: 110, loss: 0.029881676658988\n",
            "step: 120, loss: 7.123948307707906e-05\n",
            "step: 130, loss: 0.005305778235197067\n",
            "step: 140, loss: 3.13662167172879e-05\n",
            "step: 150, loss: 3.448593633947894e-05\n",
            "step: 160, loss: 2.0753161152242683e-05\n",
            "step: 170, loss: 2.890630094043445e-05\n",
            "step: 180, loss: 0.00014921659021638334\n",
            "step: 190, loss: 0.0017687318613752723\n",
            "step: 200, loss: 0.0004346230416558683\n",
            "step: 210, loss: 0.004834584891796112\n",
            "step: 220, loss: 2.0104849681956694e-05\n",
            "step: 230, loss: 3.016977643710561e-05\n",
            "step: 240, loss: 0.00031699243118055165\n",
            "step: 250, loss: 1.9553504898794927e-05\n",
            "step: 260, loss: 2.2403108232538216e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 270, loss: 4.558507862384431e-05\n",
            "step: 280, loss: 0.0007451985729858279\n",
            "step: 290, loss: 0.00027608906384557486\n",
            "step: 300, loss: 0.03463573753833771\n",
            "step: 310, loss: 0.00027348779258318245\n",
            "step: 320, loss: 2.0063427655259147e-05\n",
            "step: 330, loss: 0.0005982369766570628\n",
            "step: 340, loss: 3.252369424444623e-05\n",
            "step: 350, loss: 0.0007248807232826948\n",
            "step: 360, loss: 0.004651464521884918\n",
            "step: 370, loss: 1.963923750736285e-05\n",
            "step: 380, loss: 9.746908472152427e-05\n",
            "step: 390, loss: 2.162471537303645e-05\n",
            "step: 400, loss: 0.0005023559788241982\n",
            "step: 410, loss: 0.0003949759411625564\n",
            "step: 420, loss: 0.0038464281242340803\n",
            "step: 430, loss: 0.0004885190282948315\n",
            "step: 440, loss: 0.0006218367489054799\n",
            "step: 450, loss: 8.741407509660348e-05\n",
            "step: 460, loss: 1.7240281522390433e-05\n",
            "step: 470, loss: 5.4305943194776773e-05\n",
            "step: 480, loss: 4.815360807697289e-05\n",
            "step: 490, loss: 0.00015305842680390924\n",
            "step: 500, loss: 3.116873267572373e-05\n",
            "step: 510, loss: 0.0017905129352584481\n",
            "step: 520, loss: 0.00014529420877806842\n",
            "step: 530, loss: 3.202432708349079e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9314179796107508, f1=0.9234338747099768, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010952414013445377\n",
            "step: 10, loss: 0.0017523972783237696\n",
            "step: 20, loss: 0.0010293220402672887\n",
            "step: 30, loss: 0.000547003117389977\n",
            "step: 40, loss: 2.4202425265684724e-05\n",
            "step: 50, loss: 0.00026766007067635655\n",
            "step: 60, loss: 0.00014570688654202968\n",
            "step: 70, loss: 5.307127139531076e-05\n",
            "step: 80, loss: 5.6339478760492057e-05\n",
            "step: 90, loss: 2.0916873836540617e-05\n",
            "step: 100, loss: 0.00010070428106701002\n",
            "step: 110, loss: 0.00034246116410940886\n",
            "step: 120, loss: 0.00037706599687226117\n",
            "step: 130, loss: 1.9620758394012228e-05\n",
            "step: 140, loss: 0.00019849237287417054\n",
            "step: 150, loss: 7.045800884952769e-05\n",
            "step: 160, loss: 3.159181142109446e-05\n",
            "step: 170, loss: 0.0014164750464260578\n",
            "step: 180, loss: 2.5758683477761224e-05\n",
            "step: 190, loss: 8.81825399119407e-05\n",
            "step: 200, loss: 2.2391641323338263e-05\n",
            "step: 210, loss: 0.00010468206892255694\n",
            "step: 220, loss: 0.019589092582464218\n",
            "step: 230, loss: 6.263492105063051e-05\n",
            "step: 240, loss: 0.00037853349931538105\n",
            "step: 250, loss: 2.899703940784093e-05\n",
            "step: 260, loss: 4.189044557278976e-05\n",
            "step: 270, loss: 1.510210677224677e-05\n",
            "step: 280, loss: 3.5140878026140854e-05\n",
            "step: 290, loss: 9.625293023418635e-05\n",
            "step: 300, loss: 1.9188233636668883e-05\n",
            "step: 310, loss: 0.0020423568785190582\n",
            "step: 320, loss: 0.002004310954362154\n",
            "step: 330, loss: 0.0013622678816318512\n",
            "step: 340, loss: 0.0004048287228215486\n",
            "step: 350, loss: 0.06711915880441666\n",
            "step: 360, loss: 6.169242260511965e-05\n",
            "step: 370, loss: 2.185108678531833e-05\n",
            "step: 380, loss: 0.003358916612342\n",
            "step: 390, loss: 0.0012685713591054082\n",
            "step: 400, loss: 8.278964378405362e-05\n",
            "step: 410, loss: 0.0014452970353886485\n",
            "step: 420, loss: 5.1913917559431866e-05\n",
            "step: 430, loss: 0.00984988734126091\n",
            "step: 440, loss: 0.0030406436417251825\n",
            "step: 450, loss: 0.00014636333798989654\n",
            "step: 460, loss: 0.03327853977680206\n",
            "step: 470, loss: 0.00021641667990479618\n",
            "step: 480, loss: 7.524935062974691e-05\n",
            "step: 490, loss: 0.129658043384552\n",
            "step: 500, loss: 0.00017147246398963034\n",
            "step: 510, loss: 0.00011355571041349322\n",
            "step: 520, loss: 0.0023754085414111614\n",
            "step: 530, loss: 0.00018728662689682096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9290382819794585, f1=0.922071861875875, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004557681386359036\n",
            "step: 10, loss: 3.9286638639168814e-05\n",
            "step: 20, loss: 0.00016034967848099768\n",
            "step: 30, loss: 5.466366928885691e-05\n",
            "step: 40, loss: 0.000247269548708573\n",
            "step: 50, loss: 0.0009115785942412913\n",
            "step: 60, loss: 1.3477896573022008e-05\n",
            "step: 70, loss: 0.00015978410374373198\n",
            "step: 80, loss: 9.087024227483198e-05\n",
            "step: 90, loss: 8.732245623832569e-05\n",
            "step: 100, loss: 0.0007566530839540064\n",
            "step: 110, loss: 3.382021895959042e-05\n",
            "step: 120, loss: 0.008317249827086926\n",
            "step: 130, loss: 0.002157095354050398\n",
            "step: 140, loss: 0.0012873171363025904\n",
            "step: 150, loss: 6.605660746572539e-05\n",
            "step: 160, loss: 0.025873471051454544\n",
            "step: 170, loss: 0.00061472796369344\n",
            "step: 180, loss: 2.70941382041201e-05\n",
            "step: 190, loss: 0.0004671672359108925\n",
            "step: 200, loss: 0.027678649872541428\n",
            "step: 210, loss: 5.534254523809068e-05\n",
            "step: 220, loss: 2.2947149773244746e-05\n",
            "step: 230, loss: 6.192325963638723e-05\n",
            "step: 240, loss: 0.10745718330144882\n",
            "step: 250, loss: 3.567552994354628e-05\n",
            "step: 260, loss: 0.0006414758390747011\n",
            "step: 270, loss: 0.00012091578537365422\n",
            "step: 280, loss: 4.832372724195011e-05\n",
            "step: 290, loss: 0.0014299445319920778\n",
            "step: 300, loss: 0.000925149885006249\n",
            "step: 310, loss: 0.018660271540284157\n",
            "step: 320, loss: 0.0007066850084811449\n",
            "step: 330, loss: 0.0003909215156454593\n",
            "step: 340, loss: 0.00045982006122358143\n",
            "step: 350, loss: 0.0005293504800647497\n",
            "step: 360, loss: 0.00017719704192131758\n",
            "step: 370, loss: 8.2128492067568e-05\n",
            "step: 380, loss: 0.0004041957436129451\n",
            "step: 390, loss: 0.05272939056158066\n",
            "step: 400, loss: 0.0006339288665913045\n",
            "step: 410, loss: 2.530860365368426e-05\n",
            "step: 420, loss: 0.00012395209341775626\n",
            "step: 430, loss: 0.03911793604493141\n",
            "step: 440, loss: 0.00026262577739544213\n",
            "step: 450, loss: 0.00039630825631320477\n",
            "step: 460, loss: 0.0007972960011102259\n",
            "step: 470, loss: 3.4109481930499896e-05\n",
            "step: 480, loss: 1.5567704394925386e-05\n",
            "step: 490, loss: 2.2089619960752316e-05\n",
            "step: 500, loss: 0.002170903142541647\n",
            "step: 510, loss: 5.6270444474648684e-05\n",
            "step: 520, loss: 0.00038709226646460593\n",
            "step: 530, loss: 0.00017303212371189147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.931390977443609, f1=0.9233662435354959, best_f1=0.9259606373008434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021284574177116156\n",
            "step: 10, loss: 3.6727207771036774e-05\n",
            "step: 20, loss: 0.00010582920367596671\n",
            "step: 30, loss: 2.668967135832645e-05\n",
            "step: 40, loss: 0.09936359524726868\n",
            "step: 50, loss: 0.0006512815016321838\n",
            "step: 60, loss: 0.0001636846427572891\n",
            "step: 70, loss: 0.0026942340191453695\n",
            "step: 80, loss: 4.2082701838808134e-05\n",
            "step: 90, loss: 4.00913086195942e-05\n",
            "step: 100, loss: 6.52499875286594e-05\n",
            "step: 110, loss: 9.146879892796278e-05\n",
            "step: 120, loss: 0.0003484625485725701\n",
            "step: 130, loss: 0.03565428778529167\n",
            "step: 140, loss: 4.4946136767975986e-05\n",
            "step: 150, loss: 2.644812411745079e-05\n",
            "step: 160, loss: 0.00013446535740513355\n",
            "step: 170, loss: 2.0026613128720783e-05\n",
            "step: 180, loss: 4.752800305141136e-05\n",
            "step: 190, loss: 9.08120273379609e-05\n",
            "step: 200, loss: 0.00010521128569962457\n",
            "step: 210, loss: 0.001243765465915203\n",
            "step: 220, loss: 0.005777209531515837\n",
            "step: 230, loss: 6.990303518250585e-05\n",
            "step: 240, loss: 5.3358358854893595e-05\n",
            "step: 250, loss: 6.139335164334625e-05\n",
            "step: 260, loss: 0.00029409799026325345\n",
            "step: 270, loss: 1.595510730112437e-05\n",
            "step: 280, loss: 6.906141788931563e-05\n",
            "step: 290, loss: 1.46289912663633e-05\n",
            "step: 300, loss: 6.685619155177847e-05\n",
            "step: 310, loss: 3.327314698253758e-05\n",
            "step: 320, loss: 5.4006213758839294e-05\n",
            "step: 330, loss: 0.00016589846927672625\n",
            "step: 340, loss: 3.1633364415029064e-05\n",
            "step: 350, loss: 5.937884998274967e-05\n",
            "step: 360, loss: 0.001380734029226005\n",
            "step: 370, loss: 1.2900557521788869e-05\n",
            "step: 380, loss: 3.1336974643636495e-05\n",
            "step: 390, loss: 3.51572671206668e-05\n",
            "step: 400, loss: 4.593687117449008e-05\n",
            "step: 410, loss: 4.876240564044565e-05\n",
            "step: 420, loss: 5.522774517885409e-05\n",
            "step: 430, loss: 0.0002819288638420403\n",
            "step: 440, loss: 0.00019617073121480644\n",
            "step: 450, loss: 1.4852397725917399e-05\n",
            "step: 460, loss: 1.7180556824314408e-05\n",
            "step: 470, loss: 0.00123534572776407\n",
            "step: 480, loss: 4.342197280493565e-05\n",
            "step: 490, loss: 1.4975435988162644e-05\n",
            "step: 500, loss: 8.357119077118114e-05\n",
            "step: 510, loss: 7.274094241438434e-05\n",
            "step: 520, loss: 0.00028515540179796517\n",
            "step: 530, loss: 0.0002927251043729484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9311141932501156, f1=0.9249999999999999, best_f1=0.9259606373008434\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 244.77it/s]\n",
            "load_f1 = 0.932963476652797\n",
            "real_f1 = 0.9308118081180812\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 242.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bcd769-5155-4f18-adfb-3617499f1fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5591502785682678\n",
            "step: 10, loss: 0.3613527715206146\n",
            "step: 20, loss: 0.39411234855651855\n",
            "step: 30, loss: 0.3216361105442047\n",
            "step: 40, loss: 0.18724118173122406\n",
            "step: 50, loss: 0.43697598576545715\n",
            "step: 60, loss: 0.28674712777137756\n",
            "step: 70, loss: 0.20105627179145813\n",
            "step: 80, loss: 0.17675067484378815\n",
            "step: 90, loss: 0.33595386147499084\n",
            "step: 100, loss: 0.3378739655017853\n",
            "step: 110, loss: 0.23076455295085907\n",
            "step: 120, loss: 0.23276865482330322\n",
            "step: 130, loss: 0.23462887108325958\n",
            "step: 140, loss: 0.16104814410209656\n",
            "step: 150, loss: 0.23408181965351105\n",
            "step: 160, loss: 0.23178400099277496\n",
            "step: 170, loss: 0.31275424361228943\n",
            "step: 180, loss: 0.16943374276161194\n",
            "step: 190, loss: 0.18166367709636688\n",
            "step: 200, loss: 0.27086660265922546\n",
            "step: 210, loss: 0.24833205342292786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5514950166112956, f1=0.5912162162162163, best_f1=0.5912162162162163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1649065911769867\n",
            "step: 10, loss: 0.24398532509803772\n",
            "step: 20, loss: 0.18750892579555511\n",
            "step: 30, loss: 0.1821415275335312\n",
            "step: 40, loss: 0.2662965655326843\n",
            "step: 50, loss: 0.18595688045024872\n",
            "step: 60, loss: 0.3547584116458893\n",
            "step: 70, loss: 0.11711627244949341\n",
            "step: 80, loss: 0.23342519998550415\n",
            "step: 90, loss: 0.13851754367351532\n",
            "step: 100, loss: 0.018960338085889816\n",
            "step: 110, loss: 0.13626286387443542\n",
            "step: 120, loss: 0.12121453136205673\n",
            "step: 130, loss: 0.047426048666238785\n",
            "step: 140, loss: 0.1610105186700821\n",
            "step: 150, loss: 0.17077048122882843\n",
            "step: 160, loss: 0.24461747705936432\n",
            "step: 170, loss: 0.11932774633169174\n",
            "step: 180, loss: 0.22440169751644135\n",
            "step: 190, loss: 0.22837390005588531\n",
            "step: 200, loss: 0.03139519318938255\n",
            "step: 210, loss: 0.18470245599746704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6083499005964215, f1=0.625250501002004, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045591261237859726\n",
            "step: 10, loss: 0.1278681606054306\n",
            "step: 20, loss: 0.19060535728931427\n",
            "step: 30, loss: 0.1658904254436493\n",
            "step: 40, loss: 0.09354937821626663\n",
            "step: 50, loss: 0.036323755979537964\n",
            "step: 60, loss: 0.24660800397396088\n",
            "step: 70, loss: 0.13405925035476685\n",
            "step: 80, loss: 0.26943400502204895\n",
            "step: 90, loss: 0.04171594977378845\n",
            "step: 100, loss: 0.15771688520908356\n",
            "step: 110, loss: 0.12751291692256927\n",
            "step: 120, loss: 0.1586630493402481\n",
            "step: 130, loss: 0.2060648500919342\n",
            "step: 140, loss: 0.10663919150829315\n",
            "step: 150, loss: 0.19242489337921143\n",
            "step: 160, loss: 0.05317793786525726\n",
            "step: 170, loss: 0.21696029603481293\n",
            "step: 180, loss: 0.13553883135318756\n",
            "step: 190, loss: 0.16403083503246307\n",
            "step: 200, loss: 0.07264146208763123\n",
            "step: 210, loss: 0.15116764605045319\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.593625498007968, f1=0.6403162055335969, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13267377018928528\n",
            "step: 10, loss: 0.05697869509458542\n",
            "step: 20, loss: 0.10906592011451721\n",
            "step: 30, loss: 0.07549665868282318\n",
            "step: 40, loss: 0.025530178099870682\n",
            "step: 50, loss: 0.15391141176223755\n",
            "step: 60, loss: 0.2652380168437958\n",
            "step: 70, loss: 0.27657848596572876\n",
            "step: 80, loss: 0.11108837276697159\n",
            "step: 90, loss: 0.018197782337665558\n",
            "step: 100, loss: 0.2990138828754425\n",
            "step: 110, loss: 0.1427747756242752\n",
            "step: 120, loss: 0.1481386125087738\n",
            "step: 130, loss: 0.14254054427146912\n",
            "step: 140, loss: 0.2310260385274887\n",
            "step: 150, loss: 0.07445728033781052\n",
            "step: 160, loss: 0.06962702423334122\n",
            "step: 170, loss: 0.20345048606395721\n",
            "step: 180, loss: 0.41849446296691895\n",
            "step: 190, loss: 0.0881384089589119\n",
            "step: 200, loss: 0.0822555273771286\n",
            "step: 210, loss: 0.20363013446331024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.5968379446640317, f1=0.6144578313253013, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11260853707790375\n",
            "step: 10, loss: 0.06000814214348793\n",
            "step: 20, loss: 0.2354642003774643\n",
            "step: 30, loss: 0.06631727516651154\n",
            "step: 40, loss: 0.04398727789521217\n",
            "step: 50, loss: 0.041064053773880005\n",
            "step: 60, loss: 0.07252753525972366\n",
            "step: 70, loss: 0.10024722665548325\n",
            "step: 80, loss: 0.08362053334712982\n",
            "step: 90, loss: 0.053299225866794586\n",
            "step: 100, loss: 0.002173184184357524\n",
            "step: 110, loss: 0.13400733470916748\n",
            "step: 120, loss: 0.12565433979034424\n",
            "step: 130, loss: 0.09779592603445053\n",
            "step: 140, loss: 0.20859694480895996\n",
            "step: 150, loss: 0.04487431421875954\n",
            "step: 160, loss: 0.23332437872886658\n",
            "step: 170, loss: 0.18565227091312408\n",
            "step: 180, loss: 0.1343863308429718\n",
            "step: 190, loss: 0.012436214834451675\n",
            "step: 200, loss: 0.12580639123916626\n",
            "step: 210, loss: 0.01407389435917139\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6056074766355141, f1=0.6151012891344384, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05441299453377724\n",
            "step: 10, loss: 0.07025355100631714\n",
            "step: 20, loss: 0.019229399040341377\n",
            "step: 30, loss: 0.005415529012680054\n",
            "step: 40, loss: 0.10630866885185242\n",
            "step: 50, loss: 0.0041344380006194115\n",
            "step: 60, loss: 0.10304360091686249\n",
            "step: 70, loss: 0.02603466808795929\n",
            "step: 80, loss: 0.18398098647594452\n",
            "step: 90, loss: 0.07671240717172623\n",
            "step: 100, loss: 0.02032586932182312\n",
            "step: 110, loss: 0.07279624044895172\n",
            "step: 120, loss: 0.1069151982665062\n",
            "step: 130, loss: 0.1314527690410614\n",
            "step: 140, loss: 0.1219763457775116\n",
            "step: 150, loss: 0.03562869876623154\n",
            "step: 160, loss: 0.07956328988075256\n",
            "step: 170, loss: 0.1914142221212387\n",
            "step: 180, loss: 0.03437277674674988\n",
            "step: 190, loss: 0.1316065788269043\n",
            "step: 200, loss: 0.0761704370379448\n",
            "step: 210, loss: 0.048204224556684494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5911504424778761, f1=0.5824561403508771, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023162715137004852\n",
            "step: 10, loss: 0.008523036725819111\n",
            "step: 20, loss: 0.2017107605934143\n",
            "step: 30, loss: 0.0317201130092144\n",
            "step: 40, loss: 0.08575337380170822\n",
            "step: 50, loss: 0.07452843338251114\n",
            "step: 60, loss: 0.07109422236680984\n",
            "step: 70, loss: 0.02816164866089821\n",
            "step: 80, loss: 0.03218967095017433\n",
            "step: 90, loss: 0.026388706639409065\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.0032414481975138187\n",
            "step: 110, loss: 0.21451573073863983\n",
            "step: 120, loss: 0.2267279326915741\n",
            "step: 130, loss: 0.012716061435639858\n",
            "step: 140, loss: 0.01395772211253643\n",
            "step: 150, loss: 0.00949208065867424\n",
            "step: 160, loss: 0.0908818319439888\n",
            "step: 170, loss: 0.009706457145512104\n",
            "step: 180, loss: 0.1019054502248764\n",
            "step: 190, loss: 0.05234857276082039\n",
            "step: 200, loss: 0.03806641697883606\n",
            "step: 210, loss: 0.032880209386348724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.593939393939394, f1=0.5961538461538463, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028903136029839516\n",
            "step: 10, loss: 0.15500101447105408\n",
            "step: 20, loss: 0.007914886809885502\n",
            "step: 30, loss: 0.11936437338590622\n",
            "step: 40, loss: 0.009027604945003986\n",
            "step: 50, loss: 0.011880666017532349\n",
            "step: 60, loss: 0.07361028343439102\n",
            "step: 70, loss: 0.022706251591444016\n",
            "step: 80, loss: 0.012476702220737934\n",
            "step: 90, loss: 0.1122315526008606\n",
            "step: 100, loss: 0.1473335176706314\n",
            "step: 110, loss: 0.085179403424263\n",
            "step: 120, loss: 0.007880846038460732\n",
            "step: 130, loss: 0.16739127039909363\n",
            "step: 140, loss: 0.05408293008804321\n",
            "step: 150, loss: 0.15496322512626648\n",
            "step: 160, loss: 0.05019364506006241\n",
            "step: 170, loss: 0.006828028708696365\n",
            "step: 180, loss: 0.0722016915678978\n",
            "step: 190, loss: 0.02914159744977951\n",
            "step: 200, loss: 0.027768932282924652\n",
            "step: 210, loss: 0.18416157364845276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6071428571428571, f1=0.62109375, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04386015236377716\n",
            "step: 10, loss: 0.07102607190608978\n",
            "step: 20, loss: 0.0029076773207634687\n",
            "step: 30, loss: 0.004648316185921431\n",
            "step: 40, loss: 0.07152880728244781\n",
            "step: 50, loss: 0.018205761909484863\n",
            "step: 60, loss: 0.1509484201669693\n",
            "step: 70, loss: 0.014810883440077305\n",
            "step: 80, loss: 0.002002421300858259\n",
            "step: 90, loss: 0.0013932063011452556\n",
            "step: 100, loss: 0.010582963936030865\n",
            "step: 110, loss: 0.035534393042325974\n",
            "step: 120, loss: 0.10395380109548569\n",
            "step: 130, loss: 0.004434146918356419\n",
            "step: 140, loss: 0.04375215247273445\n",
            "step: 150, loss: 0.157255619764328\n",
            "step: 160, loss: 0.006731214001774788\n",
            "step: 170, loss: 0.060211341828107834\n",
            "step: 180, loss: 0.029613690450787544\n",
            "step: 190, loss: 0.0013018825557082891\n",
            "step: 200, loss: 0.08462777733802795\n",
            "step: 210, loss: 0.01687575876712799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5828092243186583, f1=0.6144329896907217, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01573832519352436\n",
            "step: 10, loss: 0.026536880061030388\n",
            "step: 20, loss: 0.0023113638162612915\n",
            "step: 30, loss: 0.052346136420965195\n",
            "step: 40, loss: 0.028913790360093117\n",
            "step: 50, loss: 0.015396859496831894\n",
            "step: 60, loss: 0.09416355192661285\n",
            "step: 70, loss: 0.031678393483161926\n",
            "step: 80, loss: 0.028858410194516182\n",
            "step: 90, loss: 0.05184203013777733\n",
            "step: 100, loss: 0.15435612201690674\n",
            "step: 110, loss: 0.03239277005195618\n",
            "step: 120, loss: 0.01181041169911623\n",
            "step: 130, loss: 0.012025140225887299\n",
            "step: 140, loss: 0.0035267272032797337\n",
            "step: 150, loss: 0.017039678990840912\n",
            "step: 160, loss: 0.004432699643075466\n",
            "step: 170, loss: 0.0037692529149353504\n",
            "step: 180, loss: 0.13799013197422028\n",
            "step: 190, loss: 0.31894147396087646\n",
            "step: 200, loss: 0.02181566320359707\n",
            "step: 210, loss: 0.07398750633001328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5846153846153846, f1=0.5924276169265034, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013474922627210617\n",
            "step: 10, loss: 0.15390244126319885\n",
            "step: 20, loss: 0.027818597853183746\n",
            "step: 30, loss: 0.0027631535194814205\n",
            "step: 40, loss: 0.06422076374292374\n",
            "step: 50, loss: 0.014816771261394024\n",
            "step: 60, loss: 0.02561783976852894\n",
            "step: 70, loss: 0.012597897090017796\n",
            "step: 80, loss: 0.06502343714237213\n",
            "step: 90, loss: 0.053141456097364426\n",
            "step: 100, loss: 0.05638173222541809\n",
            "step: 110, loss: 0.08860575407743454\n",
            "step: 120, loss: 0.16500775516033173\n",
            "step: 130, loss: 0.005077691283077002\n",
            "step: 140, loss: 0.02554749883711338\n",
            "step: 150, loss: 0.005646026227623224\n",
            "step: 160, loss: 0.01825859025120735\n",
            "step: 170, loss: 0.03969348967075348\n",
            "step: 180, loss: 0.007273429539054632\n",
            "step: 190, loss: 0.024570705369114876\n",
            "step: 200, loss: 0.0016347152413800359\n",
            "step: 210, loss: 0.0009984353091567755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6077586206896552, f1=0.6189473684210527, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1200600191950798\n",
            "step: 10, loss: 0.00418408727273345\n",
            "step: 20, loss: 0.08801961690187454\n",
            "step: 30, loss: 0.0017530692275613546\n",
            "step: 40, loss: 0.11040445417165756\n",
            "step: 50, loss: 0.0005586952320300043\n",
            "step: 60, loss: 0.19840098917484283\n",
            "step: 70, loss: 0.00265975808724761\n",
            "step: 80, loss: 0.03006741963326931\n",
            "step: 90, loss: 0.017563750967383385\n",
            "step: 100, loss: 0.13851819932460785\n",
            "step: 110, loss: 0.006219794973731041\n",
            "step: 120, loss: 0.00197496241889894\n",
            "step: 130, loss: 0.007950210012495518\n",
            "step: 140, loss: 0.05000010132789612\n",
            "step: 150, loss: 0.006016101222485304\n",
            "step: 160, loss: 0.0013374147238209844\n",
            "step: 170, loss: 0.0038259057328104973\n",
            "step: 180, loss: 0.06941510736942291\n",
            "step: 190, loss: 0.0893123671412468\n",
            "step: 200, loss: 0.02144745923578739\n",
            "step: 210, loss: 0.04871445521712303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5818181818181819, f1=0.6217821782178218, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08095782995223999\n",
            "step: 10, loss: 0.0011152105871587992\n",
            "step: 20, loss: 0.019505972042679787\n",
            "step: 30, loss: 0.023687945678830147\n",
            "step: 40, loss: 0.032832060009241104\n",
            "step: 50, loss: 0.02420087531208992\n",
            "step: 60, loss: 0.004106320906430483\n",
            "step: 70, loss: 0.03025454841554165\n",
            "step: 80, loss: 0.013238067738711834\n",
            "step: 90, loss: 0.008049060590565205\n",
            "step: 100, loss: 0.003416093299165368\n",
            "step: 110, loss: 0.003128480864688754\n",
            "step: 120, loss: 0.02598910592496395\n",
            "step: 130, loss: 0.002298376290127635\n",
            "step: 140, loss: 0.004281855653971434\n",
            "step: 150, loss: 0.001670072553679347\n",
            "step: 160, loss: 0.030875544995069504\n",
            "step: 170, loss: 0.003285597078502178\n",
            "step: 180, loss: 0.03814493864774704\n",
            "step: 190, loss: 0.0009014732204377651\n",
            "step: 200, loss: 0.0033673730213195086\n",
            "step: 210, loss: 0.007379299029707909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5795454545454546, f1=0.5932835820895523, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028381897136569023\n",
            "step: 10, loss: 0.025667831301689148\n",
            "step: 20, loss: 0.0019910119008272886\n",
            "step: 30, loss: 0.008918197825551033\n",
            "step: 40, loss: 0.11539403349161148\n",
            "step: 50, loss: 0.031124746426939964\n",
            "step: 60, loss: 0.0019605595152825117\n",
            "step: 70, loss: 0.006556937005370855\n",
            "step: 80, loss: 0.016773425042629242\n",
            "step: 90, loss: 0.01768982969224453\n",
            "step: 100, loss: 0.0063179535791277885\n",
            "step: 110, loss: 0.03638530895113945\n",
            "step: 120, loss: 0.08853685110807419\n",
            "step: 130, loss: 0.0565265491604805\n",
            "step: 140, loss: 0.034425996243953705\n",
            "step: 150, loss: 0.03367842733860016\n",
            "step: 160, loss: 0.003418096574023366\n",
            "step: 170, loss: 0.003531690454110503\n",
            "step: 180, loss: 0.03568505123257637\n",
            "step: 190, loss: 0.012487329542636871\n",
            "step: 200, loss: 0.0352676659822464\n",
            "step: 210, loss: 0.12617981433868408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5857740585774057, f1=0.6118143459915611, best_f1=0.625250501002004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005472994409501553\n",
            "step: 10, loss: 0.011466771364212036\n",
            "step: 20, loss: 0.0016832052497193217\n",
            "step: 30, loss: 0.044325947761535645\n",
            "step: 40, loss: 0.0036056996323168278\n",
            "step: 50, loss: 0.0018275425536558032\n",
            "step: 60, loss: 0.007100405637174845\n",
            "step: 70, loss: 0.006366051733493805\n",
            "step: 80, loss: 0.0044759949669241905\n",
            "step: 90, loss: 0.04776865616440773\n",
            "step: 100, loss: 0.007201839704066515\n",
            "step: 110, loss: 0.005461679771542549\n",
            "step: 120, loss: 0.0018361721886321902\n",
            "step: 130, loss: 0.004483080469071865\n",
            "step: 140, loss: 0.0014329658588394523\n",
            "step: 150, loss: 0.00812985934317112\n",
            "step: 160, loss: 0.0036259079352021217\n",
            "step: 170, loss: 0.001514939940534532\n",
            "step: 180, loss: 0.0009638717165216804\n",
            "step: 190, loss: 0.06109052151441574\n",
            "step: 200, loss: 0.004372614901512861\n",
            "step: 210, loss: 0.005876230541616678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5848214285714285, f1=0.6035242290748898, best_f1=0.625250501002004\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 415.56it/s]\n",
            "load_f1 = 0.6050420168067228\n",
            "real_f1 = 0.6004319654427647\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 232.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1502f1-3de7-483c-e632-74d025932ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5754364728927612\n",
            "step: 10, loss: 0.37274765968322754\n",
            "step: 20, loss: 0.31797242164611816\n",
            "step: 30, loss: 0.38777491450309753\n",
            "step: 40, loss: 0.41840001940727234\n",
            "step: 50, loss: 0.29489102959632874\n",
            "step: 60, loss: 0.26926136016845703\n",
            "step: 70, loss: 0.28857454657554626\n",
            "step: 80, loss: 0.31272536516189575\n",
            "step: 90, loss: 0.3145526647567749\n",
            "step: 100, loss: 0.3247881531715393\n",
            "step: 110, loss: 0.5963802337646484\n",
            "step: 120, loss: 0.127916619181633\n",
            "step: 130, loss: 0.11961735039949417\n",
            "step: 140, loss: 0.03874047473073006\n",
            "step: 150, loss: 0.18944121897220612\n",
            "step: 160, loss: 0.1244734525680542\n",
            "step: 170, loss: 0.23771129548549652\n",
            "step: 180, loss: 0.044117845594882965\n",
            "step: 190, loss: 0.19871702790260315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6582278481012659, f1=0.6601941747572816, best_f1=0.6601941747572816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26037633419036865\n",
            "step: 10, loss: 0.03783087432384491\n",
            "step: 20, loss: 0.2921459674835205\n",
            "step: 30, loss: 0.17619511485099792\n",
            "step: 40, loss: 0.13289429247379303\n",
            "step: 50, loss: 0.13138295710086823\n",
            "step: 60, loss: 0.2308504581451416\n",
            "step: 70, loss: 0.1576625406742096\n",
            "step: 80, loss: 0.1568375527858734\n",
            "step: 90, loss: 0.1833440363407135\n",
            "step: 100, loss: 0.025409035384655\n",
            "step: 110, loss: 0.19905102252960205\n",
            "step: 120, loss: 0.2394402027130127\n",
            "step: 130, loss: 0.04824410006403923\n",
            "step: 140, loss: 0.07908527553081512\n",
            "step: 150, loss: 0.09054945409297943\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.025986745953559875\n",
            "step: 170, loss: 0.1799587458372116\n",
            "step: 180, loss: 0.2691480815410614\n",
            "step: 190, loss: 0.04906884580850601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7079207920792079, f1=0.7281795511221945, best_f1=0.7281795511221945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08878526091575623\n",
            "step: 10, loss: 0.05932341888546944\n",
            "step: 20, loss: 0.038496989756822586\n",
            "step: 30, loss: 0.024085767567157745\n",
            "step: 40, loss: 0.09293122589588165\n",
            "step: 50, loss: 0.28392305970191956\n",
            "step: 60, loss: 0.08300559222698212\n",
            "step: 70, loss: 0.21386893093585968\n",
            "step: 80, loss: 0.1338077336549759\n",
            "step: 90, loss: 0.12979887425899506\n",
            "step: 100, loss: 0.06947584450244904\n",
            "step: 110, loss: 0.01303180493414402\n",
            "step: 120, loss: 0.027393827214837074\n",
            "step: 130, loss: 0.010716382414102554\n",
            "step: 140, loss: 0.014396742917597294\n",
            "step: 150, loss: 0.11635508388280869\n",
            "step: 160, loss: 0.047177549451589584\n",
            "step: 170, loss: 0.15652401745319366\n",
            "step: 180, loss: 0.0773048922419548\n",
            "step: 190, loss: 0.14031147956848145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7643979057591622, f1=0.7745358090185677, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.048203226178884506\n",
            "step: 10, loss: 0.1501987874507904\n",
            "step: 20, loss: 0.24852940440177917\n",
            "step: 30, loss: 0.03003346547484398\n",
            "step: 40, loss: 0.03034527786076069\n",
            "step: 50, loss: 0.05675509572029114\n",
            "step: 60, loss: 0.03240597993135452\n",
            "step: 70, loss: 0.33701956272125244\n",
            "step: 80, loss: 0.33305782079696655\n",
            "step: 90, loss: 0.03773710876703262\n",
            "step: 100, loss: 0.06160951778292656\n",
            "step: 110, loss: 0.03761230409145355\n",
            "step: 120, loss: 0.05093357712030411\n",
            "step: 130, loss: 0.17178459465503693\n",
            "step: 140, loss: 0.0440576933324337\n",
            "step: 150, loss: 0.12456435710191727\n",
            "step: 160, loss: 0.032967451959848404\n",
            "step: 170, loss: 0.03734033927321434\n",
            "step: 180, loss: 0.008512976579368114\n",
            "step: 190, loss: 0.3473653495311737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7365728900255755, f1=0.7539267015706806, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045702770352363586\n",
            "step: 10, loss: 0.006118652876466513\n",
            "step: 20, loss: 0.0210807453840971\n",
            "step: 30, loss: 0.012880942784249783\n",
            "step: 40, loss: 0.038603682070970535\n",
            "step: 50, loss: 0.05207731947302818\n",
            "step: 60, loss: 0.0715387761592865\n",
            "step: 70, loss: 0.030235914513468742\n",
            "step: 80, loss: 0.021247612312436104\n",
            "step: 90, loss: 0.027065740898251534\n",
            "step: 100, loss: 0.004803547170013189\n",
            "step: 110, loss: 0.012857906520366669\n",
            "step: 120, loss: 0.00398228969424963\n",
            "step: 130, loss: 0.08527609705924988\n",
            "step: 140, loss: 0.021487023681402206\n",
            "step: 150, loss: 0.0024724891409277916\n",
            "step: 160, loss: 0.014997152611613274\n",
            "step: 170, loss: 0.01653899811208248\n",
            "step: 180, loss: 0.337453693151474\n",
            "step: 190, loss: 0.04702224209904671\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7540106951871658, f1=0.7444444444444444, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01593375951051712\n",
            "step: 10, loss: 0.023716069757938385\n",
            "step: 20, loss: 0.07532109320163727\n",
            "step: 30, loss: 0.0017495391657575965\n",
            "step: 40, loss: 0.050738271325826645\n",
            "step: 50, loss: 0.03626423329114914\n",
            "step: 60, loss: 0.010376734659075737\n",
            "step: 70, loss: 0.15959316492080688\n",
            "step: 80, loss: 0.03377803787589073\n",
            "step: 90, loss: 0.04377557337284088\n",
            "step: 100, loss: 0.0013181017711758614\n",
            "step: 110, loss: 0.017013540491461754\n",
            "step: 120, loss: 0.12305683642625809\n",
            "step: 130, loss: 0.015220696106553078\n",
            "step: 140, loss: 0.176267609000206\n",
            "step: 150, loss: 0.0018706442788243294\n",
            "step: 160, loss: 0.009925637394189835\n",
            "step: 170, loss: 0.017540384083986282\n",
            "step: 180, loss: 0.006281902547925711\n",
            "step: 190, loss: 0.026079688221216202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7365591397849462, f1=0.7252747252747253, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01578487642109394\n",
            "step: 10, loss: 0.003496079472824931\n",
            "step: 20, loss: 0.013002662919461727\n",
            "step: 30, loss: 0.018900863826274872\n",
            "step: 40, loss: 0.007132804486900568\n",
            "step: 50, loss: 0.11181433498859406\n",
            "step: 60, loss: 0.006253461353480816\n",
            "step: 70, loss: 0.007701423484832048\n",
            "step: 80, loss: 0.03723960369825363\n",
            "step: 90, loss: 0.00292415008880198\n",
            "step: 100, loss: 0.0013104141689836979\n",
            "step: 110, loss: 0.0042153713293373585\n",
            "step: 120, loss: 0.002991153858602047\n",
            "step: 130, loss: 0.001926500117406249\n",
            "step: 140, loss: 0.01873941719532013\n",
            "step: 150, loss: 0.03232698142528534\n",
            "step: 160, loss: 0.030232563614845276\n",
            "step: 170, loss: 0.004448912106454372\n",
            "step: 180, loss: 0.0005568084889091551\n",
            "step: 190, loss: 0.05468330904841423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7539267015706806, f1=0.765498652291105, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.035207733511924744\n",
            "step: 10, loss: 0.005920293275266886\n",
            "step: 20, loss: 0.0009519823361188173\n",
            "step: 30, loss: 0.03827248141169548\n",
            "step: 40, loss: 0.11022505164146423\n",
            "step: 50, loss: 0.00623162230476737\n",
            "step: 60, loss: 0.07783018052577972\n",
            "step: 70, loss: 0.003960049711167812\n",
            "step: 80, loss: 0.0006186144892126322\n",
            "step: 90, loss: 0.010671843774616718\n",
            "step: 100, loss: 0.0020144630689173937\n",
            "step: 110, loss: 0.0010518807685002685\n",
            "step: 120, loss: 0.00224191602319479\n",
            "step: 130, loss: 0.002175620524212718\n",
            "step: 140, loss: 0.011253807693719864\n",
            "step: 150, loss: 0.041598230600357056\n",
            "step: 160, loss: 0.0024811681360006332\n",
            "step: 170, loss: 0.01257340982556343\n",
            "step: 180, loss: 0.0205396581441164\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 190, loss: 0.004808906465768814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7526881720430108, f1=0.7439353099730459, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002952330047264695\n",
            "step: 10, loss: 0.21507377922534943\n",
            "step: 20, loss: 0.008034517988562584\n",
            "step: 30, loss: 0.002143464284017682\n",
            "step: 40, loss: 0.003939331509172916\n",
            "step: 50, loss: 0.005708572920411825\n",
            "step: 60, loss: 0.0005106689059175551\n",
            "step: 70, loss: 0.0003350301121827215\n",
            "step: 80, loss: 0.0011463987175375223\n",
            "step: 90, loss: 0.003389565274119377\n",
            "step: 100, loss: 0.06567111611366272\n",
            "step: 110, loss: 0.0014302728231996298\n",
            "step: 120, loss: 0.0023578708060085773\n",
            "step: 130, loss: 0.0306412260979414\n",
            "step: 140, loss: 0.0069315265864133835\n",
            "step: 150, loss: 0.0021470419596880674\n",
            "step: 160, loss: 0.0006488671060651541\n",
            "step: 170, loss: 0.012831378728151321\n",
            "step: 180, loss: 0.0598333477973938\n",
            "step: 190, loss: 0.0007076472975313663\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7582417582417582, f1=0.7771739130434783, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036215006839483976\n",
            "step: 10, loss: 0.050187818706035614\n",
            "step: 20, loss: 0.006155471317470074\n",
            "step: 30, loss: 0.0008961513522081077\n",
            "step: 40, loss: 0.05966255068778992\n",
            "step: 50, loss: 0.0023726671934127808\n",
            "step: 60, loss: 0.15314781665802002\n",
            "step: 70, loss: 0.01478331908583641\n",
            "step: 80, loss: 0.0006805305602028966\n",
            "step: 90, loss: 0.00259325560182333\n",
            "step: 100, loss: 0.001779771875590086\n",
            "step: 110, loss: 0.0010314921382814646\n",
            "step: 120, loss: 0.18224453926086426\n",
            "step: 130, loss: 0.002317753853276372\n",
            "step: 140, loss: 0.03699515014886856\n",
            "step: 150, loss: 0.011227714829146862\n",
            "step: 160, loss: 0.0009746207506395876\n",
            "step: 170, loss: 0.0005534274969249964\n",
            "step: 180, loss: 0.0037149081472307444\n",
            "step: 190, loss: 0.0010712611256167293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7507002801120448, f1=0.7843137254901961, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002747817779891193\n",
            "step: 10, loss: 0.0005502650747075677\n",
            "step: 20, loss: 0.006647662725299597\n",
            "step: 30, loss: 0.01450225431472063\n",
            "step: 40, loss: 0.0007770500378683209\n",
            "step: 50, loss: 0.005278327967971563\n",
            "step: 60, loss: 0.0007083794916979969\n",
            "step: 70, loss: 0.0007962769595906138\n",
            "step: 80, loss: 0.003490337636321783\n",
            "step: 90, loss: 0.0003993635473307222\n",
            "step: 100, loss: 0.00044244443415664136\n",
            "step: 110, loss: 0.00471471669152379\n",
            "step: 120, loss: 0.0011786588001996279\n",
            "step: 130, loss: 0.000720759155228734\n",
            "step: 140, loss: 0.0020894978661090136\n",
            "step: 150, loss: 0.0007823261548765004\n",
            "step: 160, loss: 0.0018853646470233798\n",
            "step: 170, loss: 0.00054491515038535\n",
            "step: 180, loss: 0.0004602793778758496\n",
            "step: 190, loss: 0.0017370289424434304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7548209366391184, f1=0.7821229050279329, best_f1=0.7745358090185677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000996092800050974\n",
            "step: 10, loss: 0.005592815112322569\n",
            "step: 20, loss: 0.0007136081112548709\n",
            "step: 30, loss: 0.002792682498693466\n",
            "step: 40, loss: 0.015257886610925198\n",
            "step: 50, loss: 0.002441779710352421\n",
            "step: 60, loss: 0.02063959650695324\n",
            "step: 70, loss: 0.004388793371617794\n",
            "step: 80, loss: 0.003128783078864217\n",
            "step: 90, loss: 0.0027037356048822403\n",
            "step: 100, loss: 0.0007241209968924522\n",
            "step: 110, loss: 0.0007264756131917238\n",
            "step: 120, loss: 0.0006471549277193844\n",
            "step: 130, loss: 0.003468045499175787\n",
            "step: 140, loss: 0.0033049657940864563\n",
            "step: 150, loss: 0.00045071454951539636\n",
            "step: 160, loss: 0.012760859914124012\n",
            "step: 170, loss: 0.004614532925188541\n",
            "step: 180, loss: 0.0004950602306053042\n",
            "step: 190, loss: 0.0010553475003689528\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7717391304347826, f1=0.7683923705722071, best_f1=0.7683923705722071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013539273058995605\n",
            "step: 10, loss: 0.008541020564734936\n",
            "step: 20, loss: 0.0004981959355063736\n",
            "step: 30, loss: 0.008011904545128345\n",
            "step: 40, loss: 0.0005605563637800515\n",
            "step: 50, loss: 0.0022754520177841187\n",
            "step: 60, loss: 0.0006143946666270494\n",
            "step: 70, loss: 0.040749140083789825\n",
            "step: 80, loss: 0.008408461697399616\n",
            "step: 90, loss: 0.005914660636335611\n",
            "step: 100, loss: 0.026829397305846214\n",
            "step: 110, loss: 0.0009821048006415367\n",
            "step: 120, loss: 0.0013852481497451663\n",
            "step: 130, loss: 0.0025408139917999506\n",
            "step: 140, loss: 0.0004097140918020159\n",
            "step: 150, loss: 0.0008089351467788219\n",
            "step: 160, loss: 0.0009586484520696104\n",
            "step: 170, loss: 0.0005699563771486282\n",
            "step: 180, loss: 0.0012870568316429853\n",
            "step: 190, loss: 0.001294466550461948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7692307692307692, f1=0.7777777777777778, best_f1=0.7683923705722071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02436958998441696\n",
            "step: 10, loss: 0.0012322379043325782\n",
            "step: 20, loss: 0.0007447054958902299\n",
            "step: 30, loss: 0.00042913787183351815\n",
            "step: 40, loss: 0.0030688161496073008\n",
            "step: 50, loss: 0.0005830790614709258\n",
            "step: 60, loss: 0.0007623675046488643\n",
            "step: 70, loss: 0.0009539610473439097\n",
            "step: 80, loss: 0.008611004799604416\n",
            "step: 90, loss: 0.0022218339145183563\n",
            "step: 100, loss: 0.0034733039792627096\n",
            "step: 110, loss: 0.004033922683447599\n",
            "step: 120, loss: 0.1413804292678833\n",
            "step: 130, loss: 0.00281276972964406\n",
            "step: 140, loss: 0.10581211000680923\n",
            "step: 150, loss: 0.0010144587140530348\n",
            "step: 160, loss: 0.0038661437574774027\n",
            "step: 170, loss: 0.0012007084442302585\n",
            "step: 180, loss: 0.005690384656190872\n",
            "step: 190, loss: 0.0008638322469778359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7594936708860759, f1=0.7512437810945273, best_f1=0.7683923705722071\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013674176298081875\n",
            "step: 10, loss: 0.0005550537607632577\n",
            "step: 20, loss: 0.0004908666596747935\n",
            "step: 30, loss: 0.0008842690149322152\n",
            "step: 40, loss: 0.00031305180164054036\n",
            "step: 50, loss: 0.003786759916692972\n",
            "step: 60, loss: 0.008723122999072075\n",
            "step: 70, loss: 0.00046964408829808235\n",
            "step: 80, loss: 0.004242732655256987\n",
            "step: 90, loss: 0.0006857895641587675\n",
            "step: 100, loss: 0.0116479666903615\n",
            "step: 110, loss: 0.0010926782852038741\n",
            "step: 120, loss: 0.00037967541720718145\n",
            "step: 130, loss: 0.004531232174485922\n",
            "step: 140, loss: 0.07506348937749863\n",
            "step: 150, loss: 0.000262886518612504\n",
            "step: 160, loss: 0.004413124639540911\n",
            "step: 170, loss: 0.0012763881823047996\n",
            "step: 180, loss: 0.003920495975762606\n",
            "step: 190, loss: 0.0008956511155702174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7563025210084034, f1=0.7627118644067796, best_f1=0.7683923705722071\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:09, 210.47it/s]\n",
            "load_f1 = 0.6855670103092784\n",
            "real_f1 = 0.6616541353383459\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 236.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ef9e04-062f-4a94-94b9-6d1e4d51c7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rDownloading:   0% 0.00/433 [00:00<?, ?B/s]\rDownloading: 100% 433/433 [00:00<00:00, 594kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 25.1MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 64.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6382588148117065\n",
            "step: 10, loss: 0.3735152781009674\n",
            "step: 20, loss: 0.3016364574432373\n",
            "step: 30, loss: 0.39490753412246704\n",
            "step: 40, loss: 0.2798600196838379\n",
            "step: 50, loss: 0.25113895535469055\n",
            "step: 60, loss: 0.23774713277816772\n",
            "step: 70, loss: 0.3686505854129791\n",
            "step: 80, loss: 0.3841409385204315\n",
            "step: 90, loss: 0.2571116089820862\n",
            "step: 100, loss: 0.27272000908851624\n",
            "step: 110, loss: 0.21391870081424713\n",
            "step: 120, loss: 0.3739422857761383\n",
            "step: 130, loss: 0.06271576136350632\n",
            "step: 140, loss: 0.1982114315032959\n",
            "step: 150, loss: 0.24036845564842224\n",
            "step: 160, loss: 0.14275866746902466\n",
            "step: 170, loss: 0.18395505845546722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6603325415676958, f1=0.6588785046728973, best_f1=0.6588785046728973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20569409430027008\n",
            "step: 10, loss: 0.18306735157966614\n",
            "step: 20, loss: 0.12092436105012894\n",
            "step: 30, loss: 0.1690874546766281\n",
            "step: 40, loss: 0.04890768229961395\n",
            "step: 50, loss: 0.10741548985242844\n",
            "step: 60, loss: 0.11618684977293015\n",
            "step: 70, loss: 0.08915266394615173\n",
            "step: 80, loss: 0.04381820932030678\n",
            "step: 90, loss: 0.0768558457493782\n",
            "step: 100, loss: 0.25695720314979553\n",
            "step: 110, loss: 0.06408365070819855\n",
            "step: 120, loss: 0.10167885571718216\n",
            "step: 130, loss: 0.1486247330904007\n",
            "step: 140, loss: 0.30544617772102356\n",
            "step: 150, loss: 0.1608988493680954\n",
            "step: 160, loss: 0.19830051064491272\n",
            "step: 170, loss: 0.07241767644882202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7643979057591622, f1=0.7543424317617866, best_f1=0.7543424317617866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1771627813577652\n",
            "step: 10, loss: 0.2019398957490921\n",
            "step: 20, loss: 0.06437958776950836\n",
            "step: 30, loss: 0.21090947091579437\n",
            "step: 40, loss: 0.10232047736644745\n",
            "step: 50, loss: 0.10335147380828857\n",
            "step: 60, loss: 0.21263235807418823\n",
            "step: 70, loss: 0.06719411164522171\n",
            "step: 80, loss: 0.03580252453684807\n",
            "step: 90, loss: 0.2759377956390381\n",
            "step: 100, loss: 0.05085710808634758\n",
            "step: 110, loss: 0.045040614902973175\n",
            "step: 120, loss: 0.1273248940706253\n",
            "step: 130, loss: 0.19810569286346436\n",
            "step: 140, loss: 0.1661810576915741\n",
            "step: 150, loss: 0.011889401823282242\n",
            "step: 160, loss: 0.00785076804459095\n",
            "step: 170, loss: 0.09051244705915451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7832512315270935, f1=0.772093023255814, best_f1=0.772093023255814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010548298247158527\n",
            "step: 10, loss: 0.03318019211292267\n",
            "step: 20, loss: 0.025857439264655113\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.02258303388953209\n",
            "step: 40, loss: 0.008005013689398766\n",
            "step: 50, loss: 0.04870303347706795\n",
            "step: 60, loss: 0.24029922485351562\n",
            "step: 70, loss: 0.01448972336947918\n",
            "step: 80, loss: 0.04867205396294594\n",
            "step: 90, loss: 0.1414155662059784\n",
            "step: 100, loss: 0.22290083765983582\n",
            "step: 110, loss: 0.274906724691391\n",
            "step: 120, loss: 0.008361630141735077\n",
            "step: 130, loss: 0.05368444696068764\n",
            "step: 140, loss: 0.07393131405115128\n",
            "step: 150, loss: 0.19539028406143188\n",
            "step: 160, loss: 0.054271988570690155\n",
            "step: 170, loss: 0.006403012666851282\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7780548628428928, f1=0.7864077669902912, best_f1=0.772093023255814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03532961755990982\n",
            "step: 10, loss: 0.016211099922657013\n",
            "step: 20, loss: 0.06329654902219772\n",
            "step: 30, loss: 0.031524017453193665\n",
            "step: 40, loss: 0.027151847258210182\n",
            "step: 50, loss: 0.16494455933570862\n",
            "step: 60, loss: 0.08793344348669052\n",
            "step: 70, loss: 0.1436934769153595\n",
            "step: 80, loss: 0.25962185859680176\n",
            "step: 90, loss: 0.07049696147441864\n",
            "step: 100, loss: 0.2611754834651947\n",
            "step: 110, loss: 0.1556156426668167\n",
            "step: 120, loss: 0.05373804271221161\n",
            "step: 130, loss: 0.04895049333572388\n",
            "step: 140, loss: 0.00290064699947834\n",
            "step: 150, loss: 0.032980777323246\n",
            "step: 160, loss: 0.02752435952425003\n",
            "step: 170, loss: 0.01210855320096016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8166259168704155, f1=0.8076009501187648, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016344113275408745\n",
            "step: 10, loss: 0.021594736725091934\n",
            "step: 20, loss: 0.03043089434504509\n",
            "step: 30, loss: 0.022987335920333862\n",
            "step: 40, loss: 0.007218166720122099\n",
            "step: 50, loss: 0.2372358739376068\n",
            "step: 60, loss: 0.1054198145866394\n",
            "step: 70, loss: 0.18157272040843964\n",
            "step: 80, loss: 0.056406378746032715\n",
            "step: 90, loss: 0.038307882845401764\n",
            "step: 100, loss: 0.01019169669598341\n",
            "step: 110, loss: 0.0014121334534138441\n",
            "step: 120, loss: 0.03763160482048988\n",
            "step: 130, loss: 0.036635588854551315\n",
            "step: 140, loss: 0.12697049975395203\n",
            "step: 150, loss: 0.024159051477909088\n",
            "step: 160, loss: 0.12716637551784515\n",
            "step: 170, loss: 0.07398227602243423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7830687830687831, f1=0.761904761904762, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02028767764568329\n",
            "step: 10, loss: 0.005131332203745842\n",
            "step: 20, loss: 0.07159207761287689\n",
            "step: 30, loss: 0.004372369032353163\n",
            "step: 40, loss: 0.024927766993641853\n",
            "step: 50, loss: 0.033300913870334625\n",
            "step: 60, loss: 0.006300886627286673\n",
            "step: 70, loss: 0.002125043887645006\n",
            "step: 80, loss: 0.01694617047905922\n",
            "step: 90, loss: 0.00323398201726377\n",
            "step: 100, loss: 0.0005338441696949303\n",
            "step: 110, loss: 0.19125881791114807\n",
            "step: 120, loss: 0.05573524534702301\n",
            "step: 130, loss: 0.10902043431997299\n",
            "step: 140, loss: 0.18522877991199493\n",
            "step: 150, loss: 0.009846481494605541\n",
            "step: 160, loss: 0.08686904609203339\n",
            "step: 170, loss: 0.16354775428771973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7941176470588236, f1=0.7713625866050808, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011609701439738274\n",
            "step: 10, loss: 0.028374915942549706\n",
            "step: 20, loss: 0.0066437809728085995\n",
            "step: 30, loss: 0.0238620787858963\n",
            "step: 40, loss: 0.0003630045976024121\n",
            "step: 50, loss: 0.005208578426390886\n",
            "step: 60, loss: 0.005066650453954935\n",
            "step: 70, loss: 0.03749929368495941\n",
            "step: 80, loss: 0.01736893132328987\n",
            "step: 90, loss: 0.012864232063293457\n",
            "step: 100, loss: 0.03616498410701752\n",
            "step: 110, loss: 0.07423548400402069\n",
            "step: 120, loss: 0.00868464820086956\n",
            "step: 130, loss: 0.004862077534198761\n",
            "step: 140, loss: 0.007677697576582432\n",
            "step: 150, loss: 0.30763518810272217\n",
            "step: 160, loss: 0.005310962442308664\n",
            "step: 170, loss: 0.0493488535284996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7858942065491185, f1=0.7677725118483412, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004167004663031548\n",
            "step: 10, loss: 0.08652154356241226\n",
            "step: 20, loss: 0.004325177054852247\n",
            "step: 30, loss: 0.0270827803760767\n",
            "step: 40, loss: 0.003389936638996005\n",
            "step: 50, loss: 0.0006410720525309443\n",
            "step: 60, loss: 0.08585745841264725\n",
            "step: 70, loss: 0.014046336524188519\n",
            "step: 80, loss: 0.0009947381913661957\n",
            "step: 90, loss: 0.025737429037690163\n",
            "step: 100, loss: 0.06945355981588364\n",
            "step: 110, loss: 0.007418195251375437\n",
            "step: 120, loss: 0.0031481250189244747\n",
            "step: 130, loss: 0.03264731168746948\n",
            "step: 140, loss: 0.13159678876399994\n",
            "step: 150, loss: 0.0028419073205441236\n",
            "step: 160, loss: 0.1538509875535965\n",
            "step: 170, loss: 0.021081287413835526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7980295566502462, f1=0.7741935483870966, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07351604849100113\n",
            "step: 10, loss: 0.002565047238022089\n",
            "step: 20, loss: 0.07928292453289032\n",
            "step: 30, loss: 0.012552985921502113\n",
            "step: 40, loss: 0.0002325509412912652\n",
            "step: 50, loss: 0.047606877982616425\n",
            "step: 60, loss: 0.0003409194468986243\n",
            "step: 70, loss: 0.0016318848356604576\n",
            "step: 80, loss: 0.0002706522645894438\n",
            "step: 90, loss: 0.009161020629107952\n",
            "step: 100, loss: 0.0010724577587097883\n",
            "step: 110, loss: 0.0008367894333787262\n",
            "step: 120, loss: 0.023019710555672646\n",
            "step: 130, loss: 0.0416899211704731\n",
            "step: 140, loss: 0.0026076447684317827\n",
            "step: 150, loss: 0.019716234877705574\n",
            "step: 160, loss: 0.009303918108344078\n",
            "step: 170, loss: 0.001285894657485187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7918781725888325, f1=0.7772511848341233, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02750048041343689\n",
            "step: 10, loss: 0.006612843368202448\n",
            "step: 20, loss: 0.0014381235232576728\n",
            "step: 30, loss: 0.0004373996052891016\n",
            "step: 40, loss: 0.0007802999462001026\n",
            "step: 50, loss: 0.004636287689208984\n",
            "step: 60, loss: 0.06557587534189224\n",
            "step: 70, loss: 0.00039361976087093353\n",
            "step: 80, loss: 0.0006167241372168064\n",
            "step: 90, loss: 0.002802010625600815\n",
            "step: 100, loss: 0.0010802808683365583\n",
            "step: 110, loss: 0.027742763981223106\n",
            "step: 120, loss: 0.0006091149407438934\n",
            "step: 130, loss: 0.0002473840431775898\n",
            "step: 140, loss: 0.00316755217500031\n",
            "step: 150, loss: 0.007609803695231676\n",
            "step: 160, loss: 0.026591569185256958\n",
            "step: 170, loss: 0.0011070745531469584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8095238095238095, f1=0.7555555555555555, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015222594141960144\n",
            "step: 10, loss: 0.000613891170360148\n",
            "step: 20, loss: 0.00042807511636056006\n",
            "step: 30, loss: 0.0019926163367927074\n",
            "step: 40, loss: 0.0027604647912085056\n",
            "step: 50, loss: 0.0027726374100893736\n",
            "step: 60, loss: 0.014198647812008858\n",
            "step: 70, loss: 0.06885785609483719\n",
            "step: 80, loss: 0.0007278930279426277\n",
            "step: 90, loss: 0.0027084939647465944\n",
            "step: 100, loss: 0.002341283019632101\n",
            "step: 110, loss: 0.0026189018972218037\n",
            "step: 120, loss: 0.0010365914786234498\n",
            "step: 130, loss: 0.06958725303411484\n",
            "step: 140, loss: 0.007576644420623779\n",
            "step: 150, loss: 0.005858494900166988\n",
            "step: 160, loss: 0.0007107358542270958\n",
            "step: 170, loss: 0.0004820397007279098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.762402088772846, f1=0.7853658536585365, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23767608404159546\n",
            "step: 10, loss: 0.05700230970978737\n",
            "step: 20, loss: 0.003402977716177702\n",
            "step: 30, loss: 0.0008768919506110251\n",
            "step: 40, loss: 0.026398399844765663\n",
            "step: 50, loss: 0.0012653230223804712\n",
            "step: 60, loss: 0.009306834079325199\n",
            "step: 70, loss: 0.020082546398043633\n",
            "step: 80, loss: 0.004694918170571327\n",
            "step: 90, loss: 0.000393409573007375\n",
            "step: 100, loss: 0.006348202005028725\n",
            "step: 110, loss: 0.000408947526011616\n",
            "step: 120, loss: 0.027860363945364952\n",
            "step: 130, loss: 0.0011033340124413371\n",
            "step: 140, loss: 0.0005995667888782918\n",
            "step: 150, loss: 0.0019223574781790376\n",
            "step: 160, loss: 0.10570166260004044\n",
            "step: 170, loss: 0.0014787674881517887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7830687830687831, f1=0.8061224489795918, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03312757611274719\n",
            "step: 10, loss: 0.0004603721317835152\n",
            "step: 20, loss: 0.00744609534740448\n",
            "step: 30, loss: 0.006881830282509327\n",
            "step: 40, loss: 0.0006749862222932279\n",
            "step: 50, loss: 0.07282920926809311\n",
            "step: 60, loss: 0.002896952209994197\n",
            "step: 70, loss: 0.0021101871971040964\n",
            "step: 80, loss: 0.0063686855137348175\n",
            "step: 90, loss: 0.001964298076927662\n",
            "step: 100, loss: 0.0023893583565950394\n",
            "step: 110, loss: 0.10865268111228943\n",
            "step: 120, loss: 0.055931005626916885\n",
            "step: 130, loss: 0.04228787124156952\n",
            "step: 140, loss: 0.004499190021306276\n",
            "step: 150, loss: 0.0027054203674197197\n",
            "step: 160, loss: 0.0018933992832899094\n",
            "step: 170, loss: 0.003037984250113368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8140703517587939, f1=0.7990314769975787, best_f1=0.8076009501187648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003691143065225333\n",
            "step: 10, loss: 0.00466390373185277\n",
            "step: 20, loss: 0.04183083772659302\n",
            "step: 30, loss: 0.0011103228898718953\n",
            "step: 40, loss: 0.001974657643586397\n",
            "step: 50, loss: 0.004648052155971527\n",
            "step: 60, loss: 0.018611283972859383\n",
            "step: 70, loss: 0.00034575039171613753\n",
            "step: 80, loss: 0.02585310861468315\n",
            "step: 90, loss: 0.0007342136232182384\n",
            "step: 100, loss: 0.007424386218190193\n",
            "step: 110, loss: 0.000858645944390446\n",
            "step: 120, loss: 0.0045915404334664345\n",
            "step: 130, loss: 0.004480821546167135\n",
            "step: 140, loss: 0.024431873112916946\n",
            "step: 150, loss: 0.004447432234883308\n",
            "step: 160, loss: 0.012131715193390846\n",
            "step: 170, loss: 0.004003035370260477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8110831234256927, f1=0.7971014492753623, best_f1=0.8076009501187648\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 289.70it/s]\n",
            "load_f1 = 0.33093525179856115\n",
            "real_f1 = 0.3290845886442642\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 264.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f02d15-2e93-49e2-af84-9afb908dd301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6036078929901123\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.6152273416519165\n",
            "step: 20, loss: 0.45180797576904297\n",
            "step: 30, loss: 0.28557056188583374\n",
            "step: 40, loss: 0.20734496414661407\n",
            "step: 50, loss: 0.0689634159207344\n",
            "step: 60, loss: 0.1445380300283432\n",
            "step: 70, loss: 0.09373138844966888\n",
            "step: 80, loss: 0.09237721562385559\n",
            "step: 90, loss: 0.16688038408756256\n",
            "step: 100, loss: 0.01571529358625412\n",
            "step: 110, loss: 0.21531370282173157\n",
            "step: 120, loss: 0.014417739585042\n",
            "step: 130, loss: 0.011900393292307854\n",
            "step: 140, loss: 0.002966728061437607\n",
            "step: 150, loss: 0.010401282459497452\n",
            "step: 160, loss: 0.02881181612610817\n",
            "step: 170, loss: 0.21406282484531403\n",
            "step: 180, loss: 0.1759716123342514\n",
            "step: 190, loss: 0.08901552855968475\n",
            "step: 200, loss: 0.1024487167596817\n",
            "step: 210, loss: 0.0043702335096895695\n",
            "step: 220, loss: 0.024508029222488403\n",
            "step: 230, loss: 0.05183238536119461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9682539682539683, f1=0.9608294930875576, best_f1=0.9608294930875576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011756865307688713\n",
            "step: 10, loss: 0.010090998373925686\n",
            "step: 20, loss: 0.11384432762861252\n",
            "step: 30, loss: 0.19167260825634003\n",
            "step: 40, loss: 0.03965531289577484\n",
            "step: 50, loss: 0.004085137043148279\n",
            "step: 60, loss: 0.018265375867486\n",
            "step: 70, loss: 0.11246372014284134\n",
            "step: 80, loss: 0.09244529902935028\n",
            "step: 90, loss: 0.025097284466028214\n",
            "step: 100, loss: 0.11751779168844223\n",
            "step: 110, loss: 0.13281042873859406\n",
            "step: 120, loss: 0.14326918125152588\n",
            "step: 130, loss: 0.028123434633016586\n",
            "step: 140, loss: 0.0035040637012571096\n",
            "step: 150, loss: 0.012316172942519188\n",
            "step: 160, loss: 0.017388876527547836\n",
            "step: 170, loss: 0.002487249206751585\n",
            "step: 180, loss: 0.0833701565861702\n",
            "step: 190, loss: 0.003050851169973612\n",
            "step: 200, loss: 0.004548192024230957\n",
            "step: 210, loss: 0.033368874341249466\n",
            "step: 220, loss: 0.03578527644276619\n",
            "step: 230, loss: 0.029627203941345215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9765886287625419, f1=0.9743589743589743, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00814746879041195\n",
            "step: 10, loss: 0.012469637207686901\n",
            "step: 20, loss: 0.003033985150977969\n",
            "step: 30, loss: 0.1544475257396698\n",
            "step: 40, loss: 0.16189265251159668\n",
            "step: 50, loss: 0.009965581819415092\n",
            "step: 60, loss: 0.007917391136288643\n",
            "step: 70, loss: 0.033881817013025284\n",
            "step: 80, loss: 0.0011874452466145158\n",
            "step: 90, loss: 0.005773146171122789\n",
            "step: 100, loss: 0.004999507684260607\n",
            "step: 110, loss: 0.0010267851175740361\n",
            "step: 120, loss: 0.055479198694229126\n",
            "step: 130, loss: 0.0026390256825834513\n",
            "step: 140, loss: 0.005984729155898094\n",
            "step: 150, loss: 0.008014963939785957\n",
            "step: 160, loss: 0.01696452870965004\n",
            "step: 170, loss: 0.013586190529167652\n",
            "step: 180, loss: 0.05645343288779259\n",
            "step: 190, loss: 0.007047305814921856\n",
            "step: 200, loss: 0.004471295513212681\n",
            "step: 210, loss: 0.00184512825217098\n",
            "step: 220, loss: 0.04703930392861366\n",
            "step: 230, loss: 0.0006231777952052653\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.978912319644839, f1=0.9721913236929923, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001032689237035811\n",
            "step: 10, loss: 0.0007902246434241533\n",
            "step: 20, loss: 0.019731566309928894\n",
            "step: 30, loss: 0.000880143721587956\n",
            "step: 40, loss: 0.005762617569416761\n",
            "step: 50, loss: 0.001176354824565351\n",
            "step: 60, loss: 0.0015313575277104974\n",
            "step: 70, loss: 0.0009268406429328024\n",
            "step: 80, loss: 0.0022653997875750065\n",
            "step: 90, loss: 0.0032435439061373472\n",
            "step: 100, loss: 0.001043914700858295\n",
            "step: 110, loss: 0.007316647097468376\n",
            "step: 120, loss: 0.05768914520740509\n",
            "step: 130, loss: 0.0017346504610031843\n",
            "step: 140, loss: 0.018132707104086876\n",
            "step: 150, loss: 0.14891138672828674\n",
            "step: 160, loss: 0.11006245017051697\n",
            "step: 170, loss: 0.006928429938852787\n",
            "step: 180, loss: 0.0007607451407238841\n",
            "step: 190, loss: 0.00889984518289566\n",
            "step: 200, loss: 0.0009509255760349333\n",
            "step: 210, loss: 0.13595469295978546\n",
            "step: 220, loss: 0.00035656520049087703\n",
            "step: 230, loss: 0.006300784647464752\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9775784753363228, f1=0.9786276715410572, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007737474516034126\n",
            "step: 10, loss: 0.024852409958839417\n",
            "step: 20, loss: 0.002764445962384343\n",
            "step: 30, loss: 0.0003845811006613076\n",
            "step: 40, loss: 0.0004556264029815793\n",
            "step: 50, loss: 0.000492453807964921\n",
            "step: 60, loss: 0.16097892820835114\n",
            "step: 70, loss: 0.009141388349235058\n",
            "step: 80, loss: 0.00048485337174497545\n",
            "step: 90, loss: 0.008931964635848999\n",
            "step: 100, loss: 0.00045525195309892297\n",
            "step: 110, loss: 0.0010581298265606165\n",
            "step: 120, loss: 0.0023466141428798437\n",
            "step: 130, loss: 0.0016209472669288516\n",
            "step: 140, loss: 0.0035701259039342403\n",
            "step: 150, loss: 0.0008669144008308649\n",
            "step: 160, loss: 0.0011627546045929193\n",
            "step: 170, loss: 0.01963425613939762\n",
            "step: 180, loss: 0.022440314292907715\n",
            "step: 190, loss: 0.12412431836128235\n",
            "step: 200, loss: 0.018044373020529747\n",
            "step: 210, loss: 0.0050128246657550335\n",
            "step: 220, loss: 0.0031435585115104914\n",
            "step: 230, loss: 0.0034496800508350134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9777777777777777, f1=0.9765886287625419, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024016056209802628\n",
            "step: 10, loss: 0.00027635128935799\n",
            "step: 20, loss: 0.0006065406487323344\n",
            "step: 30, loss: 0.00019112932204734534\n",
            "step: 40, loss: 0.0010742275044322014\n",
            "step: 50, loss: 0.0005832203896716237\n",
            "step: 60, loss: 0.006855517625808716\n",
            "step: 70, loss: 0.0040524061769247055\n",
            "step: 80, loss: 0.000738044735044241\n",
            "step: 90, loss: 0.001861167955212295\n",
            "step: 100, loss: 0.05479247495532036\n",
            "step: 110, loss: 0.0038012610748410225\n",
            "step: 120, loss: 0.000976684968918562\n",
            "step: 130, loss: 0.00025349666248075664\n",
            "step: 140, loss: 0.0009988733800128102\n",
            "step: 150, loss: 0.04685477539896965\n",
            "step: 160, loss: 0.00039396705687977374\n",
            "step: 170, loss: 0.00030082507873885334\n",
            "step: 180, loss: 0.014672794379293919\n",
            "step: 190, loss: 0.0037771398201584816\n",
            "step: 200, loss: 0.0001003577999654226\n",
            "step: 210, loss: 0.0006859177374280989\n",
            "step: 220, loss: 0.00010901251516770571\n",
            "step: 230, loss: 0.023104393854737282\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9755011135857461, f1=0.9765886287625419, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00028315564850345254\n",
            "step: 10, loss: 0.00011850603186758235\n",
            "step: 20, loss: 0.0003445638867560774\n",
            "step: 30, loss: 0.00013752082304563373\n",
            "step: 40, loss: 0.001010993029922247\n",
            "step: 50, loss: 0.0003810396883636713\n",
            "step: 60, loss: 0.009294604882597923\n",
            "step: 70, loss: 0.0060250586830079556\n",
            "step: 80, loss: 0.0004123076214455068\n",
            "step: 90, loss: 0.0003191581345163286\n",
            "step: 100, loss: 0.0001292716187890619\n",
            "step: 110, loss: 0.0008423259132541716\n",
            "step: 120, loss: 0.00027511161169968545\n",
            "step: 130, loss: 0.000244701310293749\n",
            "step: 140, loss: 0.00043041701428592205\n",
            "step: 150, loss: 0.000681597157381475\n",
            "step: 160, loss: 0.09402839839458466\n",
            "step: 170, loss: 0.010603900998830795\n",
            "step: 180, loss: 0.0008323994698002934\n",
            "step: 190, loss: 0.0005324704106897116\n",
            "step: 200, loss: 0.04756081476807594\n",
            "step: 210, loss: 8.444728155154735e-05\n",
            "step: 220, loss: 0.0003569523396436125\n",
            "step: 230, loss: 0.00039907643804326653\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9765886287625419, f1=0.9776286353467561, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.217625195859e-05\n",
            "step: 10, loss: 0.0051375748589634895\n",
            "step: 20, loss: 0.014629172161221504\n",
            "step: 30, loss: 7.40487957955338e-05\n",
            "step: 40, loss: 0.006328292191028595\n",
            "step: 50, loss: 0.0002123154845321551\n",
            "step: 60, loss: 6.766864680685103e-05\n",
            "step: 70, loss: 0.0026076713111251593\n",
            "step: 80, loss: 9.468349890084937e-05\n",
            "step: 90, loss: 3.443565219640732e-05\n",
            "step: 100, loss: 0.0001571990578668192\n",
            "step: 110, loss: 0.031099315732717514\n",
            "step: 120, loss: 0.02181997336447239\n",
            "step: 130, loss: 0.008176887407898903\n",
            "step: 140, loss: 0.00010288010525982827\n",
            "step: 150, loss: 7.061901851557195e-05\n",
            "step: 160, loss: 0.0002481411211192608\n",
            "step: 170, loss: 0.00013089798449072987\n",
            "step: 180, loss: 0.0007977747591212392\n",
            "step: 190, loss: 0.004597879014909267\n",
            "step: 200, loss: 0.003407923271879554\n",
            "step: 210, loss: 0.002522081835195422\n",
            "step: 220, loss: 0.0004047861439175904\n",
            "step: 230, loss: 0.0002851660829037428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9773242630385486, f1=0.9748858447488584, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000531586934812367\n",
            "step: 10, loss: 0.0013185156276449561\n",
            "step: 20, loss: 0.00039960749563761055\n",
            "step: 30, loss: 0.00019168273138348013\n",
            "step: 40, loss: 0.004598418716341257\n",
            "step: 50, loss: 0.0003616784233599901\n",
            "step: 60, loss: 0.0001669190387474373\n",
            "step: 70, loss: 0.001031421241350472\n",
            "step: 80, loss: 7.694278610870242e-05\n",
            "step: 90, loss: 0.0012806139420717955\n",
            "step: 100, loss: 0.00021733860194217414\n",
            "step: 110, loss: 5.8438843552721664e-05\n",
            "step: 120, loss: 6.0039456002414227e-05\n",
            "step: 130, loss: 0.00014221062883734703\n",
            "step: 140, loss: 5.088116813567467e-05\n",
            "step: 150, loss: 0.00018736865604296327\n",
            "step: 160, loss: 5.30338948010467e-05\n",
            "step: 170, loss: 7.193767669377849e-05\n",
            "step: 180, loss: 0.00025283879949711263\n",
            "step: 190, loss: 0.00011649663065327331\n",
            "step: 200, loss: 0.0009690753649920225\n",
            "step: 210, loss: 0.0002960807178169489\n",
            "step: 220, loss: 0.00013008325186092407\n",
            "step: 230, loss: 6.907113856868818e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9794988610478361, f1=0.9772727272727272, best_f1=0.9772727272727272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.28502922761254e-05\n",
            "step: 10, loss: 8.723825158085674e-05\n",
            "step: 20, loss: 0.0005236138240434229\n",
            "step: 30, loss: 8.190791413653642e-05\n",
            "step: 40, loss: 0.0061009894125163555\n",
            "step: 50, loss: 0.009475363418459892\n",
            "step: 60, loss: 7.228054164443165e-05\n",
            "step: 70, loss: 0.002020786749199033\n",
            "step: 80, loss: 0.000144995326991193\n",
            "step: 90, loss: 0.0009590164409019053\n",
            "step: 100, loss: 0.00023019526270218194\n",
            "step: 110, loss: 0.001922071911394596\n",
            "step: 120, loss: 9.519093873677775e-05\n",
            "step: 130, loss: 0.0002002950495807454\n",
            "step: 140, loss: 0.06417283415794373\n",
            "step: 150, loss: 0.02628319337964058\n",
            "step: 160, loss: 0.00021991250105202198\n",
            "step: 170, loss: 8.019653614610434e-05\n",
            "step: 180, loss: 0.000388311076676473\n",
            "step: 190, loss: 0.006344526540488005\n",
            "step: 200, loss: 8.451961184618995e-05\n",
            "step: 210, loss: 0.0010915591847151518\n",
            "step: 220, loss: 0.0003859613789245486\n",
            "step: 230, loss: 0.0011279081227257848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9785310734463276, f1=0.9785310734463276, best_f1=0.9772727272727272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003414472157601267\n",
            "step: 10, loss: 0.0001898606715258211\n",
            "step: 20, loss: 0.0003876727423630655\n",
            "step: 30, loss: 0.0016443085623905063\n",
            "step: 40, loss: 0.00034929902176372707\n",
            "step: 50, loss: 0.00011326761159580201\n",
            "step: 60, loss: 0.002068394096568227\n",
            "step: 70, loss: 0.00022603616525884718\n",
            "step: 80, loss: 5.873217014595866e-05\n",
            "step: 90, loss: 0.00011113355139968917\n",
            "step: 100, loss: 0.0027018056716769934\n",
            "step: 110, loss: 0.0015743756666779518\n",
            "step: 120, loss: 6.721351383021101e-05\n",
            "step: 130, loss: 8.511533815180883e-05\n",
            "step: 140, loss: 4.594562778947875e-05\n",
            "step: 150, loss: 0.0006710501038469374\n",
            "step: 160, loss: 6.374645454343408e-05\n",
            "step: 170, loss: 0.0699312761425972\n",
            "step: 180, loss: 0.0007551810122095048\n",
            "step: 190, loss: 5.767084803665057e-05\n",
            "step: 200, loss: 0.00015057400742080063\n",
            "step: 210, loss: 4.461204298422672e-05\n",
            "step: 220, loss: 6.776185909984633e-05\n",
            "step: 230, loss: 0.00011119216651422903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9807037457434733, f1=0.9761634506242906, best_f1=0.9761634506242906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.791964551666752e-05\n",
            "step: 10, loss: 0.00033154728589579463\n",
            "step: 20, loss: 0.00045829766895622015\n",
            "step: 30, loss: 0.0003994816215708852\n",
            "step: 40, loss: 0.00013552462041843683\n",
            "step: 50, loss: 0.0016194666968658566\n",
            "step: 60, loss: 0.0005457103834487498\n",
            "step: 70, loss: 4.7770761739229783e-05\n",
            "step: 80, loss: 0.0017115080263465643\n",
            "step: 90, loss: 5.536648677662015e-05\n",
            "step: 100, loss: 0.00014546074089594185\n",
            "step: 110, loss: 0.00010550850129220635\n",
            "step: 120, loss: 0.00021133493282832205\n",
            "step: 130, loss: 8.144097228068858e-05\n",
            "step: 140, loss: 5.589555439655669e-05\n",
            "step: 150, loss: 0.0001614154316484928\n",
            "step: 160, loss: 5.657095607602969e-05\n",
            "step: 170, loss: 9.439831046620384e-05\n",
            "step: 180, loss: 0.00010083200322696939\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.00013000410399399698\n",
            "step: 200, loss: 3.954590283683501e-05\n",
            "step: 210, loss: 5.631167732644826e-05\n",
            "step: 220, loss: 3.753856071853079e-05\n",
            "step: 230, loss: 0.026254555210471153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9753363228699552, f1=0.9798657718120806, best_f1=0.9761634506242906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011874268238898367\n",
            "step: 10, loss: 8.899473323253915e-05\n",
            "step: 20, loss: 0.0001426788221579045\n",
            "step: 30, loss: 0.00012454397801775485\n",
            "step: 40, loss: 0.00017640288569964468\n",
            "step: 50, loss: 7.956816989462823e-05\n",
            "step: 60, loss: 0.0006769172614440322\n",
            "step: 70, loss: 4.3111504055559635e-05\n",
            "step: 80, loss: 4.1233102820115164e-05\n",
            "step: 90, loss: 0.00013422466872725636\n",
            "step: 100, loss: 6.442348239943385e-05\n",
            "step: 110, loss: 0.016497278586030006\n",
            "step: 120, loss: 0.0016634105704724789\n",
            "step: 130, loss: 5.926973608438857e-05\n",
            "step: 140, loss: 0.0001027822945616208\n",
            "step: 150, loss: 4.26746737502981e-05\n",
            "step: 160, loss: 5.665925345965661e-05\n",
            "step: 170, loss: 0.0001047076511895284\n",
            "step: 180, loss: 0.002264486625790596\n",
            "step: 190, loss: 4.07858460675925e-05\n",
            "step: 200, loss: 0.0005867459112778306\n",
            "step: 210, loss: 3.3875301596708596e-05\n",
            "step: 220, loss: 2.9336029911064543e-05\n",
            "step: 230, loss: 4.906893809675239e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9785310734463276, f1=0.9808342728297633, best_f1=0.9761634506242906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.7077148337848485e-05\n",
            "step: 10, loss: 9.284060070058331e-05\n",
            "step: 20, loss: 7.282695878529921e-05\n",
            "step: 30, loss: 4.623569475370459e-05\n",
            "step: 40, loss: 0.08867929875850677\n",
            "step: 50, loss: 6.0266873333603144e-05\n",
            "step: 60, loss: 0.0002721527707763016\n",
            "step: 70, loss: 3.7397014239104465e-05\n",
            "step: 80, loss: 5.056061127106659e-05\n",
            "step: 90, loss: 4.997327778255567e-05\n",
            "step: 100, loss: 0.00030364730628207326\n",
            "step: 110, loss: 0.0006172626162879169\n",
            "step: 120, loss: 2.364021747780498e-05\n",
            "step: 130, loss: 4.1240557038690895e-05\n",
            "step: 140, loss: 3.6573670513462275e-05\n",
            "step: 150, loss: 4.646456363843754e-05\n",
            "step: 160, loss: 0.0004200846015010029\n",
            "step: 170, loss: 3.293419285910204e-05\n",
            "step: 180, loss: 4.75426932098344e-05\n",
            "step: 190, loss: 5.306049934006296e-05\n",
            "step: 200, loss: 3.127658783341758e-05\n",
            "step: 210, loss: 4.941636507282965e-05\n",
            "step: 220, loss: 3.4096250601578504e-05\n",
            "step: 230, loss: 0.006280047353357077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9763779527559054, f1=0.9808773903262092, best_f1=0.9761634506242906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.947753637272399e-05\n",
            "step: 10, loss: 2.746592326730024e-05\n",
            "step: 20, loss: 3.2766754884505644e-05\n",
            "step: 30, loss: 0.00011404320684960112\n",
            "step: 40, loss: 6.555276195285842e-05\n",
            "step: 50, loss: 0.011572490446269512\n",
            "step: 60, loss: 0.0001118622676585801\n",
            "step: 70, loss: 0.0002069891634164378\n",
            "step: 80, loss: 0.0014323790092021227\n",
            "step: 90, loss: 6.652955198660493e-05\n",
            "step: 100, loss: 5.696090738638304e-05\n",
            "step: 110, loss: 2.7763917387346737e-05\n",
            "step: 120, loss: 0.000411814748076722\n",
            "step: 130, loss: 3.834271410596557e-05\n",
            "step: 140, loss: 5.076561865280382e-05\n",
            "step: 150, loss: 7.183818524936214e-05\n",
            "step: 160, loss: 2.7070916985394433e-05\n",
            "step: 170, loss: 4.14801761507988e-05\n",
            "step: 180, loss: 0.0009572265553288162\n",
            "step: 190, loss: 0.00023142009740695357\n",
            "step: 200, loss: 7.183517300290987e-05\n",
            "step: 210, loss: 0.0007025222876109183\n",
            "step: 220, loss: 6.986431253608316e-05\n",
            "step: 230, loss: 9.291228343499824e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9753363228699552, f1=0.9809203142536477, best_f1=0.9761634506242906\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 220.37it/s]\n",
            "load_f1 = 0.9764837625979844\n",
            "real_f1 = 0.9730941704035874\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:15, 274.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8872fc53-5cb5-41e8-fbce-fa39f04c51dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.612539529800415\n",
            "step: 10, loss: 0.6007376909255981\n",
            "step: 20, loss: 0.5624711513519287\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.2437007576227188\n",
            "step: 40, loss: 0.32185128331184387\n",
            "step: 50, loss: 0.18242985010147095\n",
            "step: 60, loss: 0.05959119647741318\n",
            "step: 70, loss: 0.13986727595329285\n",
            "step: 80, loss: 0.081941619515419\n",
            "step: 90, loss: 0.29854947328567505\n",
            "step: 100, loss: 0.08296584337949753\n",
            "step: 110, loss: 0.08092795312404633\n",
            "step: 120, loss: 0.13931117951869965\n",
            "step: 130, loss: 0.1499040573835373\n",
            "step: 140, loss: 0.09027031809091568\n",
            "step: 150, loss: 0.03967338055372238\n",
            "step: 160, loss: 0.018117673695087433\n",
            "step: 170, loss: 0.22706809639930725\n",
            "step: 180, loss: 0.2165350317955017\n",
            "step: 190, loss: 0.055701710283756256\n",
            "step: 200, loss: 0.17940029501914978\n",
            "step: 210, loss: 0.0318361334502697\n",
            "step: 220, loss: 0.21765488386154175\n",
            "step: 230, loss: 0.1720486879348755\n",
            "step: 240, loss: 0.049955546855926514\n",
            "step: 250, loss: 0.016379330307245255\n",
            "step: 260, loss: 0.21765612065792084\n",
            "step: 270, loss: 0.012840325012803078\n",
            "step: 280, loss: 0.0360676534473896\n",
            "step: 290, loss: 0.12615269422531128\n",
            "step: 300, loss: 0.031007282435894012\n",
            "step: 310, loss: 0.07493545860052109\n",
            "step: 320, loss: 0.12486805766820908\n",
            "step: 330, loss: 0.0275421142578125\n",
            "step: 340, loss: 0.044410545378923416\n",
            "step: 350, loss: 0.014216299168765545\n",
            "step: 360, loss: 0.16979876160621643\n",
            "step: 370, loss: 0.09490679204463959\n",
            "step: 380, loss: 0.048034049570560455\n",
            "step: 390, loss: 0.2706860303878784\n",
            "step: 400, loss: 0.2467442750930786\n",
            "step: 410, loss: 0.06360463798046112\n",
            "step: 420, loss: 0.044862113893032074\n",
            "step: 430, loss: 0.24631854891777039\n",
            "step: 440, loss: 0.029950561001896858\n",
            "step: 450, loss: 0.035796087235212326\n",
            "step: 460, loss: 0.01994735561311245\n",
            "step: 470, loss: 0.18450325727462769\n",
            "step: 480, loss: 0.0899742990732193\n",
            "step: 490, loss: 0.09610113501548767\n",
            "step: 500, loss: 0.03191032260656357\n",
            "step: 510, loss: 0.11166496574878693\n",
            "step: 520, loss: 0.17629078030586243\n",
            "step: 530, loss: 0.008954904973506927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9279404927940492, f1=0.9280373831775701, best_f1=0.9280373831775701\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1695973128080368\n",
            "step: 10, loss: 0.06522028893232346\n",
            "step: 20, loss: 0.03267551586031914\n",
            "step: 30, loss: 0.03492056205868721\n",
            "step: 40, loss: 0.05062995105981827\n",
            "step: 50, loss: 0.12876221537590027\n",
            "step: 60, loss: 0.009125510230660439\n",
            "step: 70, loss: 0.1287851780653\n",
            "step: 80, loss: 0.04045557603240013\n",
            "step: 90, loss: 0.012581935152411461\n",
            "step: 100, loss: 0.08457966893911362\n",
            "step: 110, loss: 0.011520937085151672\n",
            "step: 120, loss: 0.14905765652656555\n",
            "step: 130, loss: 0.13462843000888824\n",
            "step: 140, loss: 0.033239495009183884\n",
            "step: 150, loss: 0.060535840690135956\n",
            "step: 160, loss: 0.044420093297958374\n",
            "step: 170, loss: 0.03516561537981033\n",
            "step: 180, loss: 0.02739412523806095\n",
            "step: 190, loss: 0.061443135142326355\n",
            "step: 200, loss: 0.03034185618162155\n",
            "step: 210, loss: 0.04483763128519058\n",
            "step: 220, loss: 0.14928443729877472\n",
            "step: 230, loss: 0.013972745276987553\n",
            "step: 240, loss: 0.011970628052949905\n",
            "step: 250, loss: 0.1051458865404129\n",
            "step: 260, loss: 0.006479908712208271\n",
            "step: 270, loss: 0.24318918585777283\n",
            "step: 280, loss: 0.006205155048519373\n",
            "step: 290, loss: 0.06427910923957825\n",
            "step: 300, loss: 0.1797700673341751\n",
            "step: 310, loss: 0.046685829758644104\n",
            "step: 320, loss: 0.15137533843517303\n",
            "step: 330, loss: 0.0299531240016222\n",
            "step: 340, loss: 0.0649411529302597\n",
            "step: 350, loss: 0.0037367248442023993\n",
            "step: 360, loss: 0.08548375219106674\n",
            "step: 370, loss: 0.10999441146850586\n",
            "step: 380, loss: 0.03665752336382866\n",
            "step: 390, loss: 0.04773888364434242\n",
            "step: 400, loss: 0.05775545537471771\n",
            "step: 410, loss: 0.02215137705206871\n",
            "step: 420, loss: 0.266801655292511\n",
            "step: 430, loss: 0.028185002505779266\n",
            "step: 440, loss: 0.16229966282844543\n",
            "step: 450, loss: 0.033191580325365067\n",
            "step: 460, loss: 0.015391351655125618\n",
            "step: 470, loss: 0.023455481976270676\n",
            "step: 480, loss: 0.193501815199852\n",
            "step: 490, loss: 0.01025659404695034\n",
            "step: 500, loss: 0.31333959102630615\n",
            "step: 510, loss: 0.018794545903801918\n",
            "step: 520, loss: 0.10907261818647385\n",
            "step: 530, loss: 0.011690504848957062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9404157043879908, f1=0.9320388349514563, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07547207921743393\n",
            "step: 10, loss: 0.10974660515785217\n",
            "step: 20, loss: 0.043557241559028625\n",
            "step: 30, loss: 0.14370006322860718\n",
            "step: 40, loss: 0.014082224108278751\n",
            "step: 50, loss: 0.1122189462184906\n",
            "step: 60, loss: 0.027691636234521866\n",
            "step: 70, loss: 0.00854448787868023\n",
            "step: 80, loss: 0.01642853207886219\n",
            "step: 90, loss: 0.0032316267024725676\n",
            "step: 100, loss: 0.038648881018161774\n",
            "step: 110, loss: 0.013810219243168831\n",
            "step: 120, loss: 0.09204892814159393\n",
            "step: 130, loss: 0.008380815386772156\n",
            "step: 140, loss: 0.08418113738298416\n",
            "step: 150, loss: 0.12392687797546387\n",
            "step: 160, loss: 0.04033222049474716\n",
            "step: 170, loss: 0.17452390491962433\n",
            "step: 180, loss: 0.06015633046627045\n",
            "step: 190, loss: 0.10004155337810516\n",
            "step: 200, loss: 0.06488199532032013\n",
            "step: 210, loss: 0.04420146718621254\n",
            "step: 220, loss: 0.020940491929650307\n",
            "step: 230, loss: 0.054696280509233475\n",
            "step: 240, loss: 0.0018130383687093854\n",
            "step: 250, loss: 0.051420945674180984\n",
            "step: 260, loss: 0.026890723034739494\n",
            "step: 270, loss: 0.003982358146458864\n",
            "step: 280, loss: 0.05497172474861145\n",
            "step: 290, loss: 0.003442977322265506\n",
            "step: 300, loss: 0.08986035734415054\n",
            "step: 310, loss: 0.006005724426358938\n",
            "step: 320, loss: 0.01726338267326355\n",
            "step: 330, loss: 0.0027898463886231184\n",
            "step: 340, loss: 0.01641993597149849\n",
            "step: 350, loss: 0.0451357439160347\n",
            "step: 360, loss: 0.018638119101524353\n",
            "step: 370, loss: 0.007747657597064972\n",
            "step: 380, loss: 0.01147131621837616\n",
            "step: 390, loss: 0.02235812321305275\n",
            "step: 400, loss: 0.008752563036978245\n",
            "step: 410, loss: 0.004960167687386274\n",
            "step: 420, loss: 0.10696311295032501\n",
            "step: 430, loss: 0.014440351165831089\n",
            "step: 440, loss: 0.009818029589951038\n",
            "step: 450, loss: 0.035812657326459885\n",
            "step: 460, loss: 0.01569502055644989\n",
            "step: 470, loss: 0.03596826270222664\n",
            "step: 480, loss: 0.020908020436763763\n",
            "step: 490, loss: 0.00670167850330472\n",
            "step: 500, loss: 0.037339624017477036\n",
            "step: 510, loss: 0.004730351269245148\n",
            "step: 520, loss: 0.131903275847435\n",
            "step: 530, loss: 0.19350086152553558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9346136259716507, f1=0.9294492489758761, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023882290348410606\n",
            "step: 10, loss: 0.0036671168636530638\n",
            "step: 20, loss: 0.0022179584484547377\n",
            "step: 30, loss: 0.01243915967643261\n",
            "step: 40, loss: 0.002182109048590064\n",
            "step: 50, loss: 0.01975908875465393\n",
            "step: 60, loss: 0.02441808208823204\n",
            "step: 70, loss: 0.0024599116295576096\n",
            "step: 80, loss: 0.072407066822052\n",
            "step: 90, loss: 0.11537441611289978\n",
            "step: 100, loss: 0.011206711642444134\n",
            "step: 110, loss: 0.010767092928290367\n",
            "step: 120, loss: 0.001123512047342956\n",
            "step: 130, loss: 0.001602945732884109\n",
            "step: 140, loss: 0.006343752145767212\n",
            "step: 150, loss: 0.02727525494992733\n",
            "step: 160, loss: 0.0707787573337555\n",
            "step: 170, loss: 0.007252747192978859\n",
            "step: 180, loss: 0.058642324060201645\n",
            "step: 190, loss: 0.13143014907836914\n",
            "step: 200, loss: 0.02655891701579094\n",
            "step: 210, loss: 0.010799426585435867\n",
            "step: 220, loss: 0.006556638050824404\n",
            "step: 230, loss: 0.07151476293802261\n",
            "step: 240, loss: 0.011864340864121914\n",
            "step: 250, loss: 0.0042561376467347145\n",
            "step: 260, loss: 0.002471138024702668\n",
            "step: 270, loss: 0.006026002578437328\n",
            "step: 280, loss: 0.10887911170721054\n",
            "step: 290, loss: 0.030600707978010178\n",
            "step: 300, loss: 0.0005110171041451395\n",
            "step: 310, loss: 0.001590306987054646\n",
            "step: 320, loss: 0.03655937314033508\n",
            "step: 330, loss: 0.00473359739407897\n",
            "step: 340, loss: 0.0029972896445542574\n",
            "step: 350, loss: 0.0748758465051651\n",
            "step: 360, loss: 0.05884779989719391\n",
            "step: 370, loss: 0.020837081596255302\n",
            "step: 380, loss: 0.005481378640979528\n",
            "step: 390, loss: 0.005005761049687862\n",
            "step: 400, loss: 0.0018449736526235938\n",
            "step: 410, loss: 0.007748785428702831\n",
            "step: 420, loss: 0.03731592372059822\n",
            "step: 430, loss: 0.04198446124792099\n",
            "step: 440, loss: 0.00837025884538889\n",
            "step: 450, loss: 0.019667528569698334\n",
            "step: 460, loss: 0.039593856781721115\n",
            "step: 470, loss: 0.11703465133905411\n",
            "step: 480, loss: 0.027191834524273872\n",
            "step: 490, loss: 0.004428436979651451\n",
            "step: 500, loss: 0.03667779639363289\n",
            "step: 510, loss: 0.02123061567544937\n",
            "step: 520, loss: 0.03823247179389\n",
            "step: 530, loss: 0.04972797632217407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9330166270783848, f1=0.9199999999999999, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.058525823056697845\n",
            "step: 10, loss: 0.03690040856599808\n",
            "step: 20, loss: 0.006459630560129881\n",
            "step: 30, loss: 0.001638609217479825\n",
            "step: 40, loss: 0.023685086518526077\n",
            "step: 50, loss: 0.0008471276378259063\n",
            "step: 60, loss: 0.0016830350505188107\n",
            "step: 70, loss: 0.006774791516363621\n",
            "step: 80, loss: 0.017636718228459358\n",
            "step: 90, loss: 0.020267294719815254\n",
            "step: 100, loss: 0.048251453787088394\n",
            "step: 110, loss: 0.011666390113532543\n",
            "step: 120, loss: 0.011005318723618984\n",
            "step: 130, loss: 0.0016487633110955358\n",
            "step: 140, loss: 0.01432145107537508\n",
            "step: 150, loss: 0.03139529377222061\n",
            "step: 160, loss: 0.0006563981878571212\n",
            "step: 170, loss: 0.042963068932294846\n",
            "step: 180, loss: 0.008547229692339897\n",
            "step: 190, loss: 0.022387715056538582\n",
            "step: 200, loss: 0.005698881112039089\n",
            "step: 210, loss: 0.0517289936542511\n",
            "step: 220, loss: 0.008221532218158245\n",
            "step: 230, loss: 0.008512409403920174\n",
            "step: 240, loss: 0.016249017789959908\n",
            "step: 250, loss: 0.0007711664657108486\n",
            "step: 260, loss: 0.003290983848273754\n",
            "step: 270, loss: 0.0023546444717794657\n",
            "step: 280, loss: 0.010318230837583542\n",
            "step: 290, loss: 0.19981050491333008\n",
            "step: 300, loss: 0.004178183153271675\n",
            "step: 310, loss: 0.04586195945739746\n",
            "step: 320, loss: 0.08877512067556381\n",
            "step: 330, loss: 0.18828926980495453\n",
            "step: 340, loss: 0.0021946895867586136\n",
            "step: 350, loss: 0.0017443675315007567\n",
            "step: 360, loss: 0.021190078929066658\n",
            "step: 370, loss: 0.024897852912545204\n",
            "step: 380, loss: 0.0009271949529647827\n",
            "step: 390, loss: 0.0003172223223373294\n",
            "step: 400, loss: 0.0359068401157856\n",
            "step: 410, loss: 0.006764169316738844\n",
            "step: 420, loss: 0.032952312380075455\n",
            "step: 430, loss: 0.1309361308813095\n",
            "step: 440, loss: 0.006775474641472101\n",
            "step: 450, loss: 0.0034837096463888884\n",
            "step: 460, loss: 0.003830639412626624\n",
            "step: 470, loss: 0.006725902669131756\n",
            "step: 480, loss: 0.006309840362519026\n",
            "step: 490, loss: 0.0011841715313494205\n",
            "step: 500, loss: 0.007080774288624525\n",
            "step: 510, loss: 0.05169985443353653\n",
            "step: 520, loss: 0.0006964142667129636\n",
            "step: 530, loss: 0.002298937411978841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9233644859813084, f1=0.9222170470423847, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043550390750169754\n",
            "step: 10, loss: 0.0011228818912059069\n",
            "step: 20, loss: 0.11631005257368088\n",
            "step: 30, loss: 0.0028982805088162422\n",
            "step: 40, loss: 0.012512968853116035\n",
            "step: 50, loss: 0.008690607734024525\n",
            "step: 60, loss: 0.0003205888206139207\n",
            "step: 70, loss: 0.0032731303945183754\n",
            "step: 80, loss: 0.0026876763440668583\n",
            "step: 90, loss: 0.001203829189762473\n",
            "step: 100, loss: 0.03198922798037529\n",
            "step: 110, loss: 0.0027945549227297306\n",
            "step: 120, loss: 0.0018240209901705384\n",
            "step: 130, loss: 0.000325516244629398\n",
            "step: 140, loss: 0.026325562968850136\n",
            "step: 150, loss: 0.01186356507241726\n",
            "step: 160, loss: 0.00112153182271868\n",
            "step: 170, loss: 0.0009730765596032143\n",
            "step: 180, loss: 0.0009958522859960794\n",
            "step: 190, loss: 0.0008913589990697801\n",
            "step: 200, loss: 0.0018787285080179572\n",
            "step: 210, loss: 0.004296950995922089\n",
            "step: 220, loss: 0.005003493744879961\n",
            "step: 230, loss: 0.001644596690312028\n",
            "step: 240, loss: 0.004812891129404306\n",
            "step: 250, loss: 0.0005202738102525473\n",
            "step: 260, loss: 0.0013772986130788922\n",
            "step: 270, loss: 0.06291315704584122\n",
            "step: 280, loss: 0.007486633490771055\n",
            "step: 290, loss: 0.0001378756860503927\n",
            "step: 300, loss: 0.0022486792877316475\n",
            "step: 310, loss: 0.004352970514446497\n",
            "step: 320, loss: 0.003070610575377941\n",
            "step: 330, loss: 0.002423195168375969\n",
            "step: 340, loss: 0.11394134908914566\n",
            "step: 350, loss: 0.0005456984508782625\n",
            "step: 360, loss: 0.0228460393846035\n",
            "step: 370, loss: 0.09782504290342331\n",
            "step: 380, loss: 0.0003547055821400136\n",
            "step: 390, loss: 0.02618803083896637\n",
            "step: 400, loss: 0.1689712405204773\n",
            "step: 410, loss: 0.0012861844152212143\n",
            "step: 420, loss: 0.0096141891553998\n",
            "step: 430, loss: 0.003378876717761159\n",
            "step: 440, loss: 0.0005865413113497198\n",
            "step: 450, loss: 0.007902524434030056\n",
            "step: 460, loss: 0.00018095631094183773\n",
            "step: 470, loss: 0.015973227098584175\n",
            "step: 480, loss: 0.0005924727884121239\n",
            "step: 490, loss: 0.011510507203638554\n",
            "step: 500, loss: 0.0004400070174597204\n",
            "step: 510, loss: 0.0055885459296405315\n",
            "step: 520, loss: 0.027672715485095978\n",
            "step: 530, loss: 0.009249089285731316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9314179796107508, f1=0.9303184125519152, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006428018677979708\n",
            "step: 10, loss: 0.0017036960925906897\n",
            "step: 20, loss: 0.00035262989695183933\n",
            "step: 30, loss: 0.045962147414684296\n",
            "step: 40, loss: 0.16558726131916046\n",
            "step: 50, loss: 0.01675632782280445\n",
            "step: 60, loss: 0.007743889931589365\n",
            "step: 70, loss: 0.0005883038393221796\n",
            "step: 80, loss: 0.0017266873037442565\n",
            "step: 90, loss: 0.0005696297739632428\n",
            "step: 100, loss: 0.0007586744613945484\n",
            "step: 110, loss: 0.0016795392148196697\n",
            "step: 120, loss: 0.0021792433690279722\n",
            "step: 130, loss: 0.005687952972948551\n",
            "step: 140, loss: 0.002030841074883938\n",
            "step: 150, loss: 0.006925404537469149\n",
            "step: 160, loss: 0.0017430073348805308\n",
            "step: 170, loss: 0.016240235418081284\n",
            "step: 180, loss: 0.002093693008646369\n",
            "step: 190, loss: 0.0016912742285057902\n",
            "step: 200, loss: 0.0004905050154775381\n",
            "step: 210, loss: 0.0023753882851451635\n",
            "step: 220, loss: 0.0008328315452672541\n",
            "step: 230, loss: 0.03464220091700554\n",
            "step: 240, loss: 0.0005091334460303187\n",
            "step: 250, loss: 0.00030671738204546273\n",
            "step: 260, loss: 0.00013957842020317912\n",
            "step: 270, loss: 0.002002979861572385\n",
            "step: 280, loss: 0.001779036014340818\n",
            "step: 290, loss: 0.0006181359058246017\n",
            "step: 300, loss: 0.00474225590005517\n",
            "step: 310, loss: 0.00045756157487630844\n",
            "step: 320, loss: 0.0003393776423763484\n",
            "step: 330, loss: 0.0016053059371188283\n",
            "step: 340, loss: 0.033139102160930634\n",
            "step: 350, loss: 0.00046437772107310593\n",
            "step: 360, loss: 0.0012241447111591697\n",
            "step: 370, loss: 0.0016049838159233332\n",
            "step: 380, loss: 0.0014304801588878036\n",
            "step: 390, loss: 0.0011242193868383765\n",
            "step: 400, loss: 0.0012514443369582295\n",
            "step: 410, loss: 0.00887310691177845\n",
            "step: 420, loss: 0.00045626694918610156\n",
            "step: 430, loss: 8.366588008357212e-05\n",
            "step: 440, loss: 0.0007609363528899848\n",
            "step: 450, loss: 0.003916352055966854\n",
            "step: 460, loss: 0.0007729401113465428\n",
            "step: 470, loss: 0.0056147328577935696\n",
            "step: 480, loss: 0.048433106392621994\n",
            "step: 490, loss: 0.02228441834449768\n",
            "step: 500, loss: 0.002294318052008748\n",
            "step: 510, loss: 0.020011894404888153\n",
            "step: 520, loss: 0.013712609186768532\n",
            "step: 530, loss: 0.03646249696612358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9309534992954438, f1=0.9283054003724395, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006287486292421818\n",
            "step: 10, loss: 0.022284941747784615\n",
            "step: 20, loss: 0.00017863484390545636\n",
            "step: 30, loss: 0.01746944896876812\n",
            "step: 40, loss: 0.0003265242849010974\n",
            "step: 50, loss: 0.009374422952532768\n",
            "step: 60, loss: 0.0044149667955935\n",
            "step: 70, loss: 0.0002858213265426457\n",
            "step: 80, loss: 0.004177585709840059\n",
            "step: 90, loss: 0.0015793804777786136\n",
            "step: 100, loss: 0.0006096781580708921\n",
            "step: 110, loss: 0.0014509472530335188\n",
            "step: 120, loss: 0.001545475795865059\n",
            "step: 130, loss: 0.00028771947836503386\n",
            "step: 140, loss: 0.0018895177636295557\n",
            "step: 150, loss: 8.738039468880743e-05\n",
            "step: 160, loss: 0.002819651272147894\n",
            "step: 170, loss: 0.1573459953069687\n",
            "step: 180, loss: 0.0015921695157885551\n",
            "step: 190, loss: 0.0014021472306922078\n",
            "step: 200, loss: 0.0024819679092615843\n",
            "step: 210, loss: 0.010667751543223858\n",
            "step: 220, loss: 0.02137145958840847\n",
            "step: 230, loss: 0.00024570507230237126\n",
            "step: 240, loss: 0.002168073086068034\n",
            "step: 250, loss: 0.007616675458848476\n",
            "step: 260, loss: 0.00035161798587068915\n",
            "step: 270, loss: 0.06416121125221252\n",
            "step: 280, loss: 0.00951263215392828\n",
            "step: 290, loss: 0.0004756729758810252\n",
            "step: 300, loss: 0.0006635561003349721\n",
            "step: 310, loss: 0.00012914426042698324\n",
            "step: 320, loss: 0.002603285713121295\n",
            "step: 330, loss: 0.00208613695576787\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 340, loss: 7.458896288881078e-05\n",
            "step: 350, loss: 0.03453119099140167\n",
            "step: 360, loss: 0.001095122192054987\n",
            "step: 370, loss: 0.0001761282910592854\n",
            "step: 380, loss: 0.00027704835520125926\n",
            "step: 390, loss: 0.1439063847064972\n",
            "step: 400, loss: 0.0007303737802430987\n",
            "step: 410, loss: 0.001799126504920423\n",
            "step: 420, loss: 0.0018453693483024836\n",
            "step: 430, loss: 0.016751756891608238\n",
            "step: 440, loss: 0.00018392868514638394\n",
            "step: 450, loss: 0.0001745184708852321\n",
            "step: 460, loss: 0.0014098670799285173\n",
            "step: 470, loss: 0.0009314129711128771\n",
            "step: 480, loss: 0.006192260887473822\n",
            "step: 490, loss: 0.0013981384690850973\n",
            "step: 500, loss: 0.003034073393791914\n",
            "step: 510, loss: 0.0034522339701652527\n",
            "step: 520, loss: 0.0035228286869823933\n",
            "step: 530, loss: 0.0002499047259334475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9337042188224385, f1=0.9278445883441258, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017573704244568944\n",
            "step: 10, loss: 0.009481282904744148\n",
            "step: 20, loss: 0.013248326256871223\n",
            "step: 30, loss: 0.00044694339158013463\n",
            "step: 40, loss: 0.00043145116069354117\n",
            "step: 50, loss: 0.0005805812543258071\n",
            "step: 60, loss: 0.0001743226166581735\n",
            "step: 70, loss: 0.0066195297986269\n",
            "step: 80, loss: 0.003220738610252738\n",
            "step: 90, loss: 0.0004192916676402092\n",
            "step: 100, loss: 0.012838626280426979\n",
            "step: 110, loss: 0.0007751917582936585\n",
            "step: 120, loss: 0.010284420102834702\n",
            "step: 130, loss: 0.006471442058682442\n",
            "step: 140, loss: 0.1012495905160904\n",
            "step: 150, loss: 0.00025465694488957524\n",
            "step: 160, loss: 0.001144660054706037\n",
            "step: 170, loss: 0.012486564926803112\n",
            "step: 180, loss: 0.003937527537345886\n",
            "step: 190, loss: 0.04122256487607956\n",
            "step: 200, loss: 0.000624787644483149\n",
            "step: 210, loss: 0.0012394190998747945\n",
            "step: 220, loss: 0.005510449409484863\n",
            "step: 230, loss: 0.0005243556224741042\n",
            "step: 240, loss: 0.0013435790315270424\n",
            "step: 250, loss: 0.0005232792464084923\n",
            "step: 260, loss: 0.0034960173070430756\n",
            "step: 270, loss: 0.0021423869766294956\n",
            "step: 280, loss: 0.0027560784947127104\n",
            "step: 290, loss: 0.021110357716679573\n",
            "step: 300, loss: 8.954948862083256e-05\n",
            "step: 310, loss: 0.005259099882096052\n",
            "step: 320, loss: 9.64014689088799e-05\n",
            "step: 330, loss: 0.00028351915534585714\n",
            "step: 340, loss: 0.01096726581454277\n",
            "step: 350, loss: 0.10027226060628891\n",
            "step: 360, loss: 0.00029067715513519943\n",
            "step: 370, loss: 0.0007017283933237195\n",
            "step: 380, loss: 0.0005643905606120825\n",
            "step: 390, loss: 0.0015063855098560452\n",
            "step: 400, loss: 0.00250408542342484\n",
            "step: 410, loss: 0.00021554350678343326\n",
            "step: 420, loss: 0.00021258114429656416\n",
            "step: 430, loss: 0.00022444178466685116\n",
            "step: 440, loss: 0.0010923697846010327\n",
            "step: 450, loss: 0.00014773134898860008\n",
            "step: 460, loss: 0.00028959388146176934\n",
            "step: 470, loss: 4.434067159309052e-05\n",
            "step: 480, loss: 0.00010122691310243681\n",
            "step: 490, loss: 0.025851821526885033\n",
            "step: 500, loss: 0.002188492799177766\n",
            "step: 510, loss: 0.07345735281705856\n",
            "step: 520, loss: 0.0007180970278568566\n",
            "step: 530, loss: 0.0025032127741724253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9324515824279641, f1=0.9255419415645617, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009919940494000912\n",
            "step: 10, loss: 0.00014846927660983056\n",
            "step: 20, loss: 9.890851652016863e-05\n",
            "step: 30, loss: 0.0041439286433160305\n",
            "step: 40, loss: 9.627994586480781e-05\n",
            "step: 50, loss: 0.00012456455442588776\n",
            "step: 60, loss: 0.0004332124663051218\n",
            "step: 70, loss: 0.0003782058774959296\n",
            "step: 80, loss: 0.00011916742369066924\n",
            "step: 90, loss: 6.115844735177234e-05\n",
            "step: 100, loss: 0.0009292068425565958\n",
            "step: 110, loss: 0.00024488335475325584\n",
            "step: 120, loss: 7.39885144867003e-05\n",
            "step: 130, loss: 0.00030081780278123915\n",
            "step: 140, loss: 0.0035655044484883547\n",
            "step: 150, loss: 0.026545992121100426\n",
            "step: 160, loss: 0.04385937750339508\n",
            "step: 170, loss: 0.0001267561747226864\n",
            "step: 180, loss: 0.0005483294371515512\n",
            "step: 190, loss: 0.0016057504108175635\n",
            "step: 200, loss: 0.0022350852377712727\n",
            "step: 210, loss: 0.004507541656494141\n",
            "step: 220, loss: 0.013093451969325542\n",
            "step: 230, loss: 0.0007459177286364138\n",
            "step: 240, loss: 0.001497928868047893\n",
            "step: 250, loss: 7.888372056186199e-05\n",
            "step: 260, loss: 0.0018091417150571942\n",
            "step: 270, loss: 0.011634387075901031\n",
            "step: 280, loss: 0.0002541830763220787\n",
            "step: 290, loss: 0.0015062065795063972\n",
            "step: 300, loss: 3.515453863656148e-05\n",
            "step: 310, loss: 0.0025460466276854277\n",
            "step: 320, loss: 0.0075770178809762\n",
            "step: 330, loss: 0.000369425630196929\n",
            "step: 340, loss: 0.0017335321754217148\n",
            "step: 350, loss: 0.0003181329811923206\n",
            "step: 360, loss: 0.00022517589968629181\n",
            "step: 370, loss: 0.003213524119928479\n",
            "step: 380, loss: 0.005196982063353062\n",
            "step: 390, loss: 0.0007015284500084817\n",
            "step: 400, loss: 0.0008512192871421576\n",
            "step: 410, loss: 0.0015464364551007748\n",
            "step: 420, loss: 0.0006984236533753574\n",
            "step: 430, loss: 0.0001343133335467428\n",
            "step: 440, loss: 0.0002944017469417304\n",
            "step: 450, loss: 6.513280823128298e-05\n",
            "step: 460, loss: 0.0004636655794456601\n",
            "step: 470, loss: 0.00020512212358880788\n",
            "step: 480, loss: 0.009149348363280296\n",
            "step: 490, loss: 9.555427823215723e-05\n",
            "step: 500, loss: 0.011778326705098152\n",
            "step: 510, loss: 0.005636685527861118\n",
            "step: 520, loss: 0.0004733378009404987\n",
            "step: 530, loss: 0.0003338250098749995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9284043051006083, f1=0.9245810055865922, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006342533743008971\n",
            "step: 10, loss: 0.0078055644407868385\n",
            "step: 20, loss: 0.0005781407235190272\n",
            "step: 30, loss: 0.00038076608325354755\n",
            "step: 40, loss: 0.02490786463022232\n",
            "step: 50, loss: 0.0009741586982272565\n",
            "step: 60, loss: 0.020133057609200478\n",
            "step: 70, loss: 0.001086935168132186\n",
            "step: 80, loss: 0.000649516296107322\n",
            "step: 90, loss: 9.484904148848727e-05\n",
            "step: 100, loss: 0.006409071385860443\n",
            "step: 110, loss: 0.0025458454620093107\n",
            "step: 120, loss: 5.5212705774465576e-05\n",
            "step: 130, loss: 5.3545369155472144e-05\n",
            "step: 140, loss: 0.00043154251761734486\n",
            "step: 150, loss: 0.0006277438369579613\n",
            "step: 160, loss: 0.00037447086651809514\n",
            "step: 170, loss: 0.00042942914296872914\n",
            "step: 180, loss: 0.0003211272123735398\n",
            "step: 190, loss: 0.002582363085821271\n",
            "step: 200, loss: 0.00443987175822258\n",
            "step: 210, loss: 0.007273130584508181\n",
            "step: 220, loss: 0.0007280074059963226\n",
            "step: 230, loss: 0.00018236774485558271\n",
            "step: 240, loss: 0.000370649213436991\n",
            "step: 250, loss: 3.476162601145916e-05\n",
            "step: 260, loss: 5.055178189650178e-05\n",
            "step: 270, loss: 0.0015917748678475618\n",
            "step: 280, loss: 0.002789123449474573\n",
            "step: 290, loss: 0.0010150392772629857\n",
            "step: 300, loss: 4.595027712639421e-05\n",
            "step: 310, loss: 0.00023889644944574684\n",
            "step: 320, loss: 2.380047044425737e-05\n",
            "step: 330, loss: 4.179299503448419e-05\n",
            "step: 340, loss: 7.231304334709421e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 350, loss: 0.0017985226586461067\n",
            "step: 360, loss: 0.00021044487948529422\n",
            "step: 370, loss: 0.00030319119105115533\n",
            "step: 380, loss: 0.00018445003661327064\n",
            "step: 390, loss: 2.4787568690953776e-05\n",
            "step: 400, loss: 0.00012688741844613105\n",
            "step: 410, loss: 0.002755097346380353\n",
            "step: 420, loss: 7.081671355990693e-05\n",
            "step: 430, loss: 1.7907246729009785e-05\n",
            "step: 440, loss: 0.0006318610976450145\n",
            "step: 450, loss: 6.906757334945723e-05\n",
            "step: 460, loss: 0.008290276862680912\n",
            "step: 470, loss: 0.0002813223691191524\n",
            "step: 480, loss: 0.0007952692685648799\n",
            "step: 490, loss: 0.0007242323481477797\n",
            "step: 500, loss: 0.00013028782268520445\n",
            "step: 510, loss: 3.222544546588324e-05\n",
            "step: 520, loss: 2.227643744845409e-05\n",
            "step: 530, loss: 0.004470885265618563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9322671683913453, f1=0.9284386617100371, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.208261766005307e-05\n",
            "step: 10, loss: 2.2932143110665493e-05\n",
            "step: 20, loss: 6.314588245004416e-05\n",
            "step: 30, loss: 0.005377018358558416\n",
            "step: 40, loss: 0.0002304660010850057\n",
            "step: 50, loss: 0.10306071490049362\n",
            "step: 60, loss: 0.0024136349093168974\n",
            "step: 70, loss: 0.0002352394221816212\n",
            "step: 80, loss: 0.00012763863196596503\n",
            "step: 90, loss: 2.688017593754921e-05\n",
            "step: 100, loss: 0.00014588648627977818\n",
            "step: 110, loss: 0.0007943774689920247\n",
            "step: 120, loss: 0.00010119465150637552\n",
            "step: 130, loss: 0.00035081591340713203\n",
            "step: 140, loss: 2.6799074476002716e-05\n",
            "step: 150, loss: 3.7293302739271894e-05\n",
            "step: 160, loss: 3.2387557439506054e-05\n",
            "step: 170, loss: 0.0006306211580522358\n",
            "step: 180, loss: 7.54933716962114e-05\n",
            "step: 190, loss: 0.0006652857409790158\n",
            "step: 200, loss: 0.00023623008746653795\n",
            "step: 210, loss: 0.00039329382707364857\n",
            "step: 220, loss: 7.785351772326976e-05\n",
            "step: 230, loss: 3.982018097303808e-05\n",
            "step: 240, loss: 0.0006356038502417505\n",
            "step: 250, loss: 9.886796033242717e-05\n",
            "step: 260, loss: 0.00012875613174401224\n",
            "step: 270, loss: 4.9462727474747226e-05\n",
            "step: 280, loss: 0.00032613053917884827\n",
            "step: 290, loss: 0.00016292027430608869\n",
            "step: 300, loss: 0.15391884744167328\n",
            "step: 310, loss: 0.0005761931533925235\n",
            "step: 320, loss: 0.0002674439747352153\n",
            "step: 330, loss: 0.00044514398905448616\n",
            "step: 340, loss: 0.003122118767350912\n",
            "step: 350, loss: 0.0002823449904099107\n",
            "step: 360, loss: 0.0007229824550449848\n",
            "step: 370, loss: 0.003027458442375064\n",
            "step: 380, loss: 0.00030347079155035317\n",
            "step: 390, loss: 0.0016159974038600922\n",
            "step: 400, loss: 0.0006729810265824199\n",
            "step: 410, loss: 0.0037917979061603546\n",
            "step: 420, loss: 0.006473610643297434\n",
            "step: 430, loss: 0.0037030442617833614\n",
            "step: 440, loss: 0.0007074567256495357\n",
            "step: 450, loss: 0.004100136924535036\n",
            "step: 460, loss: 0.00015446910401806235\n",
            "step: 470, loss: 0.00014502473641186953\n",
            "step: 480, loss: 0.009742665104568005\n",
            "step: 490, loss: 0.041484285145998\n",
            "step: 500, loss: 2.8106111130909994e-05\n",
            "step: 510, loss: 0.17260821163654327\n",
            "step: 520, loss: 5.0213020585943013e-05\n",
            "step: 530, loss: 2.0745384972542524e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9325210871602625, f1=0.9307298930729893, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032883891835808754\n",
            "step: 10, loss: 1.7974294678424485e-05\n",
            "step: 20, loss: 5.9062029322376475e-05\n",
            "step: 30, loss: 0.003215006086975336\n",
            "step: 40, loss: 4.874963269685395e-05\n",
            "step: 50, loss: 0.00014905465650372207\n",
            "step: 60, loss: 6.011091682012193e-05\n",
            "step: 70, loss: 5.857169162482023e-05\n",
            "step: 80, loss: 3.5291872336529195e-05\n",
            "step: 90, loss: 0.008073651231825352\n",
            "step: 100, loss: 0.0008895514765754342\n",
            "step: 110, loss: 0.0004022720968350768\n",
            "step: 120, loss: 6.966797809582204e-05\n",
            "step: 130, loss: 4.066131077706814e-05\n",
            "step: 140, loss: 0.0004455853486433625\n",
            "step: 150, loss: 0.0002519389963708818\n",
            "step: 160, loss: 2.717023562581744e-05\n",
            "step: 170, loss: 0.00010492293222341686\n",
            "step: 180, loss: 6.648853741353378e-05\n",
            "step: 190, loss: 0.00013978377683088183\n",
            "step: 200, loss: 8.338138286489993e-05\n",
            "step: 210, loss: 0.00042107916669920087\n",
            "step: 220, loss: 2.1788806407130323e-05\n",
            "step: 230, loss: 0.0009437822154723108\n",
            "step: 240, loss: 0.0014084193389862776\n",
            "step: 250, loss: 4.705788160208613e-05\n",
            "step: 260, loss: 0.009297044947743416\n",
            "step: 270, loss: 3.383804869372398e-05\n",
            "step: 280, loss: 0.0005144180031493306\n",
            "step: 290, loss: 0.0002941979328170419\n",
            "step: 300, loss: 8.141156286001205e-05\n",
            "step: 310, loss: 0.0010307912016287446\n",
            "step: 320, loss: 0.0019692375790327787\n",
            "step: 330, loss: 0.029122615233063698\n",
            "step: 340, loss: 0.0010816220892593265\n",
            "step: 350, loss: 6.48352870484814e-05\n",
            "step: 360, loss: 0.0006878736312501132\n",
            "step: 370, loss: 0.00010189467866439372\n",
            "step: 380, loss: 3.7116973544470966e-05\n",
            "step: 390, loss: 0.0004993322654627264\n",
            "step: 400, loss: 0.00013613600458484143\n",
            "step: 410, loss: 0.0009318387019447982\n",
            "step: 420, loss: 3.90896930184681e-05\n",
            "step: 430, loss: 0.0001875103043857962\n",
            "step: 440, loss: 0.0043356819078326225\n",
            "step: 450, loss: 0.0006321895634755492\n",
            "step: 460, loss: 0.001306155463680625\n",
            "step: 470, loss: 0.0003849720233120024\n",
            "step: 480, loss: 8.811564475763589e-05\n",
            "step: 490, loss: 0.0003565190127119422\n",
            "step: 500, loss: 0.00017976530943997204\n",
            "step: 510, loss: 0.0005412086029537022\n",
            "step: 520, loss: 0.0023307872470468283\n",
            "step: 530, loss: 0.00010679599654395133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9321642824180896, f1=0.9321642824180896, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019874349236488342\n",
            "step: 10, loss: 3.432609446463175e-05\n",
            "step: 20, loss: 0.00026155277737416327\n",
            "step: 30, loss: 0.00011183810420334339\n",
            "step: 40, loss: 0.006163571495562792\n",
            "step: 50, loss: 0.003831228706985712\n",
            "step: 60, loss: 1.2226294529682491e-05\n",
            "step: 70, loss: 0.0005673042614944279\n",
            "step: 80, loss: 2.1788251615362242e-05\n",
            "step: 90, loss: 0.00010023025242844597\n",
            "step: 100, loss: 0.00044856162276118994\n",
            "step: 110, loss: 0.0001318799622822553\n",
            "step: 120, loss: 0.0003291511966381222\n",
            "step: 130, loss: 0.010910570621490479\n",
            "step: 140, loss: 0.0002275221486343071\n",
            "step: 150, loss: 0.0002035423822235316\n",
            "step: 160, loss: 0.0002912736963480711\n",
            "step: 170, loss: 4.522491872194223e-05\n",
            "step: 180, loss: 3.3131593227153644e-05\n",
            "step: 190, loss: 4.165831342106685e-05\n",
            "step: 200, loss: 0.002237205859273672\n",
            "step: 210, loss: 2.6142508431803435e-05\n",
            "step: 220, loss: 0.0005520968115888536\n",
            "step: 230, loss: 0.003382098861038685\n",
            "step: 240, loss: 4.035193342133425e-05\n",
            "step: 250, loss: 0.00010394854325568303\n",
            "step: 260, loss: 0.0034223017282783985\n",
            "step: 270, loss: 0.00032291028765030205\n",
            "step: 280, loss: 2.9241868105600588e-05\n",
            "step: 290, loss: 0.0003777882957365364\n",
            "step: 300, loss: 0.004657924175262451\n",
            "step: 310, loss: 0.0003413849335629493\n",
            "step: 320, loss: 0.0005653548869304359\n",
            "step: 330, loss: 0.03386924788355827\n",
            "step: 340, loss: 9.02196261449717e-05\n",
            "step: 350, loss: 0.001506954780779779\n",
            "step: 360, loss: 0.005661098752170801\n",
            "step: 370, loss: 0.00013036544260103256\n",
            "step: 380, loss: 0.0003216266050003469\n",
            "step: 390, loss: 0.019820561632514\n",
            "step: 400, loss: 0.0025441194884479046\n",
            "step: 410, loss: 0.00028046660008840263\n",
            "step: 420, loss: 0.017232252284884453\n",
            "step: 430, loss: 0.059314992278814316\n",
            "step: 440, loss: 0.1599215865135193\n",
            "step: 450, loss: 0.0006028801435604692\n",
            "step: 460, loss: 0.00014642317546531558\n",
            "step: 470, loss: 0.000897340418305248\n",
            "step: 480, loss: 0.004113788716495037\n",
            "step: 490, loss: 6.718761869706213e-05\n",
            "step: 500, loss: 0.00040303950663655996\n",
            "step: 510, loss: 0.001590378349646926\n",
            "step: 520, loss: 0.0005415552295744419\n",
            "step: 530, loss: 0.002628063317388296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9294287085933749, f1=0.9243937232524965, best_f1=0.9320388349514563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.3403113775420934e-05\n",
            "step: 10, loss: 0.006396114826202393\n",
            "step: 20, loss: 0.0008520123083144426\n",
            "step: 30, loss: 8.180148870451376e-05\n",
            "step: 40, loss: 0.0032774663995951414\n",
            "step: 50, loss: 0.00632402254268527\n",
            "step: 60, loss: 4.067784539074637e-05\n",
            "step: 70, loss: 0.0006301904213614762\n",
            "step: 80, loss: 0.00042377557838335633\n",
            "step: 90, loss: 0.00015042908489704132\n",
            "step: 100, loss: 8.846016862662509e-05\n",
            "step: 110, loss: 0.00021637165627907962\n",
            "step: 120, loss: 0.0012302040122449398\n",
            "step: 130, loss: 0.06937138736248016\n",
            "step: 140, loss: 5.214561315369792e-05\n",
            "step: 150, loss: 0.017290500923991203\n",
            "step: 160, loss: 9.694366599433124e-05\n",
            "step: 170, loss: 8.130733476718888e-05\n",
            "step: 180, loss: 3.340329567436129e-05\n",
            "step: 190, loss: 0.0005319345509633422\n",
            "step: 200, loss: 0.0002581071457825601\n",
            "step: 210, loss: 0.0002659937890712172\n",
            "step: 220, loss: 4.821053153136745e-05\n",
            "step: 230, loss: 0.0007281076395884156\n",
            "step: 240, loss: 7.217701931949705e-05\n",
            "step: 250, loss: 0.00010663629655027762\n",
            "step: 260, loss: 0.00011303027713438496\n",
            "step: 270, loss: 2.0272649635444395e-05\n",
            "step: 280, loss: 0.0002021140680881217\n",
            "step: 290, loss: 3.8019174098735675e-05\n",
            "step: 300, loss: 4.072340016136877e-05\n",
            "step: 310, loss: 0.00022253874340094626\n",
            "step: 320, loss: 0.0001415690203430131\n",
            "step: 330, loss: 0.00033415108919143677\n",
            "step: 340, loss: 0.0001441667991457507\n",
            "step: 350, loss: 0.0032087310682982206\n",
            "step: 360, loss: 0.004294856917113066\n",
            "step: 370, loss: 8.021417306736112e-05\n",
            "step: 380, loss: 6.675915938103572e-05\n",
            "step: 390, loss: 0.013451694510877132\n",
            "step: 400, loss: 5.010953827877529e-05\n",
            "step: 410, loss: 0.0020385445095598698\n",
            "step: 420, loss: 0.0003100468311458826\n",
            "step: 430, loss: 0.0011648408835753798\n",
            "step: 440, loss: 0.0005582162993960083\n",
            "step: 450, loss: 9.734220657264814e-05\n",
            "step: 460, loss: 2.604227302072104e-05\n",
            "step: 470, loss: 0.00019324007735121995\n",
            "step: 480, loss: 2.4090219085337594e-05\n",
            "step: 490, loss: 0.0236648116260767\n",
            "step: 500, loss: 0.00028630258748307824\n",
            "step: 510, loss: 6.424028833862394e-05\n",
            "step: 520, loss: 2.914550350396894e-05\n",
            "step: 530, loss: 0.00014154837117530406\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9291784702549575, f1=0.9312119794103885, best_f1=0.9320388349514563\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:21, 266.15it/s]\n",
            "load_f1 = 0.93666204345816\n",
            "real_f1 = 0.9342592592592593\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 266.48it/s]\n"
          ]
        }
      ]
    }
  ]
}