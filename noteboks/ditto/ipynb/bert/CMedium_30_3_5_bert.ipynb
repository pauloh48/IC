{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PSgSb9vUtCyX",
        "outputId": "4ba2acb4-4970-47b6-99c5-482e0b7966f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 23.28 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "Checking out files: 100% (128/128), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 2.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 76.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 8.8 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 21.52 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.0 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 50.5 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 73.4 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=aa652793ccf87d8489567a17a7f4c8d651e91d7c941257851a53d9a749efee6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1e93cc126502dbc39af619df66254087fce2af90a7c9f03c6e184aee70f0a7a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hfO6D_uLby",
        "outputId": "4bea8a5f-2786-41a0-d015-a253408069dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10117, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 10117 (delta 119), reused 152 (delta 62), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10117/10117), 14.97 MiB | 22.02 MiB/s, done.\n",
            "Resolving deltas: 100% (6927/6927), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-pjuwhrsm\n",
            "Created temporary directory: /tmp/pip-req-tracker-pbi1pejy\n",
            "Initialized build tracking at /tmp/pip-req-tracker-pbi1pejy\n",
            "Created build tracker: /tmp/pip-req-tracker-pbi1pejy\n",
            "Entered build tracker: /tmp/pip-req-tracker-pbi1pejy\n",
            "Created temporary directory: /tmp/pip-install-62g4y6ax\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-31e_efpy\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-pbi1pejy'\n",
            "    Running setup.py (path:/tmp/pip-req-build-31e_efpy/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-8t246188\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-8t246188/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-8t246188/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-8t246188/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-8t246188/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8t246188/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-8t246188/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-31e_efpy has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-pbi1pejy'\n",
            "Created temporary directory: /tmp/pip-unpack-d8nknlve\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-sctiqzok\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-sctiqzok\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-31e_efpy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-31e_efpy/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-sctiqzok\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-sctiqzok/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298244 sha256=af39b63dc61d0d0f846a74e3b34fc96c6c6102322ce44c199db8d50e9d9edfd6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjuwhrsm/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-pbi1pejy'\n",
            "/content/ditto\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKSZwBG_uyzV",
        "outputId": "8284d372-108c-41d2-e395-768774134fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.49-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting botocore==1.27.49\n",
            "  Downloading botocore-1.27.49-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 42.7 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.49->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.49->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.49 botocore-1.27.49 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ],
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR84V9pFRkw8",
        "outputId": "9d3fbe7f-cf2b-40d1-8088-cb2fa7e98b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyGpUo9ifJM"
      },
      "source": [
        "## importa config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "4024e143-f61e-4efe-8acf-590c9969db8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 1054, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 1054 (delta 51), reused 46 (delta 21), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (1054/1054), 257.86 MiB | 27.98 MiB/s, done.\n",
            "Resolving deltas: 100% (635/635), done.\n",
            "Checking out files: 100% (1304/1304), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw03GW7dmkqy"
      },
      "source": [
        "## remove e move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQVym9vwmx-g",
        "outputId": "24e0a71e-9b0b-483f-a024-39af9e8204a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "outputs": [],
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/CMedium_30_3_5/configs.json /content/ditto/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm4nohJxf9bD"
      },
      "source": [
        "# DA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDvm9a1dIlo"
      },
      "source": [
        "## DA STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qxLFPNvcGgH",
        "outputId": "0ffab58e-64b4-4ec0-9780-7dd4495dbf9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 344kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 351kB/s]\n",
            "Downloading: 100% 440M/440M [00:11<00:00, 39.4MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5635988116264343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.34782608695652173, f1=0.30769230769230765, best_f1=0.30769230769230765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5061610341072083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.3529411764705882, f1=0.3157894736842105, best_f1=0.3157894736842105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5584540963172913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.39285714285714285, f1=0.4285714285714286, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28716859221458435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5128205128205129, f1=0.4827586206896552, best_f1=0.4827586206896552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24942944943904877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6250000000000001, f1=0.5517241379310344, best_f1=0.5517241379310344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29641950130462646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.689655172413793, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04376140609383583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.689655172413793, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01952725276350975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6666666666666666, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002095213858410716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7200000000000001, f1=0.5925925925925927, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04278089478611946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7407407407407408, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005829306319355965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7692307692307692, f1=0.6206896551724138, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008200390264391899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7692307692307692, f1=0.5925925925925927, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005688575562089682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7692307692307692, f1=0.6153846153846153, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005621185526251793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7692307692307692, f1=0.6153846153846153, best_f1=0.6206896551724138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004622721578925848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7692307692307692, f1=0.6153846153846153, best_f1=0.6206896551724138\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 65989.22it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7407407407407408\n",
            "real_f1 = 0.7586206896551724\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.52it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjO-q4GLeCE1"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ7mparQevgM",
        "outputId": "106476b3-5682-44e0-87b7-d5940d5796df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6160876750946045\n",
            "step: 10, loss: 0.6018023490905762\n",
            "step: 20, loss: 0.2681180238723755\n",
            "step: 30, loss: 0.18086719512939453\n",
            "step: 40, loss: 0.20037473738193512\n",
            "step: 50, loss: 0.022347619757056236\n",
            "step: 60, loss: 0.1854766309261322\n",
            "step: 70, loss: 0.03433620184659958\n",
            "step: 80, loss: 0.259749174118042\n",
            "step: 90, loss: 0.11945196241140366\n",
            "step: 100, loss: 0.0055139861069619656\n",
            "step: 110, loss: 0.158479243516922\n",
            "step: 120, loss: 0.007996385917067528\n",
            "step: 130, loss: 0.010712778195738792\n",
            "step: 140, loss: 0.00883543211966753\n",
            "step: 150, loss: 0.03889644891023636\n",
            "step: 160, loss: 0.006078546866774559\n",
            "step: 170, loss: 0.1026257649064064\n",
            "step: 180, loss: 0.006650817114859819\n",
            "step: 190, loss: 0.012892743572592735\n",
            "step: 200, loss: 0.024237196892499924\n",
            "step: 210, loss: 0.008738347329199314\n",
            "step: 220, loss: 0.008464175276458263\n",
            "step: 230, loss: 0.03514161705970764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9775784753363228, f1=0.9728506787330317, best_f1=0.9728506787330317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013350460678339005\n",
            "step: 10, loss: 0.01256899256259203\n",
            "step: 20, loss: 0.14440888166427612\n",
            "step: 30, loss: 0.192470520734787\n",
            "step: 40, loss: 0.04191848263144493\n",
            "step: 50, loss: 0.0142704788595438\n",
            "step: 60, loss: 0.003815451869741082\n",
            "step: 70, loss: 0.14706972241401672\n",
            "step: 80, loss: 0.0030917120166122913\n",
            "step: 90, loss: 0.029606731608510017\n",
            "step: 100, loss: 0.003855952061712742\n",
            "step: 110, loss: 0.03426450490951538\n",
            "step: 120, loss: 0.017274700105190277\n",
            "step: 130, loss: 0.003525871317833662\n",
            "step: 140, loss: 0.0011079009855166078\n",
            "step: 150, loss: 0.01482408121228218\n",
            "step: 160, loss: 0.08440714329481125\n",
            "step: 170, loss: 0.0029603014700114727\n",
            "step: 180, loss: 0.0030776916537433863\n",
            "step: 190, loss: 0.013415050692856312\n",
            "step: 200, loss: 0.03348895534873009\n",
            "step: 210, loss: 0.0012233792804181576\n",
            "step: 220, loss: 0.12650133669376373\n",
            "step: 230, loss: 0.11773496866226196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.984090909090909, f1=0.9784824462061155, best_f1=0.9784824462061155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034546179231256247\n",
            "step: 10, loss: 0.0019857995212078094\n",
            "step: 20, loss: 0.002006630878895521\n",
            "step: 30, loss: 0.009160848334431648\n",
            "step: 40, loss: 0.1411891132593155\n",
            "step: 50, loss: 0.030618462711572647\n",
            "step: 60, loss: 0.0033418352250009775\n",
            "step: 70, loss: 0.002989760134369135\n",
            "step: 80, loss: 0.0012401058338582516\n",
            "step: 90, loss: 0.21989597380161285\n",
            "step: 100, loss: 0.00045267361565493047\n",
            "step: 110, loss: 0.008927907794713974\n",
            "step: 120, loss: 0.015044140629470348\n",
            "step: 130, loss: 0.002370523987337947\n",
            "step: 140, loss: 0.0015473297098651528\n",
            "step: 150, loss: 0.0011234706034883857\n",
            "step: 160, loss: 0.01801779679954052\n",
            "step: 170, loss: 0.003389124060049653\n",
            "step: 180, loss: 0.003247613087296486\n",
            "step: 190, loss: 0.001137851388193667\n",
            "step: 200, loss: 0.04911498725414276\n",
            "step: 210, loss: 0.0031491145491600037\n",
            "step: 220, loss: 0.0011752460850402713\n",
            "step: 230, loss: 0.002412821166217327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9852774631936579, f1=0.9819004524886877, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006021800800226629\n",
            "step: 10, loss: 0.00032842805376276374\n",
            "step: 20, loss: 0.0006343288696371019\n",
            "step: 30, loss: 0.0006561180925928056\n",
            "step: 40, loss: 0.0010310182115063071\n",
            "step: 50, loss: 0.005246225744485855\n",
            "step: 60, loss: 0.0020305230282247066\n",
            "step: 70, loss: 0.0026736147701740265\n",
            "step: 80, loss: 0.002126127015799284\n",
            "step: 90, loss: 0.03878271207213402\n",
            "step: 100, loss: 0.002600582782179117\n",
            "step: 110, loss: 0.0015472408849745989\n",
            "step: 120, loss: 0.024392282590270042\n",
            "step: 130, loss: 0.013361495919525623\n",
            "step: 140, loss: 0.002332124160602689\n",
            "step: 150, loss: 0.11734918504953384\n",
            "step: 160, loss: 0.0012736502103507519\n",
            "step: 170, loss: 0.0066804904490709305\n",
            "step: 180, loss: 0.00036191430990584195\n",
            "step: 190, loss: 0.004251465201377869\n",
            "step: 200, loss: 0.0045812674798071384\n",
            "step: 210, loss: 0.01821957528591156\n",
            "step: 220, loss: 0.00025382378953509033\n",
            "step: 230, loss: 0.002504659118130803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.983050847457627, f1=0.9775280898876404, best_f1=0.9819004524886877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031720749102532864\n",
            "step: 10, loss: 0.059211622923612595\n",
            "step: 20, loss: 0.00046752826892770827\n",
            "step: 30, loss: 0.00026547384914010763\n",
            "step: 40, loss: 0.00023455399787053466\n",
            "step: 50, loss: 0.00023505845456384122\n",
            "step: 60, loss: 0.00017752127314452082\n",
            "step: 70, loss: 0.0002850932942237705\n",
            "step: 80, loss: 0.0003926422505173832\n",
            "step: 90, loss: 0.00018988968804478645\n",
            "step: 100, loss: 0.0020740048494189978\n",
            "step: 110, loss: 0.0017462230753153563\n",
            "step: 120, loss: 0.00045416128705255687\n",
            "step: 130, loss: 0.0014475167263299227\n",
            "step: 140, loss: 0.0010945504764094949\n",
            "step: 150, loss: 0.0009551157709211111\n",
            "step: 160, loss: 0.001842604367993772\n",
            "step: 170, loss: 0.011562490835785866\n",
            "step: 180, loss: 0.0039943503215909\n",
            "step: 190, loss: 0.0009702492388896644\n",
            "step: 200, loss: 0.0002972417860291898\n",
            "step: 210, loss: 0.00022842800535727292\n",
            "step: 220, loss: 0.00110739772208035\n",
            "step: 230, loss: 0.0003450707299634814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9863945578231292, f1=0.979591836734694, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027431712951511145\n",
            "step: 10, loss: 0.0018098083091899753\n",
            "step: 20, loss: 0.00181412766687572\n",
            "step: 30, loss: 0.00035263318568468094\n",
            "step: 40, loss: 0.00021490214567165822\n",
            "step: 50, loss: 0.00024577201111242175\n",
            "step: 60, loss: 0.0012251903535798192\n",
            "step: 70, loss: 0.0021738784853368998\n",
            "step: 80, loss: 0.002662754850462079\n",
            "step: 90, loss: 0.014161123894155025\n",
            "step: 100, loss: 0.026176149025559425\n",
            "step: 110, loss: 0.00024485355243086815\n",
            "step: 120, loss: 0.0005083243595436215\n",
            "step: 130, loss: 0.004236895591020584\n",
            "step: 140, loss: 0.0002534798695705831\n",
            "step: 150, loss: 0.009849369525909424\n",
            "step: 160, loss: 0.004498937167227268\n",
            "step: 170, loss: 0.006775137037038803\n",
            "step: 180, loss: 0.024405047297477722\n",
            "step: 190, loss: 0.020158017054200172\n",
            "step: 200, loss: 9.569044777890667e-05\n",
            "step: 210, loss: 0.00016190327005460858\n",
            "step: 220, loss: 0.07677457481622696\n",
            "step: 230, loss: 0.008544377982616425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9808342728297633, f1=0.9841628959276018, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002067157533019781\n",
            "step: 10, loss: 0.00022737204562872648\n",
            "step: 20, loss: 0.0012219792697578669\n",
            "step: 30, loss: 0.00042536744149401784\n",
            "step: 40, loss: 0.0003855264512822032\n",
            "step: 50, loss: 0.0003955896245315671\n",
            "step: 60, loss: 0.00020934153872076422\n",
            "step: 70, loss: 0.0005717657040804625\n",
            "step: 80, loss: 0.00010340262087993324\n",
            "step: 90, loss: 9.397843678016216e-05\n",
            "step: 100, loss: 0.00010760100849438459\n",
            "step: 110, loss: 0.0001267932530026883\n",
            "step: 120, loss: 0.0003733652411028743\n",
            "step: 130, loss: 0.0002303321671206504\n",
            "step: 140, loss: 0.0012814165093004704\n",
            "step: 150, loss: 0.008032613433897495\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.15215006470680237\n",
            "step: 170, loss: 0.008186562918126583\n",
            "step: 180, loss: 0.010567222721874714\n",
            "step: 190, loss: 0.0022281284909695387\n",
            "step: 200, loss: 0.07886621356010437\n",
            "step: 210, loss: 7.202782580861822e-05\n",
            "step: 220, loss: 0.0003333166823722422\n",
            "step: 230, loss: 0.002163076773285866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9841269841269841, f1=0.9785310734463276, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014635889965575188\n",
            "step: 10, loss: 0.00022088034893386066\n",
            "step: 20, loss: 0.00010885130177484825\n",
            "step: 30, loss: 0.0001436168677173555\n",
            "step: 40, loss: 0.00013158736692275852\n",
            "step: 50, loss: 0.0016504063969478011\n",
            "step: 60, loss: 6.149974069558084e-05\n",
            "step: 70, loss: 0.00013633383787237108\n",
            "step: 80, loss: 0.004310193005949259\n",
            "step: 90, loss: 6.583526555914432e-05\n",
            "step: 100, loss: 9.246812987839803e-05\n",
            "step: 110, loss: 0.0002981809084303677\n",
            "step: 120, loss: 0.0002575028338469565\n",
            "step: 130, loss: 0.0001780326128937304\n",
            "step: 140, loss: 9.68596141319722e-05\n",
            "step: 150, loss: 0.00024810974719002843\n",
            "step: 160, loss: 0.0001407427917001769\n",
            "step: 170, loss: 0.00014777047908864915\n",
            "step: 180, loss: 0.008218144066631794\n",
            "step: 190, loss: 0.0011599333956837654\n",
            "step: 200, loss: 0.0005101955612190068\n",
            "step: 210, loss: 0.00018974574049934745\n",
            "step: 220, loss: 0.00035344663774594665\n",
            "step: 230, loss: 0.00011413659376557916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9875141884222476, f1=0.9773242630385486, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.317881681956351e-05\n",
            "step: 10, loss: 0.001210426795296371\n",
            "step: 20, loss: 0.0002301610802533105\n",
            "step: 30, loss: 0.00025261801783926785\n",
            "step: 40, loss: 0.04126520827412605\n",
            "step: 50, loss: 7.118452049326152e-05\n",
            "step: 60, loss: 0.00013722272706218064\n",
            "step: 70, loss: 0.0004085826512891799\n",
            "step: 80, loss: 0.000113921363663394\n",
            "step: 90, loss: 0.00013491125719156116\n",
            "step: 100, loss: 0.0002860722888726741\n",
            "step: 110, loss: 0.00014891741739120334\n",
            "step: 120, loss: 0.0003106636868324131\n",
            "step: 130, loss: 7.169425953179598e-05\n",
            "step: 140, loss: 7.975712651386857e-05\n",
            "step: 150, loss: 0.0009823720902204514\n",
            "step: 160, loss: 0.00011240984167670831\n",
            "step: 170, loss: 0.00018419271509628743\n",
            "step: 180, loss: 0.003946591634303331\n",
            "step: 190, loss: 0.0031248906161636114\n",
            "step: 200, loss: 0.00016765334294177592\n",
            "step: 210, loss: 0.0010742044541984797\n",
            "step: 220, loss: 0.0005555255338549614\n",
            "step: 230, loss: 0.00017454901535529643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9796380090497738, f1=0.9774266365688488, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003403273585718125\n",
            "step: 10, loss: 0.0001025576566462405\n",
            "step: 20, loss: 5.247506487648934e-05\n",
            "step: 30, loss: 0.0001529433357063681\n",
            "step: 40, loss: 7.457844185410067e-05\n",
            "step: 50, loss: 0.000928123074118048\n",
            "step: 60, loss: 0.0007978428620845079\n",
            "step: 70, loss: 0.0006757571827620268\n",
            "step: 80, loss: 0.00016458617756143212\n",
            "step: 90, loss: 0.0005120630958117545\n",
            "step: 100, loss: 0.0007627706509083509\n",
            "step: 110, loss: 0.00031156474142335355\n",
            "step: 120, loss: 0.0012099641608074307\n",
            "step: 130, loss: 0.00014727719826623797\n",
            "step: 140, loss: 0.0003940555907320231\n",
            "step: 150, loss: 0.08449140191078186\n",
            "step: 160, loss: 7.379780436167493e-05\n",
            "step: 170, loss: 0.0002692803682293743\n",
            "step: 180, loss: 0.0005908907041884959\n",
            "step: 190, loss: 0.01817174442112446\n",
            "step: 200, loss: 8.263201743829995e-05\n",
            "step: 210, loss: 7.676012319279835e-05\n",
            "step: 220, loss: 0.00015503317990805954\n",
            "step: 230, loss: 0.00013328998466022313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9875141884222476, f1=0.9774266365688488, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.858639269717969e-05\n",
            "step: 10, loss: 0.001845458522439003\n",
            "step: 20, loss: 0.0025268886238336563\n",
            "step: 30, loss: 0.004521454684436321\n",
            "step: 40, loss: 0.0010890841949731112\n",
            "step: 50, loss: 0.0006638956838287413\n",
            "step: 60, loss: 9.751596371643245e-05\n",
            "step: 70, loss: 0.00046564877266064286\n",
            "step: 80, loss: 4.356162025942467e-05\n",
            "step: 90, loss: 0.00015100683958735317\n",
            "step: 100, loss: 6.712022877763957e-05\n",
            "step: 110, loss: 0.0001410567492712289\n",
            "step: 120, loss: 7.046532118692994e-05\n",
            "step: 130, loss: 0.0006261744420044124\n",
            "step: 140, loss: 6.798693357268348e-05\n",
            "step: 150, loss: 0.035053592175245285\n",
            "step: 160, loss: 6.114948337199166e-05\n",
            "step: 170, loss: 0.015075819566845894\n",
            "step: 180, loss: 4.2794425098691136e-05\n",
            "step: 190, loss: 4.172561602899805e-05\n",
            "step: 200, loss: 6.41128426650539e-05\n",
            "step: 210, loss: 9.949040395440534e-05\n",
            "step: 220, loss: 4.4519292714539915e-05\n",
            "step: 230, loss: 0.0001706875627860427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9853107344632768, f1=0.9841986455981941, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.081039489596151e-05\n",
            "step: 10, loss: 0.0002925916633103043\n",
            "step: 20, loss: 4.89172525703907e-05\n",
            "step: 30, loss: 0.00012835251982323825\n",
            "step: 40, loss: 0.0001252399815712124\n",
            "step: 50, loss: 0.00018556472787167877\n",
            "step: 60, loss: 0.00034460038295947015\n",
            "step: 70, loss: 9.98566611087881e-05\n",
            "step: 80, loss: 0.005743478424847126\n",
            "step: 90, loss: 2.6404373784316704e-05\n",
            "step: 100, loss: 0.00041355082066729665\n",
            "step: 110, loss: 5.85090383538045e-05\n",
            "step: 120, loss: 0.0003947024524677545\n",
            "step: 130, loss: 3.478218422969803e-05\n",
            "step: 140, loss: 3.733365883817896e-05\n",
            "step: 150, loss: 4.730048021883704e-05\n",
            "step: 160, loss: 3.7009172956459224e-05\n",
            "step: 170, loss: 9.293061884818599e-05\n",
            "step: 180, loss: 0.00019319847342558205\n",
            "step: 190, loss: 4.5019878598395735e-05\n",
            "step: 200, loss: 2.7134419724461623e-05\n",
            "step: 210, loss: 4.037998951389454e-05\n",
            "step: 220, loss: 3.424156966502778e-05\n",
            "step: 230, loss: 0.052252013236284256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9852104664391355, f1=0.9807037457434733, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016174526535905898\n",
            "step: 10, loss: 0.00019348332716617733\n",
            "step: 20, loss: 0.00035463995300233364\n",
            "step: 30, loss: 5.231301474850625e-05\n",
            "step: 40, loss: 0.0003660500515252352\n",
            "step: 50, loss: 4.4865642848890275e-05\n",
            "step: 60, loss: 3.404817107366398e-05\n",
            "step: 70, loss: 2.5089359041885473e-05\n",
            "step: 80, loss: 2.7741694793803617e-05\n",
            "step: 90, loss: 6.884805770823732e-05\n",
            "step: 100, loss: 2.8098986149416305e-05\n",
            "step: 110, loss: 0.024962354451417923\n",
            "step: 120, loss: 0.03659442812204361\n",
            "step: 130, loss: 5.7144261518260464e-05\n",
            "step: 140, loss: 2.7737927666748874e-05\n",
            "step: 150, loss: 2.9663857276318595e-05\n",
            "step: 160, loss: 5.152618541615084e-05\n",
            "step: 170, loss: 8.078755490714684e-05\n",
            "step: 180, loss: 7.237375393742695e-05\n",
            "step: 190, loss: 4.8345260438509285e-05\n",
            "step: 200, loss: 0.0010405739303678274\n",
            "step: 210, loss: 2.771107028820552e-05\n",
            "step: 220, loss: 2.657563891261816e-05\n",
            "step: 230, loss: 3.53317336703185e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9852440408626559, f1=0.9807909604519773, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.580085492809303e-05\n",
            "step: 10, loss: 0.0005257012671791017\n",
            "step: 20, loss: 3.893667962984182e-05\n",
            "step: 30, loss: 3.542244667187333e-05\n",
            "step: 40, loss: 4.293794700060971e-05\n",
            "step: 50, loss: 4.139192242291756e-05\n",
            "step: 60, loss: 3.40472579409834e-05\n",
            "step: 70, loss: 4.2949912312906235e-05\n",
            "step: 80, loss: 2.6903468096861616e-05\n",
            "step: 90, loss: 3.409264172660187e-05\n",
            "step: 100, loss: 3.6356665077619255e-05\n",
            "step: 110, loss: 2.9517388611566275e-05\n",
            "step: 120, loss: 1.8123322661267594e-05\n",
            "step: 130, loss: 2.9134589567547664e-05\n",
            "step: 140, loss: 3.6152279790258035e-05\n",
            "step: 150, loss: 5.972018334432505e-05\n",
            "step: 160, loss: 0.0071009136736392975\n",
            "step: 170, loss: 2.383026367169805e-05\n",
            "step: 180, loss: 4.121972597204149e-05\n",
            "step: 190, loss: 4.500124850892462e-05\n",
            "step: 200, loss: 3.674238905659877e-05\n",
            "step: 210, loss: 3.9631922845728695e-05\n",
            "step: 220, loss: 4.2728577682282776e-05\n",
            "step: 230, loss: 8.827705460134894e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9841269841269841, f1=0.9796839729119639, best_f1=0.9773242630385486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.48508840741124e-05\n",
            "step: 10, loss: 2.264534487039782e-05\n",
            "step: 20, loss: 7.261910650413483e-05\n",
            "step: 30, loss: 3.9247544918907806e-05\n",
            "step: 40, loss: 5.6631128245498985e-05\n",
            "step: 50, loss: 0.0034759780392050743\n",
            "step: 60, loss: 5.1198814617237076e-05\n",
            "step: 70, loss: 3.550750625436194e-05\n",
            "step: 80, loss: 5.660008901031688e-05\n",
            "step: 90, loss: 8.094956137938425e-05\n",
            "step: 100, loss: 3.5995402868138626e-05\n",
            "step: 110, loss: 3.7084850191604346e-05\n",
            "step: 120, loss: 3.775065124500543e-05\n",
            "step: 130, loss: 4.912374424748123e-05\n",
            "step: 140, loss: 2.2474321667687036e-05\n",
            "step: 150, loss: 9.206754475599155e-05\n",
            "step: 160, loss: 3.2962983823381364e-05\n",
            "step: 170, loss: 2.614327604533173e-05\n",
            "step: 180, loss: 0.0001458519254811108\n",
            "step: 190, loss: 3.3101707231253386e-05\n",
            "step: 200, loss: 8.670039096614346e-05\n",
            "step: 210, loss: 2.844554910552688e-05\n",
            "step: 220, loss: 6.036358172423206e-05\n",
            "step: 230, loss: 2.5927518436219543e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9841269841269841, f1=0.9785310734463276, best_f1=0.9773242630385486\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 160.90it/s]\n",
            "load_f1 = 0.9829738933030647\n",
            "real_f1 = 0.9807474518686297\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.83it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_G0OicNeCnd"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkIRgx40ezP8",
        "outputId": "0525f795-811e-4f71-c1df-12f12cc06747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 289kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 256kB/s] \n",
            "Downloading: 100% 440M/440M [00:08<00:00, 54.5MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6131656765937805\n",
            "step: 10, loss: 0.5458610653877258\n",
            "step: 20, loss: 0.5058370232582092\n",
            "step: 30, loss: 0.15447235107421875\n",
            "step: 40, loss: 0.14685778319835663\n",
            "step: 50, loss: 0.1476902961730957\n",
            "step: 60, loss: 0.04248557612299919\n",
            "step: 70, loss: 0.11163368821144104\n",
            "step: 80, loss: 0.0776122435927391\n",
            "step: 90, loss: 0.17099924385547638\n",
            "step: 100, loss: 0.0525943785905838\n",
            "step: 110, loss: 0.09751500934362411\n",
            "step: 120, loss: 0.0867849811911583\n",
            "step: 130, loss: 0.04742724448442459\n",
            "step: 140, loss: 0.25287553668022156\n",
            "step: 150, loss: 0.04952855780720711\n",
            "step: 160, loss: 0.061722543090581894\n",
            "step: 170, loss: 0.22188138961791992\n",
            "step: 180, loss: 0.18582426011562347\n",
            "step: 190, loss: 0.02190866693854332\n",
            "step: 200, loss: 0.1433989703655243\n",
            "step: 210, loss: 0.16660095751285553\n",
            "step: 220, loss: 0.24091005325317383\n",
            "step: 230, loss: 0.13381385803222656\n",
            "step: 240, loss: 0.21433721482753754\n",
            "step: 250, loss: 0.028790626674890518\n",
            "step: 260, loss: 0.05066675692796707\n",
            "step: 270, loss: 0.02707209624350071\n",
            "step: 280, loss: 0.111310675740242\n",
            "step: 290, loss: 0.056364450603723526\n",
            "step: 300, loss: 0.042654287070035934\n",
            "step: 310, loss: 0.2655971646308899\n",
            "step: 320, loss: 0.10262089222669601\n",
            "step: 330, loss: 0.0629330649971962\n",
            "step: 340, loss: 0.020097561180591583\n",
            "step: 350, loss: 0.15459631383419037\n",
            "step: 360, loss: 0.05065270885825157\n",
            "step: 370, loss: 0.06070220097899437\n",
            "step: 380, loss: 0.05613747611641884\n",
            "step: 390, loss: 0.11426419764757156\n",
            "step: 400, loss: 0.21832701563835144\n",
            "step: 410, loss: 0.0588616207242012\n",
            "step: 420, loss: 0.02027837559580803\n",
            "step: 430, loss: 0.17252127826213837\n",
            "step: 440, loss: 0.018073372542858124\n",
            "step: 450, loss: 0.01458664145320654\n",
            "step: 460, loss: 0.008270681835711002\n",
            "step: 470, loss: 0.19439199566841125\n",
            "step: 480, loss: 0.08142128586769104\n",
            "step: 490, loss: 0.1538284420967102\n",
            "step: 500, loss: 0.03482329845428467\n",
            "step: 510, loss: 0.1016848236322403\n",
            "step: 520, loss: 0.06757648289203644\n",
            "step: 530, loss: 0.005553641356527805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9234972677595629, f1=0.913457181694608, best_f1=0.913457181694608\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1305985152721405\n",
            "step: 10, loss: 0.030911823734641075\n",
            "step: 20, loss: 0.04126792773604393\n",
            "step: 30, loss: 0.10978595167398453\n",
            "step: 40, loss: 0.12300001084804535\n",
            "step: 50, loss: 0.21756966412067413\n",
            "step: 60, loss: 0.024722937494516373\n",
            "step: 70, loss: 0.01998637057840824\n",
            "step: 80, loss: 0.07076039910316467\n",
            "step: 90, loss: 0.015235917642712593\n",
            "step: 100, loss: 0.0680018737912178\n",
            "step: 110, loss: 0.043319132179021835\n",
            "step: 120, loss: 0.10190130770206451\n",
            "step: 130, loss: 0.18661358952522278\n",
            "step: 140, loss: 0.027722137048840523\n",
            "step: 150, loss: 0.05659234896302223\n",
            "step: 160, loss: 0.07032795250415802\n",
            "step: 170, loss: 0.025896186009049416\n",
            "step: 180, loss: 0.037320010364055634\n",
            "step: 190, loss: 0.09087035804986954\n",
            "step: 200, loss: 0.01340766903012991\n",
            "step: 210, loss: 0.02226976305246353\n",
            "step: 220, loss: 0.08735481649637222\n",
            "step: 230, loss: 0.006780026480555534\n",
            "step: 240, loss: 0.06627029925584793\n",
            "step: 250, loss: 0.002735364018008113\n",
            "step: 260, loss: 0.0056840842589735985\n",
            "step: 270, loss: 0.3281061351299286\n",
            "step: 280, loss: 0.026774635538458824\n",
            "step: 290, loss: 0.039085179567337036\n",
            "step: 300, loss: 0.18705812096595764\n",
            "step: 310, loss: 0.020771857351064682\n",
            "step: 320, loss: 0.04258488118648529\n",
            "step: 330, loss: 0.03876735642552376\n",
            "step: 340, loss: 0.01081305742263794\n",
            "step: 350, loss: 0.172308549284935\n",
            "step: 360, loss: 0.19582411646842957\n",
            "step: 370, loss: 0.12950780987739563\n",
            "step: 380, loss: 0.1112857311964035\n",
            "step: 390, loss: 0.07132012397050858\n",
            "step: 400, loss: 0.054627515375614166\n",
            "step: 410, loss: 0.022618787363171577\n",
            "step: 420, loss: 0.08627623319625854\n",
            "step: 430, loss: 0.003754950128495693\n",
            "step: 440, loss: 0.1264556348323822\n",
            "step: 450, loss: 0.007657757960259914\n",
            "step: 460, loss: 0.028602948412299156\n",
            "step: 470, loss: 0.14125174283981323\n",
            "step: 480, loss: 0.22733300924301147\n",
            "step: 490, loss: 0.013397961854934692\n",
            "step: 500, loss: 0.25262150168418884\n",
            "step: 510, loss: 0.017906788736581802\n",
            "step: 520, loss: 0.021194148808717728\n",
            "step: 530, loss: 0.031111840158700943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9266266728195662, f1=0.926605504587156, best_f1=0.926605504587156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02678108774125576\n",
            "step: 10, loss: 0.046760592609643936\n",
            "step: 20, loss: 0.07594453543424606\n",
            "step: 30, loss: 0.06872236728668213\n",
            "step: 40, loss: 0.0038001397624611855\n",
            "step: 50, loss: 0.1758647859096527\n",
            "step: 60, loss: 0.014343290589749813\n",
            "step: 70, loss: 0.004514650907367468\n",
            "step: 80, loss: 0.0014125462621450424\n",
            "step: 90, loss: 0.009315931238234043\n",
            "step: 100, loss: 0.03360210731625557\n",
            "step: 110, loss: 0.00443840678781271\n",
            "step: 120, loss: 0.033248938620090485\n",
            "step: 130, loss: 0.09761621057987213\n",
            "step: 140, loss: 0.0729832723736763\n",
            "step: 150, loss: 0.0314851738512516\n",
            "step: 160, loss: 0.03203263506293297\n",
            "step: 170, loss: 0.03640840947628021\n",
            "step: 180, loss: 0.036208897829055786\n",
            "step: 190, loss: 0.004420209676027298\n",
            "step: 200, loss: 0.013377116061747074\n",
            "step: 210, loss: 0.04865632951259613\n",
            "step: 220, loss: 0.11405551433563232\n",
            "step: 230, loss: 0.18915890157222748\n",
            "step: 240, loss: 0.003535299561917782\n",
            "step: 250, loss: 0.026771824806928635\n",
            "step: 260, loss: 0.032778050750494\n",
            "step: 270, loss: 0.007490411400794983\n",
            "step: 280, loss: 0.18551599979400635\n",
            "step: 290, loss: 0.006641969084739685\n",
            "step: 300, loss: 0.1315172165632248\n",
            "step: 310, loss: 0.01085018552839756\n",
            "step: 320, loss: 0.009458638727664948\n",
            "step: 330, loss: 0.004372093360871077\n",
            "step: 340, loss: 0.017262034118175507\n",
            "step: 350, loss: 0.008621478453278542\n",
            "step: 360, loss: 0.03596022352576256\n",
            "step: 370, loss: 0.008347981609404087\n",
            "step: 380, loss: 0.003852132009342313\n",
            "step: 390, loss: 0.035826053470373154\n",
            "step: 400, loss: 0.055689338594675064\n",
            "step: 410, loss: 0.005416004452854395\n",
            "step: 420, loss: 0.1468142718076706\n",
            "step: 430, loss: 0.06069837510585785\n",
            "step: 440, loss: 0.0029892357997596264\n",
            "step: 450, loss: 0.08638142049312592\n",
            "step: 460, loss: 0.018461931496858597\n",
            "step: 470, loss: 0.04480127617716789\n",
            "step: 480, loss: 0.006850267760455608\n",
            "step: 490, loss: 0.005469798110425472\n",
            "step: 500, loss: 0.0024734449107199907\n",
            "step: 510, loss: 0.006273058243095875\n",
            "step: 520, loss: 0.030196253210306168\n",
            "step: 530, loss: 0.027587831020355225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.927710843373494, f1=0.9285384970032272, best_f1=0.9285384970032272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014804691076278687\n",
            "step: 10, loss: 0.012086433358490467\n",
            "step: 20, loss: 0.020868618041276932\n",
            "step: 30, loss: 0.005080789793282747\n",
            "step: 40, loss: 0.02273429185152054\n",
            "step: 50, loss: 0.012821373529732227\n",
            "step: 60, loss: 0.020252780988812447\n",
            "step: 70, loss: 0.0015752790495753288\n",
            "step: 80, loss: 0.010092348791658878\n",
            "step: 90, loss: 0.010008088313043118\n",
            "step: 100, loss: 0.0009223191882483661\n",
            "step: 110, loss: 0.013730574399232864\n",
            "step: 120, loss: 0.001716467086225748\n",
            "step: 130, loss: 0.00043538110912777483\n",
            "step: 140, loss: 0.0006742006517015398\n",
            "step: 150, loss: 0.009345073252916336\n",
            "step: 160, loss: 0.04001487046480179\n",
            "step: 170, loss: 0.006178424693644047\n",
            "step: 180, loss: 0.004946308210492134\n",
            "step: 190, loss: 0.00573036540299654\n",
            "step: 200, loss: 0.0030443419236689806\n",
            "step: 210, loss: 0.020636631175875664\n",
            "step: 220, loss: 0.002836846746504307\n",
            "step: 230, loss: 0.06946935504674911\n",
            "step: 240, loss: 0.005375944077968597\n",
            "step: 250, loss: 0.019950436428189278\n",
            "step: 260, loss: 0.005844524595886469\n",
            "step: 270, loss: 0.005779662169516087\n",
            "step: 280, loss: 0.017766520380973816\n",
            "step: 290, loss: 0.06899330019950867\n",
            "step: 300, loss: 0.0047972677275538445\n",
            "step: 310, loss: 0.024105791002511978\n",
            "step: 320, loss: 0.0941750556230545\n",
            "step: 330, loss: 0.15647543966770172\n",
            "step: 340, loss: 0.007590575609356165\n",
            "step: 350, loss: 0.006063947454094887\n",
            "step: 360, loss: 0.00699726864695549\n",
            "step: 370, loss: 0.004108448047190905\n",
            "step: 380, loss: 0.0012377570383250713\n",
            "step: 390, loss: 0.00087932770838961\n",
            "step: 400, loss: 0.20908403396606445\n",
            "step: 410, loss: 0.0013914033770561218\n",
            "step: 420, loss: 0.06749977916479111\n",
            "step: 430, loss: 0.08145041763782501\n",
            "step: 440, loss: 0.0013273631921038032\n",
            "step: 450, loss: 0.001638802932575345\n",
            "step: 460, loss: 0.03902290388941765\n",
            "step: 470, loss: 0.0019619956146925688\n",
            "step: 480, loss: 0.0287595484405756\n",
            "step: 490, loss: 0.0032333971466869116\n",
            "step: 500, loss: 0.010242629796266556\n",
            "step: 510, loss: 0.020034845918416977\n",
            "step: 520, loss: 0.11103448271751404\n",
            "step: 530, loss: 0.014600371941924095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9255966307908282, f1=0.925497454881999, best_f1=0.9285384970032272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043291281908750534\n",
            "step: 10, loss: 0.12854821979999542\n",
            "step: 20, loss: 0.0019938405603170395\n",
            "step: 30, loss: 0.0057868813164532185\n",
            "step: 40, loss: 0.03703327476978302\n",
            "step: 50, loss: 0.006276112515479326\n",
            "step: 60, loss: 0.15512603521347046\n",
            "step: 70, loss: 0.019740235060453415\n",
            "step: 80, loss: 0.0008248545927926898\n",
            "step: 90, loss: 0.020496107637882233\n",
            "step: 100, loss: 0.03170965611934662\n",
            "step: 110, loss: 0.0004636347875930369\n",
            "step: 120, loss: 0.034992434084415436\n",
            "step: 130, loss: 0.0007003091741353273\n",
            "step: 140, loss: 0.0007035773014649749\n",
            "step: 150, loss: 0.02567366510629654\n",
            "step: 160, loss: 0.00047185615403577685\n",
            "step: 170, loss: 0.014311667531728745\n",
            "step: 180, loss: 0.0010864053620025516\n",
            "step: 190, loss: 0.00121020816732198\n",
            "step: 200, loss: 0.0013233856298029423\n",
            "step: 210, loss: 0.001415578997693956\n",
            "step: 220, loss: 0.009321454912424088\n",
            "step: 230, loss: 0.0013668567407876253\n",
            "step: 240, loss: 0.003309893887490034\n",
            "step: 250, loss: 0.0005285277147777379\n",
            "step: 260, loss: 0.002256567357107997\n",
            "step: 270, loss: 0.009686905890703201\n",
            "step: 280, loss: 0.008804845623672009\n",
            "step: 290, loss: 0.12971149384975433\n",
            "step: 300, loss: 0.0015979389427229762\n",
            "step: 310, loss: 0.003377100918442011\n",
            "step: 320, loss: 0.1381145864725113\n",
            "step: 330, loss: 0.011835281737148762\n",
            "step: 340, loss: 0.05409636348485947\n",
            "step: 350, loss: 0.0023051791358739138\n",
            "step: 360, loss: 0.002527270233258605\n",
            "step: 370, loss: 0.0032353391870856285\n",
            "step: 380, loss: 0.0014200415462255478\n",
            "step: 390, loss: 0.00026720380992628634\n",
            "step: 400, loss: 0.00825646985322237\n",
            "step: 410, loss: 0.0012494797119870782\n",
            "step: 420, loss: 0.012193293310701847\n",
            "step: 430, loss: 0.0008605736657045782\n",
            "step: 440, loss: 0.10051179677248001\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 450, loss: 0.1461179107427597\n",
            "step: 460, loss: 0.005174926016479731\n",
            "step: 470, loss: 0.0035996234510093927\n",
            "step: 480, loss: 0.016104472801089287\n",
            "step: 490, loss: 0.0006035795086063445\n",
            "step: 500, loss: 0.007561319041997194\n",
            "step: 510, loss: 0.022437019273638725\n",
            "step: 520, loss: 0.006287725176662207\n",
            "step: 530, loss: 0.0028350530192255974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9186875891583451, f1=0.9164700330344502, best_f1=0.9285384970032272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004058316349983215\n",
            "step: 10, loss: 0.0001838245225371793\n",
            "step: 20, loss: 0.040564149618148804\n",
            "step: 30, loss: 0.0011987807229161263\n",
            "step: 40, loss: 0.05114335939288139\n",
            "step: 50, loss: 0.07196895033121109\n",
            "step: 60, loss: 0.00016412034165114164\n",
            "step: 70, loss: 0.00013274408411234617\n",
            "step: 80, loss: 0.0007327237399294972\n",
            "step: 90, loss: 0.010106394998729229\n",
            "step: 100, loss: 0.0016318743582814932\n",
            "step: 110, loss: 0.0010711518116295338\n",
            "step: 120, loss: 0.0010127925779670477\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.0013157054781913757\n",
            "step: 140, loss: 0.021876949816942215\n",
            "step: 150, loss: 0.0016078135231509805\n",
            "step: 160, loss: 0.0013090481515973806\n",
            "step: 170, loss: 0.0006438368000090122\n",
            "step: 180, loss: 0.0003600372583605349\n",
            "step: 190, loss: 0.00040750522748567164\n",
            "step: 200, loss: 0.0005405033007264137\n",
            "step: 210, loss: 0.01024746522307396\n",
            "step: 220, loss: 0.007990465499460697\n",
            "step: 230, loss: 0.002026102039963007\n",
            "step: 240, loss: 0.039435695856809616\n",
            "step: 250, loss: 0.00041031098226085305\n",
            "step: 260, loss: 0.00019969682034570724\n",
            "step: 270, loss: 0.049291349947452545\n",
            "step: 280, loss: 0.0019438094459474087\n",
            "step: 290, loss: 0.004202405922114849\n",
            "step: 300, loss: 0.0016126229893416166\n",
            "step: 310, loss: 0.0010413707932457328\n",
            "step: 320, loss: 0.0005364437238313258\n",
            "step: 330, loss: 0.002188070910051465\n",
            "step: 340, loss: 0.02774086222052574\n",
            "step: 350, loss: 0.0023191431537270546\n",
            "step: 360, loss: 0.003170965239405632\n",
            "step: 370, loss: 0.003434722777456045\n",
            "step: 380, loss: 0.00022205521236173809\n",
            "step: 390, loss: 0.01229170709848404\n",
            "step: 400, loss: 0.00012634861923288554\n",
            "step: 410, loss: 0.00032810488482937217\n",
            "step: 420, loss: 0.0011282290797680616\n",
            "step: 430, loss: 0.00041956492350436747\n",
            "step: 440, loss: 0.00023225814220495522\n",
            "step: 450, loss: 0.001395867089740932\n",
            "step: 460, loss: 9.816299279918894e-05\n",
            "step: 470, loss: 0.007431095466017723\n",
            "step: 480, loss: 0.003999523818492889\n",
            "step: 490, loss: 0.018637122586369514\n",
            "step: 500, loss: 0.0012754888739436865\n",
            "step: 510, loss: 0.003633993212133646\n",
            "step: 520, loss: 0.008581340312957764\n",
            "step: 530, loss: 0.010082930326461792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9163498098859316, f1=0.9202626641651033, best_f1=0.9285384970032272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021738703071605414\n",
            "step: 10, loss: 0.0066016665659844875\n",
            "step: 20, loss: 0.004434582777321339\n",
            "step: 30, loss: 0.0007183939451351762\n",
            "step: 40, loss: 0.0037869391962885857\n",
            "step: 50, loss: 0.015510812401771545\n",
            "step: 60, loss: 0.0012470022775232792\n",
            "step: 70, loss: 0.007227855734527111\n",
            "step: 80, loss: 0.0031273963395506144\n",
            "step: 90, loss: 0.00048213504487648606\n",
            "step: 100, loss: 0.00029935178463347256\n",
            "step: 110, loss: 0.011442124843597412\n",
            "step: 120, loss: 0.00025160194491036236\n",
            "step: 130, loss: 0.0005743289948441088\n",
            "step: 140, loss: 0.0004558709042612463\n",
            "step: 150, loss: 0.003920306917279959\n",
            "step: 160, loss: 0.0007167247822508216\n",
            "step: 170, loss: 0.0010188949527218938\n",
            "step: 180, loss: 0.0018948856741189957\n",
            "step: 190, loss: 0.021148094907402992\n",
            "step: 200, loss: 0.00026757948216982186\n",
            "step: 210, loss: 0.02699396014213562\n",
            "step: 220, loss: 0.0009421081631444395\n",
            "step: 230, loss: 0.0068640802055597305\n",
            "step: 240, loss: 0.0011676347348839045\n",
            "step: 250, loss: 0.0011712070554494858\n",
            "step: 260, loss: 0.00027767583378590643\n",
            "step: 270, loss: 0.0004405493091326207\n",
            "step: 280, loss: 0.016141362488269806\n",
            "step: 290, loss: 0.0017366843530908227\n",
            "step: 300, loss: 0.004615856800228357\n",
            "step: 310, loss: 0.0003020838485099375\n",
            "step: 320, loss: 9.190929995384067e-05\n",
            "step: 330, loss: 0.0001463072549086064\n",
            "step: 340, loss: 0.004520641174167395\n",
            "step: 350, loss: 0.00040708479355089366\n",
            "step: 360, loss: 0.0006666618282906711\n",
            "step: 370, loss: 0.0005783864762634039\n",
            "step: 380, loss: 0.00375442230142653\n",
            "step: 390, loss: 0.00017146665777545422\n",
            "step: 400, loss: 0.00423762621358037\n",
            "step: 410, loss: 0.0007924732053652406\n",
            "step: 420, loss: 0.007538542151451111\n",
            "step: 430, loss: 0.006341320462524891\n",
            "step: 440, loss: 0.0007803954067640007\n",
            "step: 450, loss: 0.026004116982221603\n",
            "step: 460, loss: 0.0002460702962707728\n",
            "step: 470, loss: 0.09539392590522766\n",
            "step: 480, loss: 0.16107897460460663\n",
            "step: 490, loss: 0.0028791569638997316\n",
            "step: 500, loss: 0.0009505567140877247\n",
            "step: 510, loss: 0.0027590489480644464\n",
            "step: 520, loss: 0.006974717602133751\n",
            "step: 530, loss: 0.003945984877645969\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9331489165514063, f1=0.9222988505747127, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010921044740825891\n",
            "step: 10, loss: 0.020361388102173805\n",
            "step: 20, loss: 0.08378364145755768\n",
            "step: 30, loss: 0.008279534988105297\n",
            "step: 40, loss: 0.00034030849928967655\n",
            "step: 50, loss: 0.005392452701926231\n",
            "step: 60, loss: 0.0024112381506711245\n",
            "step: 70, loss: 0.0002028268645517528\n",
            "step: 80, loss: 0.011014261282980442\n",
            "step: 90, loss: 0.0009648966370150447\n",
            "step: 100, loss: 0.00010070623102365062\n",
            "step: 110, loss: 0.0002112688816851005\n",
            "step: 120, loss: 0.0013201559195294976\n",
            "step: 130, loss: 0.000734530040062964\n",
            "step: 140, loss: 0.00029166246531531215\n",
            "step: 150, loss: 0.00012841868738178164\n",
            "step: 160, loss: 0.024322422221302986\n",
            "step: 170, loss: 0.00032959316740743816\n",
            "step: 180, loss: 0.0003277207724750042\n",
            "step: 190, loss: 0.003552323207259178\n",
            "step: 200, loss: 0.00249241734854877\n",
            "step: 210, loss: 0.00034394761314615607\n",
            "step: 220, loss: 0.0009527344373054802\n",
            "step: 230, loss: 0.0052878460846841335\n",
            "step: 240, loss: 0.00012086041533621028\n",
            "step: 250, loss: 3.9583377656526864e-05\n",
            "step: 260, loss: 0.0007245094748213887\n",
            "step: 270, loss: 0.006818788591772318\n",
            "step: 280, loss: 0.008596500381827354\n",
            "step: 290, loss: 6.44441315671429e-05\n",
            "step: 300, loss: 0.0010301503352820873\n",
            "step: 310, loss: 0.009022170677781105\n",
            "step: 320, loss: 0.0010068707633763552\n",
            "step: 330, loss: 0.0003560494224075228\n",
            "step: 340, loss: 0.0006923438631929457\n",
            "step: 350, loss: 0.0003758264647331089\n",
            "step: 360, loss: 0.010658439248800278\n",
            "step: 370, loss: 0.0002450503525324166\n",
            "step: 380, loss: 0.0011224672198295593\n",
            "step: 390, loss: 0.13025808334350586\n",
            "step: 400, loss: 0.000171125924680382\n",
            "step: 410, loss: 0.00013490533456206322\n",
            "step: 420, loss: 0.010449953377246857\n",
            "step: 430, loss: 0.011853385716676712\n",
            "step: 440, loss: 0.07594014704227448\n",
            "step: 450, loss: 0.0006051716045476496\n",
            "step: 460, loss: 0.00031532542197965086\n",
            "step: 470, loss: 7.018230098765343e-05\n",
            "step: 480, loss: 5.0195918447570875e-05\n",
            "step: 490, loss: 0.0005498963291756809\n",
            "step: 500, loss: 7.722756708972156e-05\n",
            "step: 510, loss: 6.876578117953613e-05\n",
            "step: 520, loss: 0.0035827327519655228\n",
            "step: 530, loss: 9.892028901958838e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.928772258669166, f1=0.9211502782931354, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010357258142903447\n",
            "step: 10, loss: 0.0009734708000905812\n",
            "step: 20, loss: 0.01923714205622673\n",
            "step: 30, loss: 0.00017981410201173276\n",
            "step: 40, loss: 0.0017274554120376706\n",
            "step: 50, loss: 0.0001310365623794496\n",
            "step: 60, loss: 0.0003583342186175287\n",
            "step: 70, loss: 0.0008943747379817069\n",
            "step: 80, loss: 0.0007519164937548339\n",
            "step: 90, loss: 0.0005022430559620261\n",
            "step: 100, loss: 0.00014042614202480763\n",
            "step: 110, loss: 0.00015568580420222133\n",
            "step: 120, loss: 0.00027589488308876753\n",
            "step: 130, loss: 0.0016645489959046245\n",
            "step: 140, loss: 0.0007608064915984869\n",
            "step: 150, loss: 0.00015988202358130366\n",
            "step: 160, loss: 0.0004945645923726261\n",
            "step: 170, loss: 0.006144167389720678\n",
            "step: 180, loss: 0.00040157182957045734\n",
            "step: 190, loss: 0.00791709590703249\n",
            "step: 200, loss: 8.192509994842112e-05\n",
            "step: 210, loss: 0.1543179452419281\n",
            "step: 220, loss: 0.0014282730408012867\n",
            "step: 230, loss: 0.02345952019095421\n",
            "step: 240, loss: 0.0019041035557165742\n",
            "step: 250, loss: 0.00032100133830681443\n",
            "step: 260, loss: 0.004358056001365185\n",
            "step: 270, loss: 5.694273568224162e-05\n",
            "step: 280, loss: 0.0001923874660860747\n",
            "step: 290, loss: 0.04751084744930267\n",
            "step: 300, loss: 8.211174281314015e-05\n",
            "step: 310, loss: 0.015100072138011456\n",
            "step: 320, loss: 0.00026588627952151\n",
            "step: 330, loss: 0.00325647066347301\n",
            "step: 340, loss: 0.0005455795908346772\n",
            "step: 350, loss: 0.00010868873505387455\n",
            "step: 360, loss: 3.3891825296450406e-05\n",
            "step: 370, loss: 0.0004447197716217488\n",
            "step: 380, loss: 0.00011239347804803401\n",
            "step: 390, loss: 6.765041325706989e-05\n",
            "step: 400, loss: 0.03620930761098862\n",
            "step: 410, loss: 0.0011681760661303997\n",
            "step: 420, loss: 4.565929339150898e-05\n",
            "step: 430, loss: 0.00017882413521874696\n",
            "step: 440, loss: 0.0011872949544340372\n",
            "step: 450, loss: 0.00010721817670855671\n",
            "step: 460, loss: 7.978601934155449e-05\n",
            "step: 470, loss: 8.555312524549663e-05\n",
            "step: 480, loss: 5.0915623432956636e-05\n",
            "step: 490, loss: 0.00013587398279923946\n",
            "step: 500, loss: 0.0011068281019106507\n",
            "step: 510, loss: 0.0004409915709402412\n",
            "step: 520, loss: 0.0008438042714260519\n",
            "step: 530, loss: 0.0003293339104857296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9273323956868261, f1=0.9265799256505577, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015680679643992335\n",
            "step: 10, loss: 4.828748933505267e-05\n",
            "step: 20, loss: 0.00012043834431096911\n",
            "step: 30, loss: 9.270678856410086e-05\n",
            "step: 40, loss: 0.0007738824933767319\n",
            "step: 50, loss: 0.00010209157335339114\n",
            "step: 60, loss: 3.770979310502298e-05\n",
            "step: 70, loss: 6.449321517720819e-05\n",
            "step: 80, loss: 3.9322763768723235e-05\n",
            "step: 90, loss: 4.470790736377239e-05\n",
            "step: 100, loss: 3.734450729098171e-05\n",
            "step: 110, loss: 3.833141818176955e-05\n",
            "step: 120, loss: 0.00021239471971057355\n",
            "step: 130, loss: 3.9337497582891956e-05\n",
            "step: 140, loss: 0.007993093691766262\n",
            "step: 150, loss: 4.580306267598644e-05\n",
            "step: 160, loss: 0.0007461826899088919\n",
            "step: 170, loss: 7.867729436839e-05\n",
            "step: 180, loss: 6.659035716438666e-05\n",
            "step: 190, loss: 0.00011180675210198388\n",
            "step: 200, loss: 0.00034112768480554223\n",
            "step: 210, loss: 4.933813397656195e-05\n",
            "step: 220, loss: 0.0009684744873084128\n",
            "step: 230, loss: 0.0001458650513086468\n",
            "step: 240, loss: 4.7137804358499125e-05\n",
            "step: 250, loss: 0.0018064940813928843\n",
            "step: 260, loss: 0.00031190848676487803\n",
            "step: 270, loss: 0.00022566519328393042\n",
            "step: 280, loss: 0.0009958567097783089\n",
            "step: 290, loss: 4.39781833847519e-05\n",
            "step: 300, loss: 0.000292174139758572\n",
            "step: 310, loss: 0.00019844478811137378\n",
            "step: 320, loss: 0.006015370134264231\n",
            "step: 330, loss: 0.07953262329101562\n",
            "step: 340, loss: 0.11812973022460938\n",
            "step: 350, loss: 0.23891304433345795\n",
            "step: 360, loss: 0.0005034773494116962\n",
            "step: 370, loss: 0.003968555014580488\n",
            "step: 380, loss: 0.00373770366422832\n",
            "step: 390, loss: 0.010802504606544971\n",
            "step: 400, loss: 0.0008199251024052501\n",
            "step: 410, loss: 0.003184306202456355\n",
            "step: 420, loss: 0.00035524129634723067\n",
            "step: 430, loss: 0.0004685016756411642\n",
            "step: 440, loss: 0.0023057428188622\n",
            "step: 450, loss: 8.31131765153259e-05\n",
            "step: 460, loss: 6.49130015517585e-05\n",
            "step: 470, loss: 0.00021413227659650147\n",
            "step: 480, loss: 9.14420306799002e-05\n",
            "step: 490, loss: 0.0009731760947033763\n",
            "step: 500, loss: 0.004903409630060196\n",
            "step: 510, loss: 5.386117845773697e-05\n",
            "step: 520, loss: 8.749881817493588e-05\n",
            "step: 530, loss: 0.0010404884815216064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9322191272051997, f1=0.9294605809128631, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009660134674049914\n",
            "step: 10, loss: 7.21010728739202e-05\n",
            "step: 20, loss: 0.00011268431262578815\n",
            "step: 30, loss: 0.0019629704765975475\n",
            "step: 40, loss: 4.33086788689252e-05\n",
            "step: 50, loss: 0.000813081453088671\n",
            "step: 60, loss: 0.0035864838864654303\n",
            "step: 70, loss: 0.00016378966392949224\n",
            "step: 80, loss: 0.0077219558879733086\n",
            "step: 90, loss: 4.50500774604734e-05\n",
            "step: 100, loss: 0.0038780171889811754\n",
            "step: 110, loss: 0.00016249463078565896\n",
            "step: 120, loss: 0.0007657759706489742\n",
            "step: 130, loss: 0.00014982848369982094\n",
            "step: 140, loss: 0.00013913247676100582\n",
            "step: 150, loss: 2.6989153411705047e-05\n",
            "step: 160, loss: 0.0004889483097940683\n",
            "step: 170, loss: 4.342377360444516e-05\n",
            "step: 180, loss: 6.835486419731751e-05\n",
            "step: 190, loss: 0.00027528416831046343\n",
            "step: 200, loss: 0.0021802678238600492\n",
            "step: 210, loss: 0.00018617989553604275\n",
            "step: 220, loss: 0.00046130173723213375\n",
            "step: 230, loss: 2.9108703529345803e-05\n",
            "step: 240, loss: 0.0001407690579071641\n",
            "step: 250, loss: 0.0028639452066272497\n",
            "step: 260, loss: 2.988718006236013e-05\n",
            "step: 270, loss: 0.0003528591769281775\n",
            "step: 280, loss: 0.0010520886862650514\n",
            "step: 290, loss: 6.482327444246039e-05\n",
            "step: 300, loss: 0.0002893354103434831\n",
            "step: 310, loss: 0.00016966431576292962\n",
            "step: 320, loss: 0.0001957167114596814\n",
            "step: 330, loss: 6.306588329607621e-05\n",
            "step: 340, loss: 6.378210673574358e-05\n",
            "step: 350, loss: 8.44752139528282e-05\n",
            "step: 360, loss: 0.00020311646221671253\n",
            "step: 370, loss: 0.00026261014863848686\n",
            "step: 380, loss: 4.328084469307214e-05\n",
            "step: 390, loss: 3.9991569792618975e-05\n",
            "step: 400, loss: 0.0026106329169124365\n",
            "step: 410, loss: 3.5232405934948474e-05\n",
            "step: 420, loss: 4.042144792038016e-05\n",
            "step: 430, loss: 4.159224044997245e-05\n",
            "step: 440, loss: 0.00016772831440903246\n",
            "step: 450, loss: 0.00046256970381364226\n",
            "step: 460, loss: 0.0006466695922426879\n",
            "step: 470, loss: 9.457465057494119e-05\n",
            "step: 480, loss: 0.00011145022290293127\n",
            "step: 490, loss: 0.0004910605493932962\n",
            "step: 500, loss: 0.0010309139033779502\n",
            "step: 510, loss: 0.0002679661847651005\n",
            "step: 520, loss: 2.8020938771078363e-05\n",
            "step: 530, loss: 3.789913171203807e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9279999999999999, f1=0.9207089552238805, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.7651072489097714e-05\n",
            "step: 10, loss: 2.9238393835839815e-05\n",
            "step: 20, loss: 0.00010491560533409938\n",
            "step: 30, loss: 0.0017742933705449104\n",
            "step: 40, loss: 0.0002698609314393252\n",
            "step: 50, loss: 0.03850200027227402\n",
            "step: 60, loss: 0.0014946392038837075\n",
            "step: 70, loss: 5.503812280949205e-05\n",
            "step: 80, loss: 3.321683834656142e-05\n",
            "step: 90, loss: 0.001357845263555646\n",
            "step: 100, loss: 0.0005885578575544059\n",
            "step: 110, loss: 3.770927287405357e-05\n",
            "step: 120, loss: 5.4235290008364245e-05\n",
            "step: 130, loss: 0.00032224980532191694\n",
            "step: 140, loss: 7.719272252870724e-05\n",
            "step: 150, loss: 3.876471964758821e-05\n",
            "step: 160, loss: 2.099907760566566e-05\n",
            "step: 170, loss: 4.816586442757398e-05\n",
            "step: 180, loss: 6.767593731638044e-05\n",
            "step: 190, loss: 3.242734965169802e-05\n",
            "step: 200, loss: 0.0006487223436124623\n",
            "step: 210, loss: 3.386849130038172e-05\n",
            "step: 220, loss: 6.027620838722214e-05\n",
            "step: 230, loss: 1.6938689441303723e-05\n",
            "step: 240, loss: 5.0249233026988804e-05\n",
            "step: 250, loss: 5.944000440649688e-05\n",
            "step: 260, loss: 2.7089401555713266e-05\n",
            "step: 270, loss: 3.651745282695629e-05\n",
            "step: 280, loss: 0.0010484351078048348\n",
            "step: 290, loss: 0.00027757376665249467\n",
            "step: 300, loss: 0.00012251586304046214\n",
            "step: 310, loss: 6.828539335401729e-05\n",
            "step: 320, loss: 2.0186891561024822e-05\n",
            "step: 330, loss: 3.633794767665677e-05\n",
            "step: 340, loss: 0.00014479397214017808\n",
            "step: 350, loss: 0.0001359164743917063\n",
            "step: 360, loss: 0.0008567713666707277\n",
            "step: 370, loss: 8.025446004467085e-05\n",
            "step: 380, loss: 7.942911906866357e-05\n",
            "step: 390, loss: 9.477381536271423e-05\n",
            "step: 400, loss: 2.43443064391613e-05\n",
            "step: 410, loss: 0.0004709537897724658\n",
            "step: 420, loss: 0.005941905081272125\n",
            "step: 430, loss: 0.004216314293444157\n",
            "step: 440, loss: 0.0001228846376761794\n",
            "step: 450, loss: 3.854314491036348e-05\n",
            "step: 460, loss: 1.785881795512978e-05\n",
            "step: 470, loss: 0.0004735949041787535\n",
            "step: 480, loss: 0.00014676403952762485\n",
            "step: 490, loss: 0.0011026135180145502\n",
            "step: 500, loss: 6.935171404620633e-05\n",
            "step: 510, loss: 0.00014025942073203623\n",
            "step: 520, loss: 2.894820863730274e-05\n",
            "step: 530, loss: 4.199921022518538e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9284369114877589, f1=0.9265325222274216, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001372679602354765\n",
            "step: 10, loss: 2.418760777800344e-05\n",
            "step: 20, loss: 1.7486261640442535e-05\n",
            "step: 30, loss: 0.0005396975320763886\n",
            "step: 40, loss: 4.473160515772179e-05\n",
            "step: 50, loss: 9.143666102318093e-05\n",
            "step: 60, loss: 7.117149652913213e-05\n",
            "step: 70, loss: 5.220070306677371e-05\n",
            "step: 80, loss: 2.0525969375739805e-05\n",
            "step: 90, loss: 0.0053299907594919205\n",
            "step: 100, loss: 0.00011098036338808015\n",
            "step: 110, loss: 0.00010036156891146675\n",
            "step: 120, loss: 5.3816438594367355e-05\n",
            "step: 130, loss: 2.0626519471989013e-05\n",
            "step: 140, loss: 3.3492502552689984e-05\n",
            "step: 150, loss: 0.0020242012105882168\n",
            "step: 160, loss: 2.2250607798923738e-05\n",
            "step: 170, loss: 0.00035261319135315716\n",
            "step: 180, loss: 3.503131665638648e-05\n",
            "step: 190, loss: 2.158362985937856e-05\n",
            "step: 200, loss: 2.143491292372346e-05\n",
            "step: 210, loss: 2.800549555104226e-05\n",
            "step: 220, loss: 1.601482290425338e-05\n",
            "step: 230, loss: 3.6290664866101e-05\n",
            "step: 240, loss: 0.0002467202430125326\n",
            "step: 250, loss: 2.0205601686029695e-05\n",
            "step: 260, loss: 3.1814819521969184e-05\n",
            "step: 270, loss: 1.616750523680821e-05\n",
            "step: 280, loss: 2.2347408958012238e-05\n",
            "step: 290, loss: 3.355910666869022e-05\n",
            "step: 300, loss: 0.0002918803074862808\n",
            "step: 310, loss: 0.0009047173080034554\n",
            "step: 320, loss: 0.001981559442356229\n",
            "step: 330, loss: 0.0005207728245295584\n",
            "step: 340, loss: 2.634401425893884e-05\n",
            "step: 350, loss: 3.144257789244875e-05\n",
            "step: 360, loss: 2.948559085780289e-05\n",
            "step: 370, loss: 3.16068617394194e-05\n",
            "step: 380, loss: 3.62726605089847e-05\n",
            "step: 390, loss: 0.00015168561367318034\n",
            "step: 400, loss: 3.0036908356123604e-05\n",
            "step: 410, loss: 0.00015940114099066705\n",
            "step: 420, loss: 1.670027086220216e-05\n",
            "step: 430, loss: 0.0013448743848130107\n",
            "step: 440, loss: 2.5416617063456215e-05\n",
            "step: 450, loss: 7.910388376330957e-05\n",
            "step: 460, loss: 0.0006614992744289339\n",
            "step: 470, loss: 1.3325223335414194e-05\n",
            "step: 480, loss: 2.428340849291999e-05\n",
            "step: 490, loss: 3.4791326470440254e-05\n",
            "step: 500, loss: 1.3135238987160847e-05\n",
            "step: 510, loss: 2.1247824406600557e-05\n",
            "step: 520, loss: 0.00018942367751151323\n",
            "step: 530, loss: 1.823107959353365e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9299303944315545, f1=0.9252767527675276, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002359656849876046\n",
            "step: 10, loss: 2.1214887965470552e-05\n",
            "step: 20, loss: 0.0007738119456917048\n",
            "step: 30, loss: 0.00010745474719442427\n",
            "step: 40, loss: 0.0005091029452160001\n",
            "step: 50, loss: 0.00028053909773007035\n",
            "step: 60, loss: 1.645434895181097e-05\n",
            "step: 70, loss: 1.760919258231297e-05\n",
            "step: 80, loss: 8.012853504624218e-05\n",
            "step: 90, loss: 3.265397026552819e-05\n",
            "step: 100, loss: 0.005030362866818905\n",
            "step: 110, loss: 3.657710476545617e-05\n",
            "step: 120, loss: 3.641576040536165e-05\n",
            "step: 130, loss: 0.02732202783226967\n",
            "step: 140, loss: 4.923817323287949e-05\n",
            "step: 150, loss: 1.8205177184427157e-05\n",
            "step: 160, loss: 5.3155537898419425e-05\n",
            "step: 170, loss: 0.00015877022815402597\n",
            "step: 180, loss: 2.370304719079286e-05\n",
            "step: 190, loss: 0.0032022136729210615\n",
            "step: 200, loss: 3.113873754045926e-05\n",
            "step: 210, loss: 2.0578179828589782e-05\n",
            "step: 220, loss: 2.136769035132602e-05\n",
            "step: 230, loss: 0.0016770681831985712\n",
            "step: 240, loss: 4.5125405449653044e-05\n",
            "step: 250, loss: 2.6124660507775843e-05\n",
            "step: 260, loss: 3.605367601267062e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 270, loss: 5.6708133342908695e-05\n",
            "step: 280, loss: 4.7821322368690744e-05\n",
            "step: 290, loss: 5.8124114730162546e-05\n",
            "step: 300, loss: 2.225071693828795e-05\n",
            "step: 310, loss: 0.00029828035621903837\n",
            "step: 320, loss: 0.00012964929919689894\n",
            "step: 330, loss: 5.704248178517446e-05\n",
            "step: 340, loss: 3.50222981069237e-05\n",
            "step: 350, loss: 0.00011240167077630758\n",
            "step: 360, loss: 0.06337511539459229\n",
            "step: 370, loss: 2.558783489803318e-05\n",
            "step: 380, loss: 2.4813423806335777e-05\n",
            "step: 390, loss: 0.004944520071148872\n",
            "step: 400, loss: 5.191111995372921e-05\n",
            "step: 410, loss: 1.8134125639335252e-05\n",
            "step: 420, loss: 2.9051407182123512e-05\n",
            "step: 430, loss: 6.616143218707293e-05\n",
            "step: 440, loss: 8.440229430561885e-05\n",
            "step: 450, loss: 7.559170626336709e-05\n",
            "step: 460, loss: 2.599348772491794e-05\n",
            "step: 470, loss: 1.828705171647016e-05\n",
            "step: 480, loss: 2.2254038412938826e-05\n",
            "step: 490, loss: 1.874533154477831e-05\n",
            "step: 500, loss: 1.4975514204706997e-05\n",
            "step: 510, loss: 0.01219907682389021\n",
            "step: 520, loss: 4.823922427021898e-05\n",
            "step: 530, loss: 2.7684996894095093e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9299905392620624, f1=0.9257518796992481, best_f1=0.9222988505747127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.178856811951846e-05\n",
            "step: 10, loss: 1.655498090258334e-05\n",
            "step: 20, loss: 2.7081228836323135e-05\n",
            "step: 30, loss: 0.00018950695812236518\n",
            "step: 40, loss: 0.009091636165976524\n",
            "step: 50, loss: 4.28300081694033e-05\n",
            "step: 60, loss: 3.564063081284985e-05\n",
            "step: 70, loss: 0.002760433591902256\n",
            "step: 80, loss: 2.8909586035297252e-05\n",
            "step: 90, loss: 2.9628974516526796e-05\n",
            "step: 100, loss: 2.3509121092502028e-05\n",
            "step: 110, loss: 5.68201721762307e-05\n",
            "step: 120, loss: 0.0005630645900964737\n",
            "step: 130, loss: 0.050705261528491974\n",
            "step: 140, loss: 8.820874791126698e-05\n",
            "step: 150, loss: 1.910296850837767e-05\n",
            "step: 160, loss: 9.069030056707561e-05\n",
            "step: 170, loss: 0.000134886140585877\n",
            "step: 180, loss: 3.662397284642793e-05\n",
            "step: 190, loss: 4.559089938993566e-05\n",
            "step: 200, loss: 1.8171520423493348e-05\n",
            "step: 210, loss: 0.0003102347836829722\n",
            "step: 220, loss: 2.3658307327423245e-05\n",
            "step: 230, loss: 3.305726932012476e-05\n",
            "step: 240, loss: 4.398372766445391e-05\n",
            "step: 250, loss: 2.311823845957406e-05\n",
            "step: 260, loss: 1.5817422536201775e-05\n",
            "step: 270, loss: 1.5213919141388033e-05\n",
            "step: 280, loss: 3.4248583688167855e-05\n",
            "step: 290, loss: 1.2978801350982394e-05\n",
            "step: 300, loss: 1.6148931536008604e-05\n",
            "step: 310, loss: 0.00010439592006150633\n",
            "step: 320, loss: 1.7776772438082844e-05\n",
            "step: 330, loss: 1.7828871932579204e-05\n",
            "step: 340, loss: 1.3276821846375242e-05\n",
            "step: 350, loss: 1.5470810467377305e-05\n",
            "step: 360, loss: 0.0020314217545092106\n",
            "step: 370, loss: 1.5146791156439576e-05\n",
            "step: 380, loss: 3.820312849711627e-05\n",
            "step: 390, loss: 4.577890285872854e-05\n",
            "step: 400, loss: 3.617135007516481e-05\n",
            "step: 410, loss: 2.7073750970885158e-05\n",
            "step: 420, loss: 2.689815482881386e-05\n",
            "step: 430, loss: 2.9517474104068242e-05\n",
            "step: 440, loss: 1.9151428205077536e-05\n",
            "step: 450, loss: 1.673365659371484e-05\n",
            "step: 460, loss: 1.8804876162903383e-05\n",
            "step: 470, loss: 0.000165477380505763\n",
            "step: 480, loss: 1.2908030839753337e-05\n",
            "step: 490, loss: 1.4681198990729172e-05\n",
            "step: 500, loss: 4.272803198546171e-05\n",
            "step: 510, loss: 2.33196078625042e-05\n",
            "step: 520, loss: 1.5038816854939796e-05\n",
            "step: 530, loss: 4.012647696072236e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9314553990610329, f1=0.9270346117867166, best_f1=0.9222988505747127\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:30, 185.84it/s]\n",
            "load_f1 = 0.9282739472466451\n",
            "real_f1 = 0.9265799256505577\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 177.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkSbOTQeC3W"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4XWqpo1e0O0",
        "outputId": "3f08a1b2-6abe-48a6-8c5c-94eb0007a8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.559829592704773\n",
            "step: 10, loss: 0.35776859521865845\n",
            "step: 20, loss: 0.39042720198631287\n",
            "step: 30, loss: 0.3186863362789154\n",
            "step: 40, loss: 0.17897576093673706\n",
            "step: 50, loss: 0.4299011528491974\n",
            "step: 60, loss: 0.3272276222705841\n",
            "step: 70, loss: 0.18039266765117645\n",
            "step: 80, loss: 0.18885678052902222\n",
            "step: 90, loss: 0.4533100426197052\n",
            "step: 100, loss: 0.3786313831806183\n",
            "step: 110, loss: 0.19130678474903107\n",
            "step: 120, loss: 0.2685830295085907\n",
            "step: 130, loss: 0.15352457761764526\n",
            "step: 140, loss: 0.3449651002883911\n",
            "step: 150, loss: 0.2473987638950348\n",
            "step: 160, loss: 0.3724004626274109\n",
            "step: 170, loss: 0.22690705955028534\n",
            "step: 180, loss: 0.15363478660583496\n",
            "step: 190, loss: 0.19710369408130646\n",
            "step: 200, loss: 0.2925253212451935\n",
            "step: 210, loss: 0.20270362496376038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6018348623853211, f1=0.6021897810218978, best_f1=0.6021897810218978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12574860453605652\n",
            "step: 10, loss: 0.17844849824905396\n",
            "step: 20, loss: 0.17750893533229828\n",
            "step: 30, loss: 0.21541379392147064\n",
            "step: 40, loss: 0.19454790651798248\n",
            "step: 50, loss: 0.19529353082180023\n",
            "step: 60, loss: 0.4758639931678772\n",
            "step: 70, loss: 0.1465493142604828\n",
            "step: 80, loss: 0.19298091530799866\n",
            "step: 90, loss: 0.09367386996746063\n",
            "step: 100, loss: 0.028457511216402054\n",
            "step: 110, loss: 0.13631656765937805\n",
            "step: 120, loss: 0.1633710414171219\n",
            "step: 130, loss: 0.05323651060461998\n",
            "step: 140, loss: 0.1816863715648651\n",
            "step: 150, loss: 0.25186797976493835\n",
            "step: 160, loss: 0.19961333274841309\n",
            "step: 170, loss: 0.10729032009840012\n",
            "step: 180, loss: 0.18284305930137634\n",
            "step: 190, loss: 0.16727280616760254\n",
            "step: 200, loss: 0.14718946814537048\n",
            "step: 210, loss: 0.1958560198545456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6183206106870229, f1=0.6435452793834296, best_f1=0.6435452793834296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09317246824502945\n",
            "step: 10, loss: 0.13484583795070648\n",
            "step: 20, loss: 0.08088520914316177\n",
            "step: 30, loss: 0.11232322454452515\n",
            "step: 40, loss: 0.21535876393318176\n",
            "step: 50, loss: 0.03610612452030182\n",
            "step: 60, loss: 0.15268006920814514\n",
            "step: 70, loss: 0.15341807901859283\n",
            "step: 80, loss: 0.18252821266651154\n",
            "step: 90, loss: 0.05264953523874283\n",
            "step: 100, loss: 0.19984206557273865\n",
            "step: 110, loss: 0.13932503759860992\n",
            "step: 120, loss: 0.08967564254999161\n",
            "step: 130, loss: 0.11559707671403885\n",
            "step: 140, loss: 0.1335253268480301\n",
            "step: 150, loss: 0.26776447892189026\n",
            "step: 160, loss: 0.04518653452396393\n",
            "step: 170, loss: 0.1609368622303009\n",
            "step: 180, loss: 0.09510163217782974\n",
            "step: 190, loss: 0.15319523215293884\n",
            "step: 200, loss: 0.0805501714348793\n",
            "step: 210, loss: 0.08823201060295105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6273764258555133, f1=0.6412825651302605, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11995422095060349\n",
            "step: 10, loss: 0.059032220393419266\n",
            "step: 20, loss: 0.07176954299211502\n",
            "step: 30, loss: 0.06019839271903038\n",
            "step: 40, loss: 0.024808313697576523\n",
            "step: 50, loss: 0.18353614211082458\n",
            "step: 60, loss: 0.2340761125087738\n",
            "step: 70, loss: 0.23058989644050598\n",
            "step: 80, loss: 0.09006053954362869\n",
            "step: 90, loss: 0.05483076721429825\n",
            "step: 100, loss: 0.19351772964000702\n",
            "step: 110, loss: 0.13253122568130493\n",
            "step: 120, loss: 0.1307401955127716\n",
            "step: 130, loss: 0.12883317470550537\n",
            "step: 140, loss: 0.08895077556371689\n",
            "step: 150, loss: 0.023212725296616554\n",
            "step: 160, loss: 0.050493981689214706\n",
            "step: 170, loss: 0.07662392407655716\n",
            "step: 180, loss: 0.26482880115509033\n",
            "step: 190, loss: 0.07217232882976532\n",
            "step: 200, loss: 0.15220418572425842\n",
            "step: 210, loss: 0.12020494788885117\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6213991769547326, f1=0.6279569892473118, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11448584496974945\n",
            "step: 10, loss: 0.046234145760536194\n",
            "step: 20, loss: 0.2947741150856018\n",
            "step: 30, loss: 0.08354359865188599\n",
            "step: 40, loss: 0.06284034997224808\n",
            "step: 50, loss: 0.24151821434497833\n",
            "step: 60, loss: 0.2293650209903717\n",
            "step: 70, loss: 0.22479480504989624\n",
            "step: 80, loss: 0.24923983216285706\n",
            "step: 90, loss: 0.08424267172813416\n",
            "step: 100, loss: 0.024092217907309532\n",
            "step: 110, loss: 0.3609181046485901\n",
            "step: 120, loss: 0.19254416227340698\n",
            "step: 130, loss: 0.10329414159059525\n",
            "step: 140, loss: 0.117304727435112\n",
            "step: 150, loss: 0.2606677711009979\n",
            "step: 160, loss: 0.10258123278617859\n",
            "step: 170, loss: 0.10274742543697357\n",
            "step: 180, loss: 0.1348002403974533\n",
            "step: 190, loss: 0.022876542061567307\n",
            "step: 200, loss: 0.05991535633802414\n",
            "step: 210, loss: 0.00906207226216793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6203007518796992, f1=0.625, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04331550374627113\n",
            "step: 10, loss: 0.07968304306268692\n",
            "step: 20, loss: 0.032599128782749176\n",
            "step: 30, loss: 0.0024082453455775976\n",
            "step: 40, loss: 0.060487568378448486\n",
            "step: 50, loss: 0.021975858137011528\n",
            "step: 60, loss: 0.15264420211315155\n",
            "step: 70, loss: 0.007188237737864256\n",
            "step: 80, loss: 0.10273823142051697\n",
            "step: 90, loss: 0.195957213640213\n",
            "step: 100, loss: 0.00803341530263424\n",
            "step: 110, loss: 0.07057903707027435\n",
            "step: 120, loss: 0.050091709941625595\n",
            "step: 130, loss: 0.046031128615140915\n",
            "step: 140, loss: 0.08132356405258179\n",
            "step: 150, loss: 0.039233241230249405\n",
            "step: 160, loss: 0.021033912897109985\n",
            "step: 170, loss: 0.08800260722637177\n",
            "step: 180, loss: 0.010479362681508064\n",
            "step: 190, loss: 0.07149553298950195\n",
            "step: 200, loss: 0.02541089989244938\n",
            "step: 210, loss: 0.0052398014813661575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6243194192377496, f1=0.6497064579256361, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006910440046340227\n",
            "step: 10, loss: 0.04854799807071686\n",
            "step: 20, loss: 0.006619859021157026\n",
            "step: 30, loss: 0.011738290078938007\n",
            "step: 40, loss: 0.11927629262208939\n",
            "step: 50, loss: 0.10439682006835938\n",
            "step: 60, loss: 0.048819173127412796\n",
            "step: 70, loss: 0.013168424367904663\n",
            "step: 80, loss: 0.03828953579068184\n",
            "step: 90, loss: 0.15870708227157593\n",
            "step: 100, loss: 0.039869654923677444\n",
            "step: 110, loss: 0.18083828687667847\n",
            "step: 120, loss: 0.06951256096363068\n",
            "step: 130, loss: 0.2034972906112671\n",
            "step: 140, loss: 0.13993923366069794\n",
            "step: 150, loss: 0.01416271273046732\n",
            "step: 160, loss: 0.040799420326948166\n",
            "step: 170, loss: 0.01577628217637539\n",
            "step: 180, loss: 0.07724988460540771\n",
            "step: 190, loss: 0.12268868088722229\n",
            "step: 200, loss: 0.03923599794507027\n",
            "step: 210, loss: 0.02939586341381073\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6116838487972508, f1=0.59391771019678, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06745665520429611\n",
            "step: 10, loss: 0.06572891771793365\n",
            "step: 20, loss: 0.012342211790382862\n",
            "step: 30, loss: 0.08709292858839035\n",
            "step: 40, loss: 0.025871656835079193\n",
            "step: 50, loss: 0.0251813605427742\n",
            "step: 60, loss: 0.06988243013620377\n",
            "step: 70, loss: 0.02601679600775242\n",
            "step: 80, loss: 0.023583395406603813\n",
            "step: 90, loss: 0.007538295350968838\n",
            "step: 100, loss: 0.09433901309967041\n",
            "step: 110, loss: 0.06031635031104088\n",
            "step: 120, loss: 0.0036945920437574387\n",
            "step: 130, loss: 0.003519246820360422\n",
            "step: 140, loss: 0.08927781879901886\n",
            "step: 150, loss: 0.0037123365327715874\n",
            "step: 160, loss: 0.017537791281938553\n",
            "step: 170, loss: 0.06770670413970947\n",
            "step: 180, loss: 0.0931725725531578\n",
            "step: 190, loss: 0.016220007091760635\n",
            "step: 200, loss: 0.05031910911202431\n",
            "step: 210, loss: 0.12452200800180435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6135922330097088, f1=0.6134969325153374, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00246094330213964\n",
            "step: 10, loss: 0.10758835822343826\n",
            "step: 20, loss: 0.033909041434526443\n",
            "step: 30, loss: 0.047613371163606644\n",
            "step: 40, loss: 0.19224314391613007\n",
            "step: 50, loss: 0.08148784190416336\n",
            "step: 60, loss: 0.14618459343910217\n",
            "step: 70, loss: 0.00969864521175623\n",
            "step: 80, loss: 0.06483925879001617\n",
            "step: 90, loss: 0.004139646887779236\n",
            "step: 100, loss: 0.001787970308214426\n",
            "step: 110, loss: 0.016557883471250534\n",
            "step: 120, loss: 0.017126860097050667\n",
            "step: 130, loss: 0.030833708122372627\n",
            "step: 140, loss: 0.029075928032398224\n",
            "step: 150, loss: 0.3052505850791931\n",
            "step: 160, loss: 0.008951952680945396\n",
            "step: 170, loss: 0.13689586520195007\n",
            "step: 180, loss: 0.03422942012548447\n",
            "step: 190, loss: 0.0014097891980782151\n",
            "step: 200, loss: 0.013234484940767288\n",
            "step: 210, loss: 0.006467558443546295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6204238921001927, f1=0.6061855670103093, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01427327562123537\n",
            "step: 10, loss: 0.27994051575660706\n",
            "step: 20, loss: 0.001233346643857658\n",
            "step: 30, loss: 0.04699354246258736\n",
            "step: 40, loss: 0.0007948959246277809\n",
            "step: 50, loss: 0.003112548030912876\n",
            "step: 60, loss: 0.02149033360183239\n",
            "step: 70, loss: 0.06540756672620773\n",
            "step: 80, loss: 0.012947739101946354\n",
            "step: 90, loss: 0.03781115263700485\n",
            "step: 100, loss: 0.04017501324415207\n",
            "step: 110, loss: 0.010996662080287933\n",
            "step: 120, loss: 0.01174743752926588\n",
            "step: 130, loss: 0.027638860046863556\n",
            "step: 140, loss: 0.0011542070424184203\n",
            "step: 150, loss: 0.16462579369544983\n",
            "step: 160, loss: 0.06449762731790543\n",
            "step: 170, loss: 0.004935525357723236\n",
            "step: 180, loss: 0.21298734843730927\n",
            "step: 190, loss: 0.08709767460823059\n",
            "step: 200, loss: 0.012541464529931545\n",
            "step: 210, loss: 0.13217133283615112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5826771653543307, f1=0.6012526096033404, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02400214783847332\n",
            "step: 10, loss: 0.029684890061616898\n",
            "step: 20, loss: 0.014870557002723217\n",
            "step: 30, loss: 0.0008163708262145519\n",
            "step: 40, loss: 0.07255128771066666\n",
            "step: 50, loss: 0.0030096168629825115\n",
            "step: 60, loss: 0.10529438406229019\n",
            "step: 70, loss: 0.0033130634110420942\n",
            "step: 80, loss: 0.07199306041002274\n",
            "step: 90, loss: 0.008342739194631577\n",
            "step: 100, loss: 0.10553369671106339\n",
            "step: 110, loss: 0.002644707215949893\n",
            "step: 120, loss: 0.01621069200336933\n",
            "step: 130, loss: 0.006892361678183079\n",
            "step: 140, loss: 0.00349289714358747\n",
            "step: 150, loss: 0.0045784576795995235\n",
            "step: 160, loss: 0.0012221290962770581\n",
            "step: 170, loss: 0.0036168701481074095\n",
            "step: 180, loss: 0.004953187424689531\n",
            "step: 190, loss: 0.0024082637391984463\n",
            "step: 200, loss: 0.0018022630829364061\n",
            "step: 210, loss: 0.0121039729565382\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6035502958579881, f1=0.6157112526539279, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0049063158221542835\n",
            "step: 10, loss: 0.002793485764414072\n",
            "step: 20, loss: 0.01104117464274168\n",
            "step: 30, loss: 0.04448234289884567\n",
            "step: 40, loss: 0.18234257400035858\n",
            "step: 50, loss: 0.007332105189561844\n",
            "step: 60, loss: 0.02411622740328312\n",
            "step: 70, loss: 0.003658879781141877\n",
            "step: 80, loss: 0.001604176126420498\n",
            "step: 90, loss: 0.023240922018885612\n",
            "step: 100, loss: 0.007232555653899908\n",
            "step: 110, loss: 0.019063841551542282\n",
            "step: 120, loss: 0.010915701277554035\n",
            "step: 130, loss: 0.005922381300479174\n",
            "step: 140, loss: 0.00047436487511731684\n",
            "step: 150, loss: 0.014637229964137077\n",
            "step: 160, loss: 0.059326671063899994\n",
            "step: 170, loss: 0.006898671854287386\n",
            "step: 180, loss: 0.0029452317394316196\n",
            "step: 190, loss: 0.004076004959642887\n",
            "step: 200, loss: 0.012017332017421722\n",
            "step: 210, loss: 0.021309897303581238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.592734225621415, f1=0.6088709677419355, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09238404780626297\n",
            "step: 10, loss: 0.017514409497380257\n",
            "step: 20, loss: 0.04014328494668007\n",
            "step: 30, loss: 0.100232794880867\n",
            "step: 40, loss: 0.0030118864960968494\n",
            "step: 50, loss: 0.18575164675712585\n",
            "step: 60, loss: 0.004096794407814741\n",
            "step: 70, loss: 0.014701404608786106\n",
            "step: 80, loss: 0.013660695403814316\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.0014107715105637908\n",
            "step: 100, loss: 0.0012931954115629196\n",
            "step: 110, loss: 0.0030788076110184193\n",
            "step: 120, loss: 0.005915038753300905\n",
            "step: 130, loss: 0.032180674374103546\n",
            "step: 140, loss: 0.01194145530462265\n",
            "step: 150, loss: 0.013425626792013645\n",
            "step: 160, loss: 0.010564970783889294\n",
            "step: 170, loss: 0.0019470155239105225\n",
            "step: 180, loss: 0.129952073097229\n",
            "step: 190, loss: 0.013345077633857727\n",
            "step: 200, loss: 0.0022687159944325686\n",
            "step: 210, loss: 0.004499695263803005\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5950095969289827, f1=0.601593625498008, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007945649558678269\n",
            "step: 10, loss: 0.009926346130669117\n",
            "step: 20, loss: 0.0064526633359491825\n",
            "step: 30, loss: 0.003687534248456359\n",
            "step: 40, loss: 0.0035970236640423536\n",
            "step: 50, loss: 0.0034843445755541325\n",
            "step: 60, loss: 0.0021747162099927664\n",
            "step: 70, loss: 0.025172870606184006\n",
            "step: 80, loss: 0.01103243138641119\n",
            "step: 90, loss: 0.0207555890083313\n",
            "step: 100, loss: 0.019083837047219276\n",
            "step: 110, loss: 0.0014865281991660595\n",
            "step: 120, loss: 0.020500605925917625\n",
            "step: 130, loss: 0.0017621406586840749\n",
            "step: 140, loss: 0.032581206411123276\n",
            "step: 150, loss: 0.0093985041603446\n",
            "step: 160, loss: 0.016708925366401672\n",
            "step: 170, loss: 0.021064331755042076\n",
            "step: 180, loss: 0.0016234348295256495\n",
            "step: 190, loss: 0.0019423733465373516\n",
            "step: 200, loss: 0.03774555027484894\n",
            "step: 210, loss: 0.017470024526119232\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6040515653775322, f1=0.607843137254902, best_f1=0.6412825651302605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004411890637129545\n",
            "step: 10, loss: 0.001025805831886828\n",
            "step: 20, loss: 0.007354225032031536\n",
            "step: 30, loss: 0.02948819287121296\n",
            "step: 40, loss: 0.001547246240079403\n",
            "step: 50, loss: 0.0012607157696038485\n",
            "step: 60, loss: 0.28747981786727905\n",
            "step: 70, loss: 0.0008451069006696343\n",
            "step: 80, loss: 0.001054777530953288\n",
            "step: 90, loss: 0.01243531983345747\n",
            "step: 100, loss: 0.003116788575425744\n",
            "step: 110, loss: 0.06164657324552536\n",
            "step: 120, loss: 0.004041338339447975\n",
            "step: 130, loss: 0.3406500220298767\n",
            "step: 140, loss: 0.002521826885640621\n",
            "step: 150, loss: 0.0009397291578352451\n",
            "step: 160, loss: 0.015117661096155643\n",
            "step: 170, loss: 0.004524680320173502\n",
            "step: 180, loss: 0.0003568529209587723\n",
            "step: 190, loss: 0.0033984980545938015\n",
            "step: 200, loss: 0.01065011601895094\n",
            "step: 210, loss: 0.007974245585501194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6003683241252302, f1=0.6062992125984252, best_f1=0.6412825651302605\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 261.13it/s]\n",
            "load_f1 = 0.6290322580645161\n",
            "real_f1 = 0.6285714285714287\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 188.81it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewoOK8t9eDFM"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJDquM2Oe05D",
        "outputId": "b0ac3304-af09-41ce-96f5-8fbba2f11454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5747236013412476\n",
            "step: 10, loss: 0.3638470470905304\n",
            "step: 20, loss: 0.2946171164512634\n",
            "step: 30, loss: 0.4313517212867737\n",
            "step: 40, loss: 0.431461900472641\n",
            "step: 50, loss: 0.2898389995098114\n",
            "step: 60, loss: 0.26397329568862915\n",
            "step: 70, loss: 0.2432250827550888\n",
            "step: 80, loss: 0.2559962570667267\n",
            "step: 90, loss: 0.29242852330207825\n",
            "step: 100, loss: 0.3037935495376587\n",
            "step: 110, loss: 0.5626217126846313\n",
            "step: 120, loss: 0.14348027110099792\n",
            "step: 130, loss: 0.13336396217346191\n",
            "step: 140, loss: 0.07332278788089752\n",
            "step: 150, loss: 0.15631060302257538\n",
            "step: 160, loss: 0.14427506923675537\n",
            "step: 170, loss: 0.22846178710460663\n",
            "step: 180, loss: 0.10320916771888733\n",
            "step: 190, loss: 0.2561503052711487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7055702917771883, f1=0.7112299465240641, best_f1=0.7112299465240641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.295270174741745\n",
            "step: 10, loss: 0.09230179339647293\n",
            "step: 20, loss: 0.21448473632335663\n",
            "step: 30, loss: 0.11265292763710022\n",
            "step: 40, loss: 0.1952105015516281\n",
            "step: 50, loss: 0.02495179884135723\n",
            "step: 60, loss: 0.15653595328330994\n",
            "step: 70, loss: 0.2365403026342392\n",
            "step: 80, loss: 0.14229846000671387\n",
            "step: 90, loss: 0.15057721734046936\n",
            "step: 100, loss: 0.015796279534697533\n",
            "step: 110, loss: 0.1645171046257019\n",
            "step: 120, loss: 0.31518158316612244\n",
            "step: 130, loss: 0.04722382128238678\n",
            "step: 140, loss: 0.06716885417699814\n",
            "step: 150, loss: 0.11182794719934464\n",
            "step: 160, loss: 0.13478714227676392\n",
            "step: 170, loss: 0.09588626772165298\n",
            "step: 180, loss: 0.10214047878980637\n",
            "step: 190, loss: 0.025740914046764374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7589498806682579, f1=0.7310344827586207, best_f1=0.7310344827586207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03886287286877632\n",
            "step: 10, loss: 0.165261909365654\n",
            "step: 20, loss: 0.12429823726415634\n",
            "step: 30, loss: 0.02047938108444214\n",
            "step: 40, loss: 0.031813520938158035\n",
            "step: 50, loss: 0.08257211744785309\n",
            "step: 60, loss: 0.06522100418806076\n",
            "step: 70, loss: 0.14192283153533936\n",
            "step: 80, loss: 0.07181935012340546\n",
            "step: 90, loss: 0.10299234837293625\n",
            "step: 100, loss: 0.10679931938648224\n",
            "step: 110, loss: 0.022499749436974525\n",
            "step: 120, loss: 0.10645530372858047\n",
            "step: 130, loss: 0.022092550992965698\n",
            "step: 140, loss: 0.016562819480895996\n",
            "step: 150, loss: 0.13293829560279846\n",
            "step: 160, loss: 0.032913465052843094\n",
            "step: 170, loss: 0.13016541302204132\n",
            "step: 180, loss: 0.061252661049366\n",
            "step: 190, loss: 0.05525524169206619\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7603305785123966, f1=0.7645429362880887, best_f1=0.7645429362880887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03176116570830345\n",
            "step: 10, loss: 0.11215424537658691\n",
            "step: 20, loss: 0.008654404431581497\n",
            "step: 30, loss: 0.011429822072386742\n",
            "step: 40, loss: 0.022609923034906387\n",
            "step: 50, loss: 0.06457199901342392\n",
            "step: 60, loss: 0.014971532858908176\n",
            "step: 70, loss: 0.23135961592197418\n",
            "step: 80, loss: 0.15465039014816284\n",
            "step: 90, loss: 0.016582228243350983\n",
            "step: 100, loss: 0.0705464705824852\n",
            "step: 110, loss: 0.00744925532490015\n",
            "step: 120, loss: 0.15323418378829956\n",
            "step: 130, loss: 0.1531699001789093\n",
            "step: 140, loss: 0.015389365144073963\n",
            "step: 150, loss: 0.037868086248636246\n",
            "step: 160, loss: 0.056008562445640564\n",
            "step: 170, loss: 0.08883054554462433\n",
            "step: 180, loss: 0.01867492124438286\n",
            "step: 190, loss: 0.28805580735206604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7765957446808511, f1=0.7708894878706198, best_f1=0.7708894878706198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11971718072891235\n",
            "step: 10, loss: 0.029741816222667694\n",
            "step: 20, loss: 0.05840824916958809\n",
            "step: 30, loss: 0.007809980306774378\n",
            "step: 40, loss: 0.053557198494672775\n",
            "step: 50, loss: 0.11741618067026138\n",
            "step: 60, loss: 0.13429324328899384\n",
            "step: 70, loss: 0.025861380621790886\n",
            "step: 80, loss: 0.05446785315871239\n",
            "step: 90, loss: 0.008069389499723911\n",
            "step: 100, loss: 0.0036188322119414806\n",
            "step: 110, loss: 0.013332745060324669\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.14618515968322754\n",
            "step: 130, loss: 0.041115809231996536\n",
            "step: 140, loss: 0.018154626712203026\n",
            "step: 150, loss: 0.007402552291750908\n",
            "step: 160, loss: 0.08700843900442123\n",
            "step: 170, loss: 0.013060525991022587\n",
            "step: 180, loss: 0.01994853839278221\n",
            "step: 190, loss: 0.11875659227371216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7456647398843931, f1=0.7603305785123966, best_f1=0.7708894878706198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009288841858506203\n",
            "step: 10, loss: 0.15127800405025482\n",
            "step: 20, loss: 0.019703608006238937\n",
            "step: 30, loss: 0.0009264039108529687\n",
            "step: 40, loss: 0.11951709538698196\n",
            "step: 50, loss: 0.15691593289375305\n",
            "step: 60, loss: 0.049912914633750916\n",
            "step: 70, loss: 0.05108461156487465\n",
            "step: 80, loss: 0.011548801325261593\n",
            "step: 90, loss: 0.006949618458747864\n",
            "step: 100, loss: 0.0012812769273295999\n",
            "step: 110, loss: 0.018597444519400597\n",
            "step: 120, loss: 0.007441353052854538\n",
            "step: 130, loss: 0.03729420527815819\n",
            "step: 140, loss: 0.1789143681526184\n",
            "step: 150, loss: 0.004625928122550249\n",
            "step: 160, loss: 0.04423690587282181\n",
            "step: 170, loss: 0.04422051087021828\n",
            "step: 180, loss: 0.018787488341331482\n",
            "step: 190, loss: 0.2225644290447235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7590027700831025, f1=0.765498652291105, best_f1=0.7708894878706198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024834826588630676\n",
            "step: 10, loss: 0.018124382942914963\n",
            "step: 20, loss: 0.11240343004465103\n",
            "step: 30, loss: 0.007269229274243116\n",
            "step: 40, loss: 0.10424968600273132\n",
            "step: 50, loss: 0.05602165311574936\n",
            "step: 60, loss: 0.01984022930264473\n",
            "step: 70, loss: 0.004588773939758539\n",
            "step: 80, loss: 0.06548745185136795\n",
            "step: 90, loss: 0.014920404180884361\n",
            "step: 100, loss: 0.0018296337220817804\n",
            "step: 110, loss: 0.021954642608761787\n",
            "step: 120, loss: 0.0019309065537527204\n",
            "step: 130, loss: 0.049784742295742035\n",
            "step: 140, loss: 0.083681620657444\n",
            "step: 150, loss: 0.01589987613260746\n",
            "step: 160, loss: 0.0318194255232811\n",
            "step: 170, loss: 0.0017467293655499816\n",
            "step: 180, loss: 0.006374107673764229\n",
            "step: 190, loss: 0.0030625779181718826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.783289817232376, f1=0.7864583333333334, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05195891857147217\n",
            "step: 10, loss: 0.05226174741983414\n",
            "step: 20, loss: 0.0012838663533329964\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.0018832386704161763\n",
            "step: 40, loss: 0.0009554364951327443\n",
            "step: 50, loss: 0.00393577991053462\n",
            "step: 60, loss: 0.001337807858362794\n",
            "step: 70, loss: 0.004316406790167093\n",
            "step: 80, loss: 0.00043436125270090997\n",
            "step: 90, loss: 0.004922202322632074\n",
            "step: 100, loss: 0.037031255662441254\n",
            "step: 110, loss: 0.0012711520539596677\n",
            "step: 120, loss: 0.009230081923305988\n",
            "step: 130, loss: 0.04704660177230835\n",
            "step: 140, loss: 0.048279065638780594\n",
            "step: 150, loss: 0.08298180997371674\n",
            "step: 160, loss: 0.005188571754842997\n",
            "step: 170, loss: 0.003530934453010559\n",
            "step: 180, loss: 0.03257189691066742\n",
            "step: 190, loss: 0.03970470279455185\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7450980392156863, f1=0.7814207650273224, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004016768652945757\n",
            "step: 10, loss: 0.053457748144865036\n",
            "step: 20, loss: 0.0029473016038537025\n",
            "step: 30, loss: 0.004308617208153009\n",
            "step: 40, loss: 0.012330293655395508\n",
            "step: 50, loss: 0.00314158177934587\n",
            "step: 60, loss: 0.0009170083794742823\n",
            "step: 70, loss: 0.0009093850967474282\n",
            "step: 80, loss: 0.0038862922228872776\n",
            "step: 90, loss: 0.047834884375333786\n",
            "step: 100, loss: 0.0035457261838018894\n",
            "step: 110, loss: 0.0014239661395549774\n",
            "step: 120, loss: 0.015846529975533485\n",
            "step: 130, loss: 0.0025319410488009453\n",
            "step: 140, loss: 0.01223894115537405\n",
            "step: 150, loss: 0.06211300194263458\n",
            "step: 160, loss: 0.0006073106196708977\n",
            "step: 170, loss: 0.003031275002285838\n",
            "step: 180, loss: 0.0031873853877186775\n",
            "step: 190, loss: 0.002588848117738962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7357512953367874, f1=0.7633587786259541, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006958133541047573\n",
            "step: 10, loss: 0.018991561606526375\n",
            "step: 20, loss: 0.038469646126031876\n",
            "step: 30, loss: 0.0006795639055781066\n",
            "step: 40, loss: 0.030146148055791855\n",
            "step: 50, loss: 0.0009015586110763252\n",
            "step: 60, loss: 0.004854666534811258\n",
            "step: 70, loss: 0.0018037321278825402\n",
            "step: 80, loss: 0.0008089960319921374\n",
            "step: 90, loss: 0.00399294076487422\n",
            "step: 100, loss: 0.062265701591968536\n",
            "step: 110, loss: 0.0023801412899047136\n",
            "step: 120, loss: 0.0357554629445076\n",
            "step: 130, loss: 0.0018132735276594758\n",
            "step: 140, loss: 0.0007521822117269039\n",
            "step: 150, loss: 0.0020835495088249445\n",
            "step: 160, loss: 0.004574147053062916\n",
            "step: 170, loss: 0.001317595480941236\n",
            "step: 180, loss: 0.0005500927800312638\n",
            "step: 190, loss: 0.01732248067855835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7566137566137565, f1=0.783289817232376, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003821968275588006\n",
            "step: 10, loss: 0.0019804849289357662\n",
            "step: 20, loss: 0.03550595045089722\n",
            "step: 30, loss: 0.0260370634496212\n",
            "step: 40, loss: 0.0034650007728487253\n",
            "step: 50, loss: 0.0033667266834527254\n",
            "step: 60, loss: 0.00043802594882436097\n",
            "step: 70, loss: 0.004694852977991104\n",
            "step: 80, loss: 0.0008801307994872332\n",
            "step: 90, loss: 0.002265841932967305\n",
            "step: 100, loss: 0.0003444919129833579\n",
            "step: 110, loss: 0.0013234724756330252\n",
            "step: 120, loss: 0.0026577834505587816\n",
            "step: 130, loss: 0.005270994268357754\n",
            "step: 140, loss: 0.0004575580242089927\n",
            "step: 150, loss: 0.0008810079889371991\n",
            "step: 160, loss: 0.004362972918897867\n",
            "step: 170, loss: 0.0005695278523489833\n",
            "step: 180, loss: 0.0010966291883960366\n",
            "step: 190, loss: 0.001476907404139638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7377049180327869, f1=0.7828418230563003, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039927856414578855\n",
            "step: 10, loss: 0.015712466090917587\n",
            "step: 20, loss: 0.0030201047193259\n",
            "step: 30, loss: 0.0008365672547370195\n",
            "step: 40, loss: 0.0017339398618787527\n",
            "step: 50, loss: 0.03452081233263016\n",
            "step: 60, loss: 0.0007609252352267504\n",
            "step: 70, loss: 0.0013077151961624622\n",
            "step: 80, loss: 0.0010516950860619545\n",
            "step: 90, loss: 0.0009636275353841484\n",
            "step: 100, loss: 0.0015973594272509217\n",
            "step: 110, loss: 0.0022747728507965803\n",
            "step: 120, loss: 0.03926264867186546\n",
            "step: 130, loss: 0.006297870073467493\n",
            "step: 140, loss: 0.0012183568906039\n",
            "step: 150, loss: 0.0005265043000690639\n",
            "step: 160, loss: 0.00022966426331549883\n",
            "step: 170, loss: 0.0006011102814227343\n",
            "step: 180, loss: 0.00021377943630795926\n",
            "step: 190, loss: 0.0008079196559265256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7553191489361701, f1=0.7862796833773087, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00101335602812469\n",
            "step: 10, loss: 0.0012214835733175278\n",
            "step: 20, loss: 0.00044338623411022127\n",
            "step: 30, loss: 0.0008343024528585374\n",
            "step: 40, loss: 0.0003450032672844827\n",
            "step: 50, loss: 0.027049703523516655\n",
            "step: 60, loss: 0.0004750251828227192\n",
            "step: 70, loss: 0.004563787952065468\n",
            "step: 80, loss: 0.0008668092777952552\n",
            "step: 90, loss: 0.0006108786328695714\n",
            "step: 100, loss: 0.0003043382312171161\n",
            "step: 110, loss: 0.16960668563842773\n",
            "step: 120, loss: 0.007802753243595362\n",
            "step: 130, loss: 0.000542774680070579\n",
            "step: 140, loss: 0.0004172123153693974\n",
            "step: 150, loss: 0.0010260781273245811\n",
            "step: 160, loss: 0.0010999090736731887\n",
            "step: 170, loss: 0.00340235885232687\n",
            "step: 180, loss: 0.0005073210340924561\n",
            "step: 190, loss: 0.009664095006883144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7407407407407407, f1=0.7835616438356164, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02005401812493801\n",
            "step: 10, loss: 0.0008349038544110954\n",
            "step: 20, loss: 0.0002009925083257258\n",
            "step: 30, loss: 0.0003266129642724991\n",
            "step: 40, loss: 0.0008473624475300312\n",
            "step: 50, loss: 0.0008258640882559121\n",
            "step: 60, loss: 0.0007931379368528724\n",
            "step: 70, loss: 0.0006780883413739502\n",
            "step: 80, loss: 0.0012964335037395358\n",
            "step: 90, loss: 0.11589497327804565\n",
            "step: 100, loss: 0.03202691674232483\n",
            "step: 110, loss: 0.0023480383679270744\n",
            "step: 120, loss: 0.0012164578074589372\n",
            "step: 130, loss: 0.0010739531135186553\n",
            "step: 140, loss: 0.0008553519146516919\n",
            "step: 150, loss: 0.0003947324585169554\n",
            "step: 160, loss: 0.0016354481922462583\n",
            "step: 170, loss: 0.006974177900701761\n",
            "step: 180, loss: 0.00349019980058074\n",
            "step: 190, loss: 0.0008975538075901568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7409638554216867, f1=0.7610619469026548, best_f1=0.7864583333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004958240315318108\n",
            "step: 10, loss: 0.00043410234502516687\n",
            "step: 20, loss: 0.0009233352611772716\n",
            "step: 30, loss: 0.0007016299059614539\n",
            "step: 40, loss: 0.0015881785657256842\n",
            "step: 50, loss: 0.003964090254157782\n",
            "step: 60, loss: 0.006848291028290987\n",
            "step: 70, loss: 0.01478884182870388\n",
            "step: 80, loss: 0.0016825946513563395\n",
            "step: 90, loss: 0.003822986502200365\n",
            "step: 100, loss: 0.00033926713513210416\n",
            "step: 110, loss: 0.0016910646809265018\n",
            "step: 120, loss: 0.0015290766023099422\n",
            "step: 130, loss: 0.0005379270878620446\n",
            "step: 140, loss: 0.004306087736040354\n",
            "step: 150, loss: 0.0004184666904620826\n",
            "step: 160, loss: 0.00046936215949244797\n",
            "step: 170, loss: 0.0005404283874668181\n",
            "step: 180, loss: 0.004987275693565607\n",
            "step: 190, loss: 0.0008709009271115065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7388888888888889, f1=0.7780821917808218, best_f1=0.7864583333333334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 164.93it/s]\n",
            "load_f1 = 0.5522388059701493\n",
            "real_f1 = 0.5253623188405796\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 186.13it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      },
      "source": [
        "## DA TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1O9a5RjeDtU"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2WpDwuee1mM",
        "outputId": "920884dd-4438-4f6b-f202-870744fd70ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6160624623298645\n",
            "step: 10, loss: 0.3578070104122162\n",
            "step: 20, loss: 0.3031207323074341\n",
            "step: 30, loss: 0.40407049655914307\n",
            "step: 40, loss: 0.2734251320362091\n",
            "step: 50, loss: 0.23552265763282776\n",
            "step: 60, loss: 0.32287082076072693\n",
            "step: 70, loss: 0.37386763095855713\n",
            "step: 80, loss: 0.3643961250782013\n",
            "step: 90, loss: 0.2786964178085327\n",
            "step: 100, loss: 0.17185182869434357\n",
            "step: 110, loss: 0.2549002170562744\n",
            "step: 120, loss: 0.23130127787590027\n",
            "step: 130, loss: 0.07773682475090027\n",
            "step: 140, loss: 0.20865677297115326\n",
            "step: 150, loss: 0.32827886939048767\n",
            "step: 160, loss: 0.18956556916236877\n",
            "step: 170, loss: 0.1797092705965042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6635730858468678, f1=0.6823529411764706, best_f1=0.6823529411764706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1147431880235672\n",
            "step: 10, loss: 0.17865954339504242\n",
            "step: 20, loss: 0.0471271313726902\n",
            "step: 30, loss: 0.15875324606895447\n",
            "step: 40, loss: 0.14253467321395874\n",
            "step: 50, loss: 0.218867227435112\n",
            "step: 60, loss: 0.07670901715755463\n",
            "step: 70, loss: 0.2089819759130478\n",
            "step: 80, loss: 0.11561757326126099\n",
            "step: 90, loss: 0.13069388270378113\n",
            "step: 100, loss: 0.2010175883769989\n",
            "step: 110, loss: 0.07862098515033722\n",
            "step: 120, loss: 0.08925696462392807\n",
            "step: 130, loss: 0.10106905549764633\n",
            "step: 140, loss: 0.2541296184062958\n",
            "step: 150, loss: 0.20542685687541962\n",
            "step: 160, loss: 0.2103762924671173\n",
            "step: 170, loss: 0.08257003873586655\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7563451776649747, f1=0.784503631961259, best_f1=0.784503631961259\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14385943114757538\n",
            "step: 10, loss: 0.03141995146870613\n",
            "step: 20, loss: 0.038731470704078674\n",
            "step: 30, loss: 0.15978626906871796\n",
            "step: 40, loss: 0.10911650955677032\n",
            "step: 50, loss: 0.14847300946712494\n",
            "step: 60, loss: 0.14721040427684784\n",
            "step: 70, loss: 0.0734223797917366\n",
            "step: 80, loss: 0.04688633233308792\n",
            "step: 90, loss: 0.11484850943088531\n",
            "step: 100, loss: 0.016970839351415634\n",
            "step: 110, loss: 0.09403610229492188\n",
            "step: 120, loss: 0.13487356901168823\n",
            "step: 130, loss: 0.10617193579673767\n",
            "step: 140, loss: 0.11184470355510712\n",
            "step: 150, loss: 0.030839012935757637\n",
            "step: 160, loss: 0.03724424168467522\n",
            "step: 170, loss: 0.10294406116008759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.772378516624041, f1=0.7884615384615384, best_f1=0.7884615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02860618382692337\n",
            "step: 10, loss: 0.025852227583527565\n",
            "step: 20, loss: 0.0023072666954249144\n",
            "step: 30, loss: 0.10964122414588928\n",
            "step: 40, loss: 0.006808501202613115\n",
            "step: 50, loss: 0.025144772604107857\n",
            "step: 60, loss: 0.12931177020072937\n",
            "step: 70, loss: 0.0035351132974028587\n",
            "step: 80, loss: 0.017634117975831032\n",
            "step: 90, loss: 0.06205407530069351\n",
            "step: 100, loss: 0.1772691160440445\n",
            "step: 110, loss: 0.06961572170257568\n",
            "step: 120, loss: 0.021637756377458572\n",
            "step: 130, loss: 0.027258306741714478\n",
            "step: 140, loss: 0.014280815608799458\n",
            "step: 150, loss: 0.08033650368452072\n",
            "step: 160, loss: 0.19547638297080994\n",
            "step: 170, loss: 0.05943191424012184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7609756097560975, f1=0.7895981087470448, best_f1=0.7884615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025256626307964325\n",
            "step: 10, loss: 0.0074385106563568115\n",
            "step: 20, loss: 0.038938429206609726\n",
            "step: 30, loss: 0.007590157445520163\n",
            "step: 40, loss: 0.026013024151325226\n",
            "step: 50, loss: 0.07185228914022446\n",
            "step: 60, loss: 0.015393208712339401\n",
            "step: 70, loss: 0.13099992275238037\n",
            "step: 80, loss: 0.08710107952356339\n",
            "step: 90, loss: 0.0809776559472084\n",
            "step: 100, loss: 0.027644561603665352\n",
            "step: 110, loss: 0.03333596512675285\n",
            "step: 120, loss: 0.008177747018635273\n",
            "step: 130, loss: 0.050626907497644424\n",
            "step: 140, loss: 0.05312949791550636\n",
            "step: 150, loss: 0.19108740985393524\n",
            "step: 160, loss: 0.09564556926488876\n",
            "step: 170, loss: 0.062960185110569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7692307692307693, f1=0.7769423558897243, best_f1=0.7884615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027413146570324898\n",
            "step: 10, loss: 0.007073075510561466\n",
            "step: 20, loss: 0.012950733304023743\n",
            "step: 30, loss: 0.007242200430482626\n",
            "step: 40, loss: 0.017294298857450485\n",
            "step: 50, loss: 0.11034324765205383\n",
            "step: 60, loss: 0.0037846537306904793\n",
            "step: 70, loss: 0.11395540833473206\n",
            "step: 80, loss: 0.09265533834695816\n",
            "step: 90, loss: 0.03480174019932747\n",
            "step: 100, loss: 0.012932802550494671\n",
            "step: 110, loss: 0.0015487887430936098\n",
            "step: 120, loss: 0.007301964797079563\n",
            "step: 130, loss: 0.01617995835840702\n",
            "step: 140, loss: 0.01805436611175537\n",
            "step: 150, loss: 0.05511849746108055\n",
            "step: 160, loss: 0.1884027123451233\n",
            "step: 170, loss: 0.006385842338204384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7837150127226464, f1=0.7892156862745098, best_f1=0.7892156862745098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00823309924453497\n",
            "step: 10, loss: 0.041872888803482056\n",
            "step: 20, loss: 0.002709451597183943\n",
            "step: 30, loss: 0.012005547061562538\n",
            "step: 40, loss: 0.018291160464286804\n",
            "step: 50, loss: 0.009601768106222153\n",
            "step: 60, loss: 0.0021386633161455393\n",
            "step: 70, loss: 0.18515728414058685\n",
            "step: 80, loss: 0.008033890277147293\n",
            "step: 90, loss: 0.00039678646135143936\n",
            "step: 100, loss: 0.002632558811455965\n",
            "step: 110, loss: 0.0029281540773808956\n",
            "step: 120, loss: 0.0009828683687373996\n",
            "step: 130, loss: 0.14113228023052216\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 140, loss: 0.002929759444668889\n",
            "step: 150, loss: 0.0023013290483504534\n",
            "step: 160, loss: 0.005755018908530474\n",
            "step: 170, loss: 0.04228533059358597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7535545023696684, f1=0.7820224719101124, best_f1=0.7892156862745098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.054257072508335114\n",
            "step: 10, loss: 0.004721847362816334\n",
            "step: 20, loss: 0.000993782072328031\n",
            "step: 30, loss: 0.011911991983652115\n",
            "step: 40, loss: 0.0006604493246413767\n",
            "step: 50, loss: 0.007002763915807009\n",
            "step: 60, loss: 0.000902554311323911\n",
            "step: 70, loss: 0.0021249365527182817\n",
            "step: 80, loss: 0.01895422674715519\n",
            "step: 90, loss: 0.002239004010334611\n",
            "step: 100, loss: 0.0404781736433506\n",
            "step: 110, loss: 0.07070623338222504\n",
            "step: 120, loss: 0.0009899426950141788\n",
            "step: 130, loss: 0.0022594607435166836\n",
            "step: 140, loss: 0.0010425494983792305\n",
            "step: 150, loss: 0.057124823331832886\n",
            "step: 160, loss: 0.021468980237841606\n",
            "step: 170, loss: 0.0010089146671816707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7939698492462312, f1=0.7924528301886793, best_f1=0.7924528301886793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006422956357710063\n",
            "step: 10, loss: 0.017525367438793182\n",
            "step: 20, loss: 0.013099067844450474\n",
            "step: 30, loss: 0.0094574224203825\n",
            "step: 40, loss: 0.005321857053786516\n",
            "step: 50, loss: 0.0004915713798254728\n",
            "step: 60, loss: 0.044678572565317154\n",
            "step: 70, loss: 0.0010431779082864523\n",
            "step: 80, loss: 0.000452656444394961\n",
            "step: 90, loss: 0.01230384036898613\n",
            "step: 100, loss: 0.00665422435849905\n",
            "step: 110, loss: 0.001301756128668785\n",
            "step: 120, loss: 0.01966683752834797\n",
            "step: 130, loss: 0.13909536600112915\n",
            "step: 140, loss: 0.0009327511070296168\n",
            "step: 150, loss: 0.001398555003106594\n",
            "step: 160, loss: 0.030807800590991974\n",
            "step: 170, loss: 0.014537235721945763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7849462365591398, f1=0.7806122448979592, best_f1=0.7924528301886793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020426468923687935\n",
            "step: 10, loss: 0.05994807556271553\n",
            "step: 20, loss: 0.0861298143863678\n",
            "step: 30, loss: 0.003493951866403222\n",
            "step: 40, loss: 0.0005925192963331938\n",
            "step: 50, loss: 0.08383143693208694\n",
            "step: 60, loss: 0.0018264725804328918\n",
            "step: 70, loss: 0.006449682638049126\n",
            "step: 80, loss: 0.016379227861762047\n",
            "step: 90, loss: 0.02923404611647129\n",
            "step: 100, loss: 0.0010468121618032455\n",
            "step: 110, loss: 0.02825297974050045\n",
            "step: 120, loss: 0.010149788111448288\n",
            "step: 130, loss: 0.0006529054371640086\n",
            "step: 140, loss: 0.032737959176301956\n",
            "step: 150, loss: 0.015462280251085758\n",
            "step: 160, loss: 0.10302317142486572\n",
            "step: 170, loss: 0.0004631828924175352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7969151670951157, f1=0.7680798004987531, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03153115510940552\n",
            "step: 10, loss: 0.08350678533315659\n",
            "step: 20, loss: 0.00024414859944954515\n",
            "step: 30, loss: 0.0002358444908168167\n",
            "step: 40, loss: 0.002970406087115407\n",
            "step: 50, loss: 0.011629598215222359\n",
            "step: 60, loss: 0.013318079523742199\n",
            "step: 70, loss: 0.0013037562603130937\n",
            "step: 80, loss: 0.003489808179438114\n",
            "step: 90, loss: 0.00028080071206204593\n",
            "step: 100, loss: 0.0007029577973298728\n",
            "step: 110, loss: 0.047434091567993164\n",
            "step: 120, loss: 0.000543037080205977\n",
            "step: 130, loss: 0.00013988149294164032\n",
            "step: 140, loss: 0.01843039132654667\n",
            "step: 150, loss: 0.054552506655454636\n",
            "step: 160, loss: 0.011115096509456635\n",
            "step: 170, loss: 0.0003409550408832729\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7931034482758621, f1=0.7923627684964201, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03343457728624344\n",
            "step: 10, loss: 0.0005043488345108926\n",
            "step: 20, loss: 0.0015275615733116865\n",
            "step: 30, loss: 0.0032992339693009853\n",
            "step: 40, loss: 0.00020959859830327332\n",
            "step: 50, loss: 0.002728637307882309\n",
            "step: 60, loss: 0.00040208676364272833\n",
            "step: 70, loss: 0.04172198846936226\n",
            "step: 80, loss: 0.0001676696410868317\n",
            "step: 90, loss: 0.001133674755692482\n",
            "step: 100, loss: 0.0008063169079832733\n",
            "step: 110, loss: 0.0005821168306283653\n",
            "step: 120, loss: 0.028084512799978256\n",
            "step: 130, loss: 0.004327119793742895\n",
            "step: 140, loss: 0.0027129563968628645\n",
            "step: 150, loss: 0.0032099010422825813\n",
            "step: 160, loss: 0.0004068036796525121\n",
            "step: 170, loss: 0.0016046181553974748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7883597883597883, f1=0.7774936061381074, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01176624745130539\n",
            "step: 10, loss: 0.0005817834171466529\n",
            "step: 20, loss: 0.0004980841185897589\n",
            "step: 30, loss: 8.16402753116563e-05\n",
            "step: 40, loss: 0.00023512494226451963\n",
            "step: 50, loss: 0.000334053736878559\n",
            "step: 60, loss: 0.0010646425653249025\n",
            "step: 70, loss: 0.012457323260605335\n",
            "step: 80, loss: 0.002598751336336136\n",
            "step: 90, loss: 0.00012721741222776473\n",
            "step: 100, loss: 0.00011289468966424465\n",
            "step: 110, loss: 6.926029891474172e-05\n",
            "step: 120, loss: 0.023852545768022537\n",
            "step: 130, loss: 0.00031276908703148365\n",
            "step: 140, loss: 0.00021103907783981413\n",
            "step: 150, loss: 0.0006746590952388942\n",
            "step: 160, loss: 0.0006202015792950988\n",
            "step: 170, loss: 0.0003393988881725818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7801047120418847, f1=0.7877237851662404, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018694688333198428\n",
            "step: 10, loss: 0.00010724979074439034\n",
            "step: 20, loss: 0.0010057686595246196\n",
            "step: 30, loss: 0.0029675636906176805\n",
            "step: 40, loss: 0.0008308644173666835\n",
            "step: 50, loss: 0.0001815819850889966\n",
            "step: 60, loss: 0.00010043365182355046\n",
            "step: 70, loss: 0.00031637787469662726\n",
            "step: 80, loss: 0.0008953088545240462\n",
            "step: 90, loss: 0.009594021365046501\n",
            "step: 100, loss: 0.015772048383951187\n",
            "step: 110, loss: 0.00015931023517623544\n",
            "step: 120, loss: 0.03544095531105995\n",
            "step: 130, loss: 0.0010173969203606248\n",
            "step: 140, loss: 0.00012654894089791924\n",
            "step: 150, loss: 0.0003564849030226469\n",
            "step: 160, loss: 0.00014504048158414662\n",
            "step: 170, loss: 0.0014452841132879257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.782608695652174, f1=0.7836538461538461, best_f1=0.7680798004987531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002919707912951708\n",
            "step: 10, loss: 0.0024327924475073814\n",
            "step: 20, loss: 0.013726986944675446\n",
            "step: 30, loss: 0.0002483545686118305\n",
            "step: 40, loss: 0.0003644108073785901\n",
            "step: 50, loss: 0.0010198104428127408\n",
            "step: 60, loss: 0.009680924005806446\n",
            "step: 70, loss: 0.00011388528218958527\n",
            "step: 80, loss: 0.005458802450448275\n",
            "step: 90, loss: 0.0008848047000356019\n",
            "step: 100, loss: 0.0048545957542955875\n",
            "step: 110, loss: 0.00011845019616885111\n",
            "step: 120, loss: 0.011036824434995651\n",
            "step: 130, loss: 0.00014448566071223468\n",
            "step: 140, loss: 0.00018600614566821605\n",
            "step: 150, loss: 0.0001583127595949918\n",
            "step: 160, loss: 0.00016376070561818779\n",
            "step: 170, loss: 0.00019657188386190683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.782608695652174, f1=0.7853658536585365, best_f1=0.7680798004987531\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 230.33it/s]\n",
            "load_f1 = 0.5894736842105264\n",
            "real_f1 = 0.5838509316770186\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 186.56it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pmKonkXeD7k"
      },
      "source": [
        "## DA DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRxHd3j2eEH8"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lnLoRSEe2fE",
        "outputId": "4d3f2f8a-8985-4443-fe5e-3d17370b7411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 419kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.64MB/s]\n",
            "Downloading: 100% 440M/440M [00:12<00:00, 35.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6235719919204712\n",
            "step: 10, loss: 0.6312541365623474\n",
            "step: 20, loss: 0.44503018260002136\n",
            "step: 30, loss: 0.1743483543395996\n",
            "step: 40, loss: 0.3155535161495209\n",
            "step: 50, loss: 0.03272319585084915\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.03556232154369354\n",
            "step: 70, loss: 0.07774319499731064\n",
            "step: 80, loss: 0.033296555280685425\n",
            "step: 90, loss: 0.1401262879371643\n",
            "step: 100, loss: 0.008157103322446346\n",
            "step: 110, loss: 0.2631863057613373\n",
            "step: 120, loss: 0.01205198373645544\n",
            "step: 130, loss: 0.06549979001283646\n",
            "step: 140, loss: 0.07244355976581573\n",
            "step: 150, loss: 0.022079085931181908\n",
            "step: 160, loss: 0.008636778220534325\n",
            "step: 170, loss: 0.14245842397212982\n",
            "step: 180, loss: 0.004977049306035042\n",
            "step: 190, loss: 0.022717732936143875\n",
            "step: 200, loss: 0.14678341150283813\n",
            "step: 210, loss: 0.02345052920281887\n",
            "step: 220, loss: 0.03050711192190647\n",
            "step: 230, loss: 0.09031583368778229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9623893805309734, f1=0.9574944071588367, best_f1=0.9574944071588367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013091005384922028\n",
            "step: 10, loss: 0.12677666544914246\n",
            "step: 20, loss: 0.26159656047821045\n",
            "step: 30, loss: 0.21529702842235565\n",
            "step: 40, loss: 0.04898177459836006\n",
            "step: 50, loss: 0.010202880948781967\n",
            "step: 60, loss: 0.04364316910505295\n",
            "step: 70, loss: 0.032218411564826965\n",
            "step: 80, loss: 0.011400094255805016\n",
            "step: 90, loss: 0.027033142745494843\n",
            "step: 100, loss: 0.12784825265407562\n",
            "step: 110, loss: 0.11775511503219604\n",
            "step: 120, loss: 0.010033784434199333\n",
            "step: 130, loss: 0.026946881785988808\n",
            "step: 140, loss: 0.013425392098724842\n",
            "step: 150, loss: 0.015337568707764149\n",
            "step: 160, loss: 0.023819509893655777\n",
            "step: 170, loss: 0.0027103303000330925\n",
            "step: 180, loss: 0.14464601874351501\n",
            "step: 190, loss: 0.010040361434221268\n",
            "step: 200, loss: 0.002524025971069932\n",
            "step: 210, loss: 0.0013327037449926138\n",
            "step: 220, loss: 0.037308190017938614\n",
            "step: 230, loss: 0.05776024982333183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9592760180995475, f1=0.9511918274687856, best_f1=0.9574944071588367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013250243850052357\n",
            "step: 10, loss: 0.004036106634885073\n",
            "step: 20, loss: 0.061369236558675766\n",
            "step: 30, loss: 0.07392865419387817\n",
            "step: 40, loss: 0.12276484817266464\n",
            "step: 50, loss: 0.02685686945915222\n",
            "step: 60, loss: 0.005020324606448412\n",
            "step: 70, loss: 0.012022594921290874\n",
            "step: 80, loss: 0.002182349795475602\n",
            "step: 90, loss: 0.16025806963443756\n",
            "step: 100, loss: 0.0006016682600602508\n",
            "step: 110, loss: 0.0006202809745445848\n",
            "step: 120, loss: 0.030162647366523743\n",
            "step: 130, loss: 0.0005714509170502424\n",
            "step: 140, loss: 0.00392556469887495\n",
            "step: 150, loss: 0.0033767998684197664\n",
            "step: 160, loss: 0.0338832288980484\n",
            "step: 170, loss: 0.08676934242248535\n",
            "step: 180, loss: 0.0060205888003110886\n",
            "step: 190, loss: 0.028023183345794678\n",
            "step: 200, loss: 0.10408516973257065\n",
            "step: 210, loss: 0.19926415383815765\n",
            "step: 220, loss: 0.002148954663425684\n",
            "step: 230, loss: 0.005834612064063549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9631284916201117, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017420463263988495\n",
            "step: 10, loss: 0.0016254675574600697\n",
            "step: 20, loss: 0.0021867582108825445\n",
            "step: 30, loss: 0.0015318489167839289\n",
            "step: 40, loss: 0.0027749286964535713\n",
            "step: 50, loss: 0.0013703955337405205\n",
            "step: 60, loss: 0.007128322031348944\n",
            "step: 70, loss: 0.008927100338041782\n",
            "step: 80, loss: 0.0004905008245259523\n",
            "step: 90, loss: 0.0009122289484366775\n",
            "step: 100, loss: 0.007226633373647928\n",
            "step: 110, loss: 0.006021332461386919\n",
            "step: 120, loss: 0.01563136838376522\n",
            "step: 130, loss: 0.004285208415240049\n",
            "step: 140, loss: 0.0024543567560613155\n",
            "step: 150, loss: 0.1412256956100464\n",
            "step: 160, loss: 0.06644104421138763\n",
            "step: 170, loss: 0.033130258321762085\n",
            "step: 180, loss: 0.004881853703409433\n",
            "step: 190, loss: 0.008014014922082424\n",
            "step: 200, loss: 0.0033086270559579134\n",
            "step: 210, loss: 0.17250193655490875\n",
            "step: 220, loss: 0.0005824373802170157\n",
            "step: 230, loss: 0.028264909982681274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9668874172185431, f1=0.9656699889258028, best_f1=0.9656699889258028\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019204160198569298\n",
            "step: 10, loss: 0.011113934218883514\n",
            "step: 20, loss: 0.13057874143123627\n",
            "step: 30, loss: 0.0005883671110495925\n",
            "step: 40, loss: 0.0014754958683624864\n",
            "step: 50, loss: 0.012627651914954185\n",
            "step: 60, loss: 0.007281249389052391\n",
            "step: 70, loss: 0.0009360458352603018\n",
            "step: 80, loss: 0.004042844753712416\n",
            "step: 90, loss: 0.0010451892158016562\n",
            "step: 100, loss: 0.00029603755683638155\n",
            "step: 110, loss: 0.0003046469937544316\n",
            "step: 120, loss: 0.00020256711286492646\n",
            "step: 130, loss: 0.002675189170986414\n",
            "step: 140, loss: 0.001932028797455132\n",
            "step: 150, loss: 0.002173067070543766\n",
            "step: 160, loss: 0.0020535450894385576\n",
            "step: 170, loss: 0.0007931040017865598\n",
            "step: 180, loss: 0.0019240269903093576\n",
            "step: 190, loss: 0.1394355297088623\n",
            "step: 200, loss: 0.00202564406208694\n",
            "step: 210, loss: 0.021097412332892418\n",
            "step: 220, loss: 0.0038689810317009687\n",
            "step: 230, loss: 0.0007448342512361705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.972129319955407, f1=0.9743589743589743, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00432361988350749\n",
            "step: 10, loss: 0.013641505502164364\n",
            "step: 20, loss: 0.007067626807838678\n",
            "step: 30, loss: 0.0002451637701597065\n",
            "step: 40, loss: 0.0007077838527038693\n",
            "step: 50, loss: 0.000759169168304652\n",
            "step: 60, loss: 0.0002947907196357846\n",
            "step: 70, loss: 0.001224396051838994\n",
            "step: 80, loss: 0.001199872582219541\n",
            "step: 90, loss: 0.005420915316790342\n",
            "step: 100, loss: 0.022458994761109352\n",
            "step: 110, loss: 0.0008206773782148957\n",
            "step: 120, loss: 0.13915029168128967\n",
            "step: 130, loss: 0.0003514142881613225\n",
            "step: 140, loss: 0.003247681772336364\n",
            "step: 150, loss: 0.1354210376739502\n",
            "step: 160, loss: 0.0014391348231583834\n",
            "step: 170, loss: 0.014730680733919144\n",
            "step: 180, loss: 0.05496084690093994\n",
            "step: 190, loss: 0.06980933994054794\n",
            "step: 200, loss: 0.0003061093739233911\n",
            "step: 210, loss: 0.0004922673106193542\n",
            "step: 220, loss: 0.0010589660378172994\n",
            "step: 230, loss: 0.0908600240945816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9720044792833147, f1=0.967452300785634, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007723123766481876\n",
            "step: 10, loss: 0.0005137314437888563\n",
            "step: 20, loss: 0.0032699613366276026\n",
            "step: 30, loss: 0.002399869728833437\n",
            "step: 40, loss: 0.0012799170799553394\n",
            "step: 50, loss: 0.001106968498788774\n",
            "step: 60, loss: 0.004230877384543419\n",
            "step: 70, loss: 0.006149178836494684\n",
            "step: 80, loss: 0.0014120384585112333\n",
            "step: 90, loss: 0.0015380405820906162\n",
            "step: 100, loss: 0.000505371019244194\n",
            "step: 110, loss: 0.00017354084411635995\n",
            "step: 120, loss: 0.00021571613615378737\n",
            "step: 130, loss: 0.0002089899790007621\n",
            "step: 140, loss: 0.00018867829930968583\n",
            "step: 150, loss: 0.00032768052187748253\n",
            "step: 160, loss: 0.03891143947839737\n",
            "step: 170, loss: 0.0006922144675627351\n",
            "step: 180, loss: 0.0014044004492461681\n",
            "step: 190, loss: 0.0006649699644185603\n",
            "step: 200, loss: 0.004238834138959646\n",
            "step: 210, loss: 0.000346022832673043\n",
            "step: 220, loss: 0.0003877574345096946\n",
            "step: 230, loss: 0.0003502405306790024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.971815107102593, f1=0.9642857142857144, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002640104212332517\n",
            "step: 10, loss: 0.0004599575186148286\n",
            "step: 20, loss: 0.00015282363165169954\n",
            "step: 30, loss: 0.00017096269584726542\n",
            "step: 40, loss: 0.0016660018591210246\n",
            "step: 50, loss: 0.0024245737586170435\n",
            "step: 60, loss: 0.0004002189380116761\n",
            "step: 70, loss: 0.000574981328099966\n",
            "step: 80, loss: 0.0003363689756952226\n",
            "step: 90, loss: 0.000370873516658321\n",
            "step: 100, loss: 0.0004554196202661842\n",
            "step: 110, loss: 0.030712099745869637\n",
            "step: 120, loss: 0.005222718697041273\n",
            "step: 130, loss: 0.00020582716388162225\n",
            "step: 140, loss: 0.0003411401412449777\n",
            "step: 150, loss: 0.0007278675329871476\n",
            "step: 160, loss: 0.000693178444635123\n",
            "step: 170, loss: 0.0020069656893610954\n",
            "step: 180, loss: 0.002248372882604599\n",
            "step: 190, loss: 0.005698119290173054\n",
            "step: 200, loss: 0.001121897017583251\n",
            "step: 210, loss: 0.013062468729913235\n",
            "step: 220, loss: 0.008130480535328388\n",
            "step: 230, loss: 0.00048407536814920604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9663677130044843, f1=0.9652855543113102, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004459308402147144\n",
            "step: 10, loss: 0.0008910122560337186\n",
            "step: 20, loss: 0.0004981684032827616\n",
            "step: 30, loss: 0.00016456547018606216\n",
            "step: 40, loss: 0.015984494239091873\n",
            "step: 50, loss: 0.00016438739839941263\n",
            "step: 60, loss: 0.0006324811838567257\n",
            "step: 70, loss: 0.00024359545204788446\n",
            "step: 80, loss: 0.00029621145222336054\n",
            "step: 90, loss: 0.000471437320811674\n",
            "step: 100, loss: 0.0001402506313752383\n",
            "step: 110, loss: 7.136404019547626e-05\n",
            "step: 120, loss: 6.081740866648033e-05\n",
            "step: 130, loss: 7.100459333742037e-05\n",
            "step: 140, loss: 0.00016343149763997644\n",
            "step: 150, loss: 0.007013535127043724\n",
            "step: 160, loss: 0.000367784989066422\n",
            "step: 170, loss: 0.0005733614671044052\n",
            "step: 180, loss: 0.0015353224007412791\n",
            "step: 190, loss: 0.0011442756513133645\n",
            "step: 200, loss: 0.004505376797169447\n",
            "step: 210, loss: 0.00039843094418756664\n",
            "step: 220, loss: 0.0003667129494715482\n",
            "step: 230, loss: 0.0018496447009965777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9627118644067796, f1=0.9619686800894856, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006077237892895937\n",
            "step: 10, loss: 4.282246300135739e-05\n",
            "step: 20, loss: 0.0003629799175541848\n",
            "step: 30, loss: 0.00010069936251966283\n",
            "step: 40, loss: 0.000624754058662802\n",
            "step: 50, loss: 0.020585864782333374\n",
            "step: 60, loss: 0.009427233599126339\n",
            "step: 70, loss: 0.0002897621598094702\n",
            "step: 80, loss: 0.00012812820204999298\n",
            "step: 90, loss: 0.001003130222670734\n",
            "step: 100, loss: 0.0003515472635626793\n",
            "step: 110, loss: 0.00022536053438670933\n",
            "step: 120, loss: 0.000308350776322186\n",
            "step: 130, loss: 0.0001521651865914464\n",
            "step: 140, loss: 0.14156359434127808\n",
            "step: 150, loss: 0.001988019561395049\n",
            "step: 160, loss: 0.00031556037720292807\n",
            "step: 170, loss: 0.0002284057263750583\n",
            "step: 180, loss: 0.0012674498138949275\n",
            "step: 190, loss: 0.000868332339450717\n",
            "step: 200, loss: 0.00011665540660033002\n",
            "step: 210, loss: 0.0003537542943377048\n",
            "step: 220, loss: 0.00015558292216155678\n",
            "step: 230, loss: 0.0002788772981148213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9730941704035874, f1=0.9665178571428571, best_f1=0.9665178571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017030004528351128\n",
            "step: 10, loss: 0.00023842962400522083\n",
            "step: 20, loss: 0.001713883480988443\n",
            "step: 30, loss: 0.001153363031335175\n",
            "step: 40, loss: 0.004348310641944408\n",
            "step: 50, loss: 0.00039090286009013653\n",
            "step: 60, loss: 0.0003587358514778316\n",
            "step: 70, loss: 9.907613275572658e-05\n",
            "step: 80, loss: 8.38916894281283e-05\n",
            "step: 90, loss: 6.437191768782213e-05\n",
            "step: 100, loss: 8.225362398661673e-05\n",
            "step: 110, loss: 0.004257523454725742\n",
            "step: 120, loss: 0.0002352349110879004\n",
            "step: 130, loss: 8.512751082889736e-05\n",
            "step: 140, loss: 0.00011704112694133073\n",
            "step: 150, loss: 0.027655940502882004\n",
            "step: 160, loss: 5.418476939667016e-05\n",
            "step: 170, loss: 0.002356015145778656\n",
            "step: 180, loss: 0.0001042915610014461\n",
            "step: 190, loss: 4.896125756204128e-05\n",
            "step: 200, loss: 0.00804123654961586\n",
            "step: 210, loss: 7.584123522974551e-05\n",
            "step: 220, loss: 7.60011316742748e-05\n",
            "step: 230, loss: 0.00010624439164530486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9709821428571428, f1=0.9688888888888889, best_f1=0.9665178571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012226813123561442\n",
            "step: 10, loss: 5.196837446419522e-05\n",
            "step: 20, loss: 6.549262616317719e-05\n",
            "step: 30, loss: 0.07563123106956482\n",
            "step: 40, loss: 0.10069742798805237\n",
            "step: 50, loss: 0.0011240638559684157\n",
            "step: 60, loss: 0.001253784168511629\n",
            "step: 70, loss: 0.00037281360710039735\n",
            "step: 80, loss: 0.0002492171188350767\n",
            "step: 90, loss: 5.836330456077121e-05\n",
            "step: 100, loss: 0.0008114710217341781\n",
            "step: 110, loss: 0.00010043055954156443\n",
            "step: 120, loss: 0.0001888629049062729\n",
            "step: 130, loss: 7.799797458574176e-05\n",
            "step: 140, loss: 0.01755840703845024\n",
            "step: 150, loss: 8.74441975611262e-05\n",
            "step: 160, loss: 0.00013035540177952498\n",
            "step: 170, loss: 0.0002489285543560982\n",
            "step: 180, loss: 0.01778586208820343\n",
            "step: 190, loss: 0.0004961126251146197\n",
            "step: 200, loss: 7.203467248473316e-05\n",
            "step: 210, loss: 8.93826800165698e-05\n",
            "step: 220, loss: 5.22269110661e-05\n",
            "step: 230, loss: 0.012588047422468662\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9709821428571428, f1=0.9732142857142857, best_f1=0.9665178571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014219032891560346\n",
            "step: 10, loss: 0.00032578568789176643\n",
            "step: 20, loss: 0.0004779742448590696\n",
            "step: 30, loss: 0.00010954052413580939\n",
            "step: 40, loss: 0.0009094499982893467\n",
            "step: 50, loss: 0.0004981684032827616\n",
            "step: 60, loss: 0.00014645190094597638\n",
            "step: 70, loss: 5.5133761634351686e-05\n",
            "step: 80, loss: 0.00012871874787379056\n",
            "step: 90, loss: 0.00022089671983849257\n",
            "step: 100, loss: 0.00011408278078306466\n",
            "step: 110, loss: 0.02608230523765087\n",
            "step: 120, loss: 0.00296822190284729\n",
            "step: 130, loss: 6.994720024522394e-05\n",
            "step: 140, loss: 0.0005194912082515657\n",
            "step: 150, loss: 9.206045069731772e-05\n",
            "step: 160, loss: 0.0001019654591800645\n",
            "step: 170, loss: 0.00018997187726199627\n",
            "step: 180, loss: 8.165314648067579e-05\n",
            "step: 190, loss: 8.848056313581765e-05\n",
            "step: 200, loss: 0.0001990756281884387\n",
            "step: 210, loss: 3.954118801630102e-05\n",
            "step: 220, loss: 4.660409103962593e-05\n",
            "step: 230, loss: 0.0014095902442932129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9731543624161074, f1=0.972129319955407, best_f1=0.972129319955407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021838067914359272\n",
            "step: 10, loss: 7.048173574730754e-05\n",
            "step: 20, loss: 8.21590147097595e-05\n",
            "step: 30, loss: 7.418014138238505e-05\n",
            "step: 40, loss: 0.0029818136245012283\n",
            "step: 50, loss: 8.761577191762626e-05\n",
            "step: 60, loss: 4.132583489990793e-05\n",
            "step: 70, loss: 8.176614210242406e-05\n",
            "step: 80, loss: 5.136126492288895e-05\n",
            "step: 90, loss: 7.036420720396563e-05\n",
            "step: 100, loss: 5.425063500297256e-05\n",
            "step: 110, loss: 0.00013831813703291118\n",
            "step: 120, loss: 4.1513878386467695e-05\n",
            "step: 130, loss: 0.00023285004135686904\n",
            "step: 140, loss: 6.589859549421817e-05\n",
            "step: 150, loss: 6.614611629629508e-05\n",
            "step: 160, loss: 0.00016559792857151479\n",
            "step: 170, loss: 0.0008234684355556965\n",
            "step: 180, loss: 0.00016502045036759228\n",
            "step: 190, loss: 9.139356552623212e-05\n",
            "step: 200, loss: 0.014088200405240059\n",
            "step: 210, loss: 5.853069887962192e-05\n",
            "step: 220, loss: 9.50125468079932e-05\n",
            "step: 230, loss: 0.0007562385872006416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9742441209406495, f1=0.9710467706013363, best_f1=0.9710467706013363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.19206046697218e-05\n",
            "step: 10, loss: 3.7517100281547755e-05\n",
            "step: 20, loss: 7.227284368127584e-05\n",
            "step: 30, loss: 4.8984096792992204e-05\n",
            "step: 40, loss: 0.000554995087441057\n",
            "step: 50, loss: 0.012304098345339298\n",
            "step: 60, loss: 5.9413592680357397e-05\n",
            "step: 70, loss: 8.594730024924502e-05\n",
            "step: 80, loss: 0.00013538786151912063\n",
            "step: 90, loss: 0.0001331127277808264\n",
            "step: 100, loss: 4.6436485718004405e-05\n",
            "step: 110, loss: 5.096406675875187e-05\n",
            "step: 120, loss: 5.783807864645496e-05\n",
            "step: 130, loss: 0.002801834372803569\n",
            "step: 140, loss: 0.000407418207032606\n",
            "step: 150, loss: 0.00016813409456517547\n",
            "step: 160, loss: 2.774481072265189e-05\n",
            "step: 170, loss: 3.56487107637804e-05\n",
            "step: 180, loss: 0.0002769895363599062\n",
            "step: 190, loss: 0.00011237180297030136\n",
            "step: 200, loss: 5.2927600336261094e-05\n",
            "step: 210, loss: 0.00019126664847135544\n",
            "step: 220, loss: 0.00014384149108082056\n",
            "step: 230, loss: 0.00011808740964625031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9727891156462585, f1=0.9718785151856018, best_f1=0.9710467706013363\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 201.29it/s]\n",
            "load_f1 = 0.9738933030646991\n",
            "real_f1 = 0.971687429218573\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 232.89it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW78AaaneEUs"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hwn5WkZe3Kb",
        "outputId": "9dea1ed7-836b-41e4-8236-1ab25e7ba80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6302250027656555\n",
            "step: 10, loss: 0.6038451790809631\n",
            "step: 20, loss: 0.5161104798316956\n",
            "step: 30, loss: 0.09748882055282593\n",
            "step: 40, loss: 0.2806627154350281\n",
            "step: 50, loss: 0.2453959733247757\n",
            "step: 60, loss: 0.12035278975963593\n",
            "step: 70, loss: 0.11354031413793564\n",
            "step: 80, loss: 0.07295063883066177\n",
            "step: 90, loss: 0.28525063395500183\n",
            "step: 100, loss: 0.16525551676750183\n",
            "step: 110, loss: 0.05724275857210159\n",
            "step: 120, loss: 0.0814087763428688\n",
            "step: 130, loss: 0.05201048031449318\n",
            "step: 140, loss: 0.10093515366315842\n",
            "step: 150, loss: 0.06478074938058853\n",
            "step: 160, loss: 0.048340797424316406\n",
            "step: 170, loss: 0.29047518968582153\n",
            "step: 180, loss: 0.049302928149700165\n",
            "step: 190, loss: 0.019503595307469368\n",
            "step: 200, loss: 0.11769647896289825\n",
            "step: 210, loss: 0.09026382118463516\n",
            "step: 220, loss: 0.22211149334907532\n",
            "step: 230, loss: 0.1651073694229126\n",
            "step: 240, loss: 0.041513584554195404\n",
            "step: 250, loss: 0.028243031352758408\n",
            "step: 260, loss: 0.050888728350400925\n",
            "step: 270, loss: 0.017366793006658554\n",
            "step: 280, loss: 0.053321704268455505\n",
            "step: 290, loss: 0.10532017797231674\n",
            "step: 300, loss: 0.11266354471445084\n",
            "step: 310, loss: 0.31819409132003784\n",
            "step: 320, loss: 0.15257909893989563\n",
            "step: 330, loss: 0.16040053963661194\n",
            "step: 340, loss: 0.09763430804014206\n",
            "step: 350, loss: 0.013851833529770374\n",
            "step: 360, loss: 0.21912352740764618\n",
            "step: 370, loss: 0.22137920558452606\n",
            "step: 380, loss: 0.058818988502025604\n",
            "step: 390, loss: 0.12519849836826324\n",
            "step: 400, loss: 0.30948740243911743\n",
            "step: 410, loss: 0.07181332260370255\n",
            "step: 420, loss: 0.04059557989239693\n",
            "step: 430, loss: 0.18530656397342682\n",
            "step: 440, loss: 0.099912129342556\n",
            "step: 450, loss: 0.025031598284840584\n",
            "step: 460, loss: 0.008938023820519447\n",
            "step: 470, loss: 0.18649184703826904\n",
            "step: 480, loss: 0.07858013361692429\n",
            "step: 490, loss: 0.03912341967225075\n",
            "step: 500, loss: 0.15591290593147278\n",
            "step: 510, loss: 0.0549447238445282\n",
            "step: 520, loss: 0.07969561219215393\n",
            "step: 530, loss: 0.008781654760241508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9187411930483794, f1=0.9195509822263798, best_f1=0.9195509822263798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08876228332519531\n",
            "step: 10, loss: 0.04319675639271736\n",
            "step: 20, loss: 0.11850115656852722\n",
            "step: 30, loss: 0.016308004036545753\n",
            "step: 40, loss: 0.14535073935985565\n",
            "step: 50, loss: 0.17103733122348785\n",
            "step: 60, loss: 0.004638063721358776\n",
            "step: 70, loss: 0.043776825070381165\n",
            "step: 80, loss: 0.048565544188022614\n",
            "step: 90, loss: 0.017754605039954185\n",
            "step: 100, loss: 0.06382699310779572\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 110, loss: 0.10140970349311829\n",
            "step: 120, loss: 0.05352018028497696\n",
            "step: 130, loss: 0.08571358025074005\n",
            "step: 140, loss: 0.13956420123577118\n",
            "step: 150, loss: 0.03773575648665428\n",
            "step: 160, loss: 0.04560444876551628\n",
            "step: 170, loss: 0.13334132730960846\n",
            "step: 180, loss: 0.101840078830719\n",
            "step: 190, loss: 0.07128775119781494\n",
            "step: 200, loss: 0.01245121844112873\n",
            "step: 210, loss: 0.029691461473703384\n",
            "step: 220, loss: 0.06520841270685196\n",
            "step: 230, loss: 0.03965844586491585\n",
            "step: 240, loss: 0.009655789472162724\n",
            "step: 250, loss: 0.07626362890005112\n",
            "step: 260, loss: 0.01933286525309086\n",
            "step: 270, loss: 0.11091397702693939\n",
            "step: 280, loss: 0.093777135014534\n",
            "step: 290, loss: 0.032721828669309616\n",
            "step: 300, loss: 0.16835632920265198\n",
            "step: 310, loss: 0.008610650897026062\n",
            "step: 320, loss: 0.15062308311462402\n",
            "step: 330, loss: 0.03260566294193268\n",
            "step: 340, loss: 0.04457272216677666\n",
            "step: 350, loss: 0.003002872923389077\n",
            "step: 360, loss: 0.06869805604219437\n",
            "step: 370, loss: 0.15922288596630096\n",
            "step: 380, loss: 0.0481741838157177\n",
            "step: 390, loss: 0.05202513560652733\n",
            "step: 400, loss: 0.06318418681621552\n",
            "step: 410, loss: 0.026560330763459206\n",
            "step: 420, loss: 0.0314779095351696\n",
            "step: 430, loss: 0.07775139808654785\n",
            "step: 440, loss: 0.1117905005812645\n",
            "step: 450, loss: 0.08316381275653839\n",
            "step: 460, loss: 0.0516340434551239\n",
            "step: 470, loss: 0.06466436386108398\n",
            "step: 480, loss: 0.1486876904964447\n",
            "step: 490, loss: 0.04633941501379013\n",
            "step: 500, loss: 0.1712951511144638\n",
            "step: 510, loss: 0.04505280777812004\n",
            "step: 520, loss: 0.07663647830486298\n",
            "step: 530, loss: 0.043983567506074905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.924119241192412, f1=0.9213993639254885, best_f1=0.9213993639254885\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1760009527206421\n",
            "step: 10, loss: 0.07405032217502594\n",
            "step: 20, loss: 0.12596289813518524\n",
            "step: 30, loss: 0.14265458285808563\n",
            "step: 40, loss: 0.05463588237762451\n",
            "step: 50, loss: 0.15679363906383514\n",
            "step: 60, loss: 0.10502288490533829\n",
            "step: 70, loss: 0.004521934315562248\n",
            "step: 80, loss: 0.003046893747523427\n",
            "step: 90, loss: 0.004370460286736488\n",
            "step: 100, loss: 0.05257042497396469\n",
            "step: 110, loss: 0.02574666403234005\n",
            "step: 120, loss: 0.025660738348960876\n",
            "step: 130, loss: 0.012615772895514965\n",
            "step: 140, loss: 0.09793974459171295\n",
            "step: 150, loss: 0.010956361889839172\n",
            "step: 160, loss: 0.02142884023487568\n",
            "step: 170, loss: 0.06635164469480515\n",
            "step: 180, loss: 0.018128501251339912\n",
            "step: 190, loss: 0.0413098968565464\n",
            "step: 200, loss: 0.023650797083973885\n",
            "step: 210, loss: 0.05601651594042778\n",
            "step: 220, loss: 0.011896486394107342\n",
            "step: 230, loss: 0.12688812613487244\n",
            "step: 240, loss: 0.007060409989207983\n",
            "step: 250, loss: 0.11991072446107864\n",
            "step: 260, loss: 0.12654545903205872\n",
            "step: 270, loss: 0.02026602439582348\n",
            "step: 280, loss: 0.041101280599832535\n",
            "step: 290, loss: 0.002813482191413641\n",
            "step: 300, loss: 0.06399372965097427\n",
            "step: 310, loss: 0.06683029979467392\n",
            "step: 320, loss: 0.0649319738149643\n",
            "step: 330, loss: 0.024575350806117058\n",
            "step: 340, loss: 0.07797335088253021\n",
            "step: 350, loss: 0.006113077979534864\n",
            "step: 360, loss: 0.08218758553266525\n",
            "step: 370, loss: 0.027509603649377823\n",
            "step: 380, loss: 0.028196679428219795\n",
            "step: 390, loss: 0.006157590541988611\n",
            "step: 400, loss: 0.031093386933207512\n",
            "step: 410, loss: 0.0024311791639775038\n",
            "step: 420, loss: 0.19460844993591309\n",
            "step: 430, loss: 0.11904307454824448\n",
            "step: 440, loss: 0.0024620427284389734\n",
            "step: 450, loss: 0.0751820057630539\n",
            "step: 460, loss: 0.019387628883123398\n",
            "step: 470, loss: 0.0710376650094986\n",
            "step: 480, loss: 0.012079360894858837\n",
            "step: 490, loss: 0.0054804477840662\n",
            "step: 500, loss: 0.0052123055793344975\n",
            "step: 510, loss: 0.08942129462957382\n",
            "step: 520, loss: 0.08144115656614304\n",
            "step: 530, loss: 0.07565817981958389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9325323475046211, f1=0.9212344541685858, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12098387628793716\n",
            "step: 10, loss: 0.011998728848993778\n",
            "step: 20, loss: 0.001081200665794313\n",
            "step: 30, loss: 0.0008126205066218972\n",
            "step: 40, loss: 0.001451744232326746\n",
            "step: 50, loss: 0.15247711539268494\n",
            "step: 60, loss: 0.011494290083646774\n",
            "step: 70, loss: 0.0021878888364881277\n",
            "step: 80, loss: 0.002061365405097604\n",
            "step: 90, loss: 0.011755391955375671\n",
            "step: 100, loss: 0.0029999157413840294\n",
            "step: 110, loss: 0.04007978364825249\n",
            "step: 120, loss: 0.0006663057720288634\n",
            "step: 130, loss: 0.006214340217411518\n",
            "step: 140, loss: 0.013181421905755997\n",
            "step: 150, loss: 0.0373711921274662\n",
            "step: 160, loss: 0.03899335116147995\n",
            "step: 170, loss: 0.07521740347146988\n",
            "step: 180, loss: 0.0021598709281533957\n",
            "step: 190, loss: 0.08878687769174576\n",
            "step: 200, loss: 0.01233365572988987\n",
            "step: 210, loss: 0.012305489741265774\n",
            "step: 220, loss: 0.11788327991962433\n",
            "step: 230, loss: 0.040291380137205124\n",
            "step: 240, loss: 0.03261631727218628\n",
            "step: 250, loss: 0.01837640441954136\n",
            "step: 260, loss: 0.0022529815323650837\n",
            "step: 270, loss: 0.01090586930513382\n",
            "step: 280, loss: 0.24225464463233948\n",
            "step: 290, loss: 0.018812544643878937\n",
            "step: 300, loss: 0.0016524099046364427\n",
            "step: 310, loss: 0.05470063537359238\n",
            "step: 320, loss: 0.05278979241847992\n",
            "step: 330, loss: 0.006443447899073362\n",
            "step: 340, loss: 0.011309956200420856\n",
            "step: 350, loss: 0.04902617633342743\n",
            "step: 360, loss: 0.011352160945534706\n",
            "step: 370, loss: 0.0020838938653469086\n",
            "step: 380, loss: 0.03230440616607666\n",
            "step: 390, loss: 0.02070094272494316\n",
            "step: 400, loss: 0.21535159647464752\n",
            "step: 410, loss: 0.0015099863521754742\n",
            "step: 420, loss: 0.029712751507759094\n",
            "step: 430, loss: 0.10224878787994385\n",
            "step: 440, loss: 0.043208807706832886\n",
            "step: 450, loss: 0.026257144287228584\n",
            "step: 460, loss: 0.11492492258548737\n",
            "step: 470, loss: 0.11560314893722534\n",
            "step: 480, loss: 0.10541535168886185\n",
            "step: 490, loss: 0.012905392795801163\n",
            "step: 500, loss: 0.04742595925927162\n",
            "step: 510, loss: 0.011798305436968803\n",
            "step: 520, loss: 0.06797755509614944\n",
            "step: 530, loss: 0.04671471565961838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9258384506376948, f1=0.9183577159037282, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.058638691902160645\n",
            "step: 10, loss: 0.05339004099369049\n",
            "step: 20, loss: 0.004188617691397667\n",
            "step: 30, loss: 0.013172424398362637\n",
            "step: 40, loss: 0.031248265877366066\n",
            "step: 50, loss: 0.01692415215075016\n",
            "step: 60, loss: 0.036754775792360306\n",
            "step: 70, loss: 0.013767620548605919\n",
            "step: 80, loss: 0.004325320012867451\n",
            "step: 90, loss: 0.015936600044369698\n",
            "step: 100, loss: 0.0008381104562431574\n",
            "step: 110, loss: 0.0007968852878548205\n",
            "step: 120, loss: 0.01982778124511242\n",
            "step: 130, loss: 0.001191031071357429\n",
            "step: 140, loss: 0.000769401784054935\n",
            "step: 150, loss: 0.001801306032575667\n",
            "step: 160, loss: 0.0006443828460760415\n",
            "step: 170, loss: 0.03072969987988472\n",
            "step: 180, loss: 0.006691726390272379\n",
            "step: 190, loss: 0.00916751567274332\n",
            "step: 200, loss: 0.00055363291176036\n",
            "step: 210, loss: 0.05589164420962334\n",
            "step: 220, loss: 0.01129551138728857\n",
            "step: 230, loss: 0.04896091669797897\n",
            "step: 240, loss: 0.027588438242673874\n",
            "step: 250, loss: 0.004020960070192814\n",
            "step: 260, loss: 0.029538594186306\n",
            "step: 270, loss: 0.0007911713910289109\n",
            "step: 280, loss: 0.005663261748850346\n",
            "step: 290, loss: 0.15720519423484802\n",
            "step: 300, loss: 0.02179023064672947\n",
            "step: 310, loss: 0.015924178063869476\n",
            "step: 320, loss: 0.06302438676357269\n",
            "step: 330, loss: 0.0570971854031086\n",
            "step: 340, loss: 0.005905468482524157\n",
            "step: 350, loss: 0.0033506930340081453\n",
            "step: 360, loss: 0.010528025217354298\n",
            "step: 370, loss: 0.02127882093191147\n",
            "step: 380, loss: 0.0008114184020087123\n",
            "step: 390, loss: 0.002848926931619644\n",
            "step: 400, loss: 0.0017209772486239672\n",
            "step: 410, loss: 0.0036415723152458668\n",
            "step: 420, loss: 0.00978479441255331\n",
            "step: 430, loss: 0.004956972319632769\n",
            "step: 440, loss: 0.006404751446098089\n",
            "step: 450, loss: 0.011752728372812271\n",
            "step: 460, loss: 0.0020956178195774555\n",
            "step: 470, loss: 0.002014781581237912\n",
            "step: 480, loss: 0.0110493004322052\n",
            "step: 490, loss: 0.0007918477058410645\n",
            "step: 500, loss: 0.012021144852042198\n",
            "step: 510, loss: 0.030527126044034958\n",
            "step: 520, loss: 0.014780278317630291\n",
            "step: 530, loss: 0.007682028226554394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9212454212454212, f1=0.9151765245300322, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00874044094234705\n",
            "step: 10, loss: 0.00201193755492568\n",
            "step: 20, loss: 0.030092110857367516\n",
            "step: 30, loss: 0.013223436661064625\n",
            "step: 40, loss: 0.010905597358942032\n",
            "step: 50, loss: 0.05359882116317749\n",
            "step: 60, loss: 0.0014007508289068937\n",
            "step: 70, loss: 0.022007299587130547\n",
            "step: 80, loss: 0.03872787579894066\n",
            "step: 90, loss: 0.004998335149139166\n",
            "step: 100, loss: 0.004706377163529396\n",
            "step: 110, loss: 0.015420286916196346\n",
            "step: 120, loss: 0.02252179943025112\n",
            "step: 130, loss: 0.006553218699991703\n",
            "step: 140, loss: 0.0018456911202520132\n",
            "step: 150, loss: 0.011072161607444286\n",
            "step: 160, loss: 0.013158202171325684\n",
            "step: 170, loss: 0.009014456532895565\n",
            "step: 180, loss: 0.0017579549457877874\n",
            "step: 190, loss: 0.05516933277249336\n",
            "step: 200, loss: 0.0019895625300705433\n",
            "step: 210, loss: 0.14412324130535126\n",
            "step: 220, loss: 0.012215739116072655\n",
            "step: 230, loss: 0.0018216597381979227\n",
            "step: 240, loss: 0.0038908047135919333\n",
            "step: 250, loss: 0.010050315409898758\n",
            "step: 260, loss: 0.08862768858671188\n",
            "step: 270, loss: 0.2288464605808258\n",
            "step: 280, loss: 0.017138194292783737\n",
            "step: 290, loss: 0.00032645996543578804\n",
            "step: 300, loss: 0.0009880606085062027\n",
            "step: 310, loss: 0.004474916961044073\n",
            "step: 320, loss: 0.0010632158955559134\n",
            "step: 330, loss: 0.0036238946486264467\n",
            "step: 340, loss: 0.08090467751026154\n",
            "step: 350, loss: 0.022007599472999573\n",
            "step: 360, loss: 0.004710082430392504\n",
            "step: 370, loss: 0.0060706231743097305\n",
            "step: 380, loss: 0.0015068614156916738\n",
            "step: 390, loss: 0.02429390512406826\n",
            "step: 400, loss: 0.0003751316398847848\n",
            "step: 410, loss: 0.0008251933613792062\n",
            "step: 420, loss: 0.0011433014879003167\n",
            "step: 430, loss: 0.0068066539242863655\n",
            "step: 440, loss: 0.00020681307069025934\n",
            "step: 450, loss: 0.0013968328712508082\n",
            "step: 460, loss: 0.0005989199853502214\n",
            "step: 470, loss: 0.0018502050079405308\n",
            "step: 480, loss: 0.029339835047721863\n",
            "step: 490, loss: 0.021835025399923325\n",
            "step: 500, loss: 0.07965972274541855\n",
            "step: 510, loss: 0.041801560670137405\n",
            "step: 520, loss: 0.015844237059354782\n",
            "step: 530, loss: 0.009707520715892315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9147072383586906, f1=0.9075011504832029, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002905701519921422\n",
            "step: 10, loss: 0.04377870261669159\n",
            "step: 20, loss: 0.0053194435313344\n",
            "step: 30, loss: 0.008224772289395332\n",
            "step: 40, loss: 0.0036933317314833403\n",
            "step: 50, loss: 0.06073717772960663\n",
            "step: 60, loss: 0.06994025409221649\n",
            "step: 70, loss: 0.0021049880888313055\n",
            "step: 80, loss: 0.11922919005155563\n",
            "step: 90, loss: 0.00039139713044278324\n",
            "step: 100, loss: 0.004184774588793516\n",
            "step: 110, loss: 0.005365288816392422\n",
            "step: 120, loss: 0.006214737426489592\n",
            "step: 130, loss: 0.007298514246940613\n",
            "step: 140, loss: 0.0003853141679428518\n",
            "step: 150, loss: 0.0014340534107759595\n",
            "step: 160, loss: 0.0004395207797642797\n",
            "step: 170, loss: 0.013589242473244667\n",
            "step: 180, loss: 0.0006418642587959766\n",
            "step: 190, loss: 0.0017067530425265431\n",
            "step: 200, loss: 0.0013009586837142706\n",
            "step: 210, loss: 0.002488624071702361\n",
            "step: 220, loss: 0.0008795188623480499\n",
            "step: 230, loss: 0.0009242697269655764\n",
            "step: 240, loss: 0.001791432499885559\n",
            "step: 250, loss: 0.002051058690994978\n",
            "step: 260, loss: 0.00389857217669487\n",
            "step: 270, loss: 0.00029448344139382243\n",
            "step: 280, loss: 0.000524416973348707\n",
            "step: 290, loss: 0.0026340738404542208\n",
            "step: 300, loss: 0.003574914764612913\n",
            "step: 310, loss: 0.006511386949568987\n",
            "step: 320, loss: 0.0018496590200811625\n",
            "step: 330, loss: 0.006237942725419998\n",
            "step: 340, loss: 0.1143079623579979\n",
            "step: 350, loss: 0.0004261976864654571\n",
            "step: 360, loss: 0.014561928808689117\n",
            "step: 370, loss: 0.05151280388236046\n",
            "step: 380, loss: 0.0027601453475654125\n",
            "step: 390, loss: 0.015200838446617126\n",
            "step: 400, loss: 0.013640262186527252\n",
            "step: 410, loss: 0.0023828090634196997\n",
            "step: 420, loss: 0.0021784405689686537\n",
            "step: 430, loss: 0.0018883362645283341\n",
            "step: 440, loss: 0.014714059419929981\n",
            "step: 450, loss: 0.001249093096703291\n",
            "step: 460, loss: 0.0007307646446861327\n",
            "step: 470, loss: 0.001342179486528039\n",
            "step: 480, loss: 0.14275559782981873\n",
            "step: 490, loss: 0.002202223753556609\n",
            "step: 500, loss: 0.0005050423787906766\n",
            "step: 510, loss: 0.0007912747096270323\n",
            "step: 520, loss: 0.13942719995975494\n",
            "step: 530, loss: 0.0034127114340662956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9283402681460935, f1=0.9224376731301939, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04639667645096779\n",
            "step: 10, loss: 0.04428839683532715\n",
            "step: 20, loss: 0.0001700367429293692\n",
            "step: 30, loss: 0.2563575506210327\n",
            "step: 40, loss: 0.0003089072706643492\n",
            "step: 50, loss: 0.01057679858058691\n",
            "step: 60, loss: 0.0007076546316966414\n",
            "step: 70, loss: 0.00036983322934247553\n",
            "step: 80, loss: 0.0008833082392811775\n",
            "step: 90, loss: 0.001209366717375815\n",
            "step: 100, loss: 0.0007488750852644444\n",
            "step: 110, loss: 0.0007314807153306901\n",
            "step: 120, loss: 0.010682051070034504\n",
            "step: 130, loss: 0.00034061583573929965\n",
            "step: 140, loss: 0.0023006577976047993\n",
            "step: 150, loss: 0.002977414056658745\n",
            "step: 160, loss: 0.005348959006369114\n",
            "step: 170, loss: 0.12049820274114609\n",
            "step: 180, loss: 0.0003410865319892764\n",
            "step: 190, loss: 0.0019568633288145065\n",
            "step: 200, loss: 0.0010210213949903846\n",
            "step: 210, loss: 0.0076292697340250015\n",
            "step: 220, loss: 0.0025606707204133272\n",
            "step: 230, loss: 0.007837476208806038\n",
            "step: 240, loss: 0.005093211308121681\n",
            "step: 250, loss: 0.0004048305272590369\n",
            "step: 260, loss: 0.0014818183844909072\n",
            "step: 270, loss: 0.008770718239247799\n",
            "step: 280, loss: 0.030615665018558502\n",
            "step: 290, loss: 0.0030398976523429155\n",
            "step: 300, loss: 0.07608683407306671\n",
            "step: 310, loss: 0.0029295324347913265\n",
            "step: 320, loss: 0.0006802669959142804\n",
            "step: 330, loss: 0.020052138715982437\n",
            "step: 340, loss: 0.0026610333006829023\n",
            "step: 350, loss: 0.0011559923877939582\n",
            "step: 360, loss: 0.0013147371355444193\n",
            "step: 370, loss: 0.0026985136792063713\n",
            "step: 380, loss: 0.005854760762304068\n",
            "step: 390, loss: 0.09775550663471222\n",
            "step: 400, loss: 0.002515428001061082\n",
            "step: 410, loss: 0.032712921500205994\n",
            "step: 420, loss: 0.000891786243300885\n",
            "step: 430, loss: 0.02569187618792057\n",
            "step: 440, loss: 0.020303957164287567\n",
            "step: 450, loss: 0.0027516174595803022\n",
            "step: 460, loss: 0.0002328574628336355\n",
            "step: 470, loss: 0.0030031688511371613\n",
            "step: 480, loss: 0.06704086810350418\n",
            "step: 490, loss: 0.016141269356012344\n",
            "step: 500, loss: 0.00848457682877779\n",
            "step: 510, loss: 0.09458467364311218\n",
            "step: 520, loss: 0.0023805780801922083\n",
            "step: 530, loss: 0.0028500198386609554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.916400182731841, f1=0.9155963302752294, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023266101256012917\n",
            "step: 10, loss: 0.0002857996732927859\n",
            "step: 20, loss: 0.012873944826424122\n",
            "step: 30, loss: 0.0031101941131055355\n",
            "step: 40, loss: 0.0082656005397439\n",
            "step: 50, loss: 0.003032093634828925\n",
            "step: 60, loss: 0.00041824011714197695\n",
            "step: 70, loss: 0.0026342326309531927\n",
            "step: 80, loss: 0.026598412543535233\n",
            "step: 90, loss: 8.871910540619865e-05\n",
            "step: 100, loss: 0.0006678003701381385\n",
            "step: 110, loss: 0.00021668615227099508\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.00029162713326513767\n",
            "step: 130, loss: 0.010782865807414055\n",
            "step: 140, loss: 0.0025916744489222765\n",
            "step: 150, loss: 0.00046868156641721725\n",
            "step: 160, loss: 0.00048773831804282963\n",
            "step: 170, loss: 0.0019862456247210503\n",
            "step: 180, loss: 0.0012362035922706127\n",
            "step: 190, loss: 8.238603186327964e-05\n",
            "step: 200, loss: 0.0014808750711381435\n",
            "step: 210, loss: 0.0012722999090328813\n",
            "step: 220, loss: 0.018710318952798843\n",
            "step: 230, loss: 8.03864240879193e-05\n",
            "step: 240, loss: 9.297644282924011e-05\n",
            "step: 250, loss: 0.0012528634397312999\n",
            "step: 260, loss: 0.04747233912348747\n",
            "step: 270, loss: 7.40795730962418e-05\n",
            "step: 280, loss: 6.229591963347048e-05\n",
            "step: 290, loss: 0.0567450076341629\n",
            "step: 300, loss: 0.0010603959672152996\n",
            "step: 310, loss: 0.004587868694216013\n",
            "step: 320, loss: 0.00013463989307638258\n",
            "step: 330, loss: 0.0010963680688291788\n",
            "step: 340, loss: 0.002952509792521596\n",
            "step: 350, loss: 0.0017087379237636924\n",
            "step: 360, loss: 8.62800152390264e-05\n",
            "step: 370, loss: 0.014620008878409863\n",
            "step: 380, loss: 0.0004372764378786087\n",
            "step: 390, loss: 0.00012837632675655186\n",
            "step: 400, loss: 0.1948128193616867\n",
            "step: 410, loss: 0.00026787343085743487\n",
            "step: 420, loss: 0.00023963910643942654\n",
            "step: 430, loss: 7.071781146805733e-05\n",
            "step: 440, loss: 0.14004188776016235\n",
            "step: 450, loss: 0.000871015596203506\n",
            "step: 460, loss: 0.04058503732085228\n",
            "step: 470, loss: 0.0003485338238533586\n",
            "step: 480, loss: 0.0002567890624050051\n",
            "step: 490, loss: 0.006271893624216318\n",
            "step: 500, loss: 0.0026048540603369474\n",
            "step: 510, loss: 0.02865811623632908\n",
            "step: 520, loss: 0.016452129930257797\n",
            "step: 530, loss: 0.000902092142496258\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9224952741020794, f1=0.9147727272727272, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04504238814115524\n",
            "step: 10, loss: 0.00017232920799870044\n",
            "step: 20, loss: 7.506286056013778e-05\n",
            "step: 30, loss: 0.00015238046762533486\n",
            "step: 40, loss: 0.0068795704282820225\n",
            "step: 50, loss: 0.0007091272273100913\n",
            "step: 60, loss: 0.00016458430036436766\n",
            "step: 70, loss: 0.00020098606182727963\n",
            "step: 80, loss: 0.0001166449292213656\n",
            "step: 90, loss: 8.88704089447856e-05\n",
            "step: 100, loss: 0.0003404493036214262\n",
            "step: 110, loss: 0.0004080232174601406\n",
            "step: 120, loss: 5.039574534748681e-05\n",
            "step: 130, loss: 0.0003491508250590414\n",
            "step: 140, loss: 0.00429672421887517\n",
            "step: 150, loss: 8.05396048235707e-05\n",
            "step: 160, loss: 0.00015649438137188554\n",
            "step: 170, loss: 0.0006399747217074037\n",
            "step: 180, loss: 0.0014831353910267353\n",
            "step: 190, loss: 0.001790543901734054\n",
            "step: 200, loss: 0.0020709228701889515\n",
            "step: 210, loss: 0.00014434766490012407\n",
            "step: 220, loss: 0.033473074436187744\n",
            "step: 230, loss: 0.0062331752851605415\n",
            "step: 240, loss: 0.02203412353992462\n",
            "step: 250, loss: 0.00021854280203115195\n",
            "step: 260, loss: 0.00019334979879204184\n",
            "step: 270, loss: 0.005802854895591736\n",
            "step: 280, loss: 5.132237492944114e-05\n",
            "step: 290, loss: 0.0007258890545926988\n",
            "step: 300, loss: 8.423512190347537e-05\n",
            "step: 310, loss: 0.00036453717621043324\n",
            "step: 320, loss: 0.002730043139308691\n",
            "step: 330, loss: 0.007491314318031073\n",
            "step: 340, loss: 8.661273022880778e-05\n",
            "step: 350, loss: 0.17738023400306702\n",
            "step: 360, loss: 0.0004222370916977525\n",
            "step: 370, loss: 0.004112042486667633\n",
            "step: 380, loss: 0.010753348469734192\n",
            "step: 390, loss: 0.00013865919027011842\n",
            "step: 400, loss: 0.0005203666514717042\n",
            "step: 410, loss: 0.0022258548997342587\n",
            "step: 420, loss: 0.0001913469604915008\n",
            "step: 430, loss: 4.046557660331018e-05\n",
            "step: 440, loss: 0.011149042285978794\n",
            "step: 450, loss: 0.0004039178602397442\n",
            "step: 460, loss: 0.00012075287668267265\n",
            "step: 470, loss: 0.00037053506821393967\n",
            "step: 480, loss: 0.0023434290196746588\n",
            "step: 490, loss: 0.02406022697687149\n",
            "step: 500, loss: 0.000727738777641207\n",
            "step: 510, loss: 0.0022046815138310194\n",
            "step: 520, loss: 0.002380875637754798\n",
            "step: 530, loss: 0.0009850080823525786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.929007992477668, f1=0.9220595181861123, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005943115102127194\n",
            "step: 10, loss: 0.004176100715994835\n",
            "step: 20, loss: 7.37801383365877e-05\n",
            "step: 30, loss: 0.00018138060113415122\n",
            "step: 40, loss: 0.005661665927618742\n",
            "step: 50, loss: 0.021024765446782112\n",
            "step: 60, loss: 0.012049095705151558\n",
            "step: 70, loss: 0.00025990375434048474\n",
            "step: 80, loss: 0.00015357561642304063\n",
            "step: 90, loss: 0.00023726679501123726\n",
            "step: 100, loss: 0.011510899290442467\n",
            "step: 110, loss: 0.0005626757047139108\n",
            "step: 120, loss: 0.0005477702361531556\n",
            "step: 130, loss: 0.00016974573372863233\n",
            "step: 140, loss: 0.009983189404010773\n",
            "step: 150, loss: 4.1381957998964936e-05\n",
            "step: 160, loss: 4.88298901473172e-05\n",
            "step: 170, loss: 6.302051042439416e-05\n",
            "step: 180, loss: 0.0019864484202116728\n",
            "step: 190, loss: 0.00038629831396974623\n",
            "step: 200, loss: 0.00813739188015461\n",
            "step: 210, loss: 3.814632509602234e-05\n",
            "step: 220, loss: 0.0007556685013696551\n",
            "step: 230, loss: 8.56292899698019e-05\n",
            "step: 240, loss: 0.0007263542502187192\n",
            "step: 250, loss: 3.422522422624752e-05\n",
            "step: 260, loss: 1.8045080651063472e-05\n",
            "step: 270, loss: 8.533773507224396e-05\n",
            "step: 280, loss: 0.0006629017880186439\n",
            "step: 290, loss: 0.00010974716860800982\n",
            "step: 300, loss: 0.0001675503299338743\n",
            "step: 310, loss: 6.0189555370016024e-05\n",
            "step: 320, loss: 0.00011244464985793456\n",
            "step: 330, loss: 0.014465905725955963\n",
            "step: 340, loss: 0.0011725407093763351\n",
            "step: 350, loss: 0.08785989880561829\n",
            "step: 360, loss: 0.00019493905710987747\n",
            "step: 370, loss: 0.0024735284969210625\n",
            "step: 380, loss: 0.002124931663274765\n",
            "step: 390, loss: 0.001632816856727004\n",
            "step: 400, loss: 0.0008110309136100113\n",
            "step: 410, loss: 0.0002561317232903093\n",
            "step: 420, loss: 6.419149576686323e-05\n",
            "step: 430, loss: 0.00018253397138323635\n",
            "step: 440, loss: 0.0002530553028918803\n",
            "step: 450, loss: 0.00011380259093130007\n",
            "step: 460, loss: 0.16269715130329132\n",
            "step: 470, loss: 0.00018061599985230714\n",
            "step: 480, loss: 0.0006868285126984119\n",
            "step: 490, loss: 0.00019630380847956985\n",
            "step: 500, loss: 0.00201388169080019\n",
            "step: 510, loss: 0.00026346775121055543\n",
            "step: 520, loss: 6.446083716582507e-05\n",
            "step: 530, loss: 0.00025097018806263804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9202453987730062, f1=0.9183098591549296, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00030986889032647014\n",
            "step: 10, loss: 0.20935294032096863\n",
            "step: 20, loss: 0.0016007269732654095\n",
            "step: 30, loss: 0.0007365625351667404\n",
            "step: 40, loss: 0.0015321398386731744\n",
            "step: 50, loss: 0.0621129609644413\n",
            "step: 60, loss: 0.0010023211361840367\n",
            "step: 70, loss: 0.0006690490408800542\n",
            "step: 80, loss: 0.0015578980091959238\n",
            "step: 90, loss: 0.00013259038678370416\n",
            "step: 100, loss: 0.0016761929728090763\n",
            "step: 110, loss: 0.004970670212060213\n",
            "step: 120, loss: 0.0005283395876176655\n",
            "step: 130, loss: 0.00025706831365823746\n",
            "step: 140, loss: 0.011514660902321339\n",
            "step: 150, loss: 5.013977352064103e-05\n",
            "step: 160, loss: 0.0008595040999352932\n",
            "step: 170, loss: 0.0020603800658136606\n",
            "step: 180, loss: 0.0011577350087463856\n",
            "step: 190, loss: 0.001539932913146913\n",
            "step: 200, loss: 6.995522562647238e-05\n",
            "step: 210, loss: 0.0009122503688558936\n",
            "step: 220, loss: 0.0004965708358213305\n",
            "step: 230, loss: 3.1607782148057595e-05\n",
            "step: 240, loss: 0.00070728495484218\n",
            "step: 250, loss: 6.624701200053096e-05\n",
            "step: 260, loss: 4.170846659690142e-05\n",
            "step: 270, loss: 0.00013171069440431893\n",
            "step: 280, loss: 0.0031224871054291725\n",
            "step: 290, loss: 0.00010433148418087512\n",
            "step: 300, loss: 0.0012554337736219168\n",
            "step: 310, loss: 0.00012256018817424774\n",
            "step: 320, loss: 0.00011402986274333671\n",
            "step: 330, loss: 0.00011101758718723431\n",
            "step: 340, loss: 5.6342956668231636e-05\n",
            "step: 350, loss: 0.0002458595554344356\n",
            "step: 360, loss: 0.013479214161634445\n",
            "step: 370, loss: 0.00018090457888320088\n",
            "step: 380, loss: 0.00022084172815084457\n",
            "step: 390, loss: 0.005433692596852779\n",
            "step: 400, loss: 0.00018017413094639778\n",
            "step: 410, loss: 0.0006480916636064649\n",
            "step: 420, loss: 0.007784045301377773\n",
            "step: 430, loss: 0.0026068936567753553\n",
            "step: 440, loss: 4.387778608361259e-05\n",
            "step: 450, loss: 0.00039212327101267874\n",
            "step: 460, loss: 0.011836127378046513\n",
            "step: 470, loss: 2.3986329324543476e-05\n",
            "step: 480, loss: 0.0001276315888389945\n",
            "step: 490, loss: 0.020660489797592163\n",
            "step: 500, loss: 0.00013708742335438728\n",
            "step: 510, loss: 0.009684021584689617\n",
            "step: 520, loss: 1.920340582728386e-05\n",
            "step: 530, loss: 0.00565453851595521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9280806229958772, f1=0.923076923076923, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015003428561612964\n",
            "step: 10, loss: 0.004843758884817362\n",
            "step: 20, loss: 6.438112905016169e-05\n",
            "step: 30, loss: 0.015284527093172073\n",
            "step: 40, loss: 0.00023280714231077582\n",
            "step: 50, loss: 0.0002563239831943065\n",
            "step: 60, loss: 4.4482865632744506e-05\n",
            "step: 70, loss: 0.0001271293731406331\n",
            "step: 80, loss: 0.000892245676368475\n",
            "step: 90, loss: 0.00010497234325157478\n",
            "step: 100, loss: 0.025473689660429955\n",
            "step: 110, loss: 0.00034878987935371697\n",
            "step: 120, loss: 0.0003538321179803461\n",
            "step: 130, loss: 8.154456736519933e-05\n",
            "step: 140, loss: 0.00014352859579958022\n",
            "step: 150, loss: 0.00019487336976453662\n",
            "step: 160, loss: 3.0950384825700894e-05\n",
            "step: 170, loss: 0.007285665720701218\n",
            "step: 180, loss: 0.0002944829757325351\n",
            "step: 190, loss: 0.0005169240175746381\n",
            "step: 200, loss: 7.097709749359637e-05\n",
            "step: 210, loss: 0.002658301964402199\n",
            "step: 220, loss: 0.002350015565752983\n",
            "step: 230, loss: 0.0018597589805722237\n",
            "step: 240, loss: 0.0009411817300133407\n",
            "step: 250, loss: 0.0002213567349826917\n",
            "step: 260, loss: 0.00013640968245454133\n",
            "step: 270, loss: 3.4993296139873564e-05\n",
            "step: 280, loss: 0.00022547272965312004\n",
            "step: 290, loss: 0.008919261395931244\n",
            "step: 300, loss: 0.00018859909323509783\n",
            "step: 310, loss: 0.001730430289171636\n",
            "step: 320, loss: 0.0028710917104035616\n",
            "step: 330, loss: 0.0017885328270494938\n",
            "step: 340, loss: 0.0016336010303348303\n",
            "step: 350, loss: 0.004935577046126127\n",
            "step: 360, loss: 0.00024014578957576305\n",
            "step: 370, loss: 0.0012134222779422998\n",
            "step: 380, loss: 0.0001120098095270805\n",
            "step: 390, loss: 0.000407145795179531\n",
            "step: 400, loss: 0.000891119590960443\n",
            "step: 410, loss: 0.0010353360557928681\n",
            "step: 420, loss: 5.9812980907736346e-05\n",
            "step: 430, loss: 0.0475681871175766\n",
            "step: 440, loss: 0.004315834958106279\n",
            "step: 450, loss: 0.000124356200103648\n",
            "step: 460, loss: 0.0011796399485319853\n",
            "step: 470, loss: 0.00014778849435970187\n",
            "step: 480, loss: 0.002062553074210882\n",
            "step: 490, loss: 0.005143217742443085\n",
            "step: 500, loss: 0.00036132728564552963\n",
            "step: 510, loss: 0.034076251089572906\n",
            "step: 520, loss: 0.0001364851777907461\n",
            "step: 530, loss: 0.00019268396135885268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.928505957836847, f1=0.9248395967002749, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010587840806692839\n",
            "step: 10, loss: 4.0428098145639524e-05\n",
            "step: 20, loss: 4.213955980958417e-05\n",
            "step: 30, loss: 0.00015189223631750792\n",
            "step: 40, loss: 0.0004969554138369858\n",
            "step: 50, loss: 0.0007784549961797893\n",
            "step: 60, loss: 1.7083963030017912e-05\n",
            "step: 70, loss: 0.00016067142132669687\n",
            "step: 80, loss: 7.344842015299946e-05\n",
            "step: 90, loss: 0.00040079187601804733\n",
            "step: 100, loss: 0.0011265343055129051\n",
            "step: 110, loss: 0.000255791557719931\n",
            "step: 120, loss: 0.0005412030150182545\n",
            "step: 130, loss: 0.029115255922079086\n",
            "step: 140, loss: 0.0016571817686781287\n",
            "step: 150, loss: 6.731542089255527e-05\n",
            "step: 160, loss: 0.0005862706457264721\n",
            "step: 170, loss: 0.00016616255743429065\n",
            "step: 180, loss: 8.930249168770388e-05\n",
            "step: 190, loss: 0.00018014713714364916\n",
            "step: 200, loss: 0.00019093368609901518\n",
            "step: 210, loss: 0.0061459727585315704\n",
            "step: 220, loss: 0.000415733054978773\n",
            "step: 230, loss: 0.006838504690676928\n",
            "step: 240, loss: 7.648475730093196e-05\n",
            "step: 250, loss: 6.137334275990725e-05\n",
            "step: 260, loss: 0.0004901115316897631\n",
            "step: 270, loss: 0.0002926003362517804\n",
            "step: 280, loss: 4.104817708139308e-05\n",
            "step: 290, loss: 0.08789726346731186\n",
            "step: 300, loss: 0.0004964372492395341\n",
            "step: 310, loss: 0.0086137093603611\n",
            "step: 320, loss: 0.0005968902842141688\n",
            "step: 330, loss: 8.889229502528906e-05\n",
            "step: 340, loss: 0.0020492514595389366\n",
            "step: 350, loss: 0.0004912580479867756\n",
            "step: 360, loss: 0.00028543107328005135\n",
            "step: 370, loss: 0.001385555020533502\n",
            "step: 380, loss: 0.00426469836384058\n",
            "step: 390, loss: 0.00392710929736495\n",
            "step: 400, loss: 0.00014741692575626075\n",
            "step: 410, loss: 5.7619814469944686e-05\n",
            "step: 420, loss: 0.0004829696554224938\n",
            "step: 430, loss: 0.00027551420498639345\n",
            "step: 440, loss: 0.00012256513582542539\n",
            "step: 450, loss: 0.0018081931630149484\n",
            "step: 460, loss: 0.00014871392340864986\n",
            "step: 470, loss: 0.0038288370706140995\n",
            "step: 480, loss: 5.782215885119513e-05\n",
            "step: 490, loss: 5.830790178151801e-05\n",
            "step: 500, loss: 0.0005048480816185474\n",
            "step: 510, loss: 0.0020215401891618967\n",
            "step: 520, loss: 0.00030105069163255394\n",
            "step: 530, loss: 0.00676416140049696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9251263206247129, f1=0.9236430542778289, best_f1=0.9212344541685858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.23956070234999e-05\n",
            "step: 10, loss: 0.0005285475053824484\n",
            "step: 20, loss: 5.304102160152979e-05\n",
            "step: 30, loss: 0.0002057532110484317\n",
            "step: 40, loss: 0.024829963222146034\n",
            "step: 50, loss: 0.013058536686003208\n",
            "step: 60, loss: 0.00018493420793674886\n",
            "step: 70, loss: 0.00038567109731957316\n",
            "step: 80, loss: 7.870338595239446e-05\n",
            "step: 90, loss: 7.101098162820563e-05\n",
            "step: 100, loss: 0.005725916940718889\n",
            "step: 110, loss: 0.0010993409669026732\n",
            "step: 120, loss: 0.0004999577649869025\n",
            "step: 130, loss: 0.06292437016963959\n",
            "step: 140, loss: 0.00023989970213733613\n",
            "step: 150, loss: 8.180052100215107e-05\n",
            "step: 160, loss: 0.0003311385225970298\n",
            "step: 170, loss: 0.047392960637807846\n",
            "step: 180, loss: 0.0002764771634247154\n",
            "step: 190, loss: 0.00010768631909741089\n",
            "step: 200, loss: 0.0005052877822890878\n",
            "step: 210, loss: 0.00022666405129712075\n",
            "step: 220, loss: 0.0013387297512963414\n",
            "step: 230, loss: 0.0007282859878614545\n",
            "step: 240, loss: 0.0004138728545513004\n",
            "step: 250, loss: 3.221457882318646e-05\n",
            "step: 260, loss: 0.006094148848205805\n",
            "step: 270, loss: 0.0008099455153569579\n",
            "step: 280, loss: 0.00026352249551564455\n",
            "step: 290, loss: 2.1870735508855432e-05\n",
            "step: 300, loss: 4.12547196901869e-05\n",
            "step: 310, loss: 0.0006411564536392689\n",
            "step: 320, loss: 0.00027683531516231596\n",
            "step: 330, loss: 0.00011553544027265161\n",
            "step: 340, loss: 0.00020988352480344474\n",
            "step: 350, loss: 4.2882740672212094e-05\n",
            "step: 360, loss: 0.0024158854503184557\n",
            "step: 370, loss: 4.505993638304062e-05\n",
            "step: 380, loss: 0.0003833126393146813\n",
            "step: 390, loss: 0.0010678964899852872\n",
            "step: 400, loss: 0.0001541090023238212\n",
            "step: 410, loss: 0.00023277146101463586\n",
            "step: 420, loss: 0.0010536487679928541\n",
            "step: 430, loss: 9.31398244574666e-05\n",
            "step: 440, loss: 0.00040544537478126585\n",
            "step: 450, loss: 4.3451200326671824e-05\n",
            "step: 460, loss: 5.067072925157845e-05\n",
            "step: 470, loss: 0.0036464217118918896\n",
            "step: 480, loss: 1.836166666180361e-05\n",
            "step: 490, loss: 1.7568238035892136e-05\n",
            "step: 500, loss: 0.07054716348648071\n",
            "step: 510, loss: 0.00011602212907746434\n",
            "step: 520, loss: 6.377444515237585e-05\n",
            "step: 530, loss: 7.842217746656388e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.924860853432282, f1=0.9213691026827011, best_f1=0.9212344541685858\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 232.73it/s]\n",
            "load_f1 = 0.9300373134328358\n",
            "real_f1 = 0.926852288815479\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 247.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rVRw-HgNFH"
      },
      "source": [
        "# BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ifpsOJMgNFH"
      },
      "source": [
        "## BASELINE STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE7bDM3EgNFI",
        "outputId": "788c94da-d3d8-42e2-f4f3-479b52c7ac66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.587847888469696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4, f1=0.3, best_f1=0.3\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4625902473926544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4545454545454545, f1=0.3703703703703704, best_f1=0.3703703703703704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.42398425936698914\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4782608695652174, f1=0.4444444444444445, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24234354496002197\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5454545454545454, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22795715928077698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.56, f1=0.5384615384615384, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31284844875335693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.64, f1=0.5384615384615384, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1613273024559021\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5789473684210527, f1=0.5333333333333333, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0679832175374031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6153846153846153, f1=0.6153846153846153, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031599095091223717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.689655172413793, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025994719937443733\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6666666666666666, f1=0.6000000000000001, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00458888616412878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6666666666666666, f1=0.5806451612903226, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003829779801890254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6666666666666666, f1=0.5384615384615384, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004520161543041468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6470588235294117, f1=0.5161290322580646, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003078499808907509\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6470588235294117, f1=0.5161290322580646, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031352125108242035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6470588235294117, f1=0.5161290322580646, best_f1=0.5714285714285714\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 118424.35it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5925925925925927\n",
            "real_f1 = 0.6451612903225806\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjgIIwdgNFK",
        "outputId": "f17cf7fd-89d8-477a-9cb2-0aba28e51db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6242391467094421\n",
            "step: 10, loss: 0.6266266107559204\n",
            "step: 20, loss: 0.4266778826713562\n",
            "step: 30, loss: 0.20987480878829956\n",
            "step: 40, loss: 0.21948955953121185\n",
            "step: 50, loss: 0.03903389722108841\n",
            "step: 60, loss: 0.08414338529109955\n",
            "step: 70, loss: 0.07976385951042175\n",
            "step: 80, loss: 0.33827802538871765\n",
            "step: 90, loss: 0.13062845170497894\n",
            "step: 100, loss: 0.007839958183467388\n",
            "step: 110, loss: 0.1834685504436493\n",
            "step: 120, loss: 0.027192767709493637\n",
            "step: 130, loss: 0.08174163848161697\n",
            "step: 140, loss: 0.010036395862698555\n",
            "step: 150, loss: 0.05165475234389305\n",
            "step: 160, loss: 0.005022560711950064\n",
            "step: 170, loss: 0.04914340004324913\n",
            "step: 180, loss: 0.025765379890799522\n",
            "step: 190, loss: 0.20060782134532928\n",
            "step: 200, loss: 0.006420792546123266\n",
            "step: 210, loss: 0.007259258534759283\n",
            "step: 220, loss: 0.006005681585520506\n",
            "step: 230, loss: 0.009157905355095863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9704545454545453, f1=0.9691428571428571, best_f1=0.9691428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006734682247042656\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.005011027678847313\n",
            "step: 20, loss: 0.18964609503746033\n",
            "step: 30, loss: 0.20212820172309875\n",
            "step: 40, loss: 0.024414876475930214\n",
            "step: 50, loss: 0.003727738047018647\n",
            "step: 60, loss: 0.029692169278860092\n",
            "step: 70, loss: 0.15628230571746826\n",
            "step: 80, loss: 0.0027548186480998993\n",
            "step: 90, loss: 0.0179987121373415\n",
            "step: 100, loss: 0.017782285809516907\n",
            "step: 110, loss: 0.04863859340548515\n",
            "step: 120, loss: 0.02533699944615364\n",
            "step: 130, loss: 0.008118835277855396\n",
            "step: 140, loss: 0.0026431081350892782\n",
            "step: 150, loss: 0.0026938566006720066\n",
            "step: 160, loss: 0.003721051849424839\n",
            "step: 170, loss: 0.0019007865339517593\n",
            "step: 180, loss: 0.00328909233212471\n",
            "step: 190, loss: 0.025836337357759476\n",
            "step: 200, loss: 0.018005317077040672\n",
            "step: 210, loss: 0.0014629296492785215\n",
            "step: 220, loss: 0.14592087268829346\n",
            "step: 230, loss: 0.008364034816622734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9798657718120806, f1=0.9731543624161074, best_f1=0.9731543624161074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01884802244603634\n",
            "step: 10, loss: 0.004649294540286064\n",
            "step: 20, loss: 0.02532583847641945\n",
            "step: 30, loss: 0.07122429460287094\n",
            "step: 40, loss: 0.05856494605541229\n",
            "step: 50, loss: 0.028740694746375084\n",
            "step: 60, loss: 0.011538926512002945\n",
            "step: 70, loss: 0.003411197802051902\n",
            "step: 80, loss: 0.0015420462004840374\n",
            "step: 90, loss: 0.13930128514766693\n",
            "step: 100, loss: 0.0017618043348193169\n",
            "step: 110, loss: 0.0065087322145700455\n",
            "step: 120, loss: 0.010865382850170135\n",
            "step: 130, loss: 0.018178531900048256\n",
            "step: 140, loss: 0.01644333265721798\n",
            "step: 150, loss: 0.0013578759972006083\n",
            "step: 160, loss: 0.05789363756775856\n",
            "step: 170, loss: 0.010008550249040127\n",
            "step: 180, loss: 0.010929792188107967\n",
            "step: 190, loss: 0.001306407735683024\n",
            "step: 200, loss: 0.05442251265048981\n",
            "step: 210, loss: 0.0016160471132025123\n",
            "step: 220, loss: 0.0007812193944118917\n",
            "step: 230, loss: 0.0007493476150557399\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9854096520763187, f1=0.978675645342312, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005946848541498184\n",
            "step: 10, loss: 0.0002395348419668153\n",
            "step: 20, loss: 0.0026561312843114138\n",
            "step: 30, loss: 0.00022864299535285681\n",
            "step: 40, loss: 0.005014744587242603\n",
            "step: 50, loss: 0.0017519722459837794\n",
            "step: 60, loss: 0.000516207714099437\n",
            "step: 70, loss: 0.00043679913505911827\n",
            "step: 80, loss: 0.0007838273886591196\n",
            "step: 90, loss: 0.0033117716666311026\n",
            "step: 100, loss: 0.00987301953136921\n",
            "step: 110, loss: 0.0003554584691300988\n",
            "step: 120, loss: 0.069847971200943\n",
            "step: 130, loss: 0.03356263041496277\n",
            "step: 140, loss: 0.0039002487901598215\n",
            "step: 150, loss: 0.05954599007964134\n",
            "step: 160, loss: 0.0004829337412957102\n",
            "step: 170, loss: 0.0038269897922873497\n",
            "step: 180, loss: 0.000782207353040576\n",
            "step: 190, loss: 0.005645770579576492\n",
            "step: 200, loss: 0.0067988173104822636\n",
            "step: 210, loss: 0.10684774070978165\n",
            "step: 220, loss: 0.0001986689167097211\n",
            "step: 230, loss: 0.10741663724184036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9772209567198178, f1=0.9761634506242906, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00043843622552230954\n",
            "step: 10, loss: 0.0004450827545952052\n",
            "step: 20, loss: 0.007556903641670942\n",
            "step: 30, loss: 0.0031345109455287457\n",
            "step: 40, loss: 0.0023928177542984486\n",
            "step: 50, loss: 0.00033020967384800315\n",
            "step: 60, loss: 0.00017091994232032448\n",
            "step: 70, loss: 0.00012592741404660046\n",
            "step: 80, loss: 0.00011333748261677101\n",
            "step: 90, loss: 0.00011910550529137254\n",
            "step: 100, loss: 0.00015466798504348844\n",
            "step: 110, loss: 0.00015740703383926302\n",
            "step: 120, loss: 0.00018963584443554282\n",
            "step: 130, loss: 0.00037531161797232926\n",
            "step: 140, loss: 0.002814473584294319\n",
            "step: 150, loss: 0.0035831390414386988\n",
            "step: 160, loss: 0.0003451882512308657\n",
            "step: 170, loss: 0.014148014597594738\n",
            "step: 180, loss: 0.0028120276983827353\n",
            "step: 190, loss: 0.0005953506333753467\n",
            "step: 200, loss: 0.0017511597834527493\n",
            "step: 210, loss: 0.00019288867770228535\n",
            "step: 220, loss: 0.0017949535977095366\n",
            "step: 230, loss: 0.0007231712224893272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9819413092550789, f1=0.9775280898876404, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001273146364837885\n",
            "step: 10, loss: 0.0011284974170848727\n",
            "step: 20, loss: 0.002785699674859643\n",
            "step: 30, loss: 0.002658666344359517\n",
            "step: 40, loss: 0.00036400932003743947\n",
            "step: 50, loss: 0.0006389129557646811\n",
            "step: 60, loss: 0.0004940041108056903\n",
            "step: 70, loss: 0.003793338779360056\n",
            "step: 80, loss: 0.0004817110311705619\n",
            "step: 90, loss: 0.0007529695867560804\n",
            "step: 100, loss: 0.04906781390309334\n",
            "step: 110, loss: 0.00028653652407228947\n",
            "step: 120, loss: 0.0002506764139980078\n",
            "step: 130, loss: 0.002290213480591774\n",
            "step: 140, loss: 9.816742385737598e-05\n",
            "step: 150, loss: 0.00034354435047134757\n",
            "step: 160, loss: 0.0011052436893805861\n",
            "step: 170, loss: 0.0020446819253265858\n",
            "step: 180, loss: 0.007260940503329039\n",
            "step: 190, loss: 0.07082823663949966\n",
            "step: 200, loss: 0.0006163923535495996\n",
            "step: 210, loss: 0.0004880149499513209\n",
            "step: 220, loss: 0.00020276207942515612\n",
            "step: 230, loss: 0.0023500409442931414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9822616407982262, f1=0.9722530521642618, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009309685789048672\n",
            "step: 10, loss: 0.00018144186469726264\n",
            "step: 20, loss: 0.00012862158473581076\n",
            "step: 30, loss: 0.00016694888472557068\n",
            "step: 40, loss: 0.00010027035023085773\n",
            "step: 50, loss: 0.00011321876081638038\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.0005217221332713962\n",
            "step: 70, loss: 0.0001745181652950123\n",
            "step: 80, loss: 8.240487659350038e-05\n",
            "step: 90, loss: 5.951894127065316e-05\n",
            "step: 100, loss: 7.987471326487139e-05\n",
            "step: 110, loss: 0.0029743502382189035\n",
            "step: 120, loss: 0.0002692114794626832\n",
            "step: 130, loss: 8.637017162982374e-05\n",
            "step: 140, loss: 0.00011315302981529385\n",
            "step: 150, loss: 0.0029333655256778\n",
            "step: 160, loss: 0.19679898023605347\n",
            "step: 170, loss: 0.021651841700077057\n",
            "step: 180, loss: 0.0016980415675789118\n",
            "step: 190, loss: 0.00012389480252750218\n",
            "step: 200, loss: 0.03608980029821396\n",
            "step: 210, loss: 0.00013511300494428724\n",
            "step: 220, loss: 0.001460359781049192\n",
            "step: 230, loss: 0.00027362830587662756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9832026875699889, f1=0.9752808988764046, best_f1=0.978675645342312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.54528441070579e-05\n",
            "step: 10, loss: 0.00027005665469914675\n",
            "step: 20, loss: 0.0009143012575805187\n",
            "step: 30, loss: 9.01569874258712e-05\n",
            "step: 40, loss: 0.0002694849972613156\n",
            "step: 50, loss: 0.0030428965110331774\n",
            "step: 60, loss: 5.887470251764171e-05\n",
            "step: 70, loss: 0.00041880778735503554\n",
            "step: 80, loss: 0.0001769327063811943\n",
            "step: 90, loss: 5.2903156756656244e-05\n",
            "step: 100, loss: 9.190425043925643e-05\n",
            "step: 110, loss: 0.000942235579714179\n",
            "step: 120, loss: 0.00011243653716519475\n",
            "step: 130, loss: 7.205130532383919e-05\n",
            "step: 140, loss: 5.5775060900487006e-05\n",
            "step: 150, loss: 8.712062845006585e-05\n",
            "step: 160, loss: 6.0613558162003756e-05\n",
            "step: 170, loss: 7.841604383429512e-05\n",
            "step: 180, loss: 9.572452836437151e-05\n",
            "step: 190, loss: 0.0001878864277387038\n",
            "step: 200, loss: 9.297556971432641e-05\n",
            "step: 210, loss: 0.00015506109048146755\n",
            "step: 220, loss: 8.219949813792482e-05\n",
            "step: 230, loss: 7.545949483755976e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9854423292273236, f1=0.9753363228699552, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021428630861919373\n",
            "step: 10, loss: 0.00010019232286140323\n",
            "step: 20, loss: 0.0010927984258159995\n",
            "step: 30, loss: 0.00040846530464477837\n",
            "step: 40, loss: 0.02896260842680931\n",
            "step: 50, loss: 0.00017212006787303835\n",
            "step: 60, loss: 0.00010341432789573446\n",
            "step: 70, loss: 0.0003498884616419673\n",
            "step: 80, loss: 0.0002141281438525766\n",
            "step: 90, loss: 0.00018773482588585466\n",
            "step: 100, loss: 0.00010532359010539949\n",
            "step: 110, loss: 6.473105167970061e-05\n",
            "step: 120, loss: 0.00010345910413889214\n",
            "step: 130, loss: 9.390605555381626e-05\n",
            "step: 140, loss: 5.297815368976444e-05\n",
            "step: 150, loss: 0.00025733542861416936\n",
            "step: 160, loss: 5.916849841014482e-05\n",
            "step: 170, loss: 0.0005734579754061997\n",
            "step: 180, loss: 0.008062329143285751\n",
            "step: 190, loss: 0.00041701726149767637\n",
            "step: 200, loss: 0.0011711736442521214\n",
            "step: 210, loss: 0.0006917942664586008\n",
            "step: 220, loss: 5.600725853582844e-05\n",
            "step: 230, loss: 0.00010797091817948967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9810479375696767, f1=0.9776785714285714, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005657189758494496\n",
            "step: 10, loss: 6.631190626649186e-05\n",
            "step: 20, loss: 5.879594755242579e-05\n",
            "step: 30, loss: 0.0001246041210833937\n",
            "step: 40, loss: 6.777016824344173e-05\n",
            "step: 50, loss: 9.5322699053213e-05\n",
            "step: 60, loss: 6.888202915433794e-05\n",
            "step: 70, loss: 0.0006961021572351456\n",
            "step: 80, loss: 6.12148578511551e-05\n",
            "step: 90, loss: 0.00021262151130940765\n",
            "step: 100, loss: 6.099780512158759e-05\n",
            "step: 110, loss: 0.0006304444978013635\n",
            "step: 120, loss: 0.006001182831823826\n",
            "step: 130, loss: 0.014362671412527561\n",
            "step: 140, loss: 0.00021584739442914724\n",
            "step: 150, loss: 0.00766013702377677\n",
            "step: 160, loss: 4.3665138946380466e-05\n",
            "step: 170, loss: 5.2137780585326254e-05\n",
            "step: 180, loss: 0.00021246541291475296\n",
            "step: 190, loss: 0.005673495586961508\n",
            "step: 200, loss: 6.458538700826466e-05\n",
            "step: 210, loss: 0.00040924979839473963\n",
            "step: 220, loss: 0.0003851281653624028\n",
            "step: 230, loss: 0.14747408032417297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9807909604519773, f1=0.9763779527559054, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002835665945895016\n",
            "step: 10, loss: 0.0003551999107003212\n",
            "step: 20, loss: 0.0009087966755032539\n",
            "step: 30, loss: 0.005054493900388479\n",
            "step: 40, loss: 0.006894343998283148\n",
            "step: 50, loss: 0.0005085880402475595\n",
            "step: 60, loss: 0.0006411659414879978\n",
            "step: 70, loss: 0.0007753423997201025\n",
            "step: 80, loss: 6.157390453154221e-05\n",
            "step: 90, loss: 0.0001418109895894304\n",
            "step: 100, loss: 0.00010551218292675912\n",
            "step: 110, loss: 6.306271825451404e-05\n",
            "step: 120, loss: 6.356678204610944e-05\n",
            "step: 130, loss: 0.0003809318004641682\n",
            "step: 140, loss: 9.300765668740496e-05\n",
            "step: 150, loss: 0.04026822745800018\n",
            "step: 160, loss: 0.0002026966103585437\n",
            "step: 170, loss: 0.02306022122502327\n",
            "step: 180, loss: 0.00012595363659784198\n",
            "step: 190, loss: 7.116395863704383e-05\n",
            "step: 200, loss: 0.00013622864207718521\n",
            "step: 210, loss: 6.798862887080759e-05\n",
            "step: 220, loss: 5.192446769797243e-05\n",
            "step: 230, loss: 7.989338337210938e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9820224719101124, f1=0.9776286353467561, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.106719906209037e-05\n",
            "step: 10, loss: 0.0008192838868126273\n",
            "step: 20, loss: 0.00020069845777470618\n",
            "step: 30, loss: 8.02193972049281e-05\n",
            "step: 40, loss: 0.0017333857249468565\n",
            "step: 50, loss: 0.0006105411448515952\n",
            "step: 60, loss: 0.0002027969603659585\n",
            "step: 70, loss: 0.0009720682282932103\n",
            "step: 80, loss: 9.761184628587216e-05\n",
            "step: 90, loss: 0.0002806873235385865\n",
            "step: 100, loss: 5.1907289162045345e-05\n",
            "step: 110, loss: 0.00012767137377522886\n",
            "step: 120, loss: 6.02511026954744e-05\n",
            "step: 130, loss: 4.905588139081374e-05\n",
            "step: 140, loss: 6.839012348791584e-05\n",
            "step: 150, loss: 0.00015218163025565445\n",
            "step: 160, loss: 7.563504914287478e-05\n",
            "step: 170, loss: 7.498580089304596e-05\n",
            "step: 180, loss: 0.0001313331740675494\n",
            "step: 190, loss: 4.263297523721121e-05\n",
            "step: 200, loss: 3.1354837119579315e-05\n",
            "step: 210, loss: 7.171991455834359e-05\n",
            "step: 220, loss: 6.256998312892392e-05\n",
            "step: 230, loss: 0.09196766465902328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9808342728297633, f1=0.9774774774774775, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002935676311608404\n",
            "step: 10, loss: 0.0002728900290094316\n",
            "step: 20, loss: 0.00023785790835972875\n",
            "step: 30, loss: 0.0001943271781783551\n",
            "step: 40, loss: 0.00012857289402745664\n",
            "step: 50, loss: 6.170063716126606e-05\n",
            "step: 60, loss: 0.00011577363329706714\n",
            "step: 70, loss: 3.620789721026085e-05\n",
            "step: 80, loss: 4.668175461119972e-05\n",
            "step: 90, loss: 0.0003917403519153595\n",
            "step: 100, loss: 3.1093950383365154e-05\n",
            "step: 110, loss: 0.020878398790955544\n",
            "step: 120, loss: 0.031689129769802094\n",
            "step: 130, loss: 5.998308188281953e-05\n",
            "step: 140, loss: 4.0216782508650795e-05\n",
            "step: 150, loss: 3.614160232245922e-05\n",
            "step: 160, loss: 9.616209717933089e-05\n",
            "step: 170, loss: 0.0002546050527598709\n",
            "step: 180, loss: 0.00014580163406208158\n",
            "step: 190, loss: 2.8829294024035335e-05\n",
            "step: 200, loss: 0.00011674908455461264\n",
            "step: 210, loss: 5.513658470590599e-05\n",
            "step: 220, loss: 6.815020606154576e-05\n",
            "step: 230, loss: 3.351144914631732e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9820224719101124, f1=0.978675645342312, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.625323006417602e-05\n",
            "step: 10, loss: 0.0004734426620416343\n",
            "step: 20, loss: 3.879715586663224e-05\n",
            "step: 30, loss: 0.04048345237970352\n",
            "step: 40, loss: 4.5732766011497006e-05\n",
            "step: 50, loss: 0.0001706711482256651\n",
            "step: 60, loss: 9.290183515986428e-05\n",
            "step: 70, loss: 4.523789175436832e-05\n",
            "step: 80, loss: 3.9751197618898004e-05\n",
            "step: 90, loss: 3.8357644370989874e-05\n",
            "step: 100, loss: 5.2922081522410735e-05\n",
            "step: 110, loss: 4.0520899347029626e-05\n",
            "step: 120, loss: 2.4914052119129337e-05\n",
            "step: 130, loss: 3.806376116699539e-05\n",
            "step: 140, loss: 8.173857349902391e-05\n",
            "step: 150, loss: 4.991953755961731e-05\n",
            "step: 160, loss: 0.0018807606538757682\n",
            "step: 170, loss: 3.184967135894112e-05\n",
            "step: 180, loss: 4.345334673416801e-05\n",
            "step: 190, loss: 8.103580330498517e-05\n",
            "step: 200, loss: 5.62880486540962e-05\n",
            "step: 210, loss: 0.00014572619693353772\n",
            "step: 220, loss: 4.594515485223383e-05\n",
            "step: 230, loss: 9.359698742628098e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9799107142857142, f1=0.9733333333333333, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.2863474189070985e-05\n",
            "step: 10, loss: 2.700745790207293e-05\n",
            "step: 20, loss: 5.45789698662702e-05\n",
            "step: 30, loss: 0.00017816766921896487\n",
            "step: 40, loss: 6.657730409642681e-05\n",
            "step: 50, loss: 0.0055821724236011505\n",
            "step: 60, loss: 4.998300573788583e-05\n",
            "step: 70, loss: 3.881930388160981e-05\n",
            "step: 80, loss: 4.08367341151461e-05\n",
            "step: 90, loss: 7.518009806517512e-05\n",
            "step: 100, loss: 4.6973298594821244e-05\n",
            "step: 110, loss: 4.151550092501566e-05\n",
            "step: 120, loss: 0.00018712000746745616\n",
            "step: 130, loss: 6.994367140578106e-05\n",
            "step: 140, loss: 3.1265415600501e-05\n",
            "step: 150, loss: 0.00021422161080408841\n",
            "step: 160, loss: 3.1123570806812495e-05\n",
            "step: 170, loss: 2.3807870093150996e-05\n",
            "step: 180, loss: 0.005106108263134956\n",
            "step: 190, loss: 0.00011858913057949394\n",
            "step: 200, loss: 0.00013466682867147028\n",
            "step: 210, loss: 3.1127561669563875e-05\n",
            "step: 220, loss: 4.2443221900612116e-05\n",
            "step: 230, loss: 4.13936359109357e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.980963045912654, f1=0.9764837625979844, best_f1=0.9753363228699552\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 160.61it/s]\n",
            "load_f1 = 0.9853768278965129\n",
            "real_f1 = 0.9831649831649831\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.43it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7uL6uPgNFK"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyxG2qpgNFL",
        "outputId": "1940bb20-bb86-4065-9681-8ac334b4bce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6250797510147095\n",
            "step: 10, loss: 0.5887187123298645\n",
            "step: 20, loss: 0.5641039609909058\n",
            "step: 30, loss: 0.254226952791214\n",
            "step: 40, loss: 0.18394486606121063\n",
            "step: 50, loss: 0.16127072274684906\n",
            "step: 60, loss: 0.05452246218919754\n",
            "step: 70, loss: 0.11159274727106094\n",
            "step: 80, loss: 0.06729274988174438\n",
            "step: 90, loss: 0.1419660747051239\n",
            "step: 100, loss: 0.11345984786748886\n",
            "step: 110, loss: 0.08659932017326355\n",
            "step: 120, loss: 0.06891027092933655\n",
            "step: 130, loss: 0.03424646332859993\n",
            "step: 140, loss: 0.18460920453071594\n",
            "step: 150, loss: 0.08082921802997589\n",
            "step: 160, loss: 0.05057058855891228\n",
            "step: 170, loss: 0.23384569585323334\n",
            "step: 180, loss: 0.17403142154216766\n",
            "step: 190, loss: 0.00946894846856594\n",
            "step: 200, loss: 0.19892019033432007\n",
            "step: 210, loss: 0.04324018582701683\n",
            "step: 220, loss: 0.14251936972141266\n",
            "step: 230, loss: 0.1410512924194336\n",
            "step: 240, loss: 0.20615412294864655\n",
            "step: 250, loss: 0.042177148163318634\n",
            "step: 260, loss: 0.07147485762834549\n",
            "step: 270, loss: 0.02912270277738571\n",
            "step: 280, loss: 0.15640710294246674\n",
            "step: 290, loss: 0.048650048673152924\n",
            "step: 300, loss: 0.05845653638243675\n",
            "step: 310, loss: 0.3174642026424408\n",
            "step: 320, loss: 0.11068795621395111\n",
            "step: 330, loss: 0.09096035361289978\n",
            "step: 340, loss: 0.06337586790323257\n",
            "step: 350, loss: 0.21253876388072968\n",
            "step: 360, loss: 0.044987402856349945\n",
            "step: 370, loss: 0.060936544090509415\n",
            "step: 380, loss: 0.04929341748356819\n",
            "step: 390, loss: 0.14294859766960144\n",
            "step: 400, loss: 0.280866801738739\n",
            "step: 410, loss: 0.0502798818051815\n",
            "step: 420, loss: 0.011007744818925858\n",
            "step: 430, loss: 0.1935080885887146\n",
            "step: 440, loss: 0.014311693608760834\n",
            "step: 450, loss: 0.026838410645723343\n",
            "step: 460, loss: 0.01501809898763895\n",
            "step: 470, loss: 0.19534605741500854\n",
            "step: 480, loss: 0.04733283072710037\n",
            "step: 490, loss: 0.16334964334964752\n",
            "step: 500, loss: 0.059830646961927414\n",
            "step: 510, loss: 0.06238624453544617\n",
            "step: 520, loss: 0.12384489923715591\n",
            "step: 530, loss: 0.004485259298235178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.917883211678832, f1=0.912471655328798, best_f1=0.912471655328798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05575508624315262\n",
            "step: 10, loss: 0.04307973384857178\n",
            "step: 20, loss: 0.015043281018733978\n",
            "step: 30, loss: 0.1440950483083725\n",
            "step: 40, loss: 0.0964432954788208\n",
            "step: 50, loss: 0.23632508516311646\n",
            "step: 60, loss: 0.01331079937517643\n",
            "step: 70, loss: 0.02331741712987423\n",
            "step: 80, loss: 0.03925775736570358\n",
            "step: 90, loss: 0.012857876718044281\n",
            "step: 100, loss: 0.09309569001197815\n",
            "step: 110, loss: 0.038390807807445526\n",
            "step: 120, loss: 0.08959243446588516\n",
            "step: 130, loss: 0.16528062522411346\n",
            "step: 140, loss: 0.1490229070186615\n",
            "step: 150, loss: 0.06494206935167313\n",
            "step: 160, loss: 0.03392515331506729\n",
            "step: 170, loss: 0.06273775547742844\n",
            "step: 180, loss: 0.04298676922917366\n",
            "step: 190, loss: 0.09694219380617142\n",
            "step: 200, loss: 0.07954411953687668\n",
            "step: 210, loss: 0.0157123152166605\n",
            "step: 220, loss: 0.06496986746788025\n",
            "step: 230, loss: 0.014374416321516037\n",
            "step: 240, loss: 0.12080831825733185\n",
            "step: 250, loss: 0.01435163151472807\n",
            "step: 260, loss: 0.003679788438603282\n",
            "step: 270, loss: 0.30254876613616943\n",
            "step: 280, loss: 0.030759580433368683\n",
            "step: 290, loss: 0.02699473686516285\n",
            "step: 300, loss: 0.09248959273099899\n",
            "step: 310, loss: 0.01018295157700777\n",
            "step: 320, loss: 0.10172227770090103\n",
            "step: 330, loss: 0.027853596955537796\n",
            "step: 340, loss: 0.007860050536692142\n",
            "step: 350, loss: 0.0903649851679802\n",
            "step: 360, loss: 0.23068548738956451\n",
            "step: 370, loss: 0.14737568795681\n",
            "step: 380, loss: 0.05501451715826988\n",
            "step: 390, loss: 0.0870572179555893\n",
            "step: 400, loss: 0.08602914214134216\n",
            "step: 410, loss: 0.006495498586446047\n",
            "step: 420, loss: 0.10560794919729233\n",
            "step: 430, loss: 0.00508938729763031\n",
            "step: 440, loss: 0.12025956064462662\n",
            "step: 450, loss: 0.033345095813274384\n",
            "step: 460, loss: 0.0577237568795681\n",
            "step: 470, loss: 0.08972654491662979\n",
            "step: 480, loss: 0.14948216080665588\n",
            "step: 490, loss: 0.01971556432545185\n",
            "step: 500, loss: 0.39314115047454834\n",
            "step: 510, loss: 0.014877265319228172\n",
            "step: 520, loss: 0.03733058273792267\n",
            "step: 530, loss: 0.054137397557497025\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9264029438822446, f1=0.921451538814883, best_f1=0.921451538814883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03350672125816345\n",
            "step: 10, loss: 0.043358806520700455\n",
            "step: 20, loss: 0.09821482747793198\n",
            "step: 30, loss: 0.12902672588825226\n",
            "step: 40, loss: 0.00540546141564846\n",
            "step: 50, loss: 0.13534078001976013\n",
            "step: 60, loss: 0.016144467517733574\n",
            "step: 70, loss: 0.0058717066422104836\n",
            "step: 80, loss: 0.0031798393465578556\n",
            "step: 90, loss: 0.0042688134126365185\n",
            "step: 100, loss: 0.043144553899765015\n",
            "step: 110, loss: 0.018871426582336426\n",
            "step: 120, loss: 0.00902608223259449\n",
            "step: 130, loss: 0.007947873324155807\n",
            "step: 140, loss: 0.030906593427062035\n",
            "step: 150, loss: 0.11669730395078659\n",
            "step: 160, loss: 0.11036603897809982\n",
            "step: 170, loss: 0.12382383644580841\n",
            "step: 180, loss: 0.07790783047676086\n",
            "step: 190, loss: 0.061001796275377274\n",
            "step: 200, loss: 0.018860001116991043\n",
            "step: 210, loss: 0.042724233120679855\n",
            "step: 220, loss: 0.03272182121872902\n",
            "step: 230, loss: 0.1782936006784439\n",
            "step: 240, loss: 0.016425805166363716\n",
            "step: 250, loss: 0.050965528935194016\n",
            "step: 260, loss: 0.010764403268694878\n",
            "step: 270, loss: 0.005381994415074587\n",
            "step: 280, loss: 0.16192898154258728\n",
            "step: 290, loss: 0.015924474224448204\n",
            "step: 300, loss: 0.04317039996385574\n",
            "step: 310, loss: 0.022734813392162323\n",
            "step: 320, loss: 0.022821839898824692\n",
            "step: 330, loss: 0.002735887421295047\n",
            "step: 340, loss: 0.004083527252078056\n",
            "step: 350, loss: 0.011556323617696762\n",
            "step: 360, loss: 0.039252039045095444\n",
            "step: 370, loss: 0.003089030273258686\n",
            "step: 380, loss: 0.00238172453828156\n",
            "step: 390, loss: 0.003438380779698491\n",
            "step: 400, loss: 0.017920251935720444\n",
            "step: 410, loss: 0.012894213199615479\n",
            "step: 420, loss: 0.1600913256406784\n",
            "step: 430, loss: 0.0410740002989769\n",
            "step: 440, loss: 0.0017713082488626242\n",
            "step: 450, loss: 0.08658625930547714\n",
            "step: 460, loss: 0.018259769305586815\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 470, loss: 0.10197696834802628\n",
            "step: 480, loss: 0.002909349277615547\n",
            "step: 490, loss: 0.00199108081869781\n",
            "step: 500, loss: 0.006668124347925186\n",
            "step: 510, loss: 0.0013921897625550628\n",
            "step: 520, loss: 0.031242666766047478\n",
            "step: 530, loss: 0.006094149313867092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9286367795059469, f1=0.9240681086056144, best_f1=0.9240681086056144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025600600987672806\n",
            "step: 10, loss: 0.007161113899201155\n",
            "step: 20, loss: 0.005896615330129862\n",
            "step: 30, loss: 0.001799346529878676\n",
            "step: 40, loss: 0.01949252188205719\n",
            "step: 50, loss: 0.004759685136377811\n",
            "step: 60, loss: 0.004541624337434769\n",
            "step: 70, loss: 0.05139601603150368\n",
            "step: 80, loss: 0.09737925976514816\n",
            "step: 90, loss: 0.029389524832367897\n",
            "step: 100, loss: 0.0029048144351691008\n",
            "step: 110, loss: 0.02109166979789734\n",
            "step: 120, loss: 0.07819315046072006\n",
            "step: 130, loss: 0.0026675884146243334\n",
            "step: 140, loss: 0.0014740137849003077\n",
            "step: 150, loss: 0.001168881542980671\n",
            "step: 160, loss: 0.05407945439219475\n",
            "step: 170, loss: 0.0007988677243702114\n",
            "step: 180, loss: 0.0007073352462612092\n",
            "step: 190, loss: 0.005302552133798599\n",
            "step: 200, loss: 0.009596772491931915\n",
            "step: 210, loss: 0.10828985273838043\n",
            "step: 220, loss: 0.02457197941839695\n",
            "step: 230, loss: 0.029144002124667168\n",
            "step: 240, loss: 0.011445720680058002\n",
            "step: 250, loss: 0.0054041254334151745\n",
            "step: 260, loss: 0.0011280931066721678\n",
            "step: 270, loss: 0.0009744773269630969\n",
            "step: 280, loss: 0.006387227214872837\n",
            "step: 290, loss: 0.013363951817154884\n",
            "step: 300, loss: 0.0013764805626124144\n",
            "step: 310, loss: 0.01847042143344879\n",
            "step: 320, loss: 0.047149863094091415\n",
            "step: 330, loss: 0.0010966008994728327\n",
            "step: 340, loss: 0.033693477511405945\n",
            "step: 350, loss: 0.0025040870532393456\n",
            "step: 360, loss: 0.013027782551944256\n",
            "step: 370, loss: 0.0063424562104046345\n",
            "step: 380, loss: 0.003128227312117815\n",
            "step: 390, loss: 0.054766945540905\n",
            "step: 400, loss: 0.002718186704441905\n",
            "step: 410, loss: 0.004542618524283171\n",
            "step: 420, loss: 0.026281608268618584\n",
            "step: 430, loss: 0.09608111530542374\n",
            "step: 440, loss: 0.010396978817880154\n",
            "step: 450, loss: 0.008348627015948296\n",
            "step: 460, loss: 0.03843431919813156\n",
            "step: 470, loss: 0.008388269692659378\n",
            "step: 480, loss: 0.08647622913122177\n",
            "step: 490, loss: 0.04724844545125961\n",
            "step: 500, loss: 0.0084998095408082\n",
            "step: 510, loss: 0.054417531937360764\n",
            "step: 520, loss: 0.10872946679592133\n",
            "step: 530, loss: 0.027241822332143784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9301675977653632, f1=0.9206200093940817, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04678723216056824\n",
            "step: 10, loss: 0.0794137716293335\n",
            "step: 20, loss: 0.002552502788603306\n",
            "step: 30, loss: 0.015337117947638035\n",
            "step: 40, loss: 0.0010740571888163686\n",
            "step: 50, loss: 0.015589299611747265\n",
            "step: 60, loss: 0.1786511391401291\n",
            "step: 70, loss: 0.0031260009855031967\n",
            "step: 80, loss: 0.002460130024701357\n",
            "step: 90, loss: 0.0012757471995428205\n",
            "step: 100, loss: 0.0011343442602083087\n",
            "step: 110, loss: 0.0006459447322413325\n",
            "step: 120, loss: 0.00028681635740213096\n",
            "step: 130, loss: 0.0007757823332212865\n",
            "step: 140, loss: 0.03126660734415054\n",
            "step: 150, loss: 0.004438495263457298\n",
            "step: 160, loss: 0.020635532215237617\n",
            "step: 170, loss: 0.001968069700524211\n",
            "step: 180, loss: 0.06545449793338776\n",
            "step: 190, loss: 0.0007100109942257404\n",
            "step: 200, loss: 0.00044499948853626847\n",
            "step: 210, loss: 0.0008868767763487995\n",
            "step: 220, loss: 0.0014533885987475514\n",
            "step: 230, loss: 0.004593241959810257\n",
            "step: 240, loss: 0.0010927114635705948\n",
            "step: 250, loss: 0.004249231424182653\n",
            "step: 260, loss: 0.0009696796769276261\n",
            "step: 270, loss: 0.0018741838866844773\n",
            "step: 280, loss: 0.05056661739945412\n",
            "step: 290, loss: 0.0949772372841835\n",
            "step: 300, loss: 0.012931006029248238\n",
            "step: 310, loss: 0.0021907268092036247\n",
            "step: 320, loss: 0.03754889592528343\n",
            "step: 330, loss: 0.030360383912920952\n",
            "step: 340, loss: 0.0014180365251377225\n",
            "step: 350, loss: 0.014971042983233929\n",
            "step: 360, loss: 0.002958579920232296\n",
            "step: 370, loss: 0.007045377977192402\n",
            "step: 380, loss: 0.0009864837629720569\n",
            "step: 390, loss: 0.0008893618360161781\n",
            "step: 400, loss: 0.006767706014215946\n",
            "step: 410, loss: 0.006080777384340763\n",
            "step: 420, loss: 0.0036097296979278326\n",
            "step: 430, loss: 0.0009906936902552843\n",
            "step: 440, loss: 0.10266191512346268\n",
            "step: 450, loss: 0.08649110049009323\n",
            "step: 460, loss: 0.000967235944699496\n",
            "step: 470, loss: 0.004393104929476976\n",
            "step: 480, loss: 0.0015203234506770968\n",
            "step: 490, loss: 0.0010624362621456385\n",
            "step: 500, loss: 0.05089491605758667\n",
            "step: 510, loss: 0.019357478246092796\n",
            "step: 520, loss: 0.030453043058514595\n",
            "step: 530, loss: 0.0011756070889532566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.928772258669166, f1=0.9231494578029231, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009808354079723358\n",
            "step: 10, loss: 0.0004252186918165535\n",
            "step: 20, loss: 0.14085093140602112\n",
            "step: 30, loss: 0.001645568641833961\n",
            "step: 40, loss: 0.00025147420819848776\n",
            "step: 50, loss: 0.07115717977285385\n",
            "step: 60, loss: 0.0004055105964653194\n",
            "step: 70, loss: 0.0003944097552448511\n",
            "step: 80, loss: 0.001841581892222166\n",
            "step: 90, loss: 0.0021935799159109592\n",
            "step: 100, loss: 0.000953779264818877\n",
            "step: 110, loss: 0.002639350015670061\n",
            "step: 120, loss: 0.009910915978252888\n",
            "step: 130, loss: 0.0004932831507176161\n",
            "step: 140, loss: 0.018125537782907486\n",
            "step: 150, loss: 0.001288301544263959\n",
            "step: 160, loss: 0.0020090951584279537\n",
            "step: 170, loss: 0.003197686281055212\n",
            "step: 180, loss: 0.0013022196944803\n",
            "step: 190, loss: 0.0018449501367285848\n",
            "step: 200, loss: 0.0012633450096473098\n",
            "step: 210, loss: 0.0010828456142917275\n",
            "step: 220, loss: 0.0006337978411465883\n",
            "step: 230, loss: 0.13861051201820374\n",
            "step: 240, loss: 0.08574697375297546\n",
            "step: 250, loss: 0.0002463820856064558\n",
            "step: 260, loss: 0.00016386697825510055\n",
            "step: 270, loss: 0.052335191518068314\n",
            "step: 280, loss: 0.0010386898647993803\n",
            "step: 290, loss: 0.00034951764973811805\n",
            "step: 300, loss: 0.0010704539017751813\n",
            "step: 310, loss: 0.00046130508417263627\n",
            "step: 320, loss: 0.004442573990672827\n",
            "step: 330, loss: 0.0035402141511440277\n",
            "step: 340, loss: 0.01443234272301197\n",
            "step: 350, loss: 0.06911952048540115\n",
            "step: 360, loss: 0.0028464454226195812\n",
            "step: 370, loss: 0.006664742715656757\n",
            "step: 380, loss: 0.0017402579542249441\n",
            "step: 390, loss: 0.003874958259984851\n",
            "step: 400, loss: 0.00039665820077061653\n",
            "step: 410, loss: 0.00015473642270080745\n",
            "step: 420, loss: 0.0003209371934644878\n",
            "step: 430, loss: 0.00033329735742881894\n",
            "step: 440, loss: 0.0006968402303755283\n",
            "step: 450, loss: 0.0002613603719510138\n",
            "step: 460, loss: 0.00027816632064059377\n",
            "step: 470, loss: 0.05831604078412056\n",
            "step: 480, loss: 0.006130289286375046\n",
            "step: 490, loss: 0.004735032096505165\n",
            "step: 500, loss: 0.00012573861749842763\n",
            "step: 510, loss: 0.0009442645823583007\n",
            "step: 520, loss: 0.0016253796638920903\n",
            "step: 530, loss: 0.054195817559957504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.921189240207645, f1=0.9158091674462113, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003558097407221794\n",
            "step: 10, loss: 0.049276202917099\n",
            "step: 20, loss: 0.0034421905875205994\n",
            "step: 30, loss: 0.0007514334283769131\n",
            "step: 40, loss: 0.0007709570345468819\n",
            "step: 50, loss: 0.00173224660102278\n",
            "step: 60, loss: 0.0005805365508422256\n",
            "step: 70, loss: 0.014695494435727596\n",
            "step: 80, loss: 0.0012292233295738697\n",
            "step: 90, loss: 0.00025742221623659134\n",
            "step: 100, loss: 9.151455742539838e-05\n",
            "step: 110, loss: 8.801645890343934e-05\n",
            "step: 120, loss: 5.617044371319935e-05\n",
            "step: 130, loss: 0.0011019896483048797\n",
            "step: 140, loss: 6.125242362031713e-05\n",
            "step: 150, loss: 0.0015426294412463903\n",
            "step: 160, loss: 0.002501194830983877\n",
            "step: 170, loss: 0.00033446215093135834\n",
            "step: 180, loss: 0.003721419954672456\n",
            "step: 190, loss: 0.020262954756617546\n",
            "step: 200, loss: 0.0012120881583541632\n",
            "step: 210, loss: 0.0012567524099722505\n",
            "step: 220, loss: 4.620674371835776e-05\n",
            "step: 230, loss: 0.009867546148598194\n",
            "step: 240, loss: 0.005841247737407684\n",
            "step: 250, loss: 0.0023788856342434883\n",
            "step: 260, loss: 4.5486689487006515e-05\n",
            "step: 270, loss: 9.08671718207188e-05\n",
            "step: 280, loss: 0.0001082979651982896\n",
            "step: 290, loss: 6.887660856591538e-05\n",
            "step: 300, loss: 0.00030479003908112645\n",
            "step: 310, loss: 0.044493384659290314\n",
            "step: 320, loss: 9.9831166153308e-05\n",
            "step: 330, loss: 0.021962374448776245\n",
            "step: 340, loss: 0.03671609237790108\n",
            "step: 350, loss: 0.0037886332720518112\n",
            "step: 360, loss: 0.0030612838454544544\n",
            "step: 370, loss: 0.0006196188041940331\n",
            "step: 380, loss: 0.009239790961146355\n",
            "step: 390, loss: 0.0003866578917950392\n",
            "step: 400, loss: 0.0053094299510121346\n",
            "step: 410, loss: 0.00842411257326603\n",
            "step: 420, loss: 0.008312501944601536\n",
            "step: 430, loss: 0.00021985068451613188\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 440, loss: 0.00018194655422121286\n",
            "step: 450, loss: 0.00047102911048568785\n",
            "step: 460, loss: 0.00022373758838512003\n",
            "step: 470, loss: 0.00590928690508008\n",
            "step: 480, loss: 0.06678459793329239\n",
            "step: 490, loss: 0.002481652656570077\n",
            "step: 500, loss: 0.00034243465051986277\n",
            "step: 510, loss: 0.0008732402347959578\n",
            "step: 520, loss: 0.061503857374191284\n",
            "step: 530, loss: 0.005253934301435947\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9264432029795159, f1=0.9246511627906977, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010681685525923967\n",
            "step: 10, loss: 0.019327916204929352\n",
            "step: 20, loss: 0.00024009613844100386\n",
            "step: 30, loss: 0.10002224147319794\n",
            "step: 40, loss: 0.0005211925017647445\n",
            "step: 50, loss: 0.0045039597898721695\n",
            "step: 60, loss: 0.0017194128595292568\n",
            "step: 70, loss: 7.954756438266486e-05\n",
            "step: 80, loss: 0.003419416956603527\n",
            "step: 90, loss: 0.0002956589451059699\n",
            "step: 100, loss: 0.00470691779628396\n",
            "step: 110, loss: 0.0007129745790734887\n",
            "step: 120, loss: 0.0001772093091858551\n",
            "step: 130, loss: 0.003558488329872489\n",
            "step: 140, loss: 0.0011929775355383754\n",
            "step: 150, loss: 0.002941492246463895\n",
            "step: 160, loss: 0.0007233510841615498\n",
            "step: 170, loss: 0.006228739861398935\n",
            "step: 180, loss: 0.00014920582179911435\n",
            "step: 190, loss: 0.004560065921396017\n",
            "step: 200, loss: 0.001870226114988327\n",
            "step: 210, loss: 0.005379363428801298\n",
            "step: 220, loss: 0.003542690770700574\n",
            "step: 230, loss: 0.008711464703083038\n",
            "step: 240, loss: 0.0001434280857210979\n",
            "step: 250, loss: 0.00018898029520642012\n",
            "step: 260, loss: 6.898371793795377e-05\n",
            "step: 270, loss: 0.0004364904307294637\n",
            "step: 280, loss: 0.0017610914073884487\n",
            "step: 290, loss: 0.0001185991641250439\n",
            "step: 300, loss: 0.0004627776506822556\n",
            "step: 310, loss: 0.00014240187010727823\n",
            "step: 320, loss: 0.07187262922525406\n",
            "step: 330, loss: 0.00014403574459720403\n",
            "step: 340, loss: 0.00016933638835325837\n",
            "step: 350, loss: 0.00048175553092733026\n",
            "step: 360, loss: 0.0006620739004574716\n",
            "step: 370, loss: 0.0008134344825521111\n",
            "step: 380, loss: 0.0064505282789468765\n",
            "step: 390, loss: 0.11817541718482971\n",
            "step: 400, loss: 8.181968587450683e-05\n",
            "step: 410, loss: 0.0011810759315267205\n",
            "step: 420, loss: 0.0031540344934910536\n",
            "step: 430, loss: 0.018356677144765854\n",
            "step: 440, loss: 0.049425724893808365\n",
            "step: 450, loss: 0.00023413210874423385\n",
            "step: 460, loss: 0.00041438505286350846\n",
            "step: 470, loss: 0.0006701119709759951\n",
            "step: 480, loss: 0.0002686635416466743\n",
            "step: 490, loss: 0.0005494492361322045\n",
            "step: 500, loss: 0.00384723162278533\n",
            "step: 510, loss: 0.0002472735068295151\n",
            "step: 520, loss: 0.001934673055075109\n",
            "step: 530, loss: 0.00011866749991895631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.925891181988743, f1=0.9197559831065226, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018322638934478164\n",
            "step: 10, loss: 7.32460684957914e-05\n",
            "step: 20, loss: 0.0025750305503606796\n",
            "step: 30, loss: 0.00017799035413190722\n",
            "step: 40, loss: 5.8040219300892204e-05\n",
            "step: 50, loss: 0.0008959707338362932\n",
            "step: 60, loss: 0.00011272783012827858\n",
            "step: 70, loss: 5.343021985027008e-05\n",
            "step: 80, loss: 0.0012175935553386807\n",
            "step: 90, loss: 4.817948865820654e-05\n",
            "step: 100, loss: 0.0006645424873568118\n",
            "step: 110, loss: 0.00015861117572057992\n",
            "step: 120, loss: 0.0004600349930115044\n",
            "step: 130, loss: 0.008751949295401573\n",
            "step: 140, loss: 0.00033132240059785545\n",
            "step: 150, loss: 6.767792365280911e-05\n",
            "step: 160, loss: 0.0012173072900623083\n",
            "step: 170, loss: 0.005235464312136173\n",
            "step: 180, loss: 5.388877616496757e-05\n",
            "step: 190, loss: 0.00012580177281051874\n",
            "step: 200, loss: 0.0006382153951562941\n",
            "step: 210, loss: 3.897122587659396e-05\n",
            "step: 220, loss: 0.00017750260303728282\n",
            "step: 230, loss: 3.933612970286049e-05\n",
            "step: 240, loss: 7.523285603383556e-05\n",
            "step: 250, loss: 0.003910810220986605\n",
            "step: 260, loss: 0.00034600007347762585\n",
            "step: 270, loss: 2.3911954485811293e-05\n",
            "step: 280, loss: 6.801607378292829e-05\n",
            "step: 290, loss: 0.15972809493541718\n",
            "step: 300, loss: 3.6874487705063075e-05\n",
            "step: 310, loss: 0.023187890648841858\n",
            "step: 320, loss: 0.00018402919522486627\n",
            "step: 330, loss: 0.0005695761647075415\n",
            "step: 340, loss: 0.00012776489893440157\n",
            "step: 350, loss: 0.00019346906628925353\n",
            "step: 360, loss: 0.00014842880773358047\n",
            "step: 370, loss: 0.0018816690426319838\n",
            "step: 380, loss: 0.0027462656144052744\n",
            "step: 390, loss: 3.483272303128615e-05\n",
            "step: 400, loss: 0.0012335613137111068\n",
            "step: 410, loss: 8.78600767464377e-05\n",
            "step: 420, loss: 3.761177868000232e-05\n",
            "step: 430, loss: 6.789965846110135e-05\n",
            "step: 440, loss: 0.0011669030645862222\n",
            "step: 450, loss: 5.064687866251916e-05\n",
            "step: 460, loss: 5.04674062540289e-05\n",
            "step: 470, loss: 0.00027950244839303195\n",
            "step: 480, loss: 2.7487656552693807e-05\n",
            "step: 490, loss: 0.0031056164298206568\n",
            "step: 500, loss: 0.0021301929373294115\n",
            "step: 510, loss: 4.602320768753998e-05\n",
            "step: 520, loss: 7.827287481632084e-05\n",
            "step: 530, loss: 0.000196693858015351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9287696577243294, f1=0.9280742459396751, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.280570515722502e-05\n",
            "step: 10, loss: 1.941188020282425e-05\n",
            "step: 20, loss: 2.053293792414479e-05\n",
            "step: 30, loss: 2.3386837710859254e-05\n",
            "step: 40, loss: 0.00026092250482179224\n",
            "step: 50, loss: 0.05659900978207588\n",
            "step: 60, loss: 8.020392124308273e-05\n",
            "step: 70, loss: 1.731118572934065e-05\n",
            "step: 80, loss: 2.655308344401419e-05\n",
            "step: 90, loss: 1.8801256373990327e-05\n",
            "step: 100, loss: 1.7869968360173516e-05\n",
            "step: 110, loss: 1.4331028069136664e-05\n",
            "step: 120, loss: 0.006422759499400854\n",
            "step: 130, loss: 0.00011536549573065713\n",
            "step: 140, loss: 0.006148782558739185\n",
            "step: 150, loss: 4.032504148199223e-05\n",
            "step: 160, loss: 0.0004001730994787067\n",
            "step: 170, loss: 0.00015595208969898522\n",
            "step: 180, loss: 3.946812284993939e-05\n",
            "step: 190, loss: 0.0005406655254773796\n",
            "step: 200, loss: 9.999216854339465e-05\n",
            "step: 210, loss: 5.3858668252360076e-05\n",
            "step: 220, loss: 6.096833021729253e-05\n",
            "step: 230, loss: 0.0018148522358387709\n",
            "step: 240, loss: 3.0818100640317425e-05\n",
            "step: 250, loss: 2.0872372260782868e-05\n",
            "step: 260, loss: 0.00032127872691489756\n",
            "step: 270, loss: 3.2249168725684285e-05\n",
            "step: 280, loss: 6.41747610643506e-05\n",
            "step: 290, loss: 0.001617533271200955\n",
            "step: 300, loss: 2.498835419828538e-05\n",
            "step: 310, loss: 0.00013737908739130944\n",
            "step: 320, loss: 0.00580306863412261\n",
            "step: 330, loss: 2.5706973246997222e-05\n",
            "step: 340, loss: 0.00012392154894769192\n",
            "step: 350, loss: 6.615990423597395e-05\n",
            "step: 360, loss: 0.00025394820841029286\n",
            "step: 370, loss: 0.005366286262869835\n",
            "step: 380, loss: 0.0006430480862036347\n",
            "step: 390, loss: 8.684357453603297e-05\n",
            "step: 400, loss: 0.00023228874488268048\n",
            "step: 410, loss: 0.0020217266865074635\n",
            "step: 420, loss: 2.669048262760043e-05\n",
            "step: 430, loss: 2.091695751005318e-05\n",
            "step: 440, loss: 0.002319267252460122\n",
            "step: 450, loss: 5.0208604079671204e-05\n",
            "step: 460, loss: 0.00015813414938747883\n",
            "step: 470, loss: 6.083244807086885e-05\n",
            "step: 480, loss: 2.4154247512342408e-05\n",
            "step: 490, loss: 3.269115404691547e-05\n",
            "step: 500, loss: 0.0023034897167235613\n",
            "step: 510, loss: 1.8644768715603277e-05\n",
            "step: 520, loss: 2.198233596573118e-05\n",
            "step: 530, loss: 0.0010348965879529715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9261682242990654, f1=0.9260808926080892, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.464337351033464e-05\n",
            "step: 10, loss: 0.00035198492696508765\n",
            "step: 20, loss: 8.546280150767416e-05\n",
            "step: 30, loss: 5.5291275202762336e-05\n",
            "step: 40, loss: 0.0042645628564059734\n",
            "step: 50, loss: 0.003049843944609165\n",
            "step: 60, loss: 0.04738639295101166\n",
            "step: 70, loss: 0.00019773990788962692\n",
            "step: 80, loss: 7.309587090276182e-05\n",
            "step: 90, loss: 0.04685129597783089\n",
            "step: 100, loss: 0.0011344114318490028\n",
            "step: 110, loss: 7.22767726983875e-05\n",
            "step: 120, loss: 5.2957719162805006e-05\n",
            "step: 130, loss: 0.00010243301949230954\n",
            "step: 140, loss: 0.15743772685527802\n",
            "step: 150, loss: 0.009928376413881779\n",
            "step: 160, loss: 0.0045269387774169445\n",
            "step: 170, loss: 0.00013291335199028254\n",
            "step: 180, loss: 0.00016614531341474503\n",
            "step: 190, loss: 0.0013954900205135345\n",
            "step: 200, loss: 0.005682979244738817\n",
            "step: 210, loss: 0.0015501350862905383\n",
            "step: 220, loss: 0.001319295377470553\n",
            "step: 230, loss: 7.520099461544305e-05\n",
            "step: 240, loss: 0.00021431346249300987\n",
            "step: 250, loss: 4.3935648136539385e-05\n",
            "step: 260, loss: 4.322181484894827e-05\n",
            "step: 270, loss: 8.111312490655109e-05\n",
            "step: 280, loss: 0.0002545974566601217\n",
            "step: 290, loss: 0.00011961490963585675\n",
            "step: 300, loss: 0.00014126359019428492\n",
            "step: 310, loss: 0.00024061588919721544\n",
            "step: 320, loss: 0.00018603485659696162\n",
            "step: 330, loss: 5.540153870242648e-05\n",
            "step: 340, loss: 0.00026183240697719157\n",
            "step: 350, loss: 0.00011255427671130747\n",
            "step: 360, loss: 0.0006147646927274764\n",
            "step: 370, loss: 0.0009131599217653275\n",
            "step: 380, loss: 5.465500726131722e-05\n",
            "step: 390, loss: 2.666813088580966e-05\n",
            "step: 400, loss: 0.00011891303438460454\n",
            "step: 410, loss: 6.741393735865131e-05\n",
            "step: 420, loss: 0.0004338362778071314\n",
            "step: 430, loss: 2.9946180802653544e-05\n",
            "step: 440, loss: 0.05042971670627594\n",
            "step: 450, loss: 7.29370949557051e-05\n",
            "step: 460, loss: 0.000399062322685495\n",
            "step: 470, loss: 0.00020445515110623091\n",
            "step: 480, loss: 0.00010211890185019001\n",
            "step: 490, loss: 0.001080626156181097\n",
            "step: 500, loss: 0.0022543095983564854\n",
            "step: 510, loss: 0.000629057758487761\n",
            "step: 520, loss: 7.558259676443413e-05\n",
            "step: 530, loss: 0.00012601594789884984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9238095238095239, f1=0.9135802469135803, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9328239179449156e-05\n",
            "step: 10, loss: 2.1803567506140098e-05\n",
            "step: 20, loss: 9.131793194683269e-05\n",
            "step: 30, loss: 0.008962715975940228\n",
            "step: 40, loss: 2.347249392187223e-05\n",
            "step: 50, loss: 0.04392389953136444\n",
            "step: 60, loss: 0.003539496101438999\n",
            "step: 70, loss: 7.163918780861422e-05\n",
            "step: 80, loss: 0.00018449542403686792\n",
            "step: 90, loss: 3.6885070585412905e-05\n",
            "step: 100, loss: 0.00017176076653413475\n",
            "step: 110, loss: 0.02156962640583515\n",
            "step: 120, loss: 3.4601751394802704e-05\n",
            "step: 130, loss: 0.0009424585732631385\n",
            "step: 140, loss: 3.9404221752192825e-05\n",
            "step: 150, loss: 0.00012809262261725962\n",
            "step: 160, loss: 6.33262752671726e-05\n",
            "step: 170, loss: 0.0003017761337105185\n",
            "step: 180, loss: 0.0006991034606471658\n",
            "step: 190, loss: 0.0009499956504441798\n",
            "step: 200, loss: 3.75003473891411e-05\n",
            "step: 210, loss: 2.756245703494642e-05\n",
            "step: 220, loss: 0.00013074545131530613\n",
            "step: 230, loss: 2.221663271484431e-05\n",
            "step: 240, loss: 0.00012975769641343504\n",
            "step: 250, loss: 1.9680192053783685e-05\n",
            "step: 260, loss: 4.958543286193162e-05\n",
            "step: 270, loss: 0.22519567608833313\n",
            "step: 280, loss: 0.004756158217787743\n",
            "step: 290, loss: 0.0007305640610866249\n",
            "step: 300, loss: 0.0022171675227582455\n",
            "step: 310, loss: 0.0009207238326780498\n",
            "step: 320, loss: 0.0014140127459540963\n",
            "step: 330, loss: 0.00016537723422516137\n",
            "step: 340, loss: 4.013722718809731e-05\n",
            "step: 350, loss: 0.00012881914153695107\n",
            "step: 360, loss: 0.0014276469592005014\n",
            "step: 370, loss: 0.025622021406888962\n",
            "step: 380, loss: 4.973714385414496e-05\n",
            "step: 390, loss: 7.260053098434582e-05\n",
            "step: 400, loss: 2.4515615223208442e-05\n",
            "step: 410, loss: 0.0004349726950749755\n",
            "step: 420, loss: 0.0103434594348073\n",
            "step: 430, loss: 0.008696286007761955\n",
            "step: 440, loss: 0.0002660305181052536\n",
            "step: 450, loss: 0.0006597579922527075\n",
            "step: 460, loss: 0.000266455375822261\n",
            "step: 470, loss: 2.916334051406011e-05\n",
            "step: 480, loss: 0.0010487495455890894\n",
            "step: 490, loss: 0.0002804962277878076\n",
            "step: 500, loss: 2.7438742108643055e-05\n",
            "step: 510, loss: 0.00010361581371398643\n",
            "step: 520, loss: 7.54788561607711e-05\n",
            "step: 530, loss: 3.387190372450277e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9264229523368811, f1=0.9216589861751152, best_f1=0.9206200093940817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030044084414839745\n",
            "step: 10, loss: 3.0176268410286866e-05\n",
            "step: 20, loss: 3.38489880959969e-05\n",
            "step: 30, loss: 3.394703526282683e-05\n",
            "step: 40, loss: 2.826891068252735e-05\n",
            "step: 50, loss: 0.002880196087062359\n",
            "step: 60, loss: 4.2937128455378115e-05\n",
            "step: 70, loss: 6.149071123218164e-05\n",
            "step: 80, loss: 1.6767255146987736e-05\n",
            "step: 90, loss: 0.010203626938164234\n",
            "step: 100, loss: 0.003064661519601941\n",
            "step: 110, loss: 2.083089748339262e-05\n",
            "step: 120, loss: 1.9270499251433648e-05\n",
            "step: 130, loss: 3.123718124697916e-05\n",
            "step: 140, loss: 5.286977466312237e-05\n",
            "step: 150, loss: 3.2971827749861404e-05\n",
            "step: 160, loss: 2.2935935703571886e-05\n",
            "step: 170, loss: 0.0002354475436732173\n",
            "step: 180, loss: 3.545021536410786e-05\n",
            "step: 190, loss: 0.0027327649295330048\n",
            "step: 200, loss: 2.0049123122589663e-05\n",
            "step: 210, loss: 1.9672745111165568e-05\n",
            "step: 220, loss: 1.787349538062699e-05\n",
            "step: 230, loss: 6.357634265441447e-05\n",
            "step: 240, loss: 8.787531987763941e-05\n",
            "step: 250, loss: 4.5763852540403605e-05\n",
            "step: 260, loss: 1.4733316675119568e-05\n",
            "step: 270, loss: 1.3630671674036421e-05\n",
            "step: 280, loss: 2.201958341174759e-05\n",
            "step: 290, loss: 2.285383379785344e-05\n",
            "step: 300, loss: 2.4109360310831107e-05\n",
            "step: 310, loss: 0.002224232302978635\n",
            "step: 320, loss: 0.008574427105486393\n",
            "step: 330, loss: 0.0018666093237698078\n",
            "step: 340, loss: 5.091579077998176e-05\n",
            "step: 350, loss: 7.990965968929231e-05\n",
            "step: 360, loss: 2.05443830054719e-05\n",
            "step: 370, loss: 3.230053698644042e-05\n",
            "step: 380, loss: 2.367320666962769e-05\n",
            "step: 390, loss: 7.648969040019438e-05\n",
            "step: 400, loss: 1.55342131620273e-05\n",
            "step: 410, loss: 0.0002971876529045403\n",
            "step: 420, loss: 1.8220011043013074e-05\n",
            "step: 430, loss: 0.0028848163783550262\n",
            "step: 440, loss: 2.134878195647616e-05\n",
            "step: 450, loss: 2.5174589609378017e-05\n",
            "step: 460, loss: 0.00018817614181898534\n",
            "step: 470, loss: 1.522494221717352e-05\n",
            "step: 480, loss: 2.1206851670285687e-05\n",
            "step: 490, loss: 1.779914236976765e-05\n",
            "step: 500, loss: 1.1581802937143948e-05\n",
            "step: 510, loss: 7.636936061317101e-05\n",
            "step: 520, loss: 0.0005365345859900117\n",
            "step: 530, loss: 1.7229191143997014e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9308755760368663, f1=0.9273897058823529, best_f1=0.9273897058823529\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005738034378737211\n",
            "step: 10, loss: 1.2870736100012437e-05\n",
            "step: 20, loss: 1.424534639227204e-05\n",
            "step: 30, loss: 1.8707905837800354e-05\n",
            "step: 40, loss: 2.6720606911112554e-05\n",
            "step: 50, loss: 6.854740786366165e-05\n",
            "step: 60, loss: 1.9862471162923612e-05\n",
            "step: 70, loss: 0.00017036657663993537\n",
            "step: 80, loss: 0.0001857751194620505\n",
            "step: 90, loss: 2.365498585277237e-05\n",
            "step: 100, loss: 1.7724580175126903e-05\n",
            "step: 110, loss: 1.715463622531388e-05\n",
            "step: 120, loss: 2.0790286725969054e-05\n",
            "step: 130, loss: 0.030519628897309303\n",
            "step: 140, loss: 1.3440698239719495e-05\n",
            "step: 150, loss: 1.8733899196377024e-05\n",
            "step: 160, loss: 1.9445620637270622e-05\n",
            "step: 170, loss: 0.0010717620607465506\n",
            "step: 180, loss: 4.832641570828855e-05\n",
            "step: 190, loss: 2.1110181478434242e-05\n",
            "step: 200, loss: 3.361606286489405e-05\n",
            "step: 210, loss: 1.7404276150045916e-05\n",
            "step: 220, loss: 2.0715684513561428e-05\n",
            "step: 230, loss: 1.7028018191922456e-05\n",
            "step: 240, loss: 3.6543297028401867e-05\n",
            "step: 250, loss: 1.4200647456164006e-05\n",
            "step: 260, loss: 1.4904713680152781e-05\n",
            "step: 270, loss: 1.6722453437978402e-05\n",
            "step: 280, loss: 1.7784192095859908e-05\n",
            "step: 290, loss: 0.02804575115442276\n",
            "step: 300, loss: 2.180349110858515e-05\n",
            "step: 310, loss: 0.12597854435443878\n",
            "step: 320, loss: 2.7446900276117958e-05\n",
            "step: 330, loss: 2.4984634364955127e-05\n",
            "step: 340, loss: 1.939727553690318e-05\n",
            "step: 350, loss: 6.21384460828267e-05\n",
            "step: 360, loss: 3.2784755603643134e-05\n",
            "step: 370, loss: 1.9233362763770856e-05\n",
            "step: 380, loss: 8.082168642431498e-05\n",
            "step: 390, loss: 0.008119452744722366\n",
            "step: 400, loss: 0.00035801532794721425\n",
            "step: 410, loss: 1.4625260519096628e-05\n",
            "step: 420, loss: 1.2345489267318044e-05\n",
            "step: 430, loss: 6.181476783240214e-05\n",
            "step: 440, loss: 3.9146285416791216e-05\n",
            "step: 450, loss: 1.933016028488055e-05\n",
            "step: 460, loss: 1.6957203115453012e-05\n",
            "step: 470, loss: 4.6121196646708995e-05\n",
            "step: 480, loss: 2.4812057745293714e-05\n",
            "step: 490, loss: 3.1491461413679644e-05\n",
            "step: 500, loss: 1.4476272554020397e-05\n",
            "step: 510, loss: 2.3114096620702185e-05\n",
            "step: 520, loss: 1.475567933084676e-05\n",
            "step: 530, loss: 1.6357509593944997e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9294605809128631, f1=0.9247015610651974, best_f1=0.9273897058823529\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.0789757147431374e-05\n",
            "step: 10, loss: 1.644689473323524e-05\n",
            "step: 20, loss: 2.5582261514500715e-05\n",
            "step: 30, loss: 2.764306373137515e-05\n",
            "step: 40, loss: 0.030485982075333595\n",
            "step: 50, loss: 2.9182287107687443e-05\n",
            "step: 60, loss: 1.1578070370887872e-05\n",
            "step: 70, loss: 0.00027608033269643784\n",
            "step: 80, loss: 1.540383709652815e-05\n",
            "step: 90, loss: 1.8562783225206658e-05\n",
            "step: 100, loss: 2.767964542726986e-05\n",
            "step: 110, loss: 0.00010680985724320635\n",
            "step: 120, loss: 0.01481596753001213\n",
            "step: 130, loss: 0.08010175824165344\n",
            "step: 140, loss: 1.7039052181644365e-05\n",
            "step: 150, loss: 2.425438469799701e-05\n",
            "step: 160, loss: 1.8447190086590126e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 170, loss: 1.925920878420584e-05\n",
            "step: 180, loss: 1.0736198419181164e-05\n",
            "step: 190, loss: 2.8054162612534128e-05\n",
            "step: 200, loss: 1.2926630915899295e-05\n",
            "step: 210, loss: 4.7966652346076444e-05\n",
            "step: 220, loss: 3.097073931712657e-05\n",
            "step: 230, loss: 1.5016412362456322e-05\n",
            "step: 240, loss: 1.9564529793569818e-05\n",
            "step: 250, loss: 2.136726470780559e-05\n",
            "step: 260, loss: 1.450237959943479e-05\n",
            "step: 270, loss: 2.1162753910175525e-05\n",
            "step: 280, loss: 1.840626100602094e-05\n",
            "step: 290, loss: 1.2550359315355308e-05\n",
            "step: 300, loss: 1.1943157915084157e-05\n",
            "step: 310, loss: 1.6778449207777157e-05\n",
            "step: 320, loss: 1.4822592675045598e-05\n",
            "step: 330, loss: 1.77468427864369e-05\n",
            "step: 340, loss: 1.1458888366178144e-05\n",
            "step: 350, loss: 9.849588423094247e-06\n",
            "step: 360, loss: 0.00012583770148921758\n",
            "step: 370, loss: 1.5880308637861162e-05\n",
            "step: 380, loss: 1.8156646547140554e-05\n",
            "step: 390, loss: 3.95232746086549e-05\n",
            "step: 400, loss: 2.5960012862924486e-05\n",
            "step: 410, loss: 1.8935270418296568e-05\n",
            "step: 420, loss: 1.6387282812502235e-05\n",
            "step: 430, loss: 3.7188296118984e-05\n",
            "step: 440, loss: 1.611165498616174e-05\n",
            "step: 450, loss: 1.1451439604570623e-05\n",
            "step: 460, loss: 1.6569811123190448e-05\n",
            "step: 470, loss: 0.0014208215288817883\n",
            "step: 480, loss: 1.3448105164570734e-05\n",
            "step: 490, loss: 1.3310322174220346e-05\n",
            "step: 500, loss: 1.980686283786781e-05\n",
            "step: 510, loss: 1.5318077203119174e-05\n",
            "step: 520, loss: 1.5854582670726813e-05\n",
            "step: 530, loss: 1.5973833797033876e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9288354898336414, f1=0.920517560073937, best_f1=0.9273897058823529\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:29, 196.76it/s]\n",
            "load_f1 = 0.9290976058931861\n",
            "real_f1 = 0.928801102434543\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.89it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_EWW7DgNFL"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQ7ANLogNFM",
        "outputId": "ef50601f-6cfe-4e7c-f946-909c2123e9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5590924620628357\n",
            "step: 10, loss: 0.3788471519947052\n",
            "step: 20, loss: 0.35555291175842285\n",
            "step: 30, loss: 0.3202214539051056\n",
            "step: 40, loss: 0.1708986610174179\n",
            "step: 50, loss: 0.4165574908256531\n",
            "step: 60, loss: 0.27367472648620605\n",
            "step: 70, loss: 0.15921321511268616\n",
            "step: 80, loss: 0.19384491443634033\n",
            "step: 90, loss: 0.4316141605377197\n",
            "step: 100, loss: 0.3469602167606354\n",
            "step: 110, loss: 0.19710111618041992\n",
            "step: 120, loss: 0.2479216605424881\n",
            "step: 130, loss: 0.16350336372852325\n",
            "step: 140, loss: 0.2183479368686676\n",
            "step: 150, loss: 0.20442163944244385\n",
            "step: 160, loss: 0.40174299478530884\n",
            "step: 170, loss: 0.264134019613266\n",
            "step: 180, loss: 0.18820112943649292\n",
            "step: 190, loss: 0.30141642689704895\n",
            "step: 200, loss: 0.3057123124599457\n",
            "step: 210, loss: 0.2319084256887436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5652173913043478, f1=0.5918367346938775, best_f1=0.5918367346938775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1129695251584053\n",
            "step: 10, loss: 0.21500715613365173\n",
            "step: 20, loss: 0.13155905902385712\n",
            "step: 30, loss: 0.24512116611003876\n",
            "step: 40, loss: 0.1736481785774231\n",
            "step: 50, loss: 0.21717245876789093\n",
            "step: 60, loss: 0.3929520845413208\n",
            "step: 70, loss: 0.1393406093120575\n",
            "step: 80, loss: 0.15166085958480835\n",
            "step: 90, loss: 0.06469780951738358\n",
            "step: 100, loss: 0.03471573069691658\n",
            "step: 110, loss: 0.09817763417959213\n",
            "step: 120, loss: 0.17431136965751648\n",
            "step: 130, loss: 0.030292386189103127\n",
            "step: 140, loss: 0.11218669265508652\n",
            "step: 150, loss: 0.282514363527298\n",
            "step: 160, loss: 0.21046514809131622\n",
            "step: 170, loss: 0.09330980479717255\n",
            "step: 180, loss: 0.18660902976989746\n",
            "step: 190, loss: 0.20271643996238708\n",
            "step: 200, loss: 0.03335237875580788\n",
            "step: 210, loss: 0.18149052560329437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5964912280701755, f1=0.6175869120654397, best_f1=0.6175869120654397\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06233687326312065\n",
            "step: 10, loss: 0.1414567530155182\n",
            "step: 20, loss: 0.1377442330121994\n",
            "step: 30, loss: 0.11876001209020615\n",
            "step: 40, loss: 0.04959765076637268\n",
            "step: 50, loss: 0.0649949312210083\n",
            "step: 60, loss: 0.14186246693134308\n",
            "step: 70, loss: 0.13395054638385773\n",
            "step: 80, loss: 0.10470361262559891\n",
            "step: 90, loss: 0.04131290316581726\n",
            "step: 100, loss: 0.23510627448558807\n",
            "step: 110, loss: 0.1234106793999672\n",
            "step: 120, loss: 0.06867152452468872\n",
            "step: 130, loss: 0.08159685879945755\n",
            "step: 140, loss: 0.06724978238344193\n",
            "step: 150, loss: 0.16192883253097534\n",
            "step: 160, loss: 0.02506379410624504\n",
            "step: 170, loss: 0.10674363374710083\n",
            "step: 180, loss: 0.13503296673297882\n",
            "step: 190, loss: 0.1804877370595932\n",
            "step: 200, loss: 0.03337739780545235\n",
            "step: 210, loss: 0.12475192546844482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6048951048951048, f1=0.5924596050269301, best_f1=0.5924596050269301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07037438452243805\n",
            "step: 10, loss: 0.034259941428899765\n",
            "step: 20, loss: 0.027360081672668457\n",
            "step: 30, loss: 0.07753288745880127\n",
            "step: 40, loss: 0.014488773420453072\n",
            "step: 50, loss: 0.1619146764278412\n",
            "step: 60, loss: 0.17716985940933228\n",
            "step: 70, loss: 0.2243281453847885\n",
            "step: 80, loss: 0.049238912761211395\n",
            "step: 90, loss: 0.012851041741669178\n",
            "step: 100, loss: 0.10581080615520477\n",
            "step: 110, loss: 0.15385296940803528\n",
            "step: 120, loss: 0.11151090264320374\n",
            "step: 130, loss: 0.1520218551158905\n",
            "step: 140, loss: 0.08734625577926636\n",
            "step: 150, loss: 0.055153556168079376\n",
            "step: 160, loss: 0.04917468503117561\n",
            "step: 170, loss: 0.11825382709503174\n",
            "step: 180, loss: 0.28122377395629883\n",
            "step: 190, loss: 0.09033510088920593\n",
            "step: 200, loss: 0.18335723876953125\n",
            "step: 210, loss: 0.09388848394155502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6317757009345795, f1=0.622568093385214, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12799789011478424\n",
            "step: 10, loss: 0.06078709661960602\n",
            "step: 20, loss: 0.1661740243434906\n",
            "step: 30, loss: 0.026084640994668007\n",
            "step: 40, loss: 0.07323455065488815\n",
            "step: 50, loss: 0.06115838885307312\n",
            "step: 60, loss: 0.10030581802129745\n",
            "step: 70, loss: 0.03887937217950821\n",
            "step: 80, loss: 0.1170053482055664\n",
            "step: 90, loss: 0.08971276134252548\n",
            "step: 100, loss: 0.01165288221091032\n",
            "step: 110, loss: 0.11820022016763687\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.140786811709404\n",
            "step: 130, loss: 0.15925461053848267\n",
            "step: 140, loss: 0.12074024230241776\n",
            "step: 150, loss: 0.0527220219373703\n",
            "step: 160, loss: 0.16714462637901306\n",
            "step: 170, loss: 0.15420173108577728\n",
            "step: 180, loss: 0.0531000941991806\n",
            "step: 190, loss: 0.05363060161471367\n",
            "step: 200, loss: 0.05666341632604599\n",
            "step: 210, loss: 0.029221754521131516\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6161971830985915, f1=0.5693950177935942, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04859522357583046\n",
            "step: 10, loss: 0.03887087479233742\n",
            "step: 20, loss: 0.06923296302556992\n",
            "step: 30, loss: 0.0025536406319588423\n",
            "step: 40, loss: 0.013050488196313381\n",
            "step: 50, loss: 0.009347464889287949\n",
            "step: 60, loss: 0.1984003335237503\n",
            "step: 70, loss: 0.04436331242322922\n",
            "step: 80, loss: 0.035453855991363525\n",
            "step: 90, loss: 0.2120271772146225\n",
            "step: 100, loss: 0.021692121401429176\n",
            "step: 110, loss: 0.012622465379536152\n",
            "step: 120, loss: 0.037720777094364166\n",
            "step: 130, loss: 0.07641693949699402\n",
            "step: 140, loss: 0.041147150099277496\n",
            "step: 150, loss: 0.016373103484511375\n",
            "step: 160, loss: 0.01723695732653141\n",
            "step: 170, loss: 0.10900861024856567\n",
            "step: 180, loss: 0.010970264673233032\n",
            "step: 190, loss: 0.02787916734814644\n",
            "step: 200, loss: 0.02456754818558693\n",
            "step: 210, loss: 0.009517267346382141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5979797979797981, f1=0.6012269938650306, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002802124712616205\n",
            "step: 10, loss: 0.004655301570892334\n",
            "step: 20, loss: 0.030624235048890114\n",
            "step: 30, loss: 0.013798044994473457\n",
            "step: 40, loss: 0.0008616087725386024\n",
            "step: 50, loss: 0.1795848309993744\n",
            "step: 60, loss: 0.09272187948226929\n",
            "step: 70, loss: 0.01455574482679367\n",
            "step: 80, loss: 0.029834512621164322\n",
            "step: 90, loss: 0.08699300140142441\n",
            "step: 100, loss: 0.02471756935119629\n",
            "step: 110, loss: 0.1683957278728485\n",
            "step: 120, loss: 0.060408953577280045\n",
            "step: 130, loss: 0.012270599603652954\n",
            "step: 140, loss: 0.0573556125164032\n",
            "step: 150, loss: 0.01705934852361679\n",
            "step: 160, loss: 0.025066424161195755\n",
            "step: 170, loss: 0.0012401192216202617\n",
            "step: 180, loss: 0.009936521761119366\n",
            "step: 190, loss: 0.20835043489933014\n",
            "step: 200, loss: 0.022121695801615715\n",
            "step: 210, loss: 0.013226390816271305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6046511627906976, f1=0.6090373280943024, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003066035220399499\n",
            "step: 10, loss: 0.036620207130908966\n",
            "step: 20, loss: 0.003569790394976735\n",
            "step: 30, loss: 0.002728547900915146\n",
            "step: 40, loss: 0.0033121779561042786\n",
            "step: 50, loss: 0.012561982497572899\n",
            "step: 60, loss: 0.002226373879238963\n",
            "step: 70, loss: 0.01549326628446579\n",
            "step: 80, loss: 0.021660545840859413\n",
            "step: 90, loss: 0.008796717040240765\n",
            "step: 100, loss: 0.04575921967625618\n",
            "step: 110, loss: 0.11028692126274109\n",
            "step: 120, loss: 0.017255134880542755\n",
            "step: 130, loss: 0.0018712584860622883\n",
            "step: 140, loss: 0.04853309690952301\n",
            "step: 150, loss: 0.007341315969824791\n",
            "step: 160, loss: 0.052344854921102524\n",
            "step: 170, loss: 0.020526684820652008\n",
            "step: 180, loss: 0.09134487807750702\n",
            "step: 190, loss: 0.005968289449810982\n",
            "step: 200, loss: 0.0029266367200762033\n",
            "step: 210, loss: 0.08823520690202713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5902335456475584, f1=0.5974025974025974, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05991426482796669\n",
            "step: 10, loss: 0.02178001031279564\n",
            "step: 20, loss: 0.0034706543665379286\n",
            "step: 30, loss: 0.03634351119399071\n",
            "step: 40, loss: 0.018322410061955452\n",
            "step: 50, loss: 0.004910433664917946\n",
            "step: 60, loss: 0.02252560295164585\n",
            "step: 70, loss: 0.0038726578932255507\n",
            "step: 80, loss: 0.001836448791436851\n",
            "step: 90, loss: 0.000957471551373601\n",
            "step: 100, loss: 0.06056511402130127\n",
            "step: 110, loss: 0.019644295796751976\n",
            "step: 120, loss: 0.007551337592303753\n",
            "step: 130, loss: 0.059359192848205566\n",
            "step: 140, loss: 0.021715031936764717\n",
            "step: 150, loss: 0.12270450592041016\n",
            "step: 160, loss: 0.004067180212587118\n",
            "step: 170, loss: 0.0007677412359043956\n",
            "step: 180, loss: 0.05218027904629707\n",
            "step: 190, loss: 0.0017936421791091561\n",
            "step: 200, loss: 0.010583673603832722\n",
            "step: 210, loss: 0.0005627088248729706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5842217484008528, f1=0.5986984815618221, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005959575646556914\n",
            "step: 10, loss: 0.0024538652505725622\n",
            "step: 20, loss: 0.00031818568822927773\n",
            "step: 30, loss: 0.006532663945108652\n",
            "step: 40, loss: 0.0005341543001122773\n",
            "step: 50, loss: 0.020652366802096367\n",
            "step: 60, loss: 0.00567589420825243\n",
            "step: 70, loss: 0.03681771457195282\n",
            "step: 80, loss: 0.0018455700483173132\n",
            "step: 90, loss: 0.01307629980146885\n",
            "step: 100, loss: 0.02228493057191372\n",
            "step: 110, loss: 0.0003114108694717288\n",
            "step: 120, loss: 0.00033975334372371435\n",
            "step: 130, loss: 0.008436412550508976\n",
            "step: 140, loss: 0.002812353428453207\n",
            "step: 150, loss: 0.004715615417808294\n",
            "step: 160, loss: 0.003038896946236491\n",
            "step: 170, loss: 0.017969565466046333\n",
            "step: 180, loss: 0.02635512501001358\n",
            "step: 190, loss: 0.06967788189649582\n",
            "step: 200, loss: 0.0023397470358759165\n",
            "step: 210, loss: 0.029781492426991463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5831622176591376, f1=0.6068376068376068, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028371257707476616\n",
            "step: 10, loss: 0.02738381177186966\n",
            "step: 20, loss: 0.00878753513097763\n",
            "step: 30, loss: 0.0004606659640558064\n",
            "step: 40, loss: 0.018737919628620148\n",
            "step: 50, loss: 0.0012604553485289216\n",
            "step: 60, loss: 0.0027703389059752226\n",
            "step: 70, loss: 0.002990937326103449\n",
            "step: 80, loss: 0.03574798256158829\n",
            "step: 90, loss: 0.0002099749690387398\n",
            "step: 100, loss: 0.011633596383035183\n",
            "step: 110, loss: 0.0059507545083761215\n",
            "step: 120, loss: 0.027009882032871246\n",
            "step: 130, loss: 0.0019039887702092528\n",
            "step: 140, loss: 0.00015032815281301737\n",
            "step: 150, loss: 0.00019315083045512438\n",
            "step: 160, loss: 0.0007277224212884903\n",
            "step: 170, loss: 0.0005207165377214551\n",
            "step: 180, loss: 0.0043829018250107765\n",
            "step: 190, loss: 0.00196390924975276\n",
            "step: 200, loss: 0.036338187754154205\n",
            "step: 210, loss: 0.0014137544203549623\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.604914933837429, f1=0.6039215686274509, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020389100536704063\n",
            "step: 10, loss: 0.001741307321935892\n",
            "step: 20, loss: 0.017858335748314857\n",
            "step: 30, loss: 0.02418668568134308\n",
            "step: 40, loss: 0.0011570433853194118\n",
            "step: 50, loss: 0.022174960002303123\n",
            "step: 60, loss: 0.0005898622912354767\n",
            "step: 70, loss: 0.0019709060434252024\n",
            "step: 80, loss: 0.0008519911207258701\n",
            "step: 90, loss: 0.021457916125655174\n",
            "step: 100, loss: 0.22634465992450714\n",
            "step: 110, loss: 0.004880909807980061\n",
            "step: 120, loss: 0.0028822312597185373\n",
            "step: 130, loss: 0.00669308053329587\n",
            "step: 140, loss: 0.0002459526585880667\n",
            "step: 150, loss: 0.0012323344126343727\n",
            "step: 160, loss: 0.028545280918478966\n",
            "step: 170, loss: 0.0007660348201170564\n",
            "step: 180, loss: 0.0010789104271680117\n",
            "step: 190, loss: 0.04456828907132149\n",
            "step: 200, loss: 0.0030412902124226093\n",
            "step: 210, loss: 0.005509525071829557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5991561181434599, f1=0.6004228329809724, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018490031361579895\n",
            "step: 10, loss: 0.00014806863327976316\n",
            "step: 20, loss: 0.011868514120578766\n",
            "step: 30, loss: 0.0014819809002801776\n",
            "step: 40, loss: 0.031928323209285736\n",
            "step: 50, loss: 0.0018534479895606637\n",
            "step: 60, loss: 0.001031358609907329\n",
            "step: 70, loss: 0.06434672325849533\n",
            "step: 80, loss: 0.0028558073099702597\n",
            "step: 90, loss: 0.0001467307738494128\n",
            "step: 100, loss: 0.00022561968944501132\n",
            "step: 110, loss: 0.0010408916277810931\n",
            "step: 120, loss: 0.0007470757700502872\n",
            "step: 130, loss: 0.00043596018804237247\n",
            "step: 140, loss: 0.00033667802927084267\n",
            "step: 150, loss: 0.0030599385499954224\n",
            "step: 160, loss: 0.04745488613843918\n",
            "step: 170, loss: 0.0017310489201918244\n",
            "step: 180, loss: 0.012893936596810818\n",
            "step: 190, loss: 0.0003572539135348052\n",
            "step: 200, loss: 0.0008229516679421067\n",
            "step: 210, loss: 0.0010084344539791346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5887640449438203, f1=0.5947136563876653, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.978132584365085e-05\n",
            "step: 10, loss: 0.003999378066509962\n",
            "step: 20, loss: 0.0002165373443858698\n",
            "step: 30, loss: 0.0004922226071357727\n",
            "step: 40, loss: 0.00045453401980921626\n",
            "step: 50, loss: 0.00039587978972122073\n",
            "step: 60, loss: 0.004506767261773348\n",
            "step: 70, loss: 0.00033577598514966667\n",
            "step: 80, loss: 0.00033415385405533016\n",
            "step: 90, loss: 0.0035019880160689354\n",
            "step: 100, loss: 0.0011144704185426235\n",
            "step: 110, loss: 0.001425613765604794\n",
            "step: 120, loss: 0.03990292549133301\n",
            "step: 130, loss: 0.0014697350561618805\n",
            "step: 140, loss: 0.0008285561925731599\n",
            "step: 150, loss: 0.028536800295114517\n",
            "step: 160, loss: 0.004388016182929277\n",
            "step: 170, loss: 0.000502069597132504\n",
            "step: 180, loss: 0.0003402537840884179\n",
            "step: 190, loss: 0.030032198876142502\n",
            "step: 200, loss: 0.007054335903376341\n",
            "step: 210, loss: 0.010737268254160881\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5936842105263158, f1=0.6083333333333334, best_f1=0.622568093385214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033286598045378923\n",
            "step: 10, loss: 0.00018632158753462136\n",
            "step: 20, loss: 0.0008595044491812587\n",
            "step: 30, loss: 0.020967302843928337\n",
            "step: 40, loss: 0.00031627604039385915\n",
            "step: 50, loss: 0.00012316094944253564\n",
            "step: 60, loss: 0.003360751550644636\n",
            "step: 70, loss: 0.0003249008732382208\n",
            "step: 80, loss: 0.0006368382018990815\n",
            "step: 90, loss: 0.0027672662399709225\n",
            "step: 100, loss: 0.00022562764934264123\n",
            "step: 110, loss: 0.00013094086898490787\n",
            "step: 120, loss: 0.0001719384454190731\n",
            "step: 130, loss: 0.00039110175566747785\n",
            "step: 140, loss: 0.00025869658566080034\n",
            "step: 150, loss: 0.00015413929941132665\n",
            "step: 160, loss: 0.00025858331355266273\n",
            "step: 170, loss: 0.0002446213911753148\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.18809722363948822\n",
            "step: 190, loss: 0.00013197652879171073\n",
            "step: 200, loss: 0.007127661257982254\n",
            "step: 210, loss: 0.0006931893294677138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5917926565874729, f1=0.5970149253731343, best_f1=0.622568093385214\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:08, 282.12it/s]\n",
            "load_f1 = 0.62708719851577\n",
            "real_f1 = 0.6303939962476548\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.72it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIoASlugNFN",
        "outputId": "e46c349d-985c-4dd5-b0aa-6827330d1e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6038344502449036\n",
            "step: 10, loss: 0.35017532110214233\n",
            "step: 20, loss: 0.2715592682361603\n",
            "step: 30, loss: 0.45775747299194336\n",
            "step: 40, loss: 0.42936789989471436\n",
            "step: 50, loss: 0.3434927761554718\n",
            "step: 60, loss: 0.2798023521900177\n",
            "step: 70, loss: 0.2740243375301361\n",
            "step: 80, loss: 0.22001154720783234\n",
            "step: 90, loss: 0.27973175048828125\n",
            "step: 100, loss: 0.317672461271286\n",
            "step: 110, loss: 0.35004350543022156\n",
            "step: 120, loss: 0.17725518345832825\n",
            "step: 130, loss: 0.15874965488910675\n",
            "step: 140, loss: 0.06637181341648102\n",
            "step: 150, loss: 0.12555311620235443\n",
            "step: 160, loss: 0.12995010614395142\n",
            "step: 170, loss: 0.23692256212234497\n",
            "step: 180, loss: 0.03440112993121147\n",
            "step: 190, loss: 0.30314207077026367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6700767263427111, f1=0.696103896103896, best_f1=0.696103896103896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2866505980491638\n",
            "step: 10, loss: 0.06993915885686874\n",
            "step: 20, loss: 0.1366489976644516\n",
            "step: 30, loss: 0.18200545012950897\n",
            "step: 40, loss: 0.16216707229614258\n",
            "step: 50, loss: 0.05553894490003586\n",
            "step: 60, loss: 0.24920590221881866\n",
            "step: 70, loss: 0.16984480619430542\n",
            "step: 80, loss: 0.17196887731552124\n",
            "step: 90, loss: 0.1540011465549469\n",
            "step: 100, loss: 0.039440326392650604\n",
            "step: 110, loss: 0.19511893391609192\n",
            "step: 120, loss: 0.24440671503543854\n",
            "step: 130, loss: 0.12939487397670746\n",
            "step: 140, loss: 0.07291470468044281\n",
            "step: 150, loss: 0.09700916707515717\n",
            "step: 160, loss: 0.10869219154119492\n",
            "step: 170, loss: 0.060631632804870605\n",
            "step: 180, loss: 0.1954669952392578\n",
            "step: 190, loss: 0.05332573503255844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7570332480818414, f1=0.7616580310880829, best_f1=0.7616580310880829\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05574587732553482\n",
            "step: 10, loss: 0.1839180439710617\n",
            "step: 20, loss: 0.13458497822284698\n",
            "step: 30, loss: 0.051852863281965256\n",
            "step: 40, loss: 0.1338832527399063\n",
            "step: 50, loss: 0.1448737382888794\n",
            "step: 60, loss: 0.032624926418066025\n",
            "step: 70, loss: 0.05789513513445854\n",
            "step: 80, loss: 0.144624263048172\n",
            "step: 90, loss: 0.10477012395858765\n",
            "step: 100, loss: 0.01987914741039276\n",
            "step: 110, loss: 0.01346411369740963\n",
            "step: 120, loss: 0.021878071129322052\n",
            "step: 130, loss: 0.013012605719268322\n",
            "step: 140, loss: 0.01139745395630598\n",
            "step: 150, loss: 0.16934578120708466\n",
            "step: 160, loss: 0.08496378362178802\n",
            "step: 170, loss: 0.21114617586135864\n",
            "step: 180, loss: 0.06802897900342941\n",
            "step: 190, loss: 0.08922412246465683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7788944723618091, f1=0.781725888324873, best_f1=0.781725888324873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05951181426644325\n",
            "step: 10, loss: 0.0826515257358551\n",
            "step: 20, loss: 0.12221337109804153\n",
            "step: 30, loss: 0.040649283677339554\n",
            "step: 40, loss: 0.030574627220630646\n",
            "step: 50, loss: 0.1413862556219101\n",
            "step: 60, loss: 0.11799022555351257\n",
            "step: 70, loss: 0.03825432062149048\n",
            "step: 80, loss: 0.06849279999732971\n",
            "step: 90, loss: 0.01529902033507824\n",
            "step: 100, loss: 0.008093807846307755\n",
            "step: 110, loss: 0.004545166622847319\n",
            "step: 120, loss: 0.031082328408956528\n",
            "step: 130, loss: 0.07338318973779678\n",
            "step: 140, loss: 0.02394293248653412\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.04061061143875122\n",
            "step: 160, loss: 0.034691616892814636\n",
            "step: 170, loss: 0.14576487243175507\n",
            "step: 180, loss: 0.1859397143125534\n",
            "step: 190, loss: 0.10667972266674042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7614213197969544, f1=0.7696335078534032, best_f1=0.781725888324873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05628993362188339\n",
            "step: 10, loss: 0.013861773535609245\n",
            "step: 20, loss: 0.0395953506231308\n",
            "step: 30, loss: 0.016098838299512863\n",
            "step: 40, loss: 0.07240705192089081\n",
            "step: 50, loss: 0.1980750411748886\n",
            "step: 60, loss: 0.016094839200377464\n",
            "step: 70, loss: 0.006672574672847986\n",
            "step: 80, loss: 0.011747248470783234\n",
            "step: 90, loss: 0.004169085528701544\n",
            "step: 100, loss: 0.00250050937756896\n",
            "step: 110, loss: 0.0071774981915950775\n",
            "step: 120, loss: 0.005610847380012274\n",
            "step: 130, loss: 0.1130940243601799\n",
            "step: 140, loss: 0.028937002643942833\n",
            "step: 150, loss: 0.013678775168955326\n",
            "step: 160, loss: 0.1018708273768425\n",
            "step: 170, loss: 0.0019111447036266327\n",
            "step: 180, loss: 0.013929974287748337\n",
            "step: 190, loss: 0.08244487643241882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8097826086956521, f1=0.7978436657681941, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0156876090914011\n",
            "step: 10, loss: 0.17347535490989685\n",
            "step: 20, loss: 0.00862239208072424\n",
            "step: 30, loss: 0.0008386505651287735\n",
            "step: 40, loss: 0.07178957015275955\n",
            "step: 50, loss: 0.022650932893157005\n",
            "step: 60, loss: 0.01103738322854042\n",
            "step: 70, loss: 0.06556116789579391\n",
            "step: 80, loss: 0.0356634221971035\n",
            "step: 90, loss: 0.007329197134822607\n",
            "step: 100, loss: 0.0016532412264496088\n",
            "step: 110, loss: 0.013296172954142094\n",
            "step: 120, loss: 0.0043404605239629745\n",
            "step: 130, loss: 0.00849572941660881\n",
            "step: 140, loss: 0.01977849379181862\n",
            "step: 150, loss: 0.0012131640687584877\n",
            "step: 160, loss: 0.051714953035116196\n",
            "step: 170, loss: 0.02540382742881775\n",
            "step: 180, loss: 0.006146129686385393\n",
            "step: 190, loss: 0.034208428114652634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8142076502732241, f1=0.7722222222222223, best_f1=0.7722222222222223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004687666427344084\n",
            "step: 10, loss: 0.05074580758810043\n",
            "step: 20, loss: 0.09888456761837006\n",
            "step: 30, loss: 0.0013615497155115008\n",
            "step: 40, loss: 0.08408064395189285\n",
            "step: 50, loss: 0.006513007916510105\n",
            "step: 60, loss: 0.027552573010325432\n",
            "step: 70, loss: 0.00419675512239337\n",
            "step: 80, loss: 0.006944779772311449\n",
            "step: 90, loss: 0.007735304068773985\n",
            "step: 100, loss: 0.0016948094125837088\n",
            "step: 110, loss: 0.010794227942824364\n",
            "step: 120, loss: 0.0010750622022897005\n",
            "step: 130, loss: 0.00895669311285019\n",
            "step: 140, loss: 0.014861628413200378\n",
            "step: 150, loss: 0.018635345622897148\n",
            "step: 160, loss: 0.023238398134708405\n",
            "step: 170, loss: 0.003018451388925314\n",
            "step: 180, loss: 0.0009943261975422502\n",
            "step: 190, loss: 0.0009419245179742575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7977528089887641, f1=0.793388429752066, best_f1=0.7722222222222223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022958205081522465\n",
            "step: 10, loss: 0.005293913185596466\n",
            "step: 20, loss: 0.0014985427260398865\n",
            "step: 30, loss: 0.007070038467645645\n",
            "step: 40, loss: 0.0008490215986967087\n",
            "step: 50, loss: 0.002504267729818821\n",
            "step: 60, loss: 0.0017501991242170334\n",
            "step: 70, loss: 0.06039140745997429\n",
            "step: 80, loss: 0.00044610502663999796\n",
            "step: 90, loss: 0.006708289030939341\n",
            "step: 100, loss: 0.0014727701200172305\n",
            "step: 110, loss: 0.0005374825559556484\n",
            "step: 120, loss: 0.0016992503078654408\n",
            "step: 130, loss: 0.0004411455010995269\n",
            "step: 140, loss: 0.001318500842899084\n",
            "step: 150, loss: 0.09237765520811081\n",
            "step: 160, loss: 0.0007494536694139242\n",
            "step: 170, loss: 0.0012380536645650864\n",
            "step: 180, loss: 0.04594253748655319\n",
            "step: 190, loss: 0.07998346537351608\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7851458885941645, f1=0.7712765957446809, best_f1=0.7722222222222223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003315781708806753\n",
            "step: 10, loss: 0.023590462282299995\n",
            "step: 20, loss: 0.0012184525839984417\n",
            "step: 30, loss: 0.00043794390512630343\n",
            "step: 40, loss: 0.0009224951500073075\n",
            "step: 50, loss: 0.0010112002491950989\n",
            "step: 60, loss: 0.0014340955531224608\n",
            "step: 70, loss: 0.0006675987388007343\n",
            "step: 80, loss: 0.0007677605026401579\n",
            "step: 90, loss: 0.07965237647294998\n",
            "step: 100, loss: 0.015889819711446762\n",
            "step: 110, loss: 0.0010777920251712203\n",
            "step: 120, loss: 0.0015872593503445387\n",
            "step: 130, loss: 0.0009206653339788318\n",
            "step: 140, loss: 0.008350096642971039\n",
            "step: 150, loss: 0.0005297409952618182\n",
            "step: 160, loss: 0.00033751249429769814\n",
            "step: 170, loss: 0.0010871633421629667\n",
            "step: 180, loss: 0.0012494507245719433\n",
            "step: 190, loss: 0.0006729209562763572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8072916666666666, f1=0.7855297157622738, best_f1=0.7722222222222223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004184326680842787\n",
            "step: 10, loss: 0.0016902424395084381\n",
            "step: 20, loss: 0.0031089133117347956\n",
            "step: 30, loss: 0.00022060533228795975\n",
            "step: 40, loss: 0.0025761539582163095\n",
            "step: 50, loss: 0.0003890780499204993\n",
            "step: 60, loss: 0.0003693464968819171\n",
            "step: 70, loss: 0.0012539072195068002\n",
            "step: 80, loss: 0.0013376668794080615\n",
            "step: 90, loss: 0.013101840391755104\n",
            "step: 100, loss: 0.00047198584070429206\n",
            "step: 110, loss: 0.000991112319752574\n",
            "step: 120, loss: 0.09727384150028229\n",
            "step: 130, loss: 0.00038899757782928646\n",
            "step: 140, loss: 0.0004054451419506222\n",
            "step: 150, loss: 0.1711529642343521\n",
            "step: 160, loss: 0.002984993625432253\n",
            "step: 170, loss: 0.0264178104698658\n",
            "step: 180, loss: 0.0004983368562534451\n",
            "step: 190, loss: 0.000552344077732414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.827027027027027, f1=0.7999999999999999, best_f1=0.7999999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003705908020492643\n",
            "step: 10, loss: 0.0012042816961184144\n",
            "step: 20, loss: 0.004894117824733257\n",
            "step: 30, loss: 0.005296419840306044\n",
            "step: 40, loss: 0.00021445816673804075\n",
            "step: 50, loss: 0.015013475902378559\n",
            "step: 60, loss: 0.00026035262271761894\n",
            "step: 70, loss: 0.0012837356189265847\n",
            "step: 80, loss: 0.0004498417256399989\n",
            "step: 90, loss: 0.0003372995415702462\n",
            "step: 100, loss: 0.00035705018672160804\n",
            "step: 110, loss: 0.000279326835880056\n",
            "step: 120, loss: 0.0004636045196093619\n",
            "step: 130, loss: 0.0003609077539294958\n",
            "step: 140, loss: 0.0002742950455285609\n",
            "step: 150, loss: 0.00028726071468554437\n",
            "step: 160, loss: 0.00035407079849392176\n",
            "step: 170, loss: 0.00023450484150089324\n",
            "step: 180, loss: 0.007516215555369854\n",
            "step: 190, loss: 0.0003287677827756852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8283378746594007, f1=0.7978436657681941, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011096570233348757\n",
            "step: 10, loss: 0.00045436384971253574\n",
            "step: 20, loss: 0.00014550946070812643\n",
            "step: 30, loss: 0.00022780393192078918\n",
            "step: 40, loss: 0.00025090196868404746\n",
            "step: 50, loss: 0.0004000073822680861\n",
            "step: 60, loss: 0.00030608067754656076\n",
            "step: 70, loss: 0.00016314905951730907\n",
            "step: 80, loss: 0.003022619755938649\n",
            "step: 90, loss: 0.0006568431854248047\n",
            "step: 100, loss: 0.0002699411998037249\n",
            "step: 110, loss: 0.0021726808045059443\n",
            "step: 120, loss: 0.0003946149372495711\n",
            "step: 130, loss: 0.0007218809332698584\n",
            "step: 140, loss: 0.004055683966726065\n",
            "step: 150, loss: 0.0003714847262017429\n",
            "step: 160, loss: 0.00012358093226794153\n",
            "step: 170, loss: 0.0015653324080631137\n",
            "step: 180, loss: 0.00011071354674641043\n",
            "step: 190, loss: 0.0002455060603097081\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8074866310160429, f1=0.8064516129032259, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014517382951453328\n",
            "step: 10, loss: 0.00033493011142127216\n",
            "step: 20, loss: 0.0003480854502413422\n",
            "step: 30, loss: 0.0009037598501890898\n",
            "step: 40, loss: 0.0030680804047733545\n",
            "step: 50, loss: 0.0003650157304946333\n",
            "step: 60, loss: 0.00019236010848544538\n",
            "step: 70, loss: 0.005168217699974775\n",
            "step: 80, loss: 0.002211923711001873\n",
            "step: 90, loss: 0.00041171431075781584\n",
            "step: 100, loss: 0.0009057394927367568\n",
            "step: 110, loss: 0.00014765870582778007\n",
            "step: 120, loss: 0.0021073929965496063\n",
            "step: 130, loss: 0.00010746010957518592\n",
            "step: 140, loss: 0.0009295449126511812\n",
            "step: 150, loss: 0.00014540596748702228\n",
            "step: 160, loss: 0.014327836222946644\n",
            "step: 170, loss: 0.00038273577229119837\n",
            "step: 180, loss: 0.00010961421503452584\n",
            "step: 190, loss: 0.006107485853135586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8111111111111112, f1=0.7932960893854748, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000506409618537873\n",
            "step: 10, loss: 0.00012024952593492344\n",
            "step: 20, loss: 0.00020144552399870008\n",
            "step: 30, loss: 0.00011811555305030197\n",
            "step: 40, loss: 0.001144975540228188\n",
            "step: 50, loss: 0.00037750022602267563\n",
            "step: 60, loss: 0.0002546116302255541\n",
            "step: 70, loss: 0.00034126153332181275\n",
            "step: 80, loss: 0.00030485709430649877\n",
            "step: 90, loss: 0.00111105025280267\n",
            "step: 100, loss: 0.0078098708763718605\n",
            "step: 110, loss: 0.0002484606229700148\n",
            "step: 120, loss: 0.0002285455702804029\n",
            "step: 130, loss: 0.008626549504697323\n",
            "step: 140, loss: 0.000326736131682992\n",
            "step: 150, loss: 0.00033519690623506904\n",
            "step: 160, loss: 0.0015948378713801503\n",
            "step: 170, loss: 0.0002657890727277845\n",
            "step: 180, loss: 0.0024104544427245855\n",
            "step: 190, loss: 0.00023224359028972685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8162162162162163, f1=0.7988980716253442, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012296802597120404\n",
            "step: 10, loss: 0.00022616132628172636\n",
            "step: 20, loss: 0.00032929808367043734\n",
            "step: 30, loss: 0.00012574854190461338\n",
            "step: 40, loss: 9.556335862725973e-05\n",
            "step: 50, loss: 0.00030527188209816813\n",
            "step: 60, loss: 0.000506063865032047\n",
            "step: 70, loss: 0.01630699448287487\n",
            "step: 80, loss: 0.00025547383120283484\n",
            "step: 90, loss: 0.00010894666775129735\n",
            "step: 100, loss: 0.0007596118957735598\n",
            "step: 110, loss: 0.00016577982751186937\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 9.875404794001952e-05\n",
            "step: 130, loss: 0.00024688043049536645\n",
            "step: 140, loss: 0.002163863042369485\n",
            "step: 150, loss: 0.00040724503924138844\n",
            "step: 160, loss: 0.00019300970598123968\n",
            "step: 170, loss: 0.0004650020564440638\n",
            "step: 180, loss: 0.0019756571855396032\n",
            "step: 190, loss: 0.00012260272342246026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8133704735376045, f1=0.7910863509749304, best_f1=0.7978436657681941\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 164.43it/s]\n",
            "load_f1 = 0.8264462809917356\n",
            "real_f1 = 0.8232044198895028\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 186.43it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkqC6MWgNFO"
      },
      "source": [
        "## BASELINE TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPR9KRSgNFO"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62jt5GiEgNFO",
        "outputId": "4da5ef4b-6dfc-49e4-8a35-19bea09929f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6169766187667847\n",
            "step: 10, loss: 0.3609139621257782\n",
            "step: 20, loss: 0.2963274121284485\n",
            "step: 30, loss: 0.36663150787353516\n",
            "step: 40, loss: 0.26752370595932007\n",
            "step: 50, loss: 0.24720355868339539\n",
            "step: 60, loss: 0.2527465224266052\n",
            "step: 70, loss: 0.35222703218460083\n",
            "step: 80, loss: 0.37202006578445435\n",
            "step: 90, loss: 0.2159721702337265\n",
            "step: 100, loss: 0.19585421681404114\n",
            "step: 110, loss: 0.22838014364242554\n",
            "step: 120, loss: 0.12382663786411285\n",
            "step: 130, loss: 0.08099700510501862\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 140, loss: 0.23456795513629913\n",
            "step: 150, loss: 0.25511595606803894\n",
            "step: 160, loss: 0.20600946247577667\n",
            "step: 170, loss: 0.14765296876430511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7072599531615924, f1=0.7294117647058824, best_f1=0.7294117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08232636004686356\n",
            "step: 10, loss: 0.21141158044338226\n",
            "step: 20, loss: 0.034188784658908844\n",
            "step: 30, loss: 0.19880151748657227\n",
            "step: 40, loss: 0.11937592178583145\n",
            "step: 50, loss: 0.17250648140907288\n",
            "step: 60, loss: 0.05537216737866402\n",
            "step: 70, loss: 0.1524004191160202\n",
            "step: 80, loss: 0.04225882515311241\n",
            "step: 90, loss: 0.09858202934265137\n",
            "step: 100, loss: 0.1250404715538025\n",
            "step: 110, loss: 0.04151167348027229\n",
            "step: 120, loss: 0.08941173553466797\n",
            "step: 130, loss: 0.05915664881467819\n",
            "step: 140, loss: 0.29104286432266235\n",
            "step: 150, loss: 0.23402152955532074\n",
            "step: 160, loss: 0.11328577250242233\n",
            "step: 170, loss: 0.04569673910737038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7684478371501272, f1=0.7641025641025642, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07455157488584518\n",
            "step: 10, loss: 0.03568779304623604\n",
            "step: 20, loss: 0.02094150520861149\n",
            "step: 30, loss: 0.19865120947360992\n",
            "step: 40, loss: 0.08782533556222916\n",
            "step: 50, loss: 0.08339294046163559\n",
            "step: 60, loss: 0.13512353599071503\n",
            "step: 70, loss: 0.07756906002759933\n",
            "step: 80, loss: 0.053874872624874115\n",
            "step: 90, loss: 0.08863933384418488\n",
            "step: 100, loss: 0.009021107107400894\n",
            "step: 110, loss: 0.11679885536432266\n",
            "step: 120, loss: 0.06308195739984512\n",
            "step: 130, loss: 0.09288956969976425\n",
            "step: 140, loss: 0.13570202887058258\n",
            "step: 150, loss: 0.02733656018972397\n",
            "step: 160, loss: 0.015585382468998432\n",
            "step: 170, loss: 0.06934770196676254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7405405405405405, f1=0.7745358090185676, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010014748200774193\n",
            "step: 10, loss: 0.08738133311271667\n",
            "step: 20, loss: 0.013822992332279682\n",
            "step: 30, loss: 0.04675177484750748\n",
            "step: 40, loss: 0.0021393944043666124\n",
            "step: 50, loss: 0.26736506819725037\n",
            "step: 60, loss: 0.1621464639902115\n",
            "step: 70, loss: 0.004816029220819473\n",
            "step: 80, loss: 0.025243734940886497\n",
            "step: 90, loss: 0.08554710447788239\n",
            "step: 100, loss: 0.12797288596630096\n",
            "step: 110, loss: 0.12136855721473694\n",
            "step: 120, loss: 0.010613437741994858\n",
            "step: 130, loss: 0.1364338994026184\n",
            "step: 140, loss: 0.027881663292646408\n",
            "step: 150, loss: 0.08244501054286957\n",
            "step: 160, loss: 0.24660897254943848\n",
            "step: 170, loss: 0.014413172379136086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7524752475247525, f1=0.7817745803357313, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014362513087689877\n",
            "step: 10, loss: 0.01141625177115202\n",
            "step: 20, loss: 0.012127174064517021\n",
            "step: 30, loss: 0.15125772356987\n",
            "step: 40, loss: 0.03064757026731968\n",
            "step: 50, loss: 0.03945663571357727\n",
            "step: 60, loss: 0.02619469352066517\n",
            "step: 70, loss: 0.25246331095695496\n",
            "step: 80, loss: 0.11597655713558197\n",
            "step: 90, loss: 0.1040320098400116\n",
            "step: 100, loss: 0.01807698979973793\n",
            "step: 110, loss: 0.06973173469305038\n",
            "step: 120, loss: 0.010964103043079376\n",
            "step: 130, loss: 0.0686064139008522\n",
            "step: 140, loss: 0.00676066754385829\n",
            "step: 150, loss: 0.013936156406998634\n",
            "step: 160, loss: 0.10828331112861633\n",
            "step: 170, loss: 0.008933210745453835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.748051948051948, f1=0.7969151670951157, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006159698124974966\n",
            "step: 10, loss: 0.009616849012672901\n",
            "step: 20, loss: 0.008465137332677841\n",
            "step: 30, loss: 0.034556951373815536\n",
            "step: 40, loss: 0.02265981025993824\n",
            "step: 50, loss: 0.0355074405670166\n",
            "step: 60, loss: 0.002760391216725111\n",
            "step: 70, loss: 0.08572655916213989\n",
            "step: 80, loss: 0.021818581968545914\n",
            "step: 90, loss: 0.07362550497055054\n",
            "step: 100, loss: 0.005717556923627853\n",
            "step: 110, loss: 0.0016719831619411707\n",
            "step: 120, loss: 0.005282358266413212\n",
            "step: 130, loss: 0.007280683610588312\n",
            "step: 140, loss: 0.028033411130309105\n",
            "step: 150, loss: 0.03907712921500206\n",
            "step: 160, loss: 0.06280979514122009\n",
            "step: 170, loss: 0.0028916066512465477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7549999999999999, f1=0.7536945812807883, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000819300243165344\n",
            "step: 10, loss: 0.0012687182752415538\n",
            "step: 20, loss: 0.009683684445917606\n",
            "step: 30, loss: 0.015627404674887657\n",
            "step: 40, loss: 0.003998574800789356\n",
            "step: 50, loss: 0.07463137805461884\n",
            "step: 60, loss: 0.002249309793114662\n",
            "step: 70, loss: 0.009632511995732784\n",
            "step: 80, loss: 0.027839437127113342\n",
            "step: 90, loss: 0.0002612208481878042\n",
            "step: 100, loss: 0.008359747938811779\n",
            "step: 110, loss: 0.023699507117271423\n",
            "step: 120, loss: 0.005811577662825584\n",
            "step: 130, loss: 0.10290469229221344\n",
            "step: 140, loss: 0.0042922357097268105\n",
            "step: 150, loss: 0.014056673273444176\n",
            "step: 160, loss: 0.000561302702408284\n",
            "step: 170, loss: 0.17600810527801514\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7386934673366835, f1=0.7889447236180904, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007049474399536848\n",
            "step: 10, loss: 0.009763794019818306\n",
            "step: 20, loss: 0.00038532837061211467\n",
            "step: 30, loss: 0.013582250103354454\n",
            "step: 40, loss: 0.000492663006298244\n",
            "step: 50, loss: 0.006989933550357819\n",
            "step: 60, loss: 0.0010960528161376715\n",
            "step: 70, loss: 0.0033255810849368572\n",
            "step: 80, loss: 0.014244125224649906\n",
            "step: 90, loss: 0.004634442273527384\n",
            "step: 100, loss: 0.04138204827904701\n",
            "step: 110, loss: 0.03242680802941322\n",
            "step: 120, loss: 0.0004872340359725058\n",
            "step: 130, loss: 0.000776361848693341\n",
            "step: 140, loss: 0.0004890849813818932\n",
            "step: 150, loss: 0.06716973334550858\n",
            "step: 160, loss: 0.001014763256534934\n",
            "step: 170, loss: 0.0020617626141756773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7530562347188264, f1=0.7819905213270142, best_f1=0.7641025641025642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007940666982904077\n",
            "step: 10, loss: 0.029993589967489243\n",
            "step: 20, loss: 0.0032508561853319407\n",
            "step: 30, loss: 0.022845886647701263\n",
            "step: 40, loss: 0.006875941995531321\n",
            "step: 50, loss: 0.0014149354537948966\n",
            "step: 60, loss: 0.10964754223823547\n",
            "step: 70, loss: 0.05197994410991669\n",
            "step: 80, loss: 0.0012384160654619336\n",
            "step: 90, loss: 0.02603832259774208\n",
            "step: 100, loss: 0.0060099209658801556\n",
            "step: 110, loss: 0.015001441352069378\n",
            "step: 120, loss: 0.001994151622056961\n",
            "step: 130, loss: 0.35352230072021484\n",
            "step: 140, loss: 0.0003944489872083068\n",
            "step: 150, loss: 0.018034713342785835\n",
            "step: 160, loss: 0.17346368730068207\n",
            "step: 170, loss: 0.004010668024420738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7792915531335151, f1=0.7957559681697612, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05896247923374176\n",
            "step: 10, loss: 0.00838873628526926\n",
            "step: 20, loss: 0.09144581109285355\n",
            "step: 30, loss: 0.05650656670331955\n",
            "step: 40, loss: 0.000889665971044451\n",
            "step: 50, loss: 0.05006851255893707\n",
            "step: 60, loss: 0.0007450677803717554\n",
            "step: 70, loss: 0.0173883568495512\n",
            "step: 80, loss: 0.001098880311474204\n",
            "step: 90, loss: 0.01371186226606369\n",
            "step: 100, loss: 0.0002209121303167194\n",
            "step: 110, loss: 0.001054995460435748\n",
            "step: 120, loss: 0.0006215343601070344\n",
            "step: 130, loss: 0.004401498474180698\n",
            "step: 140, loss: 0.03148452192544937\n",
            "step: 150, loss: 0.01806139014661312\n",
            "step: 160, loss: 0.004724104888737202\n",
            "step: 170, loss: 0.001209661248140037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7493670886075949, f1=0.7791563275434243, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02667255699634552\n",
            "step: 10, loss: 0.03189006820321083\n",
            "step: 20, loss: 0.0026150611229240894\n",
            "step: 30, loss: 0.00019661210535559803\n",
            "step: 40, loss: 0.00022266226005740464\n",
            "step: 50, loss: 0.011319456622004509\n",
            "step: 60, loss: 0.02306235209107399\n",
            "step: 70, loss: 0.00013700798444915563\n",
            "step: 80, loss: 0.0004388083762023598\n",
            "step: 90, loss: 0.0001511702430434525\n",
            "step: 100, loss: 0.00026523828273639083\n",
            "step: 110, loss: 0.025249967351555824\n",
            "step: 120, loss: 0.00017823318194132298\n",
            "step: 130, loss: 0.00015910889487713575\n",
            "step: 140, loss: 0.00026642915327101946\n",
            "step: 150, loss: 0.007523934356868267\n",
            "step: 160, loss: 0.004268669057637453\n",
            "step: 170, loss: 0.0010571142192929983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7463414634146343, f1=0.7684964200477328, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002678896998986602\n",
            "step: 10, loss: 0.009116039611399174\n",
            "step: 20, loss: 0.005588125437498093\n",
            "step: 30, loss: 0.0005626385100185871\n",
            "step: 40, loss: 0.0027874454390257597\n",
            "step: 50, loss: 0.00018570994143374264\n",
            "step: 60, loss: 0.0005580022698268294\n",
            "step: 70, loss: 0.04348693788051605\n",
            "step: 80, loss: 0.0001644780277274549\n",
            "step: 90, loss: 0.0002355682518100366\n",
            "step: 100, loss: 0.0055463663302361965\n",
            "step: 110, loss: 0.000800244917627424\n",
            "step: 120, loss: 0.0002972127986140549\n",
            "step: 130, loss: 0.023939916864037514\n",
            "step: 140, loss: 0.001537152798846364\n",
            "step: 150, loss: 0.00731431832537055\n",
            "step: 160, loss: 0.0002137282572221011\n",
            "step: 170, loss: 0.0004210870247334242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7461139896373058, f1=0.7735368956743003, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012142891995608807\n",
            "step: 10, loss: 0.03993809223175049\n",
            "step: 20, loss: 0.000637478893622756\n",
            "step: 30, loss: 0.00011940652620978653\n",
            "step: 40, loss: 0.000847768853418529\n",
            "step: 50, loss: 0.00029426085529848933\n",
            "step: 60, loss: 0.017026495188474655\n",
            "step: 70, loss: 0.00027059103013016284\n",
            "step: 80, loss: 0.0008053270285017788\n",
            "step: 90, loss: 0.00015387112216558307\n",
            "step: 100, loss: 0.00038528168806806207\n",
            "step: 110, loss: 0.00027672480791807175\n",
            "step: 120, loss: 0.002644680207595229\n",
            "step: 130, loss: 0.00014539090625476092\n",
            "step: 140, loss: 0.0006854782113805413\n",
            "step: 150, loss: 0.0025327354669570923\n",
            "step: 160, loss: 0.0006412667571566999\n",
            "step: 170, loss: 0.006871034391224384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7608695652173912, f1=0.7913279132791328, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017939240206032991\n",
            "step: 10, loss: 0.00012724737462121993\n",
            "step: 20, loss: 0.00015185891243163496\n",
            "step: 30, loss: 0.00013427012891042978\n",
            "step: 40, loss: 0.00017502981063444167\n",
            "step: 50, loss: 0.00014134906814433634\n",
            "step: 60, loss: 0.00012228888226673007\n",
            "step: 70, loss: 0.00015700209769420326\n",
            "step: 80, loss: 0.0003339051327202469\n",
            "step: 90, loss: 0.0027553082909435034\n",
            "step: 100, loss: 0.0033016130328178406\n",
            "step: 110, loss: 0.00010384165943833068\n",
            "step: 120, loss: 0.06772724539041519\n",
            "step: 130, loss: 0.0006785726291127503\n",
            "step: 140, loss: 0.0006280656671151519\n",
            "step: 150, loss: 0.00011919863027287647\n",
            "step: 160, loss: 0.0021097261924296618\n",
            "step: 170, loss: 0.00037717001396231353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.77088948787062, f1=0.8010752688172044, best_f1=0.7957559681697612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00025426523643545806\n",
            "step: 10, loss: 0.0004889380652457476\n",
            "step: 20, loss: 0.004963186103850603\n",
            "step: 30, loss: 0.00014204639592207968\n",
            "step: 40, loss: 0.0015646623214706779\n",
            "step: 50, loss: 0.00016685058653820306\n",
            "step: 60, loss: 0.025563469156622887\n",
            "step: 70, loss: 0.00014563740114681423\n",
            "step: 80, loss: 0.005889447871595621\n",
            "step: 90, loss: 0.00014653084508609027\n",
            "step: 100, loss: 0.00023762833734508604\n",
            "step: 110, loss: 8.133308438118547e-05\n",
            "step: 120, loss: 0.00011262425687164068\n",
            "step: 130, loss: 0.0010017314925789833\n",
            "step: 140, loss: 0.00022569249267689884\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.0003675313200801611\n",
            "step: 160, loss: 0.00010112916788784787\n",
            "step: 170, loss: 0.00047753192484378815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7647058823529412, f1=0.806366047745358, best_f1=0.7957559681697612\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 245.21it/s]\n",
            "load_f1 = 0.7759562841530055\n",
            "real_f1 = 0.7814207650273223\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 233.10it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djX3yHRNgNFP"
      },
      "source": [
        "## BASELINE DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b011EMgogNFP"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc36a1d-b730-493d-fc3c-3fba87300d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 566kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 4.10MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 63.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6149151921272278\n",
            "step: 10, loss: 0.6157604455947876\n",
            "step: 20, loss: 0.49896278977394104\n",
            "step: 30, loss: 0.32860293984413147\n",
            "step: 40, loss: 0.21530596911907196\n",
            "step: 50, loss: 0.029073212295770645\n",
            "step: 60, loss: 0.0796390026807785\n",
            "step: 70, loss: 0.03719450533390045\n",
            "step: 80, loss: 0.049051523208618164\n",
            "step: 90, loss: 0.16498950123786926\n",
            "step: 100, loss: 0.009620314463973045\n",
            "step: 110, loss: 0.3332911431789398\n",
            "step: 120, loss: 0.012820156291127205\n",
            "step: 130, loss: 0.10954955220222473\n",
            "step: 140, loss: 0.03204610198736191\n",
            "step: 150, loss: 0.07400670647621155\n",
            "step: 160, loss: 0.03684694692492485\n",
            "step: 170, loss: 0.07838038355112076\n",
            "step: 180, loss: 0.034760475158691406\n",
            "step: 190, loss: 0.05884244665503502\n",
            "step: 200, loss: 0.12855884432792664\n",
            "step: 210, loss: 0.01315909530967474\n",
            "step: 220, loss: 0.02909490279853344\n",
            "step: 230, loss: 0.01693233661353588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9617977528089887, f1=0.9641255605381166, best_f1=0.9641255605381166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022869789972901344\n",
            "step: 10, loss: 0.10462938249111176\n",
            "step: 20, loss: 0.19796209037303925\n",
            "step: 30, loss: 0.1694573312997818\n",
            "step: 40, loss: 0.07310540974140167\n",
            "step: 50, loss: 0.014539841562509537\n",
            "step: 60, loss: 0.0042620557360351086\n",
            "step: 70, loss: 0.11017307639122009\n",
            "step: 80, loss: 0.0963747426867485\n",
            "step: 90, loss: 0.062332406640052795\n",
            "step: 100, loss: 0.04180927574634552\n",
            "step: 110, loss: 0.08752545714378357\n",
            "step: 120, loss: 0.09090100228786469\n",
            "step: 130, loss: 0.006869492586702108\n",
            "step: 140, loss: 0.0029354507569223642\n",
            "step: 150, loss: 0.011863415129482746\n",
            "step: 160, loss: 0.016987983137369156\n",
            "step: 170, loss: 0.0006061301683075726\n",
            "step: 180, loss: 0.13848721981048584\n",
            "step: 190, loss: 0.055257368832826614\n",
            "step: 200, loss: 0.028225481510162354\n",
            "step: 210, loss: 0.003544504288583994\n",
            "step: 220, loss: 0.009566976688802242\n",
            "step: 230, loss: 0.04798177629709244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9621380846325166, f1=0.9622222222222223, best_f1=0.9622222222222223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09455239772796631\n",
            "step: 10, loss: 0.01825740374624729\n",
            "step: 20, loss: 0.03781408444046974\n",
            "step: 30, loss: 0.051320385187864304\n",
            "step: 40, loss: 0.11732278764247894\n",
            "step: 50, loss: 0.017408037558197975\n",
            "step: 60, loss: 0.015465062111616135\n",
            "step: 70, loss: 0.025335563346743584\n",
            "step: 80, loss: 0.0023153366055339575\n",
            "step: 90, loss: 0.11514794081449509\n",
            "step: 100, loss: 0.0007392295519821346\n",
            "step: 110, loss: 0.0015571234980598092\n",
            "step: 120, loss: 0.002156763104721904\n",
            "step: 130, loss: 0.0005039318348281085\n",
            "step: 140, loss: 0.00663351034745574\n",
            "step: 150, loss: 0.009415450505912304\n",
            "step: 160, loss: 0.23461447656154633\n",
            "step: 170, loss: 0.11469081044197083\n",
            "step: 180, loss: 0.041946396231651306\n",
            "step: 190, loss: 0.014984416775405407\n",
            "step: 200, loss: 0.014497308060526848\n",
            "step: 210, loss: 0.05925232172012329\n",
            "step: 220, loss: 0.0278960932046175\n",
            "step: 230, loss: 0.0020892454776912928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9658314350797267, f1=0.9681093394077448, best_f1=0.9681093394077448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005631730891764164\n",
            "step: 10, loss: 0.001217568526044488\n",
            "step: 20, loss: 0.0025111373979598284\n",
            "step: 30, loss: 0.0008844159310683608\n",
            "step: 40, loss: 0.0015325963031500578\n",
            "step: 50, loss: 0.0002706674567889422\n",
            "step: 60, loss: 0.008224003948271275\n",
            "step: 70, loss: 0.0011344236554577947\n",
            "step: 80, loss: 0.0008539056871086359\n",
            "step: 90, loss: 0.010451119393110275\n",
            "step: 100, loss: 0.0007176707149483263\n",
            "step: 110, loss: 0.00046631533768959343\n",
            "step: 120, loss: 0.007465501315891743\n",
            "step: 130, loss: 0.007206170354038477\n",
            "step: 140, loss: 0.0015816095983609557\n",
            "step: 150, loss: 0.1518407016992569\n",
            "step: 160, loss: 0.038453128188848495\n",
            "step: 170, loss: 0.014981027692556381\n",
            "step: 180, loss: 0.0010065758833661675\n",
            "step: 190, loss: 0.008912356570363045\n",
            "step: 200, loss: 0.001578883035108447\n",
            "step: 210, loss: 0.13200876116752625\n",
            "step: 220, loss: 0.0005104629672132432\n",
            "step: 230, loss: 0.07833696901798248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.971815107102593, f1=0.9785794813979707, best_f1=0.9785794813979707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005110279307700694\n",
            "step: 10, loss: 0.0004100445657968521\n",
            "step: 20, loss: 0.05782388150691986\n",
            "step: 30, loss: 0.05479517579078674\n",
            "step: 40, loss: 0.025241967290639877\n",
            "step: 50, loss: 0.07644841820001602\n",
            "step: 60, loss: 0.08582468330860138\n",
            "step: 70, loss: 0.010826285928487778\n",
            "step: 80, loss: 0.0024159515742212534\n",
            "step: 90, loss: 0.0019364876206964254\n",
            "step: 100, loss: 0.0013604938285425305\n",
            "step: 110, loss: 0.0012465707259252667\n",
            "step: 120, loss: 0.00865617860108614\n",
            "step: 130, loss: 0.0010975106852129102\n",
            "step: 140, loss: 0.007250786758959293\n",
            "step: 150, loss: 0.0006920408341102302\n",
            "step: 160, loss: 0.0025436733849346638\n",
            "step: 170, loss: 0.055687155574560165\n",
            "step: 180, loss: 0.0008323714137077332\n",
            "step: 190, loss: 0.0412159264087677\n",
            "step: 200, loss: 0.0011693417327478528\n",
            "step: 210, loss: 0.0005218542646616697\n",
            "step: 220, loss: 0.0009170316625386477\n",
            "step: 230, loss: 0.0502018928527832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9694224235560589, f1=0.9797752808988766, best_f1=0.9785794813979707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008890473283827305\n",
            "step: 10, loss: 0.002575952559709549\n",
            "step: 20, loss: 0.0030976193957030773\n",
            "step: 30, loss: 0.00022896476730238646\n",
            "step: 40, loss: 0.000265709386439994\n",
            "step: 50, loss: 0.00042010904871858656\n",
            "step: 60, loss: 0.0008432003669440746\n",
            "step: 70, loss: 0.00034848126233555377\n",
            "step: 80, loss: 0.01746382936835289\n",
            "step: 90, loss: 0.0008231747196987271\n",
            "step: 100, loss: 0.03359656780958176\n",
            "step: 110, loss: 0.03777102753520012\n",
            "step: 120, loss: 0.001957149710506201\n",
            "step: 130, loss: 0.00029827351681888103\n",
            "step: 140, loss: 0.001356559805572033\n",
            "step: 150, loss: 0.0027922845911234617\n",
            "step: 160, loss: 0.0017945842118933797\n",
            "step: 170, loss: 0.0006623430526815355\n",
            "step: 180, loss: 0.021669279783964157\n",
            "step: 190, loss: 0.001246413798071444\n",
            "step: 200, loss: 0.00024402826966252178\n",
            "step: 210, loss: 0.00452799117192626\n",
            "step: 220, loss: 0.00031304333242587745\n",
            "step: 230, loss: 0.05988805741071701\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9730941704035874, f1=0.9842342342342343, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015092747053131461\n",
            "step: 10, loss: 0.00016251049237325788\n",
            "step: 20, loss: 0.005065700504928827\n",
            "step: 30, loss: 0.0001669956836849451\n",
            "step: 40, loss: 0.00025292523787356913\n",
            "step: 50, loss: 0.0004377156146802008\n",
            "step: 60, loss: 0.00017552934878040105\n",
            "step: 70, loss: 0.08622075617313385\n",
            "step: 80, loss: 0.007468738127499819\n",
            "step: 90, loss: 8.738309406908229e-05\n",
            "step: 100, loss: 0.0002339666389161721\n",
            "step: 110, loss: 0.00014324243238661438\n",
            "step: 120, loss: 0.00011252309195697308\n",
            "step: 130, loss: 0.00017427746206521988\n",
            "step: 140, loss: 0.00036476648529060185\n",
            "step: 150, loss: 0.0004878986510448158\n",
            "step: 160, loss: 0.025045771151781082\n",
            "step: 170, loss: 0.0027980993036180735\n",
            "step: 180, loss: 0.0006236847839318216\n",
            "step: 190, loss: 0.00038604048313573003\n",
            "step: 200, loss: 0.029536070302128792\n",
            "step: 210, loss: 0.0001246351603185758\n",
            "step: 220, loss: 0.000990306376479566\n",
            "step: 230, loss: 0.0012232891749590635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.968609865470852, f1=0.9753363228699552, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005807020934298635\n",
            "step: 10, loss: 0.0037615762557834387\n",
            "step: 20, loss: 0.00018793570052366704\n",
            "step: 30, loss: 0.000210518017411232\n",
            "step: 40, loss: 0.0028293782379478216\n",
            "step: 50, loss: 0.0036455111112445593\n",
            "step: 60, loss: 0.0002691514091566205\n",
            "step: 70, loss: 0.0002476766530890018\n",
            "step: 80, loss: 0.0011632053647190332\n",
            "step: 90, loss: 0.00010551418381510302\n",
            "step: 100, loss: 0.000576366379391402\n",
            "step: 110, loss: 0.12386876344680786\n",
            "step: 120, loss: 0.0026103754062205553\n",
            "step: 130, loss: 0.0011880015954375267\n",
            "step: 140, loss: 0.00018622119387146086\n",
            "step: 150, loss: 0.00017118534015025944\n",
            "step: 160, loss: 0.00043792877113446593\n",
            "step: 170, loss: 0.036523688584566116\n",
            "step: 180, loss: 0.008909414522349834\n",
            "step: 190, loss: 0.06747126579284668\n",
            "step: 200, loss: 0.00016515323659405112\n",
            "step: 210, loss: 0.0017101522535085678\n",
            "step: 220, loss: 0.0004229988844599575\n",
            "step: 230, loss: 0.0011381716467440128\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9697648376259798, f1=0.972129319955407, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018154091958422214\n",
            "step: 10, loss: 0.0027412448544055223\n",
            "step: 20, loss: 0.00047335962881334126\n",
            "step: 30, loss: 6.789527833461761e-05\n",
            "step: 40, loss: 0.013827181421220303\n",
            "step: 50, loss: 0.00038439975469373167\n",
            "step: 60, loss: 0.0020947833545506\n",
            "step: 70, loss: 9.65762956184335e-05\n",
            "step: 80, loss: 0.00014533383364323527\n",
            "step: 90, loss: 0.0004019986081402749\n",
            "step: 100, loss: 0.00018692782032303512\n",
            "step: 110, loss: 6.687956192763522e-05\n",
            "step: 120, loss: 5.608419087366201e-05\n",
            "step: 130, loss: 0.00018417200772091746\n",
            "step: 140, loss: 8.082435670075938e-05\n",
            "step: 150, loss: 0.0013088196283206344\n",
            "step: 160, loss: 5.339454946806654e-05\n",
            "step: 170, loss: 0.00018721104424912483\n",
            "step: 180, loss: 0.000766140699852258\n",
            "step: 190, loss: 0.0002704434155020863\n",
            "step: 200, loss: 0.0001174425779026933\n",
            "step: 210, loss: 0.00013837922597303987\n",
            "step: 220, loss: 4.767367136082612e-05\n",
            "step: 230, loss: 0.00037635283661074936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9741282339707535, f1=0.9752252252252253, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002638149890117347\n",
            "step: 10, loss: 0.00016553536988794804\n",
            "step: 20, loss: 4.868226096732542e-05\n",
            "step: 30, loss: 9.274829062633216e-05\n",
            "step: 40, loss: 7.356169953709468e-05\n",
            "step: 50, loss: 5.6704491726122797e-05\n",
            "step: 60, loss: 0.00017264235066249967\n",
            "step: 70, loss: 0.0002908711030613631\n",
            "step: 80, loss: 8.8931767095346e-05\n",
            "step: 90, loss: 0.0002569802454672754\n",
            "step: 100, loss: 4.6418284910032526e-05\n",
            "step: 110, loss: 7.845889194868505e-05\n",
            "step: 120, loss: 0.00028941742493771017\n",
            "step: 130, loss: 0.00010317245323676616\n",
            "step: 140, loss: 0.042606353759765625\n",
            "step: 150, loss: 0.036430664360523224\n",
            "step: 160, loss: 7.491608994314447e-05\n",
            "step: 170, loss: 4.853700374951586e-05\n",
            "step: 180, loss: 0.0013542554806917906\n",
            "step: 190, loss: 0.002299710176885128\n",
            "step: 200, loss: 3.944872514693998e-05\n",
            "step: 210, loss: 0.00031708364258520305\n",
            "step: 220, loss: 8.223443728638813e-05\n",
            "step: 230, loss: 0.0002786130062304437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9707207207207207, f1=0.9763779527559054, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.445512942969799e-05\n",
            "step: 10, loss: 0.00013229349860921502\n",
            "step: 20, loss: 0.0001612775376997888\n",
            "step: 30, loss: 0.0006086168577894568\n",
            "step: 40, loss: 0.00011980049748672172\n",
            "step: 50, loss: 5.608334322459996e-05\n",
            "step: 60, loss: 0.011346694082021713\n",
            "step: 70, loss: 0.00031005532946437597\n",
            "step: 80, loss: 3.5783712519332767e-05\n",
            "step: 90, loss: 8.930317562771961e-05\n",
            "step: 100, loss: 8.217254071496427e-05\n",
            "step: 110, loss: 0.0013939541531726718\n",
            "step: 120, loss: 4.9885718908626586e-05\n",
            "step: 130, loss: 6.067248614272103e-05\n",
            "step: 140, loss: 4.8204849008470774e-05\n",
            "step: 150, loss: 0.015378871001303196\n",
            "step: 160, loss: 2.9238979550427757e-05\n",
            "step: 170, loss: 0.00043452956015244126\n",
            "step: 180, loss: 0.0001157947481260635\n",
            "step: 190, loss: 3.423012822167948e-05\n",
            "step: 200, loss: 0.00011272386473137885\n",
            "step: 210, loss: 3.720622407854535e-05\n",
            "step: 220, loss: 4.197440648567863e-05\n",
            "step: 230, loss: 4.969084693584591e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.970917225950783, f1=0.9753914988814317, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.0433743783505633e-05\n",
            "step: 10, loss: 4.673085277318023e-05\n",
            "step: 20, loss: 6.395357922883704e-05\n",
            "step: 30, loss: 8.439907833235338e-05\n",
            "step: 40, loss: 0.00040497854934073985\n",
            "step: 50, loss: 0.0002615534176584333\n",
            "step: 60, loss: 0.00013161909009795636\n",
            "step: 70, loss: 5.8803077990887687e-05\n",
            "step: 80, loss: 0.00011117149551864713\n",
            "step: 90, loss: 3.0233513825805858e-05\n",
            "step: 100, loss: 3.214041134924628e-05\n",
            "step: 110, loss: 3.7806144973728806e-05\n",
            "step: 120, loss: 4.0352493670070544e-05\n",
            "step: 130, loss: 3.6900695704389364e-05\n",
            "step: 140, loss: 0.0007155967177823186\n",
            "step: 150, loss: 4.249274206813425e-05\n",
            "step: 160, loss: 5.0924543756991625e-05\n",
            "step: 170, loss: 3.225240288884379e-05\n",
            "step: 180, loss: 0.005162614397704601\n",
            "step: 190, loss: 0.00012887213961221278\n",
            "step: 200, loss: 3.09184288198594e-05\n",
            "step: 210, loss: 5.4305040976032615e-05\n",
            "step: 220, loss: 4.0456892747897655e-05\n",
            "step: 230, loss: 0.025572674348950386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9730941704035874, f1=0.9731543624161074, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.704731691163033e-05\n",
            "step: 10, loss: 0.00011407359124859795\n",
            "step: 20, loss: 9.23673651413992e-05\n",
            "step: 30, loss: 0.00010561208910075948\n",
            "step: 40, loss: 8.4567756857723e-05\n",
            "step: 50, loss: 6.894624675624073e-05\n",
            "step: 60, loss: 3.916191781172529e-05\n",
            "step: 70, loss: 3.125378498225473e-05\n",
            "step: 80, loss: 3.5213553928770125e-05\n",
            "step: 90, loss: 6.938260048627853e-05\n",
            "step: 100, loss: 3.917621143045835e-05\n",
            "step: 110, loss: 0.005998645443469286\n",
            "step: 120, loss: 0.014253332279622555\n",
            "step: 130, loss: 3.798888064920902e-05\n",
            "step: 140, loss: 5.951978891971521e-05\n",
            "step: 150, loss: 6.960642349440604e-05\n",
            "step: 160, loss: 0.00022091748542152345\n",
            "step: 170, loss: 7.030621054582298e-05\n",
            "step: 180, loss: 4.679746416513808e-05\n",
            "step: 190, loss: 3.4940807381644845e-05\n",
            "step: 200, loss: 5.144074748386629e-05\n",
            "step: 210, loss: 2.981217585329432e-05\n",
            "step: 220, loss: 3.809266854659654e-05\n",
            "step: 230, loss: 3.312391345389187e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9705882352941176, f1=0.9784824462061155, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.0886617827927694e-05\n",
            "step: 10, loss: 7.948796701384708e-05\n",
            "step: 20, loss: 4.48657083325088e-05\n",
            "step: 30, loss: 4.235079904901795e-05\n",
            "step: 40, loss: 0.021142516285181046\n",
            "step: 50, loss: 4.624296343536116e-05\n",
            "step: 60, loss: 4.365641507320106e-05\n",
            "step: 70, loss: 6.462140299845487e-05\n",
            "step: 80, loss: 4.236547101754695e-05\n",
            "step: 90, loss: 6.598773325094953e-05\n",
            "step: 100, loss: 4.548646757029928e-05\n",
            "step: 110, loss: 0.0002736106689553708\n",
            "step: 120, loss: 2.8467278752941638e-05\n",
            "step: 130, loss: 4.8462195991305634e-05\n",
            "step: 140, loss: 3.9672300772508606e-05\n",
            "step: 150, loss: 5.047932063462213e-05\n",
            "step: 160, loss: 0.003619818016886711\n",
            "step: 170, loss: 1.714728568913415e-05\n",
            "step: 180, loss: 0.00010102267697220668\n",
            "step: 190, loss: 0.00010253037908114493\n",
            "step: 200, loss: 3.634511449490674e-05\n",
            "step: 210, loss: 4.318798892199993e-05\n",
            "step: 220, loss: 0.0004602159315254539\n",
            "step: 230, loss: 0.0004352243850007653\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9706546275395034, f1=0.9796380090497738, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.617669088067487e-05\n",
            "step: 10, loss: 2.1010113414376974e-05\n",
            "step: 20, loss: 3.998868851340376e-05\n",
            "step: 30, loss: 5.4933741921558976e-05\n",
            "step: 40, loss: 8.861716196406633e-05\n",
            "step: 50, loss: 0.014391402713954449\n",
            "step: 60, loss: 5.862842226633802e-05\n",
            "step: 70, loss: 6.43418388790451e-05\n",
            "step: 80, loss: 4.037117832922377e-05\n",
            "step: 90, loss: 8.286676893476397e-05\n",
            "step: 100, loss: 3.3526313927723095e-05\n",
            "step: 110, loss: 4.7220277338055894e-05\n",
            "step: 120, loss: 3.583939542295411e-05\n",
            "step: 130, loss: 0.002645030850544572\n",
            "step: 140, loss: 4.5328524720389396e-05\n",
            "step: 150, loss: 5.911013431614265e-05\n",
            "step: 160, loss: 2.22170892811846e-05\n",
            "step: 170, loss: 3.973702769144438e-05\n",
            "step: 180, loss: 0.00021563551854342222\n",
            "step: 190, loss: 3.581684723030776e-05\n",
            "step: 200, loss: 2.8758440748788416e-05\n",
            "step: 210, loss: 4.55156150565017e-05\n",
            "step: 220, loss: 0.00032544598798267543\n",
            "step: 230, loss: 3.12539232254494e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9706546275395034, f1=0.9785310734463276, best_f1=0.9752252252252253\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 194.02it/s]\n",
            "load_f1 = 0.9740698985343857\n",
            "real_f1 = 0.9741282339707535\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 231.28it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h62Yut_pgNFQ"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fa468e-43be-455c-8602-8ce69709d6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6496710181236267\n",
            "step: 10, loss: 0.5355506539344788\n",
            "step: 20, loss: 0.5474937558174133\n",
            "step: 30, loss: 0.1977870911359787\n",
            "step: 40, loss: 0.199203759431839\n",
            "step: 50, loss: 0.22630909085273743\n",
            "step: 60, loss: 0.11098060756921768\n",
            "step: 70, loss: 0.11724264919757843\n",
            "step: 80, loss: 0.06727444380521774\n",
            "step: 90, loss: 0.21277949213981628\n",
            "step: 100, loss: 0.06448571383953094\n",
            "step: 110, loss: 0.09347495436668396\n",
            "step: 120, loss: 0.09402161091566086\n",
            "step: 130, loss: 0.05656568333506584\n",
            "step: 140, loss: 0.06281556934118271\n",
            "step: 150, loss: 0.029476486146450043\n",
            "step: 160, loss: 0.03431320935487747\n",
            "step: 170, loss: 0.2646041214466095\n",
            "step: 180, loss: 0.058851458132267\n",
            "step: 190, loss: 0.02995334006845951\n",
            "step: 200, loss: 0.12394876778125763\n",
            "step: 210, loss: 0.118414968252182\n",
            "step: 220, loss: 0.22789721190929413\n",
            "step: 230, loss: 0.16635273396968842\n",
            "step: 240, loss: 0.14570163190364838\n",
            "step: 250, loss: 0.03691399097442627\n",
            "step: 260, loss: 0.05003497377038002\n",
            "step: 270, loss: 0.022751180455088615\n",
            "step: 280, loss: 0.10740266740322113\n",
            "step: 290, loss: 0.07797804474830627\n",
            "step: 300, loss: 0.047100815922021866\n",
            "step: 310, loss: 0.31003692746162415\n",
            "step: 320, loss: 0.151001438498497\n",
            "step: 330, loss: 0.0743824690580368\n",
            "step: 340, loss: 0.07431641221046448\n",
            "step: 350, loss: 0.07062654942274094\n",
            "step: 360, loss: 0.18231913447380066\n",
            "step: 370, loss: 0.07309827208518982\n",
            "step: 380, loss: 0.02723441645503044\n",
            "step: 390, loss: 0.143576517701149\n",
            "step: 400, loss: 0.2597905099391937\n",
            "step: 410, loss: 0.09891984611749649\n",
            "step: 420, loss: 0.030317245051264763\n",
            "step: 430, loss: 0.10222964733839035\n",
            "step: 440, loss: 0.020777028053998947\n",
            "step: 450, loss: 0.03128473088145256\n",
            "step: 460, loss: 0.016567949205636978\n",
            "step: 470, loss: 0.10734525322914124\n",
            "step: 480, loss: 0.12771886587142944\n",
            "step: 490, loss: 0.07603038102388382\n",
            "step: 500, loss: 0.1711377501487732\n",
            "step: 510, loss: 0.05575227737426758\n",
            "step: 520, loss: 0.13102181255817413\n",
            "step: 530, loss: 0.004091911017894745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9246919214970334, f1=0.9249771271729187, best_f1=0.9249771271729187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12402001768350601\n",
            "step: 10, loss: 0.08282976597547531\n",
            "step: 20, loss: 0.011573459021747112\n",
            "step: 30, loss: 0.02298850193619728\n",
            "step: 40, loss: 0.04273645579814911\n",
            "step: 50, loss: 0.19738708436489105\n",
            "step: 60, loss: 0.011571216396987438\n",
            "step: 70, loss: 0.03472288325428963\n",
            "step: 80, loss: 0.07181359827518463\n",
            "step: 90, loss: 0.014796201139688492\n",
            "step: 100, loss: 0.019887851551175117\n",
            "step: 110, loss: 0.026556219905614853\n",
            "step: 120, loss: 0.017837906256318092\n",
            "step: 130, loss: 0.07103241235017776\n",
            "step: 140, loss: 0.08637451380491257\n",
            "step: 150, loss: 0.02713618613779545\n",
            "step: 160, loss: 0.04622432217001915\n",
            "step: 170, loss: 0.12571386992931366\n",
            "step: 180, loss: 0.050673194229602814\n",
            "step: 190, loss: 0.07489009946584702\n",
            "step: 200, loss: 0.007570087444037199\n",
            "step: 210, loss: 0.03942751884460449\n",
            "step: 220, loss: 0.025336802005767822\n",
            "step: 230, loss: 0.030194880440831184\n",
            "step: 240, loss: 0.02168498933315277\n",
            "step: 250, loss: 0.08568920195102692\n",
            "step: 260, loss: 0.03307507932186127\n",
            "step: 270, loss: 0.1979183852672577\n",
            "step: 280, loss: 0.11827126145362854\n",
            "step: 290, loss: 0.029966123402118683\n",
            "step: 300, loss: 0.19248470664024353\n",
            "step: 310, loss: 0.009243116714060307\n",
            "step: 320, loss: 0.1541922241449356\n",
            "step: 330, loss: 0.06171722337603569\n",
            "step: 340, loss: 0.015498477034270763\n",
            "step: 350, loss: 0.006829812191426754\n",
            "step: 360, loss: 0.1282108575105667\n",
            "step: 370, loss: 0.10787132382392883\n",
            "step: 380, loss: 0.06798841059207916\n",
            "step: 390, loss: 0.0438622310757637\n",
            "step: 400, loss: 0.1245722770690918\n",
            "step: 410, loss: 0.04703742265701294\n",
            "step: 420, loss: 0.11779433488845825\n",
            "step: 430, loss: 0.053284477442502975\n",
            "step: 440, loss: 0.17642110586166382\n",
            "step: 450, loss: 0.13128606975078583\n",
            "step: 460, loss: 0.04054923728108406\n",
            "step: 470, loss: 0.08555746078491211\n",
            "step: 480, loss: 0.19525031745433807\n",
            "step: 490, loss: 0.02593224123120308\n",
            "step: 500, loss: 0.18956823647022247\n",
            "step: 510, loss: 0.011702745221555233\n",
            "step: 520, loss: 0.0774252712726593\n",
            "step: 530, loss: 0.01239960640668869\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9260450160771704, f1=0.9284403669724771, best_f1=0.9284403669724771\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12761543691158295\n",
            "step: 10, loss: 0.02692047506570816\n",
            "step: 20, loss: 0.04710455238819122\n",
            "step: 30, loss: 0.07212059944868088\n",
            "step: 40, loss: 0.19531728327274323\n",
            "step: 50, loss: 0.12339923530817032\n",
            "step: 60, loss: 0.023849422112107277\n",
            "step: 70, loss: 0.0027424136642366648\n",
            "step: 80, loss: 0.017781171947717667\n",
            "step: 90, loss: 0.006694050505757332\n",
            "step: 100, loss: 0.12935857474803925\n",
            "step: 110, loss: 0.0039881146512925625\n",
            "step: 120, loss: 0.004681040532886982\n",
            "step: 130, loss: 0.020981481298804283\n",
            "step: 140, loss: 0.020270463079214096\n",
            "step: 150, loss: 0.013434026390314102\n",
            "step: 160, loss: 0.007404050324112177\n",
            "step: 170, loss: 0.014107425697147846\n",
            "step: 180, loss: 0.009945429861545563\n",
            "step: 190, loss: 0.012379590421915054\n",
            "step: 200, loss: 0.04726565629243851\n",
            "step: 210, loss: 0.09529230743646622\n",
            "step: 220, loss: 0.024814285337924957\n",
            "step: 230, loss: 0.05600526183843613\n",
            "step: 240, loss: 0.014311379753053188\n",
            "step: 250, loss: 0.03650589659810066\n",
            "step: 260, loss: 0.022331558167934418\n",
            "step: 270, loss: 0.008300729095935822\n",
            "step: 280, loss: 0.060539379715919495\n",
            "step: 290, loss: 0.0026584502775222063\n",
            "step: 300, loss: 0.06238330155611038\n",
            "step: 310, loss: 0.008183641359210014\n",
            "step: 320, loss: 0.022303259000182152\n",
            "step: 330, loss: 0.009579619392752647\n",
            "step: 340, loss: 0.002222067443653941\n",
            "step: 350, loss: 0.0033683988731354475\n",
            "step: 360, loss: 0.05438629910349846\n",
            "step: 370, loss: 0.016209183260798454\n",
            "step: 380, loss: 0.015004412271082401\n",
            "step: 390, loss: 0.017906207591295242\n",
            "step: 400, loss: 0.00947515107691288\n",
            "step: 410, loss: 0.005249433685094118\n",
            "step: 420, loss: 0.10279760509729385\n",
            "step: 430, loss: 0.015238862484693527\n",
            "step: 440, loss: 0.009358610026538372\n",
            "step: 450, loss: 0.05511041358113289\n",
            "step: 460, loss: 0.033059991896152496\n",
            "step: 470, loss: 0.07353335618972778\n",
            "step: 480, loss: 0.004735864233225584\n",
            "step: 490, loss: 0.005474486853927374\n",
            "step: 500, loss: 0.019051043316721916\n",
            "step: 510, loss: 0.02098867855966091\n",
            "step: 520, loss: 0.05631197616457939\n",
            "step: 530, loss: 0.024868082255125046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9310344827586207, f1=0.9225023342670402, best_f1=0.9225023342670402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01980866864323616\n",
            "step: 10, loss: 0.005490655545145273\n",
            "step: 20, loss: 0.005011112429201603\n",
            "step: 30, loss: 0.0011700086761265993\n",
            "step: 40, loss: 0.0028695114888250828\n",
            "step: 50, loss: 0.017948588356375694\n",
            "step: 60, loss: 0.0022288148757070303\n",
            "step: 70, loss: 0.009024304337799549\n",
            "step: 80, loss: 0.014230922795832157\n",
            "step: 90, loss: 0.07210805267095566\n",
            "step: 100, loss: 0.0016932712169364095\n",
            "step: 110, loss: 0.005408769007772207\n",
            "step: 120, loss: 0.003618905320763588\n",
            "step: 130, loss: 0.0008444340783171356\n",
            "step: 140, loss: 0.000744595134165138\n",
            "step: 150, loss: 0.0014196683187037706\n",
            "step: 160, loss: 0.07691176235675812\n",
            "step: 170, loss: 0.005852863658219576\n",
            "step: 180, loss: 0.0046478332951664925\n",
            "step: 190, loss: 0.08499796688556671\n",
            "step: 200, loss: 0.007401813752949238\n",
            "step: 210, loss: 0.044213343411684036\n",
            "step: 220, loss: 0.0064554098062217236\n",
            "step: 230, loss: 0.029861368238925934\n",
            "step: 240, loss: 0.004553196486085653\n",
            "step: 250, loss: 0.033087991178035736\n",
            "step: 260, loss: 0.009218159131705761\n",
            "step: 270, loss: 0.006069767288863659\n",
            "step: 280, loss: 0.11987975984811783\n",
            "step: 290, loss: 0.034156739711761475\n",
            "step: 300, loss: 0.0009717888315208256\n",
            "step: 310, loss: 0.09984275698661804\n",
            "step: 320, loss: 0.0021901768632233143\n",
            "step: 330, loss: 0.006877771578729153\n",
            "step: 340, loss: 0.1298915594816208\n",
            "step: 350, loss: 0.006486755795776844\n",
            "step: 360, loss: 0.008373936638236046\n",
            "step: 370, loss: 0.004640497267246246\n",
            "step: 380, loss: 0.0028901954647153616\n",
            "step: 390, loss: 0.003491985844448209\n",
            "step: 400, loss: 0.0007395677966997027\n",
            "step: 410, loss: 0.0028256536461412907\n",
            "step: 420, loss: 0.05533720925450325\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 430, loss: 0.2986679673194885\n",
            "step: 440, loss: 0.047281503677368164\n",
            "step: 450, loss: 0.03140322491526604\n",
            "step: 460, loss: 0.02597256377339363\n",
            "step: 470, loss: 0.013161000795662403\n",
            "step: 480, loss: 0.041553109884262085\n",
            "step: 490, loss: 0.011153471656143665\n",
            "step: 500, loss: 0.0029792541172355413\n",
            "step: 510, loss: 0.01649804785847664\n",
            "step: 520, loss: 0.036048270761966705\n",
            "step: 530, loss: 0.05213514715433121\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9236430542778289, f1=0.9301470588235294, best_f1=0.9225023342670402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03914127126336098\n",
            "step: 10, loss: 0.05313700810074806\n",
            "step: 20, loss: 0.003619518829509616\n",
            "step: 30, loss: 0.0025986114051193\n",
            "step: 40, loss: 0.001701953005976975\n",
            "step: 50, loss: 0.0849851742386818\n",
            "step: 60, loss: 0.07474177330732346\n",
            "step: 70, loss: 0.004792123567312956\n",
            "step: 80, loss: 0.002728985855355859\n",
            "step: 90, loss: 0.08227050304412842\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 100, loss: 0.003499666228890419\n",
            "step: 110, loss: 0.0013891300186514854\n",
            "step: 120, loss: 0.0014662927715107799\n",
            "step: 130, loss: 0.0003974221181124449\n",
            "step: 140, loss: 0.0014383683446794748\n",
            "step: 150, loss: 0.0028773059602826834\n",
            "step: 160, loss: 0.00223339325748384\n",
            "step: 170, loss: 0.004463139455765486\n",
            "step: 180, loss: 0.008507385849952698\n",
            "step: 190, loss: 0.0008084392175078392\n",
            "step: 200, loss: 0.0008547122124582529\n",
            "step: 210, loss: 0.010911817662417889\n",
            "step: 220, loss: 0.013245632871985435\n",
            "step: 230, loss: 0.0059165144339203835\n",
            "step: 240, loss: 0.004092320799827576\n",
            "step: 250, loss: 0.02236495353281498\n",
            "step: 260, loss: 0.027840912342071533\n",
            "step: 270, loss: 0.005176469683647156\n",
            "step: 280, loss: 0.019035179167985916\n",
            "step: 290, loss: 0.1346452385187149\n",
            "step: 300, loss: 0.0008709232788532972\n",
            "step: 310, loss: 0.015449218451976776\n",
            "step: 320, loss: 0.08903078734874725\n",
            "step: 330, loss: 0.01867162249982357\n",
            "step: 340, loss: 0.0026656093541532755\n",
            "step: 350, loss: 0.0023844523821026087\n",
            "step: 360, loss: 0.01517312042415142\n",
            "step: 370, loss: 0.02403443120419979\n",
            "step: 380, loss: 0.000515331921633333\n",
            "step: 390, loss: 0.00020528071036096662\n",
            "step: 400, loss: 0.002189124235883355\n",
            "step: 410, loss: 0.002498648827895522\n",
            "step: 420, loss: 0.005367943085730076\n",
            "step: 430, loss: 0.003955364692956209\n",
            "step: 440, loss: 0.010672789067029953\n",
            "step: 450, loss: 0.019898006692528725\n",
            "step: 460, loss: 0.007720068097114563\n",
            "step: 470, loss: 0.0024507285561412573\n",
            "step: 480, loss: 0.005106728989630938\n",
            "step: 490, loss: 0.00015589609392918646\n",
            "step: 500, loss: 0.0004854470316786319\n",
            "step: 510, loss: 0.22279532253742218\n",
            "step: 520, loss: 0.011761325411498547\n",
            "step: 530, loss: 0.003931534010916948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9261495587552253, f1=0.9212855146716348, best_f1=0.9225023342670402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008508889586664736\n",
            "step: 10, loss: 0.00017819119966588914\n",
            "step: 20, loss: 0.02238820306956768\n",
            "step: 30, loss: 0.0005004773265682161\n",
            "step: 40, loss: 0.002278541214764118\n",
            "step: 50, loss: 0.2002110332250595\n",
            "step: 60, loss: 8.722676284378394e-05\n",
            "step: 70, loss: 0.0003555336152203381\n",
            "step: 80, loss: 0.01965332217514515\n",
            "step: 90, loss: 0.00018340650422032923\n",
            "step: 100, loss: 0.004271746147423983\n",
            "step: 110, loss: 0.03439374640583992\n",
            "step: 120, loss: 0.022239815443754196\n",
            "step: 130, loss: 0.00035815531737171113\n",
            "step: 140, loss: 0.0008803320815786719\n",
            "step: 150, loss: 0.0013662419514730573\n",
            "step: 160, loss: 0.0024828920140862465\n",
            "step: 170, loss: 0.0003286396968178451\n",
            "step: 180, loss: 0.0003070829261559993\n",
            "step: 190, loss: 0.0026877657510340214\n",
            "step: 200, loss: 0.0024812619667500257\n",
            "step: 210, loss: 0.0005910171894356608\n",
            "step: 220, loss: 0.0026797230821102858\n",
            "step: 230, loss: 0.0002254528080811724\n",
            "step: 240, loss: 0.011726761236786842\n",
            "step: 250, loss: 0.004729270935058594\n",
            "step: 260, loss: 9.404408774571493e-05\n",
            "step: 270, loss: 0.11738213151693344\n",
            "step: 280, loss: 0.0025060181505978107\n",
            "step: 290, loss: 0.0010880957124754786\n",
            "step: 300, loss: 0.000519168097525835\n",
            "step: 310, loss: 0.00029881164664402604\n",
            "step: 320, loss: 0.0002959598496090621\n",
            "step: 330, loss: 0.00013450307596940547\n",
            "step: 340, loss: 0.03729816898703575\n",
            "step: 350, loss: 0.0026700361631810665\n",
            "step: 360, loss: 0.027725275605916977\n",
            "step: 370, loss: 0.0008200588054023683\n",
            "step: 380, loss: 7.75385633460246e-05\n",
            "step: 390, loss: 0.1773785948753357\n",
            "step: 400, loss: 0.049010019749403\n",
            "step: 410, loss: 0.0003553417627699673\n",
            "step: 420, loss: 0.0028727829921990633\n",
            "step: 430, loss: 0.0005701988702639937\n",
            "step: 440, loss: 0.00040749588515609503\n",
            "step: 450, loss: 0.0017815523315221071\n",
            "step: 460, loss: 0.00018915708642452955\n",
            "step: 470, loss: 0.001427693059667945\n",
            "step: 480, loss: 0.0005052773631177843\n",
            "step: 490, loss: 0.005355267319828272\n",
            "step: 500, loss: 0.004725165199488401\n",
            "step: 510, loss: 0.0037835370749235153\n",
            "step: 520, loss: 0.015028182417154312\n",
            "step: 530, loss: 0.005576055962592363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9325789721829325, f1=0.9163533834586467, best_f1=0.9163533834586467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014017593348398805\n",
            "step: 10, loss: 0.0009875638643279672\n",
            "step: 20, loss: 0.00015379091200884432\n",
            "step: 30, loss: 0.013843104243278503\n",
            "step: 40, loss: 6.430042412830517e-05\n",
            "step: 50, loss: 0.001847312436439097\n",
            "step: 60, loss: 0.0013850578106939793\n",
            "step: 70, loss: 6.979506724746898e-05\n",
            "step: 80, loss: 0.0012918765423819423\n",
            "step: 90, loss: 9.303769184043631e-05\n",
            "step: 100, loss: 0.004116305150091648\n",
            "step: 110, loss: 0.02110953815281391\n",
            "step: 120, loss: 0.002181330928578973\n",
            "step: 130, loss: 0.0008316265884786844\n",
            "step: 140, loss: 4.90560159960296e-05\n",
            "step: 150, loss: 0.00013529883290175349\n",
            "step: 160, loss: 0.00013500881323125213\n",
            "step: 170, loss: 0.00013305128959473222\n",
            "step: 180, loss: 0.00011801895016105846\n",
            "step: 190, loss: 0.0006034215330146253\n",
            "step: 200, loss: 0.016331614926457405\n",
            "step: 210, loss: 0.006287342868745327\n",
            "step: 220, loss: 9.591260459274054e-05\n",
            "step: 230, loss: 0.0007051219581626356\n",
            "step: 240, loss: 0.00023784601944498718\n",
            "step: 250, loss: 0.004091073293238878\n",
            "step: 260, loss: 0.0064830174669623375\n",
            "step: 270, loss: 0.0015235681785270572\n",
            "step: 280, loss: 0.0009280066005885601\n",
            "step: 290, loss: 0.00012387344031594694\n",
            "step: 300, loss: 0.02644573152065277\n",
            "step: 310, loss: 0.00040870060911402106\n",
            "step: 320, loss: 0.0016001766780391335\n",
            "step: 330, loss: 0.0011514101643115282\n",
            "step: 340, loss: 0.021427685394883156\n",
            "step: 350, loss: 0.06835515797138214\n",
            "step: 360, loss: 0.008316755294799805\n",
            "step: 370, loss: 0.0013797470601275563\n",
            "step: 380, loss: 0.0007066255784593523\n",
            "step: 390, loss: 0.00020489796588663012\n",
            "step: 400, loss: 0.00043161294888705015\n",
            "step: 410, loss: 0.0027975074481219053\n",
            "step: 420, loss: 0.000234353487030603\n",
            "step: 430, loss: 0.010694285854697227\n",
            "step: 440, loss: 0.0014771883143112063\n",
            "step: 450, loss: 0.0006938835140317678\n",
            "step: 460, loss: 0.0011100288247689605\n",
            "step: 470, loss: 0.0010099916253238916\n",
            "step: 480, loss: 0.10705417394638062\n",
            "step: 490, loss: 0.00033843907294794917\n",
            "step: 500, loss: 0.001751324743963778\n",
            "step: 510, loss: 0.0004784624034073204\n",
            "step: 520, loss: 0.0028669950552284718\n",
            "step: 530, loss: 0.06612148880958557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9355900329102022, f1=0.9276315789473684, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017169548198580742\n",
            "step: 10, loss: 0.07758825272321701\n",
            "step: 20, loss: 0.015490395948290825\n",
            "step: 30, loss: 0.001036180998198688\n",
            "step: 40, loss: 0.00010359942825743929\n",
            "step: 50, loss: 0.0078091928735375404\n",
            "step: 60, loss: 0.0034609397407621145\n",
            "step: 70, loss: 0.0035546543076634407\n",
            "step: 80, loss: 0.0003837542317342013\n",
            "step: 90, loss: 0.0002869471500162035\n",
            "step: 100, loss: 5.13641134602949e-05\n",
            "step: 110, loss: 9.581098129274324e-05\n",
            "step: 120, loss: 7.942014781292528e-05\n",
            "step: 130, loss: 8.752122084842995e-05\n",
            "step: 140, loss: 0.0002638769510667771\n",
            "step: 150, loss: 7.355814886977896e-05\n",
            "step: 160, loss: 0.00027201493503525853\n",
            "step: 170, loss: 0.0031046187505126\n",
            "step: 180, loss: 3.543757338775322e-05\n",
            "step: 190, loss: 0.0007692381623201072\n",
            "step: 200, loss: 0.0011586080072447658\n",
            "step: 210, loss: 0.0003060781746171415\n",
            "step: 220, loss: 0.0013601260725408792\n",
            "step: 230, loss: 4.3335752707207575e-05\n",
            "step: 240, loss: 4.004157744930126e-05\n",
            "step: 250, loss: 0.0010718742851167917\n",
            "step: 260, loss: 0.00015127180085983127\n",
            "step: 270, loss: 0.0003971937985625118\n",
            "step: 280, loss: 0.018752234056591988\n",
            "step: 290, loss: 0.0008885873248800635\n",
            "step: 300, loss: 8.209356019506231e-05\n",
            "step: 310, loss: 0.0002597779966890812\n",
            "step: 320, loss: 0.06893672794103622\n",
            "step: 330, loss: 0.0004869972472079098\n",
            "step: 340, loss: 0.0002921413688454777\n",
            "step: 350, loss: 0.0014933559577912092\n",
            "step: 360, loss: 0.029668506234884262\n",
            "step: 370, loss: 0.0090717738494277\n",
            "step: 380, loss: 0.0029295661952346563\n",
            "step: 390, loss: 0.13116835057735443\n",
            "step: 400, loss: 0.0009866226464509964\n",
            "step: 410, loss: 0.00476897181943059\n",
            "step: 420, loss: 0.0028756738174706697\n",
            "step: 430, loss: 0.023632198572158813\n",
            "step: 440, loss: 0.009295489639043808\n",
            "step: 450, loss: 0.000703193771187216\n",
            "step: 460, loss: 0.0004679822304751724\n",
            "step: 470, loss: 0.0003602505603339523\n",
            "step: 480, loss: 0.00313800061121583\n",
            "step: 490, loss: 0.0005164105095900595\n",
            "step: 500, loss: 0.0004383153573144227\n",
            "step: 510, loss: 0.01997150480747223\n",
            "step: 520, loss: 0.002779472852125764\n",
            "step: 530, loss: 0.0009291535243391991\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9322892676186089, f1=0.9256198347107438, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009005204774439335\n",
            "step: 10, loss: 9.1419045929797e-05\n",
            "step: 20, loss: 0.0010289368219673634\n",
            "step: 30, loss: 0.0005577282281592488\n",
            "step: 40, loss: 0.0005165900220163167\n",
            "step: 50, loss: 0.0005334774032235146\n",
            "step: 60, loss: 8.144017192535102e-05\n",
            "step: 70, loss: 6.534229760291055e-05\n",
            "step: 80, loss: 0.0037808988709002733\n",
            "step: 90, loss: 0.00013302791921887547\n",
            "step: 100, loss: 4.8399775550933555e-05\n",
            "step: 110, loss: 0.00014693886623717844\n",
            "step: 120, loss: 0.0002597273560240865\n",
            "step: 130, loss: 0.003588299499824643\n",
            "step: 140, loss: 0.0006178840412758291\n",
            "step: 150, loss: 9.660929208621383e-05\n",
            "step: 160, loss: 0.0006614931626245379\n",
            "step: 170, loss: 0.012596135027706623\n",
            "step: 180, loss: 0.00034493140992708504\n",
            "step: 190, loss: 5.82555185246747e-05\n",
            "step: 200, loss: 0.00011138707486679778\n",
            "step: 210, loss: 0.00013515220780391246\n",
            "step: 220, loss: 0.006519306916743517\n",
            "step: 230, loss: 4.0060069295577705e-05\n",
            "step: 240, loss: 6.980508624110371e-05\n",
            "step: 250, loss: 0.0006777657545171678\n",
            "step: 260, loss: 0.0002510215272195637\n",
            "step: 270, loss: 3.6533292586682364e-05\n",
            "step: 280, loss: 7.619534153491259e-05\n",
            "step: 290, loss: 0.07094468921422958\n",
            "step: 300, loss: 6.703515828121454e-05\n",
            "step: 310, loss: 0.002071283059194684\n",
            "step: 320, loss: 0.0002521497954148799\n",
            "step: 330, loss: 0.0006217581103555858\n",
            "step: 340, loss: 0.002944893203675747\n",
            "step: 350, loss: 0.00016839808085933328\n",
            "step: 360, loss: 0.0022739616688340902\n",
            "step: 370, loss: 0.006566685624420643\n",
            "step: 380, loss: 0.0005842982791364193\n",
            "step: 390, loss: 0.00037295836955308914\n",
            "step: 400, loss: 0.00010332548117730767\n",
            "step: 410, loss: 2.384881918260362e-05\n",
            "step: 420, loss: 3.06059155263938e-05\n",
            "step: 430, loss: 2.8250760806258768e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 440, loss: 0.037789005786180496\n",
            "step: 450, loss: 5.444569978863001e-05\n",
            "step: 460, loss: 0.0019329945789650083\n",
            "step: 470, loss: 2.9476943382178433e-05\n",
            "step: 480, loss: 3.093729173997417e-05\n",
            "step: 490, loss: 0.0012321891263127327\n",
            "step: 500, loss: 0.00033979411819018424\n",
            "step: 510, loss: 0.05145140737295151\n",
            "step: 520, loss: 0.0001224612060468644\n",
            "step: 530, loss: 4.8664267524145544e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9344490934449092, f1=0.9229340761374188, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001497392076998949\n",
            "step: 10, loss: 4.625748988473788e-05\n",
            "step: 20, loss: 4.461314529180527e-05\n",
            "step: 30, loss: 0.0012541132746264338\n",
            "step: 40, loss: 0.0001417471212334931\n",
            "step: 50, loss: 6.785122968722135e-05\n",
            "step: 60, loss: 0.0009081550524570048\n",
            "step: 70, loss: 7.220457337098196e-05\n",
            "step: 80, loss: 4.829062891076319e-05\n",
            "step: 90, loss: 0.033630553632974625\n",
            "step: 100, loss: 0.00012416404206305742\n",
            "step: 110, loss: 5.946851160842925e-05\n",
            "step: 120, loss: 0.0003380376729182899\n",
            "step: 130, loss: 6.114449934102595e-05\n",
            "step: 140, loss: 0.007971656508743763\n",
            "step: 150, loss: 7.46168807381764e-05\n",
            "step: 160, loss: 5.830423106090166e-05\n",
            "step: 170, loss: 8.107630856102332e-05\n",
            "step: 180, loss: 0.00014402194938156754\n",
            "step: 190, loss: 0.00197012210264802\n",
            "step: 200, loss: 0.020723868161439896\n",
            "step: 210, loss: 0.00010227707389276475\n",
            "step: 220, loss: 0.00043638679198920727\n",
            "step: 230, loss: 0.002891377080231905\n",
            "step: 240, loss: 0.0001657611137488857\n",
            "step: 250, loss: 8.540226554032415e-05\n",
            "step: 260, loss: 0.0017233610851690173\n",
            "step: 270, loss: 0.001260785385966301\n",
            "step: 280, loss: 0.00201358157210052\n",
            "step: 290, loss: 5.1293009164510295e-05\n",
            "step: 300, loss: 2.6038695068564266e-05\n",
            "step: 310, loss: 6.695985211990774e-05\n",
            "step: 320, loss: 0.0019887685775756836\n",
            "step: 330, loss: 0.02897321805357933\n",
            "step: 340, loss: 0.0009510593372397125\n",
            "step: 350, loss: 0.008703375235199928\n",
            "step: 360, loss: 0.00021042714070063084\n",
            "step: 370, loss: 0.0012009673519060016\n",
            "step: 380, loss: 0.02265334129333496\n",
            "step: 390, loss: 0.01635158248245716\n",
            "step: 400, loss: 0.0019270532066002488\n",
            "step: 410, loss: 0.0013415950816124678\n",
            "step: 420, loss: 0.0003838356351479888\n",
            "step: 430, loss: 0.00011974716471740976\n",
            "step: 440, loss: 0.00045630341628566384\n",
            "step: 450, loss: 0.00030761692323721945\n",
            "step: 460, loss: 0.0007688611513003707\n",
            "step: 470, loss: 0.0002251885598525405\n",
            "step: 480, loss: 4.405945219332352e-05\n",
            "step: 490, loss: 0.00013110156578477472\n",
            "step: 500, loss: 0.00010617743100738153\n",
            "step: 510, loss: 2.8259346436243504e-05\n",
            "step: 520, loss: 4.9564889195607975e-05\n",
            "step: 530, loss: 0.0021473721135407686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9330783938814532, f1=0.9238820171265462, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001189882168546319\n",
            "step: 10, loss: 0.00017495284555479884\n",
            "step: 20, loss: 5.342162330634892e-05\n",
            "step: 30, loss: 0.0001489101559855044\n",
            "step: 40, loss: 0.0005786457913927734\n",
            "step: 50, loss: 5.485421934281476e-05\n",
            "step: 60, loss: 0.00026989573962055147\n",
            "step: 70, loss: 3.956340151489712e-05\n",
            "step: 80, loss: 3.509642192511819e-05\n",
            "step: 90, loss: 4.718109266832471e-05\n",
            "step: 100, loss: 0.00045152020175009966\n",
            "step: 110, loss: 0.0005811608862131834\n",
            "step: 120, loss: 2.5908298994181678e-05\n",
            "step: 130, loss: 2.9866492695873603e-05\n",
            "step: 140, loss: 2.7427242457633838e-05\n",
            "step: 150, loss: 2.4049119019764476e-05\n",
            "step: 160, loss: 9.998201858252287e-05\n",
            "step: 170, loss: 3.3622847695369273e-05\n",
            "step: 180, loss: 0.0002522044233046472\n",
            "step: 190, loss: 0.00030286627588793635\n",
            "step: 200, loss: 4.639502731151879e-05\n",
            "step: 210, loss: 0.0002724077785387635\n",
            "step: 220, loss: 0.00010435856529511511\n",
            "step: 230, loss: 1.7083835700759664e-05\n",
            "step: 240, loss: 0.00016867340309545398\n",
            "step: 250, loss: 4.941126826452091e-05\n",
            "step: 260, loss: 8.130529022309929e-05\n",
            "step: 270, loss: 0.00012771868205163628\n",
            "step: 280, loss: 0.00015172007260844111\n",
            "step: 290, loss: 0.0007631753105670214\n",
            "step: 300, loss: 4.279134373064153e-05\n",
            "step: 310, loss: 0.05873999744653702\n",
            "step: 320, loss: 3.977911183028482e-05\n",
            "step: 330, loss: 0.0001933604944497347\n",
            "step: 340, loss: 6.665341788902879e-05\n",
            "step: 350, loss: 4.796592838829383e-05\n",
            "step: 360, loss: 0.00011937168892472982\n",
            "step: 370, loss: 0.0024962674360722303\n",
            "step: 380, loss: 0.00038275550468824804\n",
            "step: 390, loss: 0.0013989057624712586\n",
            "step: 400, loss: 0.001894546439871192\n",
            "step: 410, loss: 2.59309854300227e-05\n",
            "step: 420, loss: 0.0007097817142494023\n",
            "step: 430, loss: 9.290764864999801e-05\n",
            "step: 440, loss: 6.309049786068499e-05\n",
            "step: 450, loss: 5.073110878583975e-05\n",
            "step: 460, loss: 0.0036855945363640785\n",
            "step: 470, loss: 8.953203359851614e-05\n",
            "step: 480, loss: 0.0007602816913276911\n",
            "step: 490, loss: 0.00020592061628121883\n",
            "step: 500, loss: 8.990702190203592e-05\n",
            "step: 510, loss: 3.982999623985961e-05\n",
            "step: 520, loss: 4.055452154716477e-05\n",
            "step: 530, loss: 0.00012262458039913327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9300766283524904, f1=0.9276879162702188, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.322453994769603e-05\n",
            "step: 10, loss: 1.8473372620064765e-05\n",
            "step: 20, loss: 7.900261698523536e-05\n",
            "step: 30, loss: 3.083533374592662e-05\n",
            "step: 40, loss: 0.00019776266708504409\n",
            "step: 50, loss: 0.03778001666069031\n",
            "step: 60, loss: 0.003138007363304496\n",
            "step: 70, loss: 7.850486144889146e-05\n",
            "step: 80, loss: 2.2701242414768785e-05\n",
            "step: 90, loss: 0.00022913914290256798\n",
            "step: 100, loss: 7.194839417934418e-05\n",
            "step: 110, loss: 0.001064725685864687\n",
            "step: 120, loss: 9.561904153088108e-05\n",
            "step: 130, loss: 3.317788286949508e-05\n",
            "step: 140, loss: 7.131408347049728e-05\n",
            "step: 150, loss: 6.974224379519e-05\n",
            "step: 160, loss: 2.290226075274404e-05\n",
            "step: 170, loss: 4.236872337060049e-05\n",
            "step: 180, loss: 2.1289535652613267e-05\n",
            "step: 190, loss: 4.86220778839197e-05\n",
            "step: 200, loss: 0.00010820550232892856\n",
            "step: 210, loss: 0.00048012894694693387\n",
            "step: 220, loss: 0.00031280022813007236\n",
            "step: 230, loss: 8.315958257298917e-05\n",
            "step: 240, loss: 6.146874511614442e-05\n",
            "step: 250, loss: 2.1878109691897407e-05\n",
            "step: 260, loss: 3.05908324662596e-05\n",
            "step: 270, loss: 0.000125671605928801\n",
            "step: 280, loss: 0.001516978838481009\n",
            "step: 290, loss: 7.100622315192595e-05\n",
            "step: 300, loss: 0.0009961843024939299\n",
            "step: 310, loss: 2.974529706989415e-05\n",
            "step: 320, loss: 1.9989411157439463e-05\n",
            "step: 330, loss: 3.060805829591118e-05\n",
            "step: 340, loss: 3.1715073419036344e-05\n",
            "step: 350, loss: 0.00013499293709173799\n",
            "step: 360, loss: 0.0014490938046947122\n",
            "step: 370, loss: 4.9644408136373386e-05\n",
            "step: 380, loss: 6.469350773841143e-05\n",
            "step: 390, loss: 0.0008260277681984007\n",
            "step: 400, loss: 0.0003885921323671937\n",
            "step: 410, loss: 0.0006077078869566321\n",
            "step: 420, loss: 0.021701185032725334\n",
            "step: 430, loss: 0.00038491966552101076\n",
            "step: 440, loss: 0.012942506931722164\n",
            "step: 450, loss: 3.945567732444033e-05\n",
            "step: 460, loss: 1.7735825167619623e-05\n",
            "step: 470, loss: 2.0059820599271916e-05\n",
            "step: 480, loss: 5.2592607971746475e-05\n",
            "step: 490, loss: 0.0007658975082449615\n",
            "step: 500, loss: 3.11202384182252e-05\n",
            "step: 510, loss: 0.004196592606604099\n",
            "step: 520, loss: 6.894451507832855e-05\n",
            "step: 530, loss: 0.0027513799723237753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9308235294117647, f1=0.9262564584311883, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022411728277802467\n",
            "step: 10, loss: 3.510661917971447e-05\n",
            "step: 20, loss: 1.561616409162525e-05\n",
            "step: 30, loss: 7.006915984675288e-05\n",
            "step: 40, loss: 1.9672734197229147e-05\n",
            "step: 50, loss: 0.00038881489308550954\n",
            "step: 60, loss: 2.0305844373069704e-05\n",
            "step: 70, loss: 2.491354098310694e-05\n",
            "step: 80, loss: 2.6515015633776784e-05\n",
            "step: 90, loss: 5.3489504352910444e-05\n",
            "step: 100, loss: 2.066740853479132e-05\n",
            "step: 110, loss: 0.00014891085447743535\n",
            "step: 120, loss: 1.9322593288961798e-05\n",
            "step: 130, loss: 1.9046223314944655e-05\n",
            "step: 140, loss: 4.1837847675196826e-05\n",
            "step: 150, loss: 0.0012252581072971225\n",
            "step: 160, loss: 2.0104987925151363e-05\n",
            "step: 170, loss: 0.0005035370122641325\n",
            "step: 180, loss: 2.9525366699090227e-05\n",
            "step: 190, loss: 1.8648508557816967e-05\n",
            "step: 200, loss: 1.942702874657698e-05\n",
            "step: 210, loss: 3.922478572349064e-05\n",
            "step: 220, loss: 2.860709537344519e-05\n",
            "step: 230, loss: 2.2462279957835563e-05\n",
            "step: 240, loss: 1.9602146494435146e-05\n",
            "step: 250, loss: 1.9218206944060512e-05\n",
            "step: 260, loss: 1.9516388420015574e-05\n",
            "step: 270, loss: 1.3597163160739001e-05\n",
            "step: 280, loss: 3.694225233630277e-05\n",
            "step: 290, loss: 0.0001234568771906197\n",
            "step: 300, loss: 0.0003701976966112852\n",
            "step: 310, loss: 0.0015290880110114813\n",
            "step: 320, loss: 0.0024170358665287495\n",
            "step: 330, loss: 0.0026485221460461617\n",
            "step: 340, loss: 0.0003120205656159669\n",
            "step: 350, loss: 2.2477839593193494e-05\n",
            "step: 360, loss: 3.531111724441871e-05\n",
            "step: 370, loss: 0.01607969030737877\n",
            "step: 380, loss: 6.243855023058131e-05\n",
            "step: 390, loss: 0.00048432202311232686\n",
            "step: 400, loss: 1.683429400145542e-05\n",
            "step: 410, loss: 0.0005291075794957578\n",
            "step: 420, loss: 1.7843718524090946e-05\n",
            "step: 430, loss: 0.015256499871611595\n",
            "step: 440, loss: 2.3233875253936276e-05\n",
            "step: 450, loss: 1.829454231483396e-05\n",
            "step: 460, loss: 0.0002767536207102239\n",
            "step: 470, loss: 1.6923515431699343e-05\n",
            "step: 480, loss: 2.61831701209303e-05\n",
            "step: 490, loss: 4.302266097511165e-05\n",
            "step: 500, loss: 2.815435072989203e-05\n",
            "step: 510, loss: 2.2049371182220057e-05\n",
            "step: 520, loss: 0.001436396618373692\n",
            "step: 530, loss: 2.478918577253353e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9326968973747015, f1=0.922491678554446, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012039791326969862\n",
            "step: 10, loss: 1.104912007576786e-05\n",
            "step: 20, loss: 1.3779665096080862e-05\n",
            "step: 30, loss: 2.16023745451821e-05\n",
            "step: 40, loss: 3.7249621527735144e-05\n",
            "step: 50, loss: 0.0008209483348764479\n",
            "step: 60, loss: 3.175389065290801e-05\n",
            "step: 70, loss: 1.5083448488439899e-05\n",
            "step: 80, loss: 2.7286318072583526e-05\n",
            "step: 90, loss: 1.677470572758466e-05\n",
            "step: 100, loss: 2.894021599786356e-05\n",
            "step: 110, loss: 1.8529059161664918e-05\n",
            "step: 120, loss: 0.0019814774859696627\n",
            "step: 130, loss: 0.02735268883407116\n",
            "step: 140, loss: 0.00022560245997738093\n",
            "step: 150, loss: 1.195804634335218e-05\n",
            "step: 160, loss: 0.0002937245008070022\n",
            "step: 170, loss: 1.5336832802859135e-05\n",
            "step: 180, loss: 0.001262427307665348\n",
            "step: 190, loss: 1.7988921172218397e-05\n",
            "step: 200, loss: 4.332132812123746e-05\n",
            "step: 210, loss: 2.277193561894819e-05\n",
            "step: 220, loss: 1.7754420696292073e-05\n",
            "step: 230, loss: 4.295568214729428e-05\n",
            "step: 240, loss: 1.4200620171322953e-05\n",
            "step: 250, loss: 6.138619937701151e-05\n",
            "step: 260, loss: 1.9665047148009762e-05\n",
            "step: 270, loss: 1.6182302715606056e-05\n",
            "step: 280, loss: 1.7426620615879074e-05\n",
            "step: 290, loss: 0.00015904277097433805\n",
            "step: 300, loss: 2.4008893888094462e-05\n",
            "step: 310, loss: 0.0003131819248665124\n",
            "step: 320, loss: 0.0001163632477982901\n",
            "step: 330, loss: 0.0002067883760901168\n",
            "step: 340, loss: 0.00016816264542285353\n",
            "step: 350, loss: 3.1115301680983976e-05\n",
            "step: 360, loss: 0.00026693131076171994\n",
            "step: 370, loss: 5.6461918575223535e-05\n",
            "step: 380, loss: 1.7434018445783295e-05\n",
            "step: 390, loss: 0.004918873310089111\n",
            "step: 400, loss: 9.244710236089304e-05\n",
            "step: 410, loss: 1.2133141353842802e-05\n",
            "step: 420, loss: 5.853338007000275e-05\n",
            "step: 430, loss: 2.4396331355092116e-05\n",
            "step: 440, loss: 0.00011292719136690721\n",
            "step: 450, loss: 6.075943019823171e-05\n",
            "step: 460, loss: 5.1717775932047516e-05\n",
            "step: 470, loss: 0.0024539134465157986\n",
            "step: 480, loss: 1.490834529249696e-05\n",
            "step: 490, loss: 1.99596252059564e-05\n",
            "step: 500, loss: 1.5206322132144123e-05\n",
            "step: 510, loss: 0.00013787965872325003\n",
            "step: 520, loss: 5.2641044021584094e-05\n",
            "step: 530, loss: 0.0001549704757053405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9344648750589346, f1=0.9283372365339578, best_f1=0.9276315789473684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.4964284673624206e-05\n",
            "step: 10, loss: 2.5821822418947704e-05\n",
            "step: 20, loss: 4.0039631130639464e-05\n",
            "step: 30, loss: 0.0005254324642010033\n",
            "step: 40, loss: 0.04759123548865318\n",
            "step: 50, loss: 0.0008022992988117039\n",
            "step: 60, loss: 1.420050102751702e-05\n",
            "step: 70, loss: 0.0013599920785054564\n",
            "step: 80, loss: 5.173362296773121e-05\n",
            "step: 90, loss: 2.4965976990642957e-05\n",
            "step: 100, loss: 5.21525253134314e-05\n",
            "step: 110, loss: 5.952811261522584e-05\n",
            "step: 120, loss: 2.1803283743793145e-05\n",
            "step: 130, loss: 0.09006313234567642\n",
            "step: 140, loss: 2.1952215320197865e-05\n",
            "step: 150, loss: 3.1998486520024016e-05\n",
            "step: 160, loss: 2.398557808191981e-05\n",
            "step: 170, loss: 1.976926068891771e-05\n",
            "step: 180, loss: 1.1265178727626335e-05\n",
            "step: 190, loss: 3.863612437271513e-05\n",
            "step: 200, loss: 1.1704735698003788e-05\n",
            "step: 210, loss: 3.552378620952368e-05\n",
            "step: 220, loss: 1.4889747944835108e-05\n",
            "step: 230, loss: 0.00046544079668819904\n",
            "step: 240, loss: 2.074535223073326e-05\n",
            "step: 250, loss: 2.4693879822734743e-05\n",
            "step: 260, loss: 2.606018097139895e-05\n",
            "step: 270, loss: 1.8324386473977938e-05\n",
            "step: 280, loss: 1.606693331268616e-05\n",
            "step: 290, loss: 1.2472133676055819e-05\n",
            "step: 300, loss: 1.8614804503158666e-05\n",
            "step: 310, loss: 2.5964083761209622e-05\n",
            "step: 320, loss: 4.232693027006462e-05\n",
            "step: 330, loss: 2.484245487721637e-05\n",
            "step: 340, loss: 1.4718356396770105e-05\n",
            "step: 350, loss: 0.0002504390140529722\n",
            "step: 360, loss: 0.00014254823327064514\n",
            "step: 370, loss: 1.2632322977879085e-05\n",
            "step: 380, loss: 1.926302138599567e-05\n",
            "step: 390, loss: 2.3110957044991665e-05\n",
            "step: 400, loss: 2.0994955775677226e-05\n",
            "step: 410, loss: 1.7720785763231106e-05\n",
            "step: 420, loss: 2.493167448847089e-05\n",
            "step: 430, loss: 0.000182508651050739\n",
            "step: 440, loss: 4.1643634176580235e-05\n",
            "step: 450, loss: 1.38280593091622e-05\n",
            "step: 460, loss: 1.810451976780314e-05\n",
            "step: 470, loss: 0.0010577781358733773\n",
            "step: 480, loss: 1.3015953300055116e-05\n",
            "step: 490, loss: 1.3924961422162596e-05\n",
            "step: 500, loss: 3.21768966387026e-05\n",
            "step: 510, loss: 1.6837791918078437e-05\n",
            "step: 520, loss: 1.4666272363683674e-05\n",
            "step: 530, loss: 2.682047306734603e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9339622641509434, f1=0.926829268292683, best_f1=0.9276315789473684\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 240.32it/s]\n",
            "load_f1 = 0.9325789721829325\n",
            "real_f1 = 0.9355900329102022\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 245.15it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10svv34hgw7-"
      },
      "source": [
        "# DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9478f6c-e3b2-4fad-f144-428f29dcb4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=e046e4df233f8960c028ab11bb3345603c8a49e823de4a5ba35c65b205d8d8c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4p1_6jtf/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdNk8ikFgw7-"
      },
      "source": [
        "## DK STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ded277-03c6-49e4-8152-68a4c21482f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5716682076454163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5161290322580646, f1=0.11111111111111112, best_f1=0.11111111111111112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5010576248168945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5185185185185186, f1=0.23529411764705882, best_f1=0.23529411764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5724155902862549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.4444444444444444, f1=0.358974358974359, best_f1=0.23529411764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2615358829498291\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.43750000000000006, f1=0.42857142857142855, best_f1=0.23529411764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2551887035369873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5142857142857143, f1=0.4666666666666667, best_f1=0.23529411764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20130787789821625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.5925925925925927, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.060293518006801605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6486486486486486, f1=0.5555555555555556, best_f1=0.5555555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030216852203011513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6451612903225806, f1=0.45161290322580644, best_f1=0.5555555555555556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002656693570315838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.6875000000000001, f1=0.48484848484848486, best_f1=0.48484848484848486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01093687117099762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7272727272727273, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005301863886415958\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7272727272727273, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03214917331933975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7272727272727273, f1=0.5454545454545454, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034033029805868864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7272727272727273, f1=0.5454545454545454, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003788839792832732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7272727272727273, f1=0.5454545454545454, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011915256269276142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7272727272727273, f1=0.5454545454545454, best_f1=0.5\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 144521.64it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5384615384615384\n",
            "real_f1 = 0.6451612903225806\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 255.37it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c6de62-5b0c-47e1-e26a-4ae43eb956e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6352372169494629\n",
            "step: 10, loss: 0.6169479489326477\n",
            "step: 20, loss: 0.36182382702827454\n",
            "step: 30, loss: 0.15115930140018463\n",
            "step: 40, loss: 0.12240631878376007\n",
            "step: 50, loss: 0.07196946442127228\n",
            "step: 60, loss: 0.05012175813317299\n",
            "step: 70, loss: 0.22534649074077606\n",
            "step: 80, loss: 0.13424672186374664\n",
            "step: 90, loss: 0.19925104081630707\n",
            "step: 100, loss: 0.01424767542630434\n",
            "step: 110, loss: 0.24403515458106995\n",
            "step: 120, loss: 0.010288301855325699\n",
            "step: 130, loss: 0.031160131096839905\n",
            "step: 140, loss: 0.03336016833782196\n",
            "step: 150, loss: 0.12179088592529297\n",
            "step: 160, loss: 0.0076835970394313335\n",
            "step: 170, loss: 0.03510254621505737\n",
            "step: 180, loss: 0.004093771800398827\n",
            "step: 190, loss: 0.10766976326704025\n",
            "step: 200, loss: 0.013249983079731464\n",
            "step: 210, loss: 0.006447063758969307\n",
            "step: 220, loss: 0.0849737599492073\n",
            "step: 230, loss: 0.07525467872619629\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.967305524239008, f1=0.967305524239008, best_f1=0.967305524239008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002472179476171732\n",
            "step: 10, loss: 0.001087211654521525\n",
            "step: 20, loss: 0.1137421503663063\n",
            "step: 30, loss: 0.18779346346855164\n",
            "step: 40, loss: 0.003249637084081769\n",
            "step: 50, loss: 0.012441843748092651\n",
            "step: 60, loss: 0.008495357818901539\n",
            "step: 70, loss: 0.1706238090991974\n",
            "step: 80, loss: 0.00643307575955987\n",
            "step: 90, loss: 0.10310836136341095\n",
            "step: 100, loss: 0.011933065950870514\n",
            "step: 110, loss: 0.0928555577993393\n",
            "step: 120, loss: 0.04013686999678612\n",
            "step: 130, loss: 0.018045665696263313\n",
            "step: 140, loss: 0.007893882691860199\n",
            "step: 150, loss: 0.00495684240013361\n",
            "step: 160, loss: 0.003657163120806217\n",
            "step: 170, loss: 0.0012127677910029888\n",
            "step: 180, loss: 0.005460716784000397\n",
            "step: 190, loss: 0.03307904675602913\n",
            "step: 200, loss: 0.006242942530661821\n",
            "step: 210, loss: 0.002807917771860957\n",
            "step: 220, loss: 0.13385145366191864\n",
            "step: 230, loss: 0.1124572604894638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9832402234636871, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008570033125579357\n",
            "step: 10, loss: 0.0017362703802064061\n",
            "step: 20, loss: 0.000939495861530304\n",
            "step: 30, loss: 0.08323179185390472\n",
            "step: 40, loss: 0.034239623695611954\n",
            "step: 50, loss: 0.028031030669808388\n",
            "step: 60, loss: 0.009038531221449375\n",
            "step: 70, loss: 0.001448109745979309\n",
            "step: 80, loss: 0.001309614279307425\n",
            "step: 90, loss: 0.12298125773668289\n",
            "step: 100, loss: 0.0013532927259802818\n",
            "step: 110, loss: 0.0013619399396702647\n",
            "step: 120, loss: 0.0993272215127945\n",
            "step: 130, loss: 0.008354204706847668\n",
            "step: 140, loss: 0.050892896950244904\n",
            "step: 150, loss: 0.004152289126068354\n",
            "step: 160, loss: 0.02196482941508293\n",
            "step: 170, loss: 0.0017910655587911606\n",
            "step: 180, loss: 0.06807699054479599\n",
            "step: 190, loss: 0.006276256870478392\n",
            "step: 200, loss: 0.05393047630786896\n",
            "step: 210, loss: 0.011733675375580788\n",
            "step: 220, loss: 0.0042769573628902435\n",
            "step: 230, loss: 0.001960013760253787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9854423292273236, f1=0.9798206278026906, best_f1=0.9798206278026906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002037501661106944\n",
            "step: 10, loss: 0.00047950580483302474\n",
            "step: 20, loss: 0.003242599545046687\n",
            "step: 30, loss: 0.00041731807868927717\n",
            "step: 40, loss: 0.0011025354033336043\n",
            "step: 50, loss: 0.0071109444834291935\n",
            "step: 60, loss: 0.024865737184882164\n",
            "step: 70, loss: 0.0007200537947937846\n",
            "step: 80, loss: 0.0007093913736753166\n",
            "step: 90, loss: 0.0012817949755117297\n",
            "step: 100, loss: 0.008720716461539268\n",
            "step: 110, loss: 0.002647141693159938\n",
            "step: 120, loss: 0.019171392545104027\n",
            "step: 130, loss: 0.003346159355714917\n",
            "step: 140, loss: 0.0006784514407627285\n",
            "step: 150, loss: 0.0821567177772522\n",
            "step: 160, loss: 0.002141886856406927\n",
            "step: 170, loss: 0.0026957050431519747\n",
            "step: 180, loss: 0.00038086308632045984\n",
            "step: 190, loss: 0.0010535012697800994\n",
            "step: 200, loss: 0.012231686152517796\n",
            "step: 210, loss: 0.005476963706314564\n",
            "step: 220, loss: 0.0004296615079510957\n",
            "step: 230, loss: 0.001398538937792182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9784335981838819, f1=0.9759999999999999, best_f1=0.9798206278026906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001085482770577073\n",
            "step: 10, loss: 0.00047294949763454497\n",
            "step: 20, loss: 0.16412848234176636\n",
            "step: 30, loss: 0.0021191698033362627\n",
            "step: 40, loss: 0.000815067847725004\n",
            "step: 50, loss: 0.0014964619185775518\n",
            "step: 60, loss: 0.004082332830876112\n",
            "step: 70, loss: 0.0011396079789847136\n",
            "step: 80, loss: 0.013534639962017536\n",
            "step: 90, loss: 0.0011682870099321008\n",
            "step: 100, loss: 0.0002850319433491677\n",
            "step: 110, loss: 0.00023929977032821625\n",
            "step: 120, loss: 0.00010252089123241603\n",
            "step: 130, loss: 0.0009176433668471873\n",
            "step: 140, loss: 0.008009420707821846\n",
            "step: 150, loss: 0.0008159943390637636\n",
            "step: 160, loss: 0.0006864324677735567\n",
            "step: 170, loss: 0.0021367864683270454\n",
            "step: 180, loss: 0.0005131236393935978\n",
            "step: 190, loss: 0.0016509348060935736\n",
            "step: 200, loss: 0.0003366968594491482\n",
            "step: 210, loss: 0.0002540001296438277\n",
            "step: 220, loss: 0.002136721508577466\n",
            "step: 230, loss: 0.0007499762577936053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9864559819413092, f1=0.9819819819819819, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015265131369233131\n",
            "step: 10, loss: 0.0009418095578439534\n",
            "step: 20, loss: 0.01924920827150345\n",
            "step: 30, loss: 0.0013377702562138438\n",
            "step: 40, loss: 0.0004171864129602909\n",
            "step: 50, loss: 0.00046013668179512024\n",
            "step: 60, loss: 0.00033808802254498005\n",
            "step: 70, loss: 0.001227608066983521\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.0004300783621147275\n",
            "step: 90, loss: 0.0014841158408671618\n",
            "step: 100, loss: 0.022482819855213165\n",
            "step: 110, loss: 0.0009295637719333172\n",
            "step: 120, loss: 0.0005347684491425753\n",
            "step: 130, loss: 0.003151930635794997\n",
            "step: 140, loss: 0.00106927624437958\n",
            "step: 150, loss: 0.002066158689558506\n",
            "step: 160, loss: 0.0018988043302670121\n",
            "step: 170, loss: 0.0007886640378274024\n",
            "step: 180, loss: 0.002959903795272112\n",
            "step: 190, loss: 0.003343975404277444\n",
            "step: 200, loss: 0.0006487391656264663\n",
            "step: 210, loss: 0.0012008554767817259\n",
            "step: 220, loss: 0.0011735325679183006\n",
            "step: 230, loss: 0.008509812876582146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9832402234636871, f1=0.980963045912654, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012380664236843586\n",
            "step: 10, loss: 0.00046888430370017886\n",
            "step: 20, loss: 0.0012153706047683954\n",
            "step: 30, loss: 0.0003147529496345669\n",
            "step: 40, loss: 0.0001531085727037862\n",
            "step: 50, loss: 0.0006672601448372006\n",
            "step: 60, loss: 0.0002107533800881356\n",
            "step: 70, loss: 0.0008190934313461185\n",
            "step: 80, loss: 0.0001449982082704082\n",
            "step: 90, loss: 0.0030171305406838655\n",
            "step: 100, loss: 0.000262240442680195\n",
            "step: 110, loss: 0.00026960670948028564\n",
            "step: 120, loss: 0.0005611243541352451\n",
            "step: 130, loss: 0.00012261133815627545\n",
            "step: 140, loss: 0.000191961313248612\n",
            "step: 150, loss: 0.0023442646488547325\n",
            "step: 160, loss: 0.09628985077142715\n",
            "step: 170, loss: 0.0018665001261979342\n",
            "step: 180, loss: 0.005625266116112471\n",
            "step: 190, loss: 0.0009235365432687104\n",
            "step: 200, loss: 0.05981716513633728\n",
            "step: 210, loss: 0.0002652766997925937\n",
            "step: 220, loss: 0.0003515873395372182\n",
            "step: 230, loss: 0.0015765054849907756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9841986455981941, f1=0.9809203142536477, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004312242672313005\n",
            "step: 10, loss: 0.0002169121289625764\n",
            "step: 20, loss: 0.00043284331331960857\n",
            "step: 30, loss: 0.0002826303243637085\n",
            "step: 40, loss: 0.007915617898106575\n",
            "step: 50, loss: 0.006168997380882502\n",
            "step: 60, loss: 7.248019392136484e-05\n",
            "step: 70, loss: 0.00040014475234784186\n",
            "step: 80, loss: 0.00017173374362755567\n",
            "step: 90, loss: 5.379700451157987e-05\n",
            "step: 100, loss: 0.0001277208502870053\n",
            "step: 110, loss: 0.0011698628077283502\n",
            "step: 120, loss: 0.00014871227904222906\n",
            "step: 130, loss: 9.70148976193741e-05\n",
            "step: 140, loss: 0.00023158527619671077\n",
            "step: 150, loss: 0.0004331485542934388\n",
            "step: 160, loss: 0.0007616067887283862\n",
            "step: 170, loss: 0.0002477881498634815\n",
            "step: 180, loss: 0.0012675764737650752\n",
            "step: 190, loss: 0.0007174008060246706\n",
            "step: 200, loss: 0.0004015429294668138\n",
            "step: 210, loss: 0.00039147763163782656\n",
            "step: 220, loss: 0.0007801683386787772\n",
            "step: 230, loss: 0.0005254833959043026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9830890642615557, f1=0.9785310734463276, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.185679325833917e-05\n",
            "step: 10, loss: 0.00048096096725203097\n",
            "step: 20, loss: 0.0021216077730059624\n",
            "step: 30, loss: 0.0001692005171207711\n",
            "step: 40, loss: 0.06003952398896217\n",
            "step: 50, loss: 0.00017114693764597178\n",
            "step: 60, loss: 0.00018669698329176754\n",
            "step: 70, loss: 0.010566574521362782\n",
            "step: 80, loss: 0.0004121048841625452\n",
            "step: 90, loss: 0.0003668249992188066\n",
            "step: 100, loss: 0.0024336641654372215\n",
            "step: 110, loss: 0.00019013545534107834\n",
            "step: 120, loss: 0.00020552182104438543\n",
            "step: 130, loss: 0.00018364415154792368\n",
            "step: 140, loss: 0.00012961744505446404\n",
            "step: 150, loss: 0.0030037562828511\n",
            "step: 160, loss: 5.9392918046796694e-05\n",
            "step: 170, loss: 0.00013691812637262046\n",
            "step: 180, loss: 0.00022056198213249445\n",
            "step: 190, loss: 0.00015396658272948116\n",
            "step: 200, loss: 0.00010828254016814753\n",
            "step: 210, loss: 0.0002985493920277804\n",
            "step: 220, loss: 7.6774311310146e-05\n",
            "step: 230, loss: 9.77688905550167e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9841986455981941, f1=0.978675645342312, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004580616659950465\n",
            "step: 10, loss: 0.0003313010965939611\n",
            "step: 20, loss: 0.0024640436749905348\n",
            "step: 30, loss: 0.0007034221780486405\n",
            "step: 40, loss: 0.0005304018268361688\n",
            "step: 50, loss: 0.0001752467214828357\n",
            "step: 60, loss: 0.004989480599761009\n",
            "step: 70, loss: 0.0007820979808457196\n",
            "step: 80, loss: 0.00013075920287519693\n",
            "step: 90, loss: 0.0004158570372965187\n",
            "step: 100, loss: 0.00028467527590692043\n",
            "step: 110, loss: 0.00028507495881058276\n",
            "step: 120, loss: 0.00037218094803392887\n",
            "step: 130, loss: 0.05213937163352966\n",
            "step: 140, loss: 0.04829488322138786\n",
            "step: 150, loss: 0.003098799381405115\n",
            "step: 160, loss: 0.0005787723348475993\n",
            "step: 170, loss: 9.013717499328777e-05\n",
            "step: 180, loss: 0.00028888150700367987\n",
            "step: 190, loss: 0.006135253235697746\n",
            "step: 200, loss: 0.00017007514543365687\n",
            "step: 210, loss: 0.002980231773108244\n",
            "step: 220, loss: 7.459999324055389e-05\n",
            "step: 230, loss: 0.0004630677285604179\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9841628959276018, f1=0.9785310734463276, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.870665700058453e-05\n",
            "step: 10, loss: 0.0010392741532996297\n",
            "step: 20, loss: 0.0013533529127016664\n",
            "step: 30, loss: 0.0007501942454837263\n",
            "step: 40, loss: 0.0001685676397755742\n",
            "step: 50, loss: 0.00024619512259960175\n",
            "step: 60, loss: 0.0002381491503911093\n",
            "step: 70, loss: 0.00098114472348243\n",
            "step: 80, loss: 8.277566666947678e-05\n",
            "step: 90, loss: 0.0001327426580246538\n",
            "step: 100, loss: 0.0005240296013653278\n",
            "step: 110, loss: 0.000854658370371908\n",
            "step: 120, loss: 0.00021238430053927004\n",
            "step: 130, loss: 0.0001553012989461422\n",
            "step: 140, loss: 0.0001286633632844314\n",
            "step: 150, loss: 0.05579088255763054\n",
            "step: 160, loss: 8.52229495649226e-05\n",
            "step: 170, loss: 0.012871512211859226\n",
            "step: 180, loss: 0.0001259096898138523\n",
            "step: 190, loss: 4.713390444521792e-05\n",
            "step: 200, loss: 0.00019850913668051362\n",
            "step: 210, loss: 0.00012078432337148115\n",
            "step: 220, loss: 5.887702718609944e-05\n",
            "step: 230, loss: 0.00013803226465824991\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9841986455981941, f1=0.9796380090497738, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.577285239472985e-05\n",
            "step: 10, loss: 4.8132118536159396e-05\n",
            "step: 20, loss: 4.287609408493154e-05\n",
            "step: 30, loss: 0.00019781995797529817\n",
            "step: 40, loss: 0.0001902000658446923\n",
            "step: 50, loss: 0.0032057994976639748\n",
            "step: 60, loss: 0.00012643625086639076\n",
            "step: 70, loss: 0.0001092143211280927\n",
            "step: 80, loss: 0.0007745159673504531\n",
            "step: 90, loss: 3.8491940358653665e-05\n",
            "step: 100, loss: 3.937457950087264e-05\n",
            "step: 110, loss: 0.0003898228460457176\n",
            "step: 120, loss: 0.002394420327618718\n",
            "step: 130, loss: 6.915470294188708e-05\n",
            "step: 140, loss: 0.00010080420906888321\n",
            "step: 150, loss: 4.7785557399038225e-05\n",
            "step: 160, loss: 4.5137614506529644e-05\n",
            "step: 170, loss: 0.00017551681958138943\n",
            "step: 180, loss: 0.00010747456690296531\n",
            "step: 190, loss: 8.293725841213018e-05\n",
            "step: 200, loss: 0.0008555007516406476\n",
            "step: 210, loss: 0.00017057244258467108\n",
            "step: 220, loss: 0.00010236976231681183\n",
            "step: 230, loss: 0.0554422028362751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9796839729119639, f1=0.9762174405436014, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004569226875901222\n",
            "step: 10, loss: 0.0017347205430269241\n",
            "step: 20, loss: 0.0002190643281210214\n",
            "step: 30, loss: 0.0001096629275707528\n",
            "step: 40, loss: 0.002347575267776847\n",
            "step: 50, loss: 6.945618952158839e-05\n",
            "step: 60, loss: 6.220788054633886e-05\n",
            "step: 70, loss: 8.165084000211209e-05\n",
            "step: 80, loss: 0.007823976688086987\n",
            "step: 90, loss: 0.0005815388285554945\n",
            "step: 100, loss: 4.020523192593828e-05\n",
            "step: 110, loss: 0.03033762238919735\n",
            "step: 120, loss: 0.03809726983308792\n",
            "step: 130, loss: 0.0001434101432096213\n",
            "step: 140, loss: 4.7845249355304986e-05\n",
            "step: 150, loss: 6.449426291510463e-05\n",
            "step: 160, loss: 0.0001554681221023202\n",
            "step: 170, loss: 0.00030920319841243327\n",
            "step: 180, loss: 0.0003839965211227536\n",
            "step: 190, loss: 4.9272504838882014e-05\n",
            "step: 200, loss: 0.0025110922288149595\n",
            "step: 210, loss: 2.786807090160437e-05\n",
            "step: 220, loss: 6.494497210951522e-05\n",
            "step: 230, loss: 5.315931048244238e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9831271091113611, f1=0.9763779527559054, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.694602855830453e-05\n",
            "step: 10, loss: 7.770551746943966e-05\n",
            "step: 20, loss: 0.00012000366405118257\n",
            "step: 30, loss: 5.333069202606566e-05\n",
            "step: 40, loss: 8.011725731194019e-05\n",
            "step: 50, loss: 9.670990402810276e-05\n",
            "step: 60, loss: 5.2541534387273714e-05\n",
            "step: 70, loss: 4.671226997743361e-05\n",
            "step: 80, loss: 4.768471990246326e-05\n",
            "step: 90, loss: 5.757126564276405e-05\n",
            "step: 100, loss: 4.8349174903705716e-05\n",
            "step: 110, loss: 4.8665417125448585e-05\n",
            "step: 120, loss: 4.844251452595927e-05\n",
            "step: 130, loss: 4.528999124886468e-05\n",
            "step: 140, loss: 8.662914478918537e-05\n",
            "step: 150, loss: 5.424210030469112e-05\n",
            "step: 160, loss: 0.0009496846469119191\n",
            "step: 170, loss: 2.69146948994603e-05\n",
            "step: 180, loss: 6.726007268298417e-05\n",
            "step: 190, loss: 4.4130763853900135e-05\n",
            "step: 200, loss: 5.687946395482868e-05\n",
            "step: 210, loss: 5.818430508952588e-05\n",
            "step: 220, loss: 0.0001206387605634518\n",
            "step: 230, loss: 0.00010402826592326164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9819819819819819, f1=0.976324689966178, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9551969419117086e-05\n",
            "step: 10, loss: 2.9130438633728772e-05\n",
            "step: 20, loss: 0.00010710791684687138\n",
            "step: 30, loss: 7.990591257112101e-05\n",
            "step: 40, loss: 0.00024079656577669084\n",
            "step: 50, loss: 6.10832721577026e-05\n",
            "step: 60, loss: 0.00017339947225991637\n",
            "step: 70, loss: 0.002899157116189599\n",
            "step: 80, loss: 3.6636691220337525e-05\n",
            "step: 90, loss: 0.0001551083114463836\n",
            "step: 100, loss: 4.558759246720001e-05\n",
            "step: 110, loss: 0.0788603201508522\n",
            "step: 120, loss: 5.8339908719062805e-05\n",
            "step: 130, loss: 0.00012739196245092899\n",
            "step: 140, loss: 6.372025382006541e-05\n",
            "step: 150, loss: 0.0007070776191540062\n",
            "step: 160, loss: 3.5712499084183946e-05\n",
            "step: 170, loss: 7.675069355173036e-05\n",
            "step: 180, loss: 0.0006048991926945746\n",
            "step: 190, loss: 0.0003589148400351405\n",
            "step: 200, loss: 0.00042109336936846375\n",
            "step: 210, loss: 5.0465874664951116e-05\n",
            "step: 220, loss: 8.028676529647782e-05\n",
            "step: 230, loss: 3.9475249650422484e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9831271091113611, f1=0.9774774774774775, best_f1=0.9819819819819819\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 188.19it/s]\n",
            "load_f1 = 0.9875141884222476\n",
            "real_f1 = 0.9852104664391355\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 233.69it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4v1tmXbgw8B"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1369fd-dd02-41aa-951b-862f786d4912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6068850755691528\n",
            "step: 10, loss: 0.5270300507545471\n",
            "step: 20, loss: 0.5053499937057495\n",
            "step: 30, loss: 0.16254061460494995\n",
            "step: 40, loss: 0.22561100125312805\n",
            "step: 50, loss: 0.10798110067844391\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.034259356558322906\n",
            "step: 70, loss: 0.1549869179725647\n",
            "step: 80, loss: 0.03935643285512924\n",
            "step: 90, loss: 0.41250619292259216\n",
            "step: 100, loss: 0.06138390302658081\n",
            "step: 110, loss: 0.06954324245452881\n",
            "step: 120, loss: 0.1545054316520691\n",
            "step: 130, loss: 0.08995958417654037\n",
            "step: 140, loss: 0.09775088727474213\n",
            "step: 150, loss: 0.07525243610143661\n",
            "step: 160, loss: 0.05033678933978081\n",
            "step: 170, loss: 0.2547312378883362\n",
            "step: 180, loss: 0.21538399159908295\n",
            "step: 190, loss: 0.03015214204788208\n",
            "step: 200, loss: 0.1344924420118332\n",
            "step: 210, loss: 0.12014059722423553\n",
            "step: 220, loss: 0.24769316613674164\n",
            "step: 230, loss: 0.15683291852474213\n",
            "step: 240, loss: 0.15485888719558716\n",
            "step: 250, loss: 0.031582459807395935\n",
            "step: 260, loss: 0.10646944493055344\n",
            "step: 270, loss: 0.07084063440561295\n",
            "step: 280, loss: 0.19246144592761993\n",
            "step: 290, loss: 0.05080774798989296\n",
            "step: 300, loss: 0.09274981915950775\n",
            "step: 310, loss: 0.33367493748664856\n",
            "step: 320, loss: 0.09886081516742706\n",
            "step: 330, loss: 0.08466793596744537\n",
            "step: 340, loss: 0.028217598795890808\n",
            "step: 350, loss: 0.1929446905851364\n",
            "step: 360, loss: 0.05700921639800072\n",
            "step: 370, loss: 0.07332902401685715\n",
            "step: 380, loss: 0.05421784520149231\n",
            "step: 390, loss: 0.10769855231046677\n",
            "step: 400, loss: 0.16658420860767365\n",
            "step: 410, loss: 0.0550568625330925\n",
            "step: 420, loss: 0.018758926540613174\n",
            "step: 430, loss: 0.16032442450523376\n",
            "step: 440, loss: 0.010665642097592354\n",
            "step: 450, loss: 0.011556115932762623\n",
            "step: 460, loss: 0.010383358225226402\n",
            "step: 470, loss: 0.1633012443780899\n",
            "step: 480, loss: 0.0691438615322113\n",
            "step: 490, loss: 0.20907127857208252\n",
            "step: 500, loss: 0.032963838428258896\n",
            "step: 510, loss: 0.0762207955121994\n",
            "step: 520, loss: 0.07067131251096725\n",
            "step: 530, loss: 0.00677064573392272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9211009174311927, f1=0.9115895556573523, best_f1=0.9115895556573523\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16791540384292603\n",
            "step: 10, loss: 0.07842708379030228\n",
            "step: 20, loss: 0.017263347283005714\n",
            "step: 30, loss: 0.08232752233743668\n",
            "step: 40, loss: 0.05146970972418785\n",
            "step: 50, loss: 0.14575079083442688\n",
            "step: 60, loss: 0.02370128408074379\n",
            "step: 70, loss: 0.03510133549571037\n",
            "step: 80, loss: 0.06345943361520767\n",
            "step: 90, loss: 0.009678596630692482\n",
            "step: 100, loss: 0.1275177299976349\n",
            "step: 110, loss: 0.029639320448040962\n",
            "step: 120, loss: 0.13946974277496338\n",
            "step: 130, loss: 0.11204451322555542\n",
            "step: 140, loss: 0.0481988824903965\n",
            "step: 150, loss: 0.07305518537759781\n",
            "step: 160, loss: 0.03767990320920944\n",
            "step: 170, loss: 0.04217955842614174\n",
            "step: 180, loss: 0.019640080630779266\n",
            "step: 190, loss: 0.09244698286056519\n",
            "step: 200, loss: 0.017504582181572914\n",
            "step: 210, loss: 0.049987174570560455\n",
            "step: 220, loss: 0.05584704875946045\n",
            "step: 230, loss: 0.010606915690004826\n",
            "step: 240, loss: 0.07583483308553696\n",
            "step: 250, loss: 0.006550476420670748\n",
            "step: 260, loss: 0.0016905523370951414\n",
            "step: 270, loss: 0.1778893768787384\n",
            "step: 280, loss: 0.03582741320133209\n",
            "step: 290, loss: 0.06684666872024536\n",
            "step: 300, loss: 0.10572201013565063\n",
            "step: 310, loss: 0.006036738399416208\n",
            "step: 320, loss: 0.049577582627534866\n",
            "step: 330, loss: 0.11472567170858383\n",
            "step: 340, loss: 0.022645117715001106\n",
            "step: 350, loss: 0.12103116512298584\n",
            "step: 360, loss: 0.2071041762828827\n",
            "step: 370, loss: 0.1211489588022232\n",
            "step: 380, loss: 0.03376949578523636\n",
            "step: 390, loss: 0.08072736114263535\n",
            "step: 400, loss: 0.05405385047197342\n",
            "step: 410, loss: 0.011709876358509064\n",
            "step: 420, loss: 0.08083775639533997\n",
            "step: 430, loss: 0.009367204271256924\n",
            "step: 440, loss: 0.07751069217920303\n",
            "step: 450, loss: 0.015680793672800064\n",
            "step: 460, loss: 0.017594439908862114\n",
            "step: 470, loss: 0.07142972201108932\n",
            "step: 480, loss: 0.2115067094564438\n",
            "step: 490, loss: 0.016430621966719627\n",
            "step: 500, loss: 0.28401631116867065\n",
            "step: 510, loss: 0.05470903217792511\n",
            "step: 520, loss: 0.029760140925645828\n",
            "step: 530, loss: 0.07345228642225266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9304229195088677, f1=0.9258079198907602, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025695525109767914\n",
            "step: 10, loss: 0.06397245079278946\n",
            "step: 20, loss: 0.06625008583068848\n",
            "step: 30, loss: 0.026750944554805756\n",
            "step: 40, loss: 0.004400196485221386\n",
            "step: 50, loss: 0.15823854506015778\n",
            "step: 60, loss: 0.04155378416180611\n",
            "step: 70, loss: 0.0032426929101347923\n",
            "step: 80, loss: 0.001478118123486638\n",
            "step: 90, loss: 0.0038454090245068073\n",
            "step: 100, loss: 0.044139739125967026\n",
            "step: 110, loss: 0.0045501417480409145\n",
            "step: 120, loss: 0.006977580487728119\n",
            "step: 130, loss: 0.012887917459011078\n",
            "step: 140, loss: 0.1009511724114418\n",
            "step: 150, loss: 0.04071440547704697\n",
            "step: 160, loss: 0.0043445550836622715\n",
            "step: 170, loss: 0.1957513391971588\n",
            "step: 180, loss: 0.12289343774318695\n",
            "step: 190, loss: 0.021133949980139732\n",
            "step: 200, loss: 0.03882087022066116\n",
            "step: 210, loss: 0.06875845789909363\n",
            "step: 220, loss: 0.07587747275829315\n",
            "step: 230, loss: 0.11026482284069061\n",
            "step: 240, loss: 0.008057557046413422\n",
            "step: 250, loss: 0.027911925688385963\n",
            "step: 260, loss: 0.025257764384150505\n",
            "step: 270, loss: 0.0025907433591783047\n",
            "step: 280, loss: 0.11655489355325699\n",
            "step: 290, loss: 0.0040401979349553585\n",
            "step: 300, loss: 0.070164754986763\n",
            "step: 310, loss: 0.06018422171473503\n",
            "step: 320, loss: 0.03054816834628582\n",
            "step: 330, loss: 0.008808943443000317\n",
            "step: 340, loss: 0.008652576245367527\n",
            "step: 350, loss: 0.010150809772312641\n",
            "step: 360, loss: 0.03257722780108452\n",
            "step: 370, loss: 0.001774113392457366\n",
            "step: 380, loss: 0.0030333888716995716\n",
            "step: 390, loss: 0.01088614109903574\n",
            "step: 400, loss: 0.005494707729667425\n",
            "step: 410, loss: 0.009706443175673485\n",
            "step: 420, loss: 0.09982930123806\n",
            "step: 430, loss: 0.04996248707175255\n",
            "step: 440, loss: 0.02025371789932251\n",
            "step: 450, loss: 0.10619354248046875\n",
            "step: 460, loss: 0.02138112299144268\n",
            "step: 470, loss: 0.04730674624443054\n",
            "step: 480, loss: 0.00990525633096695\n",
            "step: 490, loss: 0.004663992673158646\n",
            "step: 500, loss: 0.012781146913766861\n",
            "step: 510, loss: 0.02566821128129959\n",
            "step: 520, loss: 0.04856916889548302\n",
            "step: 530, loss: 0.1643768548965454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9294449670743179, f1=0.9160739687055476, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010653638280928135\n",
            "step: 10, loss: 0.012128020636737347\n",
            "step: 20, loss: 0.002847106894478202\n",
            "step: 30, loss: 0.017210038378834724\n",
            "step: 40, loss: 0.01667102985084057\n",
            "step: 50, loss: 0.002789732301607728\n",
            "step: 60, loss: 0.0015232603764161468\n",
            "step: 70, loss: 0.007459329906851053\n",
            "step: 80, loss: 0.0017880294471979141\n",
            "step: 90, loss: 0.014198895543813705\n",
            "step: 100, loss: 0.01053677685558796\n",
            "step: 110, loss: 0.0025070025585591793\n",
            "step: 120, loss: 0.0182966236025095\n",
            "step: 130, loss: 0.0032471627928316593\n",
            "step: 140, loss: 0.0007368612568825483\n",
            "step: 150, loss: 0.0004914907622151077\n",
            "step: 160, loss: 0.009426316246390343\n",
            "step: 170, loss: 0.0010272933868691325\n",
            "step: 180, loss: 0.0005252013797871768\n",
            "step: 190, loss: 0.023803625255823135\n",
            "step: 200, loss: 0.040353596210479736\n",
            "step: 210, loss: 0.051986075937747955\n",
            "step: 220, loss: 0.02956167794764042\n",
            "step: 230, loss: 0.010033069178462029\n",
            "step: 240, loss: 0.01956423744559288\n",
            "step: 250, loss: 0.05452372878789902\n",
            "step: 260, loss: 0.007072998210787773\n",
            "step: 270, loss: 0.014221543446183205\n",
            "step: 280, loss: 0.012020662426948547\n",
            "step: 290, loss: 0.04176484793424606\n",
            "step: 300, loss: 0.0009094555280171335\n",
            "step: 310, loss: 0.016474299132823944\n",
            "step: 320, loss: 0.04027723893523216\n",
            "step: 330, loss: 0.00738409161567688\n",
            "step: 340, loss: 0.0171017125248909\n",
            "step: 350, loss: 0.0031344592571258545\n",
            "step: 360, loss: 0.009750349447131157\n",
            "step: 370, loss: 0.0028635291382670403\n",
            "step: 380, loss: 0.11517868936061859\n",
            "step: 390, loss: 0.008960118517279625\n",
            "step: 400, loss: 0.0324140228331089\n",
            "step: 410, loss: 0.00035005746758542955\n",
            "step: 420, loss: 0.04021234065294266\n",
            "step: 430, loss: 0.09426160156726837\n",
            "step: 440, loss: 0.001045728800818324\n",
            "step: 450, loss: 0.0010409763781353831\n",
            "step: 460, loss: 0.04382725805044174\n",
            "step: 470, loss: 0.03659893572330475\n",
            "step: 480, loss: 0.050328802317380905\n",
            "step: 490, loss: 0.008345894515514374\n",
            "step: 500, loss: 0.09038699418306351\n",
            "step: 510, loss: 0.025460941717028618\n",
            "step: 520, loss: 0.1461568921804428\n",
            "step: 530, loss: 0.02570686861872673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.925672594619243, f1=0.9241316270566727, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0458797812461853\n",
            "step: 10, loss: 0.08434326201677322\n",
            "step: 20, loss: 0.0018366037402302027\n",
            "step: 30, loss: 0.04388359189033508\n",
            "step: 40, loss: 0.03778635337948799\n",
            "step: 50, loss: 0.0002688771055545658\n",
            "step: 60, loss: 0.14304980635643005\n",
            "step: 70, loss: 0.05581265687942505\n",
            "step: 80, loss: 0.0005376214976422489\n",
            "step: 90, loss: 0.006547222845256329\n",
            "step: 100, loss: 0.010783782228827477\n",
            "step: 110, loss: 0.0019352651434019208\n",
            "step: 120, loss: 0.0007801895844750106\n",
            "step: 130, loss: 0.0016171840252354741\n",
            "step: 140, loss: 0.048735927790403366\n",
            "step: 150, loss: 0.0003250340814702213\n",
            "step: 160, loss: 0.0004917451296932995\n",
            "step: 170, loss: 0.03144029900431633\n",
            "step: 180, loss: 0.0011649195803329349\n",
            "step: 190, loss: 0.000747202429920435\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 200, loss: 0.00031652243342250586\n",
            "step: 210, loss: 0.001417039893567562\n",
            "step: 220, loss: 0.047500938177108765\n",
            "step: 230, loss: 0.015918642282485962\n",
            "step: 240, loss: 0.001590581377968192\n",
            "step: 250, loss: 0.00547062186524272\n",
            "step: 260, loss: 0.0018802598351612687\n",
            "step: 270, loss: 0.0006667958223260939\n",
            "step: 280, loss: 0.0006794061628170311\n",
            "step: 290, loss: 0.1075969710946083\n",
            "step: 300, loss: 0.009569823741912842\n",
            "step: 310, loss: 0.003023498458787799\n",
            "step: 320, loss: 0.19117328524589539\n",
            "step: 330, loss: 0.013615582138299942\n",
            "step: 340, loss: 0.024121765047311783\n",
            "step: 350, loss: 0.013981008902192116\n",
            "step: 360, loss: 0.010986646637320518\n",
            "step: 370, loss: 0.020659523084759712\n",
            "step: 380, loss: 0.00021544135233853012\n",
            "step: 390, loss: 0.0001454725133953616\n",
            "step: 400, loss: 0.003880077041685581\n",
            "step: 410, loss: 0.0018675551982596517\n",
            "step: 420, loss: 0.0033034593798220158\n",
            "step: 430, loss: 0.0036654879804700613\n",
            "step: 440, loss: 0.032961420714855194\n",
            "step: 450, loss: 0.34273192286491394\n",
            "step: 460, loss: 0.061486903578042984\n",
            "step: 470, loss: 0.00869095977395773\n",
            "step: 480, loss: 0.001947422162629664\n",
            "step: 490, loss: 0.0007800192688591778\n",
            "step: 500, loss: 0.0013460369082167745\n",
            "step: 510, loss: 0.0011861780658364296\n",
            "step: 520, loss: 0.000893834454473108\n",
            "step: 530, loss: 0.0005746575188823044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9296435272045028, f1=0.9294062646096307, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008166803745552897\n",
            "step: 10, loss: 0.0003122281050309539\n",
            "step: 20, loss: 0.10067299008369446\n",
            "step: 30, loss: 0.0022010121028870344\n",
            "step: 40, loss: 0.006571450736373663\n",
            "step: 50, loss: 0.023062244057655334\n",
            "step: 60, loss: 0.0002425971470074728\n",
            "step: 70, loss: 0.09787168353796005\n",
            "step: 80, loss: 0.0037571582943201065\n",
            "step: 90, loss: 0.002558661624789238\n",
            "step: 100, loss: 0.01312714908272028\n",
            "step: 110, loss: 0.02922353520989418\n",
            "step: 120, loss: 0.0002901550324168056\n",
            "step: 130, loss: 0.0002784219686873257\n",
            "step: 140, loss: 0.04294315725564957\n",
            "step: 150, loss: 0.0006443252204917371\n",
            "step: 160, loss: 0.0027552698738873005\n",
            "step: 170, loss: 0.003831075271591544\n",
            "step: 180, loss: 0.0006277422071434557\n",
            "step: 190, loss: 0.003970210440456867\n",
            "step: 200, loss: 0.003724857931956649\n",
            "step: 210, loss: 0.02550944685935974\n",
            "step: 220, loss: 0.003538607619702816\n",
            "step: 230, loss: 0.0001106637719203718\n",
            "step: 240, loss: 0.0020597209222614765\n",
            "step: 250, loss: 0.0005673793493770063\n",
            "step: 260, loss: 5.977265755063854e-05\n",
            "step: 270, loss: 0.046080779284238815\n",
            "step: 280, loss: 0.00017795883468352258\n",
            "step: 290, loss: 0.00046986580127850175\n",
            "step: 300, loss: 0.0007948126294650137\n",
            "step: 310, loss: 0.0008116299868561327\n",
            "step: 320, loss: 0.003045555204153061\n",
            "step: 330, loss: 0.0005538003169931471\n",
            "step: 340, loss: 0.08897415548563004\n",
            "step: 350, loss: 0.0020725070498883724\n",
            "step: 360, loss: 0.008904735557734966\n",
            "step: 370, loss: 0.0036759013310074806\n",
            "step: 380, loss: 0.00011459580855444074\n",
            "step: 390, loss: 0.02380215935409069\n",
            "step: 400, loss: 0.00192958430852741\n",
            "step: 410, loss: 0.0016048597171902657\n",
            "step: 420, loss: 0.00046669712173752487\n",
            "step: 430, loss: 0.001879492192529142\n",
            "step: 440, loss: 0.0028915531001985073\n",
            "step: 450, loss: 0.0009463266469538212\n",
            "step: 460, loss: 0.0002914041979238391\n",
            "step: 470, loss: 0.0032528096344321966\n",
            "step: 480, loss: 0.0016977066406980157\n",
            "step: 490, loss: 0.005632099229842424\n",
            "step: 500, loss: 0.0005692229606211185\n",
            "step: 510, loss: 0.0020178162958472967\n",
            "step: 520, loss: 0.007199910935014486\n",
            "step: 530, loss: 0.004346879664808512\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9232927970065481, f1=0.9256661991584852, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0048377676866948605\n",
            "step: 10, loss: 0.00034197629429399967\n",
            "step: 20, loss: 0.0001020417912513949\n",
            "step: 30, loss: 6.951496470719576e-05\n",
            "step: 40, loss: 0.00010909637785516679\n",
            "step: 50, loss: 0.0021011861972510815\n",
            "step: 60, loss: 0.0029389888513833284\n",
            "step: 70, loss: 7.811714749550447e-05\n",
            "step: 80, loss: 0.0006929713999852538\n",
            "step: 90, loss: 0.0001264161110157147\n",
            "step: 100, loss: 0.0005298318574205041\n",
            "step: 110, loss: 0.008709547109901905\n",
            "step: 120, loss: 0.009485532529652119\n",
            "step: 130, loss: 0.0008888348238542676\n",
            "step: 140, loss: 9.010423673316836e-05\n",
            "step: 150, loss: 0.00048658126615919173\n",
            "step: 160, loss: 0.0003971314581576735\n",
            "step: 170, loss: 0.0002651326358318329\n",
            "step: 180, loss: 0.0017467880388721824\n",
            "step: 190, loss: 0.002867927076295018\n",
            "step: 200, loss: 0.0069848294369876385\n",
            "step: 210, loss: 0.001204643864184618\n",
            "step: 220, loss: 6.562969065271318e-05\n",
            "step: 230, loss: 0.008844178169965744\n",
            "step: 240, loss: 0.03432578966021538\n",
            "step: 250, loss: 0.000731959764380008\n",
            "step: 260, loss: 0.00018881050345953554\n",
            "step: 270, loss: 0.00016009638784453273\n",
            "step: 280, loss: 0.006397663149982691\n",
            "step: 290, loss: 0.002535934094339609\n",
            "step: 300, loss: 0.0008035663631744683\n",
            "step: 310, loss: 0.00179489073343575\n",
            "step: 320, loss: 5.3138901421334594e-05\n",
            "step: 330, loss: 0.061749719083309174\n",
            "step: 340, loss: 0.014425262808799744\n",
            "step: 350, loss: 0.0001393991697113961\n",
            "step: 360, loss: 0.002048265654593706\n",
            "step: 370, loss: 0.0004080454236827791\n",
            "step: 380, loss: 0.0006155143491923809\n",
            "step: 390, loss: 0.0006880206638015807\n",
            "step: 400, loss: 0.004042471759021282\n",
            "step: 410, loss: 0.0025900632608681917\n",
            "step: 420, loss: 0.0019970559515058994\n",
            "step: 430, loss: 0.0010673528304323554\n",
            "step: 440, loss: 9.82739802566357e-05\n",
            "step: 450, loss: 0.0005087197641842067\n",
            "step: 460, loss: 0.0004889436531811953\n",
            "step: 470, loss: 0.06522249430418015\n",
            "step: 480, loss: 0.07676360011100769\n",
            "step: 490, loss: 0.0005928027676418424\n",
            "step: 500, loss: 0.0023549962788820267\n",
            "step: 510, loss: 0.0009312143665738404\n",
            "step: 520, loss: 0.02891131304204464\n",
            "step: 530, loss: 0.019451601430773735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9251637043966324, f1=0.9216417910447762, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001332942338194698\n",
            "step: 10, loss: 0.13891561329364777\n",
            "step: 20, loss: 0.001000677002593875\n",
            "step: 30, loss: 0.0002341052022529766\n",
            "step: 40, loss: 0.0006724271806888282\n",
            "step: 50, loss: 0.006406946107745171\n",
            "step: 60, loss: 0.00064830482006073\n",
            "step: 70, loss: 0.0007239675614982843\n",
            "step: 80, loss: 0.0006823260919190943\n",
            "step: 90, loss: 0.00029672510572709143\n",
            "step: 100, loss: 0.00026787607930600643\n",
            "step: 110, loss: 0.00037031841930001974\n",
            "step: 120, loss: 0.0006973493727855384\n",
            "step: 130, loss: 0.00016424534260295331\n",
            "step: 140, loss: 0.0010718133999034762\n",
            "step: 150, loss: 5.403541581472382e-05\n",
            "step: 160, loss: 0.031012283638119698\n",
            "step: 170, loss: 0.002250172197818756\n",
            "step: 180, loss: 0.00033631804399192333\n",
            "step: 190, loss: 0.005614744033664465\n",
            "step: 200, loss: 0.0025756049435585737\n",
            "step: 210, loss: 0.002515912987291813\n",
            "step: 220, loss: 0.0023972413036972284\n",
            "step: 230, loss: 0.002204161137342453\n",
            "step: 240, loss: 0.003107290482148528\n",
            "step: 250, loss: 7.390990504063666e-05\n",
            "step: 260, loss: 0.0024186221417039633\n",
            "step: 270, loss: 0.0007160250097513199\n",
            "step: 280, loss: 0.01895330473780632\n",
            "step: 290, loss: 7.950553117552772e-05\n",
            "step: 300, loss: 0.0006150673725642264\n",
            "step: 310, loss: 0.0004145342099945992\n",
            "step: 320, loss: 0.0022815773263573647\n",
            "step: 330, loss: 0.01134101115167141\n",
            "step: 340, loss: 0.00044239635462872684\n",
            "step: 350, loss: 6.321923865471035e-05\n",
            "step: 360, loss: 0.0008671259274706244\n",
            "step: 370, loss: 0.0006492460961453617\n",
            "step: 380, loss: 0.00024333785404451191\n",
            "step: 390, loss: 0.1720792055130005\n",
            "step: 400, loss: 5.131582292960957e-05\n",
            "step: 410, loss: 0.00016053838771767914\n",
            "step: 420, loss: 0.007921749725937843\n",
            "step: 430, loss: 0.009432761929929256\n",
            "step: 440, loss: 0.004747156053781509\n",
            "step: 450, loss: 0.0009881234727799892\n",
            "step: 460, loss: 7.212110358523205e-05\n",
            "step: 470, loss: 0.004527883604168892\n",
            "step: 480, loss: 0.00010603027476463467\n",
            "step: 490, loss: 0.0028193083126097918\n",
            "step: 500, loss: 0.006164982449263334\n",
            "step: 510, loss: 0.00030842985142953694\n",
            "step: 520, loss: 0.004900705069303513\n",
            "step: 530, loss: 0.0009720601956360042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9256198347107438, f1=0.9238532110091744, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013798772124573588\n",
            "step: 10, loss: 0.00014763770741410553\n",
            "step: 20, loss: 0.0020252089016139507\n",
            "step: 30, loss: 0.0008707279921509326\n",
            "step: 40, loss: 0.0008573809172958136\n",
            "step: 50, loss: 0.0003359080001246184\n",
            "step: 60, loss: 5.4514355724677444e-05\n",
            "step: 70, loss: 4.062149309902452e-05\n",
            "step: 80, loss: 0.0077841985039412975\n",
            "step: 90, loss: 0.000613228534348309\n",
            "step: 100, loss: 5.573626913246699e-05\n",
            "step: 110, loss: 5.254453208181076e-05\n",
            "step: 120, loss: 0.003340293886139989\n",
            "step: 130, loss: 0.02368498407304287\n",
            "step: 140, loss: 0.0002304536319570616\n",
            "step: 150, loss: 7.28155646356754e-05\n",
            "step: 160, loss: 0.0010484597878530622\n",
            "step: 170, loss: 0.008016744628548622\n",
            "step: 180, loss: 0.000457817135611549\n",
            "step: 190, loss: 0.004272941499948502\n",
            "step: 200, loss: 0.003100434085354209\n",
            "step: 210, loss: 0.001289085135795176\n",
            "step: 220, loss: 0.00023310797405429184\n",
            "step: 230, loss: 0.010703565552830696\n",
            "step: 240, loss: 0.0002621119492687285\n",
            "step: 250, loss: 0.005744711495935917\n",
            "step: 260, loss: 0.001359260524623096\n",
            "step: 270, loss: 6.495062552858144e-05\n",
            "step: 280, loss: 0.03975921869277954\n",
            "step: 290, loss: 0.03164052590727806\n",
            "step: 300, loss: 0.0004985590931028128\n",
            "step: 310, loss: 0.0024644476361572742\n",
            "step: 320, loss: 0.00031406202469952404\n",
            "step: 330, loss: 0.0007957498892210424\n",
            "step: 340, loss: 0.0003557655436452478\n",
            "step: 350, loss: 0.000572866469155997\n",
            "step: 360, loss: 0.0001207120149047114\n",
            "step: 370, loss: 0.0011280316393822432\n",
            "step: 380, loss: 0.0004881175409536809\n",
            "step: 390, loss: 5.5821346904849634e-05\n",
            "step: 400, loss: 0.0019133079331368208\n",
            "step: 410, loss: 0.00028056284645572305\n",
            "step: 420, loss: 0.00011238087608944625\n",
            "step: 430, loss: 0.0005641643656417727\n",
            "step: 440, loss: 0.011871113441884518\n",
            "step: 450, loss: 9.037595737027004e-05\n",
            "step: 460, loss: 6.001987640047446e-05\n",
            "step: 470, loss: 3.6196750443195924e-05\n",
            "step: 480, loss: 6.304238922894001e-05\n",
            "step: 490, loss: 0.001381817040964961\n",
            "step: 500, loss: 0.0010924231028184295\n",
            "step: 510, loss: 0.00011574942618608475\n",
            "step: 520, loss: 0.00030392681946977973\n",
            "step: 530, loss: 0.00019418375450186431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9301675977653632, f1=0.9266480965645311, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004842519119847566\n",
            "step: 10, loss: 3.695686609717086e-05\n",
            "step: 20, loss: 3.591014683479443e-05\n",
            "step: 30, loss: 7.976925553521141e-05\n",
            "step: 40, loss: 7.843045023037121e-05\n",
            "step: 50, loss: 6.654639582848176e-05\n",
            "step: 60, loss: 3.6088858905714005e-05\n",
            "step: 70, loss: 4.2513172957114875e-05\n",
            "step: 80, loss: 4.473842273000628e-05\n",
            "step: 90, loss: 4.2581374145811424e-05\n",
            "step: 100, loss: 3.33175303239841e-05\n",
            "step: 110, loss: 1.9106526451651007e-05\n",
            "step: 120, loss: 8.939039253164083e-05\n",
            "step: 130, loss: 3.764901703107171e-05\n",
            "step: 140, loss: 0.004342029802501202\n",
            "step: 150, loss: 4.606751463143155e-05\n",
            "step: 160, loss: 6.03156368015334e-05\n",
            "step: 170, loss: 0.0001359613670501858\n",
            "step: 180, loss: 4.602349872584455e-05\n",
            "step: 190, loss: 6.120264151832089e-05\n",
            "step: 200, loss: 0.0004678386030718684\n",
            "step: 210, loss: 0.005408049561083317\n",
            "step: 220, loss: 0.00018257835472468287\n",
            "step: 230, loss: 0.0004008580872323364\n",
            "step: 240, loss: 0.004132514353841543\n",
            "step: 250, loss: 4.549476216197945e-05\n",
            "step: 260, loss: 0.0015301706735044718\n",
            "step: 270, loss: 0.0008583941962569952\n",
            "step: 280, loss: 0.00011715415894286707\n",
            "step: 290, loss: 0.00017140436102636158\n",
            "step: 300, loss: 0.00011579502461245283\n",
            "step: 310, loss: 0.0032612024806439877\n",
            "step: 320, loss: 0.0025749870110303164\n",
            "step: 330, loss: 0.00010954911704175174\n",
            "step: 340, loss: 0.0036237670574337244\n",
            "step: 350, loss: 0.003109058365225792\n",
            "step: 360, loss: 0.00017480950918979943\n",
            "step: 370, loss: 0.004304860718548298\n",
            "step: 380, loss: 0.004907242022454739\n",
            "step: 390, loss: 5.680204776581377e-05\n",
            "step: 400, loss: 0.000703333062119782\n",
            "step: 410, loss: 0.0036529689095914364\n",
            "step: 420, loss: 4.364814594737254e-05\n",
            "step: 430, loss: 2.336071338504553e-05\n",
            "step: 440, loss: 0.013305873610079288\n",
            "step: 450, loss: 9.280774975195527e-05\n",
            "step: 460, loss: 2.787525772873778e-05\n",
            "step: 470, loss: 0.0001576472568558529\n",
            "step: 480, loss: 3.536658186931163e-05\n",
            "step: 490, loss: 6.552153354277834e-05\n",
            "step: 500, loss: 0.06962060928344727\n",
            "step: 510, loss: 3.50373811670579e-05\n",
            "step: 520, loss: 0.0011951640481129289\n",
            "step: 530, loss: 0.0004700070130638778\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9299363057324841, f1=0.9274965800273598, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010812528198584914\n",
            "step: 10, loss: 0.00017097790259867907\n",
            "step: 20, loss: 4.582429755828343e-05\n",
            "step: 30, loss: 0.0037738559767603874\n",
            "step: 40, loss: 0.0016771790105849504\n",
            "step: 50, loss: 0.0011728598037734628\n",
            "step: 60, loss: 0.02078554593026638\n",
            "step: 70, loss: 3.0989387596491724e-05\n",
            "step: 80, loss: 0.0014103680150583386\n",
            "step: 90, loss: 3.934040432795882e-05\n",
            "step: 100, loss: 0.0019803803879767656\n",
            "step: 110, loss: 0.0001094001199817285\n",
            "step: 120, loss: 2.1248464690870605e-05\n",
            "step: 130, loss: 2.2049405743018724e-05\n",
            "step: 140, loss: 0.0001530126464786008\n",
            "step: 150, loss: 0.007713841274380684\n",
            "step: 160, loss: 0.00014721936895512044\n",
            "step: 170, loss: 5.911299376748502e-05\n",
            "step: 180, loss: 0.0007542589446529746\n",
            "step: 190, loss: 0.00012064833572367206\n",
            "step: 200, loss: 0.0027721994556486607\n",
            "step: 210, loss: 0.013345113024115562\n",
            "step: 220, loss: 0.0006618286133743823\n",
            "step: 230, loss: 3.253415707149543e-05\n",
            "step: 240, loss: 0.0002345365210203454\n",
            "step: 250, loss: 4.683492807089351e-05\n",
            "step: 260, loss: 3.513765841489658e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 270, loss: 3.9213948184624314e-05\n",
            "step: 280, loss: 8.037065708776936e-05\n",
            "step: 290, loss: 0.0005886783474124968\n",
            "step: 300, loss: 8.378105121664703e-05\n",
            "step: 310, loss: 5.027945735491812e-05\n",
            "step: 320, loss: 0.00025599004584364593\n",
            "step: 330, loss: 7.60591501602903e-05\n",
            "step: 340, loss: 6.635684258071706e-05\n",
            "step: 350, loss: 0.00047673817607574165\n",
            "step: 360, loss: 1.9169914594385773e-05\n",
            "step: 370, loss: 0.0005038821254856884\n",
            "step: 380, loss: 3.497857687762007e-05\n",
            "step: 390, loss: 4.9144837248604745e-05\n",
            "step: 400, loss: 0.0005687687662430108\n",
            "step: 410, loss: 0.00022165068367030472\n",
            "step: 420, loss: 0.0058328211307525635\n",
            "step: 430, loss: 4.0349365008296445e-05\n",
            "step: 440, loss: 0.026302695274353027\n",
            "step: 450, loss: 0.0009234407334588468\n",
            "step: 460, loss: 0.0002796108019538224\n",
            "step: 470, loss: 0.005059396382421255\n",
            "step: 480, loss: 0.0009528544032946229\n",
            "step: 490, loss: 0.0005756905884481966\n",
            "step: 500, loss: 0.00021311726595740765\n",
            "step: 510, loss: 0.00014937987725716084\n",
            "step: 520, loss: 0.00021069913054816425\n",
            "step: 530, loss: 0.0015005457680672407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9285384970032272, f1=0.9216589861751152, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00048044483992271125\n",
            "step: 10, loss: 5.602076998911798e-05\n",
            "step: 20, loss: 0.00018355004431214184\n",
            "step: 30, loss: 0.0008237945730797946\n",
            "step: 40, loss: 8.718553726794198e-05\n",
            "step: 50, loss: 0.017996594309806824\n",
            "step: 60, loss: 0.0011184008326381445\n",
            "step: 70, loss: 0.00021814402134623379\n",
            "step: 80, loss: 3.601990101742558e-05\n",
            "step: 90, loss: 9.081153984880075e-05\n",
            "step: 100, loss: 0.0001730762596707791\n",
            "step: 110, loss: 0.00011147499026264995\n",
            "step: 120, loss: 4.8736932512838393e-05\n",
            "step: 130, loss: 0.0003427759511396289\n",
            "step: 140, loss: 8.98596626939252e-05\n",
            "step: 150, loss: 0.0001500355574535206\n",
            "step: 160, loss: 3.373630534042604e-05\n",
            "step: 170, loss: 3.6341654777061194e-05\n",
            "step: 180, loss: 6.448872591136023e-05\n",
            "step: 190, loss: 6.791493069613352e-05\n",
            "step: 200, loss: 4.024177178507671e-05\n",
            "step: 210, loss: 0.000108292733784765\n",
            "step: 220, loss: 9.320673416368663e-05\n",
            "step: 230, loss: 1.619724753254559e-05\n",
            "step: 240, loss: 0.0011384171666577458\n",
            "step: 250, loss: 2.5259678295697086e-05\n",
            "step: 260, loss: 2.5885950890369713e-05\n",
            "step: 270, loss: 3.9813916373532265e-05\n",
            "step: 280, loss: 0.0014016544446349144\n",
            "step: 290, loss: 0.00017067418957594782\n",
            "step: 300, loss: 0.00015453362721018493\n",
            "step: 310, loss: 0.00035379885230213404\n",
            "step: 320, loss: 6.968800880713388e-05\n",
            "step: 330, loss: 0.00010328862117603421\n",
            "step: 340, loss: 0.0001565238635521382\n",
            "step: 350, loss: 4.828731471206993e-05\n",
            "step: 360, loss: 0.07763983309268951\n",
            "step: 370, loss: 0.00022111384896561503\n",
            "step: 380, loss: 0.00011684570199577138\n",
            "step: 390, loss: 5.475479338201694e-05\n",
            "step: 400, loss: 0.06625822186470032\n",
            "step: 410, loss: 0.0001793636620277539\n",
            "step: 420, loss: 0.005255560390651226\n",
            "step: 430, loss: 0.0064646899700164795\n",
            "step: 440, loss: 0.00017178585403598845\n",
            "step: 450, loss: 0.000427976599894464\n",
            "step: 460, loss: 3.366605960763991e-05\n",
            "step: 470, loss: 0.0003175887104589492\n",
            "step: 480, loss: 6.272034079302102e-05\n",
            "step: 490, loss: 0.0005458603263832629\n",
            "step: 500, loss: 0.0005147717893123627\n",
            "step: 510, loss: 0.0003172580327372998\n",
            "step: 520, loss: 0.00650579109787941\n",
            "step: 530, loss: 0.0012413343647494912\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.927348449791763, f1=0.9234317343173433, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017899234080687165\n",
            "step: 10, loss: 2.083874096570071e-05\n",
            "step: 20, loss: 1.6271658751065843e-05\n",
            "step: 30, loss: 3.5913893952965736e-05\n",
            "step: 40, loss: 2.8533489967230707e-05\n",
            "step: 50, loss: 0.0056136962957680225\n",
            "step: 60, loss: 0.001105122035369277\n",
            "step: 70, loss: 3.725758506334387e-05\n",
            "step: 80, loss: 1.4789171473239549e-05\n",
            "step: 90, loss: 0.012110470794141293\n",
            "step: 100, loss: 8.740268822293729e-05\n",
            "step: 110, loss: 7.055747846607119e-05\n",
            "step: 120, loss: 0.00018081540474668145\n",
            "step: 130, loss: 2.30317182285944e-05\n",
            "step: 140, loss: 5.5120202887337655e-05\n",
            "step: 150, loss: 9.999913891078904e-05\n",
            "step: 160, loss: 6.623622175538912e-05\n",
            "step: 170, loss: 0.00012514185800682753\n",
            "step: 180, loss: 4.207580059301108e-05\n",
            "step: 190, loss: 3.7635847547790036e-05\n",
            "step: 200, loss: 1.8886863472289406e-05\n",
            "step: 210, loss: 3.414140883251093e-05\n",
            "step: 220, loss: 4.1308954678243026e-05\n",
            "step: 230, loss: 7.041620119707659e-05\n",
            "step: 240, loss: 3.892889799317345e-05\n",
            "step: 250, loss: 5.189314833842218e-05\n",
            "step: 260, loss: 5.12800324941054e-05\n",
            "step: 270, loss: 1.715080907160882e-05\n",
            "step: 280, loss: 2.447043516440317e-05\n",
            "step: 290, loss: 2.0007864804938436e-05\n",
            "step: 300, loss: 2.761822906904854e-05\n",
            "step: 310, loss: 0.002617995487526059\n",
            "step: 320, loss: 0.003935040440410376\n",
            "step: 330, loss: 0.000463979464257136\n",
            "step: 340, loss: 3.2771607948234305e-05\n",
            "step: 350, loss: 5.0793438276741654e-05\n",
            "step: 360, loss: 6.575473526027054e-05\n",
            "step: 370, loss: 0.0010346139315515757\n",
            "step: 380, loss: 0.0001727267081150785\n",
            "step: 390, loss: 0.00015245167014654726\n",
            "step: 400, loss: 7.866944361012429e-05\n",
            "step: 410, loss: 0.0002911369956564158\n",
            "step: 420, loss: 0.00023052378674037755\n",
            "step: 430, loss: 0.002605936722829938\n",
            "step: 440, loss: 2.014218807744328e-05\n",
            "step: 450, loss: 2.4038556148298085e-05\n",
            "step: 460, loss: 0.0008706369553692639\n",
            "step: 470, loss: 4.605111098499037e-05\n",
            "step: 480, loss: 1.5396215530927293e-05\n",
            "step: 490, loss: 1.966912896023132e-05\n",
            "step: 500, loss: 4.792063555214554e-05\n",
            "step: 510, loss: 8.509409963153303e-05\n",
            "step: 520, loss: 0.0003612112195696682\n",
            "step: 530, loss: 3.373314393684268e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9237248479176415, f1=0.9255813953488372, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013433685526251793\n",
            "step: 10, loss: 1.2684370631177444e-05\n",
            "step: 20, loss: 2.535588100727182e-05\n",
            "step: 30, loss: 3.102606933680363e-05\n",
            "step: 40, loss: 4.70008235424757e-05\n",
            "step: 50, loss: 8.919546962715685e-05\n",
            "step: 60, loss: 2.245123869215604e-05\n",
            "step: 70, loss: 1.9143157260259613e-05\n",
            "step: 80, loss: 0.00036911392817273736\n",
            "step: 90, loss: 7.55640139686875e-05\n",
            "step: 100, loss: 0.0001212373172165826\n",
            "step: 110, loss: 1.677835280133877e-05\n",
            "step: 120, loss: 5.31489058630541e-05\n",
            "step: 130, loss: 0.01809215545654297\n",
            "step: 140, loss: 0.0005278319586068392\n",
            "step: 150, loss: 1.3451803170028143e-05\n",
            "step: 160, loss: 0.0001636508823139593\n",
            "step: 170, loss: 0.02335994690656662\n",
            "step: 180, loss: 2.2607842765864916e-05\n",
            "step: 190, loss: 9.902994497679174e-05\n",
            "step: 200, loss: 3.610688145272434e-05\n",
            "step: 210, loss: 1.706901275611017e-05\n",
            "step: 220, loss: 2.559877793828491e-05\n",
            "step: 230, loss: 0.00021064931934233755\n",
            "step: 240, loss: 8.055589569266886e-05\n",
            "step: 250, loss: 1.9315173631184734e-05\n",
            "step: 260, loss: 0.0011424832046031952\n",
            "step: 270, loss: 4.021845597890206e-05\n",
            "step: 280, loss: 2.722992212511599e-05\n",
            "step: 290, loss: 0.0022940943017601967\n",
            "step: 300, loss: 1.9385986888664775e-05\n",
            "step: 310, loss: 0.00012624975352082402\n",
            "step: 320, loss: 0.0004607446026057005\n",
            "step: 330, loss: 3.68961482308805e-05\n",
            "step: 340, loss: 5.001311728847213e-05\n",
            "step: 350, loss: 2.8243997803656384e-05\n",
            "step: 360, loss: 8.216247078962624e-05\n",
            "step: 370, loss: 4.013631769339554e-05\n",
            "step: 380, loss: 2.8735190426232293e-05\n",
            "step: 390, loss: 0.010970995761454105\n",
            "step: 400, loss: 9.603436046745628e-05\n",
            "step: 410, loss: 0.0005105241434648633\n",
            "step: 420, loss: 0.004399530123919249\n",
            "step: 430, loss: 0.00017565286543685943\n",
            "step: 440, loss: 0.0015468198107555509\n",
            "step: 450, loss: 0.00038095080526545644\n",
            "step: 460, loss: 0.00028465516516007483\n",
            "step: 470, loss: 1.8365311916568317e-05\n",
            "step: 480, loss: 2.973545269924216e-05\n",
            "step: 490, loss: 2.0689038137788884e-05\n",
            "step: 500, loss: 1.3541184671339579e-05\n",
            "step: 510, loss: 8.366075780941173e-05\n",
            "step: 520, loss: 4.324637848185375e-05\n",
            "step: 530, loss: 3.015454058186151e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9280373831775701, f1=0.9288040949278734, best_f1=0.9258079198907602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.825344043027144e-05\n",
            "step: 10, loss: 2.106198553519789e-05\n",
            "step: 20, loss: 2.1095480406074785e-05\n",
            "step: 30, loss: 1.669642369961366e-05\n",
            "step: 40, loss: 0.011480805464088917\n",
            "step: 50, loss: 3.4396038245176896e-05\n",
            "step: 60, loss: 1.4110956726653967e-05\n",
            "step: 70, loss: 0.0007053720764815807\n",
            "step: 80, loss: 2.8868636945844628e-05\n",
            "step: 90, loss: 1.6074296581791714e-05\n",
            "step: 100, loss: 5.708055687136948e-05\n",
            "step: 110, loss: 0.0005345553508959711\n",
            "step: 120, loss: 0.00015766896831337363\n",
            "step: 130, loss: 0.1225668266415596\n",
            "step: 140, loss: 3.524774365359917e-05\n",
            "step: 150, loss: 5.9591937315417454e-05\n",
            "step: 160, loss: 4.1004524973686785e-05\n",
            "step: 170, loss: 1.466994854126824e-05\n",
            "step: 180, loss: 1.3824294910591561e-05\n",
            "step: 190, loss: 3.2191190257435665e-05\n",
            "step: 200, loss: 2.060011502180714e-05\n",
            "step: 210, loss: 0.00019804674957413226\n",
            "step: 220, loss: 2.3378393962047994e-05\n",
            "step: 230, loss: 0.00035688249045051634\n",
            "step: 240, loss: 1.9046845409320667e-05\n",
            "step: 250, loss: 5.75024496356491e-05\n",
            "step: 260, loss: 2.2675207219435833e-05\n",
            "step: 270, loss: 1.8514287148718722e-05\n",
            "step: 280, loss: 2.7203752324567176e-05\n",
            "step: 290, loss: 1.7388860214850865e-05\n",
            "step: 300, loss: 1.6308928024955094e-05\n",
            "step: 310, loss: 0.00011050837929360569\n",
            "step: 320, loss: 2.435659189359285e-05\n",
            "step: 330, loss: 5.3668543841922656e-05\n",
            "step: 340, loss: 1.1075168004026636e-05\n",
            "step: 350, loss: 9.152971870207693e-06\n",
            "step: 360, loss: 0.0008773290901444852\n",
            "step: 370, loss: 1.0825596291397233e-05\n",
            "step: 380, loss: 9.166065865429118e-05\n",
            "step: 390, loss: 0.0007199097890406847\n",
            "step: 400, loss: 2.0920715542160906e-05\n",
            "step: 410, loss: 5.2917865104973316e-05\n",
            "step: 420, loss: 5.8021956647280604e-05\n",
            "step: 430, loss: 2.5841651222435758e-05\n",
            "step: 440, loss: 0.04216241091489792\n",
            "step: 450, loss: 1.2937648534716573e-05\n",
            "step: 460, loss: 2.5341592845506966e-05\n",
            "step: 470, loss: 0.0011697913287207484\n",
            "step: 480, loss: 1.0575998203421477e-05\n",
            "step: 490, loss: 1.6074372979346663e-05\n",
            "step: 500, loss: 2.687212145247031e-05\n",
            "step: 510, loss: 2.9788381652906537e-05\n",
            "step: 520, loss: 1.681190224189777e-05\n",
            "step: 530, loss: 4.385184001876041e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9299953639313862, f1=0.9283402681460935, best_f1=0.9258079198907602\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 242.78it/s]\n",
            "load_f1 = 0.9308291342189647\n",
            "real_f1 = 0.929784304726939\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 243.14it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aa2c47-73a5-4f3c-feff-f51fc816ef5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5754161477088928\n",
            "step: 10, loss: 0.34897416830062866\n",
            "step: 20, loss: 0.33680588006973267\n",
            "step: 30, loss: 0.3245941698551178\n",
            "step: 40, loss: 0.17857354879379272\n",
            "step: 50, loss: 0.42970961332321167\n",
            "step: 60, loss: 0.21534954011440277\n",
            "step: 70, loss: 0.1345774233341217\n",
            "step: 80, loss: 0.19068831205368042\n",
            "step: 90, loss: 0.3903895616531372\n",
            "step: 100, loss: 0.3830837607383728\n",
            "step: 110, loss: 0.21810723841190338\n",
            "step: 120, loss: 0.2634891867637634\n",
            "step: 130, loss: 0.18180352449417114\n",
            "step: 140, loss: 0.22463566064834595\n",
            "step: 150, loss: 0.2992844581604004\n",
            "step: 160, loss: 0.4282751977443695\n",
            "step: 170, loss: 0.24396869540214539\n",
            "step: 180, loss: 0.17517004907131195\n",
            "step: 190, loss: 0.23118151724338531\n",
            "step: 200, loss: 0.2480921447277069\n",
            "step: 210, loss: 0.22242961823940277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5847457627118645, f1=0.6184210526315789, best_f1=0.6184210526315789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09215374290943146\n",
            "step: 10, loss: 0.19424279034137726\n",
            "step: 20, loss: 0.13589885830879211\n",
            "step: 30, loss: 0.25408855080604553\n",
            "step: 40, loss: 0.16608019173145294\n",
            "step: 50, loss: 0.1865537464618683\n",
            "step: 60, loss: 0.3740105628967285\n",
            "step: 70, loss: 0.1019446924328804\n",
            "step: 80, loss: 0.19353920221328735\n",
            "step: 90, loss: 0.0729399099946022\n",
            "step: 100, loss: 0.02645980753004551\n",
            "step: 110, loss: 0.1336207240819931\n",
            "step: 120, loss: 0.19244356453418732\n",
            "step: 130, loss: 0.045261938124895096\n",
            "step: 140, loss: 0.17426897585391998\n",
            "step: 150, loss: 0.2178621143102646\n",
            "step: 160, loss: 0.19809725880622864\n",
            "step: 170, loss: 0.15896113216876984\n",
            "step: 180, loss: 0.19714535772800446\n",
            "step: 190, loss: 0.2915029227733612\n",
            "step: 200, loss: 0.06213611364364624\n",
            "step: 210, loss: 0.2059468775987625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.5791666666666667, f1=0.6180257510729614, best_f1=0.6184210526315789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.056916821748018265\n",
            "step: 10, loss: 0.22601334750652313\n",
            "step: 20, loss: 0.18865181505680084\n",
            "step: 30, loss: 0.1775895059108734\n",
            "step: 40, loss: 0.2587548494338989\n",
            "step: 50, loss: 0.0677310973405838\n",
            "step: 60, loss: 0.17310650646686554\n",
            "step: 70, loss: 0.18380656838417053\n",
            "step: 80, loss: 0.15639671683311462\n",
            "step: 90, loss: 0.06564126163721085\n",
            "step: 100, loss: 0.38172706961631775\n",
            "step: 110, loss: 0.1758250892162323\n",
            "step: 120, loss: 0.06315157562494278\n",
            "step: 130, loss: 0.08544506877660751\n",
            "step: 140, loss: 0.08082298189401627\n",
            "step: 150, loss: 0.2948904037475586\n",
            "step: 160, loss: 0.04193758964538574\n",
            "step: 170, loss: 0.14176353812217712\n",
            "step: 180, loss: 0.07614223659038544\n",
            "step: 190, loss: 0.17220734059810638\n",
            "step: 200, loss: 0.04450476914644241\n",
            "step: 210, loss: 0.060477763414382935\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6174242424242424, f1=0.6460348162475823, best_f1=0.6460348162475823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07751989364624023\n",
            "step: 10, loss: 0.05858113616704941\n",
            "step: 20, loss: 0.10046353936195374\n",
            "step: 30, loss: 0.12040171027183533\n",
            "step: 40, loss: 0.043440911918878555\n",
            "step: 50, loss: 0.11289485543966293\n",
            "step: 60, loss: 0.08644522726535797\n",
            "step: 70, loss: 0.23739352822303772\n",
            "step: 80, loss: 0.0478249229490757\n",
            "step: 90, loss: 0.019197765737771988\n",
            "step: 100, loss: 0.22565466165542603\n",
            "step: 110, loss: 0.0758756548166275\n",
            "step: 120, loss: 0.15291297435760498\n",
            "step: 130, loss: 0.1585957109928131\n",
            "step: 140, loss: 0.12072647362947464\n",
            "step: 150, loss: 0.015219754539430141\n",
            "step: 160, loss: 0.062286268919706345\n",
            "step: 170, loss: 0.12730471789836884\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.30224016308784485\n",
            "step: 190, loss: 0.06246887147426605\n",
            "step: 200, loss: 0.15138830244541168\n",
            "step: 210, loss: 0.22620746493339539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6191369606003753, f1=0.6262230919765167, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031495027244091034\n",
            "step: 10, loss: 0.04400403052568436\n",
            "step: 20, loss: 0.1092258021235466\n",
            "step: 30, loss: 0.1526562124490738\n",
            "step: 40, loss: 0.11098208278417587\n",
            "step: 50, loss: 0.038952622562646866\n",
            "step: 60, loss: 0.12124894559383392\n",
            "step: 70, loss: 0.03506694361567497\n",
            "step: 80, loss: 0.01877041533589363\n",
            "step: 90, loss: 0.018823983147740364\n",
            "step: 100, loss: 0.007247685454785824\n",
            "step: 110, loss: 0.37933269143104553\n",
            "step: 120, loss: 0.03304028511047363\n",
            "step: 130, loss: 0.09857113659381866\n",
            "step: 140, loss: 0.13645945489406586\n",
            "step: 150, loss: 0.02339921146631241\n",
            "step: 160, loss: 0.22565779089927673\n",
            "step: 170, loss: 0.09083732962608337\n",
            "step: 180, loss: 0.10891518741846085\n",
            "step: 190, loss: 0.02743968740105629\n",
            "step: 200, loss: 0.032890912145376205\n",
            "step: 210, loss: 0.026693886145949364\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5989304812834224, f1=0.6088560885608856, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041234590113162994\n",
            "step: 10, loss: 0.07757211476564407\n",
            "step: 20, loss: 0.011473987251520157\n",
            "step: 30, loss: 0.0008761457866057754\n",
            "step: 40, loss: 0.12912943959236145\n",
            "step: 50, loss: 0.04195917770266533\n",
            "step: 60, loss: 0.11894697695970535\n",
            "step: 70, loss: 0.013871383853256702\n",
            "step: 80, loss: 0.020208412781357765\n",
            "step: 90, loss: 0.2789342701435089\n",
            "step: 100, loss: 0.0019679698161780834\n",
            "step: 110, loss: 0.004630249459296465\n",
            "step: 120, loss: 0.029633989557623863\n",
            "step: 130, loss: 0.05263344570994377\n",
            "step: 140, loss: 0.06180809438228607\n",
            "step: 150, loss: 0.017792511731386185\n",
            "step: 160, loss: 0.0008801759104244411\n",
            "step: 170, loss: 0.044599805027246475\n",
            "step: 180, loss: 0.01816042698919773\n",
            "step: 190, loss: 0.07421299070119858\n",
            "step: 200, loss: 0.002232134807854891\n",
            "step: 210, loss: 0.02166329324245453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6022304832713754, f1=0.6266924564796905, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01778092421591282\n",
            "step: 10, loss: 0.027529697865247726\n",
            "step: 20, loss: 0.02742069959640503\n",
            "step: 30, loss: 0.008053366094827652\n",
            "step: 40, loss: 0.007625663187354803\n",
            "step: 50, loss: 0.07642576098442078\n",
            "step: 60, loss: 0.025988861918449402\n",
            "step: 70, loss: 0.01455730851739645\n",
            "step: 80, loss: 0.015382027253508568\n",
            "step: 90, loss: 0.042977672070264816\n",
            "step: 100, loss: 0.01636553555727005\n",
            "step: 110, loss: 0.07840146124362946\n",
            "step: 120, loss: 0.06441865116357803\n",
            "step: 130, loss: 0.030124252662062645\n",
            "step: 140, loss: 0.02217555232346058\n",
            "step: 150, loss: 0.01294983271509409\n",
            "step: 160, loss: 0.022096781060099602\n",
            "step: 170, loss: 0.0027846277225762606\n",
            "step: 180, loss: 0.05177310109138489\n",
            "step: 190, loss: 0.07682502269744873\n",
            "step: 200, loss: 0.04156903177499771\n",
            "step: 210, loss: 0.03357374295592308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6045627376425856, f1=0.6141414141414142, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006667039822787046\n",
            "step: 10, loss: 0.029011180624365807\n",
            "step: 20, loss: 0.013170499354600906\n",
            "step: 30, loss: 0.02815677411854267\n",
            "step: 40, loss: 0.005652984604239464\n",
            "step: 50, loss: 0.0015820886474102736\n",
            "step: 60, loss: 0.005193675868213177\n",
            "step: 70, loss: 0.019417177885770798\n",
            "step: 80, loss: 0.045830339193344116\n",
            "step: 90, loss: 0.0076947519555687904\n",
            "step: 100, loss: 0.06707482039928436\n",
            "step: 110, loss: 0.05549147352576256\n",
            "step: 120, loss: 0.0009799775434657931\n",
            "step: 130, loss: 0.004160101059824228\n",
            "step: 140, loss: 0.09188925474882126\n",
            "step: 150, loss: 0.0057669151574373245\n",
            "step: 160, loss: 0.01292396429926157\n",
            "step: 170, loss: 0.02808372862637043\n",
            "step: 180, loss: 0.04501677304506302\n",
            "step: 190, loss: 0.006997307762503624\n",
            "step: 200, loss: 0.02116977982223034\n",
            "step: 210, loss: 0.12573958933353424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5927505330490406, f1=0.6065934065934065, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003554431488737464\n",
            "step: 10, loss: 0.012477933429181576\n",
            "step: 20, loss: 0.0016197956865653396\n",
            "step: 30, loss: 0.008464448153972626\n",
            "step: 40, loss: 0.039383772760629654\n",
            "step: 50, loss: 0.018687374889850616\n",
            "step: 60, loss: 0.05996128171682358\n",
            "step: 70, loss: 0.009101674892008305\n",
            "step: 80, loss: 0.001036471570841968\n",
            "step: 90, loss: 0.0008243973716162145\n",
            "step: 100, loss: 0.09330246597528458\n",
            "step: 110, loss: 0.005596187897026539\n",
            "step: 120, loss: 0.00044861395144835114\n",
            "step: 130, loss: 0.01002300065010786\n",
            "step: 140, loss: 0.013138187117874622\n",
            "step: 150, loss: 0.05883076786994934\n",
            "step: 160, loss: 0.0007739128195680678\n",
            "step: 170, loss: 0.0028359624557197094\n",
            "step: 180, loss: 0.08091127872467041\n",
            "step: 190, loss: 0.0075018578208982944\n",
            "step: 200, loss: 0.07800713181495667\n",
            "step: 210, loss: 0.033624082803726196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6019417475728155, f1=0.6309278350515464, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005543468054383993\n",
            "step: 10, loss: 0.0011802536901086569\n",
            "step: 20, loss: 0.00048385930131189525\n",
            "step: 30, loss: 0.0037716059014201164\n",
            "step: 40, loss: 0.0004392936534713954\n",
            "step: 50, loss: 0.02900778315961361\n",
            "step: 60, loss: 0.017684824764728546\n",
            "step: 70, loss: 0.01462011132389307\n",
            "step: 80, loss: 0.07530470937490463\n",
            "step: 90, loss: 0.011998825706541538\n",
            "step: 100, loss: 0.004915957804769278\n",
            "step: 110, loss: 0.03378044068813324\n",
            "step: 120, loss: 0.0009534343262203038\n",
            "step: 130, loss: 0.059244442731142044\n",
            "step: 140, loss: 0.0016663004644215107\n",
            "step: 150, loss: 0.01010469812899828\n",
            "step: 160, loss: 0.0047928160056471825\n",
            "step: 170, loss: 0.0003921282186638564\n",
            "step: 180, loss: 0.03247610479593277\n",
            "step: 190, loss: 0.054492752999067307\n",
            "step: 200, loss: 0.00022022011398803443\n",
            "step: 210, loss: 0.0030661243945360184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5777777777777777, f1=0.6027397260273972, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004382758866995573\n",
            "step: 10, loss: 0.027394382283091545\n",
            "step: 20, loss: 0.0016063520452007651\n",
            "step: 30, loss: 0.00017900655802804977\n",
            "step: 40, loss: 0.0012579687172546983\n",
            "step: 50, loss: 0.007555163465440273\n",
            "step: 60, loss: 0.017456725239753723\n",
            "step: 70, loss: 0.00013615679927170277\n",
            "step: 80, loss: 0.04921211302280426\n",
            "step: 90, loss: 0.00018276690389029682\n",
            "step: 100, loss: 0.005201069638133049\n",
            "step: 110, loss: 0.029693452641367912\n",
            "step: 120, loss: 0.012608510442078114\n",
            "step: 130, loss: 0.007048106752336025\n",
            "step: 140, loss: 0.00015561898180749267\n",
            "step: 150, loss: 7.553531031589955e-05\n",
            "step: 160, loss: 0.003952209372073412\n",
            "step: 170, loss: 0.00019789945508819073\n",
            "step: 180, loss: 0.003115114290267229\n",
            "step: 190, loss: 0.003241482423618436\n",
            "step: 200, loss: 0.0007256928947754204\n",
            "step: 210, loss: 0.0009230420691892505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5786163522012578, f1=0.5990990990990991, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004898762563243508\n",
            "step: 10, loss: 0.00016454202705062926\n",
            "step: 20, loss: 0.0024673324078321457\n",
            "step: 30, loss: 0.010574220679700375\n",
            "step: 40, loss: 0.0035417149774730206\n",
            "step: 50, loss: 0.0005476083606481552\n",
            "step: 60, loss: 0.010624601505696774\n",
            "step: 70, loss: 0.0212835855782032\n",
            "step: 80, loss: 0.00013003953790757805\n",
            "step: 90, loss: 0.0033166627399623394\n",
            "step: 100, loss: 0.0019397067371755838\n",
            "step: 110, loss: 0.02236640453338623\n",
            "step: 120, loss: 8.423945837421343e-05\n",
            "step: 130, loss: 0.00011465246643638238\n",
            "step: 140, loss: 0.00030877956305630505\n",
            "step: 150, loss: 0.005382104776799679\n",
            "step: 160, loss: 0.003842402948066592\n",
            "step: 170, loss: 0.0029839512426406145\n",
            "step: 180, loss: 0.0002803936367854476\n",
            "step: 190, loss: 0.00024134409613907337\n",
            "step: 200, loss: 0.016038792207837105\n",
            "step: 210, loss: 0.0026203731540590525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5780885780885782, f1=0.5915492957746479, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003959659021347761\n",
            "step: 10, loss: 0.0004168015730101615\n",
            "step: 20, loss: 0.029290398582816124\n",
            "step: 30, loss: 0.0020008222199976444\n",
            "step: 40, loss: 0.030802922323346138\n",
            "step: 50, loss: 0.0002691170375328511\n",
            "step: 60, loss: 0.0075044576078653336\n",
            "step: 70, loss: 0.035782698541879654\n",
            "step: 80, loss: 0.02288677543401718\n",
            "step: 90, loss: 0.00018215205636806786\n",
            "step: 100, loss: 0.0006781459087505937\n",
            "step: 110, loss: 0.0062377252615988255\n",
            "step: 120, loss: 0.00047855020966380835\n",
            "step: 130, loss: 0.00013661070261150599\n",
            "step: 140, loss: 0.00019852496916428208\n",
            "step: 150, loss: 0.0009397142566740513\n",
            "step: 160, loss: 0.0008228300721384585\n",
            "step: 170, loss: 0.0002426288992865011\n",
            "step: 180, loss: 0.05647513270378113\n",
            "step: 190, loss: 0.0003672963357530534\n",
            "step: 200, loss: 0.00026617434923537076\n",
            "step: 210, loss: 0.007963738404214382\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6109979633401222, f1=0.6177105831533477, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.459472933784127e-05\n",
            "step: 10, loss: 0.0009100593160837889\n",
            "step: 20, loss: 0.0041825734078884125\n",
            "step: 30, loss: 0.030371377244591713\n",
            "step: 40, loss: 0.00012392886856105179\n",
            "step: 50, loss: 0.00023335765581578016\n",
            "step: 60, loss: 0.00023585671442560852\n",
            "step: 70, loss: 9.679255890659988e-05\n",
            "step: 80, loss: 0.0001394123100908473\n",
            "step: 90, loss: 0.00024812135961838067\n",
            "step: 100, loss: 0.004431779496371746\n",
            "step: 110, loss: 0.00018474430544301867\n",
            "step: 120, loss: 0.030117614194750786\n",
            "step: 130, loss: 0.0048146797344088554\n",
            "step: 140, loss: 0.004255276173353195\n",
            "step: 150, loss: 0.0423605851829052\n",
            "step: 160, loss: 0.0008775480091571808\n",
            "step: 170, loss: 0.0003646161640062928\n",
            "step: 180, loss: 0.0230998732149601\n",
            "step: 190, loss: 0.0011648783693090081\n",
            "step: 200, loss: 0.00017647934146225452\n",
            "step: 210, loss: 0.011745554395020008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5995717344753747, f1=0.5995525727069352, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00031964152003638446\n",
            "step: 10, loss: 7.538275531260297e-05\n",
            "step: 20, loss: 0.0004654311924241483\n",
            "step: 30, loss: 0.0007594944909214973\n",
            "step: 40, loss: 0.0017785230884328485\n",
            "step: 50, loss: 9.939592564478517e-05\n",
            "step: 60, loss: 0.0006121420301496983\n",
            "step: 70, loss: 9.164350194623694e-05\n",
            "step: 80, loss: 0.00010517945338506252\n",
            "step: 90, loss: 0.0059972116723656654\n",
            "step: 100, loss: 0.00020162825239822268\n",
            "step: 110, loss: 9.634912566980347e-05\n",
            "step: 120, loss: 0.00017317943274974823\n",
            "step: 130, loss: 0.0004923117230646312\n",
            "step: 140, loss: 0.0005465620197355747\n",
            "step: 150, loss: 0.0002745723177213222\n",
            "step: 160, loss: 0.0008386206463910639\n",
            "step: 170, loss: 0.00012958265142515302\n",
            "step: 180, loss: 7.778783765388653e-05\n",
            "step: 190, loss: 0.000346519926097244\n",
            "step: 200, loss: 0.0011219721054658294\n",
            "step: 210, loss: 0.0002865456335712224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5906040268456376, f1=0.5903890160183066, best_f1=0.6262230919765167\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 449.30it/s]\n",
            "load_f1 = 0.6177024482109228\n",
            "real_f1 = 0.6162570888468809\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:18, 237.89it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXvTChDGgw8D"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eab173f-f7d3-44a6-aca5-10c51d058d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5781556367874146\n",
            "step: 10, loss: 0.3651178181171417\n",
            "step: 20, loss: 0.28146082162857056\n",
            "step: 30, loss: 0.4162902534008026\n",
            "step: 40, loss: 0.431736022233963\n",
            "step: 50, loss: 0.3078685402870178\n",
            "step: 60, loss: 0.27681249380111694\n",
            "step: 70, loss: 0.2715517580509186\n",
            "step: 80, loss: 0.21888846158981323\n",
            "step: 90, loss: 0.27903980016708374\n",
            "step: 100, loss: 0.3007155656814575\n",
            "step: 110, loss: 0.41351085901260376\n",
            "step: 120, loss: 0.13273294270038605\n",
            "step: 130, loss: 0.09032072126865387\n",
            "step: 140, loss: 0.04428458586335182\n",
            "step: 150, loss: 0.2377743273973465\n",
            "step: 160, loss: 0.11071816086769104\n",
            "step: 170, loss: 0.24011974036693573\n",
            "step: 180, loss: 0.0329873226583004\n",
            "step: 190, loss: 0.23725582659244537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6648199445983378, f1=0.6894736842105263, best_f1=0.6894736842105263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3292226493358612\n",
            "step: 10, loss: 0.030503004789352417\n",
            "step: 20, loss: 0.04860392585396767\n",
            "step: 30, loss: 0.13702496886253357\n",
            "step: 40, loss: 0.18070170283317566\n",
            "step: 50, loss: 0.08396454155445099\n",
            "step: 60, loss: 0.3262006342411041\n",
            "step: 70, loss: 0.1379903256893158\n",
            "step: 80, loss: 0.2805653214454651\n",
            "step: 90, loss: 0.18668873608112335\n",
            "step: 100, loss: 0.03578969091176987\n",
            "step: 110, loss: 0.19949667155742645\n",
            "step: 120, loss: 0.17020179331302643\n",
            "step: 130, loss: 0.1846172958612442\n",
            "step: 140, loss: 0.08279148489236832\n",
            "step: 150, loss: 0.08512203395366669\n",
            "step: 160, loss: 0.18918465077877045\n",
            "step: 170, loss: 0.09154900908470154\n",
            "step: 180, loss: 0.1347181648015976\n",
            "step: 190, loss: 0.047605015337467194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7783505154639175, f1=0.7712082262210797, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.052005548030138016\n",
            "step: 10, loss: 0.024145202711224556\n",
            "step: 20, loss: 0.0537530817091465\n",
            "step: 30, loss: 0.019120702520012856\n",
            "step: 40, loss: 0.034008197486400604\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 50, loss: 0.12317796796560287\n",
            "step: 60, loss: 0.048151031136512756\n",
            "step: 70, loss: 0.0979607030749321\n",
            "step: 80, loss: 0.0904306173324585\n",
            "step: 90, loss: 0.04781348630785942\n",
            "step: 100, loss: 0.04356329143047333\n",
            "step: 110, loss: 0.010820053517818451\n",
            "step: 120, loss: 0.005885446444153786\n",
            "step: 130, loss: 0.037091080099344254\n",
            "step: 140, loss: 0.013133429922163486\n",
            "step: 150, loss: 0.10760357230901718\n",
            "step: 160, loss: 0.014717970974743366\n",
            "step: 170, loss: 0.1325584352016449\n",
            "step: 180, loss: 0.04984697327017784\n",
            "step: 190, loss: 0.02680937573313713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7611548556430445, f1=0.7633587786259541, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01646447740495205\n",
            "step: 10, loss: 0.09173670411109924\n",
            "step: 20, loss: 0.005806737579405308\n",
            "step: 30, loss: 0.01804433949291706\n",
            "step: 40, loss: 0.008216137066483498\n",
            "step: 50, loss: 0.02476007118821144\n",
            "step: 60, loss: 0.00913719367235899\n",
            "step: 70, loss: 0.08946962654590607\n",
            "step: 80, loss: 0.0492778941988945\n",
            "step: 90, loss: 0.018214883282780647\n",
            "step: 100, loss: 0.028090786188840866\n",
            "step: 110, loss: 0.0037251010071486235\n",
            "step: 120, loss: 0.07813743501901627\n",
            "step: 130, loss: 0.10438932478427887\n",
            "step: 140, loss: 0.03410527482628822\n",
            "step: 150, loss: 0.003733607940375805\n",
            "step: 160, loss: 0.019943170249462128\n",
            "step: 170, loss: 0.05929122865200043\n",
            "step: 180, loss: 0.0039019808173179626\n",
            "step: 190, loss: 0.10178987681865692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.765498652291105, f1=0.7362637362637363, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.059490397572517395\n",
            "step: 10, loss: 0.014555852860212326\n",
            "step: 20, loss: 0.07736384868621826\n",
            "step: 30, loss: 0.028599750250577927\n",
            "step: 40, loss: 0.06379853188991547\n",
            "step: 50, loss: 0.11058677732944489\n",
            "step: 60, loss: 0.10603503882884979\n",
            "step: 70, loss: 0.009313062764704227\n",
            "step: 80, loss: 0.06134568527340889\n",
            "step: 90, loss: 0.014005156233906746\n",
            "step: 100, loss: 0.0019133922178298235\n",
            "step: 110, loss: 0.006024503614753485\n",
            "step: 120, loss: 0.001712458673864603\n",
            "step: 130, loss: 0.09378519654273987\n",
            "step: 140, loss: 0.04349592328071594\n",
            "step: 150, loss: 0.00976741686463356\n",
            "step: 160, loss: 0.09520883858203888\n",
            "step: 170, loss: 0.0018833102658390999\n",
            "step: 180, loss: 0.016767732799053192\n",
            "step: 190, loss: 0.05887294188141823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7613636363636365, f1=0.7392550143266476, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031315740197896957\n",
            "step: 10, loss: 0.11751872301101685\n",
            "step: 20, loss: 0.010304464027285576\n",
            "step: 30, loss: 0.001194456941448152\n",
            "step: 40, loss: 0.04937964305281639\n",
            "step: 50, loss: 0.008120400831103325\n",
            "step: 60, loss: 0.02148006483912468\n",
            "step: 70, loss: 0.023270834237337112\n",
            "step: 80, loss: 0.07851868122816086\n",
            "step: 90, loss: 0.020009370520710945\n",
            "step: 100, loss: 0.0007778799626976252\n",
            "step: 110, loss: 0.015138719230890274\n",
            "step: 120, loss: 0.015792282298207283\n",
            "step: 130, loss: 0.006239823065698147\n",
            "step: 140, loss: 0.14041130244731903\n",
            "step: 150, loss: 0.004379370249807835\n",
            "step: 160, loss: 0.041440851986408234\n",
            "step: 170, loss: 0.024154407903552055\n",
            "step: 180, loss: 0.0036286129616200924\n",
            "step: 190, loss: 0.06533271819353104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7539267015706806, f1=0.753315649867374, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05278968811035156\n",
            "step: 10, loss: 0.00590901542454958\n",
            "step: 20, loss: 0.24157099425792694\n",
            "step: 30, loss: 0.05090751126408577\n",
            "step: 40, loss: 0.003931001760065556\n",
            "step: 50, loss: 0.005446234252303839\n",
            "step: 60, loss: 0.004884723573923111\n",
            "step: 70, loss: 0.004603864625096321\n",
            "step: 80, loss: 0.0015509696677327156\n",
            "step: 90, loss: 0.01160832867026329\n",
            "step: 100, loss: 0.0007699996931478381\n",
            "step: 110, loss: 0.001050099148415029\n",
            "step: 120, loss: 0.0013658558018505573\n",
            "step: 130, loss: 0.0011198635911569\n",
            "step: 140, loss: 0.0007293605012819171\n",
            "step: 150, loss: 0.07394613325595856\n",
            "step: 160, loss: 0.010570647194981575\n",
            "step: 170, loss: 0.001692900201305747\n",
            "step: 180, loss: 0.009756483137607574\n",
            "step: 190, loss: 0.004276285879313946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7183462532299743, f1=0.733509234828496, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008616011589765549\n",
            "step: 10, loss: 0.08413570374250412\n",
            "step: 20, loss: 0.00390825979411602\n",
            "step: 30, loss: 0.047203682363033295\n",
            "step: 40, loss: 0.0013579040532931685\n",
            "step: 50, loss: 0.0329391248524189\n",
            "step: 60, loss: 0.0018450086936354637\n",
            "step: 70, loss: 0.0014761112397536635\n",
            "step: 80, loss: 0.00020237949502188712\n",
            "step: 90, loss: 0.0003920479502994567\n",
            "step: 100, loss: 0.054959218949079514\n",
            "step: 110, loss: 0.00021702238882426172\n",
            "step: 120, loss: 0.0005322650540620089\n",
            "step: 130, loss: 0.008772765286266804\n",
            "step: 140, loss: 0.0033537978306412697\n",
            "step: 150, loss: 0.0014638499123975635\n",
            "step: 160, loss: 0.0013429500395432115\n",
            "step: 170, loss: 0.001319325529038906\n",
            "step: 180, loss: 0.0027845248114317656\n",
            "step: 190, loss: 0.008395574055612087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7368421052631579, f1=0.7179487179487181, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017652444075793028\n",
            "step: 10, loss: 0.01109409797936678\n",
            "step: 20, loss: 0.0012315706117078662\n",
            "step: 30, loss: 0.0009347114246338606\n",
            "step: 40, loss: 0.0030479049310088158\n",
            "step: 50, loss: 0.0008420681115239859\n",
            "step: 60, loss: 0.0008006941643543541\n",
            "step: 70, loss: 0.00028883470804430544\n",
            "step: 80, loss: 0.001711062854155898\n",
            "step: 90, loss: 0.08329276740550995\n",
            "step: 100, loss: 0.049923595041036606\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.12441044300794601\n",
            "step: 120, loss: 0.0607636421918869\n",
            "step: 130, loss: 0.0012600296176970005\n",
            "step: 140, loss: 0.002371434587985277\n",
            "step: 150, loss: 0.0012854676460847259\n",
            "step: 160, loss: 0.0011021955870091915\n",
            "step: 170, loss: 0.001114308601245284\n",
            "step: 180, loss: 0.0008783533703535795\n",
            "step: 190, loss: 0.002409471897408366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7182320441988949, f1=0.7329545454545454, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005781440995633602\n",
            "step: 10, loss: 0.0005287314997985959\n",
            "step: 20, loss: 0.12259991466999054\n",
            "step: 30, loss: 0.006094001699239016\n",
            "step: 40, loss: 0.08862647414207458\n",
            "step: 50, loss: 0.005250099115073681\n",
            "step: 60, loss: 0.003388690995052457\n",
            "step: 70, loss: 0.004794489126652479\n",
            "step: 80, loss: 0.007285118568688631\n",
            "step: 90, loss: 0.018144501373171806\n",
            "step: 100, loss: 0.004628406371921301\n",
            "step: 110, loss: 0.009938178583979607\n",
            "step: 120, loss: 0.10293714702129364\n",
            "step: 130, loss: 0.0025090528652071953\n",
            "step: 140, loss: 0.0033427115995436907\n",
            "step: 150, loss: 0.048482827842235565\n",
            "step: 160, loss: 0.002523761475458741\n",
            "step: 170, loss: 0.0015887905610725284\n",
            "step: 180, loss: 0.007825538516044617\n",
            "step: 190, loss: 0.0035434970632195473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7258485639686684, f1=0.7547169811320755, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008330558193847537\n",
            "step: 10, loss: 0.0005510500050149858\n",
            "step: 20, loss: 0.0037446317728608847\n",
            "step: 30, loss: 0.007028399966657162\n",
            "step: 40, loss: 0.000465084973257035\n",
            "step: 50, loss: 0.004665793385356665\n",
            "step: 60, loss: 0.00042071426287293434\n",
            "step: 70, loss: 0.0013454153668135405\n",
            "step: 80, loss: 0.0007249650661833584\n",
            "step: 90, loss: 0.0001522720413049683\n",
            "step: 100, loss: 0.000383655889891088\n",
            "step: 110, loss: 0.00035442167427390814\n",
            "step: 120, loss: 0.00045174072147347033\n",
            "step: 130, loss: 0.009693591855466366\n",
            "step: 140, loss: 0.00995846837759018\n",
            "step: 150, loss: 0.00044161194819025695\n",
            "step: 160, loss: 0.003744250861927867\n",
            "step: 170, loss: 0.00028439785819500685\n",
            "step: 180, loss: 0.018313312903046608\n",
            "step: 190, loss: 0.0006600628839805722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7478753541076487, f1=0.7449856733524355, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003610410203691572\n",
            "step: 10, loss: 0.0034788944758474827\n",
            "step: 20, loss: 0.00018645540694706142\n",
            "step: 30, loss: 0.0005321408971212804\n",
            "step: 40, loss: 0.0021056567784398794\n",
            "step: 50, loss: 0.012632220983505249\n",
            "step: 60, loss: 0.0005879594245925546\n",
            "step: 70, loss: 0.00042256363667547703\n",
            "step: 80, loss: 0.0002565158065408468\n",
            "step: 90, loss: 0.0014621085720136762\n",
            "step: 100, loss: 0.00047146997530944645\n",
            "step: 110, loss: 0.00021389291214291006\n",
            "step: 120, loss: 0.05905042588710785\n",
            "step: 130, loss: 0.00027294931351207197\n",
            "step: 140, loss: 0.0008567156037315726\n",
            "step: 150, loss: 0.0004883937654085457\n",
            "step: 160, loss: 0.00010200437827734277\n",
            "step: 170, loss: 0.0010208056773990393\n",
            "step: 180, loss: 8.1390913692303e-05\n",
            "step: 190, loss: 0.00019491332932375371\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7592067988668556, f1=0.735042735042735, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006749593652784824\n",
            "step: 10, loss: 0.0003496795252431184\n",
            "step: 20, loss: 0.000160784344188869\n",
            "step: 30, loss: 0.00039345870027318597\n",
            "step: 40, loss: 0.00018011126667261124\n",
            "step: 50, loss: 0.0004739888245239854\n",
            "step: 60, loss: 0.0002035990182776004\n",
            "step: 70, loss: 0.0007342970347963274\n",
            "step: 80, loss: 0.0009894571267068386\n",
            "step: 90, loss: 0.00042712187860161066\n",
            "step: 100, loss: 0.00026153656654059887\n",
            "step: 110, loss: 0.001549556152895093\n",
            "step: 120, loss: 0.04072147607803345\n",
            "step: 130, loss: 0.0005265639629215002\n",
            "step: 140, loss: 0.0004876283055637032\n",
            "step: 150, loss: 0.00013315830437932163\n",
            "step: 160, loss: 0.00018781019025482237\n",
            "step: 170, loss: 0.0037873408291488886\n",
            "step: 180, loss: 0.00015828147297725081\n",
            "step: 190, loss: 0.013553522527217865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7627118644067796, f1=0.7282913165266107, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002899389364756644\n",
            "step: 10, loss: 0.0001610146282473579\n",
            "step: 20, loss: 0.0006373528158292174\n",
            "step: 30, loss: 7.826575892977417e-05\n",
            "step: 40, loss: 0.001036704401485622\n",
            "step: 50, loss: 0.0008241728064604104\n",
            "step: 60, loss: 0.0017631346127018332\n",
            "step: 70, loss: 0.00017791874415706843\n",
            "step: 80, loss: 0.000181359049747698\n",
            "step: 90, loss: 0.0012186556123197079\n",
            "step: 100, loss: 0.0016059176996350288\n",
            "step: 110, loss: 0.0013683594297617674\n",
            "step: 120, loss: 0.0030848965980112553\n",
            "step: 130, loss: 0.00039884535362944007\n",
            "step: 140, loss: 0.0008437593351118267\n",
            "step: 150, loss: 0.00015430942585226148\n",
            "step: 160, loss: 0.00024468559422530234\n",
            "step: 170, loss: 0.0002353420277358964\n",
            "step: 180, loss: 0.0003337294328957796\n",
            "step: 190, loss: 0.0006480647716671228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7605633802816902, f1=0.7247191011235956, best_f1=0.7712082262210797\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008156168041750789\n",
            "step: 10, loss: 0.00012554162822198123\n",
            "step: 20, loss: 0.0011248444207012653\n",
            "step: 30, loss: 0.00012077545397914946\n",
            "step: 40, loss: 0.0005397142376750708\n",
            "step: 50, loss: 0.00024067745835054666\n",
            "step: 60, loss: 0.000339469377649948\n",
            "step: 70, loss: 0.003619471797719598\n",
            "step: 80, loss: 0.00018069089855998755\n",
            "step: 90, loss: 0.000760513823479414\n",
            "step: 100, loss: 0.0005421423120424151\n",
            "step: 110, loss: 0.0010003320639953017\n",
            "step: 120, loss: 0.00015704514225944877\n",
            "step: 130, loss: 0.0001575939531903714\n",
            "step: 140, loss: 0.0024228240363299847\n",
            "step: 150, loss: 0.0005206016357988119\n",
            "step: 160, loss: 0.0021069336216896772\n",
            "step: 170, loss: 0.00028730789199471474\n",
            "step: 180, loss: 0.00042099205893464386\n",
            "step: 190, loss: 0.0015341655816882849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7584269662921349, f1=0.7211267605633802, best_f1=0.7712082262210797\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 154.29it/s]\n",
            "load_f1 = 0.5317647058823529\n",
            "real_f1 = 0.5181818181818182\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 174.44it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCCmtSggw8E"
      },
      "source": [
        "## DK TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbIZQYfgw8E"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d28581d-cbc5-4d68-b2b9-3b65ab3059e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6306821703910828\n",
            "step: 10, loss: 0.36284351348876953\n",
            "step: 20, loss: 0.2973402738571167\n",
            "step: 30, loss: 0.3935077488422394\n",
            "step: 40, loss: 0.2766757607460022\n",
            "step: 50, loss: 0.2982383966445923\n",
            "step: 60, loss: 0.21576091647148132\n",
            "step: 70, loss: 0.3728560209274292\n",
            "step: 80, loss: 0.41314560174942017\n",
            "step: 90, loss: 0.23155570030212402\n",
            "step: 100, loss: 0.1938527375459671\n",
            "step: 110, loss: 0.20932899415493011\n",
            "step: 120, loss: 0.21340259909629822\n",
            "step: 130, loss: 0.05852949991822243\n",
            "step: 140, loss: 0.20626647770404816\n",
            "step: 150, loss: 0.4107530117034912\n",
            "step: 160, loss: 0.23180192708969116\n",
            "step: 170, loss: 0.2268071174621582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7082405345211581, f1=0.7085201793721974, best_f1=0.7085201793721974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10224848240613937\n",
            "step: 10, loss: 0.27777716517448425\n",
            "step: 20, loss: 0.0765891969203949\n",
            "step: 30, loss: 0.20884814858436584\n",
            "step: 40, loss: 0.058085449039936066\n",
            "step: 50, loss: 0.16556879878044128\n",
            "step: 60, loss: 0.12990914285182953\n",
            "step: 70, loss: 0.2709573209285736\n",
            "step: 80, loss: 0.08118011057376862\n",
            "step: 90, loss: 0.20788989961147308\n",
            "step: 100, loss: 0.1983616203069687\n",
            "step: 110, loss: 0.04128403589129448\n",
            "step: 120, loss: 0.061320990324020386\n",
            "step: 130, loss: 0.07783524692058563\n",
            "step: 140, loss: 0.2827879786491394\n",
            "step: 150, loss: 0.18462884426116943\n",
            "step: 160, loss: 0.14389805495738983\n",
            "step: 170, loss: 0.046896085143089294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7785888077858881, f1=0.7599067599067598, best_f1=0.7599067599067598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09545227140188217\n",
            "step: 10, loss: 0.06240960955619812\n",
            "step: 20, loss: 0.018247684463858604\n",
            "step: 30, loss: 0.12008510529994965\n",
            "step: 40, loss: 0.07988769561052322\n",
            "step: 50, loss: 0.22565878927707672\n",
            "step: 60, loss: 0.08249008655548096\n",
            "step: 70, loss: 0.06648249179124832\n",
            "step: 80, loss: 0.01876227930188179\n",
            "step: 90, loss: 0.17157304286956787\n",
            "step: 100, loss: 0.06653354316949844\n",
            "step: 110, loss: 0.12331406772136688\n",
            "step: 120, loss: 0.04109431430697441\n",
            "step: 130, loss: 0.14459358155727386\n",
            "step: 140, loss: 0.10505212098360062\n",
            "step: 150, loss: 0.024299517273902893\n",
            "step: 160, loss: 0.06170161813497543\n",
            "step: 170, loss: 0.06355420500040054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.7748691099476439, f1=0.7858942065491185, best_f1=0.7599067599067598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028475504368543625\n",
            "step: 10, loss: 0.022709421813488007\n",
            "step: 20, loss: 0.006555144675076008\n",
            "step: 30, loss: 0.09322801232337952\n",
            "step: 40, loss: 0.009798322804272175\n",
            "step: 50, loss: 0.10953555256128311\n",
            "step: 60, loss: 0.1721712201833725\n",
            "step: 70, loss: 0.025228669866919518\n",
            "step: 80, loss: 0.13080261647701263\n",
            "step: 90, loss: 0.0846557542681694\n",
            "step: 100, loss: 0.17322812974452972\n",
            "step: 110, loss: 0.13749441504478455\n",
            "step: 120, loss: 0.007440519984811544\n",
            "step: 130, loss: 0.15530642867088318\n",
            "step: 140, loss: 0.02412419579923153\n",
            "step: 150, loss: 0.07127035409212112\n",
            "step: 160, loss: 0.20335786044597626\n",
            "step: 170, loss: 0.014884996227920055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7535545023696684, f1=0.7806004618937643, best_f1=0.7599067599067598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004570798482745886\n",
            "step: 10, loss: 0.013168823905289173\n",
            "step: 20, loss: 0.04260359704494476\n",
            "step: 30, loss: 0.022016990929841995\n",
            "step: 40, loss: 0.12827959656715393\n",
            "step: 50, loss: 0.030243681743741035\n",
            "step: 60, loss: 0.052696455270051956\n",
            "step: 70, loss: 0.1772533357143402\n",
            "step: 80, loss: 0.14135012030601501\n",
            "step: 90, loss: 0.07802389562129974\n",
            "step: 100, loss: 0.006185742560774088\n",
            "step: 110, loss: 0.05487363040447235\n",
            "step: 120, loss: 0.018313728272914886\n",
            "step: 130, loss: 0.05006563290953636\n",
            "step: 140, loss: 0.00669903913512826\n",
            "step: 150, loss: 0.008853505365550518\n",
            "step: 160, loss: 0.10006801038980484\n",
            "step: 170, loss: 0.004428825341165066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7599067599067598, f1=0.7703016241299303, best_f1=0.7599067599067598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018657641485333443\n",
            "step: 10, loss: 0.05683773383498192\n",
            "step: 20, loss: 0.02720525860786438\n",
            "step: 30, loss: 0.002854445483535528\n",
            "step: 40, loss: 0.014826199039816856\n",
            "step: 50, loss: 0.11003416776657104\n",
            "step: 60, loss: 0.01069454662501812\n",
            "step: 70, loss: 0.06922110170125961\n",
            "step: 80, loss: 0.016742577776312828\n",
            "step: 90, loss: 0.028059568256139755\n",
            "step: 100, loss: 0.005805283784866333\n",
            "step: 110, loss: 0.0012525838101282716\n",
            "step: 120, loss: 0.02566388249397278\n",
            "step: 130, loss: 0.009006829932332039\n",
            "step: 140, loss: 0.0435485914349556\n",
            "step: 150, loss: 0.010161536745727062\n",
            "step: 160, loss: 0.029279692098498344\n",
            "step: 170, loss: 0.010162964463233948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7648456057007125, f1=0.7754137115839244, best_f1=0.7599067599067598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004746209364384413\n",
            "step: 10, loss: 0.003224150277674198\n",
            "step: 20, loss: 0.003505786182358861\n",
            "step: 30, loss: 0.047513436526060104\n",
            "step: 40, loss: 0.0076171099208295345\n",
            "step: 50, loss: 0.012057202868163586\n",
            "step: 60, loss: 0.019150542095303535\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.002416432835161686\n",
            "step: 80, loss: 0.014028949663043022\n",
            "step: 90, loss: 0.0010464690858498216\n",
            "step: 100, loss: 0.0005819108919240534\n",
            "step: 110, loss: 0.03462501987814903\n",
            "step: 120, loss: 0.0010721449507400393\n",
            "step: 130, loss: 0.1338406503200531\n",
            "step: 140, loss: 0.0014807148836553097\n",
            "step: 150, loss: 0.005183515138924122\n",
            "step: 160, loss: 0.0008088255999609828\n",
            "step: 170, loss: 0.12175340205430984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7790432801822323, f1=0.7661469933184855, best_f1=0.7661469933184855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011850345879793167\n",
            "step: 10, loss: 0.006974825169891119\n",
            "step: 20, loss: 0.0034685914870351553\n",
            "step: 30, loss: 0.01989583484828472\n",
            "step: 40, loss: 0.00017785344971343875\n",
            "step: 50, loss: 0.006331956945359707\n",
            "step: 60, loss: 0.037461020052433014\n",
            "step: 70, loss: 0.0012885028263553977\n",
            "step: 80, loss: 0.004040556028485298\n",
            "step: 90, loss: 0.002369260648265481\n",
            "step: 100, loss: 0.03629009425640106\n",
            "step: 110, loss: 0.20171275734901428\n",
            "step: 120, loss: 0.008407055400311947\n",
            "step: 130, loss: 0.00473770359531045\n",
            "step: 140, loss: 0.0011068639578297734\n",
            "step: 150, loss: 0.0006869998760521412\n",
            "step: 160, loss: 0.005761581938713789\n",
            "step: 170, loss: 0.0029939545784145594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7612293144208038, f1=0.7788018433179723, best_f1=0.7661469933184855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005375835578888655\n",
            "step: 10, loss: 0.005033374764025211\n",
            "step: 20, loss: 0.00037251971662044525\n",
            "step: 30, loss: 0.021238502115011215\n",
            "step: 40, loss: 0.06410954892635345\n",
            "step: 50, loss: 0.0037274002097547054\n",
            "step: 60, loss: 0.06191441789269447\n",
            "step: 70, loss: 0.0019179085502400994\n",
            "step: 80, loss: 0.018140295520424843\n",
            "step: 90, loss: 0.02110704965889454\n",
            "step: 100, loss: 0.036127910017967224\n",
            "step: 110, loss: 0.00311398901976645\n",
            "step: 120, loss: 0.01890787109732628\n",
            "step: 130, loss: 0.0653165876865387\n",
            "step: 140, loss: 0.007220447529107332\n",
            "step: 150, loss: 0.0026820374187082052\n",
            "step: 160, loss: 0.0046689631417393684\n",
            "step: 170, loss: 0.0120914988219738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7912621359223301, f1=0.7990430622009569, best_f1=0.7990430622009569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06365052610635757\n",
            "step: 10, loss: 0.0023306452203541994\n",
            "step: 20, loss: 0.08312075585126877\n",
            "step: 30, loss: 0.10536537319421768\n",
            "step: 40, loss: 0.013754351064562798\n",
            "step: 50, loss: 0.037607431411743164\n",
            "step: 60, loss: 0.000747558893635869\n",
            "step: 70, loss: 0.007871626876294613\n",
            "step: 80, loss: 0.0026254872791469097\n",
            "step: 90, loss: 0.015232667326927185\n",
            "step: 100, loss: 0.011220937594771385\n",
            "step: 110, loss: 0.000496559776365757\n",
            "step: 120, loss: 0.0006974849966354668\n",
            "step: 130, loss: 0.0009944585617631674\n",
            "step: 140, loss: 0.018836528062820435\n",
            "step: 150, loss: 0.0014501443365588784\n",
            "step: 160, loss: 0.0014997334219515324\n",
            "step: 170, loss: 0.00020127395691815764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7486910994764399, f1=0.7930174563591021, best_f1=0.7990430622009569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003632934531196952\n",
            "step: 10, loss: 0.0329350009560585\n",
            "step: 20, loss: 0.003093196079134941\n",
            "step: 30, loss: 0.00014792087313253433\n",
            "step: 40, loss: 0.00020305249199736863\n",
            "step: 50, loss: 0.01242704689502716\n",
            "step: 60, loss: 0.0006511559477075934\n",
            "step: 70, loss: 0.0002469451283104718\n",
            "step: 80, loss: 0.03485646843910217\n",
            "step: 90, loss: 0.0004414098511915654\n",
            "step: 100, loss: 0.0029781947378069162\n",
            "step: 110, loss: 0.012367538176476955\n",
            "step: 120, loss: 0.00030079481075517833\n",
            "step: 130, loss: 0.00013183719420339912\n",
            "step: 140, loss: 0.025953449308872223\n",
            "step: 150, loss: 0.0020551211200654507\n",
            "step: 160, loss: 0.01160750724375248\n",
            "step: 170, loss: 0.0024422556161880493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7819905213270142, f1=0.7934272300469484, best_f1=0.7990430622009569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022231239825487137\n",
            "step: 10, loss: 0.10298644006252289\n",
            "step: 20, loss: 0.0034245920833200216\n",
            "step: 30, loss: 0.0005009055021218956\n",
            "step: 40, loss: 0.0006723743863403797\n",
            "step: 50, loss: 0.0015613138675689697\n",
            "step: 60, loss: 0.0007323637255467474\n",
            "step: 70, loss: 0.04461103677749634\n",
            "step: 80, loss: 0.0003845604369416833\n",
            "step: 90, loss: 0.0032001868821680546\n",
            "step: 100, loss: 0.00717270839959383\n",
            "step: 110, loss: 0.0007061440264806151\n",
            "step: 120, loss: 0.01171985361725092\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.012254010885953903\n",
            "step: 140, loss: 0.012196743860840797\n",
            "step: 150, loss: 0.01108810305595398\n",
            "step: 160, loss: 0.0004246599564794451\n",
            "step: 170, loss: 0.0002698799653444439\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7953488372093023, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05741036310791969\n",
            "step: 10, loss: 0.0005670576356351376\n",
            "step: 20, loss: 0.0015414867084473372\n",
            "step: 30, loss: 0.00013321560982149094\n",
            "step: 40, loss: 0.0007321593584492803\n",
            "step: 50, loss: 0.007814242504537106\n",
            "step: 60, loss: 0.004003442823886871\n",
            "step: 70, loss: 0.001742254476994276\n",
            "step: 80, loss: 0.003642813768237829\n",
            "step: 90, loss: 0.00011200350127182901\n",
            "step: 100, loss: 0.00025312029174529016\n",
            "step: 110, loss: 0.00045954110100865364\n",
            "step: 120, loss: 0.015145749785006046\n",
            "step: 130, loss: 0.00021304877009242773\n",
            "step: 140, loss: 0.0003473362303338945\n",
            "step: 150, loss: 0.008103648200631142\n",
            "step: 160, loss: 0.0006111040711402893\n",
            "step: 170, loss: 0.021289803087711334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.7990196078431373, f1=0.8019323671497586, best_f1=0.8019323671497586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001815431984141469\n",
            "step: 10, loss: 0.00021886274043936282\n",
            "step: 20, loss: 0.0002425064885756001\n",
            "step: 30, loss: 0.00027048320043832064\n",
            "step: 40, loss: 0.00020604619930963963\n",
            "step: 50, loss: 0.008441982790827751\n",
            "step: 60, loss: 0.00032945573912002146\n",
            "step: 70, loss: 0.0006503229378722608\n",
            "step: 80, loss: 0.0011323073413223028\n",
            "step: 90, loss: 0.0009523491025902331\n",
            "step: 100, loss: 0.00015644611266907305\n",
            "step: 110, loss: 0.00013899248733650893\n",
            "step: 120, loss: 0.024413563311100006\n",
            "step: 130, loss: 0.001796963857486844\n",
            "step: 140, loss: 0.00013484405644703656\n",
            "step: 150, loss: 0.0008002772810868919\n",
            "step: 160, loss: 0.00016170386516023427\n",
            "step: 170, loss: 0.0003732974291779101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7865707434052758, f1=0.7999999999999999, best_f1=0.8019323671497586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017131647327914834\n",
            "step: 10, loss: 0.0001945019030245021\n",
            "step: 20, loss: 0.035363588482141495\n",
            "step: 30, loss: 0.0005622311728075147\n",
            "step: 40, loss: 0.0014299058821052313\n",
            "step: 50, loss: 0.00021138113515917212\n",
            "step: 60, loss: 0.004660416394472122\n",
            "step: 70, loss: 0.00015328841982409358\n",
            "step: 80, loss: 0.0005605347687378526\n",
            "step: 90, loss: 0.002429481130093336\n",
            "step: 100, loss: 0.0003127149830106646\n",
            "step: 110, loss: 0.000126956743770279\n",
            "step: 120, loss: 0.0002279706677654758\n",
            "step: 130, loss: 0.00026218523271381855\n",
            "step: 140, loss: 0.00015536211139988154\n",
            "step: 150, loss: 0.0005007992149330676\n",
            "step: 160, loss: 0.0002115977549692616\n",
            "step: 170, loss: 0.00024877916439436376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7892156862745098, f1=0.8057553956834533, best_f1=0.8019323671497586\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 214.64it/s]\n",
            "load_f1 = 0.6081632653061224\n",
            "real_f1 = 0.6085192697768763\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 173.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HZE1zMQgw8F"
      },
      "source": [
        "## DK DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jg7qrOQgw8F"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9de27fe-eb0e-4f44-de21-72dcdf6e0134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6120129227638245\n",
            "step: 10, loss: 0.621228814125061\n",
            "step: 20, loss: 0.4405178129673004\n",
            "step: 30, loss: 0.22840973734855652\n",
            "step: 40, loss: 0.227983757853508\n",
            "step: 50, loss: 0.0506976880133152\n",
            "step: 60, loss: 0.058961909264326096\n",
            "step: 70, loss: 0.11547849327325821\n",
            "step: 80, loss: 0.03747252747416496\n",
            "step: 90, loss: 0.16254684329032898\n",
            "step: 100, loss: 0.008816612884402275\n",
            "step: 110, loss: 0.25396621227264404\n",
            "step: 120, loss: 0.01188000850379467\n",
            "step: 130, loss: 0.040173642337322235\n",
            "step: 140, loss: 0.04387220740318298\n",
            "step: 150, loss: 0.040450818836688995\n",
            "step: 160, loss: 0.13102063536643982\n",
            "step: 170, loss: 0.15929284691810608\n",
            "step: 180, loss: 0.04305034503340721\n",
            "step: 190, loss: 0.030837014317512512\n",
            "step: 200, loss: 0.149497851729393\n",
            "step: 210, loss: 0.02424868382513523\n",
            "step: 220, loss: 0.009606434032320976\n",
            "step: 230, loss: 0.026999756693840027\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9573770491803277, f1=0.9578713968957872, best_f1=0.9578713968957872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020124122500419617\n",
            "step: 10, loss: 0.025628870353102684\n",
            "step: 20, loss: 0.18921953439712524\n",
            "step: 30, loss: 0.15221524238586426\n",
            "step: 40, loss: 0.08972673863172531\n",
            "step: 50, loss: 0.008576219901442528\n",
            "step: 60, loss: 0.009662940166890621\n",
            "step: 70, loss: 0.0010515454923734069\n",
            "step: 80, loss: 0.09213929623365402\n",
            "step: 90, loss: 0.008797656744718552\n",
            "step: 100, loss: 0.15480025112628937\n",
            "step: 110, loss: 0.12422103434801102\n",
            "step: 120, loss: 0.11887349933385849\n",
            "step: 130, loss: 0.0066624777391552925\n",
            "step: 140, loss: 0.002849721582606435\n",
            "step: 150, loss: 0.004394428804516792\n",
            "step: 160, loss: 0.07540426403284073\n",
            "step: 170, loss: 0.0017878683283925056\n",
            "step: 180, loss: 0.08102727681398392\n",
            "step: 190, loss: 0.013884554617106915\n",
            "step: 200, loss: 0.0066092489287257195\n",
            "step: 210, loss: 0.002187700942158699\n",
            "step: 220, loss: 0.03703606501221657\n",
            "step: 230, loss: 0.06294190138578415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9648924122310306, f1=0.9727891156462585, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015432857908308506\n",
            "step: 10, loss: 0.005322849377989769\n",
            "step: 20, loss: 0.009454818442463875\n",
            "step: 30, loss: 0.00891720037907362\n",
            "step: 40, loss: 0.0414101667702198\n",
            "step: 50, loss: 0.009945322759449482\n",
            "step: 60, loss: 0.016662729904055595\n",
            "step: 70, loss: 0.0026916866190731525\n",
            "step: 80, loss: 0.0025856790598481894\n",
            "step: 90, loss: 0.018379464745521545\n",
            "step: 100, loss: 0.0005423406255431473\n",
            "step: 110, loss: 0.0006570170517079532\n",
            "step: 120, loss: 0.005061608739197254\n",
            "step: 130, loss: 0.0007404709467664361\n",
            "step: 140, loss: 0.01446995697915554\n",
            "step: 150, loss: 0.006027133669704199\n",
            "step: 160, loss: 0.06572186201810837\n",
            "step: 170, loss: 0.0220989678055048\n",
            "step: 180, loss: 0.028623143211007118\n",
            "step: 190, loss: 0.009920659475028515\n",
            "step: 200, loss: 0.004947133362293243\n",
            "step: 210, loss: 0.09021814167499542\n",
            "step: 220, loss: 0.002277111168950796\n",
            "step: 230, loss: 0.008570586331188679\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9711111111111111, f1=0.9688888888888889, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003128174867015332\n",
            "step: 10, loss: 0.0008814004249870777\n",
            "step: 20, loss: 0.0014638920547440648\n",
            "step: 30, loss: 0.0015647634863853455\n",
            "step: 40, loss: 0.0018144322093576193\n",
            "step: 50, loss: 0.00039254582952708006\n",
            "step: 60, loss: 0.0033676442690193653\n",
            "step: 70, loss: 0.002553738886490464\n",
            "step: 80, loss: 0.0024229097180068493\n",
            "step: 90, loss: 0.0024755820631980896\n",
            "step: 100, loss: 0.004753496963530779\n",
            "step: 110, loss: 0.0016515961615368724\n",
            "step: 120, loss: 0.0031697533559054136\n",
            "step: 130, loss: 0.0024250366259366274\n",
            "step: 140, loss: 0.00070545932976529\n",
            "step: 150, loss: 0.10661470144987106\n",
            "step: 160, loss: 0.10743238776922226\n",
            "step: 170, loss: 0.007509829010814428\n",
            "step: 180, loss: 0.003530848305672407\n",
            "step: 190, loss: 0.004375654272735119\n",
            "step: 200, loss: 0.002177590737119317\n",
            "step: 210, loss: 0.03617164492607117\n",
            "step: 220, loss: 0.0005616039852611721\n",
            "step: 230, loss: 0.006030102726072073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9683257918552037, f1=0.971815107102593, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031601432710886\n",
            "step: 10, loss: 0.0007484462694264948\n",
            "step: 20, loss: 0.06988026201725006\n",
            "step: 30, loss: 0.014145351946353912\n",
            "step: 40, loss: 0.001450452720746398\n",
            "step: 50, loss: 0.0022391248494386673\n",
            "step: 60, loss: 0.0008865951676853001\n",
            "step: 70, loss: 0.0020204265601933002\n",
            "step: 80, loss: 0.0005720174522139132\n",
            "step: 90, loss: 0.0010997939389199018\n",
            "step: 100, loss: 0.00020483025582507253\n",
            "step: 110, loss: 0.00020233314717188478\n",
            "step: 120, loss: 9.735061757965013e-05\n",
            "step: 130, loss: 0.002236417029052973\n",
            "step: 140, loss: 0.0022365874610841274\n",
            "step: 150, loss: 0.003968460951000452\n",
            "step: 160, loss: 0.016722505912184715\n",
            "step: 170, loss: 0.015748126432299614\n",
            "step: 180, loss: 0.0026299594901502132\n",
            "step: 190, loss: 0.07534866780042648\n",
            "step: 200, loss: 0.013050246052443981\n",
            "step: 210, loss: 0.02088979259133339\n",
            "step: 220, loss: 0.0006139914621599019\n",
            "step: 230, loss: 0.0037941972259432077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9707865168539327, f1=0.9718785151856018, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027853590436279774\n",
            "step: 10, loss: 0.001272656605578959\n",
            "step: 20, loss: 0.0010601339163258672\n",
            "step: 30, loss: 0.0004089047433808446\n",
            "step: 40, loss: 0.00022880033066030592\n",
            "step: 50, loss: 0.0024288659915328026\n",
            "step: 60, loss: 0.00018790891044773161\n",
            "step: 70, loss: 0.0006412736838683486\n",
            "step: 80, loss: 0.0005934145883657038\n",
            "step: 90, loss: 0.005353877786546946\n",
            "step: 100, loss: 0.007597532123327255\n",
            "step: 110, loss: 0.00072244624607265\n",
            "step: 120, loss: 0.00033337585045956075\n",
            "step: 130, loss: 0.0003062129835598171\n",
            "step: 140, loss: 0.0006191994762048125\n",
            "step: 150, loss: 0.010220308788120747\n",
            "step: 160, loss: 0.0017253314144909382\n",
            "step: 170, loss: 0.0006297912332229316\n",
            "step: 180, loss: 0.020365282893180847\n",
            "step: 190, loss: 0.014664997346699238\n",
            "step: 200, loss: 0.0005238482845015824\n",
            "step: 210, loss: 0.0004899489576928318\n",
            "step: 220, loss: 0.00025281761190854013\n",
            "step: 230, loss: 0.024851134046912193\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9731543624161074, f1=0.9700332963374029, best_f1=0.9700332963374029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004334199707955122\n",
            "step: 10, loss: 0.0002057678357232362\n",
            "step: 20, loss: 0.0010083595989271998\n",
            "step: 30, loss: 0.00031564326491206884\n",
            "step: 40, loss: 0.0007534394389949739\n",
            "step: 50, loss: 0.00026829898706637323\n",
            "step: 60, loss: 0.0010782700264826417\n",
            "step: 70, loss: 0.03942576050758362\n",
            "step: 80, loss: 0.06732559204101562\n",
            "step: 90, loss: 0.001104413764551282\n",
            "step: 100, loss: 0.14960098266601562\n",
            "step: 110, loss: 0.00039901304990053177\n",
            "step: 120, loss: 0.0006186665268614888\n",
            "step: 130, loss: 0.00022834964329376817\n",
            "step: 140, loss: 0.0021142156329005957\n",
            "step: 150, loss: 0.0006785074947401881\n",
            "step: 160, loss: 0.07733098417520523\n",
            "step: 170, loss: 0.001345776254311204\n",
            "step: 180, loss: 0.0024844282306730747\n",
            "step: 190, loss: 0.00036947568878531456\n",
            "step: 200, loss: 0.016160128638148308\n",
            "step: 210, loss: 0.00019470539700705558\n",
            "step: 220, loss: 0.0009778202511370182\n",
            "step: 230, loss: 0.0007076504989527166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9719416386083053, f1=0.972129319955407, best_f1=0.9700332963374029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023148389300331473\n",
            "step: 10, loss: 0.0006266393465921283\n",
            "step: 20, loss: 0.00021239783382043242\n",
            "step: 30, loss: 0.0002032161719398573\n",
            "step: 40, loss: 0.0019259699620306492\n",
            "step: 50, loss: 0.0002967270265799016\n",
            "step: 60, loss: 0.0007740239379927516\n",
            "step: 70, loss: 0.02282172441482544\n",
            "step: 80, loss: 0.0005114414962008595\n",
            "step: 90, loss: 0.0001647252938710153\n",
            "step: 100, loss: 0.0008732809801585972\n",
            "step: 110, loss: 0.02113417536020279\n",
            "step: 120, loss: 0.004625893663614988\n",
            "step: 130, loss: 0.007793961558490992\n",
            "step: 140, loss: 0.00019770061771851033\n",
            "step: 150, loss: 0.00015257333870977163\n",
            "step: 160, loss: 0.0001396393054164946\n",
            "step: 170, loss: 0.0003534733841661364\n",
            "step: 180, loss: 0.030885107815265656\n",
            "step: 190, loss: 0.0005147689953446388\n",
            "step: 200, loss: 0.0012387807946652174\n",
            "step: 210, loss: 0.00036136762355454266\n",
            "step: 220, loss: 0.04323078691959381\n",
            "step: 230, loss: 0.00016517704352736473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9721913236929923, f1=0.9678848283499446, best_f1=0.9700332963374029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015306135173887014\n",
            "step: 10, loss: 0.00023315080034080893\n",
            "step: 20, loss: 0.0015472613740712404\n",
            "step: 30, loss: 7.799669401720166e-05\n",
            "step: 40, loss: 0.07457059621810913\n",
            "step: 50, loss: 0.00017675806884653866\n",
            "step: 60, loss: 0.00027276729815639555\n",
            "step: 70, loss: 0.00027789882733486593\n",
            "step: 80, loss: 0.0012540628667920828\n",
            "step: 90, loss: 0.0008973304647952318\n",
            "step: 100, loss: 0.00016988179413601756\n",
            "step: 110, loss: 0.0001234780065715313\n",
            "step: 120, loss: 0.0005167751805856824\n",
            "step: 130, loss: 0.00014640139124821872\n",
            "step: 140, loss: 9.57196025410667e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 150, loss: 0.00890036765486002\n",
            "step: 160, loss: 8.852846804074943e-05\n",
            "step: 170, loss: 0.0007345370249822736\n",
            "step: 180, loss: 0.008379649370908737\n",
            "step: 190, loss: 0.00021152749832253903\n",
            "step: 200, loss: 0.15122286975383759\n",
            "step: 210, loss: 0.02961094118654728\n",
            "step: 220, loss: 0.0004090812581125647\n",
            "step: 230, loss: 0.0008820408256724477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9696969696969697, f1=0.972129319955407, best_f1=0.9700332963374029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002383348997682333\n",
            "step: 10, loss: 0.00013760995352640748\n",
            "step: 20, loss: 0.0008131799986585975\n",
            "step: 30, loss: 0.00019101840734947473\n",
            "step: 40, loss: 0.0003319120442029089\n",
            "step: 50, loss: 0.00016415806021541357\n",
            "step: 60, loss: 0.0266561359167099\n",
            "step: 70, loss: 0.0008864377159625292\n",
            "step: 80, loss: 0.00016513647278770804\n",
            "step: 90, loss: 0.00019295979291200638\n",
            "step: 100, loss: 8.51886288728565e-05\n",
            "step: 110, loss: 0.00010285902681061998\n",
            "step: 120, loss: 0.00014329739497043192\n",
            "step: 130, loss: 0.0001972253230633214\n",
            "step: 140, loss: 0.036194272339344025\n",
            "step: 150, loss: 0.01982288807630539\n",
            "step: 160, loss: 8.171053923433647e-05\n",
            "step: 170, loss: 5.53015161131043e-05\n",
            "step: 180, loss: 8.93485193955712e-05\n",
            "step: 190, loss: 0.00587886106222868\n",
            "step: 200, loss: 6.872640369692817e-05\n",
            "step: 210, loss: 0.00018338541849516332\n",
            "step: 220, loss: 0.00015917923883534968\n",
            "step: 230, loss: 0.029521355405449867\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9753914988814317, f1=0.9742441209406495, best_f1=0.9742441209406495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019111167057417333\n",
            "step: 10, loss: 0.00046766424202360213\n",
            "step: 20, loss: 0.0008151134243234992\n",
            "step: 30, loss: 0.005923447664827108\n",
            "step: 40, loss: 0.00018541907775215805\n",
            "step: 50, loss: 0.00012922289897687733\n",
            "step: 60, loss: 0.00031561427749693394\n",
            "step: 70, loss: 0.0018834611400961876\n",
            "step: 80, loss: 0.00013504078378900886\n",
            "step: 90, loss: 9.76806550170295e-05\n",
            "step: 100, loss: 0.00023641531879547983\n",
            "step: 110, loss: 0.00567863741889596\n",
            "step: 120, loss: 0.00039643043419346213\n",
            "step: 130, loss: 7.568900764454156e-05\n",
            "step: 140, loss: 0.0001726011687424034\n",
            "step: 150, loss: 0.016936028376221657\n",
            "step: 160, loss: 0.0002248376840725541\n",
            "step: 170, loss: 0.0035407179966568947\n",
            "step: 180, loss: 0.0038786442019045353\n",
            "step: 190, loss: 0.0002283173962496221\n",
            "step: 200, loss: 0.0006870842189528048\n",
            "step: 210, loss: 5.193382821744308e-05\n",
            "step: 220, loss: 8.61346343299374e-05\n",
            "step: 230, loss: 9.220497304340824e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9754464285714286, f1=0.9721913236929923, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011702313349815086\n",
            "step: 10, loss: 6.703726103296503e-05\n",
            "step: 20, loss: 0.00017264894268009812\n",
            "step: 30, loss: 0.00028005754575133324\n",
            "step: 40, loss: 0.00016858644085004926\n",
            "step: 50, loss: 0.0028785422910004854\n",
            "step: 60, loss: 0.00018527822976466268\n",
            "step: 70, loss: 7.805446512065828e-05\n",
            "step: 80, loss: 6.643019150942564e-05\n",
            "step: 90, loss: 0.00037274573696777225\n",
            "step: 100, loss: 0.0003862883895635605\n",
            "step: 110, loss: 6.54870891594328e-05\n",
            "step: 120, loss: 5.754351877840236e-05\n",
            "step: 130, loss: 7.889376138336957e-05\n",
            "step: 140, loss: 0.0007238443358801305\n",
            "step: 150, loss: 0.0006540314061567187\n",
            "step: 160, loss: 9.138359746430069e-05\n",
            "step: 170, loss: 0.00019394466653466225\n",
            "step: 180, loss: 0.004147544037550688\n",
            "step: 190, loss: 5.6047509133350104e-05\n",
            "step: 200, loss: 3.100755566265434e-05\n",
            "step: 210, loss: 5.48173047718592e-05\n",
            "step: 220, loss: 7.147278665797785e-05\n",
            "step: 230, loss: 0.06047368794679642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9754464285714286, f1=0.9711111111111111, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.198740058811381e-05\n",
            "step: 10, loss: 7.531729352194816e-05\n",
            "step: 20, loss: 7.92374339653179e-05\n",
            "step: 30, loss: 6.503114127554e-05\n",
            "step: 40, loss: 0.00011975361121585593\n",
            "step: 50, loss: 0.00013117125490680337\n",
            "step: 60, loss: 6.99805241310969e-05\n",
            "step: 70, loss: 6.694883632007986e-05\n",
            "step: 80, loss: 8.964839071268216e-05\n",
            "step: 90, loss: 0.00023276270076166838\n",
            "step: 100, loss: 6.642071093665436e-05\n",
            "step: 110, loss: 0.002167933387681842\n",
            "step: 120, loss: 0.028799254447221756\n",
            "step: 130, loss: 6.137298623798415e-05\n",
            "step: 140, loss: 6.870649667689577e-05\n",
            "step: 150, loss: 9.850790956988931e-05\n",
            "step: 160, loss: 6.465909245889634e-05\n",
            "step: 170, loss: 0.0004375089192762971\n",
            "step: 180, loss: 5.5741562391631305e-05\n",
            "step: 190, loss: 6.193630542838946e-05\n",
            "step: 200, loss: 0.0004232917563058436\n",
            "step: 210, loss: 4.793516200152226e-05\n",
            "step: 220, loss: 0.001308539416640997\n",
            "step: 230, loss: 6.19937782175839e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9754464285714286, f1=0.9711111111111111, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.665656059747562e-05\n",
            "step: 10, loss: 0.00016424212662968785\n",
            "step: 20, loss: 6.415556708816439e-05\n",
            "step: 30, loss: 0.00011194588296348229\n",
            "step: 40, loss: 0.012724898755550385\n",
            "step: 50, loss: 6.124695210019127e-05\n",
            "step: 60, loss: 8.72412565513514e-05\n",
            "step: 70, loss: 0.00026866840198636055\n",
            "step: 80, loss: 5.072389103588648e-05\n",
            "step: 90, loss: 7.698195986449718e-05\n",
            "step: 100, loss: 4.877166065853089e-05\n",
            "step: 110, loss: 8.626198541605845e-05\n",
            "step: 120, loss: 3.179377017659135e-05\n",
            "step: 130, loss: 5.144305032445118e-05\n",
            "step: 140, loss: 5.362946103559807e-05\n",
            "step: 150, loss: 4.3255346099613234e-05\n",
            "step: 160, loss: 0.0006181105272844434\n",
            "step: 170, loss: 5.381505616242066e-05\n",
            "step: 180, loss: 0.00012286132550798357\n",
            "step: 190, loss: 0.0006981941987760365\n",
            "step: 200, loss: 4.827968223253265e-05\n",
            "step: 210, loss: 0.002800901886075735\n",
            "step: 220, loss: 0.00018480255675967783\n",
            "step: 230, loss: 9.419220441486686e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9754464285714286, f1=0.9743589743589743, best_f1=0.9721913236929923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.756004662136547e-05\n",
            "step: 10, loss: 3.19110658892896e-05\n",
            "step: 20, loss: 4.4846205128123984e-05\n",
            "step: 30, loss: 0.00015690535656176507\n",
            "step: 40, loss: 0.00011378325871191919\n",
            "step: 50, loss: 0.052751846611499786\n",
            "step: 60, loss: 5.196068377699703e-05\n",
            "step: 70, loss: 6.242353265406564e-05\n",
            "step: 80, loss: 5.311435234034434e-05\n",
            "step: 90, loss: 9.762862464413047e-05\n",
            "step: 100, loss: 7.011849083937705e-05\n",
            "step: 110, loss: 4.328460636315867e-05\n",
            "step: 120, loss: 8.638645522296429e-05\n",
            "step: 130, loss: 0.00037625362165272236\n",
            "step: 140, loss: 4.691272624768317e-05\n",
            "step: 150, loss: 0.00012940031592734158\n",
            "step: 160, loss: 7.653558714082465e-05\n",
            "step: 170, loss: 0.00017302476044278592\n",
            "step: 180, loss: 3.153716534143314e-05\n",
            "step: 190, loss: 5.9757116105174646e-05\n",
            "step: 200, loss: 4.3427244236227125e-05\n",
            "step: 210, loss: 0.00010631157056195661\n",
            "step: 220, loss: 6.603678048122674e-05\n",
            "step: 230, loss: 6.677297642454505e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9754464285714286, f1=0.9721913236929923, best_f1=0.9721913236929923\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 154.82it/s]\n",
            "load_f1 = 0.9743016759776536\n",
            "real_f1 = 0.9732739420935412\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 174.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEeB2m-gw8G"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b305be-68d0-428d-b813-9d77a308c7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6401538848876953\n",
            "step: 10, loss: 0.5271133184432983\n",
            "step: 20, loss: 0.5395477414131165\n",
            "step: 30, loss: 0.2530089020729065\n",
            "step: 40, loss: 0.209387868642807\n",
            "step: 50, loss: 0.10199002921581268\n",
            "step: 60, loss: 0.12294109165668488\n",
            "step: 70, loss: 0.10537386685609818\n",
            "step: 80, loss: 0.07540570199489594\n",
            "step: 90, loss: 0.26539281010627747\n",
            "step: 100, loss: 0.06978128105401993\n",
            "step: 110, loss: 0.0655626729130745\n",
            "step: 120, loss: 0.11494070291519165\n",
            "step: 130, loss: 0.10800017416477203\n",
            "step: 140, loss: 0.07536771148443222\n",
            "step: 150, loss: 0.021639076992869377\n",
            "step: 160, loss: 0.1441420167684555\n",
            "step: 170, loss: 0.27095791697502136\n",
            "step: 180, loss: 0.06250543892383575\n",
            "step: 190, loss: 0.02859683893620968\n",
            "step: 200, loss: 0.08896791189908981\n",
            "step: 210, loss: 0.09104331582784653\n",
            "step: 220, loss: 0.32683777809143066\n",
            "step: 230, loss: 0.2027566283941269\n",
            "step: 240, loss: 0.07302534580230713\n",
            "step: 250, loss: 0.03247213363647461\n",
            "step: 260, loss: 0.043943844735622406\n",
            "step: 270, loss: 0.0671134740114212\n",
            "step: 280, loss: 0.094244584441185\n",
            "step: 290, loss: 0.09308168292045593\n",
            "step: 300, loss: 0.07471570372581482\n",
            "step: 310, loss: 0.37389904260635376\n",
            "step: 320, loss: 0.15756180882453918\n",
            "step: 330, loss: 0.07670602947473526\n",
            "step: 340, loss: 0.06688523292541504\n",
            "step: 350, loss: 0.007912587374448776\n",
            "step: 360, loss: 0.08515890687704086\n",
            "step: 370, loss: 0.12170262634754181\n",
            "step: 380, loss: 0.07265076786279678\n",
            "step: 390, loss: 0.23989136517047882\n",
            "step: 400, loss: 0.2701171636581421\n",
            "step: 410, loss: 0.1447092443704605\n",
            "step: 420, loss: 0.03716454282402992\n",
            "step: 430, loss: 0.12214881181716919\n",
            "step: 440, loss: 0.019716527312994003\n",
            "step: 450, loss: 0.015182485803961754\n",
            "step: 460, loss: 0.002188965678215027\n",
            "step: 470, loss: 0.18527346849441528\n",
            "step: 480, loss: 0.20190614461898804\n",
            "step: 490, loss: 0.0721539705991745\n",
            "step: 500, loss: 0.3154245615005493\n",
            "step: 510, loss: 0.06388755887746811\n",
            "step: 520, loss: 0.07313162088394165\n",
            "step: 530, loss: 0.002244864124804735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9198473282442748, f1=0.9177365668093201, best_f1=0.9177365668093201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06201046705245972\n",
            "step: 10, loss: 0.03548178821802139\n",
            "step: 20, loss: 0.039283204823732376\n",
            "step: 30, loss: 0.08089764416217804\n",
            "step: 40, loss: 0.049668844789266586\n",
            "step: 50, loss: 0.10941733419895172\n",
            "step: 60, loss: 0.03163411468267441\n",
            "step: 70, loss: 0.03564590960741043\n",
            "step: 80, loss: 0.07304545491933823\n",
            "step: 90, loss: 0.004586185794323683\n",
            "step: 100, loss: 0.012803740799427032\n",
            "step: 110, loss: 0.04239154979586601\n",
            "step: 120, loss: 0.15766218304634094\n",
            "step: 130, loss: 0.0419008694589138\n",
            "step: 140, loss: 0.030303968116641045\n",
            "step: 150, loss: 0.051222920417785645\n",
            "step: 160, loss: 0.02036690153181553\n",
            "step: 170, loss: 0.1762503832578659\n",
            "step: 180, loss: 0.011121721006929874\n",
            "step: 190, loss: 0.026619132608175278\n",
            "step: 200, loss: 0.04988071694970131\n",
            "step: 210, loss: 0.0226685032248497\n",
            "step: 220, loss: 0.04639068990945816\n",
            "step: 230, loss: 0.041428256779909134\n",
            "step: 240, loss: 0.027641039341688156\n",
            "step: 250, loss: 0.030928166583180428\n",
            "step: 260, loss: 0.0109483003616333\n",
            "step: 270, loss: 0.1758490949869156\n",
            "step: 280, loss: 0.13527828454971313\n",
            "step: 290, loss: 0.05138218402862549\n",
            "step: 300, loss: 0.19358640909194946\n",
            "step: 310, loss: 0.004739953204989433\n",
            "step: 320, loss: 0.17267820239067078\n",
            "step: 330, loss: 0.034636206924915314\n",
            "step: 340, loss: 0.01977488212287426\n",
            "step: 350, loss: 0.005569325760006905\n",
            "step: 360, loss: 0.0770965963602066\n",
            "step: 370, loss: 0.0976511761546135\n",
            "step: 380, loss: 0.06469468772411346\n",
            "step: 390, loss: 0.05777721107006073\n",
            "step: 400, loss: 0.05559117719531059\n",
            "step: 410, loss: 0.005029239691793919\n",
            "step: 420, loss: 0.01465305034071207\n",
            "step: 430, loss: 0.01086980476975441\n",
            "step: 440, loss: 0.14130321145057678\n",
            "step: 450, loss: 0.019810184836387634\n",
            "step: 460, loss: 0.06497209519147873\n",
            "step: 470, loss: 0.0676315501332283\n",
            "step: 480, loss: 0.18011625111103058\n",
            "step: 490, loss: 0.04334309324622154\n",
            "step: 500, loss: 0.1395157277584076\n",
            "step: 510, loss: 0.010695385746657848\n",
            "step: 520, loss: 0.04028969258069992\n",
            "step: 530, loss: 0.008257491514086723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9255813953488372, f1=0.927806241266884, best_f1=0.927806241266884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13477864861488342\n",
            "step: 10, loss: 0.025313373655080795\n",
            "step: 20, loss: 0.05544145405292511\n",
            "step: 30, loss: 0.09590356796979904\n",
            "step: 40, loss: 0.01427905447781086\n",
            "step: 50, loss: 0.18011771142482758\n",
            "step: 60, loss: 0.023379404097795486\n",
            "step: 70, loss: 0.01605592668056488\n",
            "step: 80, loss: 0.0036873691715300083\n",
            "step: 90, loss: 0.008343779481947422\n",
            "step: 100, loss: 0.06206585094332695\n",
            "step: 110, loss: 0.010071770288050175\n",
            "step: 120, loss: 0.0012836792739108205\n",
            "step: 130, loss: 0.005681023001670837\n",
            "step: 140, loss: 0.03293886035680771\n",
            "step: 150, loss: 0.026647862046957016\n",
            "step: 160, loss: 0.006347024347633123\n",
            "step: 170, loss: 0.016975073143839836\n",
            "step: 180, loss: 0.010522155091166496\n",
            "step: 190, loss: 0.007389269769191742\n",
            "step: 200, loss: 0.050525516271591187\n",
            "step: 210, loss: 0.01714610494673252\n",
            "step: 220, loss: 0.030967634171247482\n",
            "step: 230, loss: 0.08522655814886093\n",
            "step: 240, loss: 0.01852487400174141\n",
            "step: 250, loss: 0.04499583691358566\n",
            "step: 260, loss: 0.03488089144229889\n",
            "step: 270, loss: 0.035591065883636475\n",
            "step: 280, loss: 0.025784309953451157\n",
            "step: 290, loss: 0.0017195910913869739\n",
            "step: 300, loss: 0.15764306485652924\n",
            "step: 310, loss: 0.01570337638258934\n",
            "step: 320, loss: 0.008696386590600014\n",
            "step: 330, loss: 0.01095252949744463\n",
            "step: 340, loss: 0.01075314823538065\n",
            "step: 350, loss: 0.010321572422981262\n",
            "step: 360, loss: 0.005225855857133865\n",
            "step: 370, loss: 0.02231629565358162\n",
            "step: 380, loss: 0.008234603330492973\n",
            "step: 390, loss: 0.011051421985030174\n",
            "step: 400, loss: 0.010936884209513664\n",
            "step: 410, loss: 0.005891581531614065\n",
            "step: 420, loss: 0.18603384494781494\n",
            "step: 430, loss: 0.02645241655409336\n",
            "step: 440, loss: 0.004203401040285826\n",
            "step: 450, loss: 0.05596816912293434\n",
            "step: 460, loss: 0.021073097363114357\n",
            "step: 470, loss: 0.12960900366306305\n",
            "step: 480, loss: 0.003390482859686017\n",
            "step: 490, loss: 0.0026894290931522846\n",
            "step: 500, loss: 0.0026594032533466816\n",
            "step: 510, loss: 0.004874768666923046\n",
            "step: 520, loss: 0.02077385038137436\n",
            "step: 530, loss: 0.1318499594926834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9217230199166282, f1=0.9245810055865922, best_f1=0.927806241266884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045502983033657074\n",
            "step: 10, loss: 0.011053884401917458\n",
            "step: 20, loss: 0.002392670139670372\n",
            "step: 30, loss: 0.0020689694210886955\n",
            "step: 40, loss: 0.011273362673819065\n",
            "step: 50, loss: 0.008522656746208668\n",
            "step: 60, loss: 0.0013869507238268852\n",
            "step: 70, loss: 0.01089287456125021\n",
            "step: 80, loss: 0.0034949248656630516\n",
            "step: 90, loss: 0.05511876568198204\n",
            "step: 100, loss: 0.053844012320041656\n",
            "step: 110, loss: 0.0009906886843964458\n",
            "step: 120, loss: 0.0008594093378633261\n",
            "step: 130, loss: 0.0021289505530148745\n",
            "step: 140, loss: 0.0021750726737082005\n",
            "step: 150, loss: 0.0022405986674129963\n",
            "step: 160, loss: 0.02304132841527462\n",
            "step: 170, loss: 0.009213045239448547\n",
            "step: 180, loss: 0.0008210256928578019\n",
            "step: 190, loss: 0.009083117358386517\n",
            "step: 200, loss: 0.004377780016511679\n",
            "step: 210, loss: 0.03085250034928322\n",
            "step: 220, loss: 0.07536693662405014\n",
            "step: 230, loss: 0.046315521001815796\n",
            "step: 240, loss: 0.010189064778387547\n",
            "step: 250, loss: 0.03920721635222435\n",
            "step: 260, loss: 0.007785438559949398\n",
            "step: 270, loss: 0.016062874346971512\n",
            "step: 280, loss: 0.052552834153175354\n",
            "step: 290, loss: 0.012539029121398926\n",
            "step: 300, loss: 0.002102301688864827\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 310, loss: 0.09568502008914948\n",
            "step: 320, loss: 0.013332941569387913\n",
            "step: 330, loss: 0.014418313279747963\n",
            "step: 340, loss: 0.012880860827863216\n",
            "step: 350, loss: 0.004607740323990583\n",
            "step: 360, loss: 0.09032897651195526\n",
            "step: 370, loss: 0.002905243309214711\n",
            "step: 380, loss: 0.005728668533265591\n",
            "step: 390, loss: 0.01897045411169529\n",
            "step: 400, loss: 0.02726600132882595\n",
            "step: 410, loss: 0.003148965537548065\n",
            "step: 420, loss: 0.0025248529855161905\n",
            "step: 430, loss: 0.23899289965629578\n",
            "step: 440, loss: 0.025935662910342216\n",
            "step: 450, loss: 0.005943263880908489\n",
            "step: 460, loss: 0.001816740958020091\n",
            "step: 470, loss: 0.008754117414355278\n",
            "step: 480, loss: 0.07322521507740021\n",
            "step: 490, loss: 0.0033673145808279514\n",
            "step: 500, loss: 0.006238862406462431\n",
            "step: 510, loss: 0.0038687465712428093\n",
            "step: 520, loss: 0.034670691937208176\n",
            "step: 530, loss: 0.04705338180065155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9268965517241379, f1=0.9191176470588236, best_f1=0.9191176470588236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022188616916537285\n",
            "step: 10, loss: 0.05883782356977463\n",
            "step: 20, loss: 0.003287564031779766\n",
            "step: 30, loss: 0.001476478879339993\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.0035209658090025187\n",
            "step: 50, loss: 0.002397894859313965\n",
            "step: 60, loss: 0.1042519062757492\n",
            "step: 70, loss: 0.015054145827889442\n",
            "step: 80, loss: 0.000991565641015768\n",
            "step: 90, loss: 0.12588514387607574\n",
            "step: 100, loss: 0.002615648088976741\n",
            "step: 110, loss: 0.007512697484344244\n",
            "step: 120, loss: 0.00328287435695529\n",
            "step: 130, loss: 0.002181215211749077\n",
            "step: 140, loss: 0.012645849958062172\n",
            "step: 150, loss: 0.000684526632539928\n",
            "step: 160, loss: 0.0006714833434671164\n",
            "step: 170, loss: 0.0036154098343104124\n",
            "step: 180, loss: 0.0019811862148344517\n",
            "step: 190, loss: 0.04252588376402855\n",
            "step: 200, loss: 0.005292113870382309\n",
            "step: 210, loss: 0.05448869243264198\n",
            "step: 220, loss: 0.0005627591162919998\n",
            "step: 230, loss: 0.0029295969288796186\n",
            "step: 240, loss: 0.0008681846666149795\n",
            "step: 250, loss: 0.0009456724510528147\n",
            "step: 260, loss: 0.007664625067263842\n",
            "step: 270, loss: 0.00042560204747132957\n",
            "step: 280, loss: 0.013500327244400978\n",
            "step: 290, loss: 0.12938112020492554\n",
            "step: 300, loss: 0.000837442115880549\n",
            "step: 310, loss: 0.004045713227242231\n",
            "step: 320, loss: 0.08994980901479721\n",
            "step: 330, loss: 0.008183175697922707\n",
            "step: 340, loss: 0.0070104338228702545\n",
            "step: 350, loss: 0.0015418839175254107\n",
            "step: 360, loss: 0.004561872687190771\n",
            "step: 370, loss: 0.00919525045901537\n",
            "step: 380, loss: 0.00746507104486227\n",
            "step: 390, loss: 0.0019615362398326397\n",
            "step: 400, loss: 0.004757968243211508\n",
            "step: 410, loss: 0.05512811988592148\n",
            "step: 420, loss: 0.002488648286089301\n",
            "step: 430, loss: 0.011598451994359493\n",
            "step: 440, loss: 0.0009751128382049501\n",
            "step: 450, loss: 0.01512247696518898\n",
            "step: 460, loss: 0.06826120615005493\n",
            "step: 470, loss: 0.00021692723385058343\n",
            "step: 480, loss: 0.005065407603979111\n",
            "step: 490, loss: 0.0010537991765886545\n",
            "step: 500, loss: 0.01423073373734951\n",
            "step: 510, loss: 0.10451523959636688\n",
            "step: 520, loss: 0.003289534943178296\n",
            "step: 530, loss: 0.002538889180868864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9190225203641591, f1=0.9111969111969113, best_f1=0.9191176470588236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006449423381127417\n",
            "step: 10, loss: 0.0003680954105220735\n",
            "step: 20, loss: 0.0589577816426754\n",
            "step: 30, loss: 0.019729170948266983\n",
            "step: 40, loss: 0.002875187899917364\n",
            "step: 50, loss: 0.015264442190527916\n",
            "step: 60, loss: 0.0009090523817576468\n",
            "step: 70, loss: 0.002078230492770672\n",
            "step: 80, loss: 0.022494060918688774\n",
            "step: 90, loss: 0.0029107369482517242\n",
            "step: 100, loss: 0.005333289969712496\n",
            "step: 110, loss: 0.013210305012762547\n",
            "step: 120, loss: 0.004757918417453766\n",
            "step: 130, loss: 0.028911909088492393\n",
            "step: 140, loss: 0.03035266324877739\n",
            "step: 150, loss: 0.0003103116760030389\n",
            "step: 160, loss: 0.0003648687561508268\n",
            "step: 170, loss: 0.004604641813784838\n",
            "step: 180, loss: 0.0002901928673963994\n",
            "step: 190, loss: 0.002520928857848048\n",
            "step: 200, loss: 0.0005120753776282072\n",
            "step: 210, loss: 0.002474858658388257\n",
            "step: 220, loss: 0.0014927223091945052\n",
            "step: 230, loss: 0.00027030243654735386\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 240, loss: 0.02985476702451706\n",
            "step: 250, loss: 0.007041182368993759\n",
            "step: 260, loss: 0.0002211560058640316\n",
            "step: 270, loss: 0.03962944447994232\n",
            "step: 280, loss: 0.005947644356638193\n",
            "step: 290, loss: 0.0007690790807828307\n",
            "step: 300, loss: 0.00032288339571096003\n",
            "step: 310, loss: 0.0015869186026975513\n",
            "step: 320, loss: 0.00046870720689184964\n",
            "step: 330, loss: 0.00031643389957025647\n",
            "step: 340, loss: 0.1848650723695755\n",
            "step: 350, loss: 0.002311017131432891\n",
            "step: 360, loss: 0.003658146597445011\n",
            "step: 370, loss: 0.005669057834893465\n",
            "step: 380, loss: 0.024263115599751472\n",
            "step: 390, loss: 0.04082760587334633\n",
            "step: 400, loss: 0.0005967046599835157\n",
            "step: 410, loss: 0.0016596560599282384\n",
            "step: 420, loss: 0.003647084115073085\n",
            "step: 430, loss: 0.001041869167238474\n",
            "step: 440, loss: 9.09975788090378e-05\n",
            "step: 450, loss: 0.00028291920898482203\n",
            "step: 460, loss: 0.00023708517255727202\n",
            "step: 470, loss: 0.0004952634335495532\n",
            "step: 480, loss: 0.001037185313180089\n",
            "step: 490, loss: 0.00697718420997262\n",
            "step: 500, loss: 0.0017517883097752929\n",
            "step: 510, loss: 0.00316020124591887\n",
            "step: 520, loss: 0.00038662305450998247\n",
            "step: 530, loss: 0.004147205501794815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.929472209248015, f1=0.922429906542056, best_f1=0.922429906542056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002953251823782921\n",
            "step: 10, loss: 0.0010351671371608973\n",
            "step: 20, loss: 0.00023492488253396004\n",
            "step: 30, loss: 0.000992572051472962\n",
            "step: 40, loss: 0.0022657897789031267\n",
            "step: 50, loss: 0.0004619878891389817\n",
            "step: 60, loss: 0.00025149897555820644\n",
            "step: 70, loss: 0.0002734569425228983\n",
            "step: 80, loss: 0.0011926733423024416\n",
            "step: 90, loss: 0.00011846820416394621\n",
            "step: 100, loss: 0.0001382593618473038\n",
            "step: 110, loss: 0.0005337856709957123\n",
            "step: 120, loss: 0.00045462572597898543\n",
            "step: 130, loss: 0.001461649313569069\n",
            "step: 140, loss: 0.0001598942617420107\n",
            "step: 150, loss: 0.00028210534946992993\n",
            "step: 160, loss: 0.00010849224781850353\n",
            "step: 170, loss: 0.0001938873465405777\n",
            "step: 180, loss: 0.0031760758720338345\n",
            "step: 190, loss: 0.001503786537796259\n",
            "step: 200, loss: 0.0007829376263543963\n",
            "step: 210, loss: 0.0062194219790399075\n",
            "step: 220, loss: 0.00021523858595173806\n",
            "step: 230, loss: 0.0011152623919770122\n",
            "step: 240, loss: 0.06398449093103409\n",
            "step: 250, loss: 0.0020279441960155964\n",
            "step: 260, loss: 0.00021343218395486474\n",
            "step: 270, loss: 0.011989885941147804\n",
            "step: 280, loss: 0.0017484648851677775\n",
            "step: 290, loss: 0.0002052186755463481\n",
            "step: 300, loss: 0.0021055140532553196\n",
            "step: 310, loss: 0.0002376854681642726\n",
            "step: 320, loss: 0.00011328660184517503\n",
            "step: 330, loss: 0.00015824708680156618\n",
            "step: 340, loss: 0.013145903125405312\n",
            "step: 350, loss: 0.002998793264850974\n",
            "step: 360, loss: 0.0005743541987612844\n",
            "step: 370, loss: 0.0002519587869755924\n",
            "step: 380, loss: 0.0006631025462411344\n",
            "step: 390, loss: 0.0005482654669322073\n",
            "step: 400, loss: 0.001682364963926375\n",
            "step: 410, loss: 0.0029505181591957808\n",
            "step: 420, loss: 0.08443023264408112\n",
            "step: 430, loss: 0.000259123626165092\n",
            "step: 440, loss: 0.0011031870963051915\n",
            "step: 450, loss: 0.0013710687635466456\n",
            "step: 460, loss: 0.0011168767232447863\n",
            "step: 470, loss: 0.031873028725385666\n",
            "step: 480, loss: 0.2735539376735687\n",
            "step: 490, loss: 0.00311130378395319\n",
            "step: 500, loss: 0.004151828121393919\n",
            "step: 510, loss: 0.0010296439286321402\n",
            "step: 520, loss: 0.06744657456874847\n",
            "step: 530, loss: 0.01127712707966566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9255514705882354, f1=0.9249884951679707, best_f1=0.922429906542056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017118489369750023\n",
            "step: 10, loss: 0.029863614588975906\n",
            "step: 20, loss: 0.01031381543725729\n",
            "step: 30, loss: 0.004430266097187996\n",
            "step: 40, loss: 0.00019953883020207286\n",
            "step: 50, loss: 0.007979612797498703\n",
            "step: 60, loss: 0.0024538501165807247\n",
            "step: 70, loss: 0.00026062096003443\n",
            "step: 80, loss: 0.001624319120310247\n",
            "step: 90, loss: 0.002265307819470763\n",
            "step: 100, loss: 0.0001698596024652943\n",
            "step: 110, loss: 0.0001601121766725555\n",
            "step: 120, loss: 0.0010157402139157057\n",
            "step: 130, loss: 5.503366264747456e-05\n",
            "step: 140, loss: 0.0007515526376664639\n",
            "step: 150, loss: 8.86302295839414e-05\n",
            "step: 160, loss: 6.897825369378552e-05\n",
            "step: 170, loss: 0.0022159155923873186\n",
            "step: 180, loss: 7.951632142066956e-05\n",
            "step: 190, loss: 0.003278203774243593\n",
            "step: 200, loss: 0.001459159073419869\n",
            "step: 210, loss: 0.0004446603707037866\n",
            "step: 220, loss: 0.0015708478167653084\n",
            "step: 230, loss: 0.001870832871645689\n",
            "step: 240, loss: 6.597326137125492e-05\n",
            "step: 250, loss: 9.98240357148461e-05\n",
            "step: 260, loss: 0.00045359483920037746\n",
            "step: 270, loss: 0.00025999703211709857\n",
            "step: 280, loss: 0.021626897156238556\n",
            "step: 290, loss: 0.02724933996796608\n",
            "step: 300, loss: 0.000561566324904561\n",
            "step: 310, loss: 0.000547628675121814\n",
            "step: 320, loss: 0.00731453113257885\n",
            "step: 330, loss: 0.004877671133726835\n",
            "step: 340, loss: 0.004052099771797657\n",
            "step: 350, loss: 0.00016785661864560097\n",
            "step: 360, loss: 0.005841994658112526\n",
            "step: 370, loss: 0.010775534436106682\n",
            "step: 380, loss: 0.0007940199575386941\n",
            "step: 390, loss: 0.08088970929384232\n",
            "step: 400, loss: 0.0009426175965927541\n",
            "step: 410, loss: 0.018300721421837807\n",
            "step: 420, loss: 0.19054046273231506\n",
            "step: 430, loss: 0.01688367687165737\n",
            "step: 440, loss: 0.0013660952681675553\n",
            "step: 450, loss: 0.002607084810733795\n",
            "step: 460, loss: 0.00014144399028737098\n",
            "step: 470, loss: 0.002184620127081871\n",
            "step: 480, loss: 0.0001966389245353639\n",
            "step: 490, loss: 0.0012822265271097422\n",
            "step: 500, loss: 0.007905692793428898\n",
            "step: 510, loss: 0.0001091706653824076\n",
            "step: 520, loss: 0.002888580085709691\n",
            "step: 530, loss: 0.00058348587481305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9298653042266605, f1=0.9285051067780873, best_f1=0.9285051067780873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010689833434298635\n",
            "step: 10, loss: 8.95058547030203e-05\n",
            "step: 20, loss: 0.0004760146839544177\n",
            "step: 30, loss: 0.00018343847477808595\n",
            "step: 40, loss: 6.974796269787475e-05\n",
            "step: 50, loss: 9.458001295570284e-05\n",
            "step: 60, loss: 0.00010373591067036614\n",
            "step: 70, loss: 0.0032674826215952635\n",
            "step: 80, loss: 0.00016368307115044445\n",
            "step: 90, loss: 0.0001856388698797673\n",
            "step: 100, loss: 0.00022157050261739641\n",
            "step: 110, loss: 4.790012098965235e-05\n",
            "step: 120, loss: 0.002873822348192334\n",
            "step: 130, loss: 8.015539788175374e-05\n",
            "step: 140, loss: 0.0009776361985132098\n",
            "step: 150, loss: 0.0001247016916749999\n",
            "step: 160, loss: 0.00046584507799707353\n",
            "step: 170, loss: 0.001523195649497211\n",
            "step: 180, loss: 3.912073952960782e-05\n",
            "step: 190, loss: 0.0001949152210727334\n",
            "step: 200, loss: 0.00022426896612159908\n",
            "step: 210, loss: 0.002677571028470993\n",
            "step: 220, loss: 0.00014753038703929633\n",
            "step: 230, loss: 6.568247772520408e-05\n",
            "step: 240, loss: 0.00011494516365928575\n",
            "step: 250, loss: 0.001879124785773456\n",
            "step: 260, loss: 0.00029648770578205585\n",
            "step: 270, loss: 3.89717570214998e-05\n",
            "step: 280, loss: 4.268877091817558e-05\n",
            "step: 290, loss: 0.12357119470834732\n",
            "step: 300, loss: 7.252550858538598e-05\n",
            "step: 310, loss: 0.0006145605002529919\n",
            "step: 320, loss: 0.0002383867249591276\n",
            "step: 330, loss: 0.002311249729245901\n",
            "step: 340, loss: 0.0015645261155441403\n",
            "step: 350, loss: 0.00015915681433398277\n",
            "step: 360, loss: 0.00019273637735750526\n",
            "step: 370, loss: 7.587856089230627e-05\n",
            "step: 380, loss: 0.0023818337358534336\n",
            "step: 390, loss: 7.822853513062e-05\n",
            "step: 400, loss: 0.001662084017880261\n",
            "step: 410, loss: 6.153889262350276e-05\n",
            "step: 420, loss: 6.402089638868347e-05\n",
            "step: 430, loss: 0.00016927049728110433\n",
            "step: 440, loss: 0.0005607751663774252\n",
            "step: 450, loss: 8.972528303274885e-05\n",
            "step: 460, loss: 0.000203123883693479\n",
            "step: 470, loss: 3.2695665140636265e-05\n",
            "step: 480, loss: 5.440023960545659e-05\n",
            "step: 490, loss: 0.007807765156030655\n",
            "step: 500, loss: 0.00048523908481001854\n",
            "step: 510, loss: 0.00010282552102580667\n",
            "step: 520, loss: 7.220383122330531e-05\n",
            "step: 530, loss: 0.00010145410487893969\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9316081330868762, f1=0.9225746268656716, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001199384787469171\n",
            "step: 10, loss: 9.716282511362806e-05\n",
            "step: 20, loss: 3.408172415220179e-05\n",
            "step: 30, loss: 4.207864549243823e-05\n",
            "step: 40, loss: 6.263165414566174e-05\n",
            "step: 50, loss: 0.010829826816916466\n",
            "step: 60, loss: 7.134562474675477e-05\n",
            "step: 70, loss: 4.837234519072808e-05\n",
            "step: 80, loss: 7.480860222131014e-05\n",
            "step: 90, loss: 7.586805440951139e-05\n",
            "step: 100, loss: 0.0003710029413923621\n",
            "step: 110, loss: 4.690060450229794e-05\n",
            "step: 120, loss: 4.6887937060091645e-05\n",
            "step: 130, loss: 7.162632391555235e-05\n",
            "step: 140, loss: 0.0005728536634705961\n",
            "step: 150, loss: 3.950099562644027e-05\n",
            "step: 160, loss: 3.7980396882630885e-05\n",
            "step: 170, loss: 0.00020582045544870198\n",
            "step: 180, loss: 6.362967542372644e-05\n",
            "step: 190, loss: 6.185924576129764e-05\n",
            "step: 200, loss: 0.00010862851922865957\n",
            "step: 210, loss: 0.0002811908780131489\n",
            "step: 220, loss: 6.992560520302504e-05\n",
            "step: 230, loss: 6.581060006283224e-05\n",
            "step: 240, loss: 0.04364767298102379\n",
            "step: 250, loss: 8.477405935991555e-05\n",
            "step: 260, loss: 0.0019024558132514358\n",
            "step: 270, loss: 0.001685073715634644\n",
            "step: 280, loss: 9.457394480705261e-05\n",
            "step: 290, loss: 0.0001282378943869844\n",
            "step: 300, loss: 6.166086677694693e-05\n",
            "step: 310, loss: 9.320351819042116e-05\n",
            "step: 320, loss: 0.0014931083424016833\n",
            "step: 330, loss: 3.704212576849386e-05\n",
            "step: 340, loss: 0.00015080076991580427\n",
            "step: 350, loss: 0.0005966787575744092\n",
            "step: 360, loss: 9.356747614219785e-05\n",
            "step: 370, loss: 0.0009510516538284719\n",
            "step: 380, loss: 0.003025380428880453\n",
            "step: 390, loss: 9.51049369177781e-05\n",
            "step: 400, loss: 0.00146497692912817\n",
            "step: 410, loss: 0.0008401491795666516\n",
            "step: 420, loss: 0.0002553588419687003\n",
            "step: 430, loss: 5.8082208852283657e-05\n",
            "step: 440, loss: 0.020656144246459007\n",
            "step: 450, loss: 0.0007170903263613582\n",
            "step: 460, loss: 5.984727249597199e-05\n",
            "step: 470, loss: 0.0001475926983403042\n",
            "step: 480, loss: 7.059828203637153e-05\n",
            "step: 490, loss: 0.0017948005115613341\n",
            "step: 500, loss: 0.0015228809788823128\n",
            "step: 510, loss: 0.00021685686078853905\n",
            "step: 520, loss: 0.0002492867934051901\n",
            "step: 530, loss: 0.0015314146876335144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9291044776119404, f1=0.9198508853681266, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001442168722860515\n",
            "step: 10, loss: 0.00029975443612784147\n",
            "step: 20, loss: 0.0009200314525514841\n",
            "step: 30, loss: 0.00034478199086152017\n",
            "step: 40, loss: 0.0001549317967146635\n",
            "step: 50, loss: 0.0009318897500634193\n",
            "step: 60, loss: 0.0016508768312633038\n",
            "step: 70, loss: 9.218810009770095e-05\n",
            "step: 80, loss: 0.00013750455400440842\n",
            "step: 90, loss: 0.00018207027460448444\n",
            "step: 100, loss: 0.0008513516513630748\n",
            "step: 110, loss: 0.0016665119910612702\n",
            "step: 120, loss: 4.3980780901620165e-05\n",
            "step: 130, loss: 6.694552575936541e-05\n",
            "step: 140, loss: 0.000683921913150698\n",
            "step: 150, loss: 0.00020310045510996133\n",
            "step: 160, loss: 5.597972631221637e-05\n",
            "step: 170, loss: 8.920658729039133e-05\n",
            "step: 180, loss: 0.0001729264622554183\n",
            "step: 190, loss: 0.00014629164070356637\n",
            "step: 200, loss: 0.0001080253641703166\n",
            "step: 210, loss: 0.00010661972919479012\n",
            "step: 220, loss: 0.0004290277720429003\n",
            "step: 230, loss: 3.6823224945692345e-05\n",
            "step: 240, loss: 0.00013551664596889168\n",
            "step: 250, loss: 4.154441921855323e-05\n",
            "step: 260, loss: 4.733721289085224e-05\n",
            "step: 270, loss: 5.715616498491727e-05\n",
            "step: 280, loss: 8.366024849237874e-05\n",
            "step: 290, loss: 0.0005357501213438809\n",
            "step: 300, loss: 7.436168380081654e-05\n",
            "step: 310, loss: 0.0008205648045986891\n",
            "step: 320, loss: 5.317037721397355e-05\n",
            "step: 330, loss: 0.0001916781038744375\n",
            "step: 340, loss: 0.0004030222480650991\n",
            "step: 350, loss: 0.0017156670801341534\n",
            "step: 360, loss: 5.217011857894249e-05\n",
            "step: 370, loss: 0.01712566427886486\n",
            "step: 380, loss: 0.0004614498757291585\n",
            "step: 390, loss: 0.0022526136599481106\n",
            "step: 400, loss: 0.0007556856144219637\n",
            "step: 410, loss: 0.0001785576605470851\n",
            "step: 420, loss: 0.002385316649451852\n",
            "step: 430, loss: 0.0009538480662740767\n",
            "step: 440, loss: 0.0192805677652359\n",
            "step: 450, loss: 5.9182904806220904e-05\n",
            "step: 460, loss: 0.00781718548387289\n",
            "step: 470, loss: 0.0003148595569655299\n",
            "step: 480, loss: 0.0003437626291997731\n",
            "step: 490, loss: 7.853434362914413e-05\n",
            "step: 500, loss: 0.0003470649535302073\n",
            "step: 510, loss: 5.2673978643724695e-05\n",
            "step: 520, loss: 0.00020265727653168142\n",
            "step: 530, loss: 0.00014370103599503636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9263360616273472, f1=0.9155812831644958, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001486387918703258\n",
            "step: 10, loss: 3.9973907405510545e-05\n",
            "step: 20, loss: 7.446197560057044e-05\n",
            "step: 30, loss: 0.00011496577644720674\n",
            "step: 40, loss: 0.00013832775584887713\n",
            "step: 50, loss: 0.02108158729970455\n",
            "step: 60, loss: 0.003274462418630719\n",
            "step: 70, loss: 0.00013844580098520964\n",
            "step: 80, loss: 0.00024969433434307575\n",
            "step: 90, loss: 8.585509203840047e-05\n",
            "step: 100, loss: 5.2847452025162056e-05\n",
            "step: 110, loss: 0.00013165888958610594\n",
            "step: 120, loss: 0.00016157046775333583\n",
            "step: 130, loss: 6.052304161130451e-05\n",
            "step: 140, loss: 0.0012778742238879204\n",
            "step: 150, loss: 2.976998257508967e-05\n",
            "step: 160, loss: 0.00033101870212703943\n",
            "step: 170, loss: 9.185768431052566e-05\n",
            "step: 180, loss: 0.00014103377179708332\n",
            "step: 190, loss: 5.097346365801059e-05\n",
            "step: 200, loss: 8.879615052137524e-05\n",
            "step: 210, loss: 0.00010315689723938704\n",
            "step: 220, loss: 0.054803598672151566\n",
            "step: 230, loss: 6.116056465543807e-05\n",
            "step: 240, loss: 0.04043520614504814\n",
            "step: 250, loss: 0.00011788264237111434\n",
            "step: 260, loss: 4.691235153586604e-05\n",
            "step: 270, loss: 8.760247874306515e-05\n",
            "step: 280, loss: 0.07718755304813385\n",
            "step: 290, loss: 5.070135375717655e-05\n",
            "step: 300, loss: 0.0001178421953227371\n",
            "step: 310, loss: 0.0002826599811669439\n",
            "step: 320, loss: 2.8634896807488985e-05\n",
            "step: 330, loss: 0.0001201246413984336\n",
            "step: 340, loss: 3.805574669968337e-05\n",
            "step: 350, loss: 0.00011545919551281258\n",
            "step: 360, loss: 0.0298676285892725\n",
            "step: 370, loss: 0.0001131410535890609\n",
            "step: 380, loss: 0.0001526612468296662\n",
            "step: 390, loss: 0.00028613951872102916\n",
            "step: 400, loss: 0.00021017494145780802\n",
            "step: 410, loss: 0.00014596444088965654\n",
            "step: 420, loss: 0.005347154103219509\n",
            "step: 430, loss: 0.00012828284525312483\n",
            "step: 440, loss: 0.01374081801623106\n",
            "step: 450, loss: 0.0013035460142418742\n",
            "step: 460, loss: 0.00018974480917677283\n",
            "step: 470, loss: 6.970611138967797e-05\n",
            "step: 480, loss: 0.000803899485617876\n",
            "step: 490, loss: 0.001172534073702991\n",
            "step: 500, loss: 6.806878809584305e-05\n",
            "step: 510, loss: 0.00016702592256478965\n",
            "step: 520, loss: 6.533100531669334e-05\n",
            "step: 530, loss: 0.00010621146793710068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9278642149929278, f1=0.9200945626477541, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019202566472813487\n",
            "step: 10, loss: 9.150293772108853e-05\n",
            "step: 20, loss: 2.2153564714244567e-05\n",
            "step: 30, loss: 0.00010873060818994418\n",
            "step: 40, loss: 3.0103001336101443e-05\n",
            "step: 50, loss: 0.003647561650723219\n",
            "step: 60, loss: 3.551463669282384e-05\n",
            "step: 70, loss: 7.47641606722027e-05\n",
            "step: 80, loss: 0.00017598233534954488\n",
            "step: 90, loss: 0.00041824017534963787\n",
            "step: 100, loss: 4.852686106460169e-05\n",
            "step: 110, loss: 2.910364310082514e-05\n",
            "step: 120, loss: 3.0091412554611452e-05\n",
            "step: 130, loss: 2.4094375476124696e-05\n",
            "step: 140, loss: 8.09090633993037e-05\n",
            "step: 150, loss: 0.0008061896078288555\n",
            "step: 160, loss: 4.3163716327399015e-05\n",
            "step: 170, loss: 4.278985579730943e-05\n",
            "step: 180, loss: 0.0002782979281619191\n",
            "step: 190, loss: 7.269695197464898e-05\n",
            "step: 200, loss: 0.00018879343406297266\n",
            "step: 210, loss: 0.00015729782171547413\n",
            "step: 220, loss: 0.00011187530617462471\n",
            "step: 230, loss: 3.252373790019192e-05\n",
            "step: 240, loss: 4.426849773153663e-05\n",
            "step: 250, loss: 4.1117080399999395e-05\n",
            "step: 260, loss: 4.1392268030904233e-05\n",
            "step: 270, loss: 2.1274438040563837e-05\n",
            "step: 280, loss: 7.378587179118767e-05\n",
            "step: 290, loss: 0.003264690050855279\n",
            "step: 300, loss: 0.00014635425759479403\n",
            "step: 310, loss: 0.0005545567255467176\n",
            "step: 320, loss: 0.0023326745722442865\n",
            "step: 330, loss: 0.0011963449651375413\n",
            "step: 340, loss: 9.874007082544267e-05\n",
            "step: 350, loss: 0.0003455231199041009\n",
            "step: 360, loss: 2.647102519404143e-05\n",
            "step: 370, loss: 9.116619185078889e-05\n",
            "step: 380, loss: 0.00010041419591289014\n",
            "step: 390, loss: 0.04853369668126106\n",
            "step: 400, loss: 0.0009842927101999521\n",
            "step: 410, loss: 0.0001336423447355628\n",
            "step: 420, loss: 4.585032365866937e-05\n",
            "step: 430, loss: 0.0006814097869209945\n",
            "step: 440, loss: 3.927223224309273e-05\n",
            "step: 450, loss: 5.480658728629351e-05\n",
            "step: 460, loss: 0.0031743193976581097\n",
            "step: 470, loss: 0.0005329891573637724\n",
            "step: 480, loss: 3.640287832240574e-05\n",
            "step: 490, loss: 3.198771082679741e-05\n",
            "step: 500, loss: 3.797331373789348e-05\n",
            "step: 510, loss: 0.00033563192118890584\n",
            "step: 520, loss: 5.110584243084304e-05\n",
            "step: 530, loss: 0.00025098599144257605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.92524682651622, f1=0.9222846441947565, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000848327879793942\n",
            "step: 10, loss: 2.3762320779496804e-05\n",
            "step: 20, loss: 1.9806962882285006e-05\n",
            "step: 30, loss: 3.5384651710046455e-05\n",
            "step: 40, loss: 5.971762948320247e-05\n",
            "step: 50, loss: 0.00038319238228723407\n",
            "step: 60, loss: 2.39415057876613e-05\n",
            "step: 70, loss: 3.609043778851628e-05\n",
            "step: 80, loss: 3.9480975829064846e-05\n",
            "step: 90, loss: 7.587313302792609e-05\n",
            "step: 100, loss: 3.651336737675592e-05\n",
            "step: 110, loss: 2.6802617867360823e-05\n",
            "step: 120, loss: 2.295094782311935e-05\n",
            "step: 130, loss: 0.004856090992689133\n",
            "step: 140, loss: 0.00025383010506629944\n",
            "step: 150, loss: 2.964660234283656e-05\n",
            "step: 160, loss: 2.687336200324353e-05\n",
            "step: 170, loss: 4.2930398194584996e-05\n",
            "step: 180, loss: 3.188647315255366e-05\n",
            "step: 190, loss: 3.430020296946168e-05\n",
            "step: 200, loss: 7.343583274632692e-05\n",
            "step: 210, loss: 3.833406663034111e-05\n",
            "step: 220, loss: 2.9875400286982767e-05\n",
            "step: 230, loss: 0.0014031408354640007\n",
            "step: 240, loss: 3.246391861466691e-05\n",
            "step: 250, loss: 3.6327357520349324e-05\n",
            "step: 260, loss: 5.125040843267925e-05\n",
            "step: 270, loss: 8.364295354112983e-05\n",
            "step: 280, loss: 2.6623640223988332e-05\n",
            "step: 290, loss: 0.006346886977553368\n",
            "step: 300, loss: 4.700299177784473e-05\n",
            "step: 310, loss: 0.0014110878109931946\n",
            "step: 320, loss: 0.0001892985455924645\n",
            "step: 330, loss: 0.00012807831808459014\n",
            "step: 340, loss: 0.001196966040879488\n",
            "step: 350, loss: 0.00015871234063524753\n",
            "step: 360, loss: 0.0274994857609272\n",
            "step: 370, loss: 2.5115205062320456e-05\n",
            "step: 380, loss: 6.397102697519585e-05\n",
            "step: 390, loss: 0.003624941688030958\n",
            "step: 400, loss: 0.00016193697229027748\n",
            "step: 410, loss: 2.1043448214186355e-05\n",
            "step: 420, loss: 4.425036604516208e-05\n",
            "step: 430, loss: 0.00021534493134822696\n",
            "step: 440, loss: 5.045751458965242e-05\n",
            "step: 450, loss: 4.5221222535474226e-05\n",
            "step: 460, loss: 0.00011330330016789958\n",
            "step: 470, loss: 4.550943413050845e-05\n",
            "step: 480, loss: 2.9804790756315924e-05\n",
            "step: 490, loss: 0.007428786251693964\n",
            "step: 500, loss: 3.388606637599878e-05\n",
            "step: 510, loss: 0.0001841234916355461\n",
            "step: 520, loss: 8.421538950642571e-05\n",
            "step: 530, loss: 3.4485205105738714e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.930562116202173, f1=0.9210526315789475, best_f1=0.9225746268656716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.6526706278673373e-05\n",
            "step: 10, loss: 4.554905171971768e-05\n",
            "step: 20, loss: 1.7337208191747777e-05\n",
            "step: 30, loss: 5.265435902401805e-05\n",
            "step: 40, loss: 0.01608952507376671\n",
            "step: 50, loss: 6.236953049665317e-05\n",
            "step: 60, loss: 1.6349951692973264e-05\n",
            "step: 70, loss: 0.0002144872851204127\n",
            "step: 80, loss: 0.0020097438246011734\n",
            "step: 90, loss: 2.4992303224280477e-05\n",
            "step: 100, loss: 0.00016214251809287816\n",
            "step: 110, loss: 0.0002704719372559339\n",
            "step: 120, loss: 8.375842298846692e-05\n",
            "step: 130, loss: 0.046496883034706116\n",
            "step: 140, loss: 3.0499024433083832e-05\n",
            "step: 150, loss: 6.519411545014009e-05\n",
            "step: 160, loss: 4.954488031216897e-05\n",
            "step: 170, loss: 4.8988535127136856e-05\n",
            "step: 180, loss: 1.617490670469124e-05\n",
            "step: 190, loss: 7.090610597515479e-05\n",
            "step: 200, loss: 2.1277914129314013e-05\n",
            "step: 210, loss: 2.3516633518738672e-05\n",
            "step: 220, loss: 2.9227245249785483e-05\n",
            "step: 230, loss: 2.752531690930482e-05\n",
            "step: 240, loss: 3.152454155497253e-05\n",
            "step: 250, loss: 3.200228456989862e-05\n",
            "step: 260, loss: 2.8735723390127532e-05\n",
            "step: 270, loss: 2.5472902052570134e-05\n",
            "step: 280, loss: 2.9797532988595776e-05\n",
            "step: 290, loss: 6.165917875478044e-05\n",
            "step: 300, loss: 2.616490382933989e-05\n",
            "step: 310, loss: 0.0016024911310523748\n",
            "step: 320, loss: 1.7556971215526573e-05\n",
            "step: 330, loss: 4.0510469261789694e-05\n",
            "step: 340, loss: 1.7258944353670813e-05\n",
            "step: 350, loss: 1.5858311599004082e-05\n",
            "step: 360, loss: 0.00020701820903923362\n",
            "step: 370, loss: 3.605450547183864e-05\n",
            "step: 380, loss: 5.4972759244265035e-05\n",
            "step: 390, loss: 0.00018906490004155785\n",
            "step: 400, loss: 2.5506302335998043e-05\n",
            "step: 410, loss: 1.724410140013788e-05\n",
            "step: 420, loss: 7.706562610110268e-05\n",
            "step: 430, loss: 4.0890892705647275e-05\n",
            "step: 440, loss: 0.0001333348627667874\n",
            "step: 450, loss: 2.0767894966411404e-05\n",
            "step: 460, loss: 3.2225514587480575e-05\n",
            "step: 470, loss: 0.00015869484923314303\n",
            "step: 480, loss: 1.6323872841894627e-05\n",
            "step: 490, loss: 1.9609506125561893e-05\n",
            "step: 500, loss: 7.066094258334488e-05\n",
            "step: 510, loss: 2.6004847313743085e-05\n",
            "step: 520, loss: 2.995379691128619e-05\n",
            "step: 530, loss: 3.429602293181233e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9286047596826879, f1=0.9208566108007449, best_f1=0.9225746268656716\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:32, 176.53it/s]\n",
            "load_f1 = 0.9304713019132057\n",
            "real_f1 = 0.9259431765253843\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 177.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnXzXaaYhstq"
      },
      "source": [
        "# DITTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r23AxFPnhstr"
      },
      "source": [
        "## DITTO STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d984b040-9884-4a62-eadf-48c7251e0ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5642581582069397\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.36, f1=0.31250000000000006, best_f1=0.31250000000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48715269565582275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.33802816901408456, f1=0.32, best_f1=0.31250000000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5358408093452454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4285714285714286, f1=0.35714285714285715, best_f1=0.35714285714285715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26885199546813965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.55, f1=0.4666666666666667, best_f1=0.4666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2420109212398529\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5238095238095237, f1=0.5000000000000001, best_f1=0.4666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.290209025144577\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.6250000000000001, f1=0.6000000000000001, best_f1=0.6000000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07456933706998825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6470588235294117, f1=0.5625000000000001, best_f1=0.5625000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027730198577046394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7096774193548386, f1=0.5384615384615384, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036831044126302004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6666666666666667, f1=0.5517241379310344, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01122783962637186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7096774193548386, f1=0.6428571428571429, best_f1=0.5384615384615384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003516497788950801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7333333333333334, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008601327426731586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7333333333333334, f1=0.6666666666666666, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003398618195205927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7333333333333334, f1=0.6875000000000001, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.040497712790966034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7333333333333334, f1=0.6875000000000001, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005001570098102093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7333333333333334, f1=0.6875000000000001, best_f1=0.7142857142857143\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 130623.43it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5714285714285714\n",
            "real_f1 = 0.5517241379310344\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 179.09it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bang43Lhsts"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6f7886-4692-490c-8e29-341f77a72727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 259kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.05MB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 64.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6309524774551392\n",
            "step: 10, loss: 0.6353327631950378\n",
            "step: 20, loss: 0.36955833435058594\n",
            "step: 30, loss: 0.13447456061840057\n",
            "step: 40, loss: 0.3101920485496521\n",
            "step: 50, loss: 0.02532215043902397\n",
            "step: 60, loss: 0.08127039670944214\n",
            "step: 70, loss: 0.07467811554670334\n",
            "step: 80, loss: 0.16405801475048065\n",
            "step: 90, loss: 0.12084567546844482\n",
            "step: 100, loss: 0.007158154621720314\n",
            "step: 110, loss: 0.1961156725883484\n",
            "step: 120, loss: 0.0049243527464568615\n",
            "step: 130, loss: 0.029751237481832504\n",
            "step: 140, loss: 0.005904721561819315\n",
            "step: 150, loss: 0.079658642411232\n",
            "step: 160, loss: 0.024897560477256775\n",
            "step: 170, loss: 0.09821554273366928\n",
            "step: 180, loss: 0.014255683869123459\n",
            "step: 190, loss: 0.012995891273021698\n",
            "step: 200, loss: 0.017972638830542564\n",
            "step: 210, loss: 0.01534099131822586\n",
            "step: 220, loss: 0.015442471951246262\n",
            "step: 230, loss: 0.10972655564546585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9702517162471395, f1=0.9714937286202965, best_f1=0.9714937286202965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003927348647266626\n",
            "step: 10, loss: 0.004230109043419361\n",
            "step: 20, loss: 0.08434463292360306\n",
            "step: 30, loss: 0.20313726365566254\n",
            "step: 40, loss: 0.035366058349609375\n",
            "step: 50, loss: 0.02924496866762638\n",
            "step: 60, loss: 0.03061888925731182\n",
            "step: 70, loss: 0.2220505326986313\n",
            "step: 80, loss: 0.004465845879167318\n",
            "step: 90, loss: 0.004663174040615559\n",
            "step: 100, loss: 0.04422343149781227\n",
            "step: 110, loss: 0.11124139279127121\n",
            "step: 120, loss: 0.01538803894072771\n",
            "step: 130, loss: 0.004771477077156305\n",
            "step: 140, loss: 0.024769747629761696\n",
            "step: 150, loss: 0.025295615196228027\n",
            "step: 160, loss: 0.019113842397928238\n",
            "step: 170, loss: 0.0038158465176820755\n",
            "step: 180, loss: 0.0032693215180188417\n",
            "step: 190, loss: 0.04636864364147186\n",
            "step: 200, loss: 0.009639403782784939\n",
            "step: 210, loss: 0.0007935917819850147\n",
            "step: 220, loss: 0.2882872223854065\n",
            "step: 230, loss: 0.025356708094477654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9887387387387387, f1=0.9819819819819819, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00803819578140974\n",
            "step: 10, loss: 0.0013860843610018492\n",
            "step: 20, loss: 0.0008520131814293563\n",
            "step: 30, loss: 0.1120707169175148\n",
            "step: 40, loss: 0.1366945505142212\n",
            "step: 50, loss: 0.013126278296113014\n",
            "step: 60, loss: 0.01499294675886631\n",
            "step: 70, loss: 0.0026436520274728537\n",
            "step: 80, loss: 0.002001016866415739\n",
            "step: 90, loss: 0.07899680733680725\n",
            "step: 100, loss: 0.0013641545083373785\n",
            "step: 110, loss: 0.0006097399746067822\n",
            "step: 120, loss: 0.0167809147387743\n",
            "step: 130, loss: 0.007214122451841831\n",
            "step: 140, loss: 0.018978416919708252\n",
            "step: 150, loss: 0.022443639114499092\n",
            "step: 160, loss: 0.0578186996281147\n",
            "step: 170, loss: 0.004396816249936819\n",
            "step: 180, loss: 0.0035577628295868635\n",
            "step: 190, loss: 0.0028757695108652115\n",
            "step: 200, loss: 0.05102790892124176\n",
            "step: 210, loss: 0.0008730792324058712\n",
            "step: 220, loss: 0.0008424895931966603\n",
            "step: 230, loss: 0.0004099791403859854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9865168539325843, f1=0.977728285077951, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014279666356742382\n",
            "step: 10, loss: 0.0003278766816947609\n",
            "step: 20, loss: 0.0007981530507095158\n",
            "step: 30, loss: 0.004593153949826956\n",
            "step: 40, loss: 0.005910373758524656\n",
            "step: 50, loss: 0.005807633511722088\n",
            "step: 60, loss: 0.0006409790948964655\n",
            "step: 70, loss: 0.0005715576116926968\n",
            "step: 80, loss: 0.0004868078394792974\n",
            "step: 90, loss: 0.0005929536418989301\n",
            "step: 100, loss: 0.0007845099898986518\n",
            "step: 110, loss: 0.0003771836927626282\n",
            "step: 120, loss: 0.15854839980602264\n",
            "step: 130, loss: 0.0082571255043149\n",
            "step: 140, loss: 0.0009944919729605317\n",
            "step: 150, loss: 0.16535557806491852\n",
            "step: 160, loss: 0.0006451947847381234\n",
            "step: 170, loss: 0.01774054393172264\n",
            "step: 180, loss: 0.0004714437818620354\n",
            "step: 190, loss: 0.009331990033388138\n",
            "step: 200, loss: 0.0026158036198467016\n",
            "step: 210, loss: 0.010381920263171196\n",
            "step: 220, loss: 0.0005626103957183659\n",
            "step: 230, loss: 0.01326585840433836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9775280898876404, f1=0.9764309764309763, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027336194179952145\n",
            "step: 10, loss: 0.0004430878034327179\n",
            "step: 20, loss: 0.000786366465035826\n",
            "step: 30, loss: 0.000443245138740167\n",
            "step: 40, loss: 0.0002249510434921831\n",
            "step: 50, loss: 0.00046169955749064684\n",
            "step: 60, loss: 0.0007141903624869883\n",
            "step: 70, loss: 0.00020864907128270715\n",
            "step: 80, loss: 0.0002377592900302261\n",
            "step: 90, loss: 0.00042205804493278265\n",
            "step: 100, loss: 0.0016757659614086151\n",
            "step: 110, loss: 0.009700370021164417\n",
            "step: 120, loss: 0.00023702230828348547\n",
            "step: 130, loss: 0.0033940060529857874\n",
            "step: 140, loss: 0.03564024716615677\n",
            "step: 150, loss: 0.02369597554206848\n",
            "step: 160, loss: 0.0024341177195310593\n",
            "step: 170, loss: 0.06715569645166397\n",
            "step: 180, loss: 0.007793682161718607\n",
            "step: 190, loss: 0.017034141346812248\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 200, loss: 0.012957175262272358\n",
            "step: 210, loss: 0.016357239335775375\n",
            "step: 220, loss: 0.0020261872559785843\n",
            "step: 230, loss: 0.00018657476175576448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.977728285077951, f1=0.9764837625979844, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007505138055421412\n",
            "step: 10, loss: 0.0003858654235955328\n",
            "step: 20, loss: 0.05474752187728882\n",
            "step: 30, loss: 0.00017649083747528493\n",
            "step: 40, loss: 0.00024906941689550877\n",
            "step: 50, loss: 0.0001924019306898117\n",
            "step: 60, loss: 0.007871522568166256\n",
            "step: 70, loss: 0.04101525992155075\n",
            "step: 80, loss: 0.022572802379727364\n",
            "step: 90, loss: 0.07587160170078278\n",
            "step: 100, loss: 0.021740589290857315\n",
            "step: 110, loss: 0.00036275279126130044\n",
            "step: 120, loss: 0.0001301109150517732\n",
            "step: 130, loss: 0.0010542593663558364\n",
            "step: 140, loss: 0.0002748150727711618\n",
            "step: 150, loss: 0.00026373000582680106\n",
            "step: 160, loss: 0.0026004211977124214\n",
            "step: 170, loss: 0.0004876042657997459\n",
            "step: 180, loss: 0.021492116153240204\n",
            "step: 190, loss: 0.04063574969768524\n",
            "step: 200, loss: 0.0006855179672129452\n",
            "step: 210, loss: 0.0024569567758589983\n",
            "step: 220, loss: 0.00574579369276762\n",
            "step: 230, loss: 0.009538508951663971\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9777777777777777, f1=0.9732739420935412, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011323271319270134\n",
            "step: 10, loss: 0.00023284034978132695\n",
            "step: 20, loss: 0.0004553607141133398\n",
            "step: 30, loss: 0.0005193978431634605\n",
            "step: 40, loss: 0.00010867544915527105\n",
            "step: 50, loss: 7.958558853715658e-05\n",
            "step: 60, loss: 0.0027994865085929632\n",
            "step: 70, loss: 0.00037448504008352757\n",
            "step: 80, loss: 5.872127439943142e-05\n",
            "step: 90, loss: 9.536505967844278e-05\n",
            "step: 100, loss: 5.763748413301073e-05\n",
            "step: 110, loss: 0.00033230966073460877\n",
            "step: 120, loss: 6.16591569269076e-05\n",
            "step: 130, loss: 0.00014604518946725875\n",
            "step: 140, loss: 8.989129128167406e-05\n",
            "step: 150, loss: 0.011093147099018097\n",
            "step: 160, loss: 0.06474072486162186\n",
            "step: 170, loss: 0.015190738253295422\n",
            "step: 180, loss: 0.072641521692276\n",
            "step: 190, loss: 0.0009698064532130957\n",
            "step: 200, loss: 0.033833008259534836\n",
            "step: 210, loss: 5.169623182155192e-05\n",
            "step: 220, loss: 0.003871127963066101\n",
            "step: 230, loss: 0.0006480291485786438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9764309764309763, f1=0.9786276715410572, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.9178339142818004e-05\n",
            "step: 10, loss: 0.00023293413687497377\n",
            "step: 20, loss: 0.0001673968945397064\n",
            "step: 30, loss: 7.185059803305194e-05\n",
            "step: 40, loss: 0.004231436178088188\n",
            "step: 50, loss: 0.0006004883325658739\n",
            "step: 60, loss: 0.003030693856999278\n",
            "step: 70, loss: 0.0011070560431107879\n",
            "step: 80, loss: 0.001008224906399846\n",
            "step: 90, loss: 2.7793608751380816e-05\n",
            "step: 100, loss: 0.0004266496398486197\n",
            "step: 110, loss: 0.0007959711947478354\n",
            "step: 120, loss: 0.0005954501684755087\n",
            "step: 130, loss: 0.0008224673219956458\n",
            "step: 140, loss: 0.0006018474814482033\n",
            "step: 150, loss: 0.006748910993337631\n",
            "step: 160, loss: 0.0001614090579096228\n",
            "step: 170, loss: 0.0002175942063331604\n",
            "step: 180, loss: 6.932447286089882e-05\n",
            "step: 190, loss: 6.98227813700214e-05\n",
            "step: 200, loss: 4.43750323029235e-05\n",
            "step: 210, loss: 0.0037833370734006166\n",
            "step: 220, loss: 0.01209774799644947\n",
            "step: 230, loss: 5.99489503656514e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.978675645342312, f1=0.9751131221719457, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.0141214412869886e-05\n",
            "step: 10, loss: 0.0020203348249197006\n",
            "step: 20, loss: 0.0003360837581567466\n",
            "step: 30, loss: 5.033176421420649e-05\n",
            "step: 40, loss: 0.007808235008269548\n",
            "step: 50, loss: 0.00021543630282394588\n",
            "step: 60, loss: 9.612626308808103e-05\n",
            "step: 70, loss: 0.008340761996805668\n",
            "step: 80, loss: 0.0002621329331304878\n",
            "step: 90, loss: 0.0037684286944568157\n",
            "step: 100, loss: 0.0019229378085583448\n",
            "step: 110, loss: 0.0010363790206611156\n",
            "step: 120, loss: 0.001116381143219769\n",
            "step: 130, loss: 0.00019998452626168728\n",
            "step: 140, loss: 0.0004088571004103869\n",
            "step: 150, loss: 0.001927050412632525\n",
            "step: 160, loss: 9.567171218805015e-05\n",
            "step: 170, loss: 0.005590302869677544\n",
            "step: 180, loss: 0.0005509807378984988\n",
            "step: 190, loss: 0.00041721074376255274\n",
            "step: 200, loss: 5.809206413687207e-05\n",
            "step: 210, loss: 7.227940659504384e-05\n",
            "step: 220, loss: 4.9317259254166856e-05\n",
            "step: 230, loss: 0.0002313005388714373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9786276715410572, f1=0.9808773903262092, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.692706741276197e-05\n",
            "step: 10, loss: 2.6925592464976944e-05\n",
            "step: 20, loss: 8.21782523416914e-05\n",
            "step: 30, loss: 4.6928151277825236e-05\n",
            "step: 40, loss: 4.34523681178689e-05\n",
            "step: 50, loss: 4.976428681402467e-05\n",
            "step: 60, loss: 9.224157111020759e-05\n",
            "step: 70, loss: 0.00025069701950997114\n",
            "step: 80, loss: 4.011946657556109e-05\n",
            "step: 90, loss: 8.578548295190558e-05\n",
            "step: 100, loss: 3.737796578207053e-05\n",
            "step: 110, loss: 0.0001412780547980219\n",
            "step: 120, loss: 0.0015656538307666779\n",
            "step: 130, loss: 4.92650069645606e-05\n",
            "step: 140, loss: 0.024852976202964783\n",
            "step: 150, loss: 0.08739501982927322\n",
            "step: 160, loss: 5.6043369113467634e-05\n",
            "step: 170, loss: 3.383718285476789e-05\n",
            "step: 180, loss: 8.12620492069982e-05\n",
            "step: 190, loss: 0.018120955675840378\n",
            "step: 200, loss: 3.559690230758861e-05\n",
            "step: 210, loss: 2.8816679332521744e-05\n",
            "step: 220, loss: 2.9782644560327753e-05\n",
            "step: 230, loss: 0.0021072220988571644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9798657718120806, f1=0.9787709497206705, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.529914465616457e-05\n",
            "step: 10, loss: 4.868388350587338e-05\n",
            "step: 20, loss: 0.00018428699695505202\n",
            "step: 30, loss: 0.001978818327188492\n",
            "step: 40, loss: 0.00010132878378499299\n",
            "step: 50, loss: 3.101183392573148e-05\n",
            "step: 60, loss: 4.083069143234752e-05\n",
            "step: 70, loss: 8.326130046043545e-05\n",
            "step: 80, loss: 0.023242291063070297\n",
            "step: 90, loss: 0.00035666528856381774\n",
            "step: 100, loss: 3.0747487471671775e-05\n",
            "step: 110, loss: 0.000199181042262353\n",
            "step: 120, loss: 2.5882613044814207e-05\n",
            "step: 130, loss: 3.991020639659837e-05\n",
            "step: 140, loss: 0.0020656234119087458\n",
            "step: 150, loss: 0.0372624509036541\n",
            "step: 160, loss: 0.00012780290853697807\n",
            "step: 170, loss: 0.01726827397942543\n",
            "step: 180, loss: 4.298571730032563e-05\n",
            "step: 190, loss: 2.5029446987900883e-05\n",
            "step: 200, loss: 0.0008252472034655511\n",
            "step: 210, loss: 2.10214639082551e-05\n",
            "step: 220, loss: 0.0006213845917955041\n",
            "step: 230, loss: 5.742997018387541e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9777777777777777, f1=0.9765886287625419, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014526712766382843\n",
            "step: 10, loss: 0.0043494743295013905\n",
            "step: 20, loss: 3.607795952120796e-05\n",
            "step: 30, loss: 0.00014232499233912677\n",
            "step: 40, loss: 0.00016306384350173175\n",
            "step: 50, loss: 0.002519128378480673\n",
            "step: 60, loss: 0.0007827311055734754\n",
            "step: 70, loss: 0.0003259415098000318\n",
            "step: 80, loss: 0.00011849199654534459\n",
            "step: 90, loss: 5.0995193305425346e-05\n",
            "step: 100, loss: 0.007795060984790325\n",
            "step: 110, loss: 0.002199922688305378\n",
            "step: 120, loss: 0.01666383258998394\n",
            "step: 130, loss: 3.122459020232782e-05\n",
            "step: 140, loss: 0.00014551296771969646\n",
            "step: 150, loss: 0.00030263428925536573\n",
            "step: 160, loss: 2.4266140826512128e-05\n",
            "step: 170, loss: 5.861806494067423e-05\n",
            "step: 180, loss: 0.0032070851884782314\n",
            "step: 190, loss: 0.0003678657812997699\n",
            "step: 200, loss: 0.0014230821980163455\n",
            "step: 210, loss: 0.00021161034237593412\n",
            "step: 220, loss: 0.00015664042439311743\n",
            "step: 230, loss: 0.032081298530101776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9794988610478361, f1=0.9785310734463276, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014884456992149353\n",
            "step: 10, loss: 0.0008423524559475482\n",
            "step: 20, loss: 0.00031331469654105604\n",
            "step: 30, loss: 0.00010718830162659287\n",
            "step: 40, loss: 8.422827522736043e-05\n",
            "step: 50, loss: 9.607362881070003e-05\n",
            "step: 60, loss: 4.861156776314601e-05\n",
            "step: 70, loss: 2.9536699003074318e-05\n",
            "step: 80, loss: 3.639729038695805e-05\n",
            "step: 90, loss: 0.00027899720589630306\n",
            "step: 100, loss: 0.00021076883422210813\n",
            "step: 110, loss: 0.04366595670580864\n",
            "step: 120, loss: 0.03658502548933029\n",
            "step: 130, loss: 0.0005641589523293078\n",
            "step: 140, loss: 8.748171967454255e-05\n",
            "step: 150, loss: 2.5148923668893985e-05\n",
            "step: 160, loss: 0.00012186375533929095\n",
            "step: 170, loss: 0.00033012841595336795\n",
            "step: 180, loss: 0.011607036925852299\n",
            "step: 190, loss: 3.5568911698646843e-05\n",
            "step: 200, loss: 0.0001394519640598446\n",
            "step: 210, loss: 0.00013396210852079093\n",
            "step: 220, loss: 6.979445606702939e-05\n",
            "step: 230, loss: 5.610302832792513e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9797752808988766, f1=0.9798206278026906, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3021877495921217e-05\n",
            "step: 10, loss: 0.000571489566937089\n",
            "step: 20, loss: 4.7818157327128574e-05\n",
            "step: 30, loss: 2.9495991839212365e-05\n",
            "step: 40, loss: 0.0002911607443820685\n",
            "step: 50, loss: 0.00033745376276783645\n",
            "step: 60, loss: 5.7101417041849345e-05\n",
            "step: 70, loss: 0.00018171186093240976\n",
            "step: 80, loss: 3.4572691220091656e-05\n",
            "step: 90, loss: 2.4213881260948256e-05\n",
            "step: 100, loss: 2.71714980044635e-05\n",
            "step: 110, loss: 6.966407818254083e-05\n",
            "step: 120, loss: 0.00012517468712758273\n",
            "step: 130, loss: 5.769404015154578e-05\n",
            "step: 140, loss: 9.3765978817828e-05\n",
            "step: 150, loss: 0.00012859517300967127\n",
            "step: 160, loss: 0.008047889918088913\n",
            "step: 170, loss: 3.3670443372102454e-05\n",
            "step: 180, loss: 6.520949682453647e-05\n",
            "step: 190, loss: 2.8262831619940698e-05\n",
            "step: 200, loss: 3.50938098563347e-05\n",
            "step: 210, loss: 4.1609135223552585e-05\n",
            "step: 220, loss: 3.5439239582046866e-05\n",
            "step: 230, loss: 9.544281783746555e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9799107142857142, f1=0.9787709497206705, best_f1=0.9819819819819819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.066966696176678e-05\n",
            "step: 10, loss: 1.640969276195392e-05\n",
            "step: 20, loss: 6.150231638457626e-05\n",
            "step: 30, loss: 7.819347229087725e-05\n",
            "step: 40, loss: 0.0002878784143831581\n",
            "step: 50, loss: 0.0004801259783562273\n",
            "step: 60, loss: 0.0001839497999753803\n",
            "step: 70, loss: 0.0012119420571252704\n",
            "step: 80, loss: 2.542409310990479e-05\n",
            "step: 90, loss: 5.829031579196453e-05\n",
            "step: 100, loss: 0.00013501591456588358\n",
            "step: 110, loss: 0.0018160495674237609\n",
            "step: 120, loss: 0.007287368644028902\n",
            "step: 130, loss: 3.9845988794695586e-05\n",
            "step: 140, loss: 3.173365257680416e-05\n",
            "step: 150, loss: 9.672277519712225e-05\n",
            "step: 160, loss: 0.0003824495943263173\n",
            "step: 170, loss: 1.6767320630606264e-05\n",
            "step: 180, loss: 0.00012545708159450442\n",
            "step: 190, loss: 0.00014972765347920358\n",
            "step: 200, loss: 0.000402468373067677\n",
            "step: 210, loss: 2.5875326173263602e-05\n",
            "step: 220, loss: 0.008084784261882305\n",
            "step: 230, loss: 6.666833360213786e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.98, f1=0.9788182831661093, best_f1=0.9819819819819819\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 206.12it/s]\n",
            "load_f1 = 0.9831649831649831\n",
            "real_f1 = 0.9832026875699889\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 255.65it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrVM9KP9hstt"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f7093f-33b4-4389-819a-4e156e143697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6322342753410339\n",
            "step: 10, loss: 0.5408704280853271\n",
            "step: 20, loss: 0.523583710193634\n",
            "step: 30, loss: 0.1490127146244049\n",
            "step: 40, loss: 0.17368084192276\n",
            "step: 50, loss: 0.18595919013023376\n",
            "step: 60, loss: 0.06715504080057144\n",
            "step: 70, loss: 0.1337631195783615\n",
            "step: 80, loss: 0.04418506473302841\n",
            "step: 90, loss: 0.2614085078239441\n",
            "step: 100, loss: 0.03933422267436981\n",
            "step: 110, loss: 0.03659733757376671\n",
            "step: 120, loss: 0.12773193418979645\n",
            "step: 130, loss: 0.06950389593839645\n",
            "step: 140, loss: 0.21308594942092896\n",
            "step: 150, loss: 0.04952492192387581\n",
            "step: 160, loss: 0.10250748693943024\n",
            "step: 170, loss: 0.19559890031814575\n",
            "step: 180, loss: 0.10200154036283493\n",
            "step: 190, loss: 0.03325225040316582\n",
            "step: 200, loss: 0.19940772652626038\n",
            "step: 210, loss: 0.09406846761703491\n",
            "step: 220, loss: 0.2800471782684326\n",
            "step: 230, loss: 0.17923583090305328\n",
            "step: 240, loss: 0.10511860996484756\n",
            "step: 250, loss: 0.024465041235089302\n",
            "step: 260, loss: 0.13942429423332214\n",
            "step: 270, loss: 0.027932170778512955\n",
            "step: 280, loss: 0.08261004090309143\n",
            "step: 290, loss: 0.08205541968345642\n",
            "step: 300, loss: 0.08670815080404282\n",
            "step: 310, loss: 0.24042752385139465\n",
            "step: 320, loss: 0.14043736457824707\n",
            "step: 330, loss: 0.05761094391345978\n",
            "step: 340, loss: 0.07244287431240082\n",
            "step: 350, loss: 0.22161749005317688\n",
            "step: 360, loss: 0.13819853961467743\n",
            "step: 370, loss: 0.12629184126853943\n",
            "step: 380, loss: 0.019149092957377434\n",
            "step: 390, loss: 0.05877387151122093\n",
            "step: 400, loss: 0.2394690364599228\n",
            "step: 410, loss: 0.04696466773748398\n",
            "step: 420, loss: 0.058855608105659485\n",
            "step: 430, loss: 0.17166511714458466\n",
            "step: 440, loss: 0.014797236770391464\n",
            "step: 450, loss: 0.008870395831763744\n",
            "step: 460, loss: 0.01909269578754902\n",
            "step: 470, loss: 0.14125855267047882\n",
            "step: 480, loss: 0.09046563506126404\n",
            "step: 490, loss: 0.21604043245315552\n",
            "step: 500, loss: 0.11134421080350876\n",
            "step: 510, loss: 0.07705353200435638\n",
            "step: 520, loss: 0.0964888334274292\n",
            "step: 530, loss: 0.0062089283019304276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.92772186642269, f1=0.923992673992674, best_f1=0.923992673992674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18741579353809357\n",
            "step: 10, loss: 0.03193799778819084\n",
            "step: 20, loss: 0.030968135222792625\n",
            "step: 30, loss: 0.14152728021144867\n",
            "step: 40, loss: 0.08025015145540237\n",
            "step: 50, loss: 0.19317074120044708\n",
            "step: 60, loss: 0.03094249963760376\n",
            "step: 70, loss: 0.035545628517866135\n",
            "step: 80, loss: 0.10316495597362518\n",
            "step: 90, loss: 0.07577692717313766\n",
            "step: 100, loss: 0.10903696715831757\n",
            "step: 110, loss: 0.019358310848474503\n",
            "step: 120, loss: 0.08123113214969635\n",
            "step: 130, loss: 0.15020935237407684\n",
            "step: 140, loss: 0.10068026930093765\n",
            "step: 150, loss: 0.04739644005894661\n",
            "step: 160, loss: 0.05438083037734032\n",
            "step: 170, loss: 0.16687148809432983\n",
            "step: 180, loss: 0.02023128978908062\n",
            "step: 190, loss: 0.044584907591342926\n",
            "step: 200, loss: 0.09051588922739029\n",
            "step: 210, loss: 0.15247783064842224\n",
            "step: 220, loss: 0.04429084435105324\n",
            "step: 230, loss: 0.012627234682440758\n",
            "step: 240, loss: 0.10429372638463974\n",
            "step: 250, loss: 0.011985398828983307\n",
            "step: 260, loss: 0.004522934556007385\n",
            "step: 270, loss: 0.2605387270450592\n",
            "step: 280, loss: 0.035486992448568344\n",
            "step: 290, loss: 0.03865816071629524\n",
            "step: 300, loss: 0.11652872711420059\n",
            "step: 310, loss: 0.07333706319332123\n",
            "step: 320, loss: 0.08177251368761063\n",
            "step: 330, loss: 0.06687607616186142\n",
            "step: 340, loss: 0.013469809666275978\n",
            "step: 350, loss: 0.08003512024879456\n",
            "step: 360, loss: 0.1662840098142624\n",
            "step: 370, loss: 0.15200942754745483\n",
            "step: 380, loss: 0.05153115838766098\n",
            "step: 390, loss: 0.11166876554489136\n",
            "step: 400, loss: 0.08494836837053299\n",
            "step: 410, loss: 0.06295786798000336\n",
            "step: 420, loss: 0.1950501948595047\n",
            "step: 430, loss: 0.01576896756887436\n",
            "step: 440, loss: 0.08162584900856018\n",
            "step: 450, loss: 0.05522236227989197\n",
            "step: 460, loss: 0.02621881105005741\n",
            "step: 470, loss: 0.0931309387087822\n",
            "step: 480, loss: 0.24021796882152557\n",
            "step: 490, loss: 0.024218114092946053\n",
            "step: 500, loss: 0.32096004486083984\n",
            "step: 510, loss: 0.015589328482747078\n",
            "step: 520, loss: 0.03632287308573723\n",
            "step: 530, loss: 0.15255621075630188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9268738574040218, f1=0.9251824817518249, best_f1=0.923992673992674\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0441826730966568\n",
            "step: 10, loss: 0.03131771832704544\n",
            "step: 20, loss: 0.13764990866184235\n",
            "step: 30, loss: 0.11220410466194153\n",
            "step: 40, loss: 0.050907716155052185\n",
            "step: 50, loss: 0.15568652749061584\n",
            "step: 60, loss: 0.005260345060378313\n",
            "step: 70, loss: 0.0094315679743886\n",
            "step: 80, loss: 0.004941311199218035\n",
            "step: 90, loss: 0.007868305779993534\n",
            "step: 100, loss: 0.05615580081939697\n",
            "step: 110, loss: 0.01150115579366684\n",
            "step: 120, loss: 0.18542222678661346\n",
            "step: 130, loss: 0.04202726110816002\n",
            "step: 140, loss: 0.06769528239965439\n",
            "step: 150, loss: 0.045699749141931534\n",
            "step: 160, loss: 0.049739938229322433\n",
            "step: 170, loss: 0.158523827791214\n",
            "step: 180, loss: 0.061872292309999466\n",
            "step: 190, loss: 0.008173956535756588\n",
            "step: 200, loss: 0.03608530014753342\n",
            "step: 210, loss: 0.10054024308919907\n",
            "step: 220, loss: 0.050652723759412766\n",
            "step: 230, loss: 0.11315387487411499\n",
            "step: 240, loss: 0.010724533349275589\n",
            "step: 250, loss: 0.04184827581048012\n",
            "step: 260, loss: 0.11872739344835281\n",
            "step: 270, loss: 0.006083154585212469\n",
            "step: 280, loss: 0.14228668808937073\n",
            "step: 290, loss: 0.006174887530505657\n",
            "step: 300, loss: 0.09836077690124512\n",
            "step: 310, loss: 0.08391545712947845\n",
            "step: 320, loss: 0.019481206312775612\n",
            "step: 330, loss: 0.008276775479316711\n",
            "step: 340, loss: 0.009953660890460014\n",
            "step: 350, loss: 0.009386810474097729\n",
            "step: 360, loss: 0.026169266551733017\n",
            "step: 370, loss: 0.005686056800186634\n",
            "step: 380, loss: 0.018060801550745964\n",
            "step: 390, loss: 0.008293497376143932\n",
            "step: 400, loss: 0.0381668396294117\n",
            "step: 410, loss: 0.026957081630825996\n",
            "step: 420, loss: 0.02597564458847046\n",
            "step: 430, loss: 0.07767129689455032\n",
            "step: 440, loss: 0.024063993245363235\n",
            "step: 450, loss: 0.09402237832546234\n",
            "step: 460, loss: 0.03184078261256218\n",
            "step: 470, loss: 0.05514726787805557\n",
            "step: 480, loss: 0.007322478573769331\n",
            "step: 490, loss: 0.01343369111418724\n",
            "step: 500, loss: 0.002800171496346593\n",
            "step: 510, loss: 0.011028800159692764\n",
            "step: 520, loss: 0.10073144733905792\n",
            "step: 530, loss: 0.057171713560819626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9353187529083293, f1=0.9273066169617894, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006281989626586437\n",
            "step: 10, loss: 0.0027225385420024395\n",
            "step: 20, loss: 0.0006790294428355992\n",
            "step: 30, loss: 0.018199441954493523\n",
            "step: 40, loss: 0.08359578996896744\n",
            "step: 50, loss: 0.09942484647035599\n",
            "step: 60, loss: 0.05751834437251091\n",
            "step: 70, loss: 0.005214907228946686\n",
            "step: 80, loss: 0.01380927488207817\n",
            "step: 90, loss: 0.029539695009589195\n",
            "step: 100, loss: 0.0048690298572182655\n",
            "step: 110, loss: 0.023868869990110397\n",
            "step: 120, loss: 0.03301266208291054\n",
            "step: 130, loss: 0.01895071007311344\n",
            "step: 140, loss: 0.004104114603251219\n",
            "step: 150, loss: 0.002487856661900878\n",
            "step: 160, loss: 0.0119104515761137\n",
            "step: 170, loss: 0.003353266976773739\n",
            "step: 180, loss: 0.015540062449872494\n",
            "step: 190, loss: 0.020617786794900894\n",
            "step: 200, loss: 0.013670647516846657\n",
            "step: 210, loss: 0.14083324372768402\n",
            "step: 220, loss: 0.01945408061146736\n",
            "step: 230, loss: 0.07956930994987488\n",
            "step: 240, loss: 0.011040345765650272\n",
            "step: 250, loss: 0.05671041086316109\n",
            "step: 260, loss: 0.006262488197535276\n",
            "step: 270, loss: 0.0077774724923074245\n",
            "step: 280, loss: 0.029507657513022423\n",
            "step: 290, loss: 0.015324395149946213\n",
            "step: 300, loss: 0.00026352997520007193\n",
            "step: 310, loss: 0.0068644373677670956\n",
            "step: 320, loss: 0.0352204404771328\n",
            "step: 330, loss: 0.08082101494073868\n",
            "step: 340, loss: 0.0028543462976813316\n",
            "step: 350, loss: 0.03760666400194168\n",
            "step: 360, loss: 0.00512770377099514\n",
            "step: 370, loss: 0.009954886510968208\n",
            "step: 380, loss: 0.0291168000549078\n",
            "step: 390, loss: 0.050036389380693436\n",
            "step: 400, loss: 0.0024203634820878506\n",
            "step: 410, loss: 0.0025794277898967266\n",
            "step: 420, loss: 0.008877637796103954\n",
            "step: 430, loss: 0.24034643173217773\n",
            "step: 440, loss: 0.011493087746202946\n",
            "step: 450, loss: 0.0033056207466870546\n",
            "step: 460, loss: 0.04608505591750145\n",
            "step: 470, loss: 0.007756899110972881\n",
            "step: 480, loss: 0.21438738703727722\n",
            "step: 490, loss: 0.005909201223403215\n",
            "step: 500, loss: 0.01734284870326519\n",
            "step: 510, loss: 0.08859635144472122\n",
            "step: 520, loss: 0.05940551310777664\n",
            "step: 530, loss: 0.029966311529278755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9231464737793852, f1=0.9246823956442831, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06552721560001373\n",
            "step: 10, loss: 0.08055546134710312\n",
            "step: 20, loss: 0.002584031317383051\n",
            "step: 30, loss: 0.008968268521130085\n",
            "step: 40, loss: 0.0095588443800807\n",
            "step: 50, loss: 0.00863940455019474\n",
            "step: 60, loss: 0.17101788520812988\n",
            "step: 70, loss: 0.08944030851125717\n",
            "step: 80, loss: 0.0014904835261404514\n",
            "step: 90, loss: 0.007398639805614948\n",
            "step: 100, loss: 0.024298040196299553\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 110, loss: 0.0015928345965221524\n",
            "step: 120, loss: 0.013186979107558727\n",
            "step: 130, loss: 0.005009468644857407\n",
            "step: 140, loss: 0.001338920439593494\n",
            "step: 150, loss: 0.0027808877639472485\n",
            "step: 160, loss: 0.002340702572837472\n",
            "step: 170, loss: 0.03746198117733002\n",
            "step: 180, loss: 0.0036231600679457188\n",
            "step: 190, loss: 0.008374166674911976\n",
            "step: 200, loss: 0.00197369116358459\n",
            "step: 210, loss: 0.08988475799560547\n",
            "step: 220, loss: 0.002155038993805647\n",
            "step: 230, loss: 0.004746838007122278\n",
            "step: 240, loss: 0.029375722631812096\n",
            "step: 250, loss: 0.011292141862213612\n",
            "step: 260, loss: 0.009541928768157959\n",
            "step: 270, loss: 0.0006901574670337141\n",
            "step: 280, loss: 0.015505513176321983\n",
            "step: 290, loss: 0.12062451243400574\n",
            "step: 300, loss: 0.015097645111382008\n",
            "step: 310, loss: 0.009880187921226025\n",
            "step: 320, loss: 0.09474422037601471\n",
            "step: 330, loss: 0.05617875978350639\n",
            "step: 340, loss: 0.0025574357714504004\n",
            "step: 350, loss: 0.07778268307447433\n",
            "step: 360, loss: 0.002325251465663314\n",
            "step: 370, loss: 0.007440947461873293\n",
            "step: 380, loss: 0.002786786062642932\n",
            "step: 390, loss: 0.001908269478008151\n",
            "step: 400, loss: 0.06112837791442871\n",
            "step: 410, loss: 0.004828553646802902\n",
            "step: 420, loss: 0.01616903580725193\n",
            "step: 430, loss: 0.0044789682142436504\n",
            "step: 440, loss: 0.021240651607513428\n",
            "step: 450, loss: 0.0025660006795078516\n",
            "step: 460, loss: 0.00352600053884089\n",
            "step: 470, loss: 0.004376404453068972\n",
            "step: 480, loss: 0.015406597405672073\n",
            "step: 490, loss: 0.0009327676962129772\n",
            "step: 500, loss: 0.035809434950351715\n",
            "step: 510, loss: 0.01761043816804886\n",
            "step: 520, loss: 0.0020885004196316004\n",
            "step: 530, loss: 0.014144103974103928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9263947491795592, f1=0.9242990654205607, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011396723799407482\n",
            "step: 10, loss: 0.0003842939913738519\n",
            "step: 20, loss: 0.24195586144924164\n",
            "step: 30, loss: 0.002217572880908847\n",
            "step: 40, loss: 0.008290602825582027\n",
            "step: 50, loss: 0.007047407794743776\n",
            "step: 60, loss: 0.004112366121262312\n",
            "step: 70, loss: 0.001399695174768567\n",
            "step: 80, loss: 0.0005744108348153532\n",
            "step: 90, loss: 0.04332095757126808\n",
            "step: 100, loss: 0.003463148605078459\n",
            "step: 110, loss: 0.0023967719171196222\n",
            "step: 120, loss: 0.0015275543555617332\n",
            "step: 130, loss: 0.012880127876996994\n",
            "step: 140, loss: 0.04716102033853531\n",
            "step: 150, loss: 0.0010325326584279537\n",
            "step: 160, loss: 0.0010095973266288638\n",
            "step: 170, loss: 0.04044920578598976\n",
            "step: 180, loss: 0.0006842154543846846\n",
            "step: 190, loss: 0.013887298293411732\n",
            "step: 200, loss: 0.009918991476297379\n",
            "step: 210, loss: 0.016008468344807625\n",
            "step: 220, loss: 0.0037406287156045437\n",
            "step: 230, loss: 0.005408292170614004\n",
            "step: 240, loss: 0.0648561343550682\n",
            "step: 250, loss: 0.0031741023994982243\n",
            "step: 260, loss: 0.0015899627469480038\n",
            "step: 270, loss: 0.1436314582824707\n",
            "step: 280, loss: 0.06589912623167038\n",
            "step: 290, loss: 0.001131078926846385\n",
            "step: 300, loss: 0.02530484087765217\n",
            "step: 310, loss: 0.002000723499804735\n",
            "step: 320, loss: 0.004164023790508509\n",
            "step: 330, loss: 0.01271990966051817\n",
            "step: 340, loss: 0.15714266896247864\n",
            "step: 350, loss: 0.024008123204112053\n",
            "step: 360, loss: 0.004037526436150074\n",
            "step: 370, loss: 0.00864609144628048\n",
            "step: 380, loss: 0.0011871438473463058\n",
            "step: 390, loss: 0.007738953456282616\n",
            "step: 400, loss: 0.00029123135027475655\n",
            "step: 410, loss: 0.0030174695421010256\n",
            "step: 420, loss: 0.0010335989063605666\n",
            "step: 430, loss: 0.0834420695900917\n",
            "step: 440, loss: 0.10911819338798523\n",
            "step: 450, loss: 0.00043778802501037717\n",
            "step: 460, loss: 0.000617483863607049\n",
            "step: 470, loss: 0.016385184600949287\n",
            "step: 480, loss: 0.0071140252985060215\n",
            "step: 490, loss: 0.004140689503401518\n",
            "step: 500, loss: 0.00022630399325862527\n",
            "step: 510, loss: 0.015991751104593277\n",
            "step: 520, loss: 0.002099620411172509\n",
            "step: 530, loss: 0.08393146097660065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.924618320610687, f1=0.9234404536862004, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00031169914291240275\n",
            "step: 10, loss: 0.010283399373292923\n",
            "step: 20, loss: 0.001684085582382977\n",
            "step: 30, loss: 0.0008984761661849916\n",
            "step: 40, loss: 0.000498548208270222\n",
            "step: 50, loss: 0.0064149415120482445\n",
            "step: 60, loss: 0.0485406257212162\n",
            "step: 70, loss: 0.03753560036420822\n",
            "step: 80, loss: 0.009361266158521175\n",
            "step: 90, loss: 0.0001643301802687347\n",
            "step: 100, loss: 0.0010167600121349096\n",
            "step: 110, loss: 0.0011050673201680183\n",
            "step: 120, loss: 0.0002119772252626717\n",
            "step: 130, loss: 0.003117576241493225\n",
            "step: 140, loss: 0.00024349008162971586\n",
            "step: 150, loss: 0.0174198467284441\n",
            "step: 160, loss: 0.0007258100667968392\n",
            "step: 170, loss: 0.007008664309978485\n",
            "step: 180, loss: 0.02053200453519821\n",
            "step: 190, loss: 0.009550072252750397\n",
            "step: 200, loss: 0.0008869237499311566\n",
            "step: 210, loss: 0.016557075083255768\n",
            "step: 220, loss: 0.0005868254229426384\n",
            "step: 230, loss: 0.005909034516662359\n",
            "step: 240, loss: 0.01658540405333042\n",
            "step: 250, loss: 0.01894763670861721\n",
            "step: 260, loss: 0.0009835308883339167\n",
            "step: 270, loss: 0.1291879266500473\n",
            "step: 280, loss: 0.0007495644385926425\n",
            "step: 290, loss: 0.0010711801005527377\n",
            "step: 300, loss: 0.002543291077017784\n",
            "step: 310, loss: 0.001371196936815977\n",
            "step: 320, loss: 0.00011956487287534401\n",
            "step: 330, loss: 0.0007326148333959281\n",
            "step: 340, loss: 0.015881575644016266\n",
            "step: 350, loss: 0.014363614842295647\n",
            "step: 360, loss: 0.0005018475349061191\n",
            "step: 370, loss: 0.00035022158408537507\n",
            "step: 380, loss: 0.03287065401673317\n",
            "step: 390, loss: 0.0036975329276174307\n",
            "step: 400, loss: 0.008704678155481815\n",
            "step: 410, loss: 0.0063106948509812355\n",
            "step: 420, loss: 0.028039639815688133\n",
            "step: 430, loss: 0.00027445924933999777\n",
            "step: 440, loss: 0.00043252581963315606\n",
            "step: 450, loss: 0.0017018248327076435\n",
            "step: 460, loss: 0.00016534203314222395\n",
            "step: 470, loss: 0.014156284742057323\n",
            "step: 480, loss: 0.10643943399190903\n",
            "step: 490, loss: 0.004011459648609161\n",
            "step: 500, loss: 0.0007829935639165342\n",
            "step: 510, loss: 0.0009510979289188981\n",
            "step: 520, loss: 0.11424171924591064\n",
            "step: 530, loss: 0.18629665672779083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9178470254957507, f1=0.9111933395004626, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006069974973797798\n",
            "step: 10, loss: 0.021028082817792892\n",
            "step: 20, loss: 0.0015337337972596288\n",
            "step: 30, loss: 0.002116419840604067\n",
            "step: 40, loss: 0.00021677042241208255\n",
            "step: 50, loss: 0.0053200083784759045\n",
            "step: 60, loss: 0.0012443923624232411\n",
            "step: 70, loss: 0.0001550250017317012\n",
            "step: 80, loss: 0.018107039853930473\n",
            "step: 90, loss: 0.0002697952149901539\n",
            "step: 100, loss: 0.0007751962402835488\n",
            "step: 110, loss: 0.024284930899739265\n",
            "step: 120, loss: 0.0012684524990618229\n",
            "step: 130, loss: 0.00023085673456080258\n",
            "step: 140, loss: 0.005839817691594362\n",
            "step: 150, loss: 0.002996866125613451\n",
            "step: 160, loss: 0.013097675517201424\n",
            "step: 170, loss: 0.001117425737902522\n",
            "step: 180, loss: 0.0025297889951616526\n",
            "step: 190, loss: 0.014243213459849358\n",
            "step: 200, loss: 0.0036141551099717617\n",
            "step: 210, loss: 0.0007524169632233679\n",
            "step: 220, loss: 0.0007020970224402845\n",
            "step: 230, loss: 0.00163585739210248\n",
            "step: 240, loss: 0.0011147791519761086\n",
            "step: 250, loss: 0.00012311417958699167\n",
            "step: 260, loss: 0.00033421628177165985\n",
            "step: 270, loss: 0.007759368512779474\n",
            "step: 280, loss: 0.00587490014731884\n",
            "step: 290, loss: 0.00011610790534177795\n",
            "step: 300, loss: 0.00013435517030302435\n",
            "step: 310, loss: 0.005830511916428804\n",
            "step: 320, loss: 0.014820123091340065\n",
            "step: 330, loss: 0.0033891331404447556\n",
            "step: 340, loss: 0.0005955698434263468\n",
            "step: 350, loss: 4.954752148478292e-05\n",
            "step: 360, loss: 0.0008657635189592838\n",
            "step: 370, loss: 0.00021888338960707188\n",
            "step: 380, loss: 0.0007376870489679277\n",
            "step: 390, loss: 0.11908270418643951\n",
            "step: 400, loss: 0.00012988634989596903\n",
            "step: 410, loss: 0.00021460592688526958\n",
            "step: 420, loss: 0.0018212919821962714\n",
            "step: 430, loss: 0.01860579289495945\n",
            "step: 440, loss: 0.002963392762467265\n",
            "step: 450, loss: 0.0007605324499309063\n",
            "step: 460, loss: 0.0012300023809075356\n",
            "step: 470, loss: 0.0005581468576565385\n",
            "step: 480, loss: 0.0025270995683968067\n",
            "step: 490, loss: 0.0012828679755330086\n",
            "step: 500, loss: 0.002538844244554639\n",
            "step: 510, loss: 0.0005600863951258361\n",
            "step: 520, loss: 0.0011736652813851833\n",
            "step: 530, loss: 0.002243548398837447\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9247211895910781, f1=0.9270544783010157, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007611307664774358\n",
            "step: 10, loss: 0.0018057284178212285\n",
            "step: 20, loss: 0.002756665460765362\n",
            "step: 30, loss: 0.000695208553224802\n",
            "step: 40, loss: 0.0002595694677438587\n",
            "step: 50, loss: 0.0009759735548868775\n",
            "step: 60, loss: 4.942653686157428e-05\n",
            "step: 70, loss: 0.0004642940766643733\n",
            "step: 80, loss: 0.0017070862231776118\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 90, loss: 0.0001529146684333682\n",
            "step: 100, loss: 0.0017023079562932253\n",
            "step: 110, loss: 0.0001562800316605717\n",
            "step: 120, loss: 0.001937009976245463\n",
            "step: 130, loss: 0.02670702524483204\n",
            "step: 140, loss: 0.05635204538702965\n",
            "step: 150, loss: 7.427267701132223e-05\n",
            "step: 160, loss: 0.0037294584326446056\n",
            "step: 170, loss: 0.013267877511680126\n",
            "step: 180, loss: 0.00042188374209217727\n",
            "step: 190, loss: 0.000311357929604128\n",
            "step: 200, loss: 0.0003205654793418944\n",
            "step: 210, loss: 0.00019440613687038422\n",
            "step: 220, loss: 0.007061345037072897\n",
            "step: 230, loss: 0.00015062035527080297\n",
            "step: 240, loss: 0.1413906365633011\n",
            "step: 250, loss: 0.0016124008689075708\n",
            "step: 260, loss: 0.026845822110772133\n",
            "step: 270, loss: 3.8081696402514353e-05\n",
            "step: 280, loss: 0.014518075622618198\n",
            "step: 290, loss: 0.048134904354810715\n",
            "step: 300, loss: 0.00033126151538453996\n",
            "step: 310, loss: 0.007221641950309277\n",
            "step: 320, loss: 0.001232979353517294\n",
            "step: 330, loss: 0.024264028295874596\n",
            "step: 340, loss: 0.001116272178478539\n",
            "step: 350, loss: 0.001178797334432602\n",
            "step: 360, loss: 0.00013610887981485575\n",
            "step: 370, loss: 0.0041011618450284\n",
            "step: 380, loss: 0.001449335366487503\n",
            "step: 390, loss: 0.0004500871291384101\n",
            "step: 400, loss: 0.007555874530225992\n",
            "step: 410, loss: 0.0016465706285089254\n",
            "step: 420, loss: 0.00013570925511885434\n",
            "step: 430, loss: 0.0002992751251440495\n",
            "step: 440, loss: 0.0017538603860884905\n",
            "step: 450, loss: 0.00043697957880795\n",
            "step: 460, loss: 0.00044999580131843686\n",
            "step: 470, loss: 0.0001290318905375898\n",
            "step: 480, loss: 0.0008040433749556541\n",
            "step: 490, loss: 0.004260864574462175\n",
            "step: 500, loss: 0.0009129149257205427\n",
            "step: 510, loss: 0.0017464299453422427\n",
            "step: 520, loss: 0.0003665339609142393\n",
            "step: 530, loss: 0.0002987234038300812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9233627496516488, f1=0.923992673992674, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002370211877860129\n",
            "step: 10, loss: 0.00015046342741698027\n",
            "step: 20, loss: 0.0001416628365404904\n",
            "step: 30, loss: 4.8325484385713935e-05\n",
            "step: 40, loss: 0.006672816816717386\n",
            "step: 50, loss: 0.0004019698826596141\n",
            "step: 60, loss: 9.159769979305565e-05\n",
            "step: 70, loss: 0.0005965133314020932\n",
            "step: 80, loss: 0.0011847903952002525\n",
            "step: 90, loss: 0.00201031775213778\n",
            "step: 100, loss: 0.0001068591300281696\n",
            "step: 110, loss: 0.00015900017751846462\n",
            "step: 120, loss: 9.210191637976095e-05\n",
            "step: 130, loss: 0.00010701100836740807\n",
            "step: 140, loss: 0.004450415261089802\n",
            "step: 150, loss: 0.003194632474333048\n",
            "step: 160, loss: 0.0028831639792770147\n",
            "step: 170, loss: 0.0002711694687604904\n",
            "step: 180, loss: 0.0020988646429032087\n",
            "step: 190, loss: 0.0004730326181743294\n",
            "step: 200, loss: 0.0018034002278000116\n",
            "step: 210, loss: 0.0029832343570888042\n",
            "step: 220, loss: 0.001314705703407526\n",
            "step: 230, loss: 0.003892711130902171\n",
            "step: 240, loss: 0.0004599791718646884\n",
            "step: 250, loss: 0.062134455889463425\n",
            "step: 260, loss: 0.00092978187603876\n",
            "step: 270, loss: 0.00012802882702089846\n",
            "step: 280, loss: 0.001449195435270667\n",
            "step: 290, loss: 0.0006390062626451254\n",
            "step: 300, loss: 8.036869985517114e-05\n",
            "step: 310, loss: 0.0002004581328947097\n",
            "step: 320, loss: 0.004559352993965149\n",
            "step: 330, loss: 0.004852449521422386\n",
            "step: 340, loss: 0.0003251806483604014\n",
            "step: 350, loss: 0.022897906601428986\n",
            "step: 360, loss: 0.014567120932042599\n",
            "step: 370, loss: 0.0030197049491107464\n",
            "step: 380, loss: 0.0015855649253353477\n",
            "step: 390, loss: 0.00034738730755634606\n",
            "step: 400, loss: 0.0024525783956050873\n",
            "step: 410, loss: 0.004069699440151453\n",
            "step: 420, loss: 0.00026663931203074753\n",
            "step: 430, loss: 0.00011809538409579545\n",
            "step: 440, loss: 0.007883870042860508\n",
            "step: 450, loss: 8.055928628891706e-05\n",
            "step: 460, loss: 0.0001110490266000852\n",
            "step: 470, loss: 0.0001510552247054875\n",
            "step: 480, loss: 2.3342281565419398e-05\n",
            "step: 490, loss: 0.00026279629673808813\n",
            "step: 500, loss: 0.003530255751684308\n",
            "step: 510, loss: 0.0001862456410890445\n",
            "step: 520, loss: 0.00018313944747205824\n",
            "step: 530, loss: 0.0010759696597233415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9251059821008009, f1=0.925891181988743, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004691213835030794\n",
            "step: 10, loss: 0.010594181716442108\n",
            "step: 20, loss: 0.00043823086889460683\n",
            "step: 30, loss: 0.0006772693595848978\n",
            "step: 40, loss: 0.05836055055260658\n",
            "step: 50, loss: 0.0015192842110991478\n",
            "step: 60, loss: 0.02075786516070366\n",
            "step: 70, loss: 0.0002280693151988089\n",
            "step: 80, loss: 0.003908604383468628\n",
            "step: 90, loss: 5.3101361118024215e-05\n",
            "step: 100, loss: 0.0018792742630466819\n",
            "step: 110, loss: 0.0007329937652684748\n",
            "step: 120, loss: 0.00046343295252881944\n",
            "step: 130, loss: 0.005455214995890856\n",
            "step: 140, loss: 0.004025232512503862\n",
            "step: 150, loss: 7.137224019970745e-05\n",
            "step: 160, loss: 0.00016604815027676523\n",
            "step: 170, loss: 8.742364298086613e-05\n",
            "step: 180, loss: 0.02160881645977497\n",
            "step: 190, loss: 0.00191867642570287\n",
            "step: 200, loss: 0.003695595543831587\n",
            "step: 210, loss: 0.0007218162063509226\n",
            "step: 220, loss: 0.001233998453244567\n",
            "step: 230, loss: 0.0002964069426525384\n",
            "step: 240, loss: 0.0009064035839401186\n",
            "step: 250, loss: 6.347743328660727e-05\n",
            "step: 260, loss: 0.00010244065924780443\n",
            "step: 270, loss: 0.0026955304201692343\n",
            "step: 280, loss: 0.00020181421132292598\n",
            "step: 290, loss: 0.003526706248521805\n",
            "step: 300, loss: 0.004871679469943047\n",
            "step: 310, loss: 0.011182836256921291\n",
            "step: 320, loss: 8.768749830778688e-05\n",
            "step: 330, loss: 0.0004986825515516102\n",
            "step: 340, loss: 0.16435886919498444\n",
            "step: 350, loss: 0.0005604855250567198\n",
            "step: 360, loss: 0.002063812455162406\n",
            "step: 370, loss: 0.0022219240199774504\n",
            "step: 380, loss: 0.0001482973893871531\n",
            "step: 390, loss: 0.00020173768280074\n",
            "step: 400, loss: 0.0011890018358826637\n",
            "step: 410, loss: 8.836227789288387e-05\n",
            "step: 420, loss: 0.0001268740015802905\n",
            "step: 430, loss: 3.462496169959195e-05\n",
            "step: 440, loss: 0.001476971199735999\n",
            "step: 450, loss: 0.00011536334204720333\n",
            "step: 460, loss: 0.001494944328442216\n",
            "step: 470, loss: 0.00022059044567868114\n",
            "step: 480, loss: 0.0015208958648145199\n",
            "step: 490, loss: 0.0010819482849910855\n",
            "step: 500, loss: 0.003576661692932248\n",
            "step: 510, loss: 0.00036646644002757967\n",
            "step: 520, loss: 1.7646427295403555e-05\n",
            "step: 530, loss: 8.498256647726521e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9279999999999999, f1=0.9296037296037295, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.2978270357707515e-05\n",
            "step: 10, loss: 4.0420040022581816e-05\n",
            "step: 20, loss: 0.00010819955787155777\n",
            "step: 30, loss: 0.0007759088766761124\n",
            "step: 40, loss: 0.0001839964825194329\n",
            "step: 50, loss: 0.04978008568286896\n",
            "step: 60, loss: 0.0014046597061678767\n",
            "step: 70, loss: 0.001565490267239511\n",
            "step: 80, loss: 0.00023644852626603097\n",
            "step: 90, loss: 0.00012131755647715181\n",
            "step: 100, loss: 0.00017519001266919076\n",
            "step: 110, loss: 0.0002973974624183029\n",
            "step: 120, loss: 0.0004714673850685358\n",
            "step: 130, loss: 0.0012158085592091084\n",
            "step: 140, loss: 1.880119634734001e-05\n",
            "step: 150, loss: 4.7683723096270114e-05\n",
            "step: 160, loss: 2.5755769456736743e-05\n",
            "step: 170, loss: 0.00017819782078731805\n",
            "step: 180, loss: 6.908526120241731e-05\n",
            "step: 190, loss: 6.609653064515442e-05\n",
            "step: 200, loss: 4.973252725903876e-05\n",
            "step: 210, loss: 6.108388333814219e-05\n",
            "step: 220, loss: 0.008433660492300987\n",
            "step: 230, loss: 1.7854783436632715e-05\n",
            "step: 240, loss: 0.008126065135002136\n",
            "step: 250, loss: 2.3058302758727223e-05\n",
            "step: 260, loss: 5.1883998821722344e-05\n",
            "step: 270, loss: 0.00550859747454524\n",
            "step: 280, loss: 0.0005938881658948958\n",
            "step: 290, loss: 0.005603653844445944\n",
            "step: 300, loss: 0.09377526491880417\n",
            "step: 310, loss: 0.0007598432712256908\n",
            "step: 320, loss: 3.024237412319053e-05\n",
            "step: 330, loss: 7.136902422644198e-05\n",
            "step: 340, loss: 0.00011924338468816131\n",
            "step: 350, loss: 0.00018772676412481815\n",
            "step: 360, loss: 0.004514878615736961\n",
            "step: 370, loss: 0.3295959532260895\n",
            "step: 380, loss: 0.00030403019627556205\n",
            "step: 390, loss: 0.01745247095823288\n",
            "step: 400, loss: 5.3141651733312756e-05\n",
            "step: 410, loss: 0.00027573155239224434\n",
            "step: 420, loss: 0.024752870202064514\n",
            "step: 430, loss: 0.004485808778554201\n",
            "step: 440, loss: 9.63168204179965e-05\n",
            "step: 450, loss: 0.00015199987683445215\n",
            "step: 460, loss: 6.193441367940977e-05\n",
            "step: 470, loss: 0.0806107223033905\n",
            "step: 480, loss: 0.00018795568030327559\n",
            "step: 490, loss: 0.0007843024795874953\n",
            "step: 500, loss: 8.345155220013112e-05\n",
            "step: 510, loss: 0.0028137427289038897\n",
            "step: 520, loss: 0.000301576656056568\n",
            "step: 530, loss: 2.9003864256083034e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.924020764511562, f1=0.9256661991584852, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000470706116175279\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.0017225549090653658\n",
            "step: 20, loss: 0.00015944623737595975\n",
            "step: 30, loss: 0.0003991667181253433\n",
            "step: 40, loss: 4.0115231968229637e-05\n",
            "step: 50, loss: 0.005064442288130522\n",
            "step: 60, loss: 0.0007603391422890127\n",
            "step: 70, loss: 0.00029997562523931265\n",
            "step: 80, loss: 0.0001318665308644995\n",
            "step: 90, loss: 0.037893764674663544\n",
            "step: 100, loss: 0.00027594377752393484\n",
            "step: 110, loss: 5.385743861552328e-05\n",
            "step: 120, loss: 0.0002875733480323106\n",
            "step: 130, loss: 0.00028979225317016244\n",
            "step: 140, loss: 0.00020767253590747714\n",
            "step: 150, loss: 0.0003457533603068441\n",
            "step: 160, loss: 0.07452693581581116\n",
            "step: 170, loss: 0.0002612207317724824\n",
            "step: 180, loss: 2.906763256760314e-05\n",
            "step: 190, loss: 0.0018670862773433328\n",
            "step: 200, loss: 8.14655504655093e-05\n",
            "step: 210, loss: 0.00017356194439344108\n",
            "step: 220, loss: 0.00030821646214462817\n",
            "step: 230, loss: 0.0003135166480205953\n",
            "step: 240, loss: 0.0003976882726419717\n",
            "step: 250, loss: 0.00015571160474792123\n",
            "step: 260, loss: 0.0008520273258909583\n",
            "step: 270, loss: 3.704003393067978e-05\n",
            "step: 280, loss: 6.37486664345488e-05\n",
            "step: 290, loss: 8.542381692677736e-05\n",
            "step: 300, loss: 0.0001514780306024477\n",
            "step: 310, loss: 0.004474738147109747\n",
            "step: 320, loss: 0.01249807421118021\n",
            "step: 330, loss: 0.0033873948268592358\n",
            "step: 340, loss: 0.00019889218674506992\n",
            "step: 350, loss: 0.0003093962150160223\n",
            "step: 360, loss: 6.572713755303994e-05\n",
            "step: 370, loss: 0.00012080020678695291\n",
            "step: 380, loss: 0.00045204575872048736\n",
            "step: 390, loss: 0.001572229783050716\n",
            "step: 400, loss: 0.0001613130298210308\n",
            "step: 410, loss: 0.001028162776492536\n",
            "step: 420, loss: 7.079893111949787e-05\n",
            "step: 430, loss: 0.0013663740828633308\n",
            "step: 440, loss: 0.001630643499083817\n",
            "step: 450, loss: 0.006842336617410183\n",
            "step: 460, loss: 0.007156752049922943\n",
            "step: 470, loss: 0.00027096335543319583\n",
            "step: 480, loss: 6.19588972767815e-05\n",
            "step: 490, loss: 6.666514673270285e-05\n",
            "step: 500, loss: 4.8294063162757084e-05\n",
            "step: 510, loss: 0.00029093417106196284\n",
            "step: 520, loss: 0.003515374381095171\n",
            "step: 530, loss: 4.514653846854344e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9235880398671096, f1=0.9262564584311883, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000506674696225673\n",
            "step: 10, loss: 0.00020781852072104812\n",
            "step: 20, loss: 0.0004485551326069981\n",
            "step: 30, loss: 3.95294810004998e-05\n",
            "step: 40, loss: 0.00184946833178401\n",
            "step: 50, loss: 0.0009200516506098211\n",
            "step: 60, loss: 0.0011141477152705193\n",
            "step: 70, loss: 3.8756617868784815e-05\n",
            "step: 80, loss: 0.00022810566588304937\n",
            "step: 90, loss: 0.0002603710163384676\n",
            "step: 100, loss: 0.002347462810575962\n",
            "step: 110, loss: 0.00015681145305279642\n",
            "step: 120, loss: 0.0001551572058815509\n",
            "step: 130, loss: 0.016474341973662376\n",
            "step: 140, loss: 0.008556803688406944\n",
            "step: 150, loss: 6.222486990736797e-05\n",
            "step: 160, loss: 5.077393871033564e-05\n",
            "step: 170, loss: 0.001421546097844839\n",
            "step: 180, loss: 2.6619989512255415e-05\n",
            "step: 190, loss: 0.00010893047146964818\n",
            "step: 200, loss: 0.002607255708426237\n",
            "step: 210, loss: 0.0001302546152146533\n",
            "step: 220, loss: 4.865380105911754e-05\n",
            "step: 230, loss: 0.018987605348229408\n",
            "step: 240, loss: 0.00016796017007436603\n",
            "step: 250, loss: 0.00010934309830190614\n",
            "step: 260, loss: 0.0023119794204831123\n",
            "step: 270, loss: 6.551338447025046e-05\n",
            "step: 280, loss: 6.108253728598356e-05\n",
            "step: 290, loss: 0.0008294066064991057\n",
            "step: 300, loss: 0.0001624959404580295\n",
            "step: 310, loss: 0.003957983106374741\n",
            "step: 320, loss: 0.0003006841870956123\n",
            "step: 330, loss: 0.00021098021534271538\n",
            "step: 340, loss: 0.05189394950866699\n",
            "step: 350, loss: 0.001737135462462902\n",
            "step: 360, loss: 0.00045470992336049676\n",
            "step: 370, loss: 7.87892349762842e-05\n",
            "step: 380, loss: 9.796453377930447e-05\n",
            "step: 390, loss: 0.015381338074803352\n",
            "step: 400, loss: 0.00012614102161023766\n",
            "step: 410, loss: 7.005969382589683e-05\n",
            "step: 420, loss: 3.9169026422314346e-05\n",
            "step: 430, loss: 0.002239689463749528\n",
            "step: 440, loss: 0.00014116572856437415\n",
            "step: 450, loss: 0.00015059176075737923\n",
            "step: 460, loss: 7.119214569684118e-05\n",
            "step: 470, loss: 0.0011633670656010509\n",
            "step: 480, loss: 0.0001111097153625451\n",
            "step: 490, loss: 3.5024306271225214e-05\n",
            "step: 500, loss: 0.00013855966972187161\n",
            "step: 510, loss: 0.0006861527217552066\n",
            "step: 520, loss: 5.3344785555964336e-05\n",
            "step: 530, loss: 5.25333707628306e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9202226345083488, f1=0.9247706422018349, best_f1=0.9273066169617894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.6852984117576852e-05\n",
            "step: 10, loss: 1.429372150596464e-05\n",
            "step: 20, loss: 0.00014547089813277125\n",
            "step: 30, loss: 7.193500641733408e-05\n",
            "step: 40, loss: 0.004681602586060762\n",
            "step: 50, loss: 0.0037333837244659662\n",
            "step: 60, loss: 2.095800118695479e-05\n",
            "step: 70, loss: 0.0007054278394207358\n",
            "step: 80, loss: 4.034408266306855e-05\n",
            "step: 90, loss: 3.004320024047047e-05\n",
            "step: 100, loss: 0.0001027195030474104\n",
            "step: 110, loss: 0.0032089308369904757\n",
            "step: 120, loss: 0.031122397631406784\n",
            "step: 130, loss: 0.18771006166934967\n",
            "step: 140, loss: 0.00016600980598013848\n",
            "step: 150, loss: 5.674261410604231e-05\n",
            "step: 160, loss: 0.00016258865070994943\n",
            "step: 170, loss: 5.517115641850978e-05\n",
            "step: 180, loss: 0.00036490033380687237\n",
            "step: 190, loss: 0.0018414832884445786\n",
            "step: 200, loss: 0.04084758833050728\n",
            "step: 210, loss: 0.0006854118546471\n",
            "step: 220, loss: 5.6837176089175045e-05\n",
            "step: 230, loss: 0.00011865880514960736\n",
            "step: 240, loss: 5.450102617032826e-05\n",
            "step: 250, loss: 3.6990008084103465e-05\n",
            "step: 260, loss: 4.274760794942267e-05\n",
            "step: 270, loss: 2.7918886189581826e-05\n",
            "step: 280, loss: 3.55015326931607e-05\n",
            "step: 290, loss: 1.754952063492965e-05\n",
            "step: 300, loss: 1.9583469111239538e-05\n",
            "step: 310, loss: 6.470639345934615e-05\n",
            "step: 320, loss: 0.0002427146100671962\n",
            "step: 330, loss: 2.0335814042482525e-05\n",
            "step: 340, loss: 3.129323886241764e-05\n",
            "step: 350, loss: 2.988901724165771e-05\n",
            "step: 360, loss: 0.0002187026693718508\n",
            "step: 370, loss: 3.4548436815384775e-05\n",
            "step: 380, loss: 0.0007116995984688401\n",
            "step: 390, loss: 0.002041775966063142\n",
            "step: 400, loss: 0.00010049864067696035\n",
            "step: 410, loss: 5.5362965213134885e-05\n",
            "step: 420, loss: 0.00010748136264737695\n",
            "step: 430, loss: 0.0008655879646539688\n",
            "step: 440, loss: 6.407079490600154e-05\n",
            "step: 450, loss: 1.7683545593172312e-05\n",
            "step: 460, loss: 5.3089002904016525e-05\n",
            "step: 470, loss: 0.0004505760734900832\n",
            "step: 480, loss: 5.596923074335791e-05\n",
            "step: 490, loss: 1.57465910888277e-05\n",
            "step: 500, loss: 0.0427996963262558\n",
            "step: 510, loss: 0.0006114896386861801\n",
            "step: 520, loss: 0.00010259923874400556\n",
            "step: 530, loss: 0.0003505299100652337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9251893939393939, f1=0.9240150093808631, best_f1=0.9273066169617894\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:20, 275.07it/s]\n",
            "load_f1 = 0.9395348837209302\n",
            "real_f1 = 0.9406307977736549\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 267.29it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGyaWAphstt"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02560404-ee31-478a-8ede-bb7470a9012a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5627313256263733\n",
            "step: 10, loss: 0.3636850118637085\n",
            "step: 20, loss: 0.36867907643318176\n",
            "step: 30, loss: 0.3171383738517761\n",
            "step: 40, loss: 0.17773061990737915\n",
            "step: 50, loss: 0.42412233352661133\n",
            "step: 60, loss: 0.2381936013698578\n",
            "step: 70, loss: 0.19261719286441803\n",
            "step: 80, loss: 0.21679040789604187\n",
            "step: 90, loss: 0.39025482535362244\n",
            "step: 100, loss: 0.35806670784950256\n",
            "step: 110, loss: 0.2342950850725174\n",
            "step: 120, loss: 0.2790891230106354\n",
            "step: 130, loss: 0.2325652837753296\n",
            "step: 140, loss: 0.16302460432052612\n",
            "step: 150, loss: 0.2714042365550995\n",
            "step: 160, loss: 0.34730198979377747\n",
            "step: 170, loss: 0.2665281295776367\n",
            "step: 180, loss: 0.17581890523433685\n",
            "step: 190, loss: 0.24125449359416962\n",
            "step: 200, loss: 0.33124417066574097\n",
            "step: 210, loss: 0.1888381987810135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5719298245614035, f1=0.6082474226804123, best_f1=0.6082474226804123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11632522195577621\n",
            "step: 10, loss: 0.19165131449699402\n",
            "step: 20, loss: 0.17832064628601074\n",
            "step: 30, loss: 0.19204361736774445\n",
            "step: 40, loss: 0.2147732675075531\n",
            "step: 50, loss: 0.1836603730916977\n",
            "step: 60, loss: 0.37626558542251587\n",
            "step: 70, loss: 0.11170294135808945\n",
            "step: 80, loss: 0.20137391984462738\n",
            "step: 90, loss: 0.09013444930315018\n",
            "step: 100, loss: 0.02407083846628666\n",
            "step: 110, loss: 0.12079711258411407\n",
            "step: 120, loss: 0.18295852839946747\n",
            "step: 130, loss: 0.03479720652103424\n",
            "step: 140, loss: 0.14249208569526672\n",
            "step: 150, loss: 0.24641817808151245\n",
            "step: 160, loss: 0.19672951102256775\n",
            "step: 170, loss: 0.20949013531208038\n",
            "step: 180, loss: 0.1504782736301422\n",
            "step: 190, loss: 0.21385183930397034\n",
            "step: 200, loss: 0.05945597216486931\n",
            "step: 210, loss: 0.1727837175130844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5955056179775281, f1=0.598848368522073, best_f1=0.598848368522073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07283313572406769\n",
            "step: 10, loss: 0.15944956243038177\n",
            "step: 20, loss: 0.13192914426326752\n",
            "step: 30, loss: 0.18072094023227692\n",
            "step: 40, loss: 0.09981321543455124\n",
            "step: 50, loss: 0.044434867799282074\n",
            "step: 60, loss: 0.14078868925571442\n",
            "step: 70, loss: 0.18338023126125336\n",
            "step: 80, loss: 0.1837131828069687\n",
            "step: 90, loss: 0.053550902754068375\n",
            "step: 100, loss: 0.2548820376396179\n",
            "step: 110, loss: 0.15040093660354614\n",
            "step: 120, loss: 0.11738014221191406\n",
            "step: 130, loss: 0.10153563320636749\n",
            "step: 140, loss: 0.13115614652633667\n",
            "step: 150, loss: 0.19873987138271332\n",
            "step: 160, loss: 0.09167853742837906\n",
            "step: 170, loss: 0.18252359330654144\n",
            "step: 180, loss: 0.11461610347032547\n",
            "step: 190, loss: 0.14666791260242462\n",
            "step: 200, loss: 0.049963776022195816\n",
            "step: 210, loss: 0.10398021340370178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6459627329192545, f1=0.6419753086419754, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13195273280143738\n",
            "step: 10, loss: 0.04108063876628876\n",
            "step: 20, loss: 0.20995505154132843\n",
            "step: 30, loss: 0.12753646075725555\n",
            "step: 40, loss: 0.02228063903748989\n",
            "step: 50, loss: 0.2024925947189331\n",
            "step: 60, loss: 0.18741635978221893\n",
            "step: 70, loss: 0.2945934236049652\n",
            "step: 80, loss: 0.12492763996124268\n",
            "step: 90, loss: 0.028382305055856705\n",
            "step: 100, loss: 0.2561679482460022\n",
            "step: 110, loss: 0.32127100229263306\n",
            "step: 120, loss: 0.23987482488155365\n",
            "step: 130, loss: 0.1893261969089508\n",
            "step: 140, loss: 0.1263992339372635\n",
            "step: 150, loss: 0.0391581691801548\n",
            "step: 160, loss: 0.1050899475812912\n",
            "step: 170, loss: 0.146198570728302\n",
            "step: 180, loss: 0.34382691979408264\n",
            "step: 190, loss: 0.10990410298109055\n",
            "step: 200, loss: 0.058949414640665054\n",
            "step: 210, loss: 0.11207396537065506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.5999999999999999, f1=0.6259842519685039, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09496463090181351\n",
            "step: 10, loss: 0.08974441885948181\n",
            "step: 20, loss: 0.21965420246124268\n",
            "step: 30, loss: 0.10705877840518951\n",
            "step: 40, loss: 0.022767236456274986\n",
            "step: 50, loss: 0.19138777256011963\n",
            "step: 60, loss: 0.16287483274936676\n",
            "step: 70, loss: 0.1071845144033432\n",
            "step: 80, loss: 0.06869249790906906\n",
            "step: 90, loss: 0.16836583614349365\n",
            "step: 100, loss: 0.01480349525809288\n",
            "step: 110, loss: 0.10538613051176071\n",
            "step: 120, loss: 0.180852010846138\n",
            "step: 130, loss: 0.11777577549219131\n",
            "step: 140, loss: 0.13320188224315643\n",
            "step: 150, loss: 0.0382142998278141\n",
            "step: 160, loss: 0.19265957176685333\n",
            "step: 170, loss: 0.17808300256729126\n",
            "step: 180, loss: 0.06284131854772568\n",
            "step: 190, loss: 0.020699240267276764\n",
            "step: 200, loss: 0.06842151284217834\n",
            "step: 210, loss: 0.017668914049863815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5969498910675382, f1=0.611353711790393, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11723175644874573\n",
            "step: 10, loss: 0.060088541358709335\n",
            "step: 20, loss: 0.020140578970313072\n",
            "step: 30, loss: 0.001891078194603324\n",
            "step: 40, loss: 0.06721244752407074\n",
            "step: 50, loss: 0.029240857809782028\n",
            "step: 60, loss: 0.19988618791103363\n",
            "step: 70, loss: 0.03422854095697403\n",
            "step: 80, loss: 0.14502036571502686\n",
            "step: 90, loss: 0.14777928590774536\n",
            "step: 100, loss: 0.029405958950519562\n",
            "step: 110, loss: 0.07534154504537582\n",
            "step: 120, loss: 0.0666266530752182\n",
            "step: 130, loss: 0.08941447734832764\n",
            "step: 140, loss: 0.11189819872379303\n",
            "step: 150, loss: 0.11905359476804733\n",
            "step: 160, loss: 0.006318338215351105\n",
            "step: 170, loss: 0.03336084634065628\n",
            "step: 180, loss: 0.011646774597465992\n",
            "step: 190, loss: 0.2904628813266754\n",
            "step: 200, loss: 0.002397349802777171\n",
            "step: 210, loss: 0.04880882054567337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6138211382113822, f1=0.598360655737705, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031162869185209274\n",
            "step: 10, loss: 0.03560047596693039\n",
            "step: 20, loss: 0.05501590296626091\n",
            "step: 30, loss: 0.03509599342942238\n",
            "step: 40, loss: 0.04214245826005936\n",
            "step: 50, loss: 0.11953133344650269\n",
            "step: 60, loss: 0.09149319678544998\n",
            "step: 70, loss: 0.020939458161592484\n",
            "step: 80, loss: 0.09098031371831894\n",
            "step: 90, loss: 0.11441361159086227\n",
            "step: 100, loss: 0.01297060027718544\n",
            "step: 110, loss: 0.09997212886810303\n",
            "step: 120, loss: 0.11763186752796173\n",
            "step: 130, loss: 0.045648243278265\n",
            "step: 140, loss: 0.05159392207860947\n",
            "step: 150, loss: 0.027293652296066284\n",
            "step: 160, loss: 0.018603915348649025\n",
            "step: 170, loss: 0.009838255122303963\n",
            "step: 180, loss: 0.024453474208712578\n",
            "step: 190, loss: 0.18431676924228668\n",
            "step: 200, loss: 0.023902345448732376\n",
            "step: 210, loss: 0.0508066788315773\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5906313645621182, f1=0.5978494623655914, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013351015746593475\n",
            "step: 10, loss: 0.10731825977563858\n",
            "step: 20, loss: 0.0024179001338779926\n",
            "step: 30, loss: 0.04445340484380722\n",
            "step: 40, loss: 0.0030142266768962145\n",
            "step: 50, loss: 0.010832877829670906\n",
            "step: 60, loss: 0.06737146526575089\n",
            "step: 70, loss: 0.10593615472316742\n",
            "step: 80, loss: 0.026981748640537262\n",
            "step: 90, loss: 0.012885301373898983\n",
            "step: 100, loss: 0.0973348543047905\n",
            "step: 110, loss: 0.08062287420034409\n",
            "step: 120, loss: 0.04261203482747078\n",
            "step: 130, loss: 0.0021453818771988153\n",
            "step: 140, loss: 0.06673210859298706\n",
            "step: 150, loss: 0.0198410265147686\n",
            "step: 160, loss: 0.030985843390226364\n",
            "step: 170, loss: 0.05850386619567871\n",
            "step: 180, loss: 0.07343168556690216\n",
            "step: 190, loss: 0.011654342524707317\n",
            "step: 200, loss: 0.018703462556004524\n",
            "step: 210, loss: 0.22479870915412903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6031042128603104, f1=0.6132723112128146, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008775776252150536\n",
            "step: 10, loss: 0.05210297182202339\n",
            "step: 20, loss: 0.029559364542365074\n",
            "step: 30, loss: 0.0253071878105402\n",
            "step: 40, loss: 0.199273943901062\n",
            "step: 50, loss: 0.0484335720539093\n",
            "step: 60, loss: 0.08868417143821716\n",
            "step: 70, loss: 0.01088655274361372\n",
            "step: 80, loss: 0.016815800219774246\n",
            "step: 90, loss: 0.003656730055809021\n",
            "step: 100, loss: 0.009903131984174252\n",
            "step: 110, loss: 0.042206063866615295\n",
            "step: 120, loss: 0.02244407869875431\n",
            "step: 130, loss: 0.05750494822859764\n",
            "step: 140, loss: 0.044609054923057556\n",
            "step: 150, loss: 0.18058113753795624\n",
            "step: 160, loss: 0.015905214473605156\n",
            "step: 170, loss: 0.007599723991006613\n",
            "step: 180, loss: 0.013260343112051487\n",
            "step: 190, loss: 0.001918022520840168\n",
            "step: 200, loss: 0.0058121709153056145\n",
            "step: 210, loss: 0.008510168641805649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5891783567134268, f1=0.5849462365591398, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10783641040325165\n",
            "step: 10, loss: 0.004258967470377684\n",
            "step: 20, loss: 0.006707071326673031\n",
            "step: 30, loss: 0.1612420380115509\n",
            "step: 40, loss: 0.002313343109562993\n",
            "step: 50, loss: 0.021190805360674858\n",
            "step: 60, loss: 0.011554659344255924\n",
            "step: 70, loss: 0.047579552978277206\n",
            "step: 80, loss: 0.014581447467207909\n",
            "step: 90, loss: 0.04116307944059372\n",
            "step: 100, loss: 0.006547159980982542\n",
            "step: 110, loss: 0.0007810928509570658\n",
            "step: 120, loss: 0.05386599153280258\n",
            "step: 130, loss: 0.04074714332818985\n",
            "step: 140, loss: 0.003603016957640648\n",
            "step: 150, loss: 0.06635317951440811\n",
            "step: 160, loss: 0.012020908296108246\n",
            "step: 170, loss: 0.004334998317062855\n",
            "step: 180, loss: 0.14419473707675934\n",
            "step: 190, loss: 0.08214121311903\n",
            "step: 200, loss: 0.016923442482948303\n",
            "step: 210, loss: 0.05791740119457245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5811623246492985, f1=0.6049382716049383, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025067104026675224\n",
            "step: 10, loss: 0.03795302286744118\n",
            "step: 20, loss: 0.03156169131398201\n",
            "step: 30, loss: 0.0013994361506775022\n",
            "step: 40, loss: 0.006536488886922598\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 50, loss: 0.044127922505140305\n",
            "step: 60, loss: 0.02988109178841114\n",
            "step: 70, loss: 0.013831629417836666\n",
            "step: 80, loss: 0.03571006655693054\n",
            "step: 90, loss: 0.0058989860117435455\n",
            "step: 100, loss: 0.007244451902806759\n",
            "step: 110, loss: 0.02934345416724682\n",
            "step: 120, loss: 0.08876820653676987\n",
            "step: 130, loss: 0.009960015304386616\n",
            "step: 140, loss: 0.012057258747518063\n",
            "step: 150, loss: 0.0017973007634282112\n",
            "step: 160, loss: 0.0026901846285909414\n",
            "step: 170, loss: 0.01384836994111538\n",
            "step: 180, loss: 0.001708770520053804\n",
            "step: 190, loss: 0.01224595122039318\n",
            "step: 200, loss: 0.002563005778938532\n",
            "step: 210, loss: 0.009703869000077248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6020618556701032, f1=0.6167023554603854, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1537763476371765\n",
            "step: 10, loss: 0.003674056613817811\n",
            "step: 20, loss: 0.010187164880335331\n",
            "step: 30, loss: 0.08767236024141312\n",
            "step: 40, loss: 0.00929409172385931\n",
            "step: 50, loss: 0.002654454205185175\n",
            "step: 60, loss: 0.03259784355759621\n",
            "step: 70, loss: 0.017160698771476746\n",
            "step: 80, loss: 0.00873531773686409\n",
            "step: 90, loss: 0.07227100431919098\n",
            "step: 100, loss: 0.038332171738147736\n",
            "step: 110, loss: 0.010493296198546886\n",
            "step: 120, loss: 0.005485785659402609\n",
            "step: 130, loss: 0.013367773033678532\n",
            "step: 140, loss: 0.05221294239163399\n",
            "step: 150, loss: 0.008460917510092258\n",
            "step: 160, loss: 0.0538356676697731\n",
            "step: 170, loss: 0.011115808971226215\n",
            "step: 180, loss: 0.11544598639011383\n",
            "step: 190, loss: 0.027789540588855743\n",
            "step: 200, loss: 0.031141530722379684\n",
            "step: 210, loss: 0.003419145243242383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6101694915254237, f1=0.5869565217391304, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.044722992926836014\n",
            "step: 10, loss: 0.0011472568148747087\n",
            "step: 20, loss: 0.008443407714366913\n",
            "step: 30, loss: 0.007295448798686266\n",
            "step: 40, loss: 0.016079967841506004\n",
            "step: 50, loss: 0.003967003431171179\n",
            "step: 60, loss: 0.0033878714311867952\n",
            "step: 70, loss: 0.12707088887691498\n",
            "step: 80, loss: 0.07143152505159378\n",
            "step: 90, loss: 0.0008305480587296188\n",
            "step: 100, loss: 0.11170954257249832\n",
            "step: 110, loss: 0.0114598935469985\n",
            "step: 120, loss: 0.0016525564715266228\n",
            "step: 130, loss: 0.0012524212943390012\n",
            "step: 140, loss: 0.006034764461219311\n",
            "step: 150, loss: 0.0035303218755871058\n",
            "step: 160, loss: 0.0028589502908289433\n",
            "step: 170, loss: 0.012989218346774578\n",
            "step: 180, loss: 0.015447924844920635\n",
            "step: 190, loss: 0.0022095399908721447\n",
            "step: 200, loss: 0.2006683051586151\n",
            "step: 210, loss: 0.027573762461543083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5957446808510638, f1=0.5970149253731344, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026411470025777817\n",
            "step: 10, loss: 0.012885796837508678\n",
            "step: 20, loss: 0.0029952186159789562\n",
            "step: 30, loss: 0.016505341976881027\n",
            "step: 40, loss: 0.0018286730628460646\n",
            "step: 50, loss: 0.0021503979805856943\n",
            "step: 60, loss: 0.005636812187731266\n",
            "step: 70, loss: 0.0006380904233083129\n",
            "step: 80, loss: 0.00022889654792379588\n",
            "step: 90, loss: 0.007858232595026493\n",
            "step: 100, loss: 0.01762227527797222\n",
            "step: 110, loss: 0.001606841804459691\n",
            "step: 120, loss: 0.039334263652563095\n",
            "step: 130, loss: 0.13458746671676636\n",
            "step: 140, loss: 0.012825156562030315\n",
            "step: 150, loss: 0.05778297036886215\n",
            "step: 160, loss: 0.01871071755886078\n",
            "step: 170, loss: 0.003945476375520229\n",
            "step: 180, loss: 0.0008398511563427746\n",
            "step: 190, loss: 0.007742001209408045\n",
            "step: 200, loss: 0.0005414935294538736\n",
            "step: 210, loss: 0.03238924220204353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5936842105263158, f1=0.5739130434782609, best_f1=0.6419753086419754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019000471802428365\n",
            "step: 10, loss: 0.0003369585028849542\n",
            "step: 20, loss: 0.0021067883353680372\n",
            "step: 30, loss: 0.030295901000499725\n",
            "step: 40, loss: 0.0012032677186653018\n",
            "step: 50, loss: 0.0030139146838337183\n",
            "step: 60, loss: 0.021125953644514084\n",
            "step: 70, loss: 0.0006704982952214777\n",
            "step: 80, loss: 0.004083804786205292\n",
            "step: 90, loss: 0.039203137159347534\n",
            "step: 100, loss: 0.06697995215654373\n",
            "step: 110, loss: 0.0006078680162318051\n",
            "step: 120, loss: 0.008364833891391754\n",
            "step: 130, loss: 0.018331564962863922\n",
            "step: 140, loss: 0.0011440314119681716\n",
            "step: 150, loss: 0.0016042966162785888\n",
            "step: 160, loss: 0.0018258177442476153\n",
            "step: 170, loss: 0.004846662748605013\n",
            "step: 180, loss: 0.0020231844391673803\n",
            "step: 190, loss: 0.002902979264035821\n",
            "step: 200, loss: 0.010898243635892868\n",
            "step: 210, loss: 0.010478045791387558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5962732919254657, f1=0.5910064239828693, best_f1=0.6419753086419754\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 475.58it/s]\n",
            "load_f1 = 0.6481113320079521\n",
            "real_f1 = 0.6387225548902197\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 262.53it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0eWrGYhstu"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa152434-9fbe-43e6-8c73-ac6efe63eea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5748521685600281\n",
            "step: 10, loss: 0.37487688660621643\n",
            "step: 20, loss: 0.29720446467399597\n",
            "step: 30, loss: 0.39682579040527344\n",
            "step: 40, loss: 0.4304918944835663\n",
            "step: 50, loss: 0.2964189946651459\n",
            "step: 60, loss: 0.2650407552719116\n",
            "step: 70, loss: 0.29675060510635376\n",
            "step: 80, loss: 0.29969140887260437\n",
            "step: 90, loss: 0.2609870433807373\n",
            "step: 100, loss: 0.3520505726337433\n",
            "step: 110, loss: 0.5293878316879272\n",
            "step: 120, loss: 0.0945090651512146\n",
            "step: 130, loss: 0.11219549924135208\n",
            "step: 140, loss: 0.06763942539691925\n",
            "step: 150, loss: 0.18469977378845215\n",
            "step: 160, loss: 0.17562930285930634\n",
            "step: 170, loss: 0.32708799839019775\n",
            "step: 180, loss: 0.07588095963001251\n",
            "step: 190, loss: 0.2753635346889496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5529953917050692, f1=0.6082949308755761, best_f1=0.6082949308755761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2650958299636841\n",
            "step: 10, loss: 0.06543368846178055\n",
            "step: 20, loss: 0.1782960295677185\n",
            "step: 30, loss: 0.26542577147483826\n",
            "step: 40, loss: 0.19192978739738464\n",
            "step: 50, loss: 0.18320152163505554\n",
            "step: 60, loss: 0.2532225549221039\n",
            "step: 70, loss: 0.22741146385669708\n",
            "step: 80, loss: 0.22941556572914124\n",
            "step: 90, loss: 0.14478415250778198\n",
            "step: 100, loss: 0.028513429686427116\n",
            "step: 110, loss: 0.18424242734909058\n",
            "step: 120, loss: 0.1811765730381012\n",
            "step: 130, loss: 0.04680595546960831\n",
            "step: 140, loss: 0.11290017515420914\n",
            "step: 150, loss: 0.1747329980134964\n",
            "step: 160, loss: 0.15637272596359253\n",
            "step: 170, loss: 0.1667996495962143\n",
            "step: 180, loss: 0.20768673717975616\n",
            "step: 190, loss: 0.038575656712055206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7011494252873564, f1=0.7267605633802817, best_f1=0.7267605633802817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04273461923003197\n",
            "step: 10, loss: 0.15550465881824493\n",
            "step: 20, loss: 0.061559371650218964\n",
            "step: 30, loss: 0.024607524275779724\n",
            "step: 40, loss: 0.08297023177146912\n",
            "step: 50, loss: 0.1221911758184433\n",
            "step: 60, loss: 0.03127588331699371\n",
            "step: 70, loss: 0.07682093977928162\n",
            "step: 80, loss: 0.14478354156017303\n",
            "step: 90, loss: 0.05394483730196953\n",
            "step: 100, loss: 0.035542137920856476\n",
            "step: 110, loss: 0.008692593313753605\n",
            "step: 120, loss: 0.0045853061601519585\n",
            "step: 130, loss: 0.010006112046539783\n",
            "step: 140, loss: 0.1283813714981079\n",
            "step: 150, loss: 0.1666792333126068\n",
            "step: 160, loss: 0.0048165637999773026\n",
            "step: 170, loss: 0.1327958106994629\n",
            "step: 180, loss: 0.04525178670883179\n",
            "step: 190, loss: 0.10691463202238083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7319884726224785, f1=0.7486033519553073, best_f1=0.7486033519553073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17944954335689545\n",
            "step: 10, loss: 0.10731380432844162\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.1258072853088379\n",
            "step: 30, loss: 0.022564100101590157\n",
            "step: 40, loss: 0.04430245980620384\n",
            "step: 50, loss: 0.029771601781249046\n",
            "step: 60, loss: 0.139182448387146\n",
            "step: 70, loss: 0.24012234807014465\n",
            "step: 80, loss: 0.1182800680398941\n",
            "step: 90, loss: 0.02885865606367588\n",
            "step: 100, loss: 0.12413659691810608\n",
            "step: 110, loss: 0.002056713216006756\n",
            "step: 120, loss: 0.04267401993274689\n",
            "step: 130, loss: 0.10847281664609909\n",
            "step: 140, loss: 0.03269433602690697\n",
            "step: 150, loss: 0.016390779986977577\n",
            "step: 160, loss: 0.05688449740409851\n",
            "step: 170, loss: 0.05431603267788887\n",
            "step: 180, loss: 0.033221591264009476\n",
            "step: 190, loss: 0.30376970767974854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7017543859649122, f1=0.7061855670103092, best_f1=0.7486033519553073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05134729668498039\n",
            "step: 10, loss: 0.012559439986944199\n",
            "step: 20, loss: 0.061367250978946686\n",
            "step: 30, loss: 0.009353847242891788\n",
            "step: 40, loss: 0.11320505291223526\n",
            "step: 50, loss: 0.0425712913274765\n",
            "step: 60, loss: 0.13800273835659027\n",
            "step: 70, loss: 0.017940182238817215\n",
            "step: 80, loss: 0.006194050423800945\n",
            "step: 90, loss: 0.056843455880880356\n",
            "step: 100, loss: 0.007915337570011616\n",
            "step: 110, loss: 0.021527450531721115\n",
            "step: 120, loss: 0.011035775765776634\n",
            "step: 130, loss: 0.09137826412916183\n",
            "step: 140, loss: 0.19443918764591217\n",
            "step: 150, loss: 0.02141113206744194\n",
            "step: 160, loss: 0.06922278553247452\n",
            "step: 170, loss: 0.004921182990074158\n",
            "step: 180, loss: 0.06545748561620712\n",
            "step: 190, loss: 0.07788655906915665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7486033519553073, f1=0.7388888888888889, best_f1=0.7388888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010824237950146198\n",
            "step: 10, loss: 0.052975356578826904\n",
            "step: 20, loss: 0.0038508614525198936\n",
            "step: 30, loss: 0.0003843856684397906\n",
            "step: 40, loss: 0.05006714537739754\n",
            "step: 50, loss: 0.004677070304751396\n",
            "step: 60, loss: 0.015656691044569016\n",
            "step: 70, loss: 0.12366532534360886\n",
            "step: 80, loss: 0.025590483099222183\n",
            "step: 90, loss: 0.013041634112596512\n",
            "step: 100, loss: 0.0006083553307689726\n",
            "step: 110, loss: 0.08691342175006866\n",
            "step: 120, loss: 0.004115038551390171\n",
            "step: 130, loss: 0.009985098615288734\n",
            "step: 140, loss: 0.006054480094462633\n",
            "step: 150, loss: 0.002131957793608308\n",
            "step: 160, loss: 0.10262727737426758\n",
            "step: 170, loss: 0.017945587635040283\n",
            "step: 180, loss: 0.11650791019201279\n",
            "step: 190, loss: 0.022144997492432594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7354497354497355, f1=0.7403598971722366, best_f1=0.7388888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013303027488291264\n",
            "step: 10, loss: 0.01837409846484661\n",
            "step: 20, loss: 0.04791121929883957\n",
            "step: 30, loss: 0.0023113912902772427\n",
            "step: 40, loss: 0.008769119158387184\n",
            "step: 50, loss: 0.01824156753718853\n",
            "step: 60, loss: 0.01983848586678505\n",
            "step: 70, loss: 0.03165889531373978\n",
            "step: 80, loss: 0.1396162509918213\n",
            "step: 90, loss: 0.15233048796653748\n",
            "step: 100, loss: 0.00040083753992803395\n",
            "step: 110, loss: 0.006301356013864279\n",
            "step: 120, loss: 0.002064890693873167\n",
            "step: 130, loss: 0.0029742103070020676\n",
            "step: 140, loss: 0.01685856468975544\n",
            "step: 150, loss: 0.00926282349973917\n",
            "step: 160, loss: 0.02150191180408001\n",
            "step: 170, loss: 0.025579784065485\n",
            "step: 180, loss: 0.012115690857172012\n",
            "step: 190, loss: 0.002398671815171838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7413333333333334, f1=0.7539267015706806, best_f1=0.7388888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014859320595860481\n",
            "step: 10, loss: 0.002940189093351364\n",
            "step: 20, loss: 0.004794599022716284\n",
            "step: 30, loss: 0.0007640800904482603\n",
            "step: 40, loss: 0.13833820819854736\n",
            "step: 50, loss: 0.16026312112808228\n",
            "step: 60, loss: 0.13635897636413574\n",
            "step: 70, loss: 0.008713425137102604\n",
            "step: 80, loss: 0.0010229782201349735\n",
            "step: 90, loss: 0.02230852283537388\n",
            "step: 100, loss: 0.01409986987709999\n",
            "step: 110, loss: 0.0010260010603815317\n",
            "step: 120, loss: 0.00911357905715704\n",
            "step: 130, loss: 0.03540800139307976\n",
            "step: 140, loss: 0.010249323211610317\n",
            "step: 150, loss: 0.044095177203416824\n",
            "step: 160, loss: 0.024926621466875076\n",
            "step: 170, loss: 0.0010819071903824806\n",
            "step: 180, loss: 0.006460175849497318\n",
            "step: 190, loss: 0.001601187395863235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7658402203856748, f1=0.7816711590296496, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019297258695587516\n",
            "step: 10, loss: 0.008908962830901146\n",
            "step: 20, loss: 0.0021454854868352413\n",
            "step: 30, loss: 0.0022856947034597397\n",
            "step: 40, loss: 0.0009645260870456696\n",
            "step: 50, loss: 0.05505027621984482\n",
            "step: 60, loss: 0.0005515672964975238\n",
            "step: 70, loss: 0.0018890718929469585\n",
            "step: 80, loss: 0.005094135645776987\n",
            "step: 90, loss: 0.06594917178153992\n",
            "step: 100, loss: 0.12385797500610352\n",
            "step: 110, loss: 0.0010625669965520501\n",
            "step: 120, loss: 0.012544546276330948\n",
            "step: 130, loss: 0.01680082082748413\n",
            "step: 140, loss: 0.01356601808220148\n",
            "step: 150, loss: 0.014055243693292141\n",
            "step: 160, loss: 0.0004222674760967493\n",
            "step: 170, loss: 0.18307533860206604\n",
            "step: 180, loss: 0.006973284762352705\n",
            "step: 190, loss: 0.0031891940161585808\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7311178247734139, f1=0.7305389221556886, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003987593576312065\n",
            "step: 10, loss: 0.002297238912433386\n",
            "step: 20, loss: 0.023535186424851418\n",
            "step: 30, loss: 0.0006553118582814932\n",
            "step: 40, loss: 0.03983166068792343\n",
            "step: 50, loss: 0.001623573829419911\n",
            "step: 60, loss: 0.005514620803296566\n",
            "step: 70, loss: 0.005327997263520956\n",
            "step: 80, loss: 0.014910672791302204\n",
            "step: 90, loss: 0.07253073900938034\n",
            "step: 100, loss: 0.001466405694372952\n",
            "step: 110, loss: 0.008294044993817806\n",
            "step: 120, loss: 0.008930621668696404\n",
            "step: 130, loss: 0.026863252744078636\n",
            "step: 140, loss: 0.0027883811853826046\n",
            "step: 150, loss: 0.004279938526451588\n",
            "step: 160, loss: 0.011900079436600208\n",
            "step: 170, loss: 0.0032776903826743364\n",
            "step: 180, loss: 0.0010823209304362535\n",
            "step: 190, loss: 0.0013854765566065907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7485714285714286, f1=0.7821229050279329, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024426118470728397\n",
            "step: 10, loss: 0.00129895459394902\n",
            "step: 20, loss: 0.020797181874513626\n",
            "step: 30, loss: 0.10322681814432144\n",
            "step: 40, loss: 0.0022510376293212175\n",
            "step: 50, loss: 0.020744606852531433\n",
            "step: 60, loss: 0.0019037129823118448\n",
            "step: 70, loss: 0.0016988675342872739\n",
            "step: 80, loss: 0.0012561422772705555\n",
            "step: 90, loss: 0.00040701765101403\n",
            "step: 100, loss: 0.005884020589292049\n",
            "step: 110, loss: 0.0012690440053120255\n",
            "step: 120, loss: 0.007446755189448595\n",
            "step: 130, loss: 0.0005513414507731795\n",
            "step: 140, loss: 0.0029238618444651365\n",
            "step: 150, loss: 0.0005674657877534628\n",
            "step: 160, loss: 0.005097723565995693\n",
            "step: 170, loss: 0.0003128368407487869\n",
            "step: 180, loss: 0.0006609638803638518\n",
            "step: 190, loss: 0.0015282955719158053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7567567567567567, f1=0.7780821917808218, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008041078108362854\n",
            "step: 10, loss: 0.0017060736427083611\n",
            "step: 20, loss: 0.00023457707720808685\n",
            "step: 30, loss: 0.0008945315494202077\n",
            "step: 40, loss: 0.003524672705680132\n",
            "step: 50, loss: 0.003684511873871088\n",
            "step: 60, loss: 0.0005048876046203077\n",
            "step: 70, loss: 0.029697611927986145\n",
            "step: 80, loss: 0.004142424091696739\n",
            "step: 90, loss: 0.0003086104115936905\n",
            "step: 100, loss: 0.0031841371674090624\n",
            "step: 110, loss: 0.00038345070788636804\n",
            "step: 120, loss: 0.0012877410044893622\n",
            "step: 130, loss: 0.01974412240087986\n",
            "step: 140, loss: 0.07120788097381592\n",
            "step: 150, loss: 0.0016034294385463\n",
            "step: 160, loss: 0.001338059431873262\n",
            "step: 170, loss: 0.0003412103687878698\n",
            "step: 180, loss: 0.0006408001645468175\n",
            "step: 190, loss: 0.002943342784419656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7500000000000001, f1=0.7833333333333333, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003033993998542428\n",
            "step: 10, loss: 0.028149448335170746\n",
            "step: 20, loss: 0.0028671470936387777\n",
            "step: 30, loss: 0.006977815646678209\n",
            "step: 40, loss: 0.0005186983034946024\n",
            "step: 50, loss: 0.150760680437088\n",
            "step: 60, loss: 0.0010296839755028486\n",
            "step: 70, loss: 0.003256390104070306\n",
            "step: 80, loss: 0.001826481893658638\n",
            "step: 90, loss: 0.006401010788977146\n",
            "step: 100, loss: 0.0019301411230117083\n",
            "step: 110, loss: 0.005405470263212919\n",
            "step: 120, loss: 0.0008657909347675741\n",
            "step: 130, loss: 0.04688909649848938\n",
            "step: 140, loss: 0.0007391570834442973\n",
            "step: 150, loss: 0.00026477279607206583\n",
            "step: 160, loss: 0.000545013346709311\n",
            "step: 170, loss: 0.12508489191532135\n",
            "step: 180, loss: 0.00269303354434669\n",
            "step: 190, loss: 0.02329181134700775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7645429362880887, f1=0.7759562841530054, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001122458023019135\n",
            "step: 10, loss: 0.0324874073266983\n",
            "step: 20, loss: 0.0004516587941907346\n",
            "step: 30, loss: 0.0004815158899873495\n",
            "step: 40, loss: 0.008893872611224651\n",
            "step: 50, loss: 0.002399210352450609\n",
            "step: 60, loss: 0.004001338500529528\n",
            "step: 70, loss: 0.002572155324742198\n",
            "step: 80, loss: 0.01528194546699524\n",
            "step: 90, loss: 0.008191561326384544\n",
            "step: 100, loss: 0.034939032047986984\n",
            "step: 110, loss: 0.0019064489752054214\n",
            "step: 120, loss: 0.002323335036635399\n",
            "step: 130, loss: 0.0006177502800710499\n",
            "step: 140, loss: 0.016154196113348007\n",
            "step: 150, loss: 0.008881245739758015\n",
            "step: 160, loss: 0.002019954612478614\n",
            "step: 170, loss: 0.004089548718184233\n",
            "step: 180, loss: 0.0025966253597289324\n",
            "step: 190, loss: 0.0009640680509619415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7603305785123966, f1=0.779291553133515, best_f1=0.7816711590296496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02592608518898487\n",
            "step: 10, loss: 0.004354620818048716\n",
            "step: 20, loss: 0.02433491125702858\n",
            "step: 30, loss: 0.0007835496217012405\n",
            "step: 40, loss: 0.0005695315194316208\n",
            "step: 50, loss: 0.0020924247801303864\n",
            "step: 60, loss: 0.007451374549418688\n",
            "step: 70, loss: 0.02930002473294735\n",
            "step: 80, loss: 0.000982025288976729\n",
            "step: 90, loss: 0.0005219705053605139\n",
            "step: 100, loss: 0.0009468477801419795\n",
            "step: 110, loss: 0.0003993269056081772\n",
            "step: 120, loss: 0.0810328796505928\n",
            "step: 130, loss: 0.0955149382352829\n",
            "step: 140, loss: 0.008973835967481136\n",
            "step: 150, loss: 0.0004584561102092266\n",
            "step: 160, loss: 0.0002905589062720537\n",
            "step: 170, loss: 0.000595087418332696\n",
            "step: 180, loss: 0.0009055384434759617\n",
            "step: 190, loss: 0.001030450570397079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7601078167115903, f1=0.7780821917808218, best_f1=0.7816711590296496\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:09, 223.91it/s]\n",
            "load_f1 = 0.7085714285714286\n",
            "real_f1 = 0.6631016042780749\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 258.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW6LV4zMhstv"
      },
      "source": [
        "## DITTO TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3an30TrShstv"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41860ebb-2e76-43cf-8c6f-99ef9d752023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6358979940414429\n",
            "step: 10, loss: 0.3804142475128174\n",
            "step: 20, loss: 0.3046649694442749\n",
            "step: 30, loss: 0.3934731185436249\n",
            "step: 40, loss: 0.2748245298862457\n",
            "step: 50, loss: 0.2683543264865875\n",
            "step: 60, loss: 0.25545915961265564\n",
            "step: 70, loss: 0.36577385663986206\n",
            "step: 80, loss: 0.38896656036376953\n",
            "step: 90, loss: 0.22536516189575195\n",
            "step: 100, loss: 0.22806717455387115\n",
            "step: 110, loss: 0.21211041510105133\n",
            "step: 120, loss: 0.2621464133262634\n",
            "step: 130, loss: 0.0862126499414444\n",
            "step: 140, loss: 0.27472132444381714\n",
            "step: 150, loss: 0.38891729712486267\n",
            "step: 160, loss: 0.24914002418518066\n",
            "step: 170, loss: 0.23842519521713257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.6616161616161617, f1=0.6733167082294264, best_f1=0.6733167082294264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1138736754655838\n",
            "step: 10, loss: 0.05190000683069229\n",
            "step: 20, loss: 0.12480221688747406\n",
            "step: 30, loss: 0.24142640829086304\n",
            "step: 40, loss: 0.1273566335439682\n",
            "step: 50, loss: 0.21319644153118134\n",
            "step: 60, loss: 0.10823097079992294\n",
            "step: 70, loss: 0.13696718215942383\n",
            "step: 80, loss: 0.056948013603687286\n",
            "step: 90, loss: 0.14302237331867218\n",
            "step: 100, loss: 0.1824701577425003\n",
            "step: 110, loss: 0.06504753977060318\n",
            "step: 120, loss: 0.12803934514522552\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.06715527921915054\n",
            "step: 140, loss: 0.2671118676662445\n",
            "step: 150, loss: 0.153494194149971\n",
            "step: 160, loss: 0.09890249371528625\n",
            "step: 170, loss: 0.11110410839319229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7386091127098322, f1=0.7450980392156863, best_f1=0.7450980392156863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1413058191537857\n",
            "step: 10, loss: 0.07497366517782211\n",
            "step: 20, loss: 0.0832546278834343\n",
            "step: 30, loss: 0.18032243847846985\n",
            "step: 40, loss: 0.11785591393709183\n",
            "step: 50, loss: 0.2141377031803131\n",
            "step: 60, loss: 0.04996515065431595\n",
            "step: 70, loss: 0.08621150255203247\n",
            "step: 80, loss: 0.04582102224230766\n",
            "step: 90, loss: 0.2334405779838562\n",
            "step: 100, loss: 0.0704159140586853\n",
            "step: 110, loss: 0.15862907469272614\n",
            "step: 120, loss: 0.10946521908044815\n",
            "step: 130, loss: 0.11321781575679779\n",
            "step: 140, loss: 0.043863337486982346\n",
            "step: 150, loss: 0.0355304554104805\n",
            "step: 160, loss: 0.10151653736829758\n",
            "step: 170, loss: 0.08258926868438721\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7653061224489796, f1=0.8058968058968059, best_f1=0.8058968058968059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00894572027027607\n",
            "step: 10, loss: 0.037053536623716354\n",
            "step: 20, loss: 0.009346941486001015\n",
            "step: 30, loss: 0.16489222645759583\n",
            "step: 40, loss: 0.009200595319271088\n",
            "step: 50, loss: 0.07923539727926254\n",
            "step: 60, loss: 0.08126739412546158\n",
            "step: 70, loss: 0.07119815796613693\n",
            "step: 80, loss: 0.16597643494606018\n",
            "step: 90, loss: 0.12189029157161713\n",
            "step: 100, loss: 0.029812583699822426\n",
            "step: 110, loss: 0.18060152232646942\n",
            "step: 120, loss: 0.04295208305120468\n",
            "step: 130, loss: 0.11322544515132904\n",
            "step: 140, loss: 0.028526263311505318\n",
            "step: 150, loss: 0.051589235663414\n",
            "step: 160, loss: 0.47742024064064026\n",
            "step: 170, loss: 0.10311673581600189\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7540229885057472, f1=0.7755102040816327, best_f1=0.8058968058968059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.060776639729738235\n",
            "step: 10, loss: 0.028651248663663864\n",
            "step: 20, loss: 0.010208523832261562\n",
            "step: 30, loss: 0.014354722574353218\n",
            "step: 40, loss: 0.004371557384729385\n",
            "step: 50, loss: 0.018704621121287346\n",
            "step: 60, loss: 0.015147336758673191\n",
            "step: 70, loss: 0.18824176490306854\n",
            "step: 80, loss: 0.0991934984922409\n",
            "step: 90, loss: 0.05712075158953667\n",
            "step: 100, loss: 0.015488861128687859\n",
            "step: 110, loss: 0.16378842294216156\n",
            "step: 120, loss: 0.024243544787168503\n",
            "step: 130, loss: 0.1159512996673584\n",
            "step: 140, loss: 0.012942780740559101\n",
            "step: 150, loss: 0.04101656749844551\n",
            "step: 160, loss: 0.06613294035196304\n",
            "step: 170, loss: 0.020891284570097923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.772093023255814, f1=0.7671232876712328, best_f1=0.7671232876712328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005009638611227274\n",
            "step: 10, loss: 0.03316108509898186\n",
            "step: 20, loss: 0.027430636808276176\n",
            "step: 30, loss: 0.0043296427465975285\n",
            "step: 40, loss: 0.0368974544107914\n",
            "step: 50, loss: 0.21095193922519684\n",
            "step: 60, loss: 0.10023988783359528\n",
            "step: 70, loss: 0.11749496310949326\n",
            "step: 80, loss: 0.033209867775440216\n",
            "step: 90, loss: 0.032472364604473114\n",
            "step: 100, loss: 0.08856700360774994\n",
            "step: 110, loss: 0.0020373216830193996\n",
            "step: 120, loss: 0.006394118070602417\n",
            "step: 130, loss: 0.029656048864126205\n",
            "step: 140, loss: 0.019671987742185593\n",
            "step: 150, loss: 0.09279319643974304\n",
            "step: 160, loss: 0.065308578312397\n",
            "step: 170, loss: 0.015127994120121002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7990314769975787, f1=0.7645687645687645, best_f1=0.7645687645687645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007521644234657288\n",
            "step: 10, loss: 0.015870239585638046\n",
            "step: 20, loss: 0.00482525397092104\n",
            "step: 30, loss: 0.03275414928793907\n",
            "step: 40, loss: 0.014137022197246552\n",
            "step: 50, loss: 0.005934370215982199\n",
            "step: 60, loss: 0.007338986732065678\n",
            "step: 70, loss: 0.006196944508701563\n",
            "step: 80, loss: 0.048490118235349655\n",
            "step: 90, loss: 0.0010006354423239827\n",
            "step: 100, loss: 0.005630621686577797\n",
            "step: 110, loss: 0.09419819712638855\n",
            "step: 120, loss: 0.009122439660131931\n",
            "step: 130, loss: 0.16234301030635834\n",
            "step: 140, loss: 0.005155432969331741\n",
            "step: 150, loss: 0.013772835023701191\n",
            "step: 160, loss: 0.004767074249684811\n",
            "step: 170, loss: 0.12491367757320404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7788018433179723, f1=0.7927107061503416, best_f1=0.7645687645687645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018437795341014862\n",
            "step: 10, loss: 0.05411003902554512\n",
            "step: 20, loss: 0.004188641905784607\n",
            "step: 30, loss: 0.014411705546081066\n",
            "step: 40, loss: 0.0008509629406034946\n",
            "step: 50, loss: 0.0037213757168501616\n",
            "step: 60, loss: 0.007622744422405958\n",
            "step: 70, loss: 0.015246279537677765\n",
            "step: 80, loss: 0.005867003463208675\n",
            "step: 90, loss: 0.024165816605091095\n",
            "step: 100, loss: 0.06594154983758926\n",
            "step: 110, loss: 0.12211691588163376\n",
            "step: 120, loss: 0.0029590297490358353\n",
            "step: 130, loss: 0.004208317492157221\n",
            "step: 140, loss: 0.09588763862848282\n",
            "step: 150, loss: 0.09287817776203156\n",
            "step: 160, loss: 0.010723190382122993\n",
            "step: 170, loss: 0.10925018042325974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7672634271099745, f1=0.7719298245614035, best_f1=0.7645687645687645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01769275590777397\n",
            "step: 10, loss: 0.010647936724126339\n",
            "step: 20, loss: 0.007675935514271259\n",
            "step: 30, loss: 0.005844771862030029\n",
            "step: 40, loss: 0.0020356597378849983\n",
            "step: 50, loss: 0.002025072695687413\n",
            "step: 60, loss: 0.011254953220486641\n",
            "step: 70, loss: 0.0032858492340892553\n",
            "step: 80, loss: 0.0036437788512557745\n",
            "step: 90, loss: 0.02072330005466938\n",
            "step: 100, loss: 0.0401700995862484\n",
            "step: 110, loss: 0.05178840085864067\n",
            "step: 120, loss: 0.012771238572895527\n",
            "step: 130, loss: 0.11573470383882523\n",
            "step: 140, loss: 0.07880579680204391\n",
            "step: 150, loss: 0.008891687728464603\n",
            "step: 160, loss: 0.03027944453060627\n",
            "step: 170, loss: 0.0016678563551977277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8009478672985781, f1=0.8027522935779815, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14516623318195343\n",
            "step: 10, loss: 0.026846829801797867\n",
            "step: 20, loss: 0.09176377207040787\n",
            "step: 30, loss: 0.003458862891420722\n",
            "step: 40, loss: 0.15373867750167847\n",
            "step: 50, loss: 0.023653250187635422\n",
            "step: 60, loss: 0.010723154991865158\n",
            "step: 70, loss: 0.012809611856937408\n",
            "step: 80, loss: 0.008451638743281364\n",
            "step: 90, loss: 0.008611228317022324\n",
            "step: 100, loss: 0.0005462002009153366\n",
            "step: 110, loss: 0.043931953608989716\n",
            "step: 120, loss: 0.008874768391251564\n",
            "step: 130, loss: 0.03588220477104187\n",
            "step: 140, loss: 0.06699823588132858\n",
            "step: 150, loss: 0.019383950158953667\n",
            "step: 160, loss: 0.041015882045030594\n",
            "step: 170, loss: 0.02540004998445511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.779746835443038, f1=0.7919799498746868, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020194649696350098\n",
            "step: 10, loss: 0.004262535832822323\n",
            "step: 20, loss: 0.031056709587574005\n",
            "step: 30, loss: 0.0006208340055309236\n",
            "step: 40, loss: 0.00465970765799284\n",
            "step: 50, loss: 0.0004935328033752739\n",
            "step: 60, loss: 0.06415057927370071\n",
            "step: 70, loss: 0.02619049698114395\n",
            "step: 80, loss: 0.006619841791689396\n",
            "step: 90, loss: 0.002734237816184759\n",
            "step: 100, loss: 0.0017957575619220734\n",
            "step: 110, loss: 0.01120760478079319\n",
            "step: 120, loss: 0.0036362174432724714\n",
            "step: 130, loss: 0.0008764484664425254\n",
            "step: 140, loss: 0.022375546395778656\n",
            "step: 150, loss: 0.0028264415450394154\n",
            "step: 160, loss: 0.008299865759909153\n",
            "step: 170, loss: 0.017678387463092804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7970297029702972, f1=0.802919708029197, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0076223877258598804\n",
            "step: 10, loss: 0.0020676234271377325\n",
            "step: 20, loss: 0.013569879345595837\n",
            "step: 30, loss: 0.0007191505865193903\n",
            "step: 40, loss: 0.002287202747538686\n",
            "step: 50, loss: 0.0073695057071745396\n",
            "step: 60, loss: 0.14808598160743713\n",
            "step: 70, loss: 0.08310551196336746\n",
            "step: 80, loss: 0.0005109041230753064\n",
            "step: 90, loss: 0.001414524856954813\n",
            "step: 100, loss: 0.001856464659795165\n",
            "step: 110, loss: 0.0009696583729237318\n",
            "step: 120, loss: 0.0035190300550311804\n",
            "step: 130, loss: 0.008285297080874443\n",
            "step: 140, loss: 0.005106888245791197\n",
            "step: 150, loss: 0.0037658722139894962\n",
            "step: 160, loss: 0.000668843335006386\n",
            "step: 170, loss: 0.0003987459640484303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7696078431372548, f1=0.7893462469733656, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002301010536029935\n",
            "step: 10, loss: 0.0005117082037031651\n",
            "step: 20, loss: 0.0012367900926619768\n",
            "step: 30, loss: 0.0005412223981693387\n",
            "step: 40, loss: 0.0023894477635622025\n",
            "step: 50, loss: 0.0005798320635221899\n",
            "step: 60, loss: 0.012662220746278763\n",
            "step: 70, loss: 0.0032635447569191456\n",
            "step: 80, loss: 0.10046298056840897\n",
            "step: 90, loss: 0.0003247610293328762\n",
            "step: 100, loss: 0.008527371101081371\n",
            "step: 110, loss: 0.004215881694108248\n",
            "step: 120, loss: 0.014339831657707691\n",
            "step: 130, loss: 0.001981359673663974\n",
            "step: 140, loss: 0.0013724323362112045\n",
            "step: 150, loss: 0.0022587073035538197\n",
            "step: 160, loss: 0.0019471074920147657\n",
            "step: 170, loss: 0.00824027694761753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7918781725888325, f1=0.7921760391198044, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012880865251645446\n",
            "step: 10, loss: 0.00046141844359226525\n",
            "step: 20, loss: 0.0013297925470396876\n",
            "step: 30, loss: 0.0129456901922822\n",
            "step: 40, loss: 0.0002870754397008568\n",
            "step: 50, loss: 0.026870960369706154\n",
            "step: 60, loss: 0.0007449613767676055\n",
            "step: 70, loss: 0.0017353990115225315\n",
            "step: 80, loss: 0.001153200282715261\n",
            "step: 90, loss: 0.0008888922166079283\n",
            "step: 100, loss: 0.016297198832035065\n",
            "step: 110, loss: 0.005115943029522896\n",
            "step: 120, loss: 0.05458776280283928\n",
            "step: 130, loss: 0.011486588977277279\n",
            "step: 140, loss: 0.0011826370609924197\n",
            "step: 150, loss: 0.0009066889761015773\n",
            "step: 160, loss: 0.002867515431717038\n",
            "step: 170, loss: 0.002356958342716098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7888040712468193, f1=0.7990196078431373, best_f1=0.8027522935779815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004571459721773863\n",
            "step: 10, loss: 0.002555671613663435\n",
            "step: 20, loss: 0.0038489431608468294\n",
            "step: 30, loss: 0.0014192159287631512\n",
            "step: 40, loss: 0.0115605928003788\n",
            "step: 50, loss: 0.0007752456585876644\n",
            "step: 60, loss: 0.03495519608259201\n",
            "step: 70, loss: 0.001271246699616313\n",
            "step: 80, loss: 0.007296781986951828\n",
            "step: 90, loss: 0.0026544148568063974\n",
            "step: 100, loss: 0.0016490871785208583\n",
            "step: 110, loss: 0.00045118623529560864\n",
            "step: 120, loss: 0.0004791267274413258\n",
            "step: 130, loss: 0.0014314398868009448\n",
            "step: 140, loss: 0.0007437804597429931\n",
            "step: 150, loss: 0.0007842146442271769\n",
            "step: 160, loss: 0.001799589255824685\n",
            "step: 170, loss: 0.001576554961502552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7919799498746868, f1=0.7931873479318734, best_f1=0.8027522935779815\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 284.85it/s]\n",
            "load_f1 = 0.5519591141396933\n",
            "real_f1 = 0.5584642233856895\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 260.46it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VngEb4vfhstw"
      },
      "source": [
        "## DITTO DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfPaCqR4hstw"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9139eb-3469-4c8f-cfb1-b741bfe6c78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6268185973167419\n",
            "step: 10, loss: 0.6247966885566711\n",
            "step: 20, loss: 0.431181401014328\n",
            "step: 30, loss: 0.17409704625606537\n",
            "step: 40, loss: 0.2653740644454956\n",
            "step: 50, loss: 0.07025144249200821\n",
            "step: 60, loss: 0.048899997025728226\n",
            "step: 70, loss: 0.15257588028907776\n",
            "step: 80, loss: 0.044556424021720886\n",
            "step: 90, loss: 0.1734917014837265\n",
            "step: 100, loss: 0.0376373790204525\n",
            "step: 110, loss: 0.32378003001213074\n",
            "step: 120, loss: 0.013894649222493172\n",
            "step: 130, loss: 0.11060351133346558\n",
            "step: 140, loss: 0.010813730768859386\n",
            "step: 150, loss: 0.01960681937634945\n",
            "step: 160, loss: 0.015155651606619358\n",
            "step: 170, loss: 0.10890638083219528\n",
            "step: 180, loss: 0.021936491131782532\n",
            "step: 190, loss: 0.02862461656332016\n",
            "step: 200, loss: 0.11853092163801193\n",
            "step: 210, loss: 0.013124142773449421\n",
            "step: 220, loss: 0.006009258795529604\n",
            "step: 230, loss: 0.08776308596134186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9599999999999999, f1=0.9662162162162162, best_f1=0.9662162162162162\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03042772226035595\n",
            "step: 10, loss: 0.043632835149765015\n",
            "step: 20, loss: 0.1733575165271759\n",
            "step: 30, loss: 0.05590850114822388\n",
            "step: 40, loss: 0.11061368137598038\n",
            "step: 50, loss: 0.0036073678638786077\n",
            "step: 60, loss: 0.018969513475894928\n",
            "step: 70, loss: 0.10132656991481781\n",
            "step: 80, loss: 0.032144781202077866\n",
            "step: 90, loss: 0.008044418878853321\n",
            "step: 100, loss: 0.06773111969232559\n",
            "step: 110, loss: 0.1768152266740799\n",
            "step: 120, loss: 0.11156698316335678\n",
            "step: 130, loss: 0.008513095788657665\n",
            "step: 140, loss: 0.0025494606234133244\n",
            "step: 150, loss: 0.033188361674547195\n",
            "step: 160, loss: 0.0439625084400177\n",
            "step: 170, loss: 0.0036340218503028154\n",
            "step: 180, loss: 0.02226044423878193\n",
            "step: 190, loss: 0.022345520555973053\n",
            "step: 200, loss: 0.007894041016697884\n",
            "step: 210, loss: 0.0035759785678237677\n",
            "step: 220, loss: 0.02716207504272461\n",
            "step: 230, loss: 0.057946767657995224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9644444444444443, f1=0.9644444444444443, best_f1=0.9644444444444443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004424369428306818\n",
            "step: 10, loss: 0.02306448295712471\n",
            "step: 20, loss: 0.007007214706391096\n",
            "step: 30, loss: 0.13712958991527557\n",
            "step: 40, loss: 0.10491623729467392\n",
            "step: 50, loss: 0.005799897015094757\n",
            "step: 60, loss: 0.01531471498310566\n",
            "step: 70, loss: 0.019669540226459503\n",
            "step: 80, loss: 0.0014948571333661675\n",
            "step: 90, loss: 0.060753632336854935\n",
            "step: 100, loss: 0.003434245241805911\n",
            "step: 110, loss: 0.001794077455997467\n",
            "step: 120, loss: 0.004463411867618561\n",
            "step: 130, loss: 0.0006711930618621409\n",
            "step: 140, loss: 0.009841413237154484\n",
            "step: 150, loss: 0.05194548889994621\n",
            "step: 160, loss: 0.053370438516139984\n",
            "step: 170, loss: 0.03439662605524063\n",
            "step: 180, loss: 0.009777670726180077\n",
            "step: 190, loss: 0.0029427772387862206\n",
            "step: 200, loss: 0.038561102002859116\n",
            "step: 210, loss: 0.03695598989725113\n",
            "step: 220, loss: 0.001429226715117693\n",
            "step: 230, loss: 0.0012540951138362288\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9590254706533776, f1=0.970917225950783, best_f1=0.9644444444444443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006275562336668372\n",
            "step: 10, loss: 0.0008441006066277623\n",
            "step: 20, loss: 0.0011101120617240667\n",
            "step: 30, loss: 0.02866654470562935\n",
            "step: 40, loss: 0.021679997444152832\n",
            "step: 50, loss: 0.004882287234067917\n",
            "step: 60, loss: 0.0059221540577709675\n",
            "step: 70, loss: 0.0010223389836028218\n",
            "step: 80, loss: 0.0014717677840963006\n",
            "step: 90, loss: 0.0021472382359206676\n",
            "step: 100, loss: 0.003457169746980071\n",
            "step: 110, loss: 0.0012705051340162754\n",
            "step: 120, loss: 0.04275009036064148\n",
            "step: 130, loss: 0.004927722737193108\n",
            "step: 140, loss: 0.0036811537574976683\n",
            "step: 150, loss: 0.1076948493719101\n",
            "step: 160, loss: 0.03826698660850525\n",
            "step: 170, loss: 0.08008722960948944\n",
            "step: 180, loss: 0.0009755729115568101\n",
            "step: 190, loss: 0.005606337450444698\n",
            "step: 200, loss: 0.014464090578258038\n",
            "step: 210, loss: 0.12014798074960709\n",
            "step: 220, loss: 0.0013815077254548669\n",
            "step: 230, loss: 0.20374265313148499\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9697648376259798, f1=0.9697648376259798, best_f1=0.9697648376259798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022277161478996277\n",
            "step: 10, loss: 0.0012730089947581291\n",
            "step: 20, loss: 0.01660929061472416\n",
            "step: 30, loss: 0.000519410939887166\n",
            "step: 40, loss: 0.0009171775891445577\n",
            "step: 50, loss: 0.016108283773064613\n",
            "step: 60, loss: 0.007240509148687124\n",
            "step: 70, loss: 0.005857500247657299\n",
            "step: 80, loss: 0.0016151488525792956\n",
            "step: 90, loss: 0.005222050007432699\n",
            "step: 100, loss: 0.0003830326604656875\n",
            "step: 110, loss: 0.0006263061659410596\n",
            "step: 120, loss: 0.00031249551102519035\n",
            "step: 130, loss: 0.005030668340623379\n",
            "step: 140, loss: 0.00558964628726244\n",
            "step: 150, loss: 0.004065047483891249\n",
            "step: 160, loss: 0.004608800634741783\n",
            "step: 170, loss: 0.004051170777529478\n",
            "step: 180, loss: 0.0003503016196191311\n",
            "step: 190, loss: 0.1372591257095337\n",
            "step: 200, loss: 0.009842142462730408\n",
            "step: 210, loss: 0.000984074780717492\n",
            "step: 220, loss: 0.0010006538359448314\n",
            "step: 230, loss: 0.0023093107156455517\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.968609865470852, f1=0.971815107102593, best_f1=0.9697648376259798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012258029542863369\n",
            "step: 10, loss: 0.0016358381835743785\n",
            "step: 20, loss: 0.01178350206464529\n",
            "step: 30, loss: 0.0004613500786945224\n",
            "step: 40, loss: 0.0005605022306554019\n",
            "step: 50, loss: 0.0003767538000829518\n",
            "step: 60, loss: 0.0005358420312404633\n",
            "step: 70, loss: 0.00041513796895742416\n",
            "step: 80, loss: 0.027978947386145592\n",
            "step: 90, loss: 0.00017834786558523774\n",
            "step: 100, loss: 0.05534980818629265\n",
            "step: 110, loss: 0.00036079471465200186\n",
            "step: 120, loss: 0.00036807722062803805\n",
            "step: 130, loss: 0.0007021580822765827\n",
            "step: 140, loss: 0.00037151339347474277\n",
            "step: 150, loss: 0.06177107244729996\n",
            "step: 160, loss: 0.022736893966794014\n",
            "step: 170, loss: 0.00030009745387360454\n",
            "step: 180, loss: 0.004132567439228296\n",
            "step: 190, loss: 0.0007834971765987575\n",
            "step: 200, loss: 0.00018944342446047813\n",
            "step: 210, loss: 0.00032765051582828164\n",
            "step: 220, loss: 0.00020357068569865078\n",
            "step: 230, loss: 0.04698583483695984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9697648376259798, f1=0.9699666295884317, best_f1=0.9697648376259798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000907611392904073\n",
            "step: 10, loss: 0.00028120147180743515\n",
            "step: 20, loss: 0.00036858898238278925\n",
            "step: 30, loss: 0.0006132468697614968\n",
            "step: 40, loss: 0.0004114707699045539\n",
            "step: 50, loss: 0.0002476070949342102\n",
            "step: 60, loss: 0.0005992381484247744\n",
            "step: 70, loss: 0.04281932860612869\n",
            "step: 80, loss: 0.001096912194043398\n",
            "step: 90, loss: 0.0012926830677315593\n",
            "step: 100, loss: 0.0007660894771106541\n",
            "step: 110, loss: 0.0053980061784386635\n",
            "step: 120, loss: 0.00031609940924681723\n",
            "step: 130, loss: 0.0011448999866843224\n",
            "step: 140, loss: 0.0037565287202596664\n",
            "step: 150, loss: 0.0008793286397121847\n",
            "step: 160, loss: 0.03241570666432381\n",
            "step: 170, loss: 0.0005328536499291658\n",
            "step: 180, loss: 0.023248106241226196\n",
            "step: 190, loss: 0.0004164382116869092\n",
            "step: 200, loss: 0.05820523202419281\n",
            "step: 210, loss: 0.0001229563495144248\n",
            "step: 220, loss: 0.03091122955083847\n",
            "step: 230, loss: 0.005554980132728815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9700996677740864, f1=0.9700332963374029, best_f1=0.9700332963374029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000301895517623052\n",
            "step: 10, loss: 0.0007854574942030013\n",
            "step: 20, loss: 0.0008268484962172806\n",
            "step: 30, loss: 0.0010686598252505064\n",
            "step: 40, loss: 0.002461395924910903\n",
            "step: 50, loss: 0.0016163012478500605\n",
            "step: 60, loss: 0.00042478577233850956\n",
            "step: 70, loss: 0.00045695988228544593\n",
            "step: 80, loss: 0.000348825502442196\n",
            "step: 90, loss: 0.000696720730047673\n",
            "step: 100, loss: 0.0005229242378845811\n",
            "step: 110, loss: 0.029629839584231377\n",
            "step: 120, loss: 0.0017694549169391394\n",
            "step: 130, loss: 0.0005836461787112057\n",
            "step: 140, loss: 0.0002071752678602934\n",
            "step: 150, loss: 0.0001018283874145709\n",
            "step: 160, loss: 0.0002999334246851504\n",
            "step: 170, loss: 0.00012030345533275977\n",
            "step: 180, loss: 0.00012651056749746203\n",
            "step: 190, loss: 0.00010420144099043682\n",
            "step: 200, loss: 0.002532476792111993\n",
            "step: 210, loss: 0.00021720574295613915\n",
            "step: 220, loss: 0.00010462160571478307\n",
            "step: 230, loss: 0.002196486806496978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9708520179372198, f1=0.972129319955407, best_f1=0.972129319955407\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.41509031993337e-05\n",
            "step: 10, loss: 0.0001187055604532361\n",
            "step: 20, loss: 0.0008428939036093652\n",
            "step: 30, loss: 7.571868627564982e-05\n",
            "step: 40, loss: 0.02683279477059841\n",
            "step: 50, loss: 0.00017273733101319522\n",
            "step: 60, loss: 9.276321361539885e-05\n",
            "step: 70, loss: 0.00010384216147940606\n",
            "step: 80, loss: 5.145450631971471e-05\n",
            "step: 90, loss: 0.00010681209823815152\n",
            "step: 100, loss: 0.0001505156687926501\n",
            "step: 110, loss: 3.615994137362577e-05\n",
            "step: 120, loss: 5.8902427554130554e-05\n",
            "step: 130, loss: 0.013781266286969185\n",
            "step: 140, loss: 0.0001939079666044563\n",
            "step: 150, loss: 0.008810819126665592\n",
            "step: 160, loss: 0.00024715482140891254\n",
            "step: 170, loss: 0.00019718968542292714\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 180, loss: 0.035100433975458145\n",
            "step: 190, loss: 0.000562600907869637\n",
            "step: 200, loss: 0.0001854769652709365\n",
            "step: 210, loss: 0.0001293682144023478\n",
            "step: 220, loss: 0.0010974983451887965\n",
            "step: 230, loss: 0.005567556247115135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9753363228699552, f1=0.9743589743589743, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012309293379075825\n",
            "step: 10, loss: 0.0006566098891198635\n",
            "step: 20, loss: 7.080347131704912e-05\n",
            "step: 30, loss: 0.004232343751937151\n",
            "step: 40, loss: 0.003956152126193047\n",
            "step: 50, loss: 0.0009746179566718638\n",
            "step: 60, loss: 0.0002583756868261844\n",
            "step: 70, loss: 0.0007707460899837315\n",
            "step: 80, loss: 9.18918231036514e-05\n",
            "step: 90, loss: 0.1488073319196701\n",
            "step: 100, loss: 0.0006029705982655287\n",
            "step: 110, loss: 0.0003258445649407804\n",
            "step: 120, loss: 0.0006634207675233483\n",
            "step: 130, loss: 0.020126106217503548\n",
            "step: 140, loss: 0.03333708271384239\n",
            "step: 150, loss: 0.019209640100598335\n",
            "step: 160, loss: 0.00016083660011645406\n",
            "step: 170, loss: 0.00014680753520224243\n",
            "step: 180, loss: 0.0008461043471470475\n",
            "step: 190, loss: 0.009220442734658718\n",
            "step: 200, loss: 5.248318120720796e-05\n",
            "step: 210, loss: 0.00033898805850185454\n",
            "step: 220, loss: 0.0001217383032781072\n",
            "step: 230, loss: 0.00017717447190079838\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9718785151856018, f1=0.9732142857142857, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001082547940313816\n",
            "step: 10, loss: 0.0007542225066572428\n",
            "step: 20, loss: 0.0006373166688717902\n",
            "step: 30, loss: 0.0032065820414572954\n",
            "step: 40, loss: 0.00023755579604767263\n",
            "step: 50, loss: 0.0003005600592587143\n",
            "step: 60, loss: 0.0005826543783769011\n",
            "step: 70, loss: 0.0002473366039339453\n",
            "step: 80, loss: 6.826022581662983e-05\n",
            "step: 90, loss: 0.0001704392780084163\n",
            "step: 100, loss: 6.0621227021329105e-05\n",
            "step: 110, loss: 0.013467181473970413\n",
            "step: 120, loss: 0.00011571295181056485\n",
            "step: 130, loss: 4.7608460590709e-05\n",
            "step: 140, loss: 0.00017506699077785015\n",
            "step: 150, loss: 0.033431556075811386\n",
            "step: 160, loss: 0.00011749450641218573\n",
            "step: 170, loss: 0.009234398603439331\n",
            "step: 180, loss: 0.0025361478328704834\n",
            "step: 190, loss: 5.618896466330625e-05\n",
            "step: 200, loss: 0.00101162726059556\n",
            "step: 210, loss: 5.147333286004141e-05\n",
            "step: 220, loss: 9.098344162339345e-05\n",
            "step: 230, loss: 9.956162830349058e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9711111111111111, f1=0.9711751662971175, best_f1=0.9743589743589743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016272625653073192\n",
            "step: 10, loss: 0.00023416437034029514\n",
            "step: 20, loss: 8.574886305723339e-05\n",
            "step: 30, loss: 0.0021005223970860243\n",
            "step: 40, loss: 0.00026614920352585614\n",
            "step: 50, loss: 0.0015271123265847564\n",
            "step: 60, loss: 0.0009461512672714889\n",
            "step: 70, loss: 8.716042066225782e-05\n",
            "step: 80, loss: 0.0006352121708914638\n",
            "step: 90, loss: 6.405540625564754e-05\n",
            "step: 100, loss: 0.0001206728775287047\n",
            "step: 110, loss: 0.01903690956532955\n",
            "step: 120, loss: 9.165136725641787e-05\n",
            "step: 130, loss: 0.0003156965831294656\n",
            "step: 140, loss: 0.0031028420198708773\n",
            "step: 150, loss: 0.001065470976755023\n",
            "step: 160, loss: 5.886482176720165e-05\n",
            "step: 170, loss: 0.0001407195086358115\n",
            "step: 180, loss: 0.03380024805665016\n",
            "step: 190, loss: 0.010658049955964088\n",
            "step: 200, loss: 3.465500776655972e-05\n",
            "step: 210, loss: 6.48590867058374e-05\n",
            "step: 220, loss: 5.702258931705728e-05\n",
            "step: 230, loss: 0.03520560637116432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9763779527559054, f1=0.9763779527559054, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000746986479498446\n",
            "step: 10, loss: 0.0002378136559855193\n",
            "step: 20, loss: 0.00016260403208434582\n",
            "step: 30, loss: 0.0002799673820845783\n",
            "step: 40, loss: 0.00017760424816515297\n",
            "step: 50, loss: 0.000406405859393999\n",
            "step: 60, loss: 0.00013225697330199182\n",
            "step: 70, loss: 0.0001730285002849996\n",
            "step: 80, loss: 5.890129614272155e-05\n",
            "step: 90, loss: 0.0017442188691347837\n",
            "step: 100, loss: 4.487248952500522e-05\n",
            "step: 110, loss: 0.016238968819379807\n",
            "step: 120, loss: 0.022636588662862778\n",
            "step: 130, loss: 4.498849739320576e-05\n",
            "step: 140, loss: 6.942627805983648e-05\n",
            "step: 150, loss: 5.757591497967951e-05\n",
            "step: 160, loss: 0.00014203315367922187\n",
            "step: 170, loss: 0.0001559929078212008\n",
            "step: 180, loss: 4.7712404921185225e-05\n",
            "step: 190, loss: 0.00021155660215299577\n",
            "step: 200, loss: 5.216161662247032e-05\n",
            "step: 210, loss: 2.5178753276122734e-05\n",
            "step: 220, loss: 0.0009613155853003263\n",
            "step: 230, loss: 3.14851786242798e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9762174405436014, f1=0.976271186440678, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011008769797626883\n",
            "step: 10, loss: 0.0003261104866396636\n",
            "step: 20, loss: 0.00015060327132232487\n",
            "step: 30, loss: 5.9371352108428255e-05\n",
            "step: 40, loss: 0.019649265334010124\n",
            "step: 50, loss: 6.536687578773126e-05\n",
            "step: 60, loss: 6.015353574184701e-05\n",
            "step: 70, loss: 0.003872416913509369\n",
            "step: 80, loss: 0.0002312414871994406\n",
            "step: 90, loss: 6.751132605131716e-05\n",
            "step: 100, loss: 5.4874777561053634e-05\n",
            "step: 110, loss: 0.0016193098854273558\n",
            "step: 120, loss: 3.8137073715915903e-05\n",
            "step: 130, loss: 0.00018042774172499776\n",
            "step: 140, loss: 0.0025702188722789288\n",
            "step: 150, loss: 5.646973295370117e-05\n",
            "step: 160, loss: 0.010772457346320152\n",
            "step: 170, loss: 4.6063480112934485e-05\n",
            "step: 180, loss: 6.530081009259447e-05\n",
            "step: 190, loss: 0.00013166297867428511\n",
            "step: 200, loss: 5.100499402033165e-05\n",
            "step: 210, loss: 0.06368536502122879\n",
            "step: 220, loss: 0.00019043976499233395\n",
            "step: 230, loss: 0.034433912485837936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9751693002257337, f1=0.9775280898876404, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010028282849816605\n",
            "step: 10, loss: 4.347638241597451e-05\n",
            "step: 20, loss: 8.474405331071466e-05\n",
            "step: 30, loss: 0.00011707057274179533\n",
            "step: 40, loss: 0.0001518966892035678\n",
            "step: 50, loss: 0.02318352460861206\n",
            "step: 60, loss: 8.581506699556485e-05\n",
            "step: 70, loss: 0.00019724649609997869\n",
            "step: 80, loss: 0.0006677304045297205\n",
            "step: 90, loss: 0.0006172873545438051\n",
            "step: 100, loss: 0.0005846224376000464\n",
            "step: 110, loss: 0.0005989767960272729\n",
            "step: 120, loss: 0.0032325678039342165\n",
            "step: 130, loss: 0.004524335265159607\n",
            "step: 140, loss: 4.7724814066896215e-05\n",
            "step: 150, loss: 0.00016644033894408494\n",
            "step: 160, loss: 3.141425986541435e-05\n",
            "step: 170, loss: 5.056156078353524e-05\n",
            "step: 180, loss: 0.00011269754759268835\n",
            "step: 190, loss: 0.0002570237556938082\n",
            "step: 200, loss: 4.9553618737263605e-05\n",
            "step: 210, loss: 0.00011251454998273402\n",
            "step: 220, loss: 0.011925182305276394\n",
            "step: 230, loss: 3.706071947817691e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.976271186440678, f1=0.9775280898876404, best_f1=0.9763779527559054\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 216.45it/s]\n",
            "load_f1 = 0.9744160177975528\n",
            "real_f1 = 0.9700996677740864\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY0y_yZuhstx"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0b6ba6-085b-427f-ccac-fff414800229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 433/433 [00:00<00:00, 372kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 340kB/s]\n",
            "Downloading: 100% 440M/440M [00:23<00:00, 18.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6263441443443298\n",
            "step: 10, loss: 0.5422959923744202\n",
            "step: 20, loss: 0.538320779800415\n",
            "step: 30, loss: 0.28129613399505615\n",
            "step: 40, loss: 0.18666069209575653\n",
            "step: 50, loss: 0.05580400675535202\n",
            "step: 60, loss: 0.08117463439702988\n",
            "step: 70, loss: 0.21542014181613922\n",
            "step: 80, loss: 0.05957085266709328\n",
            "step: 90, loss: 0.2760779559612274\n",
            "step: 100, loss: 0.06588171422481537\n",
            "step: 110, loss: 0.08823135495185852\n",
            "step: 120, loss: 0.11532507836818695\n",
            "step: 130, loss: 0.10260006040334702\n",
            "step: 140, loss: 0.017330430448055267\n",
            "step: 150, loss: 0.04468473792076111\n",
            "step: 160, loss: 0.05760886147618294\n",
            "step: 170, loss: 0.24745383858680725\n",
            "step: 180, loss: 0.07650333642959595\n",
            "step: 190, loss: 0.03370704874396324\n",
            "step: 200, loss: 0.11344972252845764\n",
            "step: 210, loss: 0.08719383180141449\n",
            "step: 220, loss: 0.24003532528877258\n",
            "step: 230, loss: 0.17931783199310303\n",
            "step: 240, loss: 0.04055439308285713\n",
            "step: 250, loss: 0.038576625287532806\n",
            "step: 260, loss: 0.17586012184619904\n",
            "step: 270, loss: 0.03328678384423256\n",
            "step: 280, loss: 0.09782367199659348\n",
            "step: 290, loss: 0.10200545191764832\n",
            "step: 300, loss: 0.032059408724308014\n",
            "step: 310, loss: 0.18019340932369232\n",
            "step: 320, loss: 0.14011983573436737\n",
            "step: 330, loss: 0.05250430852174759\n",
            "step: 340, loss: 0.06215352192521095\n",
            "step: 350, loss: 0.01249674241989851\n",
            "step: 360, loss: 0.19167301058769226\n",
            "step: 370, loss: 0.115422323346138\n",
            "step: 380, loss: 0.048335183411836624\n",
            "step: 390, loss: 0.12309517711400986\n",
            "step: 400, loss: 0.2062489092350006\n",
            "step: 410, loss: 0.04180289804935455\n",
            "step: 420, loss: 0.027076300233602524\n",
            "step: 430, loss: 0.12122359871864319\n",
            "step: 440, loss: 0.04461876302957535\n",
            "step: 450, loss: 0.02117585577070713\n",
            "step: 460, loss: 0.005673619918525219\n",
            "step: 470, loss: 0.07085166126489639\n",
            "step: 480, loss: 0.10046060383319855\n",
            "step: 490, loss: 0.0627736747264862\n",
            "step: 500, loss: 0.13356377184391022\n",
            "step: 510, loss: 0.04027629643678665\n",
            "step: 520, loss: 0.26647230982780457\n",
            "step: 530, loss: 0.007560840807855129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9231494578029231, f1=0.9227166276346604, best_f1=0.9227166276346604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0741306021809578\n",
            "step: 10, loss: 0.09889821708202362\n",
            "step: 20, loss: 0.049205850809812546\n",
            "step: 30, loss: 0.21569184958934784\n",
            "step: 40, loss: 0.05649994686245918\n",
            "step: 50, loss: 0.09137087315320969\n",
            "step: 60, loss: 0.01186579093337059\n",
            "step: 70, loss: 0.1777612566947937\n",
            "step: 80, loss: 0.10539966821670532\n",
            "step: 90, loss: 0.03601565584540367\n",
            "step: 100, loss: 0.08990199863910675\n",
            "step: 110, loss: 0.12431672215461731\n",
            "step: 120, loss: 0.1028200089931488\n",
            "step: 130, loss: 0.0400862880051136\n",
            "step: 140, loss: 0.1871359795331955\n",
            "step: 150, loss: 0.07888779044151306\n",
            "step: 160, loss: 0.038981884717941284\n",
            "step: 170, loss: 0.24662235379219055\n",
            "step: 180, loss: 0.020145995542407036\n",
            "step: 190, loss: 0.08211145550012589\n",
            "step: 200, loss: 0.006461150478571653\n",
            "step: 210, loss: 0.0453551821410656\n",
            "step: 220, loss: 0.06555742025375366\n",
            "step: 230, loss: 0.07325336337089539\n",
            "step: 240, loss: 0.029503513127565384\n",
            "step: 250, loss: 0.04258858785033226\n",
            "step: 260, loss: 0.051397740840911865\n",
            "step: 270, loss: 0.14577752351760864\n",
            "step: 280, loss: 0.03237168490886688\n",
            "step: 290, loss: 0.0446699894964695\n",
            "step: 300, loss: 0.11270115524530411\n",
            "step: 310, loss: 0.008886703290045261\n",
            "step: 320, loss: 0.15687011182308197\n",
            "step: 330, loss: 0.059792399406433105\n",
            "step: 340, loss: 0.030367476865649223\n",
            "step: 350, loss: 0.006892746780067682\n",
            "step: 360, loss: 0.11750373244285583\n",
            "step: 370, loss: 0.14690572023391724\n",
            "step: 380, loss: 0.04403352737426758\n",
            "step: 390, loss: 0.0839281752705574\n",
            "step: 400, loss: 0.07118800282478333\n",
            "step: 410, loss: 0.017694644629955292\n",
            "step: 420, loss: 0.05997280403971672\n",
            "step: 430, loss: 0.04019733518362045\n",
            "step: 440, loss: 0.09397319704294205\n",
            "step: 450, loss: 0.03954606503248215\n",
            "step: 460, loss: 0.02838226407766342\n",
            "step: 470, loss: 0.07848024368286133\n",
            "step: 480, loss: 0.2093539535999298\n",
            "step: 490, loss: 0.02477206476032734\n",
            "step: 500, loss: 0.26993781328201294\n",
            "step: 510, loss: 0.024231867864727974\n",
            "step: 520, loss: 0.08033664524555206\n",
            "step: 530, loss: 0.010186073370277882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9299719887955181, f1=0.9188935771214253, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13743431866168976\n",
            "step: 10, loss: 0.01770239882171154\n",
            "step: 20, loss: 0.030158163979649544\n",
            "step: 30, loss: 0.05812908336520195\n",
            "step: 40, loss: 0.004175954964011908\n",
            "step: 50, loss: 0.17334023118019104\n",
            "step: 60, loss: 0.01351814717054367\n",
            "step: 70, loss: 0.006282889749854803\n",
            "step: 80, loss: 0.004224184900522232\n",
            "step: 90, loss: 0.0065091815777122974\n",
            "step: 100, loss: 0.0680222362279892\n",
            "step: 110, loss: 0.0024481010623276234\n",
            "step: 120, loss: 0.002165780169889331\n",
            "step: 130, loss: 0.0020156907849013805\n",
            "step: 140, loss: 0.02632877230644226\n",
            "step: 150, loss: 0.14535880088806152\n",
            "step: 160, loss: 0.005936040543019772\n",
            "step: 170, loss: 0.06646917015314102\n",
            "step: 180, loss: 0.04056720808148384\n",
            "step: 190, loss: 0.009109395556151867\n",
            "step: 200, loss: 0.0660613402724266\n",
            "step: 210, loss: 0.065157949924469\n",
            "step: 220, loss: 0.03729584068059921\n",
            "step: 230, loss: 0.020149964839220047\n",
            "step: 240, loss: 0.02297905460000038\n",
            "step: 250, loss: 0.034921448677778244\n",
            "step: 260, loss: 0.018696963787078857\n",
            "step: 270, loss: 0.03055976890027523\n",
            "step: 280, loss: 0.06840650737285614\n",
            "step: 290, loss: 0.005438537336885929\n",
            "step: 300, loss: 0.11902067065238953\n",
            "step: 310, loss: 0.005298024974763393\n",
            "step: 320, loss: 0.018929840996861458\n",
            "step: 330, loss: 0.04057563096284866\n",
            "step: 340, loss: 0.03281598910689354\n",
            "step: 350, loss: 0.004518305882811546\n",
            "step: 360, loss: 0.007606314029544592\n",
            "step: 370, loss: 0.021685125306248665\n",
            "step: 380, loss: 0.017247198149561882\n",
            "step: 390, loss: 0.019718112424016\n",
            "step: 400, loss: 0.0265974048525095\n",
            "step: 410, loss: 0.01043847668915987\n",
            "step: 420, loss: 0.1630028635263443\n",
            "step: 430, loss: 0.02904169261455536\n",
            "step: 440, loss: 0.07193893939256668\n",
            "step: 450, loss: 0.07647436857223511\n",
            "step: 460, loss: 0.01765414886176586\n",
            "step: 470, loss: 0.03838871046900749\n",
            "step: 480, loss: 0.006192476022988558\n",
            "step: 490, loss: 0.0017909867456182837\n",
            "step: 500, loss: 0.003522126702591777\n",
            "step: 510, loss: 0.009406518191099167\n",
            "step: 520, loss: 0.1844681352376938\n",
            "step: 530, loss: 0.04791044816374779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9275766016713091, f1=0.9199255121042831, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026366176083683968\n",
            "step: 10, loss: 0.024726320058107376\n",
            "step: 20, loss: 0.003292656969279051\n",
            "step: 30, loss: 0.0021639468614012003\n",
            "step: 40, loss: 0.04543953016400337\n",
            "step: 50, loss: 0.007482662331312895\n",
            "step: 60, loss: 0.010970402508974075\n",
            "step: 70, loss: 0.014376931823790073\n",
            "step: 80, loss: 0.009107978083193302\n",
            "step: 90, loss: 0.05482754856348038\n",
            "step: 100, loss: 0.022996000945568085\n",
            "step: 110, loss: 0.0010195377981290221\n",
            "step: 120, loss: 0.0009471333469264209\n",
            "step: 130, loss: 0.0018285212572664022\n",
            "step: 140, loss: 0.007301033474504948\n",
            "step: 150, loss: 0.0032989648170769215\n",
            "step: 160, loss: 0.07408155500888824\n",
            "step: 170, loss: 0.012756699696183205\n",
            "step: 180, loss: 0.015437295660376549\n",
            "step: 190, loss: 0.03448645398020744\n",
            "step: 200, loss: 0.013406487181782722\n",
            "step: 210, loss: 0.007601963356137276\n",
            "step: 220, loss: 0.059350114315748215\n",
            "step: 230, loss: 0.10126177221536636\n",
            "step: 240, loss: 0.02409338392317295\n",
            "step: 250, loss: 0.02651871368288994\n",
            "step: 260, loss: 0.035530004650354385\n",
            "step: 270, loss: 0.023804837837815285\n",
            "step: 280, loss: 0.041852861642837524\n",
            "step: 290, loss: 0.04186892509460449\n",
            "step: 300, loss: 0.00033584743505343795\n",
            "step: 310, loss: 0.05305545777082443\n",
            "step: 320, loss: 0.15026044845581055\n",
            "step: 330, loss: 0.145939439535141\n",
            "step: 340, loss: 0.025162385776638985\n",
            "step: 350, loss: 0.030813319608569145\n",
            "step: 360, loss: 0.02777247317135334\n",
            "step: 370, loss: 0.005911823362112045\n",
            "step: 380, loss: 0.02194920741021633\n",
            "step: 390, loss: 0.03662055358290672\n",
            "step: 400, loss: 0.0280385110527277\n",
            "step: 410, loss: 0.006120931822806597\n",
            "step: 420, loss: 0.011810424737632275\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 430, loss: 0.15639127790927887\n",
            "step: 440, loss: 0.078633151948452\n",
            "step: 450, loss: 0.0232026819139719\n",
            "step: 460, loss: 0.005140499211847782\n",
            "step: 470, loss: 0.027711516246199608\n",
            "step: 480, loss: 0.06583379209041595\n",
            "step: 490, loss: 0.007621748372912407\n",
            "step: 500, loss: 0.004234509542584419\n",
            "step: 510, loss: 0.07544154673814774\n",
            "step: 520, loss: 0.06249309703707695\n",
            "step: 530, loss: 0.06611932069063187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.925207756232687, f1=0.9162011173184358, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1296815276145935\n",
            "step: 10, loss: 0.03197420760989189\n",
            "step: 20, loss: 0.0029952405020594597\n",
            "step: 30, loss: 0.003895822213962674\n",
            "step: 40, loss: 0.009630519896745682\n",
            "step: 50, loss: 0.03153996542096138\n",
            "step: 60, loss: 0.14783278107643127\n",
            "step: 70, loss: 0.020845919847488403\n",
            "step: 80, loss: 0.0037032747641205788\n",
            "step: 90, loss: 0.00808038655668497\n",
            "step: 100, loss: 0.0046204919926822186\n",
            "step: 110, loss: 0.004851222038269043\n",
            "step: 120, loss: 0.004586380906403065\n",
            "step: 130, loss: 0.0004957496421411633\n",
            "step: 140, loss: 0.003411268349736929\n",
            "step: 150, loss: 0.005656341556459665\n",
            "step: 160, loss: 0.0005953649524599314\n",
            "step: 170, loss: 0.009735982865095139\n",
            "step: 180, loss: 0.007014965172857046\n",
            "step: 190, loss: 0.0027323788963258266\n",
            "step: 200, loss: 0.0026415123138576746\n",
            "step: 210, loss: 0.0009820784907788038\n",
            "step: 220, loss: 0.0003298665687907487\n",
            "step: 230, loss: 0.008007308468222618\n",
            "step: 240, loss: 0.05776214972138405\n",
            "step: 250, loss: 0.08681215345859528\n",
            "step: 260, loss: 0.007012795656919479\n",
            "step: 270, loss: 0.002282913541421294\n",
            "step: 280, loss: 0.0032536920625716448\n",
            "step: 290, loss: 0.1404784470796585\n",
            "step: 300, loss: 0.002164955250918865\n",
            "step: 310, loss: 0.009943959303200245\n",
            "step: 320, loss: 0.1695280373096466\n",
            "step: 330, loss: 0.2209816575050354\n",
            "step: 340, loss: 0.00922276172786951\n",
            "step: 350, loss: 0.007139923050999641\n",
            "step: 360, loss: 0.012188193388283253\n",
            "step: 370, loss: 0.05542575567960739\n",
            "step: 380, loss: 0.0010875500738620758\n",
            "step: 390, loss: 0.0004985832492820919\n",
            "step: 400, loss: 0.001076212851330638\n",
            "step: 410, loss: 0.0034615658223628998\n",
            "step: 420, loss: 0.0010999965015798807\n",
            "step: 430, loss: 0.008532071486115456\n",
            "step: 440, loss: 0.01628212071955204\n",
            "step: 450, loss: 0.006901222746819258\n",
            "step: 460, loss: 0.12064046412706375\n",
            "step: 470, loss: 0.043055661022663116\n",
            "step: 480, loss: 0.03439009562134743\n",
            "step: 490, loss: 0.003993666730821133\n",
            "step: 500, loss: 0.014692233875393867\n",
            "step: 510, loss: 0.1322866827249527\n",
            "step: 520, loss: 0.011275452561676502\n",
            "step: 530, loss: 0.016567591577768326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9239981575310916, f1=0.9218821379625399, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0063691288232803345\n",
            "step: 10, loss: 0.00019652696209959686\n",
            "step: 20, loss: 0.029393384233117104\n",
            "step: 30, loss: 0.0011084831785410643\n",
            "step: 40, loss: 0.02714735083281994\n",
            "step: 50, loss: 0.03097156062722206\n",
            "step: 60, loss: 0.005932871717959642\n",
            "step: 70, loss: 0.00661913026124239\n",
            "step: 80, loss: 0.05607961490750313\n",
            "step: 90, loss: 0.014363656751811504\n",
            "step: 100, loss: 0.0066025578416883945\n",
            "step: 110, loss: 0.020142776891589165\n",
            "step: 120, loss: 0.006160982884466648\n",
            "step: 130, loss: 0.0006805055891163647\n",
            "step: 140, loss: 0.005771709606051445\n",
            "step: 150, loss: 0.0008612805395387113\n",
            "step: 160, loss: 0.002088561886921525\n",
            "step: 170, loss: 0.0017223403556272388\n",
            "step: 180, loss: 0.0016568029532209039\n",
            "step: 190, loss: 0.06934888660907745\n",
            "step: 200, loss: 0.007234266959130764\n",
            "step: 210, loss: 0.011068717576563358\n",
            "step: 220, loss: 0.016518373042345047\n",
            "step: 230, loss: 0.0015606287634000182\n",
            "step: 240, loss: 0.04019057750701904\n",
            "step: 250, loss: 0.20633651316165924\n",
            "step: 260, loss: 0.014120636507868767\n",
            "step: 270, loss: 0.10021437704563141\n",
            "step: 280, loss: 0.021427780389785767\n",
            "step: 290, loss: 0.0007402130286209285\n",
            "step: 300, loss: 0.0002792020095512271\n",
            "step: 310, loss: 0.006749187596142292\n",
            "step: 320, loss: 0.0010196005459874868\n",
            "step: 330, loss: 0.00033122641616500914\n",
            "step: 340, loss: 0.027045907452702522\n",
            "step: 350, loss: 0.027519894763827324\n",
            "step: 360, loss: 0.014506897889077663\n",
            "step: 370, loss: 0.003855054033920169\n",
            "step: 380, loss: 0.0021626525558531284\n",
            "step: 390, loss: 0.0019924850203096867\n",
            "step: 400, loss: 0.0007444850052706897\n",
            "step: 410, loss: 0.006940852850675583\n",
            "step: 420, loss: 0.0033410806208848953\n",
            "step: 430, loss: 0.0005822185194119811\n",
            "step: 440, loss: 0.0007540853694081306\n",
            "step: 450, loss: 0.00013352853420656174\n",
            "step: 460, loss: 0.0002689276298042387\n",
            "step: 470, loss: 0.08072850108146667\n",
            "step: 480, loss: 0.0018351171165704727\n",
            "step: 490, loss: 0.006130819208920002\n",
            "step: 500, loss: 0.002241765847429633\n",
            "step: 510, loss: 0.00488983653485775\n",
            "step: 520, loss: 0.009333032183349133\n",
            "step: 530, loss: 0.010938185267150402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9172644667623147, f1=0.9155597722960152, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004841187037527561\n",
            "step: 10, loss: 0.0003756880760192871\n",
            "step: 20, loss: 0.005426596384495497\n",
            "step: 30, loss: 0.047264933586120605\n",
            "step: 40, loss: 0.01193028874695301\n",
            "step: 50, loss: 0.001998870400711894\n",
            "step: 60, loss: 0.0026355083100497723\n",
            "step: 70, loss: 0.06317660212516785\n",
            "step: 80, loss: 0.09725791960954666\n",
            "step: 90, loss: 0.0005669857491739094\n",
            "step: 100, loss: 0.00024932328960858285\n",
            "step: 110, loss: 0.0008421456441283226\n",
            "step: 120, loss: 0.008377330377697945\n",
            "step: 130, loss: 0.002331276424229145\n",
            "step: 140, loss: 0.0004238022374920547\n",
            "step: 150, loss: 0.026409268379211426\n",
            "step: 160, loss: 0.00013214029604569077\n",
            "step: 170, loss: 0.0003122409398201853\n",
            "step: 180, loss: 0.0026237976271659136\n",
            "step: 190, loss: 0.06748253852128983\n",
            "step: 200, loss: 0.00649880850687623\n",
            "step: 210, loss: 0.05998358502984047\n",
            "step: 220, loss: 0.0043152146972715855\n",
            "step: 230, loss: 0.0642395094037056\n",
            "step: 240, loss: 0.00124158279504627\n",
            "step: 250, loss: 0.0023616510443389416\n",
            "step: 260, loss: 0.0005122327129356563\n",
            "step: 270, loss: 0.0013246553717181087\n",
            "step: 280, loss: 0.0013361196033656597\n",
            "step: 290, loss: 0.00013178290100768209\n",
            "step: 300, loss: 0.0033520134165883064\n",
            "step: 310, loss: 0.00013529454008676112\n",
            "step: 320, loss: 0.0010549018625169992\n",
            "step: 330, loss: 0.0009608073742128909\n",
            "step: 340, loss: 0.001096151303499937\n",
            "step: 350, loss: 0.00031961678178049624\n",
            "step: 360, loss: 0.002277084393426776\n",
            "step: 370, loss: 0.0005786217516288161\n",
            "step: 380, loss: 0.17880846560001373\n",
            "step: 390, loss: 0.0007124514086171985\n",
            "step: 400, loss: 0.0023822966031730175\n",
            "step: 410, loss: 0.0018701753579080105\n",
            "step: 420, loss: 0.0005827357526868582\n",
            "step: 430, loss: 0.011550122871994972\n",
            "step: 440, loss: 0.0010663775028660893\n",
            "step: 450, loss: 0.012017160654067993\n",
            "step: 460, loss: 0.0012723710387945175\n",
            "step: 470, loss: 0.0008866938296705484\n",
            "step: 480, loss: 0.19309945404529572\n",
            "step: 490, loss: 0.007274719886481762\n",
            "step: 500, loss: 0.0015072921523824334\n",
            "step: 510, loss: 0.005241123028099537\n",
            "step: 520, loss: 0.001692006946541369\n",
            "step: 530, loss: 0.0003959685273002833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9222118088097471, f1=0.9210649229332087, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005318632815033197\n",
            "step: 10, loss: 0.017370056360960007\n",
            "step: 20, loss: 0.022777236998081207\n",
            "step: 30, loss: 0.0021280485671013594\n",
            "step: 40, loss: 0.0005411102320067585\n",
            "step: 50, loss: 0.0036571354139596224\n",
            "step: 60, loss: 0.001617704750970006\n",
            "step: 70, loss: 0.0007573111797682941\n",
            "step: 80, loss: 0.025031523779034615\n",
            "step: 90, loss: 0.0006895968108437955\n",
            "step: 100, loss: 0.00024584136554040015\n",
            "step: 110, loss: 0.001432926394045353\n",
            "step: 120, loss: 0.014634670689702034\n",
            "step: 130, loss: 0.0006284479168243706\n",
            "step: 140, loss: 0.0006703464314341545\n",
            "step: 150, loss: 0.001215828931890428\n",
            "step: 160, loss: 0.0016642767004668713\n",
            "step: 170, loss: 0.014393370598554611\n",
            "step: 180, loss: 0.00038603608845733106\n",
            "step: 190, loss: 0.0029081341344863176\n",
            "step: 200, loss: 0.0017589522758498788\n",
            "step: 210, loss: 0.0027652315329760313\n",
            "step: 220, loss: 0.027667183429002762\n",
            "step: 230, loss: 0.0029490788001567125\n",
            "step: 240, loss: 0.0001783768821042031\n",
            "step: 250, loss: 0.02071077935397625\n",
            "step: 260, loss: 0.0012211843859404325\n",
            "step: 270, loss: 0.00037323805736377835\n",
            "step: 280, loss: 0.006989312823861837\n",
            "step: 290, loss: 0.0003958796733058989\n",
            "step: 300, loss: 0.0014192434027791023\n",
            "step: 310, loss: 0.0006760176038369536\n",
            "step: 320, loss: 0.050158243626356125\n",
            "step: 330, loss: 0.009695669636130333\n",
            "step: 340, loss: 0.0006764255813322961\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 350, loss: 0.01430059876292944\n",
            "step: 360, loss: 0.003760232590138912\n",
            "step: 370, loss: 0.008350121788680553\n",
            "step: 380, loss: 0.03538047894835472\n",
            "step: 390, loss: 0.14317041635513306\n",
            "step: 400, loss: 0.0008646230562590063\n",
            "step: 410, loss: 0.000493479601573199\n",
            "step: 420, loss: 0.001505960477516055\n",
            "step: 430, loss: 0.02895239181816578\n",
            "step: 440, loss: 0.019907843321561813\n",
            "step: 450, loss: 0.001104547642171383\n",
            "step: 460, loss: 0.00023008558491710573\n",
            "step: 470, loss: 0.0011073397472500801\n",
            "step: 480, loss: 0.004999987781047821\n",
            "step: 490, loss: 0.023940976709127426\n",
            "step: 500, loss: 0.0018561682663857937\n",
            "step: 510, loss: 0.00022111025464255363\n",
            "step: 520, loss: 0.001934269443154335\n",
            "step: 530, loss: 0.0002829242730513215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9252506836827713, f1=0.9137614678899082, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005861541139893234\n",
            "step: 10, loss: 0.013211864978075027\n",
            "step: 20, loss: 0.0032082144170999527\n",
            "step: 30, loss: 0.0009906067280098796\n",
            "step: 40, loss: 0.01636212132871151\n",
            "step: 50, loss: 0.00012475071707740426\n",
            "step: 60, loss: 0.00010707540059229359\n",
            "step: 70, loss: 0.024846483021974564\n",
            "step: 80, loss: 0.0019516923930495977\n",
            "step: 90, loss: 0.00011088103929068893\n",
            "step: 100, loss: 0.00012236267502885312\n",
            "step: 110, loss: 0.00033133153920061886\n",
            "step: 120, loss: 0.0007630155887454748\n",
            "step: 130, loss: 0.002594588091596961\n",
            "step: 140, loss: 0.0017904838314279914\n",
            "step: 150, loss: 0.012734124436974525\n",
            "step: 160, loss: 0.0015585578512400389\n",
            "step: 170, loss: 0.010204257443547249\n",
            "step: 180, loss: 8.335476013598964e-05\n",
            "step: 190, loss: 0.0005703337374143302\n",
            "step: 200, loss: 0.00020308923558332026\n",
            "step: 210, loss: 0.0015582535415887833\n",
            "step: 220, loss: 0.06984526664018631\n",
            "step: 230, loss: 0.00015542951587121934\n",
            "step: 240, loss: 0.0003639111528173089\n",
            "step: 250, loss: 0.00948353111743927\n",
            "step: 260, loss: 0.09115090221166611\n",
            "step: 270, loss: 0.0001681304129306227\n",
            "step: 280, loss: 0.0002498587127774954\n",
            "step: 290, loss: 0.12917639315128326\n",
            "step: 300, loss: 0.002674719551578164\n",
            "step: 310, loss: 0.010633496567606926\n",
            "step: 320, loss: 0.06189614534378052\n",
            "step: 330, loss: 0.0006219984497874975\n",
            "step: 340, loss: 0.037941157817840576\n",
            "step: 350, loss: 0.00949420128017664\n",
            "step: 360, loss: 0.009729929268360138\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 370, loss: 0.002213601255789399\n",
            "step: 380, loss: 0.013033438473939896\n",
            "step: 390, loss: 0.0005703230272047222\n",
            "step: 400, loss: 0.03762536123394966\n",
            "step: 410, loss: 0.00010898045729845762\n",
            "step: 420, loss: 0.00013216760999057442\n",
            "step: 430, loss: 9.297537326347083e-05\n",
            "step: 440, loss: 0.0021905102767050266\n",
            "step: 450, loss: 0.00017416744958609343\n",
            "step: 460, loss: 0.0005382007802836597\n",
            "step: 470, loss: 0.0001227996835950762\n",
            "step: 480, loss: 0.0004956107004545629\n",
            "step: 490, loss: 0.01934344507753849\n",
            "step: 500, loss: 0.0013279395643621683\n",
            "step: 510, loss: 0.01455722376704216\n",
            "step: 520, loss: 0.00026598613476380706\n",
            "step: 530, loss: 4.731606532004662e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9222222222222223, f1=0.917789131444496, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00723790330812335\n",
            "step: 10, loss: 0.00014433912292588502\n",
            "step: 20, loss: 9.483093162998557e-05\n",
            "step: 30, loss: 0.0004406372900120914\n",
            "step: 40, loss: 0.00019988366693723947\n",
            "step: 50, loss: 0.0006521041505038738\n",
            "step: 60, loss: 0.00015310382877942175\n",
            "step: 70, loss: 8.264046482509002e-05\n",
            "step: 80, loss: 0.0002667394292075187\n",
            "step: 90, loss: 0.02997969463467598\n",
            "step: 100, loss: 0.009776776656508446\n",
            "step: 110, loss: 6.857756670797244e-05\n",
            "step: 120, loss: 0.0004380159662105143\n",
            "step: 130, loss: 0.00010730099165812135\n",
            "step: 140, loss: 0.006344191264361143\n",
            "step: 150, loss: 7.477691542590037e-05\n",
            "step: 160, loss: 3.815909440163523e-05\n",
            "step: 170, loss: 0.0003318810777273029\n",
            "step: 180, loss: 6.557336018886417e-05\n",
            "step: 190, loss: 0.0004798550216946751\n",
            "step: 200, loss: 0.01101915817707777\n",
            "step: 210, loss: 0.0030316519550979137\n",
            "step: 220, loss: 0.006839345674961805\n",
            "step: 230, loss: 0.0008719592005945742\n",
            "step: 240, loss: 0.0018691099248826504\n",
            "step: 250, loss: 0.13305984437465668\n",
            "step: 260, loss: 0.0013670481275767088\n",
            "step: 270, loss: 0.00113234284799546\n",
            "step: 280, loss: 0.00023450089793186635\n",
            "step: 290, loss: 0.005090638063848019\n",
            "step: 300, loss: 7.586859283037484e-05\n",
            "step: 310, loss: 0.00046783447032794356\n",
            "step: 320, loss: 0.011276337318122387\n",
            "step: 330, loss: 0.024186620488762856\n",
            "step: 340, loss: 0.000538929714821279\n",
            "step: 350, loss: 0.00015781256661284715\n",
            "step: 360, loss: 0.00013305575703270733\n",
            "step: 370, loss: 0.0014496354851871729\n",
            "step: 380, loss: 0.01204930804669857\n",
            "step: 390, loss: 0.00036337695200927556\n",
            "step: 400, loss: 0.0011833587195724249\n",
            "step: 410, loss: 0.001779036014340818\n",
            "step: 420, loss: 0.0007949534337967634\n",
            "step: 430, loss: 0.0020867050625383854\n",
            "step: 440, loss: 0.010790344327688217\n",
            "step: 450, loss: 0.0011327924439683557\n",
            "step: 460, loss: 0.0006373940268531442\n",
            "step: 470, loss: 0.0006492443499155343\n",
            "step: 480, loss: 0.000385663122870028\n",
            "step: 490, loss: 0.001918478519655764\n",
            "step: 500, loss: 0.0061456928960978985\n",
            "step: 510, loss: 0.00013074224989395589\n",
            "step: 520, loss: 0.0001753668620949611\n",
            "step: 530, loss: 0.00044851849088445306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9291044776119404, f1=0.9279999999999999, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014537631068378687\n",
            "step: 10, loss: 0.22688867151737213\n",
            "step: 20, loss: 0.003011011052876711\n",
            "step: 30, loss: 0.01587207242846489\n",
            "step: 40, loss: 0.00040300432010553777\n",
            "step: 50, loss: 0.004064689856022596\n",
            "step: 60, loss: 0.0050233579240739346\n",
            "step: 70, loss: 0.0020459885708987713\n",
            "step: 80, loss: 0.00013651378685608506\n",
            "step: 90, loss: 0.005565132014453411\n",
            "step: 100, loss: 0.003401447320356965\n",
            "step: 110, loss: 0.0006355591467581689\n",
            "step: 120, loss: 0.0005101982387714088\n",
            "step: 130, loss: 6.565609510289505e-05\n",
            "step: 140, loss: 0.12443101406097412\n",
            "step: 150, loss: 2.7052472432842478e-05\n",
            "step: 160, loss: 0.005066373385488987\n",
            "step: 170, loss: 0.0003255298943258822\n",
            "step: 180, loss: 0.0003073572588618845\n",
            "step: 190, loss: 0.0001260759454453364\n",
            "step: 200, loss: 0.002057983772829175\n",
            "step: 210, loss: 0.0011973886284977198\n",
            "step: 220, loss: 0.0016976551851257682\n",
            "step: 230, loss: 5.006661740480922e-05\n",
            "step: 240, loss: 0.000447752681793645\n",
            "step: 250, loss: 3.396193642402068e-05\n",
            "step: 260, loss: 5.7342967920703813e-05\n",
            "step: 270, loss: 0.00026335075381211936\n",
            "step: 280, loss: 0.00014905950229149312\n",
            "step: 290, loss: 0.00018272604211233556\n",
            "step: 300, loss: 0.00016024208161979914\n",
            "step: 310, loss: 3.871173248626292e-05\n",
            "step: 320, loss: 4.207608799333684e-05\n",
            "step: 330, loss: 0.0005184632609598339\n",
            "step: 340, loss: 0.0005878841038793325\n",
            "step: 350, loss: 7.513495074817911e-05\n",
            "step: 360, loss: 9.319264790974557e-05\n",
            "step: 370, loss: 0.003980282228440046\n",
            "step: 380, loss: 0.014624882489442825\n",
            "step: 390, loss: 6.28065608907491e-05\n",
            "step: 400, loss: 0.0018270923756062984\n",
            "step: 410, loss: 4.556407293421216e-05\n",
            "step: 420, loss: 0.00020385657262522727\n",
            "step: 430, loss: 1.8719325453275815e-05\n",
            "step: 440, loss: 0.00719557236880064\n",
            "step: 450, loss: 5.073681313660927e-05\n",
            "step: 460, loss: 0.015662625432014465\n",
            "step: 470, loss: 2.4012746507651173e-05\n",
            "step: 480, loss: 0.0014506785664707422\n",
            "step: 490, loss: 0.00015375937800854445\n",
            "step: 500, loss: 0.00015883444575592875\n",
            "step: 510, loss: 0.07460447400808334\n",
            "step: 520, loss: 5.252924165688455e-05\n",
            "step: 530, loss: 0.006852834951132536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9281045751633987, f1=0.9261682242990654, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.47285604220815e-05\n",
            "step: 10, loss: 2.4277074771816842e-05\n",
            "step: 20, loss: 0.00016832102846819907\n",
            "step: 30, loss: 0.0040268306620419025\n",
            "step: 40, loss: 0.02073713205754757\n",
            "step: 50, loss: 0.04359232261776924\n",
            "step: 60, loss: 0.0017220836598426104\n",
            "step: 70, loss: 0.0007778041181154549\n",
            "step: 80, loss: 0.00045170148951001465\n",
            "step: 90, loss: 0.00019493546278681606\n",
            "step: 100, loss: 0.0007777256541885436\n",
            "step: 110, loss: 0.16160956025123596\n",
            "step: 120, loss: 0.0001044476666720584\n",
            "step: 130, loss: 0.0020869714207947254\n",
            "step: 140, loss: 2.6728379452833906e-05\n",
            "step: 150, loss: 0.00010022400238085538\n",
            "step: 160, loss: 3.932177787646651e-05\n",
            "step: 170, loss: 6.998389289947227e-05\n",
            "step: 180, loss: 8.223217446357012e-05\n",
            "step: 190, loss: 0.002098917728289962\n",
            "step: 200, loss: 0.001897266716696322\n",
            "step: 210, loss: 0.0037682652473449707\n",
            "step: 220, loss: 0.016426095739006996\n",
            "step: 230, loss: 2.3871232770034112e-05\n",
            "step: 240, loss: 0.0002702656202018261\n",
            "step: 250, loss: 3.4121203498216346e-05\n",
            "step: 260, loss: 3.9956958062248304e-05\n",
            "step: 270, loss: 6.698981451336294e-05\n",
            "step: 280, loss: 0.0022260015830397606\n",
            "step: 290, loss: 5.4757379984948784e-05\n",
            "step: 300, loss: 0.0008666006615385413\n",
            "step: 310, loss: 8.695496944710612e-05\n",
            "step: 320, loss: 0.0006952914991416037\n",
            "step: 330, loss: 0.0013000387698411942\n",
            "step: 340, loss: 0.0001819527824409306\n",
            "step: 350, loss: 0.00029136662487871945\n",
            "step: 360, loss: 0.00458285678178072\n",
            "step: 370, loss: 0.0030386382713913918\n",
            "step: 380, loss: 0.00033005551085807383\n",
            "step: 390, loss: 0.0001195063887280412\n",
            "step: 400, loss: 5.172909004613757e-05\n",
            "step: 410, loss: 0.00026168531621806324\n",
            "step: 420, loss: 0.00268408190459013\n",
            "step: 430, loss: 0.002710238564759493\n",
            "step: 440, loss: 0.0075690713711082935\n",
            "step: 450, loss: 0.012135923840105534\n",
            "step: 460, loss: 5.9401310863904655e-05\n",
            "step: 470, loss: 0.00397678604349494\n",
            "step: 480, loss: 0.005758229177445173\n",
            "step: 490, loss: 0.004070581868290901\n",
            "step: 500, loss: 0.0001267738116439432\n",
            "step: 510, loss: 0.031442493200302124\n",
            "step: 520, loss: 0.0019667872693389654\n",
            "step: 530, loss: 0.0003689884324558079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9271402550091076, f1=0.9301688726608854, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034938151948153973\n",
            "step: 10, loss: 0.00039037875831127167\n",
            "step: 20, loss: 0.00010305019532097504\n",
            "step: 30, loss: 0.00020559628319460899\n",
            "step: 40, loss: 9.914092515828088e-05\n",
            "step: 50, loss: 0.005302437115460634\n",
            "step: 60, loss: 0.0012108908267691731\n",
            "step: 70, loss: 0.0001370461395708844\n",
            "step: 80, loss: 0.0001220771810039878\n",
            "step: 90, loss: 0.0010958834318444133\n",
            "step: 100, loss: 0.00023075116041582078\n",
            "step: 110, loss: 0.00010521559306653216\n",
            "step: 120, loss: 5.375930049922317e-05\n",
            "step: 130, loss: 6.938321894267574e-05\n",
            "step: 140, loss: 0.0001120713131967932\n",
            "step: 150, loss: 0.007804774213582277\n",
            "step: 160, loss: 0.00012899580178782344\n",
            "step: 170, loss: 5.4833544709254056e-05\n",
            "step: 180, loss: 7.120521331671625e-05\n",
            "step: 190, loss: 0.0008709412650205195\n",
            "step: 200, loss: 3.961270340369083e-05\n",
            "step: 210, loss: 7.04613994457759e-05\n",
            "step: 220, loss: 0.0005768905393779278\n",
            "step: 230, loss: 0.0007891646237112582\n",
            "step: 240, loss: 0.0013103067176416516\n",
            "step: 250, loss: 4.97673281643074e-05\n",
            "step: 260, loss: 8.416693890467286e-05\n",
            "step: 270, loss: 6.238617061171681e-05\n",
            "step: 280, loss: 3.3306518162135035e-05\n",
            "step: 290, loss: 0.0003567052772268653\n",
            "step: 300, loss: 6.883188325446099e-05\n",
            "step: 310, loss: 0.00239356467500329\n",
            "step: 320, loss: 0.0038067344576120377\n",
            "step: 330, loss: 0.01077081449329853\n",
            "step: 340, loss: 0.0017320533515885472\n",
            "step: 350, loss: 0.00017609339556656778\n",
            "step: 360, loss: 4.0535200241720304e-05\n",
            "step: 370, loss: 0.00020464735280256718\n",
            "step: 380, loss: 4.7901114157866687e-05\n",
            "step: 390, loss: 0.005025070160627365\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 400, loss: 9.882983431452885e-05\n",
            "step: 410, loss: 0.0018033041851595044\n",
            "step: 420, loss: 0.00010830177052412182\n",
            "step: 430, loss: 0.001321855466812849\n",
            "step: 440, loss: 0.0001082171147572808\n",
            "step: 450, loss: 0.0072687650099396706\n",
            "step: 460, loss: 0.0009258179925382137\n",
            "step: 470, loss: 0.0003410080971661955\n",
            "step: 480, loss: 0.00014044281851965934\n",
            "step: 490, loss: 0.00046376363025046885\n",
            "step: 500, loss: 0.0003461756859906018\n",
            "step: 510, loss: 0.0036756745539605618\n",
            "step: 520, loss: 8.132825314532965e-05\n",
            "step: 530, loss: 0.00043964950600638986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9267161410018553, f1=0.927170868347339, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002242016140371561\n",
            "step: 10, loss: 8.695901487953961e-05\n",
            "step: 20, loss: 9.11323368200101e-05\n",
            "step: 30, loss: 3.0371244065463543e-05\n",
            "step: 40, loss: 0.05223092436790466\n",
            "step: 50, loss: 0.00033573273685760796\n",
            "step: 60, loss: 1.441301174054388e-05\n",
            "step: 70, loss: 0.0004293100500945002\n",
            "step: 80, loss: 2.2976706532062963e-05\n",
            "step: 90, loss: 0.00022307624749373645\n",
            "step: 100, loss: 0.002826445270329714\n",
            "step: 110, loss: 7.939420902403072e-05\n",
            "step: 120, loss: 0.00019259807595517486\n",
            "step: 130, loss: 0.027485746890306473\n",
            "step: 140, loss: 0.0025013666599988937\n",
            "step: 150, loss: 8.515443914802745e-05\n",
            "step: 160, loss: 0.00034673407208174467\n",
            "step: 170, loss: 0.00010080429638037458\n",
            "step: 180, loss: 0.0002567768970038742\n",
            "step: 190, loss: 0.00011259245366090909\n",
            "step: 200, loss: 0.010882091708481312\n",
            "step: 210, loss: 0.000398989679524675\n",
            "step: 220, loss: 4.25642792833969e-05\n",
            "step: 230, loss: 0.04124769940972328\n",
            "step: 240, loss: 4.036070458823815e-05\n",
            "step: 250, loss: 0.00010252804349875078\n",
            "step: 260, loss: 0.00010162643593503162\n",
            "step: 270, loss: 0.0003209835267625749\n",
            "step: 280, loss: 3.3145690395031124e-05\n",
            "step: 290, loss: 0.015183967538177967\n",
            "step: 300, loss: 0.0010271512437611818\n",
            "step: 310, loss: 0.0024001356214284897\n",
            "step: 320, loss: 0.0011484628776088357\n",
            "step: 330, loss: 0.008630861528217793\n",
            "step: 340, loss: 0.004377080127596855\n",
            "step: 350, loss: 6.797469541197643e-05\n",
            "step: 360, loss: 0.00034699655952863395\n",
            "step: 370, loss: 8.689043170306832e-05\n",
            "step: 380, loss: 0.00011916739458683878\n",
            "step: 390, loss: 0.0205985140055418\n",
            "step: 400, loss: 0.00023620908905286342\n",
            "step: 410, loss: 0.00012756898649968207\n",
            "step: 420, loss: 0.00014942065172363073\n",
            "step: 430, loss: 0.0012195236049592495\n",
            "step: 440, loss: 0.00022366868506651372\n",
            "step: 450, loss: 0.0017209886573255062\n",
            "step: 460, loss: 0.0011480211978778243\n",
            "step: 470, loss: 0.00026015829644165933\n",
            "step: 480, loss: 0.0001194382130051963\n",
            "step: 490, loss: 0.00043326427112333477\n",
            "step: 500, loss: 0.0003127314557787031\n",
            "step: 510, loss: 0.0032498128712177277\n",
            "step: 520, loss: 0.0008019686792977154\n",
            "step: 530, loss: 0.00019149821309838444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9259944495837188, f1=0.9274418604651162, best_f1=0.9188935771214253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.4630884076468647e-05\n",
            "step: 10, loss: 0.00027038113330490887\n",
            "step: 20, loss: 0.00013773476530332118\n",
            "step: 30, loss: 7.687291508773342e-05\n",
            "step: 40, loss: 0.030466414988040924\n",
            "step: 50, loss: 0.0007482119835913181\n",
            "step: 60, loss: 3.979094617534429e-05\n",
            "step: 70, loss: 0.001827922067604959\n",
            "step: 80, loss: 3.2505478884559125e-05\n",
            "step: 90, loss: 2.9841507057426497e-05\n",
            "step: 100, loss: 0.0009578727767802775\n",
            "step: 110, loss: 0.001284414203837514\n",
            "step: 120, loss: 0.00016061053611338139\n",
            "step: 130, loss: 0.12842591106891632\n",
            "step: 140, loss: 0.00021117873257026076\n",
            "step: 150, loss: 6.708850560244173e-05\n",
            "step: 160, loss: 0.00013407852384261787\n",
            "step: 170, loss: 0.0001899785565910861\n",
            "step: 180, loss: 3.913562250090763e-05\n",
            "step: 190, loss: 0.00031484788632951677\n",
            "step: 200, loss: 6.346289592329413e-05\n",
            "step: 210, loss: 0.00022487793467007577\n",
            "step: 220, loss: 0.012992138043045998\n",
            "step: 230, loss: 0.0001695486280368641\n",
            "step: 240, loss: 0.027312031015753746\n",
            "step: 250, loss: 0.00015658348274882883\n",
            "step: 260, loss: 0.00012266894918866456\n",
            "step: 270, loss: 0.00030605250503867865\n",
            "step: 280, loss: 0.0002163829340133816\n",
            "step: 290, loss: 2.1054809622000903e-05\n",
            "step: 300, loss: 6.059291263227351e-05\n",
            "step: 310, loss: 0.00029934506164863706\n",
            "step: 320, loss: 5.298432006384246e-05\n",
            "step: 330, loss: 3.9302547520492226e-05\n",
            "step: 340, loss: 5.308590334607288e-05\n",
            "step: 350, loss: 0.00041241623694077134\n",
            "step: 360, loss: 0.0018596233567222953\n",
            "step: 370, loss: 9.433556988369673e-05\n",
            "step: 380, loss: 0.0003901034651789814\n",
            "step: 390, loss: 0.0004909767303615808\n",
            "step: 400, loss: 0.0004979443619959056\n",
            "step: 410, loss: 0.00029066327260807157\n",
            "step: 420, loss: 0.00038117708754725754\n",
            "step: 430, loss: 0.0003527650551404804\n",
            "step: 440, loss: 0.00016071289428509772\n",
            "step: 450, loss: 1.7389402273693122e-05\n",
            "step: 460, loss: 2.193400177930016e-05\n",
            "step: 470, loss: 0.00463105458766222\n",
            "step: 480, loss: 4.077377889188938e-05\n",
            "step: 490, loss: 0.0003451499796938151\n",
            "step: 500, loss: 0.00024244959058705717\n",
            "step: 510, loss: 0.0004830869729630649\n",
            "step: 520, loss: 5.553268056246452e-05\n",
            "step: 530, loss: 0.00012311570753809065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9259436215957956, f1=0.9261553120533587, best_f1=0.9188935771214253\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:32, 177.95it/s]\n",
            "load_f1 = 0.9262564584311883\n",
            "real_f1 = 0.9261176470588235\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 180.68it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm bert-base-uncased \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "8yLxbfdggw7_",
        "SSCCmtSggw8E",
        "5HZE1zMQgw8F"
      ],
      "name": "CMedium_30_3_5_bert.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}