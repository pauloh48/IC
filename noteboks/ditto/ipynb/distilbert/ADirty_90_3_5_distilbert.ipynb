{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADirty_90_3_5_distilbert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9629e1cf-16f4-449f-f1f5-77422b80852e"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 16.53 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 59.8 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 56.1 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 12.44 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 27.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.1 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 66.3 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449919 sha256=7d6af4980a617702d2e1f7ab4fab0c30a20291ad6cfa8d68c978b41ad0689e58\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=462aa0d62706de110bf780e824956b6f49d1138c652048037bc97b118e6afa87\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0e45c4-279b-4857-a2e5-00575d79e653"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10085, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 10085 (delta 98), reused 130 (delta 58), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10085/10085), 14.95 MiB | 12.84 MiB/s, done.\n",
            "Resolving deltas: 100% (6903/6903), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-mj7ia00_\n",
            "Created temporary directory: /tmp/pip-req-tracker-mbt492c3\n",
            "Initialized build tracking at /tmp/pip-req-tracker-mbt492c3\n",
            "Created build tracker: /tmp/pip-req-tracker-mbt492c3\n",
            "Entered build tracker: /tmp/pip-req-tracker-mbt492c3\n",
            "Created temporary directory: /tmp/pip-install-qnxfsksp\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-pqj5iuau\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-mbt492c3'\n",
            "    Running setup.py (path:/tmp/pip-req-build-pqj5iuau/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-y0hmfa86\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-y0hmfa86/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-pqj5iuau has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-mbt492c3'\n",
            "Created temporary directory: /tmp/pip-unpack-f4ykhv76\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-xgi3ju9a\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-xgi3ju9a\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-pqj5iuau/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-pqj5iuau/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-xgi3ju9a\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-xgi3ju9a/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=9bfca4d2e42f1c2876d05605a1349559e0ee5846484543ef3949f30409e9731f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mj7ia00_/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-mbt492c3'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a6452b-77b0-4824-e263-eb9e858be321"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 31.5 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.46-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 60.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 68.0 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.46\n",
            "  Downloading botocore-1.27.46-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.46->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.46->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.46 botocore-1.27.46 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1baad2-030f-465c-cac6-c9595f2bbebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "00204330-cb0e-406c-9bbe-08ac033612a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 982, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 982 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (982/982), 251.89 MiB | 17.87 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1273/1273), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed2d348-ec46-4cc5-f6e2-6962699990e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/ADirty_90_3_5/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d4e1a1-9ff6-429e-ce71-ec0aa066ac55"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 442/442 [00:00<00:00, 395kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 348kB/s]\n",
            "Downloading: 100% 268M/268M [00:03<00:00, 69.8MB/s]\n",
            "step: 0, loss: 0.8758458495140076\n",
            "epoch 1: dev_f1=0.3146067415730337, f1=0.2947368421052632, best_f1=0.2947368421052632\n",
            "step: 0, loss: 0.36972710490226746\n",
            "epoch 2: dev_f1=0.2978723404255319, f1=0.2828282828282828, best_f1=0.2947368421052632\n",
            "step: 0, loss: 0.3467859625816345\n",
            "epoch 3: dev_f1=0.35294117647058826, f1=0.34374999999999994, best_f1=0.34374999999999994\n",
            "step: 0, loss: 0.358087420463562\n",
            "epoch 4: dev_f1=0.358974358974359, f1=0.3170731707317073, best_f1=0.3170731707317073\n",
            "step: 0, loss: 0.259922593832016\n",
            "epoch 5: dev_f1=0.43076923076923085, f1=0.32876712328767127, best_f1=0.32876712328767127\n",
            "step: 0, loss: 0.25525233149528503\n",
            "epoch 6: dev_f1=0.45161290322580644, f1=0.38095238095238093, best_f1=0.38095238095238093\n",
            "step: 0, loss: 0.26379281282424927\n",
            "epoch 7: dev_f1=0.456140350877193, f1=0.37931034482758624, best_f1=0.37931034482758624\n",
            "step: 0, loss: 0.3414508104324341\n",
            "epoch 8: dev_f1=0.456140350877193, f1=0.38596491228070184, best_f1=0.37931034482758624\n",
            "step: 0, loss: 0.2124498039484024\n",
            "epoch 9: dev_f1=0.4727272727272727, f1=0.38596491228070184, best_f1=0.38596491228070184\n",
            "step: 0, loss: 0.24804510176181793\n",
            "epoch 10: dev_f1=0.55, f1=0.3902439024390244, best_f1=0.3902439024390244\n",
            "step: 0, loss: 0.1899382472038269\n",
            "epoch 11: dev_f1=0.5714285714285714, f1=0.38095238095238093, best_f1=0.38095238095238093\n",
            "step: 0, loss: 0.2318941205739975\n",
            "epoch 12: dev_f1=0.6, f1=0.3902439024390244, best_f1=0.3902439024390244\n",
            "step: 0, loss: 0.1608196645975113\n",
            "epoch 13: dev_f1=0.5714285714285714, f1=0.37209302325581395, best_f1=0.3902439024390244\n",
            "step: 0, loss: 0.16963538527488708\n",
            "epoch 14: dev_f1=0.5853658536585367, f1=0.37209302325581395, best_f1=0.3902439024390244\n",
            "step: 0, loss: 0.2338300347328186\n",
            "epoch 15: dev_f1=0.5853658536585367, f1=0.37209302325581395, best_f1=0.3902439024390244\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"matcher.py\", line 315, in <module>\n",
            "    hp.lm, hp.use_gpu, hp.fp16)\n",
            "  File \"matcher.py\", line 274, in load_model\n",
            "    raise ModelNotFoundError(checkpoint)\n",
            "ditto_light.exceptions.ModelNotFoundError: Model checkpoints/Structured/Beer/model.pt was not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178f1285-31ef-478a-800b-27e8bcaf6498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8142889142036438\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49180868268013\n",
            "step: 20, loss: 0.5944226384162903\n",
            "step: 30, loss: 0.4981488585472107\n",
            "step: 40, loss: 0.41197651624679565\n",
            "step: 50, loss: 0.29177790880203247\n",
            "step: 60, loss: 0.1782059222459793\n",
            "step: 70, loss: 0.15441790223121643\n",
            "step: 80, loss: 0.18717901408672333\n",
            "step: 90, loss: 0.013776328414678574\n",
            "step: 100, loss: 0.10326925665140152\n",
            "step: 110, loss: 0.035572364926338196\n",
            "step: 120, loss: 0.07247243821620941\n",
            "step: 130, loss: 0.018750831484794617\n",
            "step: 140, loss: 0.12470116466283798\n",
            "step: 150, loss: 0.10674717277288437\n",
            "step: 160, loss: 0.1314665675163269\n",
            "step: 170, loss: 0.005865129642188549\n",
            "step: 180, loss: 0.04757256433367729\n",
            "step: 190, loss: 0.09981340169906616\n",
            "step: 200, loss: 0.05021965503692627\n",
            "step: 210, loss: 0.01379481516778469\n",
            "step: 220, loss: 0.007516228593885899\n",
            "step: 230, loss: 0.004926974419504404\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9610678531701891, f1=0.9552572706935123, best_f1=0.9552572706935123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00555738341063261\n",
            "step: 10, loss: 0.0029913908801972866\n",
            "step: 20, loss: 0.055199433118104935\n",
            "step: 30, loss: 0.0027159410528838634\n",
            "step: 40, loss: 0.038023363798856735\n",
            "step: 50, loss: 0.0033961590379476547\n",
            "step: 60, loss: 0.07621600478887558\n",
            "step: 70, loss: 0.02917417138814926\n",
            "step: 80, loss: 0.09467590600252151\n",
            "step: 90, loss: 0.011628801003098488\n",
            "step: 100, loss: 0.023550035431981087\n",
            "step: 110, loss: 0.0682438388466835\n",
            "step: 120, loss: 0.030039725825190544\n",
            "step: 130, loss: 0.017003901302814484\n",
            "step: 140, loss: 0.012667670845985413\n",
            "step: 150, loss: 0.0049148923717439175\n",
            "step: 160, loss: 0.008284605108201504\n",
            "step: 170, loss: 0.12295383214950562\n",
            "step: 180, loss: 0.008933817967772484\n",
            "step: 190, loss: 0.06980452686548233\n",
            "step: 200, loss: 0.004022871609777212\n",
            "step: 210, loss: 0.02787526324391365\n",
            "step: 220, loss: 0.0011556862154975533\n",
            "step: 230, loss: 0.12318331003189087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9713024282560706, f1=0.968236582694414, best_f1=0.968236582694414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10003937780857086\n",
            "step: 10, loss: 0.007901323027908802\n",
            "step: 20, loss: 0.022294480353593826\n",
            "step: 30, loss: 0.016888627782464027\n",
            "step: 40, loss: 0.048129402101039886\n",
            "step: 50, loss: 0.006756053306162357\n",
            "step: 60, loss: 0.0005643513868562877\n",
            "step: 70, loss: 0.0006783962016925216\n",
            "step: 80, loss: 0.0018511834787204862\n",
            "step: 90, loss: 0.037195466458797455\n",
            "step: 100, loss: 0.0027715894393622875\n",
            "step: 110, loss: 0.03426079824566841\n",
            "step: 120, loss: 0.0026426343247294426\n",
            "step: 130, loss: 0.0008360648062080145\n",
            "step: 140, loss: 0.0032264194451272488\n",
            "step: 150, loss: 0.003088914090767503\n",
            "step: 160, loss: 0.0014462261460721493\n",
            "step: 170, loss: 0.012853261083364487\n",
            "step: 180, loss: 0.002427280182018876\n",
            "step: 190, loss: 0.013124440796673298\n",
            "step: 200, loss: 0.030656039714813232\n",
            "step: 210, loss: 0.01658695563673973\n",
            "step: 220, loss: 0.0030697593465447426\n",
            "step: 230, loss: 0.1370357871055603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9729119638826186, f1=0.9672316384180792, best_f1=0.9672316384180792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.039917293936014175\n",
            "step: 10, loss: 0.0006442363373935223\n",
            "step: 20, loss: 0.0007308863569051027\n",
            "step: 30, loss: 0.0003705599228851497\n",
            "step: 40, loss: 0.006207942962646484\n",
            "step: 50, loss: 0.009516028687357903\n",
            "step: 60, loss: 0.015055336989462376\n",
            "step: 70, loss: 0.006693728733807802\n",
            "step: 80, loss: 0.03277042508125305\n",
            "step: 90, loss: 0.0034422925673425198\n",
            "step: 100, loss: 0.0005191023228690028\n",
            "step: 110, loss: 0.022816846147179604\n",
            "step: 120, loss: 0.016175301745533943\n",
            "step: 130, loss: 0.12608769536018372\n",
            "step: 140, loss: 0.0007224288419820368\n",
            "step: 150, loss: 0.0021273281890898943\n",
            "step: 160, loss: 0.0036488487385213375\n",
            "step: 170, loss: 0.020747562870383263\n",
            "step: 180, loss: 0.07145778089761734\n",
            "step: 190, loss: 0.03493494540452957\n",
            "step: 200, loss: 0.002295959508046508\n",
            "step: 210, loss: 0.018321696668863297\n",
            "step: 220, loss: 0.0006953920819796622\n",
            "step: 230, loss: 0.0005862021935172379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9799107142857142, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007125265547074378\n",
            "step: 10, loss: 0.00038489318103529513\n",
            "step: 20, loss: 0.10332272946834564\n",
            "step: 30, loss: 0.00024281696823891252\n",
            "step: 40, loss: 0.05540768802165985\n",
            "step: 50, loss: 0.0010963438544422388\n",
            "step: 60, loss: 0.0013539429055526853\n",
            "step: 70, loss: 0.00760737806558609\n",
            "step: 80, loss: 0.0011544703738763928\n",
            "step: 90, loss: 0.011554170399904251\n",
            "step: 100, loss: 0.09446156769990921\n",
            "step: 110, loss: 0.06305515021085739\n",
            "step: 120, loss: 0.0616597943007946\n",
            "step: 130, loss: 0.11120518296957016\n",
            "step: 140, loss: 0.039398010820150375\n",
            "step: 150, loss: 0.0004869753902312368\n",
            "step: 160, loss: 0.004968450404703617\n",
            "step: 170, loss: 0.028004970401525497\n",
            "step: 180, loss: 0.0013614615891128778\n",
            "step: 190, loss: 0.0025779958814382553\n",
            "step: 200, loss: 0.01900029554963112\n",
            "step: 210, loss: 0.0004999699885956943\n",
            "step: 220, loss: 0.00026435701875016093\n",
            "step: 230, loss: 0.00030603414052166045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9733333333333333, f1=0.9656699889258028, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007906832615844905\n",
            "step: 10, loss: 0.00028997010667808354\n",
            "step: 20, loss: 0.007667347323149443\n",
            "step: 30, loss: 0.0003781469422392547\n",
            "step: 40, loss: 0.0007448080577887595\n",
            "step: 50, loss: 0.0022967832628637552\n",
            "step: 60, loss: 0.01802666485309601\n",
            "step: 70, loss: 0.020016318187117577\n",
            "step: 80, loss: 0.00023033114848658442\n",
            "step: 90, loss: 0.0004533221072051674\n",
            "step: 100, loss: 0.00047745046322233975\n",
            "step: 110, loss: 0.027483507990837097\n",
            "step: 120, loss: 0.00035151338670402765\n",
            "step: 130, loss: 0.0004898300394415855\n",
            "step: 140, loss: 0.009537864476442337\n",
            "step: 150, loss: 0.00017082845442928374\n",
            "step: 160, loss: 0.0011775599559769034\n",
            "step: 170, loss: 0.00013232413039077073\n",
            "step: 180, loss: 0.00022349249047692865\n",
            "step: 190, loss: 0.0006646704860031605\n",
            "step: 200, loss: 0.0007502930238842964\n",
            "step: 210, loss: 0.00017179589485749602\n",
            "step: 220, loss: 0.00028361883596517146\n",
            "step: 230, loss: 0.00015690621512476355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.971815107102593, f1=0.9695603156708005, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02997007593512535\n",
            "step: 10, loss: 0.0004734852409455925\n",
            "step: 20, loss: 0.0014126747846603394\n",
            "step: 30, loss: 0.00022671103943139315\n",
            "step: 40, loss: 0.01552001852542162\n",
            "step: 50, loss: 0.00011145744065288454\n",
            "step: 60, loss: 0.04879016429185867\n",
            "step: 70, loss: 0.0004865913069806993\n",
            "step: 80, loss: 0.00950525514781475\n",
            "step: 90, loss: 0.00032351480331271887\n",
            "step: 100, loss: 0.08838838338851929\n",
            "step: 110, loss: 0.001556105213239789\n",
            "step: 120, loss: 0.03574354946613312\n",
            "step: 130, loss: 0.021453583613038063\n",
            "step: 140, loss: 0.00030503360903821886\n",
            "step: 150, loss: 0.00019257277017459273\n",
            "step: 160, loss: 0.00017914340423885733\n",
            "step: 170, loss: 0.00048041949048638344\n",
            "step: 180, loss: 0.00010686242603696883\n",
            "step: 190, loss: 0.0001629893231438473\n",
            "step: 200, loss: 0.0001224736770382151\n",
            "step: 210, loss: 8.842880197335035e-05\n",
            "step: 220, loss: 0.0003298823721706867\n",
            "step: 230, loss: 0.0003951615944970399\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9752252252252253, f1=0.9753363228699552, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0054892729967832565\n",
            "step: 10, loss: 0.014757181517779827\n",
            "step: 20, loss: 0.0001506427797721699\n",
            "step: 30, loss: 0.00012217488256283104\n",
            "step: 40, loss: 0.00048663149937056005\n",
            "step: 50, loss: 0.00011830509902210906\n",
            "step: 60, loss: 0.0034436634741723537\n",
            "step: 70, loss: 0.0006301270914264023\n",
            "step: 80, loss: 0.005589242093265057\n",
            "step: 90, loss: 0.0001714328973321244\n",
            "step: 100, loss: 0.00017655998817645013\n",
            "step: 110, loss: 0.00041175153455697\n",
            "step: 120, loss: 0.00041864049853757024\n",
            "step: 130, loss: 0.014788421802222729\n",
            "step: 140, loss: 0.009452307596802711\n",
            "step: 150, loss: 0.0017736005829647183\n",
            "step: 160, loss: 0.001851081964559853\n",
            "step: 170, loss: 0.0057171983644366264\n",
            "step: 180, loss: 0.001662139780819416\n",
            "step: 190, loss: 0.00023732132103759795\n",
            "step: 200, loss: 0.0008566759061068296\n",
            "step: 210, loss: 0.00020195619435980916\n",
            "step: 220, loss: 0.0010920015629380941\n",
            "step: 230, loss: 0.01146464329212904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9788182831661093, f1=0.970917225950783, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039909259066917\n",
            "step: 10, loss: 0.03134036436676979\n",
            "step: 20, loss: 0.0014953904319554567\n",
            "step: 30, loss: 0.00035049268626607955\n",
            "step: 40, loss: 0.0002249896206194535\n",
            "step: 50, loss: 0.0003749468014575541\n",
            "step: 60, loss: 0.00013213434431236237\n",
            "step: 70, loss: 0.0006747626466676593\n",
            "step: 80, loss: 0.009571300819516182\n",
            "step: 90, loss: 0.002147089224308729\n",
            "step: 100, loss: 0.00013197505904827267\n",
            "step: 110, loss: 0.0005290087428875268\n",
            "step: 120, loss: 0.00039730226853862405\n",
            "step: 130, loss: 0.00021011418721172959\n",
            "step: 140, loss: 0.0008427641005255282\n",
            "step: 150, loss: 6.0666010540444404e-05\n",
            "step: 160, loss: 0.00031189803848974407\n",
            "step: 170, loss: 9.970007522497326e-05\n",
            "step: 180, loss: 0.0001862876961240545\n",
            "step: 190, loss: 6.054605182725936e-05\n",
            "step: 200, loss: 0.0029794303700327873\n",
            "step: 210, loss: 0.0011961165582761168\n",
            "step: 220, loss: 0.03131268918514252\n",
            "step: 230, loss: 0.008282411843538284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9727891156462585, f1=0.9708520179372198, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03950995206832886\n",
            "step: 10, loss: 6.949830276425928e-05\n",
            "step: 20, loss: 9.401289571542293e-05\n",
            "step: 30, loss: 0.0010000684997066855\n",
            "step: 40, loss: 5.4400559747591615e-05\n",
            "step: 50, loss: 0.00027439510449767113\n",
            "step: 60, loss: 0.0004892419092357159\n",
            "step: 70, loss: 9.832423529587686e-05\n",
            "step: 80, loss: 0.00014507408195640892\n",
            "step: 90, loss: 8.630954835098237e-05\n",
            "step: 100, loss: 9.254073665942997e-05\n",
            "step: 110, loss: 0.00012388468894641846\n",
            "step: 120, loss: 0.08294837921857834\n",
            "step: 130, loss: 0.00011916028597624972\n",
            "step: 140, loss: 0.006121920421719551\n",
            "step: 150, loss: 8.907353185350075e-05\n",
            "step: 160, loss: 0.005939075723290443\n",
            "step: 170, loss: 0.00029750948306173086\n",
            "step: 180, loss: 0.0015653319424018264\n",
            "step: 190, loss: 0.0003132275305688381\n",
            "step: 200, loss: 8.832794992486015e-05\n",
            "step: 210, loss: 0.0001327931968262419\n",
            "step: 220, loss: 0.0009102686308324337\n",
            "step: 230, loss: 0.00017794428276829422\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9740112994350283, f1=0.968609865470852, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00058766856091097\n",
            "step: 10, loss: 7.160293171182275e-05\n",
            "step: 20, loss: 5.9995461924700066e-05\n",
            "step: 30, loss: 0.0003603781806305051\n",
            "step: 40, loss: 0.0025560881476849318\n",
            "step: 50, loss: 0.003230989445000887\n",
            "step: 60, loss: 0.00022286377497948706\n",
            "step: 70, loss: 0.0012856151442974806\n",
            "step: 80, loss: 0.005226234905421734\n",
            "step: 90, loss: 9.269416477764025e-05\n",
            "step: 100, loss: 0.00020424378453753889\n",
            "step: 110, loss: 0.00032692719832994044\n",
            "step: 120, loss: 0.00016894318105187267\n",
            "step: 130, loss: 7.632874621776864e-05\n",
            "step: 140, loss: 0.0019005637150257826\n",
            "step: 150, loss: 5.9674021031241864e-05\n",
            "step: 160, loss: 8.846252603689209e-05\n",
            "step: 170, loss: 0.015494785271584988\n",
            "step: 180, loss: 0.00029996619559824467\n",
            "step: 190, loss: 0.00012970523675903678\n",
            "step: 200, loss: 0.00014406515401788056\n",
            "step: 210, loss: 7.183915295172483e-05\n",
            "step: 220, loss: 0.0004304400354158133\n",
            "step: 230, loss: 0.0077626388520002365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.976324689966178, f1=0.9696287964004499, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040585504029877484\n",
            "step: 10, loss: 0.0007573022157885134\n",
            "step: 20, loss: 0.00011540721607161686\n",
            "step: 30, loss: 0.0010408023372292519\n",
            "step: 40, loss: 0.00015952176181599498\n",
            "step: 50, loss: 0.00043933052802458405\n",
            "step: 60, loss: 0.001164728426374495\n",
            "step: 70, loss: 0.00027078803395852447\n",
            "step: 80, loss: 5.636879723169841e-05\n",
            "step: 90, loss: 4.864498259848915e-05\n",
            "step: 100, loss: 5.245423017186113e-05\n",
            "step: 110, loss: 8.680626342538744e-05\n",
            "step: 120, loss: 0.00013195117935538292\n",
            "step: 130, loss: 8.448510925518349e-05\n",
            "step: 140, loss: 4.841848567593843e-05\n",
            "step: 150, loss: 6.318598752841353e-05\n",
            "step: 160, loss: 0.0002523886214476079\n",
            "step: 170, loss: 0.00012506931670941412\n",
            "step: 180, loss: 0.0001418698375346139\n",
            "step: 190, loss: 6.34747848380357e-05\n",
            "step: 200, loss: 0.00042202157783322036\n",
            "step: 210, loss: 0.0016459141625091434\n",
            "step: 220, loss: 0.00026536730001680553\n",
            "step: 230, loss: 0.0001940123038366437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9751131221719457, f1=0.9685393258426966, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011966582387685776\n",
            "step: 10, loss: 0.00017168701742775738\n",
            "step: 20, loss: 9.026558109326288e-05\n",
            "step: 30, loss: 0.002538295928388834\n",
            "step: 40, loss: 0.008202294819056988\n",
            "step: 50, loss: 7.334292604355142e-05\n",
            "step: 60, loss: 0.00041673253872431815\n",
            "step: 70, loss: 6.710334128001705e-05\n",
            "step: 80, loss: 5.88548973610159e-05\n",
            "step: 90, loss: 4.722612356999889e-05\n",
            "step: 100, loss: 0.00011495008948259056\n",
            "step: 110, loss: 9.696668712422252e-05\n",
            "step: 120, loss: 0.0002285892842337489\n",
            "step: 130, loss: 7.618977542733774e-05\n",
            "step: 140, loss: 7.743102469248697e-05\n",
            "step: 150, loss: 0.0009457563864998519\n",
            "step: 160, loss: 5.6175762438215315e-05\n",
            "step: 170, loss: 0.00012348213931545615\n",
            "step: 180, loss: 0.00013321386359166354\n",
            "step: 190, loss: 5.534174124477431e-05\n",
            "step: 200, loss: 4.223092400934547e-05\n",
            "step: 210, loss: 7.505629037041217e-05\n",
            "step: 220, loss: 0.00013852967822458595\n",
            "step: 230, loss: 5.228376539889723e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9773242630385486, f1=0.9751131221719457, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018999945314135402\n",
            "step: 10, loss: 4.830619946005754e-05\n",
            "step: 20, loss: 0.0007540213991887867\n",
            "step: 30, loss: 0.0002239453897345811\n",
            "step: 40, loss: 3.789096081163734e-05\n",
            "step: 50, loss: 0.00022980596986599267\n",
            "step: 60, loss: 0.00012265128316357732\n",
            "step: 70, loss: 6.18315243627876e-05\n",
            "step: 80, loss: 9.989891259465367e-05\n",
            "step: 90, loss: 0.00017867414862848818\n",
            "step: 100, loss: 0.00413236441090703\n",
            "step: 110, loss: 0.00012041905574733391\n",
            "step: 120, loss: 6.616466998821124e-05\n",
            "step: 130, loss: 9.155623411061242e-05\n",
            "step: 140, loss: 8.296371379401535e-05\n",
            "step: 150, loss: 8.006940333871171e-05\n",
            "step: 160, loss: 2.8627757274080068e-05\n",
            "step: 170, loss: 5.7847180869430304e-05\n",
            "step: 180, loss: 8.542197610950097e-05\n",
            "step: 190, loss: 0.0002279606560477987\n",
            "step: 200, loss: 0.06457488983869553\n",
            "step: 210, loss: 6.826782191637903e-05\n",
            "step: 220, loss: 8.848823927110061e-05\n",
            "step: 230, loss: 2.211297396570444e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9762174405436014, f1=0.9728506787330317, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00031843080068938434\n",
            "step: 10, loss: 0.0002680267207324505\n",
            "step: 20, loss: 8.867026190273464e-05\n",
            "step: 30, loss: 6.143259815871716e-05\n",
            "step: 40, loss: 9.618565673008561e-05\n",
            "step: 50, loss: 6.843972369097173e-05\n",
            "step: 60, loss: 7.479610212612897e-05\n",
            "step: 70, loss: 4.672394788940437e-05\n",
            "step: 80, loss: 0.00010064426169265062\n",
            "step: 90, loss: 5.1069637265754864e-05\n",
            "step: 100, loss: 0.00012344463902991265\n",
            "step: 110, loss: 7.492150325560942e-05\n",
            "step: 120, loss: 0.00019155156041961163\n",
            "step: 130, loss: 8.963829895947129e-05\n",
            "step: 140, loss: 0.0001723897730698809\n",
            "step: 150, loss: 0.00010896188905462623\n",
            "step: 160, loss: 5.4331685532815754e-05\n",
            "step: 170, loss: 3.326169098727405e-05\n",
            "step: 180, loss: 8.266133954748511e-05\n",
            "step: 190, loss: 0.00025107571855187416\n",
            "step: 200, loss: 6.323319394141436e-05\n",
            "step: 210, loss: 0.042441561818122864\n",
            "step: 220, loss: 4.266308678779751e-05\n",
            "step: 230, loss: 5.8770045143319294e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9774266365688488, f1=0.9729119638826186, best_f1=0.9709821428571428\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:10, 229.39it/s]\n",
            "load_f1 = 0.9765886287625419\n",
            "real_f1 = 0.9776286353467561\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 250.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1abe6b7-c609-4338-db09-282147122f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 442/442 [00:00<00:00, 398kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 335kB/s] \n",
            "Downloading: 100% 268M/268M [00:04<00:00, 53.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7972330451011658\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4509055018424988\n",
            "step: 20, loss: 0.5209246873855591\n",
            "step: 30, loss: 0.3583526909351349\n",
            "step: 40, loss: 0.4215586185455322\n",
            "step: 50, loss: 0.2178347259759903\n",
            "step: 60, loss: 0.5026189088821411\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.29180771112442017\n",
            "step: 80, loss: 0.1716846525669098\n",
            "step: 90, loss: 0.21593064069747925\n",
            "step: 100, loss: 0.265526682138443\n",
            "step: 110, loss: 0.05951515585184097\n",
            "step: 120, loss: 0.12155251950025558\n",
            "step: 130, loss: 0.04002678766846657\n",
            "step: 140, loss: 0.2862164378166199\n",
            "step: 150, loss: 0.03858337923884392\n",
            "step: 160, loss: 0.17584338784217834\n",
            "step: 170, loss: 0.36656996607780457\n",
            "step: 180, loss: 0.2145356833934784\n",
            "step: 190, loss: 0.08515206724405289\n",
            "step: 200, loss: 0.1082833781838417\n",
            "step: 210, loss: 0.04875454679131508\n",
            "step: 220, loss: 0.1137450560927391\n",
            "step: 230, loss: 0.1963750123977661\n",
            "step: 240, loss: 0.11365848779678345\n",
            "step: 250, loss: 0.12245722115039825\n",
            "step: 260, loss: 0.043314557522535324\n",
            "step: 270, loss: 0.10093223303556442\n",
            "step: 280, loss: 0.13918781280517578\n",
            "step: 290, loss: 0.057606346905231476\n",
            "step: 300, loss: 0.08241718262434006\n",
            "step: 310, loss: 0.14894826710224152\n",
            "step: 320, loss: 0.1522538661956787\n",
            "step: 330, loss: 0.130212664604187\n",
            "step: 340, loss: 0.21439777314662933\n",
            "step: 350, loss: 0.04707837477326393\n",
            "step: 360, loss: 0.12151370942592621\n",
            "step: 370, loss: 0.16522778570652008\n",
            "step: 380, loss: 0.1154554933309555\n",
            "step: 390, loss: 0.23881910741329193\n",
            "step: 400, loss: 0.024518698453903198\n",
            "step: 410, loss: 0.157953143119812\n",
            "step: 420, loss: 0.03470059111714363\n",
            "step: 430, loss: 0.15410387516021729\n",
            "step: 440, loss: 0.3413172662258148\n",
            "step: 450, loss: 0.03450804576277733\n",
            "step: 460, loss: 0.08836077898740768\n",
            "step: 470, loss: 0.2619362771511078\n",
            "step: 480, loss: 0.2443220317363739\n",
            "step: 490, loss: 0.1972866952419281\n",
            "step: 500, loss: 0.01764172501862049\n",
            "step: 510, loss: 0.13037003576755524\n",
            "step: 520, loss: 0.12284542620182037\n",
            "step: 530, loss: 0.2106618881225586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8927078495123083, f1=0.8921658986175115, best_f1=0.8921658986175115\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19091400504112244\n",
            "step: 10, loss: 0.15337303280830383\n",
            "step: 20, loss: 0.14216731488704681\n",
            "step: 30, loss: 0.0713086724281311\n",
            "step: 40, loss: 0.026237810030579567\n",
            "step: 50, loss: 0.06962107867002487\n",
            "step: 60, loss: 0.27664002776145935\n",
            "step: 70, loss: 0.2518520951271057\n",
            "step: 80, loss: 0.04575943574309349\n",
            "step: 90, loss: 0.07681728154420853\n",
            "step: 100, loss: 0.33020976185798645\n",
            "step: 110, loss: 0.11175709217786789\n",
            "step: 120, loss: 0.147967129945755\n",
            "step: 130, loss: 0.23040643334388733\n",
            "step: 140, loss: 0.06516799330711365\n",
            "step: 150, loss: 0.024121925234794617\n",
            "step: 160, loss: 0.15453599393367767\n",
            "step: 170, loss: 0.06803379207849503\n",
            "step: 180, loss: 0.0208150465041399\n",
            "step: 190, loss: 0.06205582618713379\n",
            "step: 200, loss: 0.11239083856344223\n",
            "step: 210, loss: 0.09324543178081512\n",
            "step: 220, loss: 0.1311739683151245\n",
            "step: 230, loss: 0.0695868656039238\n",
            "step: 240, loss: 0.10863907635211945\n",
            "step: 250, loss: 0.07815735787153244\n",
            "step: 260, loss: 0.055469244718551636\n",
            "step: 270, loss: 0.06026845797896385\n",
            "step: 280, loss: 0.09962661564350128\n",
            "step: 290, loss: 0.06499917805194855\n",
            "step: 300, loss: 0.05474173277616501\n",
            "step: 310, loss: 0.08669169247150421\n",
            "step: 320, loss: 0.153006449341774\n",
            "step: 330, loss: 0.06468402594327927\n",
            "step: 340, loss: 0.0649450421333313\n",
            "step: 350, loss: 0.07362443953752518\n",
            "step: 360, loss: 0.2005264014005661\n",
            "step: 370, loss: 0.05847134068608284\n",
            "step: 380, loss: 0.07581474632024765\n",
            "step: 390, loss: 0.0588541105389595\n",
            "step: 400, loss: 0.11096438765525818\n",
            "step: 410, loss: 0.022338995710015297\n",
            "step: 420, loss: 0.0530117005109787\n",
            "step: 430, loss: 0.06342471390962601\n",
            "step: 440, loss: 0.01620299555361271\n",
            "step: 450, loss: 0.1415419727563858\n",
            "step: 460, loss: 0.2358543425798416\n",
            "step: 470, loss: 0.09415559470653534\n",
            "step: 480, loss: 0.27965155243873596\n",
            "step: 490, loss: 0.11158712208271027\n",
            "step: 500, loss: 0.044513896107673645\n",
            "step: 510, loss: 0.1399642527103424\n",
            "step: 520, loss: 0.05614670366048813\n",
            "step: 530, loss: 0.2574693262577057\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9075011504832029, f1=0.9024278515803941, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08324822783470154\n",
            "step: 10, loss: 0.1464596837759018\n",
            "step: 20, loss: 0.06967541575431824\n",
            "step: 30, loss: 0.16113217175006866\n",
            "step: 40, loss: 0.012102157808840275\n",
            "step: 50, loss: 0.03615861386060715\n",
            "step: 60, loss: 0.010997907258570194\n",
            "step: 70, loss: 0.025018993765115738\n",
            "step: 80, loss: 0.07728356122970581\n",
            "step: 90, loss: 0.20802423357963562\n",
            "step: 100, loss: 0.013189252465963364\n",
            "step: 110, loss: 0.04238142445683479\n",
            "step: 120, loss: 0.12466748058795929\n",
            "step: 130, loss: 0.015552029013633728\n",
            "step: 140, loss: 0.034241124987602234\n",
            "step: 150, loss: 0.03155384212732315\n",
            "step: 160, loss: 0.02117367461323738\n",
            "step: 170, loss: 0.029078206047415733\n",
            "step: 180, loss: 0.07371551543474197\n",
            "step: 190, loss: 0.03505278378725052\n",
            "step: 200, loss: 0.09377507865428925\n",
            "step: 210, loss: 0.041702110320329666\n",
            "step: 220, loss: 0.04981641471385956\n",
            "step: 230, loss: 0.04625977948307991\n",
            "step: 240, loss: 0.011381284333765507\n",
            "step: 250, loss: 0.040541764348745346\n",
            "step: 260, loss: 0.005174242425709963\n",
            "step: 270, loss: 0.005971590522676706\n",
            "step: 280, loss: 0.04196004197001457\n",
            "step: 290, loss: 0.09920982271432877\n",
            "step: 300, loss: 0.11706849932670593\n",
            "step: 310, loss: 0.12434443831443787\n",
            "step: 320, loss: 0.03466993570327759\n",
            "step: 330, loss: 0.011146286502480507\n",
            "step: 340, loss: 0.035296447575092316\n",
            "step: 350, loss: 0.053615324199199677\n",
            "step: 360, loss: 0.011292262002825737\n",
            "step: 370, loss: 0.010468779131770134\n",
            "step: 380, loss: 0.026960868388414383\n",
            "step: 390, loss: 0.10463028401136398\n",
            "step: 400, loss: 0.0675613284111023\n",
            "step: 410, loss: 0.16702453792095184\n",
            "step: 420, loss: 0.11409929394721985\n",
            "step: 430, loss: 0.024519864469766617\n",
            "step: 440, loss: 0.09460975974798203\n",
            "step: 450, loss: 0.1778477281332016\n",
            "step: 460, loss: 0.31365901231765747\n",
            "step: 470, loss: 0.03518729656934738\n",
            "step: 480, loss: 0.05305511876940727\n",
            "step: 490, loss: 0.06137982755899429\n",
            "step: 500, loss: 0.09720521420240402\n",
            "step: 510, loss: 0.07938829809427261\n",
            "step: 520, loss: 0.015086636878550053\n",
            "step: 530, loss: 0.00918387621641159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9047619047619048, f1=0.8993476234855545, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0064465999603271484\n",
            "step: 10, loss: 0.008888421580195427\n",
            "step: 20, loss: 0.03140440955758095\n",
            "step: 30, loss: 0.04197661206126213\n",
            "step: 40, loss: 0.009947292506694794\n",
            "step: 50, loss: 0.02573617734014988\n",
            "step: 60, loss: 0.0014169467613101006\n",
            "step: 70, loss: 0.027948234230279922\n",
            "step: 80, loss: 0.014770531095564365\n",
            "step: 90, loss: 0.09384023398160934\n",
            "step: 100, loss: 0.09134475141763687\n",
            "step: 110, loss: 0.018896937370300293\n",
            "step: 120, loss: 0.003013978712260723\n",
            "step: 130, loss: 0.00706806406378746\n",
            "step: 140, loss: 0.010748909786343575\n",
            "step: 150, loss: 0.09958356618881226\n",
            "step: 160, loss: 0.017171721905469894\n",
            "step: 170, loss: 0.0471026636660099\n",
            "step: 180, loss: 0.02335173450410366\n",
            "step: 190, loss: 0.013233669102191925\n",
            "step: 200, loss: 0.01628018356859684\n",
            "step: 210, loss: 0.17691904306411743\n",
            "step: 220, loss: 0.01093947235494852\n",
            "step: 230, loss: 0.2855607271194458\n",
            "step: 240, loss: 0.007668275386095047\n",
            "step: 250, loss: 0.0044075255282223225\n",
            "step: 260, loss: 0.10717009752988815\n",
            "step: 270, loss: 0.1271342635154724\n",
            "step: 280, loss: 0.004984704311937094\n",
            "step: 290, loss: 0.012570780701935291\n",
            "step: 300, loss: 0.027631357312202454\n",
            "step: 310, loss: 0.011842364445328712\n",
            "step: 320, loss: 0.07040082663297653\n",
            "step: 330, loss: 0.12961870431900024\n",
            "step: 340, loss: 0.08488541841506958\n",
            "step: 350, loss: 0.008465554565191269\n",
            "step: 360, loss: 0.05080005154013634\n",
            "step: 370, loss: 0.0402839258313179\n",
            "step: 380, loss: 0.008187676779925823\n",
            "step: 390, loss: 0.047734469175338745\n",
            "step: 400, loss: 0.04015196114778519\n",
            "step: 410, loss: 0.1401553452014923\n",
            "step: 420, loss: 0.03009692393243313\n",
            "step: 430, loss: 0.01179924700409174\n",
            "step: 440, loss: 0.10535946488380432\n",
            "step: 450, loss: 0.08533062040805817\n",
            "step: 460, loss: 0.001170016941614449\n",
            "step: 470, loss: 0.012154452502727509\n",
            "step: 480, loss: 0.01139246765524149\n",
            "step: 490, loss: 0.0634010061621666\n",
            "step: 500, loss: 0.014160208404064178\n",
            "step: 510, loss: 0.12132467329502106\n",
            "step: 520, loss: 0.1336139738559723\n",
            "step: 530, loss: 0.004964292980730534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.90625, f1=0.9032562529495045, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.046504661440849304\n",
            "step: 10, loss: 0.04874103143811226\n",
            "step: 20, loss: 0.18091417849063873\n",
            "step: 30, loss: 0.039404451847076416\n",
            "step: 40, loss: 0.06302233040332794\n",
            "step: 50, loss: 0.003557836404070258\n",
            "step: 60, loss: 0.005786739755421877\n",
            "step: 70, loss: 0.002111660083755851\n",
            "step: 80, loss: 0.0007198671228252351\n",
            "step: 90, loss: 0.002726446371525526\n",
            "step: 100, loss: 0.0038691298104822636\n",
            "step: 110, loss: 0.0044686757028102875\n",
            "step: 120, loss: 0.00828742329031229\n",
            "step: 130, loss: 0.003885135753080249\n",
            "step: 140, loss: 0.006907292176038027\n",
            "step: 150, loss: 0.05477523431181908\n",
            "step: 160, loss: 0.023478079587221146\n",
            "step: 170, loss: 0.011349444277584553\n",
            "step: 180, loss: 0.0013559970539063215\n",
            "step: 190, loss: 0.002857985207810998\n",
            "step: 200, loss: 0.05478991940617561\n",
            "step: 210, loss: 0.04218299314379692\n",
            "step: 220, loss: 0.0008949812618084252\n",
            "step: 230, loss: 0.0023301616311073303\n",
            "step: 240, loss: 0.031006738543510437\n",
            "step: 250, loss: 0.0010051662102341652\n",
            "step: 260, loss: 0.11639659106731415\n",
            "step: 270, loss: 0.17777664959430695\n",
            "step: 280, loss: 0.047467589378356934\n",
            "step: 290, loss: 0.03595574572682381\n",
            "step: 300, loss: 0.06544093042612076\n",
            "step: 310, loss: 0.013608930632472038\n",
            "step: 320, loss: 0.04850758984684944\n",
            "step: 330, loss: 0.0071489629335701466\n",
            "step: 340, loss: 0.008325188420712948\n",
            "step: 350, loss: 0.019342826679348946\n",
            "step: 360, loss: 0.03391164168715477\n",
            "step: 370, loss: 0.010790492407977581\n",
            "step: 380, loss: 0.05897977203130722\n",
            "step: 390, loss: 0.006635407917201519\n",
            "step: 400, loss: 0.04250386357307434\n",
            "step: 410, loss: 0.011027957312762737\n",
            "step: 420, loss: 0.0024728854186832905\n",
            "step: 430, loss: 0.007059520110487938\n",
            "step: 440, loss: 0.004716601222753525\n",
            "step: 450, loss: 0.017784005030989647\n",
            "step: 460, loss: 0.002381077501922846\n",
            "step: 470, loss: 0.03749258071184158\n",
            "step: 480, loss: 0.014416718855500221\n",
            "step: 490, loss: 0.0129961296916008\n",
            "step: 500, loss: 0.050927598029375076\n",
            "step: 510, loss: 0.07143392413854599\n",
            "step: 520, loss: 0.008467739447951317\n",
            "step: 530, loss: 0.02069026604294777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9016853932584271, f1=0.9022695692450209, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0168235432356596\n",
            "step: 10, loss: 0.001130398246459663\n",
            "step: 20, loss: 0.00276320893317461\n",
            "step: 30, loss: 0.001820629695430398\n",
            "step: 40, loss: 0.03927288204431534\n",
            "step: 50, loss: 0.003742064582183957\n",
            "step: 60, loss: 0.0032326197251677513\n",
            "step: 70, loss: 0.06225212663412094\n",
            "step: 80, loss: 0.0009310888708569109\n",
            "step: 90, loss: 0.004502010531723499\n",
            "step: 100, loss: 0.152687668800354\n",
            "step: 110, loss: 0.041365355253219604\n",
            "step: 120, loss: 0.014398454688489437\n",
            "step: 130, loss: 0.049829576164484024\n",
            "step: 140, loss: 0.0049765147268772125\n",
            "step: 150, loss: 0.155941441655159\n",
            "step: 160, loss: 0.0006902926252223551\n",
            "step: 170, loss: 0.002332025207579136\n",
            "step: 180, loss: 0.0037553380243480206\n",
            "step: 190, loss: 0.011430523358285427\n",
            "step: 200, loss: 0.0009303694241680205\n",
            "step: 210, loss: 0.0006819806876592338\n",
            "step: 220, loss: 0.0004009441181551665\n",
            "step: 230, loss: 0.00024074710381682962\n",
            "step: 240, loss: 0.013756124302744865\n",
            "step: 250, loss: 0.0008452875190414488\n",
            "step: 260, loss: 0.001350217149592936\n",
            "step: 270, loss: 0.02362663671374321\n",
            "step: 280, loss: 0.04301690682768822\n",
            "step: 290, loss: 0.13955339789390564\n",
            "step: 300, loss: 0.0013697873800992966\n",
            "step: 310, loss: 0.13164499402046204\n",
            "step: 320, loss: 0.2373807430267334\n",
            "step: 330, loss: 0.03983522579073906\n",
            "step: 340, loss: 0.05220731720328331\n",
            "step: 350, loss: 0.18351693451404572\n",
            "step: 360, loss: 0.012327308766543865\n",
            "step: 370, loss: 0.013137539848685265\n",
            "step: 380, loss: 0.005391756072640419\n",
            "step: 390, loss: 0.05720507353544235\n",
            "step: 400, loss: 0.000741739699151367\n",
            "step: 410, loss: 0.003923739306628704\n",
            "step: 420, loss: 0.1424456536769867\n",
            "step: 430, loss: 0.03387947380542755\n",
            "step: 440, loss: 0.04887174442410469\n",
            "step: 450, loss: 0.004868742544203997\n",
            "step: 460, loss: 0.18755395710468292\n",
            "step: 470, loss: 0.03384498134255409\n",
            "step: 480, loss: 0.004401968326419592\n",
            "step: 490, loss: 0.0038866091053932905\n",
            "step: 500, loss: 0.0035432344302535057\n",
            "step: 510, loss: 0.0037005585618317127\n",
            "step: 520, loss: 0.01767270267009735\n",
            "step: 530, loss: 0.008196122944355011\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8965839962564344, f1=0.8930581613508444, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05818792060017586\n",
            "step: 10, loss: 0.0067728739231824875\n",
            "step: 20, loss: 0.001145325368270278\n",
            "step: 30, loss: 0.16449011862277985\n",
            "step: 40, loss: 0.0033482760190963745\n",
            "step: 50, loss: 0.05491955950856209\n",
            "step: 60, loss: 0.060293812304735184\n",
            "step: 70, loss: 0.029794897884130478\n",
            "step: 80, loss: 0.0009324475540779531\n",
            "step: 90, loss: 0.01515840645879507\n",
            "step: 100, loss: 0.021910659968852997\n",
            "step: 110, loss: 0.0161609947681427\n",
            "step: 120, loss: 0.0006409492343664169\n",
            "step: 130, loss: 0.002629793481901288\n",
            "step: 140, loss: 0.001489768153987825\n",
            "step: 150, loss: 0.016152530908584595\n",
            "step: 160, loss: 0.003234212752431631\n",
            "step: 170, loss: 0.022240329533815384\n",
            "step: 180, loss: 0.0007824453641660511\n",
            "step: 190, loss: 0.0039294445887207985\n",
            "step: 200, loss: 0.023793358355760574\n",
            "step: 210, loss: 0.0021905461326241493\n",
            "step: 220, loss: 0.0004272395162843168\n",
            "step: 230, loss: 0.0014704412315040827\n",
            "step: 240, loss: 0.004464120138436556\n",
            "step: 250, loss: 0.02045774832367897\n",
            "step: 260, loss: 0.00295657548122108\n",
            "step: 270, loss: 0.00042493545333854854\n",
            "step: 280, loss: 0.0047918627969920635\n",
            "step: 290, loss: 0.0009183658985421062\n",
            "step: 300, loss: 0.0013286909088492393\n",
            "step: 310, loss: 0.030605768784880638\n",
            "step: 320, loss: 0.026523951441049576\n",
            "step: 330, loss: 0.025993570685386658\n",
            "step: 340, loss: 0.016605094075202942\n",
            "step: 350, loss: 0.03420144319534302\n",
            "step: 360, loss: 0.030508195981383324\n",
            "step: 370, loss: 0.0009181287023238838\n",
            "step: 380, loss: 0.008777698501944542\n",
            "step: 390, loss: 0.0007157435175031424\n",
            "step: 400, loss: 0.001831302186474204\n",
            "step: 410, loss: 0.000981678836978972\n",
            "step: 420, loss: 0.0014307233504951\n",
            "step: 430, loss: 0.005932639352977276\n",
            "step: 440, loss: 0.0007798600709065795\n",
            "step: 450, loss: 0.0005236382130533457\n",
            "step: 460, loss: 0.0018915541004389524\n",
            "step: 470, loss: 0.07400970906019211\n",
            "step: 480, loss: 0.0015230578137561679\n",
            "step: 490, loss: 0.03080444596707821\n",
            "step: 500, loss: 0.00029781064949929714\n",
            "step: 510, loss: 0.0016846958314999938\n",
            "step: 520, loss: 0.07203815132379532\n",
            "step: 530, loss: 0.0028157914057374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8944866920152091, f1=0.892018779342723, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014443150721490383\n",
            "step: 10, loss: 0.03480453044176102\n",
            "step: 20, loss: 0.008545254357159138\n",
            "step: 30, loss: 0.0015345766441896558\n",
            "step: 40, loss: 0.002126599196344614\n",
            "step: 50, loss: 0.0007447025272995234\n",
            "step: 60, loss: 0.0019957188051193953\n",
            "step: 70, loss: 0.0011813411256298423\n",
            "step: 80, loss: 0.00015846274618525058\n",
            "step: 90, loss: 0.0013563253451138735\n",
            "step: 100, loss: 0.0024449494667351246\n",
            "step: 110, loss: 0.0015349754830822349\n",
            "step: 120, loss: 0.010929951444268227\n",
            "step: 130, loss: 0.00031486086663790047\n",
            "step: 140, loss: 0.00015251318109221756\n",
            "step: 150, loss: 0.0010061472421512008\n",
            "step: 160, loss: 0.0001758740982040763\n",
            "step: 170, loss: 0.00022912138956598938\n",
            "step: 180, loss: 0.002866990165784955\n",
            "step: 190, loss: 0.003967075143009424\n",
            "step: 200, loss: 0.0004000603221356869\n",
            "step: 210, loss: 0.10978227108716965\n",
            "step: 220, loss: 0.16444151103496552\n",
            "step: 230, loss: 0.00046320160618051887\n",
            "step: 240, loss: 0.02403719536960125\n",
            "step: 250, loss: 0.0003465019981376827\n",
            "step: 260, loss: 0.0127321882173419\n",
            "step: 270, loss: 0.0002557092811912298\n",
            "step: 280, loss: 0.010750056244432926\n",
            "step: 290, loss: 0.002118577715009451\n",
            "step: 300, loss: 0.00021593578276224434\n",
            "step: 310, loss: 0.0339195691049099\n",
            "step: 320, loss: 0.00021848478354513645\n",
            "step: 330, loss: 0.03319605439901352\n",
            "step: 340, loss: 0.0006553017301484942\n",
            "step: 350, loss: 0.0012728434521704912\n",
            "step: 360, loss: 0.0003364357980899513\n",
            "step: 370, loss: 0.0007357750437222421\n",
            "step: 380, loss: 0.0008805306861177087\n",
            "step: 390, loss: 0.00020213500829413533\n",
            "step: 400, loss: 0.012506525963544846\n",
            "step: 410, loss: 0.00022427826479543\n",
            "step: 420, loss: 0.00033036895911209285\n",
            "step: 430, loss: 0.0038676862604916096\n",
            "step: 440, loss: 0.00016820601013023406\n",
            "step: 450, loss: 0.004832401871681213\n",
            "step: 460, loss: 0.004702270496636629\n",
            "step: 470, loss: 0.0009455378749407828\n",
            "step: 480, loss: 0.008849005214869976\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 490, loss: 0.008762999437749386\n",
            "step: 500, loss: 0.0982901006937027\n",
            "step: 510, loss: 0.00933454092592001\n",
            "step: 520, loss: 0.03845953941345215\n",
            "step: 530, loss: 0.0003758287930395454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8980156898938624, f1=0.9021043000914912, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002479234477505088\n",
            "step: 10, loss: 0.00026334499125368893\n",
            "step: 20, loss: 0.0009884879691526294\n",
            "step: 30, loss: 0.0006957259611226618\n",
            "step: 40, loss: 0.03308071941137314\n",
            "step: 50, loss: 0.0013312960509210825\n",
            "step: 60, loss: 0.0007207849994301796\n",
            "step: 70, loss: 0.07521205395460129\n",
            "step: 80, loss: 0.00013142221723683178\n",
            "step: 90, loss: 0.0007901063654571772\n",
            "step: 100, loss: 0.002476635854691267\n",
            "step: 110, loss: 0.016840225085616112\n",
            "step: 120, loss: 0.010282684117555618\n",
            "step: 130, loss: 0.0006362953572534025\n",
            "step: 140, loss: 0.0017567151226103306\n",
            "step: 150, loss: 9.657489863457158e-05\n",
            "step: 160, loss: 0.0002503430296201259\n",
            "step: 170, loss: 0.00027715499163605273\n",
            "step: 180, loss: 0.0015666650142520666\n",
            "step: 190, loss: 0.00021685188403353095\n",
            "step: 200, loss: 0.002851970959454775\n",
            "step: 210, loss: 0.00029411280411295593\n",
            "step: 220, loss: 0.0003996852319687605\n",
            "step: 230, loss: 0.00022646604338660836\n",
            "step: 240, loss: 0.0007052707951515913\n",
            "step: 250, loss: 0.000277684215689078\n",
            "step: 260, loss: 0.0007287433254532516\n",
            "step: 270, loss: 0.0023264228366315365\n",
            "step: 280, loss: 0.00039995406405068934\n",
            "step: 290, loss: 0.0001313464599661529\n",
            "step: 300, loss: 0.0014872155152261257\n",
            "step: 310, loss: 0.0018397885141894221\n",
            "step: 320, loss: 0.0007089424761943519\n",
            "step: 330, loss: 0.0003161795320920646\n",
            "step: 340, loss: 0.0003209565475117415\n",
            "step: 350, loss: 0.000876928330399096\n",
            "step: 360, loss: 0.023944811895489693\n",
            "step: 370, loss: 0.002798652509227395\n",
            "step: 380, loss: 0.0033345057163387537\n",
            "step: 390, loss: 9.439962013857439e-05\n",
            "step: 400, loss: 0.015366421081125736\n",
            "step: 410, loss: 0.0005278222379274666\n",
            "step: 420, loss: 0.0038459207862615585\n",
            "step: 430, loss: 0.0002538116823416203\n",
            "step: 440, loss: 0.02522345259785652\n",
            "step: 450, loss: 0.0006146744708530605\n",
            "step: 460, loss: 0.0024593474809080362\n",
            "step: 470, loss: 0.0009777734521776438\n",
            "step: 480, loss: 0.0010794757399708033\n",
            "step: 490, loss: 0.0004258249537087977\n",
            "step: 500, loss: 0.0015527355717495084\n",
            "step: 510, loss: 0.0013758614659309387\n",
            "step: 520, loss: 0.0004893289878964424\n",
            "step: 530, loss: 0.00012623623479157686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8986742424242423, f1=0.8925541941564562, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028213008772581816\n",
            "step: 10, loss: 0.0026698512956500053\n",
            "step: 20, loss: 0.00010752307571237907\n",
            "step: 30, loss: 0.00018058765272144228\n",
            "step: 40, loss: 0.0008703155326656997\n",
            "step: 50, loss: 0.00010895443119807169\n",
            "step: 60, loss: 0.00046292212209664285\n",
            "step: 70, loss: 0.0023441577795892954\n",
            "step: 80, loss: 0.0032039149664342403\n",
            "step: 90, loss: 0.00014104608271736652\n",
            "step: 100, loss: 0.0008585829054936767\n",
            "step: 110, loss: 0.0014072894118726254\n",
            "step: 120, loss: 0.0016104384558275342\n",
            "step: 130, loss: 0.016337135806679726\n",
            "step: 140, loss: 0.0002166620979551226\n",
            "step: 150, loss: 0.0070908102206885815\n",
            "step: 160, loss: 0.0001935803738888353\n",
            "step: 170, loss: 0.0009145893855020404\n",
            "step: 180, loss: 0.023910900577902794\n",
            "step: 190, loss: 0.0014169187052175403\n",
            "step: 200, loss: 0.10000701993703842\n",
            "step: 210, loss: 0.00022000166063662618\n",
            "step: 220, loss: 0.00031259364914149046\n",
            "step: 230, loss: 0.00011607375927269459\n",
            "step: 240, loss: 0.0001956684427568689\n",
            "step: 250, loss: 0.0010012281127274036\n",
            "step: 260, loss: 0.00027600929024629295\n",
            "step: 270, loss: 0.0207988228648901\n",
            "step: 280, loss: 0.0015373709611594677\n",
            "step: 290, loss: 0.0005813642055727541\n",
            "step: 300, loss: 0.010267230682075024\n",
            "step: 310, loss: 0.004650723189115524\n",
            "step: 320, loss: 0.00012834934750571847\n",
            "step: 330, loss: 0.0007007240201346576\n",
            "step: 340, loss: 0.0002999606658704579\n",
            "step: 350, loss: 0.00028699610265903175\n",
            "step: 360, loss: 0.00022695887309964746\n",
            "step: 370, loss: 0.0004866510280407965\n",
            "step: 380, loss: 0.00025271569029428065\n",
            "step: 390, loss: 0.000206931188586168\n",
            "step: 400, loss: 0.00043678932706825435\n",
            "step: 410, loss: 0.00035418971674516797\n",
            "step: 420, loss: 0.02089591883122921\n",
            "step: 430, loss: 0.0023190707433968782\n",
            "step: 440, loss: 0.00012454162060748786\n",
            "step: 450, loss: 0.002178195398300886\n",
            "step: 460, loss: 0.0003653698367998004\n",
            "step: 470, loss: 0.00018337472283747047\n",
            "step: 480, loss: 0.0009468558127991855\n",
            "step: 490, loss: 0.06607000529766083\n",
            "step: 500, loss: 0.0003933984262403101\n",
            "step: 510, loss: 0.00034950400004163384\n",
            "step: 520, loss: 0.00015780962712597102\n",
            "step: 530, loss: 0.00014475842181127518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8964200477326969, f1=0.8918406072106261, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00380088877864182\n",
            "step: 10, loss: 0.14035730063915253\n",
            "step: 20, loss: 0.00010507521801628172\n",
            "step: 30, loss: 0.0005361653747968376\n",
            "step: 40, loss: 0.00014171478687785566\n",
            "step: 50, loss: 0.00016301765572279692\n",
            "step: 60, loss: 0.00015449011698365211\n",
            "step: 70, loss: 0.00011102396820206195\n",
            "step: 80, loss: 0.0001406080846209079\n",
            "step: 90, loss: 0.00028567833942361176\n",
            "step: 100, loss: 0.0015274633187800646\n",
            "step: 110, loss: 0.0013511243741959333\n",
            "step: 120, loss: 0.0008327129180543125\n",
            "step: 130, loss: 0.000169809180079028\n",
            "step: 140, loss: 0.0001683347945800051\n",
            "step: 150, loss: 0.00025515753077343106\n",
            "step: 160, loss: 0.019721725955605507\n",
            "step: 170, loss: 0.0003354224609211087\n",
            "step: 180, loss: 0.00014663039473816752\n",
            "step: 190, loss: 0.00027093823882751167\n",
            "step: 200, loss: 0.001635837135836482\n",
            "step: 210, loss: 0.00026505423011258245\n",
            "step: 220, loss: 0.0002216644206782803\n",
            "step: 230, loss: 8.807596896076575e-05\n",
            "step: 240, loss: 8.62732995301485e-05\n",
            "step: 250, loss: 0.011143572628498077\n",
            "step: 260, loss: 0.00010741022560978308\n",
            "step: 270, loss: 0.0008634878322482109\n",
            "step: 280, loss: 0.00046146559179760516\n",
            "step: 290, loss: 0.0004729192878585309\n",
            "step: 300, loss: 0.028328292071819305\n",
            "step: 310, loss: 0.019365524873137474\n",
            "step: 320, loss: 0.009465659037232399\n",
            "step: 330, loss: 0.0022298935800790787\n",
            "step: 340, loss: 0.000500315974932164\n",
            "step: 350, loss: 0.0003309686726424843\n",
            "step: 360, loss: 0.00025973585434257984\n",
            "step: 370, loss: 0.0009342014091089368\n",
            "step: 380, loss: 0.00013915609451942146\n",
            "step: 390, loss: 0.0004793587722815573\n",
            "step: 400, loss: 0.00010767983621917665\n",
            "step: 410, loss: 0.00010515625035623088\n",
            "step: 420, loss: 0.0031032392289489508\n",
            "step: 430, loss: 0.022085681557655334\n",
            "step: 440, loss: 0.00018466173787601292\n",
            "step: 450, loss: 0.00016299067647196352\n",
            "step: 460, loss: 0.0007141168462112546\n",
            "step: 470, loss: 0.0006778523093089461\n",
            "step: 480, loss: 0.0002176773559767753\n",
            "step: 490, loss: 0.006305274553596973\n",
            "step: 500, loss: 0.016902029514312744\n",
            "step: 510, loss: 0.0011351616121828556\n",
            "step: 520, loss: 0.0001567354629514739\n",
            "step: 530, loss: 0.00010996011405950412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9015256588072121, f1=0.9020689655172414, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002784742508083582\n",
            "step: 10, loss: 0.00018937946879304945\n",
            "step: 20, loss: 0.00013346440391615033\n",
            "step: 30, loss: 0.00021259374625515193\n",
            "step: 40, loss: 0.00017679555458016694\n",
            "step: 50, loss: 0.00010586974531179294\n",
            "step: 60, loss: 0.0002086757158394903\n",
            "step: 70, loss: 0.00043241531238891184\n",
            "step: 80, loss: 0.0004844185314141214\n",
            "step: 90, loss: 0.011007490567862988\n",
            "step: 100, loss: 6.87897700117901e-05\n",
            "step: 110, loss: 0.00275057228282094\n",
            "step: 120, loss: 0.00011477078805910423\n",
            "step: 130, loss: 0.0011509439209476113\n",
            "step: 140, loss: 0.0001946667762240395\n",
            "step: 150, loss: 0.0007858124445192516\n",
            "step: 160, loss: 0.000697922077961266\n",
            "step: 170, loss: 0.0018411660566926003\n",
            "step: 180, loss: 0.00010721656144596636\n",
            "step: 190, loss: 7.452948921127245e-05\n",
            "step: 200, loss: 9.447424963582307e-05\n",
            "step: 210, loss: 0.0001569060405017808\n",
            "step: 220, loss: 0.00010108183778356761\n",
            "step: 230, loss: 0.08581320941448212\n",
            "step: 240, loss: 0.00019935896852985024\n",
            "step: 250, loss: 0.00777848344296217\n",
            "step: 260, loss: 0.001059732399880886\n",
            "step: 270, loss: 9.073683031601831e-05\n",
            "step: 280, loss: 0.01516098715364933\n",
            "step: 290, loss: 0.0029810448177158833\n",
            "step: 300, loss: 0.0005015431670472026\n",
            "step: 310, loss: 0.0004144952690694481\n",
            "step: 320, loss: 0.00020361154747661203\n",
            "step: 330, loss: 0.00016780242731329054\n",
            "step: 340, loss: 0.07528873533010483\n",
            "step: 350, loss: 0.004972649738192558\n",
            "step: 360, loss: 0.00033741569495759904\n",
            "step: 370, loss: 0.0005323590012267232\n",
            "step: 380, loss: 0.00019817489373963326\n",
            "step: 390, loss: 9.230933937942609e-05\n",
            "step: 400, loss: 9.723346738610417e-05\n",
            "step: 410, loss: 0.000539363652933389\n",
            "step: 420, loss: 0.000581035332288593\n",
            "step: 430, loss: 0.00017203990137204528\n",
            "step: 440, loss: 0.0005642606993205845\n",
            "step: 450, loss: 0.00018796247604768723\n",
            "step: 460, loss: 0.051832303404808044\n",
            "step: 470, loss: 0.00017760050832293928\n",
            "step: 480, loss: 0.0004726300830952823\n",
            "step: 490, loss: 0.0008024044800549746\n",
            "step: 500, loss: 0.0031276303343474865\n",
            "step: 510, loss: 0.00012532109394669533\n",
            "step: 520, loss: 0.003923980984836817\n",
            "step: 530, loss: 0.002912473399192095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8978622327790974, f1=0.898248935163275, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016188561858143657\n",
            "step: 10, loss: 6.447353371186182e-05\n",
            "step: 20, loss: 0.017986850813031197\n",
            "step: 30, loss: 0.009709252044558525\n",
            "step: 40, loss: 0.0001454859593650326\n",
            "step: 50, loss: 0.00020883948309347034\n",
            "step: 60, loss: 0.00011826863919850439\n",
            "step: 70, loss: 0.00014086294686421752\n",
            "step: 80, loss: 0.00018477799312677234\n",
            "step: 90, loss: 0.00023997851531021297\n",
            "step: 100, loss: 0.00011660567543003708\n",
            "step: 110, loss: 0.0005238850135356188\n",
            "step: 120, loss: 0.00026438466738909483\n",
            "step: 130, loss: 0.00042018061503767967\n",
            "step: 140, loss: 0.00012513634283095598\n",
            "step: 150, loss: 0.0003412093501538038\n",
            "step: 160, loss: 7.598313823109493e-05\n",
            "step: 170, loss: 0.00016907825192902237\n",
            "step: 180, loss: 0.00011907545558642596\n",
            "step: 190, loss: 0.00018877522961702198\n",
            "step: 200, loss: 0.00507873622700572\n",
            "step: 210, loss: 0.0005150459473952651\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 220, loss: 0.00022354390239343047\n",
            "step: 230, loss: 7.35078428988345e-05\n",
            "step: 240, loss: 0.0002869732561521232\n",
            "step: 250, loss: 0.08468134701251984\n",
            "step: 260, loss: 0.00034940382465720177\n",
            "step: 270, loss: 0.00013661156117450446\n",
            "step: 280, loss: 2.7580916139413603e-05\n",
            "step: 290, loss: 0.00010324132745154202\n",
            "step: 300, loss: 0.00013586321438197047\n",
            "step: 310, loss: 0.00014984951121732593\n",
            "step: 320, loss: 8.145287574734539e-05\n",
            "step: 330, loss: 0.00041898421477526426\n",
            "step: 340, loss: 0.0001849143736762926\n",
            "step: 350, loss: 0.0038655083626508713\n",
            "step: 360, loss: 0.00011076035298174247\n",
            "step: 370, loss: 0.0002883489360101521\n",
            "step: 380, loss: 0.0003677412460092455\n",
            "step: 390, loss: 5.6418786698486656e-05\n",
            "step: 400, loss: 5.451968536362983e-05\n",
            "step: 410, loss: 0.0001739011495374143\n",
            "step: 420, loss: 6.452278466895223e-05\n",
            "step: 430, loss: 5.534588490263559e-05\n",
            "step: 440, loss: 8.834496111376211e-05\n",
            "step: 450, loss: 8.952910138759762e-05\n",
            "step: 460, loss: 3.9533591916551813e-05\n",
            "step: 470, loss: 0.0004258534754626453\n",
            "step: 480, loss: 0.00025354721583426\n",
            "step: 490, loss: 0.00011631694360403344\n",
            "step: 500, loss: 0.00020389797282405198\n",
            "step: 510, loss: 7.612406625412405e-05\n",
            "step: 520, loss: 3.566782470443286e-05\n",
            "step: 530, loss: 7.817372534191236e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.900612341026849, f1=0.9016853932584271, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.501081174472347e-05\n",
            "step: 10, loss: 7.783534965710714e-05\n",
            "step: 20, loss: 0.00014949258184060454\n",
            "step: 30, loss: 0.001724373665638268\n",
            "step: 40, loss: 5.013326517655514e-05\n",
            "step: 50, loss: 7.346543134190142e-05\n",
            "step: 60, loss: 4.7229845222318545e-05\n",
            "step: 70, loss: 0.0003402344882488251\n",
            "step: 80, loss: 0.00011168492346769199\n",
            "step: 90, loss: 3.54177100234665e-05\n",
            "step: 100, loss: 8.189905202016234e-05\n",
            "step: 110, loss: 0.00011201624147361144\n",
            "step: 120, loss: 2.6242858439218253e-05\n",
            "step: 130, loss: 4.581154644256458e-05\n",
            "step: 140, loss: 0.0026101004332304\n",
            "step: 150, loss: 9.565486834617332e-05\n",
            "step: 160, loss: 0.0005078933318145573\n",
            "step: 170, loss: 0.00016301768482662737\n",
            "step: 180, loss: 5.453287667478435e-05\n",
            "step: 190, loss: 7.68046811572276e-05\n",
            "step: 200, loss: 8.349611016456038e-05\n",
            "step: 210, loss: 0.00012982069165445864\n",
            "step: 220, loss: 0.00019407921354286373\n",
            "step: 230, loss: 0.0002587500202935189\n",
            "step: 240, loss: 5.967416655039415e-05\n",
            "step: 250, loss: 5.45035618415568e-05\n",
            "step: 260, loss: 2.6876947231357917e-05\n",
            "step: 270, loss: 0.0023879087530076504\n",
            "step: 280, loss: 2.9655386242666282e-05\n",
            "step: 290, loss: 0.0002333710581297055\n",
            "step: 300, loss: 0.0005426006973721087\n",
            "step: 310, loss: 0.00010585772542981431\n",
            "step: 320, loss: 4.539620204013772e-05\n",
            "step: 330, loss: 0.0005600936128757894\n",
            "step: 340, loss: 0.0001366447249893099\n",
            "step: 350, loss: 9.733738988870755e-05\n",
            "step: 360, loss: 0.004700818099081516\n",
            "step: 370, loss: 0.0014450266025960445\n",
            "step: 380, loss: 5.7238001318182796e-05\n",
            "step: 390, loss: 5.610305379377678e-05\n",
            "step: 400, loss: 6.97882060194388e-05\n",
            "step: 410, loss: 5.240006430540234e-05\n",
            "step: 420, loss: 2.353966556256637e-05\n",
            "step: 430, loss: 0.016926201060414314\n",
            "step: 440, loss: 2.3841241272748448e-05\n",
            "step: 450, loss: 0.0003056085843127221\n",
            "step: 460, loss: 0.00011689451639540493\n",
            "step: 470, loss: 0.0002714233414735645\n",
            "step: 480, loss: 3.0102926757535897e-05\n",
            "step: 490, loss: 3.385653690202162e-05\n",
            "step: 500, loss: 4.951415758114308e-05\n",
            "step: 510, loss: 4.448063555173576e-05\n",
            "step: 520, loss: 0.00012954435078427196\n",
            "step: 530, loss: 6.161248165881261e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9024856596558317, f1=0.89720511605874, best_f1=0.9024278515803941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.685102637973614e-05\n",
            "step: 10, loss: 0.0001602011325303465\n",
            "step: 20, loss: 0.00010176684736507013\n",
            "step: 30, loss: 0.0002353454619878903\n",
            "step: 40, loss: 0.00020817460608668625\n",
            "step: 50, loss: 2.268640855618287e-05\n",
            "step: 60, loss: 2.968461558339186e-05\n",
            "step: 70, loss: 0.0001736302801873535\n",
            "step: 80, loss: 5.792028969153762e-05\n",
            "step: 90, loss: 5.962306386209093e-05\n",
            "step: 100, loss: 3.278095755376853e-05\n",
            "step: 110, loss: 9.891294757835567e-05\n",
            "step: 120, loss: 0.0028403063770383596\n",
            "step: 130, loss: 4.8351954319514334e-05\n",
            "step: 140, loss: 4.4172968046041206e-05\n",
            "step: 150, loss: 0.00022482535860035568\n",
            "step: 160, loss: 0.005719916895031929\n",
            "step: 170, loss: 0.00012977371807210147\n",
            "step: 180, loss: 5.5685661209281534e-05\n",
            "step: 190, loss: 8.19832639535889e-05\n",
            "step: 200, loss: 3.079211091971956e-05\n",
            "step: 210, loss: 3.714976628543809e-05\n",
            "step: 220, loss: 2.4917579139582813e-05\n",
            "step: 230, loss: 3.356681554578245e-05\n",
            "step: 240, loss: 3.917713911505416e-05\n",
            "step: 250, loss: 0.00034198956564068794\n",
            "step: 260, loss: 0.0004955343902111053\n",
            "step: 270, loss: 0.00525476410984993\n",
            "step: 280, loss: 3.518002631608397e-05\n",
            "step: 290, loss: 0.005738688632845879\n",
            "step: 300, loss: 0.07695567607879639\n",
            "step: 310, loss: 0.0001761810708558187\n",
            "step: 320, loss: 7.125394040485844e-05\n",
            "step: 330, loss: 0.0026933467015624046\n",
            "step: 340, loss: 6.151381239760667e-05\n",
            "step: 350, loss: 0.00014177252887748182\n",
            "step: 360, loss: 5.811472874484025e-05\n",
            "step: 370, loss: 2.3997681637411006e-05\n",
            "step: 380, loss: 7.092361192917451e-05\n",
            "step: 390, loss: 0.00011323960643494502\n",
            "step: 400, loss: 2.6664618417271413e-05\n",
            "step: 410, loss: 0.00012239639181643724\n",
            "step: 420, loss: 8.055374200921506e-05\n",
            "step: 430, loss: 2.87545844912529e-05\n",
            "step: 440, loss: 0.11375642567873001\n",
            "step: 450, loss: 0.0016367629868909717\n",
            "step: 460, loss: 2.2187394279171713e-05\n",
            "step: 470, loss: 6.999156175879762e-05\n",
            "step: 480, loss: 7.651443593204021e-05\n",
            "step: 490, loss: 7.246466702781618e-05\n",
            "step: 500, loss: 8.713372517377138e-05\n",
            "step: 510, loss: 4.770417945110239e-05\n",
            "step: 520, loss: 3.810680209426209e-05\n",
            "step: 530, loss: 5.3352803661255166e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9038642789820923, f1=0.9016853932584271, best_f1=0.9024278515803941\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 241.57it/s]\n",
            "load_f1 = 0.9053803339517625\n",
            "real_f1 = 0.9033749422098938\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 259.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48503adf-9405-486e-8311-20c36dfbe4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8456778526306152\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.06456080079078674\n",
            "step: 20, loss: 0.3807266056537628\n",
            "step: 30, loss: 0.37620556354522705\n",
            "step: 40, loss: 0.5145447850227356\n",
            "step: 50, loss: 0.30867263674736023\n",
            "step: 60, loss: 0.3658502697944641\n",
            "step: 70, loss: 0.27315881848335266\n",
            "step: 80, loss: 0.2973671555519104\n",
            "step: 90, loss: 0.4409922659397125\n",
            "step: 100, loss: 0.20932962000370026\n",
            "step: 110, loss: 0.3095425069332123\n",
            "step: 120, loss: 0.2611664831638336\n",
            "step: 130, loss: 0.22988401353359222\n",
            "step: 140, loss: 0.3222893476486206\n",
            "step: 150, loss: 0.3270992338657379\n",
            "step: 160, loss: 0.26680129766464233\n",
            "step: 170, loss: 0.20563796162605286\n",
            "step: 180, loss: 0.19600629806518555\n",
            "step: 190, loss: 0.25065919756889343\n",
            "step: 200, loss: 0.17640426754951477\n",
            "step: 210, loss: 0.47964951395988464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.38080495356037153, f1=0.384, best_f1=0.384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08926306664943695\n",
            "step: 10, loss: 0.09255017340183258\n",
            "step: 20, loss: 0.32549771666526794\n",
            "step: 30, loss: 0.19920581579208374\n",
            "step: 40, loss: 0.04807789623737335\n",
            "step: 50, loss: 0.21108008921146393\n",
            "step: 60, loss: 0.15374372899532318\n",
            "step: 70, loss: 0.2710377871990204\n",
            "step: 80, loss: 0.29782232642173767\n",
            "step: 90, loss: 0.1975618153810501\n",
            "step: 100, loss: 0.15362529456615448\n",
            "step: 110, loss: 0.06938175112009048\n",
            "step: 120, loss: 0.2047736793756485\n",
            "step: 130, loss: 0.2615467309951782\n",
            "step: 140, loss: 0.29125550389289856\n",
            "step: 150, loss: 0.32056936621665955\n",
            "step: 160, loss: 0.14926829934120178\n",
            "step: 170, loss: 0.2144795060157776\n",
            "step: 180, loss: 0.40540218353271484\n",
            "step: 190, loss: 0.23824672400951385\n",
            "step: 200, loss: 0.2477351874113083\n",
            "step: 210, loss: 0.33392563462257385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.46843853820598, f1=0.499194847020934, best_f1=0.499194847020934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2814784348011017\n",
            "step: 10, loss: 0.2609745264053345\n",
            "step: 20, loss: 0.3620871901512146\n",
            "step: 30, loss: 0.08418212085962296\n",
            "step: 40, loss: 0.18132451176643372\n",
            "step: 50, loss: 0.24412070214748383\n",
            "step: 60, loss: 0.30269986391067505\n",
            "step: 70, loss: 0.2088860124349594\n",
            "step: 80, loss: 0.15277045965194702\n",
            "step: 90, loss: 0.12222922593355179\n",
            "step: 100, loss: 0.1791199892759323\n",
            "step: 110, loss: 0.2711721956729889\n",
            "step: 120, loss: 0.2230774611234665\n",
            "step: 130, loss: 0.25552916526794434\n",
            "step: 140, loss: 0.27694422006607056\n",
            "step: 150, loss: 0.3136068284511566\n",
            "step: 160, loss: 0.14250913262367249\n",
            "step: 170, loss: 0.2360607236623764\n",
            "step: 180, loss: 0.11848119646310806\n",
            "step: 190, loss: 0.277439683675766\n",
            "step: 200, loss: 0.2322743833065033\n",
            "step: 210, loss: 0.30381691455841064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.46855345911949686, f1=0.49841269841269836, best_f1=0.49841269841269836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3300068974494934\n",
            "step: 10, loss: 0.10503236204385757\n",
            "step: 20, loss: 0.14540253579616547\n",
            "step: 30, loss: 0.08137619495391846\n",
            "step: 40, loss: 0.1437663733959198\n",
            "step: 50, loss: 0.11366615444421768\n",
            "step: 60, loss: 0.04640205577015877\n",
            "step: 70, loss: 0.22081239521503448\n",
            "step: 80, loss: 0.24357779324054718\n",
            "step: 90, loss: 0.09490343928337097\n",
            "step: 100, loss: 0.1942514330148697\n",
            "step: 110, loss: 0.1484074741601944\n",
            "step: 120, loss: 0.07102569192647934\n",
            "step: 130, loss: 0.29381638765335083\n",
            "step: 140, loss: 0.16887854039669037\n",
            "step: 150, loss: 0.23458515107631683\n",
            "step: 160, loss: 0.1458277851343155\n",
            "step: 170, loss: 0.07249008119106293\n",
            "step: 180, loss: 0.1816936880350113\n",
            "step: 190, loss: 0.2502930462360382\n",
            "step: 200, loss: 0.28147944808006287\n",
            "step: 210, loss: 0.03510282561182976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.481981981981982, f1=0.48660714285714285, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09829438477754593\n",
            "step: 10, loss: 0.04703821241855621\n",
            "step: 20, loss: 0.029856901615858078\n",
            "step: 30, loss: 0.1271059364080429\n",
            "step: 40, loss: 0.1289234459400177\n",
            "step: 50, loss: 0.165321946144104\n",
            "step: 60, loss: 0.11647560447454453\n",
            "step: 70, loss: 0.15146994590759277\n",
            "step: 80, loss: 0.12310336530208588\n",
            "step: 90, loss: 0.040792711079120636\n",
            "step: 100, loss: 0.09536556154489517\n",
            "step: 110, loss: 0.01761579141020775\n",
            "step: 120, loss: 0.04784056171774864\n",
            "step: 130, loss: 0.15993095934391022\n",
            "step: 140, loss: 0.08828330785036087\n",
            "step: 150, loss: 0.07371941953897476\n",
            "step: 160, loss: 0.10314375907182693\n",
            "step: 170, loss: 0.09207654744386673\n",
            "step: 180, loss: 0.2669273316860199\n",
            "step: 190, loss: 0.06527397781610489\n",
            "step: 200, loss: 0.10394404083490372\n",
            "step: 210, loss: 0.09904501587152481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.46208112874779544, f1=0.48359240069084625, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10854185372591019\n",
            "step: 10, loss: 0.037695176899433136\n",
            "step: 20, loss: 0.07200755178928375\n",
            "step: 30, loss: 0.18415983021259308\n",
            "step: 40, loss: 0.030093995854258537\n",
            "step: 50, loss: 0.15817750990390778\n",
            "step: 60, loss: 0.21112015843391418\n",
            "step: 70, loss: 0.02170580066740513\n",
            "step: 80, loss: 0.07810580730438232\n",
            "step: 90, loss: 0.07422081381082535\n",
            "step: 100, loss: 0.01752845011651516\n",
            "step: 110, loss: 0.029773229733109474\n",
            "step: 120, loss: 0.028803814202547073\n",
            "step: 130, loss: 0.01676318794488907\n",
            "step: 140, loss: 0.11439143121242523\n",
            "step: 150, loss: 0.09794951230287552\n",
            "step: 160, loss: 0.34386181831359863\n",
            "step: 170, loss: 0.08142910897731781\n",
            "step: 180, loss: 0.01002525445073843\n",
            "step: 190, loss: 0.12404526770114899\n",
            "step: 200, loss: 0.020253516733646393\n",
            "step: 210, loss: 0.038497425615787506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4549763033175355, f1=0.44028103044496486, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.040582265704870224\n",
            "step: 10, loss: 0.05900140479207039\n",
            "step: 20, loss: 0.1373848021030426\n",
            "step: 30, loss: 0.026689302176237106\n",
            "step: 40, loss: 0.04383700340986252\n",
            "step: 50, loss: 0.09173527359962463\n",
            "step: 60, loss: 0.22432342171669006\n",
            "step: 70, loss: 0.012117201462388039\n",
            "step: 80, loss: 0.027664005756378174\n",
            "step: 90, loss: 0.044858258217573166\n",
            "step: 100, loss: 0.027407541871070862\n",
            "step: 110, loss: 0.08844266831874847\n",
            "step: 120, loss: 0.10744810849428177\n",
            "step: 130, loss: 0.04327138140797615\n",
            "step: 140, loss: 0.0036240662448108196\n",
            "step: 150, loss: 0.08861348778009415\n",
            "step: 160, loss: 0.03239312022924423\n",
            "step: 170, loss: 0.021831003949046135\n",
            "step: 180, loss: 0.013467919081449509\n",
            "step: 190, loss: 0.03650767728686333\n",
            "step: 200, loss: 0.042513057589530945\n",
            "step: 210, loss: 0.11368042230606079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4365482233502538, f1=0.46942148760330576, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0540497750043869\n",
            "step: 10, loss: 0.04175443574786186\n",
            "step: 20, loss: 0.057762958109378815\n",
            "step: 30, loss: 0.004270041361451149\n",
            "step: 40, loss: 0.04820191487669945\n",
            "step: 50, loss: 0.06554319709539413\n",
            "step: 60, loss: 0.22499170899391174\n",
            "step: 70, loss: 0.03514831140637398\n",
            "step: 80, loss: 0.03498339653015137\n",
            "step: 90, loss: 0.03033401444554329\n",
            "step: 100, loss: 0.00880111288279295\n",
            "step: 110, loss: 0.004754809662699699\n",
            "step: 120, loss: 0.15549759566783905\n",
            "step: 130, loss: 0.06356823444366455\n",
            "step: 140, loss: 0.03484221175312996\n",
            "step: 150, loss: 0.008007647469639778\n",
            "step: 160, loss: 0.09496346116065979\n",
            "step: 170, loss: 0.05025826394557953\n",
            "step: 180, loss: 0.07667262852191925\n",
            "step: 190, loss: 0.032417938113212585\n",
            "step: 200, loss: 0.05534468963742256\n",
            "step: 210, loss: 0.12930867075920105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4383561643835616, f1=0.45889101338432126, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025095460936427116\n",
            "step: 10, loss: 0.060237541794776917\n",
            "step: 20, loss: 0.05865410342812538\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.025891628116369247\n",
            "step: 40, loss: 0.07804465293884277\n",
            "step: 50, loss: 0.03984227776527405\n",
            "step: 60, loss: 0.12377537041902542\n",
            "step: 70, loss: 0.006862349808216095\n",
            "step: 80, loss: 0.028946902602910995\n",
            "step: 90, loss: 0.10811996459960938\n",
            "step: 100, loss: 0.018027685582637787\n",
            "step: 110, loss: 0.010406683199107647\n",
            "step: 120, loss: 0.1355234682559967\n",
            "step: 130, loss: 0.07205598801374435\n",
            "step: 140, loss: 0.06764546036720276\n",
            "step: 150, loss: 0.183905690908432\n",
            "step: 160, loss: 0.006033942569047213\n",
            "step: 170, loss: 0.020103242248296738\n",
            "step: 180, loss: 0.011365673504769802\n",
            "step: 190, loss: 0.15257994830608368\n",
            "step: 200, loss: 0.14937348663806915\n",
            "step: 210, loss: 0.058171067386865616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.42043222003929276, f1=0.4518664047151277, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011562732979655266\n",
            "step: 10, loss: 0.05792880803346634\n",
            "step: 20, loss: 0.011871728114783764\n",
            "step: 30, loss: 0.006408113520592451\n",
            "step: 40, loss: 0.008177890442311764\n",
            "step: 50, loss: 0.01002572663128376\n",
            "step: 60, loss: 0.061425354331731796\n",
            "step: 70, loss: 0.013072936795651913\n",
            "step: 80, loss: 0.0005557503900490701\n",
            "step: 90, loss: 0.029567021876573563\n",
            "step: 100, loss: 0.004337146412581205\n",
            "step: 110, loss: 0.010704852640628815\n",
            "step: 120, loss: 0.0031924883369356394\n",
            "step: 130, loss: 0.07992718368768692\n",
            "step: 140, loss: 0.009591088630259037\n",
            "step: 150, loss: 0.08930835127830505\n",
            "step: 160, loss: 0.08024940639734268\n",
            "step: 170, loss: 0.019300252199172974\n",
            "step: 180, loss: 0.11412985622882843\n",
            "step: 190, loss: 0.12218368053436279\n",
            "step: 200, loss: 0.009370430372655392\n",
            "step: 210, loss: 0.008378822356462479\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.42750929368029744, f1=0.46099290780141844, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037921147886663675\n",
            "step: 10, loss: 0.010303989052772522\n",
            "step: 20, loss: 0.02413698472082615\n",
            "step: 30, loss: 0.024672802537679672\n",
            "step: 40, loss: 0.11547865718603134\n",
            "step: 50, loss: 0.008998638018965721\n",
            "step: 60, loss: 0.09179141372442245\n",
            "step: 70, loss: 0.005306830629706383\n",
            "step: 80, loss: 0.07145848125219345\n",
            "step: 90, loss: 0.018952030688524246\n",
            "step: 100, loss: 0.04799982160329819\n",
            "step: 110, loss: 0.0264721792191267\n",
            "step: 120, loss: 0.01034195814281702\n",
            "step: 130, loss: 0.08392032235860825\n",
            "step: 140, loss: 0.003943071234971285\n",
            "step: 150, loss: 0.11766071617603302\n",
            "step: 160, loss: 0.0020087396260350943\n",
            "step: 170, loss: 0.06830031424760818\n",
            "step: 180, loss: 0.004298518877476454\n",
            "step: 190, loss: 0.16259972751140594\n",
            "step: 200, loss: 0.0027929130010306835\n",
            "step: 210, loss: 0.00494022062048316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4355400696864112, f1=0.46982055464926586, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013550794683396816\n",
            "step: 10, loss: 0.005034329369664192\n",
            "step: 20, loss: 0.019061675295233727\n",
            "step: 30, loss: 0.002327112713828683\n",
            "step: 40, loss: 0.23546244204044342\n",
            "step: 50, loss: 0.0712028220295906\n",
            "step: 60, loss: 0.009803787805140018\n",
            "step: 70, loss: 0.005593783222138882\n",
            "step: 80, loss: 0.01123836264014244\n",
            "step: 90, loss: 0.0013146625133231282\n",
            "step: 100, loss: 0.006586746778339148\n",
            "step: 110, loss: 0.004865326918661594\n",
            "step: 120, loss: 0.1389550417661667\n",
            "step: 130, loss: 0.0017231358215212822\n",
            "step: 140, loss: 0.0026501782704144716\n",
            "step: 150, loss: 0.0009567174129188061\n",
            "step: 160, loss: 0.015301130712032318\n",
            "step: 170, loss: 0.0029126908630132675\n",
            "step: 180, loss: 0.005071030929684639\n",
            "step: 190, loss: 0.023630931973457336\n",
            "step: 200, loss: 0.030125709250569344\n",
            "step: 210, loss: 0.007196256425231695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.3943089430894309, f1=0.43951612903225806, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00034838836290873587\n",
            "step: 10, loss: 0.005936694797128439\n",
            "step: 20, loss: 0.006824334617704153\n",
            "step: 30, loss: 0.009562456049025059\n",
            "step: 40, loss: 0.0039743161760270596\n",
            "step: 50, loss: 0.007665056735277176\n",
            "step: 60, loss: 0.03089250437915325\n",
            "step: 70, loss: 0.18680867552757263\n",
            "step: 80, loss: 0.09470812976360321\n",
            "step: 90, loss: 0.010552536696195602\n",
            "step: 100, loss: 0.10676909238100052\n",
            "step: 110, loss: 0.0027115463744848967\n",
            "step: 120, loss: 0.01631317473948002\n",
            "step: 130, loss: 0.00716894818469882\n",
            "step: 140, loss: 0.0031693389173597097\n",
            "step: 150, loss: 0.013568056747317314\n",
            "step: 160, loss: 0.050691328942775726\n",
            "step: 170, loss: 0.09254306554794312\n",
            "step: 180, loss: 0.08573199808597565\n",
            "step: 190, loss: 0.01208547130227089\n",
            "step: 200, loss: 0.004055853933095932\n",
            "step: 210, loss: 0.0051037282682955265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.43214285714285716, f1=0.4667802385008518, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007809730246663094\n",
            "step: 10, loss: 0.023523647338151932\n",
            "step: 20, loss: 0.15476615726947784\n",
            "step: 30, loss: 0.1923884153366089\n",
            "step: 40, loss: 0.0065032378770411015\n",
            "step: 50, loss: 0.004020404070615768\n",
            "step: 60, loss: 0.07262475788593292\n",
            "step: 70, loss: 0.040449097752571106\n",
            "step: 80, loss: 0.0840236023068428\n",
            "step: 90, loss: 0.03414108604192734\n",
            "step: 100, loss: 0.0032076742500066757\n",
            "step: 110, loss: 0.003777500009164214\n",
            "step: 120, loss: 0.005967878736555576\n",
            "step: 130, loss: 0.0029075562488287687\n",
            "step: 140, loss: 0.0014018932124599814\n",
            "step: 150, loss: 0.04838516563177109\n",
            "step: 160, loss: 0.03174687176942825\n",
            "step: 170, loss: 0.07138338685035706\n",
            "step: 180, loss: 0.001905735582113266\n",
            "step: 190, loss: 0.0020229024812579155\n",
            "step: 200, loss: 0.004740070551633835\n",
            "step: 210, loss: 0.003920606803148985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.4140625, f1=0.45112781954887216, best_f1=0.48660714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008598471991717815\n",
            "step: 10, loss: 0.005418424028903246\n",
            "step: 20, loss: 0.003756130114197731\n",
            "step: 30, loss: 0.0009078930597752333\n",
            "step: 40, loss: 0.005282688420265913\n",
            "step: 50, loss: 0.0010650543263182044\n",
            "step: 60, loss: 0.00431957608088851\n",
            "step: 70, loss: 0.0037233878392726183\n",
            "step: 80, loss: 0.0018068895442411304\n",
            "step: 90, loss: 0.0014635270927101374\n",
            "step: 100, loss: 0.0038453424349427223\n",
            "step: 110, loss: 0.0035704656038433313\n",
            "step: 120, loss: 0.055393021553754807\n",
            "step: 130, loss: 0.05602944269776344\n",
            "step: 140, loss: 0.010321719571948051\n",
            "step: 150, loss: 0.007063591852784157\n",
            "step: 160, loss: 0.006331038195639849\n",
            "step: 170, loss: 0.023130172863602638\n",
            "step: 180, loss: 0.01633705571293831\n",
            "step: 190, loss: 0.015955591574311256\n",
            "step: 200, loss: 0.013965553604066372\n",
            "step: 210, loss: 0.09872263669967651\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.41221374045801534, f1=0.4618249534450652, best_f1=0.48660714285714285\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:06, 329.58it/s]\n",
            "load_f1 = 0.477751756440281\n",
            "real_f1 = 0.46976744186046515\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 253.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba1af34-ad1f-4c2c-dc92-20fcdd34820a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8678837418556213\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.16431409120559692\n",
            "step: 20, loss: 0.1421486884355545\n",
            "step: 30, loss: 0.5015112161636353\n",
            "step: 40, loss: 0.250747412443161\n",
            "step: 50, loss: 0.3105010390281677\n",
            "step: 60, loss: 0.36676767468452454\n",
            "step: 70, loss: 0.17647258937358856\n",
            "step: 80, loss: 0.5167737007141113\n",
            "step: 90, loss: 0.24180659651756287\n",
            "step: 100, loss: 0.22269174456596375\n",
            "step: 110, loss: 0.24042633175849915\n",
            "step: 120, loss: 0.4231984317302704\n",
            "step: 130, loss: 0.3465990424156189\n",
            "step: 140, loss: 0.33385542035102844\n",
            "step: 150, loss: 0.26747599244117737\n",
            "step: 160, loss: 0.2063254714012146\n",
            "step: 170, loss: 0.40739062428474426\n",
            "step: 180, loss: 0.2623629868030548\n",
            "step: 190, loss: 0.19596511125564575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4779220779220779, f1=0.4934383202099738, best_f1=0.4934383202099738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2633635699748993\n",
            "step: 10, loss: 0.04786764085292816\n",
            "step: 20, loss: 0.07715590298175812\n",
            "step: 30, loss: 0.1611405313014984\n",
            "step: 40, loss: 0.4265322685241699\n",
            "step: 50, loss: 0.3493155539035797\n",
            "step: 60, loss: 0.1917356699705124\n",
            "step: 70, loss: 0.18155165016651154\n",
            "step: 80, loss: 0.18772318959236145\n",
            "step: 90, loss: 0.11851777136325836\n",
            "step: 100, loss: 0.173895001411438\n",
            "step: 110, loss: 0.15665972232818604\n",
            "step: 120, loss: 0.27824679017066956\n",
            "step: 130, loss: 0.25331220030784607\n",
            "step: 140, loss: 0.3050307631492615\n",
            "step: 150, loss: 0.08802862465381622\n",
            "step: 160, loss: 0.04239024594426155\n",
            "step: 170, loss: 0.21196013689041138\n",
            "step: 180, loss: 0.1010487973690033\n",
            "step: 190, loss: 0.3062446415424347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6807387862796834, f1=0.6938775510204082, best_f1=0.6938775510204082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14338579773902893\n",
            "step: 10, loss: 0.17710936069488525\n",
            "step: 20, loss: 0.0349930115044117\n",
            "step: 30, loss: 0.08764366060495377\n",
            "step: 40, loss: 0.044273633509874344\n",
            "step: 50, loss: 0.10863310098648071\n",
            "step: 60, loss: 0.03498607128858566\n",
            "step: 70, loss: 0.22212651371955872\n",
            "step: 80, loss: 0.09280115365982056\n",
            "step: 90, loss: 0.05270369350910187\n",
            "step: 100, loss: 0.11386103183031082\n",
            "step: 110, loss: 0.2517056465148926\n",
            "step: 120, loss: 0.08085281401872635\n",
            "step: 130, loss: 0.022838188335299492\n",
            "step: 140, loss: 0.06517168134450912\n",
            "step: 150, loss: 0.14666874706745148\n",
            "step: 160, loss: 0.14543737471103668\n",
            "step: 170, loss: 0.057392898947000504\n",
            "step: 180, loss: 0.13371647894382477\n",
            "step: 190, loss: 0.20157945156097412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7295285359801489, f1=0.7189873417721518, best_f1=0.7189873417721518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12970228493213654\n",
            "step: 10, loss: 0.0698310062289238\n",
            "step: 20, loss: 0.015979383140802383\n",
            "step: 30, loss: 0.059415753930807114\n",
            "step: 40, loss: 0.013991555199027061\n",
            "step: 50, loss: 0.07451887428760529\n",
            "step: 60, loss: 0.07428291440010071\n",
            "step: 70, loss: 0.04724973440170288\n",
            "step: 80, loss: 0.028846148401498795\n",
            "step: 90, loss: 0.06532188504934311\n",
            "step: 100, loss: 0.04442692548036575\n",
            "step: 110, loss: 0.052253950387239456\n",
            "step: 120, loss: 0.028280247002840042\n",
            "step: 130, loss: 0.25218021869659424\n",
            "step: 140, loss: 0.06400696933269501\n",
            "step: 150, loss: 0.00625191256403923\n",
            "step: 160, loss: 0.043552033603191376\n",
            "step: 170, loss: 0.024665849283337593\n",
            "step: 180, loss: 0.1846565455198288\n",
            "step: 190, loss: 0.05218977481126785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7738693467336683, f1=0.7531806615776082, best_f1=0.7531806615776082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02527630887925625\n",
            "step: 10, loss: 0.05514708161354065\n",
            "step: 20, loss: 0.06019602715969086\n",
            "step: 30, loss: 0.011832596734166145\n",
            "step: 40, loss: 0.05788213759660721\n",
            "step: 50, loss: 0.049737196415662766\n",
            "step: 60, loss: 0.04365835338830948\n",
            "step: 70, loss: 0.007714762352406979\n",
            "step: 80, loss: 0.0766611248254776\n",
            "step: 90, loss: 0.006482226308435202\n",
            "step: 100, loss: 0.2147664576768875\n",
            "step: 110, loss: 0.021052632480859756\n",
            "step: 120, loss: 0.002196545246988535\n",
            "step: 130, loss: 0.00985052902251482\n",
            "step: 140, loss: 0.00418392289429903\n",
            "step: 150, loss: 0.010351603850722313\n",
            "step: 160, loss: 0.021520299836993217\n",
            "step: 170, loss: 0.05964547023177147\n",
            "step: 180, loss: 0.1136445477604866\n",
            "step: 190, loss: 0.25972381234169006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7671957671957672, f1=0.7559055118110236, best_f1=0.7531806615776082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023247312754392624\n",
            "step: 10, loss: 0.001750568044371903\n",
            "step: 20, loss: 0.03852487727999687\n",
            "step: 30, loss: 0.07112712413072586\n",
            "step: 40, loss: 0.09821425378322601\n",
            "step: 50, loss: 0.008450164459645748\n",
            "step: 60, loss: 0.019604306668043137\n",
            "step: 70, loss: 0.1486021876335144\n",
            "step: 80, loss: 0.0076541113667190075\n",
            "step: 90, loss: 0.005934102926403284\n",
            "step: 100, loss: 0.008492558263242245\n",
            "step: 110, loss: 0.238446444272995\n",
            "step: 120, loss: 0.012405682355165482\n",
            "step: 130, loss: 0.007572017144411802\n",
            "step: 140, loss: 0.05803164839744568\n",
            "step: 150, loss: 0.008671501651406288\n",
            "step: 160, loss: 0.011198555119335651\n",
            "step: 170, loss: 0.0016735391691327095\n",
            "step: 180, loss: 0.07497107982635498\n",
            "step: 190, loss: 0.024338332936167717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7807486631016043, f1=0.7564766839378239, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010573090054094791\n",
            "step: 10, loss: 0.04706812649965286\n",
            "step: 20, loss: 0.006131439004093409\n",
            "step: 30, loss: 0.08082418143749237\n",
            "step: 40, loss: 0.0407940149307251\n",
            "step: 50, loss: 0.029087474569678307\n",
            "step: 60, loss: 0.01677493378520012\n",
            "step: 70, loss: 0.0016355854459106922\n",
            "step: 80, loss: 0.020633600652217865\n",
            "step: 90, loss: 0.04044128954410553\n",
            "step: 100, loss: 0.02509012073278427\n",
            "step: 110, loss: 0.007132368162274361\n",
            "step: 120, loss: 0.008940208703279495\n",
            "step: 130, loss: 0.15429741144180298\n",
            "step: 140, loss: 0.015190189704298973\n",
            "step: 150, loss: 0.02633426897227764\n",
            "step: 160, loss: 0.029835378751158714\n",
            "step: 170, loss: 0.017054220661520958\n",
            "step: 180, loss: 0.009696519933640957\n",
            "step: 190, loss: 0.0445319227874279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7772020725388601, f1=0.7626262626262625, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03261570259928703\n",
            "step: 10, loss: 0.002620183164253831\n",
            "step: 20, loss: 0.011599949561059475\n",
            "step: 30, loss: 0.002687143161892891\n",
            "step: 40, loss: 0.016655683517456055\n",
            "step: 50, loss: 0.002307336777448654\n",
            "step: 60, loss: 0.0064957174472510815\n",
            "step: 70, loss: 0.0021175346337258816\n",
            "step: 80, loss: 0.026682322844862938\n",
            "step: 90, loss: 0.005398321896791458\n",
            "step: 100, loss: 0.04491671547293663\n",
            "step: 110, loss: 0.013606049120426178\n",
            "step: 120, loss: 0.003257026895880699\n",
            "step: 130, loss: 0.019658531993627548\n",
            "step: 140, loss: 0.002923742402344942\n",
            "step: 150, loss: 0.003060900606215\n",
            "step: 160, loss: 0.1262994408607483\n",
            "step: 170, loss: 0.005565932486206293\n",
            "step: 180, loss: 0.06281828880310059\n",
            "step: 190, loss: 0.17441576719284058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.768, f1=0.7407407407407407, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005637850612401962\n",
            "step: 10, loss: 0.05000387504696846\n",
            "step: 20, loss: 0.031668633222579956\n",
            "step: 30, loss: 0.008642280474305153\n",
            "step: 40, loss: 0.011809336952865124\n",
            "step: 50, loss: 0.019147520884871483\n",
            "step: 60, loss: 0.0026575883384793997\n",
            "step: 70, loss: 0.007522759959101677\n",
            "step: 80, loss: 0.003777993842959404\n",
            "step: 90, loss: 0.06488318741321564\n",
            "step: 100, loss: 0.0026136483065783978\n",
            "step: 110, loss: 0.00195132486987859\n",
            "step: 120, loss: 0.06728202849626541\n",
            "step: 130, loss: 0.0027576705906540155\n",
            "step: 140, loss: 0.004403950180858374\n",
            "step: 150, loss: 0.03507433086633682\n",
            "step: 160, loss: 0.0016380075830966234\n",
            "step: 170, loss: 0.035787977278232574\n",
            "step: 180, loss: 0.027322161942720413\n",
            "step: 190, loss: 0.022305095568299294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7440633245382585, f1=0.7180851063829788, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002110648900270462\n",
            "step: 10, loss: 0.01807994395494461\n",
            "step: 20, loss: 0.0022865834180265665\n",
            "step: 30, loss: 0.007127002347260714\n",
            "step: 40, loss: 0.003843077691271901\n",
            "step: 50, loss: 0.009056451730430126\n",
            "step: 60, loss: 0.011387858539819717\n",
            "step: 70, loss: 0.00794499833136797\n",
            "step: 80, loss: 0.0006221596850082278\n",
            "step: 90, loss: 0.008197651244699955\n",
            "step: 100, loss: 0.0058294604532420635\n",
            "step: 110, loss: 0.07864286750555038\n",
            "step: 120, loss: 0.0009001166326925159\n",
            "step: 130, loss: 0.0005471630720421672\n",
            "step: 140, loss: 0.0018644073279574513\n",
            "step: 150, loss: 0.0037562339566648006\n",
            "step: 160, loss: 0.4237498342990875\n",
            "step: 170, loss: 0.0551852323114872\n",
            "step: 180, loss: 0.04834866151213646\n",
            "step: 190, loss: 0.023153753951191902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7637362637362638, f1=0.7444444444444444, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014430033974349499\n",
            "step: 10, loss: 0.007985611446201801\n",
            "step: 20, loss: 0.005336422473192215\n",
            "step: 30, loss: 0.031627390533685684\n",
            "step: 40, loss: 0.005614247638732195\n",
            "step: 50, loss: 0.006420521065592766\n",
            "step: 60, loss: 0.0016789919463917613\n",
            "step: 70, loss: 0.0006192016880959272\n",
            "step: 80, loss: 0.0012662721564993262\n",
            "step: 90, loss: 0.0010186316212639213\n",
            "step: 100, loss: 0.0014447278808802366\n",
            "step: 110, loss: 0.0009033339447341859\n",
            "step: 120, loss: 0.007769385352730751\n",
            "step: 130, loss: 0.002590509131550789\n",
            "step: 140, loss: 0.003533207578584552\n",
            "step: 150, loss: 0.0022688566241413355\n",
            "step: 160, loss: 0.002308236202225089\n",
            "step: 170, loss: 0.06975770741701126\n",
            "step: 180, loss: 0.023638278245925903\n",
            "step: 190, loss: 0.0017413630848750472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7594936708860759, f1=0.7272727272727272, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005470136180520058\n",
            "step: 10, loss: 0.02031565085053444\n",
            "step: 20, loss: 0.0024342695251107216\n",
            "step: 30, loss: 0.0008079961407929659\n",
            "step: 40, loss: 0.0009177503525279462\n",
            "step: 50, loss: 0.020809095352888107\n",
            "step: 60, loss: 0.0004675366508308798\n",
            "step: 70, loss: 0.0035918226931244135\n",
            "step: 80, loss: 0.0060506658628582954\n",
            "step: 90, loss: 0.0024091608356684446\n",
            "step: 100, loss: 0.000633000920061022\n",
            "step: 110, loss: 0.0003271695750299841\n",
            "step: 120, loss: 0.0019564705435186625\n",
            "step: 130, loss: 0.0007202517008408904\n",
            "step: 140, loss: 0.0028176107443869114\n",
            "step: 150, loss: 0.0017312112031504512\n",
            "step: 160, loss: 0.032560329884290695\n",
            "step: 170, loss: 0.01804855465888977\n",
            "step: 180, loss: 0.0008294570725411177\n",
            "step: 190, loss: 0.009940053336322308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.769230769230769, f1=0.7553191489361701, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004076894838362932\n",
            "step: 10, loss: 0.007139982655644417\n",
            "step: 20, loss: 0.0019623765256255865\n",
            "step: 30, loss: 0.06703481823205948\n",
            "step: 40, loss: 0.00020507631415966898\n",
            "step: 50, loss: 0.005515545140951872\n",
            "step: 60, loss: 0.0017408673884347081\n",
            "step: 70, loss: 0.002031204756349325\n",
            "step: 80, loss: 0.017206432297825813\n",
            "step: 90, loss: 0.006660028826445341\n",
            "step: 100, loss: 0.0010904045775532722\n",
            "step: 110, loss: 0.0021526122000068426\n",
            "step: 120, loss: 0.001422008266672492\n",
            "step: 130, loss: 0.042837128043174744\n",
            "step: 140, loss: 0.0005386951379477978\n",
            "step: 150, loss: 0.06873048096895218\n",
            "step: 160, loss: 0.0006219229544512928\n",
            "step: 170, loss: 0.00047481898218393326\n",
            "step: 180, loss: 0.004974374081939459\n",
            "step: 190, loss: 0.0027664960362017155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7578947368421052, f1=0.7546174142480211, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013842931017279625\n",
            "step: 10, loss: 0.001934035331942141\n",
            "step: 20, loss: 0.013724755495786667\n",
            "step: 30, loss: 0.005519203841686249\n",
            "step: 40, loss: 0.000736475340090692\n",
            "step: 50, loss: 0.004557224456220865\n",
            "step: 60, loss: 0.044383157044649124\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 70, loss: 0.12283571809530258\n",
            "step: 80, loss: 0.0007859551114961505\n",
            "step: 90, loss: 0.030091840773820877\n",
            "step: 100, loss: 0.0024280776269733906\n",
            "step: 110, loss: 0.09973283112049103\n",
            "step: 120, loss: 0.0034387842752039433\n",
            "step: 130, loss: 0.00034097698517143726\n",
            "step: 140, loss: 0.0004563444817904383\n",
            "step: 150, loss: 0.0007476042956113815\n",
            "step: 160, loss: 0.018207263201475143\n",
            "step: 170, loss: 0.14001844823360443\n",
            "step: 180, loss: 0.0027432686183601618\n",
            "step: 190, loss: 0.0005339630879461765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7563451776649746, f1=0.732824427480916, best_f1=0.7564766839378239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006354871671646833\n",
            "step: 10, loss: 0.000927323242649436\n",
            "step: 20, loss: 0.00091799336951226\n",
            "step: 30, loss: 0.0015073786489665508\n",
            "step: 40, loss: 0.002638385398313403\n",
            "step: 50, loss: 0.014604699797928333\n",
            "step: 60, loss: 0.08576289564371109\n",
            "step: 70, loss: 0.0010523569071665406\n",
            "step: 80, loss: 0.0007302048616111279\n",
            "step: 90, loss: 0.025354884564876556\n",
            "step: 100, loss: 0.0018072273815050721\n",
            "step: 110, loss: 0.0011934958165511489\n",
            "step: 120, loss: 0.008690353482961655\n",
            "step: 130, loss: 0.0008788208360783756\n",
            "step: 140, loss: 0.016553644090890884\n",
            "step: 150, loss: 0.0012686122208833694\n",
            "step: 160, loss: 0.0018825225997716188\n",
            "step: 170, loss: 0.0006593374419026077\n",
            "step: 180, loss: 0.0021079801954329014\n",
            "step: 190, loss: 0.008673657663166523\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7639257294429709, f1=0.7580645161290323, best_f1=0.7564766839378239\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:09, 223.00it/s]\n",
            "load_f1 = 0.7169811320754718\n",
            "real_f1 = 0.7200000000000001\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 252.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df964bd-1767-4536-eeae-7e55597c370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8606671094894409\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.21967338025569916\n",
            "step: 20, loss: 0.14477181434631348\n",
            "step: 30, loss: 0.23876327276229858\n",
            "step: 40, loss: 0.3144698739051819\n",
            "step: 50, loss: 0.3738466501235962\n",
            "step: 60, loss: 0.43473950028419495\n",
            "step: 70, loss: 0.31004709005355835\n",
            "step: 80, loss: 0.2567903995513916\n",
            "step: 90, loss: 0.41218918561935425\n",
            "step: 100, loss: 0.23960426449775696\n",
            "step: 110, loss: 0.18556863069534302\n",
            "step: 120, loss: 0.5344347357749939\n",
            "step: 130, loss: 0.42518842220306396\n",
            "step: 140, loss: 0.46937116980552673\n",
            "step: 150, loss: 0.11786145716905594\n",
            "step: 160, loss: 0.30458864569664\n",
            "step: 170, loss: 0.16423234343528748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5932203389830508, f1=0.6186440677966103, best_f1=0.6186440677966103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32910943031311035\n",
            "step: 10, loss: 0.08308415859937668\n",
            "step: 20, loss: 0.21983429789543152\n",
            "step: 30, loss: 0.18327710032463074\n",
            "step: 40, loss: 0.18887144327163696\n",
            "step: 50, loss: 0.17633819580078125\n",
            "step: 60, loss: 0.07409349828958511\n",
            "step: 70, loss: 0.23530460894107819\n",
            "step: 80, loss: 0.21969985961914062\n",
            "step: 90, loss: 0.21164776384830475\n",
            "step: 100, loss: 0.1366240233182907\n",
            "step: 110, loss: 0.3214060068130493\n",
            "step: 120, loss: 0.13342608511447906\n",
            "step: 130, loss: 0.0680701956152916\n",
            "step: 140, loss: 0.21196089684963226\n",
            "step: 150, loss: 0.11542610824108124\n",
            "step: 160, loss: 0.15619401633739471\n",
            "step: 170, loss: 0.10625558346509933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7087378640776699, f1=0.6808510638297873, best_f1=0.6808510638297873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08947759866714478\n",
            "step: 10, loss: 0.052552517503499985\n",
            "step: 20, loss: 0.026777267456054688\n",
            "step: 30, loss: 0.25428125262260437\n",
            "step: 40, loss: 0.006266262382268906\n",
            "step: 50, loss: 0.11179666966199875\n",
            "step: 60, loss: 0.31552764773368835\n",
            "step: 70, loss: 0.13065794110298157\n",
            "step: 80, loss: 0.09079151600599289\n",
            "step: 90, loss: 0.08728902786970139\n",
            "step: 100, loss: 0.05325012281537056\n",
            "step: 110, loss: 0.08383198827505112\n",
            "step: 120, loss: 0.032366059720516205\n",
            "step: 130, loss: 0.10322288423776627\n",
            "step: 140, loss: 0.00471894908696413\n",
            "step: 150, loss: 0.14244243502616882\n",
            "step: 160, loss: 0.0688997432589531\n",
            "step: 170, loss: 0.12341451644897461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6929133858267718, f1=0.6894865525672372, best_f1=0.6808510638297873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10271412879228592\n",
            "step: 10, loss: 0.14845699071884155\n",
            "step: 20, loss: 0.08182254433631897\n",
            "step: 30, loss: 0.15591581165790558\n",
            "step: 40, loss: 0.07812397181987762\n",
            "step: 50, loss: 0.04733269661664963\n",
            "step: 60, loss: 0.07687314599752426\n",
            "step: 70, loss: 0.03715270757675171\n",
            "step: 80, loss: 0.06592180579900742\n",
            "step: 90, loss: 0.05577554181218147\n",
            "step: 100, loss: 0.03572552278637886\n",
            "step: 110, loss: 0.03765907511115074\n",
            "step: 120, loss: 0.2050389051437378\n",
            "step: 130, loss: 0.020148856565356255\n",
            "step: 140, loss: 0.015107783488929272\n",
            "step: 150, loss: 0.03370283544063568\n",
            "step: 160, loss: 0.08668313920497894\n",
            "step: 170, loss: 0.012721177190542221\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6981132075471699, f1=0.6789838337182448, best_f1=0.6808510638297873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12502938508987427\n",
            "step: 10, loss: 0.1005890816450119\n",
            "step: 20, loss: 0.05679324269294739\n",
            "step: 30, loss: 0.07273305952548981\n",
            "step: 40, loss: 0.05319948494434357\n",
            "step: 50, loss: 0.02277223765850067\n",
            "step: 60, loss: 0.004695459268987179\n",
            "step: 70, loss: 0.007990307174623013\n",
            "step: 80, loss: 0.014384904876351357\n",
            "step: 90, loss: 0.019949400797486305\n",
            "step: 100, loss: 0.08481035381555557\n",
            "step: 110, loss: 0.18740597367286682\n",
            "step: 120, loss: 0.07998830825090408\n",
            "step: 130, loss: 0.01500474289059639\n",
            "step: 140, loss: 0.09017674624919891\n",
            "step: 150, loss: 0.018847430124878883\n",
            "step: 160, loss: 0.0246720053255558\n",
            "step: 170, loss: 0.04983902350068092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7117794486215538, f1=0.6733668341708543, best_f1=0.6733668341708543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10158319771289825\n",
            "step: 10, loss: 0.07371331751346588\n",
            "step: 20, loss: 0.009928129613399506\n",
            "step: 30, loss: 0.16596955060958862\n",
            "step: 40, loss: 0.010465229861438274\n",
            "step: 50, loss: 0.009355448186397552\n",
            "step: 60, loss: 0.1217384785413742\n",
            "step: 70, loss: 0.03602183610200882\n",
            "step: 80, loss: 0.09244001656770706\n",
            "step: 90, loss: 0.0488775372505188\n",
            "step: 100, loss: 0.12918338179588318\n",
            "step: 110, loss: 0.08932068943977356\n",
            "step: 120, loss: 0.1441441923379898\n",
            "step: 130, loss: 0.22044584155082703\n",
            "step: 140, loss: 0.006307033821940422\n",
            "step: 150, loss: 0.32677334547042847\n",
            "step: 160, loss: 0.04609477147459984\n",
            "step: 170, loss: 0.05243730545043945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7047146401985112, f1=0.691542288557214, best_f1=0.6733668341708543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032607796601951122\n",
            "step: 10, loss: 0.004865690134465694\n",
            "step: 20, loss: 0.011656852439045906\n",
            "step: 30, loss: 0.20276589691638947\n",
            "step: 40, loss: 0.02477818727493286\n",
            "step: 50, loss: 0.01244254969060421\n",
            "step: 60, loss: 0.17573432624340057\n",
            "step: 70, loss: 0.007588204927742481\n",
            "step: 80, loss: 0.021175606176257133\n",
            "step: 90, loss: 0.004527768585830927\n",
            "step: 100, loss: 0.015345687977969646\n",
            "step: 110, loss: 0.02097168192267418\n",
            "step: 120, loss: 0.2229199856519699\n",
            "step: 130, loss: 0.06539281457662582\n",
            "step: 140, loss: 0.04239558055996895\n",
            "step: 150, loss: 0.11759160459041595\n",
            "step: 160, loss: 0.047328073531389236\n",
            "step: 170, loss: 0.08342380821704865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6910994764397905, f1=0.6851385390428212, best_f1=0.6733668341708543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13040213286876678\n",
            "step: 10, loss: 0.015657659620046616\n",
            "step: 20, loss: 0.047851014882326126\n",
            "step: 30, loss: 0.056282173842191696\n",
            "step: 40, loss: 0.007741358131170273\n",
            "step: 50, loss: 0.0012367962626740336\n",
            "step: 60, loss: 0.006629134528338909\n",
            "step: 70, loss: 0.006345084868371487\n",
            "step: 80, loss: 0.0017843652749434114\n",
            "step: 90, loss: 0.061430543661117554\n",
            "step: 100, loss: 0.008149714209139347\n",
            "step: 110, loss: 0.056735921651124954\n",
            "step: 120, loss: 0.003026908729225397\n",
            "step: 130, loss: 0.033875517547130585\n",
            "step: 140, loss: 0.05346100032329559\n",
            "step: 150, loss: 0.056557513773441315\n",
            "step: 160, loss: 0.010357336141169071\n",
            "step: 170, loss: 0.01334922481328249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6947890818858561, f1=0.6893203883495146, best_f1=0.6733668341708543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02125287428498268\n",
            "step: 10, loss: 0.01258022803813219\n",
            "step: 20, loss: 0.021320069208741188\n",
            "step: 30, loss: 0.04191139340400696\n",
            "step: 40, loss: 0.026741869747638702\n",
            "step: 50, loss: 0.03320491313934326\n",
            "step: 60, loss: 0.0034231289755553007\n",
            "step: 70, loss: 0.144067645072937\n",
            "step: 80, loss: 0.03252251818776131\n",
            "step: 90, loss: 0.01982244849205017\n",
            "step: 100, loss: 0.02632416971027851\n",
            "step: 110, loss: 0.0013282060390338302\n",
            "step: 120, loss: 0.03993404656648636\n",
            "step: 130, loss: 0.02325119450688362\n",
            "step: 140, loss: 0.0013438964961096644\n",
            "step: 150, loss: 0.0758049488067627\n",
            "step: 160, loss: 0.012983150780200958\n",
            "step: 170, loss: 0.029770130291581154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.712468193384224, f1=0.7007299270072993, best_f1=0.7007299270072993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03921610862016678\n",
            "step: 10, loss: 0.099824458360672\n",
            "step: 20, loss: 0.006658267229795456\n",
            "step: 30, loss: 0.0003013374807778746\n",
            "step: 40, loss: 0.07094080001115799\n",
            "step: 50, loss: 0.013507931493222713\n",
            "step: 60, loss: 0.02037145383656025\n",
            "step: 70, loss: 0.10267829149961472\n",
            "step: 80, loss: 0.04764719679951668\n",
            "step: 90, loss: 0.010317937470972538\n",
            "step: 100, loss: 0.04742836579680443\n",
            "step: 110, loss: 0.02359568141400814\n",
            "step: 120, loss: 0.0034139661584049463\n",
            "step: 130, loss: 0.015640219673514366\n",
            "step: 140, loss: 0.02050854079425335\n",
            "step: 150, loss: 0.005713210441172123\n",
            "step: 160, loss: 0.0004731559893116355\n",
            "step: 170, loss: 0.006869012955576181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7139240506329114, f1=0.6889952153110048, best_f1=0.6889952153110048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11021200567483902\n",
            "step: 10, loss: 0.031938180327415466\n",
            "step: 20, loss: 0.027034347876906395\n",
            "step: 30, loss: 0.005823586601763964\n",
            "step: 40, loss: 0.00643521174788475\n",
            "step: 50, loss: 0.0012281116796657443\n",
            "step: 60, loss: 0.009435523301362991\n",
            "step: 70, loss: 0.009424139745533466\n",
            "step: 80, loss: 0.001834479859098792\n",
            "step: 90, loss: 0.02726234681904316\n",
            "step: 100, loss: 0.0015182311180979013\n",
            "step: 110, loss: 0.007794198580086231\n",
            "step: 120, loss: 0.12406262755393982\n",
            "step: 130, loss: 0.012882858514785767\n",
            "step: 140, loss: 0.07835550606250763\n",
            "step: 150, loss: 0.002251166384667158\n",
            "step: 160, loss: 0.0011286924127489328\n",
            "step: 170, loss: 0.02713657170534134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7061855670103093, f1=0.6930693069306931, best_f1=0.6889952153110048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015827137976884842\n",
            "step: 10, loss: 0.025357455015182495\n",
            "step: 20, loss: 0.059215694665908813\n",
            "step: 30, loss: 0.02081603929400444\n",
            "step: 40, loss: 0.0007057447801344097\n",
            "step: 50, loss: 0.0013376092538237572\n",
            "step: 60, loss: 0.0008455846109427512\n",
            "step: 70, loss: 0.00821661576628685\n",
            "step: 80, loss: 0.005156348459422588\n",
            "step: 90, loss: 0.001961606787517667\n",
            "step: 100, loss: 0.04657554626464844\n",
            "step: 110, loss: 0.00020068608864676207\n",
            "step: 120, loss: 0.0034388869535177946\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.001376171363517642\n",
            "step: 140, loss: 0.001502307364717126\n",
            "step: 150, loss: 0.007244789972901344\n",
            "step: 160, loss: 0.022558659315109253\n",
            "step: 170, loss: 0.008459597826004028\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7022900763358779, f1=0.6844660194174758, best_f1=0.6889952153110048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021680226200260222\n",
            "step: 10, loss: 0.01399984024465084\n",
            "step: 20, loss: 0.0004843693459406495\n",
            "step: 30, loss: 0.005038979928940535\n",
            "step: 40, loss: 0.000654085073620081\n",
            "step: 50, loss: 0.016326557844877243\n",
            "step: 60, loss: 0.002814448205754161\n",
            "step: 70, loss: 0.015187237411737442\n",
            "step: 80, loss: 0.0038384879007935524\n",
            "step: 90, loss: 0.004940305836498737\n",
            "step: 100, loss: 0.0010626019211485982\n",
            "step: 110, loss: 0.0019251061603426933\n",
            "step: 120, loss: 0.0012601097114384174\n",
            "step: 130, loss: 0.003384593641385436\n",
            "step: 140, loss: 0.0016648524906486273\n",
            "step: 150, loss: 0.022366929799318314\n",
            "step: 160, loss: 0.002746609039604664\n",
            "step: 170, loss: 0.00021098597790114582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7047146401985112, f1=0.6715328467153285, best_f1=0.6889952153110048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017918688245117664\n",
            "step: 10, loss: 0.0004128095752093941\n",
            "step: 20, loss: 0.03671392798423767\n",
            "step: 30, loss: 0.0010758324060589075\n",
            "step: 40, loss: 0.02126956358551979\n",
            "step: 50, loss: 0.031761422753334045\n",
            "step: 60, loss: 0.0009605567320249975\n",
            "step: 70, loss: 0.0002748368715401739\n",
            "step: 80, loss: 0.0011840835213661194\n",
            "step: 90, loss: 0.016703037545084953\n",
            "step: 100, loss: 0.00038011028664186597\n",
            "step: 110, loss: 0.002530493773519993\n",
            "step: 120, loss: 0.0019133426249027252\n",
            "step: 130, loss: 0.002437037881463766\n",
            "step: 140, loss: 0.0006365231820382178\n",
            "step: 150, loss: 0.0026805144734680653\n",
            "step: 160, loss: 0.05644499510526657\n",
            "step: 170, loss: 0.00046860388829372823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7064935064935066, f1=0.6633165829145728, best_f1=0.6889952153110048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0057258908636868\n",
            "step: 10, loss: 0.15228138864040375\n",
            "step: 20, loss: 0.0007699186680838466\n",
            "step: 30, loss: 0.053554050624370575\n",
            "step: 40, loss: 0.000884641835000366\n",
            "step: 50, loss: 0.0005486374720931053\n",
            "step: 60, loss: 0.00017134449444711208\n",
            "step: 70, loss: 0.0025682293344289064\n",
            "step: 80, loss: 0.009579821489751339\n",
            "step: 90, loss: 0.0003564879298210144\n",
            "step: 100, loss: 0.0015600166516378522\n",
            "step: 110, loss: 0.015567601658403873\n",
            "step: 120, loss: 0.016037089750170708\n",
            "step: 130, loss: 0.0035275090485811234\n",
            "step: 140, loss: 0.0006470656953752041\n",
            "step: 150, loss: 0.0007219562539830804\n",
            "step: 160, loss: 0.005328684579581022\n",
            "step: 170, loss: 0.0007305228500626981\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7079207920792079, f1=0.6903073286052009, best_f1=0.6889952153110048\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:05, 322.34it/s]\n",
            "load_f1 = 0.64\n",
            "real_f1 = 0.631578947368421\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 250.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085b641a-32f4-4956-e5db-55c579cb2092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 442/442 [00:00<00:00, 421kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.88MB/s]\n",
            "Downloading: 100% 268M/268M [00:04<00:00, 64.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8175991177558899\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.48768138885498047\n",
            "step: 20, loss: 0.6124733686447144\n",
            "step: 30, loss: 0.4956091642379761\n",
            "step: 40, loss: 0.42658689618110657\n",
            "step: 50, loss: 0.3137235939502716\n",
            "step: 60, loss: 0.16500817239284515\n",
            "step: 70, loss: 0.17832067608833313\n",
            "step: 80, loss: 0.2352222055196762\n",
            "step: 90, loss: 0.08111239969730377\n",
            "step: 100, loss: 0.10318733751773834\n",
            "step: 110, loss: 0.11954237520694733\n",
            "step: 120, loss: 0.02995391935110092\n",
            "step: 130, loss: 0.019679326564073563\n",
            "step: 140, loss: 0.04131067544221878\n",
            "step: 150, loss: 0.09811985492706299\n",
            "step: 160, loss: 0.19351769983768463\n",
            "step: 170, loss: 0.018493736162781715\n",
            "step: 180, loss: 0.015678439289331436\n",
            "step: 190, loss: 0.25382131338119507\n",
            "step: 200, loss: 0.08592905104160309\n",
            "step: 210, loss: 0.03986058011651039\n",
            "step: 220, loss: 0.038219716399908066\n",
            "step: 230, loss: 0.02574940025806427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9409141583054627, f1=0.936026936026936, best_f1=0.936026936026936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016407262533903122\n",
            "step: 10, loss: 0.06462143361568451\n",
            "step: 20, loss: 0.027987942099571228\n",
            "step: 30, loss: 0.02713223174214363\n",
            "step: 40, loss: 0.027321619912981987\n",
            "step: 50, loss: 0.10899271070957184\n",
            "step: 60, loss: 0.1627611517906189\n",
            "step: 70, loss: 0.0719841867685318\n",
            "step: 80, loss: 0.005912122782319784\n",
            "step: 90, loss: 0.004701988305896521\n",
            "step: 100, loss: 0.1593262404203415\n",
            "step: 110, loss: 0.09518341720104218\n",
            "step: 120, loss: 0.021126940846443176\n",
            "step: 130, loss: 0.1845790594816208\n",
            "step: 140, loss: 0.16231290996074677\n",
            "step: 150, loss: 0.06919810175895691\n",
            "step: 160, loss: 0.09169861674308777\n",
            "step: 170, loss: 0.01438838429749012\n",
            "step: 180, loss: 0.053592510521411896\n",
            "step: 190, loss: 0.031611889600753784\n",
            "step: 200, loss: 0.0977456271648407\n",
            "step: 210, loss: 0.10297569632530212\n",
            "step: 220, loss: 0.021211247891187668\n",
            "step: 230, loss: 0.17935988306999207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9383561643835616, f1=0.9303944315545244, best_f1=0.936026936026936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09566619992256165\n",
            "step: 10, loss: 0.07394496351480484\n",
            "step: 20, loss: 0.0211337823420763\n",
            "step: 30, loss: 0.009505582973361015\n",
            "step: 40, loss: 0.10267505794763565\n",
            "step: 50, loss: 0.012212100438773632\n",
            "step: 60, loss: 0.08960198611021042\n",
            "step: 70, loss: 0.004816174041479826\n",
            "step: 80, loss: 0.052343737334012985\n",
            "step: 90, loss: 0.10664352029561996\n",
            "step: 100, loss: 0.003497040830552578\n",
            "step: 110, loss: 0.05652996152639389\n",
            "step: 120, loss: 0.008930608630180359\n",
            "step: 130, loss: 0.004514060448855162\n",
            "step: 140, loss: 0.0034262635745108128\n",
            "step: 150, loss: 0.016350699588656425\n",
            "step: 160, loss: 0.0044990424066782\n",
            "step: 170, loss: 0.027434395626187325\n",
            "step: 180, loss: 0.0031462342012673616\n",
            "step: 190, loss: 0.09239613264799118\n",
            "step: 200, loss: 0.07489701360464096\n",
            "step: 210, loss: 0.01603688858449459\n",
            "step: 220, loss: 0.02586829476058483\n",
            "step: 230, loss: 0.1799786537885666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.951739618406285, f1=0.9527027027027027, best_f1=0.9527027027027027\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.032123588025569916\n",
            "step: 10, loss: 0.010619411244988441\n",
            "step: 20, loss: 0.0017617467092350125\n",
            "step: 30, loss: 0.0018444781890138984\n",
            "step: 40, loss: 0.0029032202437520027\n",
            "step: 50, loss: 0.003927832003682852\n",
            "step: 60, loss: 0.009855743497610092\n",
            "step: 70, loss: 0.004376250319182873\n",
            "step: 80, loss: 0.15019415318965912\n",
            "step: 90, loss: 0.0072365556843578815\n",
            "step: 100, loss: 0.019499676302075386\n",
            "step: 110, loss: 0.022460713982582092\n",
            "step: 120, loss: 0.01694170944392681\n",
            "step: 130, loss: 0.001137954881414771\n",
            "step: 140, loss: 0.008535550907254219\n",
            "step: 150, loss: 0.001733037643134594\n",
            "step: 160, loss: 0.05129154399037361\n",
            "step: 170, loss: 0.005688539706170559\n",
            "step: 180, loss: 0.09148462116718292\n",
            "step: 190, loss: 0.006181502714753151\n",
            "step: 200, loss: 0.011757604777812958\n",
            "step: 210, loss: 0.0013762564631178975\n",
            "step: 220, loss: 0.00408053956925869\n",
            "step: 230, loss: 0.10501404106616974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9522752497225305, f1=0.9499443826473859, best_f1=0.9499443826473859\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017904816195368767\n",
            "step: 10, loss: 0.002230319892987609\n",
            "step: 20, loss: 0.002061673905700445\n",
            "step: 30, loss: 0.001657686079852283\n",
            "step: 40, loss: 0.0021548226941376925\n",
            "step: 50, loss: 0.003838352160528302\n",
            "step: 60, loss: 0.004265397787094116\n",
            "step: 70, loss: 0.0008768774569034576\n",
            "step: 80, loss: 0.06611622869968414\n",
            "step: 90, loss: 0.005629444494843483\n",
            "step: 100, loss: 0.01813303306698799\n",
            "step: 110, loss: 0.004162777680903673\n",
            "step: 120, loss: 0.0443522185087204\n",
            "step: 130, loss: 0.002482716692611575\n",
            "step: 140, loss: 0.0005336819449439645\n",
            "step: 150, loss: 0.00030891611822880805\n",
            "step: 160, loss: 0.15907453000545502\n",
            "step: 170, loss: 0.005802148953080177\n",
            "step: 180, loss: 0.0018558475421741605\n",
            "step: 190, loss: 0.0007458645850419998\n",
            "step: 200, loss: 0.002664446597918868\n",
            "step: 210, loss: 0.05589449405670166\n",
            "step: 220, loss: 0.002127674175426364\n",
            "step: 230, loss: 0.0008975974633358419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9513274336283186, f1=0.9465478841870824, best_f1=0.9499443826473859\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013320759171620011\n",
            "step: 10, loss: 0.005977119784802198\n",
            "step: 20, loss: 0.009712212719023228\n",
            "step: 30, loss: 0.005017673131078482\n",
            "step: 40, loss: 0.023931490257382393\n",
            "step: 50, loss: 0.004293561447411776\n",
            "step: 60, loss: 0.01120692677795887\n",
            "step: 70, loss: 0.002222924493253231\n",
            "step: 80, loss: 0.01888044737279415\n",
            "step: 90, loss: 0.0005508214235305786\n",
            "step: 100, loss: 0.0012532369000837207\n",
            "step: 110, loss: 0.10283569991588593\n",
            "step: 120, loss: 0.004782106727361679\n",
            "step: 130, loss: 0.0034251767210662365\n",
            "step: 140, loss: 0.005371833685785532\n",
            "step: 150, loss: 0.008503125049173832\n",
            "step: 160, loss: 0.04255789518356323\n",
            "step: 170, loss: 0.002406417392194271\n",
            "step: 180, loss: 0.00311990175396204\n",
            "step: 190, loss: 0.008010409772396088\n",
            "step: 200, loss: 0.011851289309561253\n",
            "step: 210, loss: 0.0004351114039309323\n",
            "step: 220, loss: 0.00024665280943736434\n",
            "step: 230, loss: 0.0050652832724153996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9565217391304347, f1=0.9528089887640449, best_f1=0.9528089887640449\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015351184643805027\n",
            "step: 10, loss: 0.0003252899332437664\n",
            "step: 20, loss: 0.0006937385187484324\n",
            "step: 30, loss: 0.004750506486743689\n",
            "step: 40, loss: 0.0019862891640514135\n",
            "step: 50, loss: 0.10359691828489304\n",
            "step: 60, loss: 0.0059444899670779705\n",
            "step: 70, loss: 0.0005841092206537724\n",
            "step: 80, loss: 0.00048396355123259127\n",
            "step: 90, loss: 0.0012753031915053725\n",
            "step: 100, loss: 0.0044107576832175255\n",
            "step: 110, loss: 0.001322719850577414\n",
            "step: 120, loss: 0.011274149641394615\n",
            "step: 130, loss: 0.0014558499678969383\n",
            "step: 140, loss: 0.0005476147634908557\n",
            "step: 150, loss: 0.039806898683309555\n",
            "step: 160, loss: 0.010554872453212738\n",
            "step: 170, loss: 0.004094318952411413\n",
            "step: 180, loss: 0.00027335749473422766\n",
            "step: 190, loss: 0.0024552468676120043\n",
            "step: 200, loss: 0.0006688323919661343\n",
            "step: 210, loss: 0.000194727152120322\n",
            "step: 220, loss: 0.0016092524165287614\n",
            "step: 230, loss: 0.0912322998046875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9605411499436302, f1=0.9561304836895389, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004268922784831375\n",
            "step: 10, loss: 0.01841229945421219\n",
            "step: 20, loss: 0.0007440719055011868\n",
            "step: 30, loss: 0.0026252414099872112\n",
            "step: 40, loss: 0.014603800140321255\n",
            "step: 50, loss: 0.0013074863236397505\n",
            "step: 60, loss: 0.0010785876074805856\n",
            "step: 70, loss: 0.0016724420711398125\n",
            "step: 80, loss: 0.0010958478087559342\n",
            "step: 90, loss: 0.0004893727600574493\n",
            "step: 100, loss: 0.0003882997843902558\n",
            "step: 110, loss: 0.0023681088350713253\n",
            "step: 120, loss: 0.0009581600315868855\n",
            "step: 130, loss: 0.06520909816026688\n",
            "step: 140, loss: 0.004600844345986843\n",
            "step: 150, loss: 0.007588961161673069\n",
            "step: 160, loss: 0.010385905392467976\n",
            "step: 170, loss: 0.04587390646338463\n",
            "step: 180, loss: 0.005948955193161964\n",
            "step: 190, loss: 0.010614495724439621\n",
            "step: 200, loss: 0.016287904232740402\n",
            "step: 210, loss: 0.0020662047900259495\n",
            "step: 220, loss: 0.003687122603878379\n",
            "step: 230, loss: 0.017572369426488876\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9550561797752809, f1=0.953229398663697, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006300063687376678\n",
            "step: 10, loss: 0.0009395845700055361\n",
            "step: 20, loss: 0.00645094271749258\n",
            "step: 30, loss: 0.00048344151582568884\n",
            "step: 40, loss: 0.00028201547684147954\n",
            "step: 50, loss: 0.0014858637005090714\n",
            "step: 60, loss: 0.00042895606020465493\n",
            "step: 70, loss: 0.0005690502002835274\n",
            "step: 80, loss: 0.1335424929857254\n",
            "step: 90, loss: 0.004958304110914469\n",
            "step: 100, loss: 0.0013786726631224155\n",
            "step: 110, loss: 0.0006006247131153941\n",
            "step: 120, loss: 0.07394147664308548\n",
            "step: 130, loss: 0.0011157176923006773\n",
            "step: 140, loss: 0.11348158866167068\n",
            "step: 150, loss: 0.00010296410619048402\n",
            "step: 160, loss: 0.13251543045043945\n",
            "step: 170, loss: 0.0023446364793926477\n",
            "step: 180, loss: 0.0004465220554266125\n",
            "step: 190, loss: 0.0076370276510715485\n",
            "step: 200, loss: 0.0008541245479136705\n",
            "step: 210, loss: 0.001851272420026362\n",
            "step: 220, loss: 0.00378498830832541\n",
            "step: 230, loss: 0.0015727749560028315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9519553072625698, f1=0.9516310461192351, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009689512080512941\n",
            "step: 10, loss: 0.00012056235573254526\n",
            "step: 20, loss: 0.0003643619711510837\n",
            "step: 30, loss: 0.00027131816023029387\n",
            "step: 40, loss: 0.0004229997284710407\n",
            "step: 50, loss: 0.0007016477175056934\n",
            "step: 60, loss: 0.15766145288944244\n",
            "step: 70, loss: 0.004148946143686771\n",
            "step: 80, loss: 0.00016088814300019294\n",
            "step: 90, loss: 0.00018468669441062957\n",
            "step: 100, loss: 0.00037572052679024637\n",
            "step: 110, loss: 0.00010793303226819262\n",
            "step: 120, loss: 0.006632066331803799\n",
            "step: 130, loss: 0.0013878496829420328\n",
            "step: 140, loss: 0.016100499778985977\n",
            "step: 150, loss: 0.0008958555990830064\n",
            "step: 160, loss: 0.0010204797144979239\n",
            "step: 170, loss: 0.00017730258696246892\n",
            "step: 180, loss: 0.07792063802480698\n",
            "step: 190, loss: 0.00044020317727699876\n",
            "step: 200, loss: 0.0005517056561075151\n",
            "step: 210, loss: 0.00012402240827213973\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 220, loss: 0.0014148971531540155\n",
            "step: 230, loss: 0.0011156772961840034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9530201342281879, f1=0.9498327759197325, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.410705438815057e-05\n",
            "step: 10, loss: 0.00029988630558364093\n",
            "step: 20, loss: 6.411983486032113e-05\n",
            "step: 30, loss: 0.00011434495536377653\n",
            "step: 40, loss: 0.0033872216008603573\n",
            "step: 50, loss: 0.018929988145828247\n",
            "step: 60, loss: 0.00013915973249822855\n",
            "step: 70, loss: 0.00011721943883458152\n",
            "step: 80, loss: 0.0024579924065619707\n",
            "step: 90, loss: 0.022534910589456558\n",
            "step: 100, loss: 0.03387235850095749\n",
            "step: 110, loss: 0.017602374777197838\n",
            "step: 120, loss: 0.0012988706585019827\n",
            "step: 130, loss: 0.0009194189915433526\n",
            "step: 140, loss: 0.0002273252175655216\n",
            "step: 150, loss: 6.160608609206975e-05\n",
            "step: 160, loss: 0.00023119596880860627\n",
            "step: 170, loss: 0.003795669414103031\n",
            "step: 180, loss: 0.00016748756752349436\n",
            "step: 190, loss: 0.0001204651125590317\n",
            "step: 200, loss: 0.00013690887135453522\n",
            "step: 210, loss: 6.661348015768453e-05\n",
            "step: 220, loss: 0.00018720237130764872\n",
            "step: 230, loss: 0.019067365676164627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9524886877828055, f1=0.9429223744292238, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014574566157534719\n",
            "step: 10, loss: 0.0020920545794069767\n",
            "step: 20, loss: 0.00010477413889020681\n",
            "step: 30, loss: 0.00028638640651479363\n",
            "step: 40, loss: 6.572635902557522e-05\n",
            "step: 50, loss: 0.00010957277117995545\n",
            "step: 60, loss: 0.006029557902365923\n",
            "step: 70, loss: 0.02050534263253212\n",
            "step: 80, loss: 0.00011974591325270012\n",
            "step: 90, loss: 0.00022655140492133796\n",
            "step: 100, loss: 0.00013868998212274164\n",
            "step: 110, loss: 0.00012773314665537328\n",
            "step: 120, loss: 0.00040547840762883425\n",
            "step: 130, loss: 0.0010109430877491832\n",
            "step: 140, loss: 9.729413432069123e-05\n",
            "step: 150, loss: 4.538573921308853e-05\n",
            "step: 160, loss: 0.0021851176861673594\n",
            "step: 170, loss: 8.974610682344064e-05\n",
            "step: 180, loss: 0.000428535946412012\n",
            "step: 190, loss: 0.00016008273814804852\n",
            "step: 200, loss: 0.00180448975879699\n",
            "step: 210, loss: 7.816818106221035e-05\n",
            "step: 220, loss: 0.04670647159218788\n",
            "step: 230, loss: 0.00011834189353976399\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9559322033898305, f1=0.9411764705882352, best_f1=0.9561304836895389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.470098302699625e-05\n",
            "step: 10, loss: 4.948004789184779e-05\n",
            "step: 20, loss: 0.00046094844583421946\n",
            "step: 30, loss: 0.04096977785229683\n",
            "step: 40, loss: 0.0006267678108997643\n",
            "step: 50, loss: 0.000554982281755656\n",
            "step: 60, loss: 7.125155389076099e-05\n",
            "step: 70, loss: 0.00013130488514434546\n",
            "step: 80, loss: 0.000434188375947997\n",
            "step: 90, loss: 4.16058610426262e-05\n",
            "step: 100, loss: 0.000389344641007483\n",
            "step: 110, loss: 9.791889897314832e-05\n",
            "step: 120, loss: 0.002712504705414176\n",
            "step: 130, loss: 0.00027001366834156215\n",
            "step: 140, loss: 0.008028123527765274\n",
            "step: 150, loss: 0.0003508762747514993\n",
            "step: 160, loss: 0.004020716063678265\n",
            "step: 170, loss: 5.483698259922676e-05\n",
            "step: 180, loss: 0.0015379118267446756\n",
            "step: 190, loss: 4.9974260036833584e-05\n",
            "step: 200, loss: 7.73123829276301e-05\n",
            "step: 210, loss: 7.327691855607554e-05\n",
            "step: 220, loss: 0.00010727307380875573\n",
            "step: 230, loss: 0.00039043358992785215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9614512471655329, f1=0.9455782312925171, best_f1=0.9455782312925171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.900717223994434e-05\n",
            "step: 10, loss: 3.617100810515694e-05\n",
            "step: 20, loss: 0.0016997501952573657\n",
            "step: 30, loss: 0.00010775875125546008\n",
            "step: 40, loss: 6.361709529301152e-05\n",
            "step: 50, loss: 0.0002860080567188561\n",
            "step: 60, loss: 4.177028313279152e-05\n",
            "step: 70, loss: 5.0594404456205666e-05\n",
            "step: 80, loss: 0.0007590454188175499\n",
            "step: 90, loss: 9.073812543647364e-05\n",
            "step: 100, loss: 0.0013352526584640145\n",
            "step: 110, loss: 8.710251859156415e-05\n",
            "step: 120, loss: 3.796309101744555e-05\n",
            "step: 130, loss: 0.00012207131658215076\n",
            "step: 140, loss: 0.00012369723117444664\n",
            "step: 150, loss: 5.47068084415514e-05\n",
            "step: 160, loss: 5.107554898131639e-05\n",
            "step: 170, loss: 0.00011324348452035338\n",
            "step: 180, loss: 0.0004869375843554735\n",
            "step: 190, loss: 0.0009824811713770032\n",
            "step: 200, loss: 6.206866237334907e-05\n",
            "step: 210, loss: 0.0007850779220461845\n",
            "step: 220, loss: 9.353940549772233e-05\n",
            "step: 230, loss: 5.649825106956996e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9614512471655329, f1=0.9478458049886621, best_f1=0.9455782312925171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.593407720676623e-05\n",
            "step: 10, loss: 0.00010115601617144421\n",
            "step: 20, loss: 0.00011217758583370596\n",
            "step: 30, loss: 0.0056344326585531235\n",
            "step: 40, loss: 9.058793511940166e-05\n",
            "step: 50, loss: 8.516891830367967e-05\n",
            "step: 60, loss: 3.8205056625884026e-05\n",
            "step: 70, loss: 0.00017003317771013826\n",
            "step: 80, loss: 6.594150909222662e-05\n",
            "step: 90, loss: 4.915034514851868e-05\n",
            "step: 100, loss: 0.000683110614772886\n",
            "step: 110, loss: 8.239257294917479e-05\n",
            "step: 120, loss: 7.443493086611852e-05\n",
            "step: 130, loss: 0.00011944396828766912\n",
            "step: 140, loss: 0.0001476663164794445\n",
            "step: 150, loss: 8.835017069941387e-05\n",
            "step: 160, loss: 5.2540963224601e-05\n",
            "step: 170, loss: 5.326302198227495e-05\n",
            "step: 180, loss: 6.704925908707082e-05\n",
            "step: 190, loss: 0.0008453863556496799\n",
            "step: 200, loss: 6.381262210197747e-05\n",
            "step: 210, loss: 0.007583961356431246\n",
            "step: 220, loss: 0.0019676911178976297\n",
            "step: 230, loss: 7.083440141286701e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9603624009060023, f1=0.9443813847900113, best_f1=0.9455782312925171\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:08, 277.47it/s]\n",
            "load_f1 = 0.9614512471655329\n",
            "real_f1 = 0.9568181818181819\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:12, 350.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8098f24-db0e-4cae-db6c-6a43e5acf685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8011091351509094\n",
            "step: 10, loss: 0.4094763696193695\n",
            "step: 20, loss: 0.5013610124588013\n",
            "step: 30, loss: 0.47188159823417664\n",
            "step: 40, loss: 0.4701383709907532\n",
            "step: 50, loss: 0.41896742582321167\n",
            "step: 60, loss: 0.4412967264652252\n",
            "step: 70, loss: 0.22796793282032013\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.38012075424194336\n",
            "step: 90, loss: 0.37699320912361145\n",
            "step: 100, loss: 0.25175172090530396\n",
            "step: 110, loss: 0.11724621057510376\n",
            "step: 120, loss: 0.18234147131443024\n",
            "step: 130, loss: 0.053947143256664276\n",
            "step: 140, loss: 0.32620659470558167\n",
            "step: 150, loss: 0.05157405883073807\n",
            "step: 160, loss: 0.15402695536613464\n",
            "step: 170, loss: 0.31113776564598083\n",
            "step: 180, loss: 0.2281460016965866\n",
            "step: 190, loss: 0.06240740790963173\n",
            "step: 200, loss: 0.25495877861976624\n",
            "step: 210, loss: 0.10058866441249847\n",
            "step: 220, loss: 0.05304091051220894\n",
            "step: 230, loss: 0.12443847954273224\n",
            "step: 240, loss: 0.08097916096448898\n",
            "step: 250, loss: 0.07266692072153091\n",
            "step: 260, loss: 0.02464454062283039\n",
            "step: 270, loss: 0.026026541367173195\n",
            "step: 280, loss: 0.22794321179389954\n",
            "step: 290, loss: 0.1015319973230362\n",
            "step: 300, loss: 0.15171052515506744\n",
            "step: 310, loss: 0.05208564177155495\n",
            "step: 320, loss: 0.1663723886013031\n",
            "step: 330, loss: 0.1756824254989624\n",
            "step: 340, loss: 0.26545023918151855\n",
            "step: 350, loss: 0.09180454909801483\n",
            "step: 360, loss: 0.09156399965286255\n",
            "step: 370, loss: 0.1034795492887497\n",
            "step: 380, loss: 0.22038252651691437\n",
            "step: 390, loss: 0.04154512658715248\n",
            "step: 400, loss: 0.01209245529025793\n",
            "step: 410, loss: 0.11948300898075104\n",
            "step: 420, loss: 0.07889721542596817\n",
            "step: 430, loss: 0.1807435303926468\n",
            "step: 440, loss: 0.21567711234092712\n",
            "step: 450, loss: 0.09536575525999069\n",
            "step: 460, loss: 0.03706255182623863\n",
            "step: 470, loss: 0.2786632180213928\n",
            "step: 480, loss: 0.22973865270614624\n",
            "step: 490, loss: 0.0827040821313858\n",
            "step: 500, loss: 0.04310613498091698\n",
            "step: 510, loss: 0.08338332921266556\n",
            "step: 520, loss: 0.2070103883743286\n",
            "step: 530, loss: 0.03699749708175659\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9013557737260404, f1=0.8933582787652012, best_f1=0.8933582787652012\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1574031412601471\n",
            "step: 10, loss: 0.1608368456363678\n",
            "step: 20, loss: 0.12236294895410538\n",
            "step: 30, loss: 0.0907229408621788\n",
            "step: 40, loss: 0.011439084075391293\n",
            "step: 50, loss: 0.03750396519899368\n",
            "step: 60, loss: 0.20558884739875793\n",
            "step: 70, loss: 0.12601740658283234\n",
            "step: 80, loss: 0.021048838272690773\n",
            "step: 90, loss: 0.11371632665395737\n",
            "step: 100, loss: 0.2932887375354767\n",
            "step: 110, loss: 0.05956589803099632\n",
            "step: 120, loss: 0.03438940271735191\n",
            "step: 130, loss: 0.06715022772550583\n",
            "step: 140, loss: 0.08333995193243027\n",
            "step: 150, loss: 0.0675572082400322\n",
            "step: 160, loss: 0.1126154437661171\n",
            "step: 170, loss: 0.1944042295217514\n",
            "step: 180, loss: 0.053837571293115616\n",
            "step: 190, loss: 0.030809279531240463\n",
            "step: 200, loss: 0.05626547709107399\n",
            "step: 210, loss: 0.026686107739806175\n",
            "step: 220, loss: 0.14528131484985352\n",
            "step: 230, loss: 0.09338206797838211\n",
            "step: 240, loss: 0.14466167986392975\n",
            "step: 250, loss: 0.16870519518852234\n",
            "step: 260, loss: 0.06865088641643524\n",
            "step: 270, loss: 0.15890951454639435\n",
            "step: 280, loss: 0.10501398146152496\n",
            "step: 290, loss: 0.04065684601664543\n",
            "step: 300, loss: 0.02150237187743187\n",
            "step: 310, loss: 0.2722081243991852\n",
            "step: 320, loss: 0.10556440055370331\n",
            "step: 330, loss: 0.05916617065668106\n",
            "step: 340, loss: 0.011463986709713936\n",
            "step: 350, loss: 0.160494863986969\n",
            "step: 360, loss: 0.23519529402256012\n",
            "step: 370, loss: 0.036400649696588516\n",
            "step: 380, loss: 0.15533936023712158\n",
            "step: 390, loss: 0.03985006734728813\n",
            "step: 400, loss: 0.05404888838529587\n",
            "step: 410, loss: 0.007148123346269131\n",
            "step: 420, loss: 0.06604582816362381\n",
            "step: 430, loss: 0.04226929321885109\n",
            "step: 440, loss: 0.023208456113934517\n",
            "step: 450, loss: 0.07595805823802948\n",
            "step: 460, loss: 0.20758464932441711\n",
            "step: 470, loss: 0.055867526680231094\n",
            "step: 480, loss: 0.2919265627861023\n",
            "step: 490, loss: 0.07113026827573776\n",
            "step: 500, loss: 0.017365284264087677\n",
            "step: 510, loss: 0.05286252126097679\n",
            "step: 520, loss: 0.03595368564128876\n",
            "step: 530, loss: 0.3289240300655365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9130434782608695, f1=0.8968140751307656, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09123406559228897\n",
            "step: 10, loss: 0.08336351066827774\n",
            "step: 20, loss: 0.18406112492084503\n",
            "step: 30, loss: 0.08104067295789719\n",
            "step: 40, loss: 0.008738872595131397\n",
            "step: 50, loss: 0.03694150596857071\n",
            "step: 60, loss: 0.014366872608661652\n",
            "step: 70, loss: 0.04913848266005516\n",
            "step: 80, loss: 0.13380157947540283\n",
            "step: 90, loss: 0.27382245659828186\n",
            "step: 100, loss: 0.04173984378576279\n",
            "step: 110, loss: 0.050779614597558975\n",
            "step: 120, loss: 0.028719432651996613\n",
            "step: 130, loss: 0.06655192375183105\n",
            "step: 140, loss: 0.09808636456727982\n",
            "step: 150, loss: 0.07611726969480515\n",
            "step: 160, loss: 0.0838540643453598\n",
            "step: 170, loss: 0.018242744728922844\n",
            "step: 180, loss: 0.09488631039857864\n",
            "step: 190, loss: 0.04748866707086563\n",
            "step: 200, loss: 0.014391510747373104\n",
            "step: 210, loss: 0.21126896142959595\n",
            "step: 220, loss: 0.012150048278272152\n",
            "step: 230, loss: 0.09237437695264816\n",
            "step: 240, loss: 0.05119309946894646\n",
            "step: 250, loss: 0.1436762809753418\n",
            "step: 260, loss: 0.021995527669787407\n",
            "step: 270, loss: 0.01910468190908432\n",
            "step: 280, loss: 0.05050790682435036\n",
            "step: 290, loss: 0.06424163281917572\n",
            "step: 300, loss: 0.06441925466060638\n",
            "step: 310, loss: 0.16688048839569092\n",
            "step: 320, loss: 0.13387955725193024\n",
            "step: 330, loss: 0.041503943502902985\n",
            "step: 340, loss: 0.006614494137465954\n",
            "step: 350, loss: 0.08347427099943161\n",
            "step: 360, loss: 0.004809064324945211\n",
            "step: 370, loss: 0.029271448031067848\n",
            "step: 380, loss: 0.027875520288944244\n",
            "step: 390, loss: 0.018784072250127792\n",
            "step: 400, loss: 0.07357151061296463\n",
            "step: 410, loss: 0.12533283233642578\n",
            "step: 420, loss: 0.10062268376350403\n",
            "step: 430, loss: 0.11399834603071213\n",
            "step: 440, loss: 0.05996858701109886\n",
            "step: 450, loss: 0.19183650612831116\n",
            "step: 460, loss: 0.1701091080904007\n",
            "step: 470, loss: 0.021844100207090378\n",
            "step: 480, loss: 0.027282074093818665\n",
            "step: 490, loss: 0.003302791155874729\n",
            "step: 500, loss: 0.12533137202262878\n",
            "step: 510, loss: 0.0798598900437355\n",
            "step: 520, loss: 0.012050083838403225\n",
            "step: 530, loss: 0.04286446049809456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.908167974157822, f1=0.9024839006439742, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027547644451260567\n",
            "step: 10, loss: 0.02170668914914131\n",
            "step: 20, loss: 0.044354699552059174\n",
            "step: 30, loss: 0.1351887732744217\n",
            "step: 40, loss: 0.01219726912677288\n",
            "step: 50, loss: 0.038872238248586655\n",
            "step: 60, loss: 0.029353423044085503\n",
            "step: 70, loss: 0.013394029811024666\n",
            "step: 80, loss: 0.017632506787776947\n",
            "step: 90, loss: 0.053297385573387146\n",
            "step: 100, loss: 0.04366143420338631\n",
            "step: 110, loss: 0.0033073769882321358\n",
            "step: 120, loss: 0.01838819310069084\n",
            "step: 130, loss: 0.1800723820924759\n",
            "step: 140, loss: 0.1586807519197464\n",
            "step: 150, loss: 0.0053738742135465145\n",
            "step: 160, loss: 0.13066290318965912\n",
            "step: 170, loss: 0.011658729985356331\n",
            "step: 180, loss: 0.1187654584646225\n",
            "step: 190, loss: 0.03327420353889465\n",
            "step: 200, loss: 0.0013731210492551327\n",
            "step: 210, loss: 0.0708201602101326\n",
            "step: 220, loss: 0.008632583543658257\n",
            "step: 230, loss: 0.25582224130630493\n",
            "step: 240, loss: 0.012494919821619987\n",
            "step: 250, loss: 0.009976860135793686\n",
            "step: 260, loss: 0.03916347399353981\n",
            "step: 270, loss: 0.023976163938641548\n",
            "step: 280, loss: 0.00785074196755886\n",
            "step: 290, loss: 0.035153038799762726\n",
            "step: 300, loss: 0.053665533661842346\n",
            "step: 310, loss: 0.015768423676490784\n",
            "step: 320, loss: 0.01986530050635338\n",
            "step: 330, loss: 0.036623015999794006\n",
            "step: 340, loss: 0.013829429633915424\n",
            "step: 350, loss: 0.003939576912671328\n",
            "step: 360, loss: 0.018206119537353516\n",
            "step: 370, loss: 0.00861439760774374\n",
            "step: 380, loss: 0.014237086288630962\n",
            "step: 390, loss: 0.029704904183745384\n",
            "step: 400, loss: 0.057538941502571106\n",
            "step: 410, loss: 0.06420572102069855\n",
            "step: 420, loss: 0.033313170075416565\n",
            "step: 430, loss: 0.023331306874752045\n",
            "step: 440, loss: 0.0473497249186039\n",
            "step: 450, loss: 0.010942271910607815\n",
            "step: 460, loss: 0.004079486709088087\n",
            "step: 470, loss: 0.001995975384488702\n",
            "step: 480, loss: 0.09905350208282471\n",
            "step: 490, loss: 0.07757411897182465\n",
            "step: 500, loss: 0.01892431639134884\n",
            "step: 510, loss: 0.13028670847415924\n",
            "step: 520, loss: 0.11982928961515427\n",
            "step: 530, loss: 0.0035184286534786224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9127769919849128, f1=0.9049858889934148, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005767287686467171\n",
            "step: 10, loss: 0.025439182296395302\n",
            "step: 20, loss: 0.033258967101573944\n",
            "step: 30, loss: 0.00994283240288496\n",
            "step: 40, loss: 0.01297435536980629\n",
            "step: 50, loss: 0.08490829169750214\n",
            "step: 60, loss: 0.025212805718183517\n",
            "step: 70, loss: 0.0029342707712203264\n",
            "step: 80, loss: 0.0032709757797420025\n",
            "step: 90, loss: 0.09686999022960663\n",
            "step: 100, loss: 0.006791945081204176\n",
            "step: 110, loss: 0.0026916726492345333\n",
            "step: 120, loss: 0.039199814200401306\n",
            "step: 130, loss: 0.028729882091283798\n",
            "step: 140, loss: 0.010595960542559624\n",
            "step: 150, loss: 0.006528983823955059\n",
            "step: 160, loss: 0.20252206921577454\n",
            "step: 170, loss: 0.05761932581663132\n",
            "step: 180, loss: 0.10069642961025238\n",
            "step: 190, loss: 0.0019680068362504244\n",
            "step: 200, loss: 0.0014174594543874264\n",
            "step: 210, loss: 0.0017507896991446614\n",
            "step: 220, loss: 0.009136438369750977\n",
            "step: 230, loss: 0.0006471445667557418\n",
            "step: 240, loss: 0.0022335408721119165\n",
            "step: 250, loss: 0.023091452196240425\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 260, loss: 0.030401088297367096\n",
            "step: 270, loss: 0.0626947358250618\n",
            "step: 280, loss: 0.031107190996408463\n",
            "step: 290, loss: 0.006311541423201561\n",
            "step: 300, loss: 0.005753392353653908\n",
            "step: 310, loss: 0.000747904006857425\n",
            "step: 320, loss: 0.11741246283054352\n",
            "step: 330, loss: 0.025279046967625618\n",
            "step: 340, loss: 0.0024649559054523706\n",
            "step: 350, loss: 0.011149241589009762\n",
            "step: 360, loss: 0.003342448966577649\n",
            "step: 370, loss: 0.003221308346837759\n",
            "step: 380, loss: 0.017818614840507507\n",
            "step: 390, loss: 0.07687628269195557\n",
            "step: 400, loss: 0.0010547630954533815\n",
            "step: 410, loss: 0.002301793312653899\n",
            "step: 420, loss: 0.00835672952234745\n",
            "step: 430, loss: 0.027272727340459824\n",
            "step: 440, loss: 0.056749127805233\n",
            "step: 450, loss: 0.053509876132011414\n",
            "step: 460, loss: 0.023668775334954262\n",
            "step: 470, loss: 0.005449789110571146\n",
            "step: 480, loss: 0.01300622895359993\n",
            "step: 490, loss: 0.029811495915055275\n",
            "step: 500, loss: 0.015401945449411869\n",
            "step: 510, loss: 0.02126743085682392\n",
            "step: 520, loss: 0.13668496906757355\n",
            "step: 530, loss: 0.04784318804740906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9047844623401232, f1=0.8948356807511737, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011453904211521149\n",
            "step: 10, loss: 0.0531328022480011\n",
            "step: 20, loss: 0.0008801231742836535\n",
            "step: 30, loss: 0.008659749291837215\n",
            "step: 40, loss: 0.004796651192009449\n",
            "step: 50, loss: 0.0435660220682621\n",
            "step: 60, loss: 0.08512244373559952\n",
            "step: 70, loss: 0.013293547555804253\n",
            "step: 80, loss: 0.0009493406396359205\n",
            "step: 90, loss: 0.10472079366445541\n",
            "step: 100, loss: 0.041333701461553574\n",
            "step: 110, loss: 0.0023544265422970057\n",
            "step: 120, loss: 0.24108120799064636\n",
            "step: 130, loss: 0.010298161767423153\n",
            "step: 140, loss: 0.04167357459664345\n",
            "step: 150, loss: 0.0030847815796732903\n",
            "step: 160, loss: 0.0041227880865335464\n",
            "step: 170, loss: 0.029088007286190987\n",
            "step: 180, loss: 0.0006236339686438441\n",
            "step: 190, loss: 0.05562610924243927\n",
            "step: 200, loss: 0.0004080980725120753\n",
            "step: 210, loss: 0.05606839433312416\n",
            "step: 220, loss: 0.0033401651307940483\n",
            "step: 230, loss: 0.000655224546790123\n",
            "step: 240, loss: 0.07311278581619263\n",
            "step: 250, loss: 0.013152903877198696\n",
            "step: 260, loss: 0.0030132823158055544\n",
            "step: 270, loss: 0.00490047549828887\n",
            "step: 280, loss: 0.003141881665214896\n",
            "step: 290, loss: 0.004030011128634214\n",
            "step: 300, loss: 0.0005602266173809767\n",
            "step: 310, loss: 0.0020160714630037546\n",
            "step: 320, loss: 0.005374527536332607\n",
            "step: 330, loss: 0.053713493049144745\n",
            "step: 340, loss: 0.012483927421271801\n",
            "step: 350, loss: 0.018159061670303345\n",
            "step: 360, loss: 0.002042977139353752\n",
            "step: 370, loss: 0.018476437777280807\n",
            "step: 380, loss: 0.004745409358292818\n",
            "step: 390, loss: 0.007156719453632832\n",
            "step: 400, loss: 0.0035260627046227455\n",
            "step: 410, loss: 0.00035289840889163315\n",
            "step: 420, loss: 0.013423705473542213\n",
            "step: 430, loss: 0.0009401264833286405\n",
            "step: 440, loss: 0.01749960333108902\n",
            "step: 450, loss: 0.015002300031483173\n",
            "step: 460, loss: 0.18656183779239655\n",
            "step: 470, loss: 0.004521525930613279\n",
            "step: 480, loss: 0.0021079860161989927\n",
            "step: 490, loss: 0.004994198214262724\n",
            "step: 500, loss: 0.07429884374141693\n",
            "step: 510, loss: 0.06988125294446945\n",
            "step: 520, loss: 0.002113231224939227\n",
            "step: 530, loss: 0.005268416367471218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9043397106859542, f1=0.8936567164179104, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10020275413990021\n",
            "step: 10, loss: 0.03665263578295708\n",
            "step: 20, loss: 0.14735007286071777\n",
            "step: 30, loss: 0.006900786887854338\n",
            "step: 40, loss: 0.0008279496105387807\n",
            "step: 50, loss: 0.008198057301342487\n",
            "step: 60, loss: 0.029353152960538864\n",
            "step: 70, loss: 0.0028449606616050005\n",
            "step: 80, loss: 0.0015807339223101735\n",
            "step: 90, loss: 0.006028399337083101\n",
            "step: 100, loss: 0.0036473332438617945\n",
            "step: 110, loss: 0.0012323021655902267\n",
            "step: 120, loss: 0.0002460202085785568\n",
            "step: 130, loss: 0.01427033357322216\n",
            "step: 140, loss: 0.002064867177978158\n",
            "step: 150, loss: 0.06445791572332382\n",
            "step: 160, loss: 0.007291924208402634\n",
            "step: 170, loss: 0.003135361708700657\n",
            "step: 180, loss: 0.052246659994125366\n",
            "step: 190, loss: 0.0037220637314021587\n",
            "step: 200, loss: 0.06364136189222336\n",
            "step: 210, loss: 0.00562805263325572\n",
            "step: 220, loss: 0.0007437487365677953\n",
            "step: 230, loss: 0.0013711616629734635\n",
            "step: 240, loss: 0.0251578688621521\n",
            "step: 250, loss: 0.035305991768836975\n",
            "step: 260, loss: 0.0034232083708047867\n",
            "step: 270, loss: 0.00047837733291089535\n",
            "step: 280, loss: 0.06139974296092987\n",
            "step: 290, loss: 0.0024832452181726694\n",
            "step: 300, loss: 0.014605535194277763\n",
            "step: 310, loss: 0.0003253151080571115\n",
            "step: 320, loss: 0.012824947014451027\n",
            "step: 330, loss: 0.05135160684585571\n",
            "step: 340, loss: 0.008384454064071178\n",
            "step: 350, loss: 0.0031315318774431944\n",
            "step: 360, loss: 0.0015196213498711586\n",
            "step: 370, loss: 0.01813136227428913\n",
            "step: 380, loss: 0.004227330908179283\n",
            "step: 390, loss: 0.008103678934276104\n",
            "step: 400, loss: 0.0005424093687906861\n",
            "step: 410, loss: 0.0003245080588385463\n",
            "step: 420, loss: 0.08139490336179733\n",
            "step: 430, loss: 0.0003492562682367861\n",
            "step: 440, loss: 0.003178022801876068\n",
            "step: 450, loss: 0.0018185763619840145\n",
            "step: 460, loss: 0.0026343914214521646\n",
            "step: 470, loss: 0.01053392980247736\n",
            "step: 480, loss: 0.003625868121162057\n",
            "step: 490, loss: 0.007442348171025515\n",
            "step: 500, loss: 0.00013113350723870099\n",
            "step: 510, loss: 0.05032581090927124\n",
            "step: 520, loss: 0.001293982146307826\n",
            "step: 530, loss: 0.01487792283296585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.906791569086651, f1=0.9037106622827618, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004483393859118223\n",
            "step: 10, loss: 0.07473433762788773\n",
            "step: 20, loss: 0.00545307993888855\n",
            "step: 30, loss: 0.05584683641791344\n",
            "step: 40, loss: 0.005192270968109369\n",
            "step: 50, loss: 0.0027667430695146322\n",
            "step: 60, loss: 0.0921572893857956\n",
            "step: 70, loss: 0.07533104717731476\n",
            "step: 80, loss: 0.007490553427487612\n",
            "step: 90, loss: 0.001105765113607049\n",
            "step: 100, loss: 0.0039633349515497684\n",
            "step: 110, loss: 0.0011004726402461529\n",
            "step: 120, loss: 0.0013790512457489967\n",
            "step: 130, loss: 0.017387645319104195\n",
            "step: 140, loss: 0.00010512234439374879\n",
            "step: 150, loss: 0.0025507682003080845\n",
            "step: 160, loss: 0.005220004823058844\n",
            "step: 170, loss: 0.0061845239251852036\n",
            "step: 180, loss: 0.0010710828937590122\n",
            "step: 190, loss: 0.055417753756046295\n",
            "step: 200, loss: 0.005090763326734304\n",
            "step: 210, loss: 0.09503105282783508\n",
            "step: 220, loss: 0.002585142385214567\n",
            "step: 230, loss: 0.0033775942865759134\n",
            "step: 240, loss: 0.05374512821435928\n",
            "step: 250, loss: 0.002030494622886181\n",
            "step: 260, loss: 0.09171643853187561\n",
            "step: 270, loss: 0.00017973432841245085\n",
            "step: 280, loss: 0.000791457830928266\n",
            "step: 290, loss: 0.012476027011871338\n",
            "step: 300, loss: 0.00028399238362908363\n",
            "step: 310, loss: 0.039360105991363525\n",
            "step: 320, loss: 0.0005776117905043066\n",
            "step: 330, loss: 0.0010263252770528197\n",
            "step: 340, loss: 0.0006994931027293205\n",
            "step: 350, loss: 0.0011453621555119753\n",
            "step: 360, loss: 0.006108297035098076\n",
            "step: 370, loss: 0.0018567141378298402\n",
            "step: 380, loss: 0.08621316403150558\n",
            "step: 390, loss: 0.005380840972065926\n",
            "step: 400, loss: 0.03342701122164726\n",
            "step: 410, loss: 0.06130418926477432\n",
            "step: 420, loss: 0.03461742028594017\n",
            "step: 430, loss: 0.0035952709149569273\n",
            "step: 440, loss: 0.0016601107781752944\n",
            "step: 450, loss: 0.000713265675585717\n",
            "step: 460, loss: 0.010914869606494904\n",
            "step: 470, loss: 0.0008784868405200541\n",
            "step: 480, loss: 0.00696626678109169\n",
            "step: 490, loss: 0.0006881196168251336\n",
            "step: 500, loss: 0.0002753747976385057\n",
            "step: 510, loss: 0.003028387436643243\n",
            "step: 520, loss: 0.003821052610874176\n",
            "step: 530, loss: 0.0002987275074701756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9056956115779645, f1=0.8996733551096594, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007860878249630332\n",
            "step: 10, loss: 0.000794224557466805\n",
            "step: 20, loss: 0.01106479112058878\n",
            "step: 30, loss: 0.016319675371050835\n",
            "step: 40, loss: 0.37972190976142883\n",
            "step: 50, loss: 0.0028631589375436306\n",
            "step: 60, loss: 0.001293072127737105\n",
            "step: 70, loss: 0.009812279604375362\n",
            "step: 80, loss: 0.003531040856614709\n",
            "step: 90, loss: 0.05121295154094696\n",
            "step: 100, loss: 0.001568412990309298\n",
            "step: 110, loss: 0.00482749380171299\n",
            "step: 120, loss: 0.0026272046379745007\n",
            "step: 130, loss: 0.0007150016026571393\n",
            "step: 140, loss: 0.0034650657325983047\n",
            "step: 150, loss: 0.0008481511613354087\n",
            "step: 160, loss: 0.001243661274202168\n",
            "step: 170, loss: 0.0009124952484853566\n",
            "step: 180, loss: 0.00017059850506484509\n",
            "step: 190, loss: 0.0106691038236022\n",
            "step: 200, loss: 0.0019172753673046827\n",
            "step: 210, loss: 0.00011217398423468694\n",
            "step: 220, loss: 0.0002756469475571066\n",
            "step: 230, loss: 0.0008619948639534414\n",
            "step: 240, loss: 0.006322868634015322\n",
            "step: 250, loss: 0.003425262402743101\n",
            "step: 260, loss: 0.0012219962663948536\n",
            "step: 270, loss: 0.00669822096824646\n",
            "step: 280, loss: 8.328596595674753e-05\n",
            "step: 290, loss: 0.00011955325317103416\n",
            "step: 300, loss: 0.00183952902443707\n",
            "step: 310, loss: 0.0005118089029565454\n",
            "step: 320, loss: 0.0011141003342345357\n",
            "step: 330, loss: 0.0010429049143567681\n",
            "step: 340, loss: 0.002311899559572339\n",
            "step: 350, loss: 0.0006570058176293969\n",
            "step: 360, loss: 0.01171304751187563\n",
            "step: 370, loss: 0.0031731559429317713\n",
            "step: 380, loss: 0.00033224382787011564\n",
            "step: 390, loss: 0.0006474981200881302\n",
            "step: 400, loss: 0.0034913201816380024\n",
            "step: 410, loss: 0.2726477384567261\n",
            "step: 420, loss: 0.015048308297991753\n",
            "step: 430, loss: 0.002175979781895876\n",
            "step: 440, loss: 0.003519258927553892\n",
            "step: 450, loss: 0.00026960301329381764\n",
            "step: 460, loss: 0.05244944244623184\n",
            "step: 470, loss: 0.0005386017146520317\n",
            "step: 480, loss: 0.0006639832863584161\n",
            "step: 490, loss: 0.00012737048382405192\n",
            "step: 500, loss: 0.00043377792462706566\n",
            "step: 510, loss: 0.025932354852557182\n",
            "step: 520, loss: 0.00024702560040168464\n",
            "step: 530, loss: 0.009181316941976547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9079008882655447, f1=0.8989710009354537, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012027104385197163\n",
            "step: 10, loss: 0.00044475888716988266\n",
            "step: 20, loss: 0.00025433904374949634\n",
            "step: 30, loss: 0.00010400939936516806\n",
            "step: 40, loss: 0.04024670645594597\n",
            "step: 50, loss: 0.001650461577810347\n",
            "step: 60, loss: 0.0003563447098713368\n",
            "step: 70, loss: 0.012703871354460716\n",
            "step: 80, loss: 0.0006266163545660675\n",
            "step: 90, loss: 0.0070595950819551945\n",
            "step: 100, loss: 0.0012273190077394247\n",
            "step: 110, loss: 0.0013886252418160439\n",
            "step: 120, loss: 0.00013402875629253685\n",
            "step: 130, loss: 0.00021823772112838924\n",
            "step: 140, loss: 0.00046664304682053626\n",
            "step: 150, loss: 0.0009944570483639836\n",
            "step: 160, loss: 9.066024358617142e-05\n",
            "step: 170, loss: 0.0007456735474988818\n",
            "step: 180, loss: 0.03243149444460869\n",
            "step: 190, loss: 0.00017155332898255438\n",
            "step: 200, loss: 0.0024174784775823355\n",
            "step: 210, loss: 0.0009720802772790194\n",
            "step: 220, loss: 0.00030745219555683434\n",
            "step: 230, loss: 0.00047114960034377873\n",
            "step: 240, loss: 9.287094871979207e-05\n",
            "step: 250, loss: 0.01990610547363758\n",
            "step: 260, loss: 0.0010035809827968478\n",
            "step: 270, loss: 0.03073889948427677\n",
            "step: 280, loss: 0.000982161145657301\n",
            "step: 290, loss: 0.010238186456263065\n",
            "step: 300, loss: 0.0036721983924508095\n",
            "step: 310, loss: 0.001142139662988484\n",
            "step: 320, loss: 0.0008982871077023447\n",
            "step: 330, loss: 0.006809975486248732\n",
            "step: 340, loss: 0.0033795060589909554\n",
            "step: 350, loss: 0.0018241096986457705\n",
            "step: 360, loss: 0.0004676792596001178\n",
            "step: 370, loss: 0.003342282259836793\n",
            "step: 380, loss: 0.000341543200192973\n",
            "step: 390, loss: 0.00031583139207214117\n",
            "step: 400, loss: 0.0012647591065615416\n",
            "step: 410, loss: 0.00035454853787086904\n",
            "step: 420, loss: 0.009085875004529953\n",
            "step: 430, loss: 0.000260374799836427\n",
            "step: 440, loss: 0.00016762290033511817\n",
            "step: 450, loss: 0.0011714767897501588\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 460, loss: 0.007588534615933895\n",
            "step: 470, loss: 0.00012571913248393685\n",
            "step: 480, loss: 0.0002861169050447643\n",
            "step: 490, loss: 0.10897193104028702\n",
            "step: 500, loss: 0.01295152772217989\n",
            "step: 510, loss: 0.0014518669340759516\n",
            "step: 520, loss: 0.0016862699994817376\n",
            "step: 530, loss: 5.083536962047219e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9118738404452691, f1=0.9005090236001851, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002138124778866768\n",
            "step: 10, loss: 0.0006059369770810008\n",
            "step: 20, loss: 0.0006647186237387359\n",
            "step: 30, loss: 0.014037054032087326\n",
            "step: 40, loss: 0.00010604114504531026\n",
            "step: 50, loss: 0.01801149547100067\n",
            "step: 60, loss: 0.00028136346372775733\n",
            "step: 70, loss: 6.748777377652004e-05\n",
            "step: 80, loss: 6.999884499236941e-05\n",
            "step: 90, loss: 0.00046857865527272224\n",
            "step: 100, loss: 0.0003007351770065725\n",
            "step: 110, loss: 0.00016637569933664054\n",
            "step: 120, loss: 0.004598225001245737\n",
            "step: 130, loss: 0.0003116761799901724\n",
            "step: 140, loss: 0.0024157315492630005\n",
            "step: 150, loss: 0.0006156233721412718\n",
            "step: 160, loss: 0.0010915981838479638\n",
            "step: 170, loss: 0.001724525704048574\n",
            "step: 180, loss: 0.0011091240448877215\n",
            "step: 190, loss: 0.001470607123337686\n",
            "step: 200, loss: 0.07523947209119797\n",
            "step: 210, loss: 0.00015155003347899765\n",
            "step: 220, loss: 0.0013172010658308864\n",
            "step: 230, loss: 0.003426524344831705\n",
            "step: 240, loss: 0.00010654663492459804\n",
            "step: 250, loss: 0.0013769888319075108\n",
            "step: 260, loss: 7.700416608713567e-05\n",
            "step: 270, loss: 0.005159109365195036\n",
            "step: 280, loss: 0.0019488735124468803\n",
            "step: 290, loss: 0.0009749167365953326\n",
            "step: 300, loss: 0.024365298449993134\n",
            "step: 310, loss: 0.006453773472458124\n",
            "step: 320, loss: 0.05740124732255936\n",
            "step: 330, loss: 0.0009971236577257514\n",
            "step: 340, loss: 0.005010729655623436\n",
            "step: 350, loss: 0.2239256650209427\n",
            "step: 360, loss: 0.0008896042127162218\n",
            "step: 370, loss: 0.0016423359047621489\n",
            "step: 380, loss: 0.00030625335057266057\n",
            "step: 390, loss: 0.13502518832683563\n",
            "step: 400, loss: 0.00013957357441540807\n",
            "step: 410, loss: 0.004173182416707277\n",
            "step: 420, loss: 0.0022506373934447765\n",
            "step: 430, loss: 0.02569248527288437\n",
            "step: 440, loss: 0.00047407718375325203\n",
            "step: 450, loss: 0.00017564267909619957\n",
            "step: 460, loss: 0.0009787128074094653\n",
            "step: 470, loss: 0.0013726719189435244\n",
            "step: 480, loss: 0.0009199604392051697\n",
            "step: 490, loss: 0.005976362619549036\n",
            "step: 500, loss: 0.0002161307929782197\n",
            "step: 510, loss: 0.0002449729945510626\n",
            "step: 520, loss: 7.174871279858053e-05\n",
            "step: 530, loss: 0.00015206815442070365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9017074296262113, f1=0.8963922294172062, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000462401716504246\n",
            "step: 10, loss: 5.76851271034684e-05\n",
            "step: 20, loss: 5.092595165478997e-05\n",
            "step: 30, loss: 0.001057518646121025\n",
            "step: 40, loss: 0.014859489165246487\n",
            "step: 50, loss: 0.004226174205541611\n",
            "step: 60, loss: 0.0008980490383692086\n",
            "step: 70, loss: 0.006500354968011379\n",
            "step: 80, loss: 0.0011043711565434933\n",
            "step: 90, loss: 0.0011087014572694898\n",
            "step: 100, loss: 0.0004045462410431355\n",
            "step: 110, loss: 0.0006413396331481636\n",
            "step: 120, loss: 0.0002971076173707843\n",
            "step: 130, loss: 0.0006593340076506138\n",
            "step: 140, loss: 0.0015317685902118683\n",
            "step: 150, loss: 0.0004147816507611424\n",
            "step: 160, loss: 0.00039257106254808605\n",
            "step: 170, loss: 0.00016289706400129944\n",
            "step: 180, loss: 0.0005404561525210738\n",
            "step: 190, loss: 0.0001479985221521929\n",
            "step: 200, loss: 0.006934932433068752\n",
            "step: 210, loss: 0.0008812513551674783\n",
            "step: 220, loss: 0.00016721953579690307\n",
            "step: 230, loss: 0.0007649441249668598\n",
            "step: 240, loss: 0.001641020062379539\n",
            "step: 250, loss: 0.005277357995510101\n",
            "step: 260, loss: 0.00043320871191099286\n",
            "step: 270, loss: 0.021292220801115036\n",
            "step: 280, loss: 0.08069636672735214\n",
            "step: 290, loss: 0.001815298805013299\n",
            "step: 300, loss: 0.00034012910327874124\n",
            "step: 310, loss: 0.0013114243047311902\n",
            "step: 320, loss: 0.0007006182568147779\n",
            "step: 330, loss: 7.009994442341849e-05\n",
            "step: 340, loss: 0.004431867506355047\n",
            "step: 350, loss: 0.0012508811196312308\n",
            "step: 360, loss: 0.0022211940959095955\n",
            "step: 370, loss: 0.00028842707979492843\n",
            "step: 380, loss: 0.005111939739435911\n",
            "step: 390, loss: 8.855982014210895e-05\n",
            "step: 400, loss: 0.00036457227542996407\n",
            "step: 410, loss: 0.0008507744641974568\n",
            "step: 420, loss: 0.000889709684997797\n",
            "step: 430, loss: 0.0004576884675770998\n",
            "step: 440, loss: 0.0018056154949590564\n",
            "step: 450, loss: 0.0005497379461303353\n",
            "step: 460, loss: 0.09548019617795944\n",
            "step: 470, loss: 7.929639832582325e-05\n",
            "step: 480, loss: 0.0005942209390923381\n",
            "step: 490, loss: 0.017584212124347687\n",
            "step: 500, loss: 0.0006851693615317345\n",
            "step: 510, loss: 0.0008564126910641789\n",
            "step: 520, loss: 0.007644485682249069\n",
            "step: 530, loss: 0.0007515870966017246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.912901723334886, f1=0.9002320185614848, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023486942518502474\n",
            "step: 10, loss: 0.00015160605835262686\n",
            "step: 20, loss: 0.0014265564968809485\n",
            "step: 30, loss: 0.027262559160590172\n",
            "step: 40, loss: 0.0001555993512738496\n",
            "step: 50, loss: 0.0003468201030045748\n",
            "step: 60, loss: 0.0008500547846779227\n",
            "step: 70, loss: 0.00039402872789651155\n",
            "step: 80, loss: 0.008635240606963634\n",
            "step: 90, loss: 0.00012718835205305368\n",
            "step: 100, loss: 0.00011351380089763552\n",
            "step: 110, loss: 0.0012660089414566755\n",
            "step: 120, loss: 0.00233538169413805\n",
            "step: 130, loss: 0.00032158184330910444\n",
            "step: 140, loss: 0.00021973997354507446\n",
            "step: 150, loss: 0.0003119955654256046\n",
            "step: 160, loss: 0.0001303623866988346\n",
            "step: 170, loss: 0.00017776225286070257\n",
            "step: 180, loss: 0.0001828503591241315\n",
            "step: 190, loss: 0.0007190638571046293\n",
            "step: 200, loss: 0.004185650497674942\n",
            "step: 210, loss: 0.0007129572331905365\n",
            "step: 220, loss: 0.04137511923909187\n",
            "step: 230, loss: 0.0014376882463693619\n",
            "step: 240, loss: 7.464802911272272e-05\n",
            "step: 250, loss: 0.02109583094716072\n",
            "step: 260, loss: 9.772894554771483e-05\n",
            "step: 270, loss: 0.0005836858763359487\n",
            "step: 280, loss: 0.0031298946123570204\n",
            "step: 290, loss: 0.0007704122108407319\n",
            "step: 300, loss: 0.001720533473417163\n",
            "step: 310, loss: 0.00029982696287333965\n",
            "step: 320, loss: 0.0009812124771997333\n",
            "step: 330, loss: 0.0005639774026349187\n",
            "step: 340, loss: 0.00024073607346508652\n",
            "step: 350, loss: 0.04685630276799202\n",
            "step: 360, loss: 7.274426752701402e-05\n",
            "step: 370, loss: 0.019682148471474648\n",
            "step: 380, loss: 0.00013376415881793946\n",
            "step: 390, loss: 0.00015560285828541964\n",
            "step: 400, loss: 0.0003104773932136595\n",
            "step: 410, loss: 9.43216509767808e-05\n",
            "step: 420, loss: 0.00018888567865360528\n",
            "step: 430, loss: 0.0003649636637419462\n",
            "step: 440, loss: 0.0005284960498102009\n",
            "step: 450, loss: 7.432911661453545e-05\n",
            "step: 460, loss: 3.725516580743715e-05\n",
            "step: 470, loss: 0.0002152867818949744\n",
            "step: 480, loss: 0.0010838292073458433\n",
            "step: 490, loss: 0.017170101404190063\n",
            "step: 500, loss: 0.0005109385820105672\n",
            "step: 510, loss: 7.287855260074139e-05\n",
            "step: 520, loss: 0.00042660036706365645\n",
            "step: 530, loss: 0.0003526130458340049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9104477611940298, f1=0.8992537313432837, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014881082461215556\n",
            "step: 10, loss: 0.0019480952760204673\n",
            "step: 20, loss: 0.0006239436916075647\n",
            "step: 30, loss: 0.00019384131883271039\n",
            "step: 40, loss: 0.0014435012126341462\n",
            "step: 50, loss: 0.001404112670570612\n",
            "step: 60, loss: 0.0003058084403164685\n",
            "step: 70, loss: 5.983271694276482e-05\n",
            "step: 80, loss: 0.00013337926066014916\n",
            "step: 90, loss: 4.4034306483808905e-05\n",
            "step: 100, loss: 0.00012727915600407869\n",
            "step: 110, loss: 0.00016766710905358195\n",
            "step: 120, loss: 6.945379573153332e-05\n",
            "step: 130, loss: 0.00017696904251351953\n",
            "step: 140, loss: 0.011535638011991978\n",
            "step: 150, loss: 0.0002569708158262074\n",
            "step: 160, loss: 0.0008036610088311136\n",
            "step: 170, loss: 0.021654469892382622\n",
            "step: 180, loss: 7.939052011352032e-05\n",
            "step: 190, loss: 0.0015738976653665304\n",
            "step: 200, loss: 5.4934280342422426e-05\n",
            "step: 210, loss: 0.0010435881558805704\n",
            "step: 220, loss: 0.0026950729079544544\n",
            "step: 230, loss: 0.0001557792566018179\n",
            "step: 240, loss: 0.0006527315126731992\n",
            "step: 250, loss: 5.128556949784979e-05\n",
            "step: 260, loss: 5.976195097900927e-05\n",
            "step: 270, loss: 0.0008545691380277276\n",
            "step: 280, loss: 0.0003716648498084396\n",
            "step: 290, loss: 0.09878689795732498\n",
            "step: 300, loss: 0.0003764159919228405\n",
            "step: 310, loss: 0.0006418786360882223\n",
            "step: 320, loss: 0.0008460166282020509\n",
            "step: 330, loss: 0.00035532668698579073\n",
            "step: 340, loss: 8.888105367077515e-05\n",
            "step: 350, loss: 0.001974627608433366\n",
            "step: 360, loss: 0.0012330625904724002\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 370, loss: 8.421078382525593e-05\n",
            "step: 380, loss: 5.312703797244467e-05\n",
            "step: 390, loss: 0.0013907612301409245\n",
            "step: 400, loss: 0.0002624897169880569\n",
            "step: 410, loss: 0.00018946525233332068\n",
            "step: 420, loss: 0.0012746648862957954\n",
            "step: 430, loss: 0.0021619643084704876\n",
            "step: 440, loss: 8.071947377175093e-05\n",
            "step: 450, loss: 9.342267003376037e-05\n",
            "step: 460, loss: 0.00017856994236353785\n",
            "step: 470, loss: 0.002208513906225562\n",
            "step: 480, loss: 7.309616194106638e-05\n",
            "step: 490, loss: 0.0001725249458104372\n",
            "step: 500, loss: 0.002044475171715021\n",
            "step: 510, loss: 0.0002834609185811132\n",
            "step: 520, loss: 6.167140236357227e-05\n",
            "step: 530, loss: 0.0008737337775528431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9107142857142857, f1=0.8989186647860837, best_f1=0.8968140751307656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000190291742910631\n",
            "step: 10, loss: 0.00017111128545366228\n",
            "step: 20, loss: 0.00036803469993174076\n",
            "step: 30, loss: 0.0001984510599868372\n",
            "step: 40, loss: 0.0001813837152440101\n",
            "step: 50, loss: 0.00012913835234940052\n",
            "step: 60, loss: 5.824149411637336e-05\n",
            "step: 70, loss: 0.00035351610858924687\n",
            "step: 80, loss: 0.00014592535444535315\n",
            "step: 90, loss: 9.114453860092908e-05\n",
            "step: 100, loss: 0.00877261534333229\n",
            "step: 110, loss: 0.008980238810181618\n",
            "step: 120, loss: 0.0035568701568990946\n",
            "step: 130, loss: 0.0031749403569847345\n",
            "step: 140, loss: 8.835265907691792e-05\n",
            "step: 150, loss: 0.00016859632160048932\n",
            "step: 160, loss: 0.00031747852335684\n",
            "step: 170, loss: 0.0007335413829423487\n",
            "step: 180, loss: 0.01063623744994402\n",
            "step: 190, loss: 0.000723932171240449\n",
            "step: 200, loss: 0.0010178685188293457\n",
            "step: 210, loss: 0.004579726606607437\n",
            "step: 220, loss: 0.0001369740639347583\n",
            "step: 230, loss: 0.00037233642069622874\n",
            "step: 240, loss: 0.0003670285805128515\n",
            "step: 250, loss: 0.00011135335080325603\n",
            "step: 260, loss: 0.0008060384425334632\n",
            "step: 270, loss: 0.0004333837714511901\n",
            "step: 280, loss: 0.0002685687504708767\n",
            "step: 290, loss: 0.0009998823516070843\n",
            "step: 300, loss: 0.002437267452478409\n",
            "step: 310, loss: 0.00020827670232392848\n",
            "step: 320, loss: 0.000488179997773841\n",
            "step: 330, loss: 0.00898405909538269\n",
            "step: 340, loss: 0.00037790031637996435\n",
            "step: 350, loss: 0.00034688450978137553\n",
            "step: 360, loss: 0.0019810895901173353\n",
            "step: 370, loss: 0.005147247575223446\n",
            "step: 380, loss: 0.00013838287850376219\n",
            "step: 390, loss: 8.173519745469093e-05\n",
            "step: 400, loss: 0.0007629881147295237\n",
            "step: 410, loss: 0.0002718088508117944\n",
            "step: 420, loss: 0.000141739787068218\n",
            "step: 430, loss: 0.00038167755701579154\n",
            "step: 440, loss: 0.00327146309427917\n",
            "step: 450, loss: 0.00814011599868536\n",
            "step: 460, loss: 0.0007116387714631855\n",
            "step: 470, loss: 9.660298383096233e-05\n",
            "step: 480, loss: 0.0005006479914300144\n",
            "step: 490, loss: 0.00013555549958255142\n",
            "step: 500, loss: 0.006457564886659384\n",
            "step: 510, loss: 0.0001948937715496868\n",
            "step: 520, loss: 0.0006021123263053596\n",
            "step: 530, loss: 8.834028267301619e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9100329722091381, f1=0.8991517436380774, best_f1=0.8968140751307656\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:17, 335.68it/s]\n",
            "load_f1 = 0.9079754601226995\n",
            "real_f1 = 0.908235294117647\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 382.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66684a35-b7d0-4db7-f6a7-63e855602ecd"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8810604214668274\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.30769230769230765, f1=0.37209302325581395, best_f1=0.37209302325581395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37269771099090576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4150943396226415, f1=0.3389830508474576, best_f1=0.3389830508474576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2817763090133667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4666666666666667, f1=0.41935483870967744, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3187721371650696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.49122807017543857, f1=0.41935483870967744, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17136512696743011\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4827586206896552, f1=0.41935483870967744, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25021272897720337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4745762711864407, f1=0.3870967741935483, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1913338452577591\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4897959183673469, f1=0.42857142857142855, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.324727863073349\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4864864864864865, f1=0.43902439024390244, best_f1=0.41935483870967744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16103851795196533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.5, f1=0.4680851063829786, best_f1=0.4680851063829786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24904607236385345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.5128205128205129, f1=0.4680851063829786, best_f1=0.4680851063829786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18968364596366882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.5142857142857143, f1=0.43902439024390244, best_f1=0.43902439024390244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32987794280052185\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.5294117647058824, f1=0.4736842105263159, best_f1=0.4736842105263159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18176516890525818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.5652173913043478, f1=0.43999999999999995, best_f1=0.43999999999999995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2146664410829544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.5777777777777778, f1=0.43999999999999995, best_f1=0.43999999999999995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17742882668972015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5777777777777778, f1=0.43999999999999995, best_f1=0.43999999999999995\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 136656.52it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5365853658536585\n",
            "real_f1 = 0.5777777777777778\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 266.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceec3882-8647-4ba9-a178-9091c390dcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8095021843910217\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49099940061569214\n",
            "step: 20, loss: 0.5960636138916016\n",
            "step: 30, loss: 0.5064984560012817\n",
            "step: 40, loss: 0.4307613670825958\n",
            "step: 50, loss: 0.3751507103443146\n",
            "step: 60, loss: 0.2683727741241455\n",
            "step: 70, loss: 0.39262694120407104\n",
            "step: 80, loss: 0.1327124685049057\n",
            "step: 90, loss: 0.011256000027060509\n",
            "step: 100, loss: 0.26104435324668884\n",
            "step: 110, loss: 0.04822683334350586\n",
            "step: 120, loss: 0.053421854972839355\n",
            "step: 130, loss: 0.019489198923110962\n",
            "step: 140, loss: 0.09274432808160782\n",
            "step: 150, loss: 0.04538514465093613\n",
            "step: 160, loss: 0.1455625742673874\n",
            "step: 170, loss: 0.010842147283256054\n",
            "step: 180, loss: 0.010646922513842583\n",
            "step: 190, loss: 0.1437896341085434\n",
            "step: 200, loss: 0.05546094849705696\n",
            "step: 210, loss: 0.029780222102999687\n",
            "step: 220, loss: 0.01745501533150673\n",
            "step: 230, loss: 0.008367025293409824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9557321225879684, f1=0.9554285714285714, best_f1=0.9554285714285714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004367798566818237\n",
            "step: 10, loss: 0.03353540971875191\n",
            "step: 20, loss: 0.004971010610461235\n",
            "step: 30, loss: 0.0011425254633650184\n",
            "step: 40, loss: 0.005080432631075382\n",
            "step: 50, loss: 0.0020157203543931246\n",
            "step: 60, loss: 0.1196683719754219\n",
            "step: 70, loss: 0.008204689249396324\n",
            "step: 80, loss: 0.02291327528655529\n",
            "step: 90, loss: 0.00970697682350874\n",
            "step: 100, loss: 0.016574684530496597\n",
            "step: 110, loss: 0.06966487318277359\n",
            "step: 120, loss: 0.013650134205818176\n",
            "step: 130, loss: 0.164642333984375\n",
            "step: 140, loss: 0.045285362750291824\n",
            "step: 150, loss: 0.008860521018505096\n",
            "step: 160, loss: 0.10857465118169785\n",
            "step: 170, loss: 0.09959159046411514\n",
            "step: 180, loss: 0.004859778564423323\n",
            "step: 190, loss: 0.07708175480365753\n",
            "step: 200, loss: 0.007122865878045559\n",
            "step: 210, loss: 0.037601739168167114\n",
            "step: 220, loss: 0.0007195539074018598\n",
            "step: 230, loss: 0.17554421722888947\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9717514124293786, f1=0.9692832764505119, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010342473164200783\n",
            "step: 10, loss: 0.020989621058106422\n",
            "step: 20, loss: 0.01991238072514534\n",
            "step: 30, loss: 0.028828037902712822\n",
            "step: 40, loss: 0.030171172693371773\n",
            "step: 50, loss: 0.01925213448703289\n",
            "step: 60, loss: 0.01623339205980301\n",
            "step: 70, loss: 0.0008191907545551658\n",
            "step: 80, loss: 0.0010486090322956443\n",
            "step: 90, loss: 0.00048213719855993986\n",
            "step: 100, loss: 0.0028867674991488457\n",
            "step: 110, loss: 0.015959467738866806\n",
            "step: 120, loss: 0.01997246779501438\n",
            "step: 130, loss: 0.0007165411952883005\n",
            "step: 140, loss: 0.0014893763000145555\n",
            "step: 150, loss: 0.007271651178598404\n",
            "step: 160, loss: 0.012041283771395683\n",
            "step: 170, loss: 0.0011163661256432533\n",
            "step: 180, loss: 0.002453566761687398\n",
            "step: 190, loss: 0.022018255665898323\n",
            "step: 200, loss: 0.002175247063860297\n",
            "step: 210, loss: 0.004550118464976549\n",
            "step: 220, loss: 0.0022017196752130985\n",
            "step: 230, loss: 0.03384935110807419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9651416122004357, f1=0.9616648411829135, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008255647262558341\n",
            "step: 10, loss: 0.0005913946661166847\n",
            "step: 20, loss: 0.0019255843944847584\n",
            "step: 30, loss: 0.00028038170421496034\n",
            "step: 40, loss: 0.02923687919974327\n",
            "step: 50, loss: 0.00414813170209527\n",
            "step: 60, loss: 0.0007002725033089519\n",
            "step: 70, loss: 0.0034014927223324776\n",
            "step: 80, loss: 0.022541899234056473\n",
            "step: 90, loss: 0.1551647186279297\n",
            "step: 100, loss: 0.004664591047912836\n",
            "step: 110, loss: 0.005687100812792778\n",
            "step: 120, loss: 0.025283081457018852\n",
            "step: 130, loss: 0.0025025152135640383\n",
            "step: 140, loss: 0.0047456566244363785\n",
            "step: 150, loss: 0.0004642292915377766\n",
            "step: 160, loss: 0.08452475816011429\n",
            "step: 170, loss: 0.006548622157424688\n",
            "step: 180, loss: 0.10969379544258118\n",
            "step: 190, loss: 0.0067677972838282585\n",
            "step: 200, loss: 0.0029307869262993336\n",
            "step: 210, loss: 0.0015101036988198757\n",
            "step: 220, loss: 0.03269356116652489\n",
            "step: 230, loss: 0.0004594948550220579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9722530521642618, f1=0.968609865470852, best_f1=0.968609865470852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04568169265985489\n",
            "step: 10, loss: 0.007152876816689968\n",
            "step: 20, loss: 0.0008876023930497468\n",
            "step: 30, loss: 0.011012932285666466\n",
            "step: 40, loss: 0.0011533291544765234\n",
            "step: 50, loss: 0.0002543512382544577\n",
            "step: 60, loss: 0.0003088160010520369\n",
            "step: 70, loss: 0.00022674084175378084\n",
            "step: 80, loss: 0.001827168627642095\n",
            "step: 90, loss: 0.0004980132798664272\n",
            "step: 100, loss: 0.010091563686728477\n",
            "step: 110, loss: 0.0005698202294297516\n",
            "step: 120, loss: 0.0675126314163208\n",
            "step: 130, loss: 0.007364946883171797\n",
            "step: 140, loss: 0.00019204147974960506\n",
            "step: 150, loss: 0.005459423176944256\n",
            "step: 160, loss: 0.07894052565097809\n",
            "step: 170, loss: 0.09478235244750977\n",
            "step: 180, loss: 0.08195536583662033\n",
            "step: 190, loss: 0.00030636307201348245\n",
            "step: 200, loss: 0.0041085947304964066\n",
            "step: 210, loss: 0.0004315483383834362\n",
            "step: 220, loss: 0.009645291604101658\n",
            "step: 230, loss: 0.0010685963789001107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9704545454545453, f1=0.9657534246575342, best_f1=0.968609865470852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007320957258343697\n",
            "step: 10, loss: 0.000352820148691535\n",
            "step: 20, loss: 0.0004010794800706208\n",
            "step: 30, loss: 0.00031292744097299874\n",
            "step: 40, loss: 0.0004650783084798604\n",
            "step: 50, loss: 0.029741140082478523\n",
            "step: 60, loss: 0.0011493571801111102\n",
            "step: 70, loss: 0.0004169034364167601\n",
            "step: 80, loss: 0.001190819893963635\n",
            "step: 90, loss: 0.00019684316066559404\n",
            "step: 100, loss: 0.0006179865449666977\n",
            "step: 110, loss: 0.04946840927004814\n",
            "step: 120, loss: 0.0023812621366232634\n",
            "step: 130, loss: 0.00035263493191450834\n",
            "step: 140, loss: 0.0008123043808154762\n",
            "step: 150, loss: 0.00037584779784083366\n",
            "step: 160, loss: 0.0014724740758538246\n",
            "step: 170, loss: 0.0009009978966787457\n",
            "step: 180, loss: 0.00018821733829099685\n",
            "step: 190, loss: 0.022242726758122444\n",
            "step: 200, loss: 0.00019269122276455164\n",
            "step: 210, loss: 0.0004637185775209218\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 220, loss: 0.00036187784280627966\n",
            "step: 230, loss: 0.0001531003217678517\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9774266365688488, f1=0.9692832764505119, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043573617935180664\n",
            "step: 10, loss: 0.0001245813473360613\n",
            "step: 20, loss: 0.00028948503313586116\n",
            "step: 30, loss: 0.0001880656200228259\n",
            "step: 40, loss: 0.008803142234683037\n",
            "step: 50, loss: 0.00021523235773202032\n",
            "step: 60, loss: 0.018398882821202278\n",
            "step: 70, loss: 0.0011313437717035413\n",
            "step: 80, loss: 0.0017683164915069938\n",
            "step: 90, loss: 0.00031129163107834756\n",
            "step: 100, loss: 0.0008642666507512331\n",
            "step: 110, loss: 0.0007700322312302887\n",
            "step: 120, loss: 0.0003938078589271754\n",
            "step: 130, loss: 0.026805022731423378\n",
            "step: 140, loss: 0.001015080721117556\n",
            "step: 150, loss: 0.00013638877135235816\n",
            "step: 160, loss: 0.00010207537707174197\n",
            "step: 170, loss: 0.00010897361062234268\n",
            "step: 180, loss: 0.0003448772768024355\n",
            "step: 190, loss: 0.0002515691739972681\n",
            "step: 200, loss: 0.0030586724169552326\n",
            "step: 210, loss: 0.05064098536968231\n",
            "step: 220, loss: 0.0002957024844363332\n",
            "step: 230, loss: 0.0010921047069132328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9683257918552037, f1=0.9681818181818181, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017497960478067398\n",
            "step: 10, loss: 0.02016899548470974\n",
            "step: 20, loss: 0.0009388226317241788\n",
            "step: 30, loss: 0.00023422205413226038\n",
            "step: 40, loss: 0.00046026622294448316\n",
            "step: 50, loss: 0.01647346466779709\n",
            "step: 60, loss: 0.0009311004541814327\n",
            "step: 70, loss: 0.00047745267511345446\n",
            "step: 80, loss: 0.0005261673941276968\n",
            "step: 90, loss: 0.00013017178571317345\n",
            "step: 100, loss: 0.00028860237216576934\n",
            "step: 110, loss: 0.0005678950692526996\n",
            "step: 120, loss: 0.00026227973285131156\n",
            "step: 130, loss: 0.06321340054273605\n",
            "step: 140, loss: 0.003905740100890398\n",
            "step: 150, loss: 0.0036368174478411674\n",
            "step: 160, loss: 0.00046321743866428733\n",
            "step: 170, loss: 0.00013595697237178683\n",
            "step: 180, loss: 0.0013972397428005934\n",
            "step: 190, loss: 6.666184344794601e-05\n",
            "step: 200, loss: 0.0012592484708875418\n",
            "step: 210, loss: 7.830945105524734e-05\n",
            "step: 220, loss: 0.0003364429867360741\n",
            "step: 230, loss: 0.01165031362324953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9717514124293786, f1=0.9642058165548099, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.61928056110628e-05\n",
            "step: 10, loss: 0.00019584069377742708\n",
            "step: 20, loss: 0.003074036445468664\n",
            "step: 30, loss: 0.00019068876281380653\n",
            "step: 40, loss: 6.701546953991055e-05\n",
            "step: 50, loss: 0.0017663240432739258\n",
            "step: 60, loss: 0.00011286233348073438\n",
            "step: 70, loss: 0.00016469792171847075\n",
            "step: 80, loss: 0.07771419733762741\n",
            "step: 90, loss: 0.024936283007264137\n",
            "step: 100, loss: 0.0003369305923115462\n",
            "step: 110, loss: 0.000569167488720268\n",
            "step: 120, loss: 0.0014942717971280217\n",
            "step: 130, loss: 0.0006292868056334555\n",
            "step: 140, loss: 0.0033468659967184067\n",
            "step: 150, loss: 0.00010209911124547943\n",
            "step: 160, loss: 0.000371790025383234\n",
            "step: 170, loss: 6.259901419980451e-05\n",
            "step: 180, loss: 0.00029134302167221904\n",
            "step: 190, loss: 7.274506788235158e-05\n",
            "step: 200, loss: 0.00010664394358173013\n",
            "step: 210, loss: 0.00015103307669050992\n",
            "step: 220, loss: 0.025855015963315964\n",
            "step: 230, loss: 0.026528995484113693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9764309764309763, f1=0.967670011148272, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008945954032242298\n",
            "step: 10, loss: 0.00013343436876311898\n",
            "step: 20, loss: 0.00023243714531417936\n",
            "step: 30, loss: 0.001943072653375566\n",
            "step: 40, loss: 0.0001978681975742802\n",
            "step: 50, loss: 0.0008106480818241835\n",
            "step: 60, loss: 0.00024751623277552426\n",
            "step: 70, loss: 0.0005343835800886154\n",
            "step: 80, loss: 0.00024610402761027217\n",
            "step: 90, loss: 0.00018754773191176355\n",
            "step: 100, loss: 0.00020167061302345246\n",
            "step: 110, loss: 0.0005441093817353249\n",
            "step: 120, loss: 0.01400141790509224\n",
            "step: 130, loss: 0.00024450934142805636\n",
            "step: 140, loss: 0.01025464292615652\n",
            "step: 150, loss: 0.00019234670617152005\n",
            "step: 160, loss: 0.0009165886440314353\n",
            "step: 170, loss: 0.0030805584974586964\n",
            "step: 180, loss: 0.0006950658280402422\n",
            "step: 190, loss: 0.0003657250781543553\n",
            "step: 200, loss: 0.00019664267892949283\n",
            "step: 210, loss: 0.00012822322605643421\n",
            "step: 220, loss: 0.0006324393907561898\n",
            "step: 230, loss: 0.0001225801242981106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.967816091954023, f1=0.960919540229885, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010168163571506739\n",
            "step: 10, loss: 0.0001764857879607007\n",
            "step: 20, loss: 0.00019733862427528948\n",
            "step: 30, loss: 0.003253050148487091\n",
            "step: 40, loss: 0.0010535051114857197\n",
            "step: 50, loss: 0.0002587676281109452\n",
            "step: 60, loss: 0.00019639445235952735\n",
            "step: 70, loss: 0.0003642791125457734\n",
            "step: 80, loss: 0.022416753694415092\n",
            "step: 90, loss: 0.00011439065565355122\n",
            "step: 100, loss: 0.0002961480349767953\n",
            "step: 110, loss: 7.803313928889111e-05\n",
            "step: 120, loss: 0.0003329305036459118\n",
            "step: 130, loss: 8.697158773429692e-05\n",
            "step: 140, loss: 9.134867286775261e-05\n",
            "step: 150, loss: 7.434211147483438e-05\n",
            "step: 160, loss: 0.00014240120071917772\n",
            "step: 170, loss: 0.015913913026452065\n",
            "step: 180, loss: 0.002133206697180867\n",
            "step: 190, loss: 0.0006181501084938645\n",
            "step: 200, loss: 9.995193977374583e-05\n",
            "step: 210, loss: 5.829624205944128e-05\n",
            "step: 220, loss: 0.00010778279829537496\n",
            "step: 230, loss: 0.020570049062371254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9725400457665903, f1=0.9692132269099202, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005360727082006633\n",
            "step: 10, loss: 0.009734324179589748\n",
            "step: 20, loss: 0.00012383754074107856\n",
            "step: 30, loss: 0.0003043217584490776\n",
            "step: 40, loss: 8.5668740211986e-05\n",
            "step: 50, loss: 0.00011376035399734974\n",
            "step: 60, loss: 0.0014036301290616393\n",
            "step: 70, loss: 8.478956442559138e-05\n",
            "step: 80, loss: 0.00011083375284215435\n",
            "step: 90, loss: 7.265230669872835e-05\n",
            "step: 100, loss: 0.00011798907507909462\n",
            "step: 110, loss: 0.02164800837635994\n",
            "step: 120, loss: 0.00010515658505028114\n",
            "step: 130, loss: 0.00047696538968011737\n",
            "step: 140, loss: 0.0002155550755560398\n",
            "step: 150, loss: 0.00039677569293417037\n",
            "step: 160, loss: 0.0027028352487832308\n",
            "step: 170, loss: 0.00010079027561005205\n",
            "step: 180, loss: 8.554793748771772e-05\n",
            "step: 190, loss: 6.567966192960739e-05\n",
            "step: 200, loss: 0.0008470629109069705\n",
            "step: 210, loss: 0.00010585062409518287\n",
            "step: 220, loss: 0.0003305162244942039\n",
            "step: 230, loss: 0.0038590095937252045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9730337078651685, f1=0.9707207207207207, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.956710760481656e-05\n",
            "step: 10, loss: 8.45703252707608e-05\n",
            "step: 20, loss: 0.00010371749522164464\n",
            "step: 30, loss: 0.0002568837662693113\n",
            "step: 40, loss: 0.00036152213579043746\n",
            "step: 50, loss: 0.0002757285546977073\n",
            "step: 60, loss: 0.00013439462054520845\n",
            "step: 70, loss: 7.841759361326694e-05\n",
            "step: 80, loss: 0.00020876598136965185\n",
            "step: 90, loss: 0.0002788228157442063\n",
            "step: 100, loss: 0.0011401857482269406\n",
            "step: 110, loss: 0.00010261332499794662\n",
            "step: 120, loss: 6.881565059302375e-05\n",
            "step: 130, loss: 0.00013166472490411252\n",
            "step: 140, loss: 0.00011378740600775927\n",
            "step: 150, loss: 0.00014058142551220953\n",
            "step: 160, loss: 6.282459798967466e-05\n",
            "step: 170, loss: 6.331045733531937e-05\n",
            "step: 180, loss: 0.0003601904318202287\n",
            "step: 190, loss: 5.539977064472623e-05\n",
            "step: 200, loss: 8.334004814969376e-05\n",
            "step: 210, loss: 0.00016017432790249586\n",
            "step: 220, loss: 4.881689164903946e-05\n",
            "step: 230, loss: 4.664199150283821e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9750566893424036, f1=0.9705882352941176, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.650365634821355e-05\n",
            "step: 10, loss: 7.874649600125849e-05\n",
            "step: 20, loss: 0.00025511422427371144\n",
            "step: 30, loss: 0.0003823733713943511\n",
            "step: 40, loss: 7.432346319546923e-05\n",
            "step: 50, loss: 7.892218854976818e-05\n",
            "step: 60, loss: 4.672251452575438e-05\n",
            "step: 70, loss: 7.993623148649931e-05\n",
            "step: 80, loss: 8.662602340336889e-05\n",
            "step: 90, loss: 9.806223533814773e-05\n",
            "step: 100, loss: 0.0031543441582471132\n",
            "step: 110, loss: 0.0006376567762345076\n",
            "step: 120, loss: 0.008804451674222946\n",
            "step: 130, loss: 9.312728070653975e-05\n",
            "step: 140, loss: 5.9377918660175055e-05\n",
            "step: 150, loss: 0.00014296693552751094\n",
            "step: 160, loss: 7.394857675535604e-05\n",
            "step: 170, loss: 8.056403748923913e-05\n",
            "step: 180, loss: 0.000137229886604473\n",
            "step: 190, loss: 0.0004865280934609473\n",
            "step: 200, loss: 4.574049307848327e-05\n",
            "step: 210, loss: 0.0020432595629245043\n",
            "step: 220, loss: 9.504299669060856e-05\n",
            "step: 230, loss: 2.8870339519926347e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.976324689966178, f1=0.971815107102593, best_f1=0.9692832764505119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002699043252505362\n",
            "step: 10, loss: 9.047111234394833e-05\n",
            "step: 20, loss: 0.00013319196295924485\n",
            "step: 30, loss: 0.00013353924441616982\n",
            "step: 40, loss: 0.002006364520639181\n",
            "step: 50, loss: 0.00014816429757047445\n",
            "step: 60, loss: 5.514195072464645e-05\n",
            "step: 70, loss: 0.0008478689123876393\n",
            "step: 80, loss: 6.724436389049515e-05\n",
            "step: 90, loss: 3.5120563552482054e-05\n",
            "step: 100, loss: 9.193797450279817e-05\n",
            "step: 110, loss: 5.842695463798009e-05\n",
            "step: 120, loss: 0.00018077658023685217\n",
            "step: 130, loss: 7.100868242559955e-05\n",
            "step: 140, loss: 0.00010097958875121549\n",
            "step: 150, loss: 9.13442563614808e-05\n",
            "step: 160, loss: 4.154968337388709e-05\n",
            "step: 170, loss: 4.0186198020819575e-05\n",
            "step: 180, loss: 5.5517088185297325e-05\n",
            "step: 190, loss: 0.011469007469713688\n",
            "step: 200, loss: 9.634863818064332e-05\n",
            "step: 210, loss: 0.02523796446621418\n",
            "step: 220, loss: 0.0001206581509904936\n",
            "step: 230, loss: 8.592524682171643e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9752252252252253, f1=0.971815107102593, best_f1=0.9692832764505119\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:10, 237.20it/s]\n",
            "load_f1 = 0.9739524348810873\n",
            "real_f1 = 0.9774266365688488\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 261.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1042118-8906-43d2-ddd1-6345e19e4ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7957703471183777\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.45284727215766907\n",
            "step: 20, loss: 0.49903640151023865\n",
            "step: 30, loss: 0.464160680770874\n",
            "step: 40, loss: 0.4749623239040375\n",
            "step: 50, loss: 0.4543660879135132\n",
            "step: 60, loss: 0.5257434248924255\n",
            "step: 70, loss: 0.21057423949241638\n",
            "step: 80, loss: 0.18718047440052032\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.2569151222705841\n",
            "step: 100, loss: 0.15560761094093323\n",
            "step: 110, loss: 0.11373285949230194\n",
            "step: 120, loss: 0.10693087428808212\n",
            "step: 130, loss: 0.052636709064245224\n",
            "step: 140, loss: 0.2502637803554535\n",
            "step: 150, loss: 0.07146061956882477\n",
            "step: 160, loss: 0.17828932404518127\n",
            "step: 170, loss: 0.3813953697681427\n",
            "step: 180, loss: 0.18677741289138794\n",
            "step: 190, loss: 0.0780547708272934\n",
            "step: 200, loss: 0.09362553060054779\n",
            "step: 210, loss: 0.10203146934509277\n",
            "step: 220, loss: 0.12706942856311798\n",
            "step: 230, loss: 0.06848398596048355\n",
            "step: 240, loss: 0.11413244903087616\n",
            "step: 250, loss: 0.10981154441833496\n",
            "step: 260, loss: 0.027040736749768257\n",
            "step: 270, loss: 0.05673886835575104\n",
            "step: 280, loss: 0.12142543494701385\n",
            "step: 290, loss: 0.16390518844127655\n",
            "step: 300, loss: 0.1296723186969757\n",
            "step: 310, loss: 0.13384319841861725\n",
            "step: 320, loss: 0.20265375077724457\n",
            "step: 330, loss: 0.19623322784900665\n",
            "step: 340, loss: 0.18340131640434265\n",
            "step: 350, loss: 0.07456284761428833\n",
            "step: 360, loss: 0.0732649713754654\n",
            "step: 370, loss: 0.15772028267383575\n",
            "step: 380, loss: 0.1728438287973404\n",
            "step: 390, loss: 0.23406672477722168\n",
            "step: 400, loss: 0.03752248361706734\n",
            "step: 410, loss: 0.09542381763458252\n",
            "step: 420, loss: 0.053496211767196655\n",
            "step: 430, loss: 0.11102361977100372\n",
            "step: 440, loss: 0.22999562323093414\n",
            "step: 450, loss: 0.05922868102788925\n",
            "step: 460, loss: 0.05790559947490692\n",
            "step: 470, loss: 0.21066737174987793\n",
            "step: 480, loss: 0.21064049005508423\n",
            "step: 490, loss: 0.06794645637273788\n",
            "step: 500, loss: 0.022641710937023163\n",
            "step: 510, loss: 0.05514945462346077\n",
            "step: 520, loss: 0.06730180233716965\n",
            "step: 530, loss: 0.21041423082351685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8935969868173258, f1=0.8918156161806209, best_f1=0.8918156161806209\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1963111162185669\n",
            "step: 10, loss: 0.1030094102025032\n",
            "step: 20, loss: 0.09878762066364288\n",
            "step: 30, loss: 0.08026096969842911\n",
            "step: 40, loss: 0.032393842935562134\n",
            "step: 50, loss: 0.02487899363040924\n",
            "step: 60, loss: 0.0930054634809494\n",
            "step: 70, loss: 0.23025423288345337\n",
            "step: 80, loss: 0.030764468014240265\n",
            "step: 90, loss: 0.03229852393269539\n",
            "step: 100, loss: 0.2747279405593872\n",
            "step: 110, loss: 0.07866419851779938\n",
            "step: 120, loss: 0.06940712779760361\n",
            "step: 130, loss: 0.06262057274580002\n",
            "step: 140, loss: 0.019640129059553146\n",
            "step: 150, loss: 0.05670788511633873\n",
            "step: 160, loss: 0.10363828390836716\n",
            "step: 170, loss: 0.07501628249883652\n",
            "step: 180, loss: 0.07787946611642838\n",
            "step: 190, loss: 0.055625706911087036\n",
            "step: 200, loss: 0.08521855622529984\n",
            "step: 210, loss: 0.0995684266090393\n",
            "step: 220, loss: 0.099794901907444\n",
            "step: 230, loss: 0.1299094706773758\n",
            "step: 240, loss: 0.10784344375133514\n",
            "step: 250, loss: 0.08131592720746994\n",
            "step: 260, loss: 0.02262706682085991\n",
            "step: 270, loss: 0.07827462255954742\n",
            "step: 280, loss: 0.08065181970596313\n",
            "step: 290, loss: 0.0597449466586113\n",
            "step: 300, loss: 0.025014888495206833\n",
            "step: 310, loss: 0.044829919934272766\n",
            "step: 320, loss: 0.16546858847141266\n",
            "step: 330, loss: 0.04514255002140999\n",
            "step: 340, loss: 0.08158759772777557\n",
            "step: 350, loss: 0.08476370573043823\n",
            "step: 360, loss: 0.09433554857969284\n",
            "step: 370, loss: 0.03601652756333351\n",
            "step: 380, loss: 0.09262759238481522\n",
            "step: 390, loss: 0.08177245408296585\n",
            "step: 400, loss: 0.08067094534635544\n",
            "step: 410, loss: 0.01656956411898136\n",
            "step: 420, loss: 0.0654192790389061\n",
            "step: 430, loss: 0.05056837573647499\n",
            "step: 440, loss: 0.016609016805887222\n",
            "step: 450, loss: 0.19314740598201752\n",
            "step: 460, loss: 0.2406437247991562\n",
            "step: 470, loss: 0.03825636953115463\n",
            "step: 480, loss: 0.22861172258853912\n",
            "step: 490, loss: 0.08342192322015762\n",
            "step: 500, loss: 0.02450701966881752\n",
            "step: 510, loss: 0.1610947847366333\n",
            "step: 520, loss: 0.040629882365465164\n",
            "step: 530, loss: 0.18069520592689514\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9060402684563759, f1=0.8983700862895493, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026418542489409447\n",
            "step: 10, loss: 0.16480779647827148\n",
            "step: 20, loss: 0.04013039916753769\n",
            "step: 30, loss: 0.2535559833049774\n",
            "step: 40, loss: 0.0194233488291502\n",
            "step: 50, loss: 0.018177587538957596\n",
            "step: 60, loss: 0.024339376017451286\n",
            "step: 70, loss: 0.012224413454532623\n",
            "step: 80, loss: 0.06874936819076538\n",
            "step: 90, loss: 0.2440304458141327\n",
            "step: 100, loss: 0.009478501975536346\n",
            "step: 110, loss: 0.011974828317761421\n",
            "step: 120, loss: 0.008110460825264454\n",
            "step: 130, loss: 0.00677523436024785\n",
            "step: 140, loss: 0.15567344427108765\n",
            "step: 150, loss: 0.050797611474990845\n",
            "step: 160, loss: 0.06841482222080231\n",
            "step: 170, loss: 0.12403298169374466\n",
            "step: 180, loss: 0.10727682709693909\n",
            "step: 190, loss: 0.009281753562390804\n",
            "step: 200, loss: 0.09812574833631516\n",
            "step: 210, loss: 0.1079275980591774\n",
            "step: 220, loss: 0.018675226718187332\n",
            "step: 230, loss: 0.0064268559217453\n",
            "step: 240, loss: 0.009542834013700485\n",
            "step: 250, loss: 0.029279964044690132\n",
            "step: 260, loss: 0.015136636793613434\n",
            "step: 270, loss: 0.012293776497244835\n",
            "step: 280, loss: 0.13606972992420197\n",
            "step: 290, loss: 0.0661509782075882\n",
            "step: 300, loss: 0.0434640571475029\n",
            "step: 310, loss: 0.0975421667098999\n",
            "step: 320, loss: 0.013728848658502102\n",
            "step: 330, loss: 0.015777939930558205\n",
            "step: 340, loss: 0.010419540107250214\n",
            "step: 350, loss: 0.0661400556564331\n",
            "step: 360, loss: 0.008326717652380466\n",
            "step: 370, loss: 0.018391352146863937\n",
            "step: 380, loss: 0.02224419079720974\n",
            "step: 390, loss: 0.05845740810036659\n",
            "step: 400, loss: 0.06091609597206116\n",
            "step: 410, loss: 0.1221604123711586\n",
            "step: 420, loss: 0.1062026172876358\n",
            "step: 430, loss: 0.047148317098617554\n",
            "step: 440, loss: 0.13498428463935852\n",
            "step: 450, loss: 0.2002817541360855\n",
            "step: 460, loss: 0.10943828523159027\n",
            "step: 470, loss: 0.01735713705420494\n",
            "step: 480, loss: 0.04762881249189377\n",
            "step: 490, loss: 0.04348323121666908\n",
            "step: 500, loss: 0.023896627128124237\n",
            "step: 510, loss: 0.008806592784821987\n",
            "step: 520, loss: 0.07922468334436417\n",
            "step: 530, loss: 0.00988561287522316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9032558139534883, f1=0.9028625058657908, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007284555584192276\n",
            "step: 10, loss: 0.0026664421893656254\n",
            "step: 20, loss: 0.022051256150007248\n",
            "step: 30, loss: 0.004081401042640209\n",
            "step: 40, loss: 0.019137412309646606\n",
            "step: 50, loss: 0.03605726733803749\n",
            "step: 60, loss: 0.005476204212754965\n",
            "step: 70, loss: 0.004785778000950813\n",
            "step: 80, loss: 0.006453245412558317\n",
            "step: 90, loss: 0.019534483551979065\n",
            "step: 100, loss: 0.08591578900814056\n",
            "step: 110, loss: 0.0022644998971372843\n",
            "step: 120, loss: 0.00463572284206748\n",
            "step: 130, loss: 0.04559957981109619\n",
            "step: 140, loss: 0.03538719564676285\n",
            "step: 150, loss: 0.008724336512386799\n",
            "step: 160, loss: 0.0045746006071567535\n",
            "step: 170, loss: 0.002844797680154443\n",
            "step: 180, loss: 0.011919708922505379\n",
            "step: 190, loss: 0.0010054142912849784\n",
            "step: 200, loss: 0.01815889962017536\n",
            "step: 210, loss: 0.15489964187145233\n",
            "step: 220, loss: 0.06370159238576889\n",
            "step: 230, loss: 0.44903379678726196\n",
            "step: 240, loss: 0.015547635033726692\n",
            "step: 250, loss: 0.004893516656011343\n",
            "step: 260, loss: 0.1590399444103241\n",
            "step: 270, loss: 0.14315423369407654\n",
            "step: 280, loss: 0.0014167819172143936\n",
            "step: 290, loss: 0.1046871691942215\n",
            "step: 300, loss: 0.015232881531119347\n",
            "step: 310, loss: 0.004540650174021721\n",
            "step: 320, loss: 0.008483648300170898\n",
            "step: 330, loss: 0.08558277040719986\n",
            "step: 340, loss: 0.08373722434043884\n",
            "step: 350, loss: 0.021801909431815147\n",
            "step: 360, loss: 0.12851890921592712\n",
            "step: 370, loss: 0.024300893768668175\n",
            "step: 380, loss: 0.008220350369811058\n",
            "step: 390, loss: 0.009879604913294315\n",
            "step: 400, loss: 0.018951255828142166\n",
            "step: 410, loss: 0.02749033086001873\n",
            "step: 420, loss: 0.033634815365076065\n",
            "step: 430, loss: 0.005339920520782471\n",
            "step: 440, loss: 0.04988139495253563\n",
            "step: 450, loss: 0.03003094717860222\n",
            "step: 460, loss: 0.0012233840534463525\n",
            "step: 470, loss: 0.049025651067495346\n",
            "step: 480, loss: 0.0069290874525904655\n",
            "step: 490, loss: 0.03689606860280037\n",
            "step: 500, loss: 0.012131918221712112\n",
            "step: 510, loss: 0.15430089831352234\n",
            "step: 520, loss: 0.1280411332845688\n",
            "step: 530, loss: 0.002400639932602644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9011600928074246, f1=0.8970656730321379, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011557993479073048\n",
            "step: 10, loss: 0.012165124528110027\n",
            "step: 20, loss: 0.004259068053215742\n",
            "step: 30, loss: 0.003912209067493677\n",
            "step: 40, loss: 0.10309725999832153\n",
            "step: 50, loss: 0.020277656614780426\n",
            "step: 60, loss: 0.005290071479976177\n",
            "step: 70, loss: 0.0008052595076151192\n",
            "step: 80, loss: 0.0008398643112741411\n",
            "step: 90, loss: 0.01754801906645298\n",
            "step: 100, loss: 0.0008760356577113271\n",
            "step: 110, loss: 0.004788028076291084\n",
            "step: 120, loss: 0.05739276483654976\n",
            "step: 130, loss: 0.0013131623854860663\n",
            "step: 140, loss: 0.0019485818920657039\n",
            "step: 150, loss: 0.0017627287888899446\n",
            "step: 160, loss: 0.04007219523191452\n",
            "step: 170, loss: 0.006592475343495607\n",
            "step: 180, loss: 0.0011561164865270257\n",
            "step: 190, loss: 0.005392597988247871\n",
            "step: 200, loss: 0.034529201686382294\n",
            "step: 210, loss: 0.00539179053157568\n",
            "step: 220, loss: 0.0007385583594441414\n",
            "step: 230, loss: 0.000662448292132467\n",
            "step: 240, loss: 0.00981524121016264\n",
            "step: 250, loss: 0.00039553618989884853\n",
            "step: 260, loss: 0.006691875867545605\n",
            "step: 270, loss: 0.02896358259022236\n",
            "step: 280, loss: 0.14538820087909698\n",
            "step: 290, loss: 0.002648907946422696\n",
            "step: 300, loss: 0.01401901338249445\n",
            "step: 310, loss: 0.0043207574635744095\n",
            "step: 320, loss: 0.08957386016845703\n",
            "step: 330, loss: 0.00210730847902596\n",
            "step: 340, loss: 0.0038970480673015118\n",
            "step: 350, loss: 0.005027954000979662\n",
            "step: 360, loss: 0.15959274768829346\n",
            "step: 370, loss: 0.0033750219736248255\n",
            "step: 380, loss: 0.02418803982436657\n",
            "step: 390, loss: 0.04768470674753189\n",
            "step: 400, loss: 0.09555517882108688\n",
            "step: 410, loss: 0.0005290394183248281\n",
            "step: 420, loss: 0.0015727889258414507\n",
            "step: 430, loss: 0.0027230659034103155\n",
            "step: 440, loss: 0.10737797617912292\n",
            "step: 450, loss: 0.0036282793153077364\n",
            "step: 460, loss: 0.0013130366569384933\n",
            "step: 470, loss: 0.09026187658309937\n",
            "step: 480, loss: 0.011017585173249245\n",
            "step: 490, loss: 0.012628080323338509\n",
            "step: 500, loss: 0.03693442419171333\n",
            "step: 510, loss: 0.031206514686346054\n",
            "step: 520, loss: 0.003847497282549739\n",
            "step: 530, loss: 0.009805646724998951\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9026303645592986, f1=0.8993973110802038, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016490183770656586\n",
            "step: 10, loss: 0.006425876170396805\n",
            "step: 20, loss: 0.000981492456048727\n",
            "step: 30, loss: 0.000986986793577671\n",
            "step: 40, loss: 0.0033893270883709192\n",
            "step: 50, loss: 0.002752600936219096\n",
            "step: 60, loss: 0.0020274363923817873\n",
            "step: 70, loss: 0.015113738365471363\n",
            "step: 80, loss: 0.000458858092315495\n",
            "step: 90, loss: 0.00027631883858703077\n",
            "step: 100, loss: 0.07961740344762802\n",
            "step: 110, loss: 0.002555467654019594\n",
            "step: 120, loss: 0.011166478507220745\n",
            "step: 130, loss: 0.0007631295011378825\n",
            "step: 140, loss: 0.002200909424573183\n",
            "step: 150, loss: 0.005093386396765709\n",
            "step: 160, loss: 0.0004060078936163336\n",
            "step: 170, loss: 0.00023183797020465136\n",
            "step: 180, loss: 0.0004532107268460095\n",
            "step: 190, loss: 0.03447427973151207\n",
            "step: 200, loss: 0.00019249532488174736\n",
            "step: 210, loss: 0.0002603463944979012\n",
            "step: 220, loss: 0.0011904468992725015\n",
            "step: 230, loss: 0.05130118131637573\n",
            "step: 240, loss: 0.034498460590839386\n",
            "step: 250, loss: 0.007509704679250717\n",
            "step: 260, loss: 0.016432203352451324\n",
            "step: 270, loss: 0.03510946035385132\n",
            "step: 280, loss: 0.023546144366264343\n",
            "step: 290, loss: 0.07066531479358673\n",
            "step: 300, loss: 0.024523958563804626\n",
            "step: 310, loss: 0.004835509229451418\n",
            "step: 320, loss: 0.1976853758096695\n",
            "step: 330, loss: 0.06634149700403214\n",
            "step: 340, loss: 0.013279481790959835\n",
            "step: 350, loss: 0.16070085763931274\n",
            "step: 360, loss: 0.00036723876837641\n",
            "step: 370, loss: 0.17868678271770477\n",
            "step: 380, loss: 0.008367855101823807\n",
            "step: 390, loss: 0.1121731624007225\n",
            "step: 400, loss: 0.0010164215927943587\n",
            "step: 410, loss: 0.0046013458631932735\n",
            "step: 420, loss: 0.009226629510521889\n",
            "step: 430, loss: 0.0011221240274608135\n",
            "step: 440, loss: 0.0049110655672848225\n",
            "step: 450, loss: 0.0009276081109419465\n",
            "step: 460, loss: 0.20466193556785583\n",
            "step: 470, loss: 0.004761855583637953\n",
            "step: 480, loss: 0.013506026938557625\n",
            "step: 490, loss: 0.004001706838607788\n",
            "step: 500, loss: 0.008353087119758129\n",
            "step: 510, loss: 0.02723833918571472\n",
            "step: 520, loss: 0.0010374002158641815\n",
            "step: 530, loss: 0.01313795242458582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8908418131359852, f1=0.8912442396313364, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07333767414093018\n",
            "step: 10, loss: 0.01973562128841877\n",
            "step: 20, loss: 0.003330631647258997\n",
            "step: 30, loss: 0.005626286379992962\n",
            "step: 40, loss: 0.0024563393089920282\n",
            "step: 50, loss: 0.009741359390318394\n",
            "step: 60, loss: 0.0002642696490511298\n",
            "step: 70, loss: 0.00047280528815463185\n",
            "step: 80, loss: 0.000455594650702551\n",
            "step: 90, loss: 0.0016369371442124248\n",
            "step: 100, loss: 0.0021601072512567043\n",
            "step: 110, loss: 0.0008366628317162395\n",
            "step: 120, loss: 0.002632472664117813\n",
            "step: 130, loss: 0.00021598349849227816\n",
            "step: 140, loss: 0.0009831400820985436\n",
            "step: 150, loss: 0.00019168941071256995\n",
            "step: 160, loss: 0.0005799129721708596\n",
            "step: 170, loss: 0.0004290013457648456\n",
            "step: 180, loss: 0.001221152488142252\n",
            "step: 190, loss: 0.0004218678514007479\n",
            "step: 200, loss: 0.05612535774707794\n",
            "step: 210, loss: 0.0014962140703573823\n",
            "step: 220, loss: 0.0015119112795218825\n",
            "step: 230, loss: 0.0008806049008853734\n",
            "step: 240, loss: 0.006690976209938526\n",
            "step: 250, loss: 0.0008193666581064463\n",
            "step: 260, loss: 0.001178559847176075\n",
            "step: 270, loss: 0.00017111508350353688\n",
            "step: 280, loss: 0.010083788074553013\n",
            "step: 290, loss: 0.005692333914339542\n",
            "step: 300, loss: 0.0005297386669553816\n",
            "step: 310, loss: 0.0004740360891446471\n",
            "step: 320, loss: 0.0039343442767858505\n",
            "step: 330, loss: 0.0008625040063634515\n",
            "step: 340, loss: 0.18687357008457184\n",
            "step: 350, loss: 0.07974398881196976\n",
            "step: 360, loss: 0.006429252214729786\n",
            "step: 370, loss: 0.0004858885076828301\n",
            "step: 380, loss: 0.0019650179892778397\n",
            "step: 390, loss: 0.0007177767110988498\n",
            "step: 400, loss: 0.0634099468588829\n",
            "step: 410, loss: 0.0006031243246980011\n",
            "step: 420, loss: 0.0005333191948011518\n",
            "step: 430, loss: 0.00021373962226789445\n",
            "step: 440, loss: 0.00045395290362648666\n",
            "step: 450, loss: 0.0018471889197826385\n",
            "step: 460, loss: 0.0027088383212685585\n",
            "step: 470, loss: 0.00262046093121171\n",
            "step: 480, loss: 0.0019147660350427032\n",
            "step: 490, loss: 0.003951144404709339\n",
            "step: 500, loss: 0.0005281392950564623\n",
            "step: 510, loss: 0.00796863529831171\n",
            "step: 520, loss: 0.069069042801857\n",
            "step: 530, loss: 0.02165716327726841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8995835261453031, f1=0.9022140221402214, best_f1=0.8983700862895493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012072538025677204\n",
            "step: 10, loss: 0.014736533164978027\n",
            "step: 20, loss: 0.0010048802942037582\n",
            "step: 30, loss: 0.0004144013801123947\n",
            "step: 40, loss: 0.003550904802978039\n",
            "step: 50, loss: 0.0027104048058390617\n",
            "step: 60, loss: 0.008827907033264637\n",
            "step: 70, loss: 0.07480009645223618\n",
            "step: 80, loss: 0.000330368522554636\n",
            "step: 90, loss: 0.0006908816867507994\n",
            "step: 100, loss: 0.0009387299069203436\n",
            "step: 110, loss: 0.0017630885122343898\n",
            "step: 120, loss: 0.002282259054481983\n",
            "step: 130, loss: 0.000237695625401102\n",
            "step: 140, loss: 0.004948015790432692\n",
            "step: 150, loss: 0.00044532708125188947\n",
            "step: 160, loss: 0.0003884357283823192\n",
            "step: 170, loss: 0.00017414231842849404\n",
            "step: 180, loss: 0.00016018351016100496\n",
            "step: 190, loss: 0.02655237913131714\n",
            "step: 200, loss: 0.0003371702623553574\n",
            "step: 210, loss: 0.17466062307357788\n",
            "step: 220, loss: 0.006897317245602608\n",
            "step: 230, loss: 0.00022241054102778435\n",
            "step: 240, loss: 0.012574972584843636\n",
            "step: 250, loss: 0.0003925265045836568\n",
            "step: 260, loss: 0.03526874631643295\n",
            "step: 270, loss: 0.0002336531615583226\n",
            "step: 280, loss: 0.00029168519540689886\n",
            "step: 290, loss: 0.0005458337836898863\n",
            "step: 300, loss: 0.00020137522369623184\n",
            "step: 310, loss: 0.012629357166588306\n",
            "step: 320, loss: 0.0015925532206892967\n",
            "step: 330, loss: 0.0007533750031143427\n",
            "step: 340, loss: 0.00011560953862499446\n",
            "step: 350, loss: 0.0016007270896807313\n",
            "step: 360, loss: 0.01443071011453867\n",
            "step: 370, loss: 0.004994941409677267\n",
            "step: 380, loss: 0.006644176784902811\n",
            "step: 390, loss: 0.0009287333232350647\n",
            "step: 400, loss: 0.11687012016773224\n",
            "step: 410, loss: 0.053381383419036865\n",
            "step: 420, loss: 0.0019245248986408114\n",
            "step: 430, loss: 0.0006322598783299327\n",
            "step: 440, loss: 0.00026368367252871394\n",
            "step: 450, loss: 0.013609025627374649\n",
            "step: 460, loss: 0.04697421193122864\n",
            "step: 470, loss: 0.0002699576143641025\n",
            "step: 480, loss: 0.0003249058499932289\n",
            "step: 490, loss: 0.0007191228214651346\n",
            "step: 500, loss: 0.0020133040379732847\n",
            "step: 510, loss: 0.00011976118548773229\n",
            "step: 520, loss: 0.0001653080980759114\n",
            "step: 530, loss: 0.0001383156341034919\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9060925332111772, f1=0.8996350364963502, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036786366254091263\n",
            "step: 10, loss: 9.784350550035015e-05\n",
            "step: 20, loss: 0.0015226822579279542\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.000552757759578526\n",
            "step: 40, loss: 0.000308136863168329\n",
            "step: 50, loss: 0.00016828106890898198\n",
            "step: 60, loss: 0.002017783699557185\n",
            "step: 70, loss: 0.014633704908192158\n",
            "step: 80, loss: 8.836078632157296e-05\n",
            "step: 90, loss: 0.0026207466144114733\n",
            "step: 100, loss: 0.0005560661084018648\n",
            "step: 110, loss: 0.00820793304592371\n",
            "step: 120, loss: 0.0002566213661339134\n",
            "step: 130, loss: 0.0009100298047997057\n",
            "step: 140, loss: 0.007589375600218773\n",
            "step: 150, loss: 0.0005159458960406482\n",
            "step: 160, loss: 0.0001651107450015843\n",
            "step: 170, loss: 0.0006832600338384509\n",
            "step: 180, loss: 0.0002207651996286586\n",
            "step: 190, loss: 0.0009102042531594634\n",
            "step: 200, loss: 0.0001870456908363849\n",
            "step: 210, loss: 0.00023078967933543026\n",
            "step: 220, loss: 6.792187923565507e-05\n",
            "step: 230, loss: 0.0001397984888171777\n",
            "step: 240, loss: 0.00038889984716661274\n",
            "step: 250, loss: 0.00018294030451215804\n",
            "step: 260, loss: 5.7903969718609005e-05\n",
            "step: 270, loss: 0.00015331456961575896\n",
            "step: 280, loss: 6.260956433834508e-05\n",
            "step: 290, loss: 5.234386117081158e-05\n",
            "step: 300, loss: 0.00020979686814825982\n",
            "step: 310, loss: 4.88580008095596e-05\n",
            "step: 320, loss: 6.823305739089847e-05\n",
            "step: 330, loss: 0.00010147925786441192\n",
            "step: 340, loss: 0.00049297243822366\n",
            "step: 350, loss: 5.4649764933856204e-05\n",
            "step: 360, loss: 0.00439056009054184\n",
            "step: 370, loss: 0.0010298638371750712\n",
            "step: 380, loss: 7.582598482258618e-05\n",
            "step: 390, loss: 0.00030572060495615005\n",
            "step: 400, loss: 0.0003216903714928776\n",
            "step: 410, loss: 0.014908099547028542\n",
            "step: 420, loss: 0.0014983774162828922\n",
            "step: 430, loss: 4.7141333197942004e-05\n",
            "step: 440, loss: 0.00010858186578843743\n",
            "step: 450, loss: 0.0009963561315089464\n",
            "step: 460, loss: 7.947564881760627e-05\n",
            "step: 470, loss: 6.476209091488272e-05\n",
            "step: 480, loss: 0.00052045495249331\n",
            "step: 490, loss: 6.791246414650232e-05\n",
            "step: 500, loss: 5.4684263886883855e-05\n",
            "step: 510, loss: 0.0004167479055467993\n",
            "step: 520, loss: 0.012371503747999668\n",
            "step: 530, loss: 5.751325443270616e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9033749422098938, f1=0.901669758812616, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0050480980426073074\n",
            "step: 10, loss: 4.822778646484949e-05\n",
            "step: 20, loss: 6.349948671413586e-05\n",
            "step: 30, loss: 5.258915189187974e-05\n",
            "step: 40, loss: 0.00010075852333102375\n",
            "step: 50, loss: 6.001077781547792e-05\n",
            "step: 60, loss: 6.108714296715334e-05\n",
            "step: 70, loss: 0.00015450469800271094\n",
            "step: 80, loss: 0.0007995122577995062\n",
            "step: 90, loss: 8.860880916472524e-05\n",
            "step: 100, loss: 0.0020601702854037285\n",
            "step: 110, loss: 0.00028528974507935345\n",
            "step: 120, loss: 0.006761244963854551\n",
            "step: 130, loss: 0.00015482779417652637\n",
            "step: 140, loss: 0.0008931452175602317\n",
            "step: 150, loss: 6.86238709022291e-05\n",
            "step: 160, loss: 0.00016742282605264336\n",
            "step: 170, loss: 0.00433776481077075\n",
            "step: 180, loss: 0.0009750283788889647\n",
            "step: 190, loss: 0.0003633436281234026\n",
            "step: 200, loss: 0.0017439196817576885\n",
            "step: 210, loss: 7.74811051087454e-05\n",
            "step: 220, loss: 0.0016206064028665423\n",
            "step: 230, loss: 0.000104499478766229\n",
            "step: 240, loss: 0.0003934820124413818\n",
            "step: 250, loss: 0.002161746146157384\n",
            "step: 260, loss: 0.00028509151889011264\n",
            "step: 270, loss: 8.515421359334141e-05\n",
            "step: 280, loss: 0.0002393585309619084\n",
            "step: 290, loss: 6.072978794691153e-05\n",
            "step: 300, loss: 0.001805689069442451\n",
            "step: 310, loss: 0.0002200217277277261\n",
            "step: 320, loss: 4.801155591849238e-05\n",
            "step: 330, loss: 0.0001225253363372758\n",
            "step: 340, loss: 0.00010765276965685189\n",
            "step: 350, loss: 6.873391976114362e-05\n",
            "step: 360, loss: 8.59339561429806e-05\n",
            "step: 370, loss: 0.0011056307703256607\n",
            "step: 380, loss: 6.449471402447671e-05\n",
            "step: 390, loss: 0.000187340920092538\n",
            "step: 400, loss: 0.0003122298512607813\n",
            "step: 410, loss: 8.079632971202955e-05\n",
            "step: 420, loss: 0.00019213106133975089\n",
            "step: 430, loss: 0.00015844847075641155\n",
            "step: 440, loss: 7.898172043496743e-05\n",
            "step: 450, loss: 0.019542722031474113\n",
            "step: 460, loss: 0.00021635439770761877\n",
            "step: 470, loss: 0.0020542836282402277\n",
            "step: 480, loss: 7.290548091987148e-05\n",
            "step: 490, loss: 0.038443200290203094\n",
            "step: 500, loss: 0.0005690681282430887\n",
            "step: 510, loss: 0.0010591563768684864\n",
            "step: 520, loss: 0.006013853941112757\n",
            "step: 530, loss: 0.000254300597589463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9016470588235295, f1=0.8930581613508444, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007233509095385671\n",
            "step: 10, loss: 0.000499016314279288\n",
            "step: 20, loss: 0.0001110262528527528\n",
            "step: 30, loss: 0.00035420217318460345\n",
            "step: 40, loss: 0.0028351445216685534\n",
            "step: 50, loss: 8.716104639461264e-05\n",
            "step: 60, loss: 0.00014019827358424664\n",
            "step: 70, loss: 8.283962233690545e-05\n",
            "step: 80, loss: 6.761603435734287e-05\n",
            "step: 90, loss: 0.00010156854841625318\n",
            "step: 100, loss: 0.0006439025746658444\n",
            "step: 110, loss: 0.0075029293075203896\n",
            "step: 120, loss: 0.006398208439350128\n",
            "step: 130, loss: 5.637681533698924e-05\n",
            "step: 140, loss: 8.7399770563934e-05\n",
            "step: 150, loss: 0.00019131848239339888\n",
            "step: 160, loss: 0.00015001185238361359\n",
            "step: 170, loss: 0.0002498566173017025\n",
            "step: 180, loss: 0.0010797104332596064\n",
            "step: 190, loss: 0.0003445987240411341\n",
            "step: 200, loss: 0.0004644383443519473\n",
            "step: 210, loss: 0.0005642975447699428\n",
            "step: 220, loss: 8.744795195525512e-05\n",
            "step: 230, loss: 6.75944538670592e-05\n",
            "step: 240, loss: 0.030654557049274445\n",
            "step: 250, loss: 5.043044438934885e-05\n",
            "step: 260, loss: 8.79075814737007e-05\n",
            "step: 270, loss: 0.00036501933936960995\n",
            "step: 280, loss: 0.00010021060006693006\n",
            "step: 290, loss: 0.000524923496413976\n",
            "step: 300, loss: 0.003069440834224224\n",
            "step: 310, loss: 0.00942252017557621\n",
            "step: 320, loss: 0.00031900827889330685\n",
            "step: 330, loss: 0.0006577761378139257\n",
            "step: 340, loss: 8.208440704038367e-05\n",
            "step: 350, loss: 0.00013197668886277825\n",
            "step: 360, loss: 9.550144022796303e-05\n",
            "step: 370, loss: 6.796663365093991e-05\n",
            "step: 380, loss: 5.918038368690759e-05\n",
            "step: 390, loss: 0.04471724107861519\n",
            "step: 400, loss: 0.0002497246314305812\n",
            "step: 410, loss: 7.974840991664678e-05\n",
            "step: 420, loss: 0.0059918370097875595\n",
            "step: 430, loss: 0.0043881731107831\n",
            "step: 440, loss: 0.0005119277047924697\n",
            "step: 450, loss: 4.4045114918844774e-05\n",
            "step: 460, loss: 0.007246713619679213\n",
            "step: 470, loss: 0.0007990517187863588\n",
            "step: 480, loss: 0.014262843877077103\n",
            "step: 490, loss: 0.000770953600294888\n",
            "step: 500, loss: 0.011991828680038452\n",
            "step: 510, loss: 0.00015751566388644278\n",
            "step: 520, loss: 0.0005350152496248484\n",
            "step: 530, loss: 9.69211760093458e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8988149498632635, f1=0.9036969420356001, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008407214772887528\n",
            "step: 10, loss: 6.523810588987544e-05\n",
            "step: 20, loss: 8.323884685523808e-05\n",
            "step: 30, loss: 0.0002600625157356262\n",
            "step: 40, loss: 6.419656710932031e-05\n",
            "step: 50, loss: 3.0509403586620465e-05\n",
            "step: 60, loss: 0.0007566519198007882\n",
            "step: 70, loss: 0.0002069625334115699\n",
            "step: 80, loss: 0.00013380579184740782\n",
            "step: 90, loss: 0.00019889949180651456\n",
            "step: 100, loss: 3.906920028384775e-05\n",
            "step: 110, loss: 8.144558523781598e-05\n",
            "step: 120, loss: 0.00013022197526879609\n",
            "step: 130, loss: 4.339013321441598e-05\n",
            "step: 140, loss: 4.7305249609053135e-05\n",
            "step: 150, loss: 0.00015470771177206188\n",
            "step: 160, loss: 7.83177965786308e-05\n",
            "step: 170, loss: 2.8944879886694252e-05\n",
            "step: 180, loss: 5.321920252754353e-05\n",
            "step: 190, loss: 4.0575203456683084e-05\n",
            "step: 200, loss: 5.626972779282369e-05\n",
            "step: 210, loss: 0.00030474993400275707\n",
            "step: 220, loss: 3.9132577512646094e-05\n",
            "step: 230, loss: 6.898136052768677e-05\n",
            "step: 240, loss: 4.5449014578480273e-05\n",
            "step: 250, loss: 4.484601959120482e-05\n",
            "step: 260, loss: 4.0998005715664476e-05\n",
            "step: 270, loss: 3.712440229719505e-05\n",
            "step: 280, loss: 7.100898073986173e-05\n",
            "step: 290, loss: 0.00017724283679854125\n",
            "step: 300, loss: 0.0001684194867266342\n",
            "step: 310, loss: 4.164713027421385e-05\n",
            "step: 320, loss: 3.972088234149851e-05\n",
            "step: 330, loss: 0.0002167819329770282\n",
            "step: 340, loss: 0.0004768634680658579\n",
            "step: 350, loss: 0.0015892706578597426\n",
            "step: 360, loss: 5.209603114053607e-05\n",
            "step: 370, loss: 3.1566953111905605e-05\n",
            "step: 380, loss: 5.29929602635093e-05\n",
            "step: 390, loss: 3.1071969715412706e-05\n",
            "step: 400, loss: 3.67524080502335e-05\n",
            "step: 410, loss: 0.0009631288703531027\n",
            "step: 420, loss: 4.957252895110287e-05\n",
            "step: 430, loss: 4.1624381992733106e-05\n",
            "step: 440, loss: 0.00021496166300494224\n",
            "step: 450, loss: 0.1485215425491333\n",
            "step: 460, loss: 0.00010233257489744574\n",
            "step: 470, loss: 0.0001466476242057979\n",
            "step: 480, loss: 4.297719715395942e-05\n",
            "step: 490, loss: 0.005113240797072649\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 500, loss: 0.003638913156464696\n",
            "step: 510, loss: 4.001178967882879e-05\n",
            "step: 520, loss: 4.484942473936826e-05\n",
            "step: 530, loss: 0.001627355464734137\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8905950095969289, f1=0.8904830224772837, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010752586968010291\n",
            "step: 10, loss: 3.423423186177388e-05\n",
            "step: 20, loss: 0.0007462750072591007\n",
            "step: 30, loss: 0.0038834363222122192\n",
            "step: 40, loss: 3.93554764741566e-05\n",
            "step: 50, loss: 0.00019814104598481208\n",
            "step: 60, loss: 0.00018984684720635414\n",
            "step: 70, loss: 3.396561442059465e-05\n",
            "step: 80, loss: 7.651395571883768e-05\n",
            "step: 90, loss: 0.0013146733399480581\n",
            "step: 100, loss: 4.826668373425491e-05\n",
            "step: 110, loss: 8.044319838518277e-05\n",
            "step: 120, loss: 0.00020069078891538084\n",
            "step: 130, loss: 7.151308818720281e-05\n",
            "step: 140, loss: 0.00018645150703378022\n",
            "step: 150, loss: 3.154865407850593e-05\n",
            "step: 160, loss: 0.0008231017272919416\n",
            "step: 170, loss: 9.206373943015933e-05\n",
            "step: 180, loss: 0.00027063069865107536\n",
            "step: 190, loss: 0.0012436588294804096\n",
            "step: 200, loss: 0.002125989180058241\n",
            "step: 210, loss: 0.00021776296489406377\n",
            "step: 220, loss: 0.00011434597399784252\n",
            "step: 230, loss: 2.851603858289309e-05\n",
            "step: 240, loss: 9.183723159367219e-05\n",
            "step: 250, loss: 0.0033737635239958763\n",
            "step: 260, loss: 0.0002535344974603504\n",
            "step: 270, loss: 0.00012284166587051004\n",
            "step: 280, loss: 0.0002059113176073879\n",
            "step: 290, loss: 0.0005261465557850897\n",
            "step: 300, loss: 9.351284825243056e-05\n",
            "step: 310, loss: 2.9465663828887045e-05\n",
            "step: 320, loss: 0.00017607523477636278\n",
            "step: 330, loss: 4.7728877689223737e-05\n",
            "step: 340, loss: 4.219428592477925e-05\n",
            "step: 350, loss: 5.820185469929129e-05\n",
            "step: 360, loss: 2.6877252821577713e-05\n",
            "step: 370, loss: 0.0035081629175692797\n",
            "step: 380, loss: 6.509045488201082e-05\n",
            "step: 390, loss: 5.130837234901264e-05\n",
            "step: 400, loss: 2.634815791680012e-05\n",
            "step: 410, loss: 6.679175567114726e-05\n",
            "step: 420, loss: 4.591913602780551e-05\n",
            "step: 430, loss: 0.00010014799045166001\n",
            "step: 440, loss: 7.019101758487523e-05\n",
            "step: 450, loss: 3.907267819158733e-05\n",
            "step: 460, loss: 2.3003110982244834e-05\n",
            "step: 470, loss: 0.0011808399576693773\n",
            "step: 480, loss: 0.00014912246842868626\n",
            "step: 490, loss: 0.0001069881473085843\n",
            "step: 500, loss: 8.436105417786166e-05\n",
            "step: 510, loss: 4.4010812416672707e-05\n",
            "step: 520, loss: 2.536469946790021e-05\n",
            "step: 530, loss: 4.984275437891483e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.896358543417367, f1=0.8978644382544103, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.450814882759005e-05\n",
            "step: 10, loss: 0.00018270683358423412\n",
            "step: 20, loss: 9.419917478226125e-05\n",
            "step: 30, loss: 0.00010236304660793394\n",
            "step: 40, loss: 3.471408854238689e-05\n",
            "step: 50, loss: 0.00018256819748785347\n",
            "step: 60, loss: 3.7165485991863534e-05\n",
            "step: 70, loss: 2.602799577289261e-05\n",
            "step: 80, loss: 3.621943324105814e-05\n",
            "step: 90, loss: 2.1412513888208196e-05\n",
            "step: 100, loss: 4.236587119521573e-05\n",
            "step: 110, loss: 3.814052979578264e-05\n",
            "step: 120, loss: 3.9959559217095375e-05\n",
            "step: 130, loss: 2.510022204660345e-05\n",
            "step: 140, loss: 0.0005873705958947539\n",
            "step: 150, loss: 0.0003167249960824847\n",
            "step: 160, loss: 0.0010453663999214768\n",
            "step: 170, loss: 0.00022505054948851466\n",
            "step: 180, loss: 7.15884380042553e-05\n",
            "step: 190, loss: 0.00020405893155839294\n",
            "step: 200, loss: 0.00012380695261526853\n",
            "step: 210, loss: 0.0005059116519987583\n",
            "step: 220, loss: 0.0001514936884632334\n",
            "step: 230, loss: 0.00017833923629950732\n",
            "step: 240, loss: 4.079611608176492e-05\n",
            "step: 250, loss: 0.00013897671306040138\n",
            "step: 260, loss: 3.23963540722616e-05\n",
            "step: 270, loss: 0.0003655760665424168\n",
            "step: 280, loss: 3.439020656514913e-05\n",
            "step: 290, loss: 3.2084051781566814e-05\n",
            "step: 300, loss: 9.137437155004591e-05\n",
            "step: 310, loss: 0.0005630278028547764\n",
            "step: 320, loss: 0.00046480362652800977\n",
            "step: 330, loss: 3.404254312044941e-05\n",
            "step: 340, loss: 0.0033134575933218002\n",
            "step: 350, loss: 4.294705286156386e-05\n",
            "step: 360, loss: 0.002352513140067458\n",
            "step: 370, loss: 3.708307849592529e-05\n",
            "step: 380, loss: 3.579805706976913e-05\n",
            "step: 390, loss: 0.00013728890917263925\n",
            "step: 400, loss: 3.6152603570371866e-05\n",
            "step: 410, loss: 4.465788515517488e-05\n",
            "step: 420, loss: 7.830376853235066e-05\n",
            "step: 430, loss: 3.676023698062636e-05\n",
            "step: 440, loss: 2.6686209821491502e-05\n",
            "step: 450, loss: 4.3359934352338314e-05\n",
            "step: 460, loss: 0.0002286985400132835\n",
            "step: 470, loss: 3.348126483615488e-05\n",
            "step: 480, loss: 0.00010068802657769993\n",
            "step: 490, loss: 3.405056850169785e-05\n",
            "step: 500, loss: 0.00011326653475407511\n",
            "step: 510, loss: 7.576767529826611e-05\n",
            "step: 520, loss: 3.0583883926738054e-05\n",
            "step: 530, loss: 4.461973730940372e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9031365313653137, f1=0.9040590405904059, best_f1=0.8996350364963502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9082353648846038e-05\n",
            "step: 10, loss: 0.00014682256733067334\n",
            "step: 20, loss: 0.0022541021462529898\n",
            "step: 30, loss: 4.536029155133292e-05\n",
            "step: 40, loss: 4.4874097511637956e-05\n",
            "step: 50, loss: 5.5476197303505614e-05\n",
            "step: 60, loss: 2.6898787837126292e-05\n",
            "step: 70, loss: 0.0005296061863191426\n",
            "step: 80, loss: 0.00027701156795956194\n",
            "step: 90, loss: 2.2581998564419337e-05\n",
            "step: 100, loss: 2.805427720886655e-05\n",
            "step: 110, loss: 0.010629831813275814\n",
            "step: 120, loss: 0.004145565442740917\n",
            "step: 130, loss: 4.165929931332357e-05\n",
            "step: 140, loss: 1.9699038602993824e-05\n",
            "step: 150, loss: 0.000846135662868619\n",
            "step: 160, loss: 3.425617978791706e-05\n",
            "step: 170, loss: 0.0015625106170773506\n",
            "step: 180, loss: 1.9468005120870657e-05\n",
            "step: 190, loss: 0.0001456224563298747\n",
            "step: 200, loss: 3.0054552553338e-05\n",
            "step: 210, loss: 0.0003772648051381111\n",
            "step: 220, loss: 0.00018872346845455468\n",
            "step: 230, loss: 4.0230843296740204e-05\n",
            "step: 240, loss: 4.854729195358232e-05\n",
            "step: 250, loss: 0.00012933477410115302\n",
            "step: 260, loss: 7.432371057802811e-05\n",
            "step: 270, loss: 3.1779531127540395e-05\n",
            "step: 280, loss: 2.9943017580080777e-05\n",
            "step: 290, loss: 0.0016894302098080516\n",
            "step: 300, loss: 0.002648429712280631\n",
            "step: 310, loss: 0.0005507060559466481\n",
            "step: 320, loss: 0.0002338803024031222\n",
            "step: 330, loss: 0.0035418409388512373\n",
            "step: 340, loss: 0.000346727785654366\n",
            "step: 350, loss: 5.7929788454202935e-05\n",
            "step: 360, loss: 0.00010648761235643178\n",
            "step: 370, loss: 1.9885268557118252e-05\n",
            "step: 380, loss: 0.00010674069199012592\n",
            "step: 390, loss: 5.5171312851598486e-05\n",
            "step: 400, loss: 0.00023212005908135325\n",
            "step: 410, loss: 7.340782758546993e-05\n",
            "step: 420, loss: 4.5631368266185746e-05\n",
            "step: 430, loss: 2.410958768450655e-05\n",
            "step: 440, loss: 0.0001722830202197656\n",
            "step: 450, loss: 8.100333798211068e-05\n",
            "step: 460, loss: 2.2473914214060642e-05\n",
            "step: 470, loss: 4.337686186772771e-05\n",
            "step: 480, loss: 0.00017660630692262203\n",
            "step: 490, loss: 3.2658288546372205e-05\n",
            "step: 500, loss: 0.0009072293760254979\n",
            "step: 510, loss: 0.0006938075530342758\n",
            "step: 520, loss: 4.457091199583374e-05\n",
            "step: 530, loss: 2.803193092404399e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.897423887587822, f1=0.8989287377736376, best_f1=0.8996350364963502\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 255.75it/s]\n",
            "load_f1 = 0.9053613053613055\n",
            "real_f1 = 0.90181479758027\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 265.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bb050b-5790-42f0-cab4-379b77de94f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8337807655334473\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.06548142433166504\n",
            "step: 20, loss: 0.38525962829589844\n",
            "step: 30, loss: 0.374788373708725\n",
            "step: 40, loss: 0.5033724904060364\n",
            "step: 50, loss: 0.3111454248428345\n",
            "step: 60, loss: 0.3680569529533386\n",
            "step: 70, loss: 0.2767786681652069\n",
            "step: 80, loss: 0.2903327941894531\n",
            "step: 90, loss: 0.43282729387283325\n",
            "step: 100, loss: 0.2011963278055191\n",
            "step: 110, loss: 0.29559141397476196\n",
            "step: 120, loss: 0.25695788860321045\n",
            "step: 130, loss: 0.23154965043067932\n",
            "step: 140, loss: 0.31882378458976746\n",
            "step: 150, loss: 0.3208945691585541\n",
            "step: 160, loss: 0.26309022307395935\n",
            "step: 170, loss: 0.19286341965198517\n",
            "step: 180, loss: 0.20009249448776245\n",
            "step: 190, loss: 0.27198657393455505\n",
            "step: 200, loss: 0.1905180662870407\n",
            "step: 210, loss: 0.5201736092567444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3961038961038961, f1=0.39781021897810215, best_f1=0.39781021897810215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08718249946832657\n",
            "step: 10, loss: 0.10454125702381134\n",
            "step: 20, loss: 0.2842254340648651\n",
            "step: 30, loss: 0.19628357887268066\n",
            "step: 40, loss: 0.06034741923213005\n",
            "step: 50, loss: 0.22324667870998383\n",
            "step: 60, loss: 0.1572829633951187\n",
            "step: 70, loss: 0.288668692111969\n",
            "step: 80, loss: 0.2895306646823883\n",
            "step: 90, loss: 0.1382617950439453\n",
            "step: 100, loss: 0.18277132511138916\n",
            "step: 110, loss: 0.04430116340517998\n",
            "step: 120, loss: 0.2696143388748169\n",
            "step: 130, loss: 0.2287580668926239\n",
            "step: 140, loss: 0.3099009394645691\n",
            "step: 150, loss: 0.25548022985458374\n",
            "step: 160, loss: 0.1698032170534134\n",
            "step: 170, loss: 0.2592656910419464\n",
            "step: 180, loss: 0.37527740001678467\n",
            "step: 190, loss: 0.1874864548444748\n",
            "step: 200, loss: 0.2811562418937683\n",
            "step: 210, loss: 0.3726328909397125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4609800362976406, f1=0.5018315018315018, best_f1=0.5018315018315018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1875815987586975\n",
            "step: 10, loss: 0.24759399890899658\n",
            "step: 20, loss: 0.3024594485759735\n",
            "step: 30, loss: 0.07400152087211609\n",
            "step: 40, loss: 0.18351198732852936\n",
            "step: 50, loss: 0.21474589407444\n",
            "step: 60, loss: 0.3609232008457184\n",
            "step: 70, loss: 0.19411729276180267\n",
            "step: 80, loss: 0.14800406992435455\n",
            "step: 90, loss: 0.16429336369037628\n",
            "step: 100, loss: 0.1340516358613968\n",
            "step: 110, loss: 0.2770861089229584\n",
            "step: 120, loss: 0.206455796957016\n",
            "step: 130, loss: 0.2239028513431549\n",
            "step: 140, loss: 0.3639140725135803\n",
            "step: 150, loss: 0.25400272011756897\n",
            "step: 160, loss: 0.10782173275947571\n",
            "step: 170, loss: 0.16466505825519562\n",
            "step: 180, loss: 0.09980249404907227\n",
            "step: 190, loss: 0.2582845389842987\n",
            "step: 200, loss: 0.2198481261730194\n",
            "step: 210, loss: 0.25736141204833984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4752475247524752, f1=0.516, best_f1=0.516\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2922063171863556\n",
            "step: 10, loss: 0.06015527993440628\n",
            "step: 20, loss: 0.14262156188488007\n",
            "step: 30, loss: 0.07847729325294495\n",
            "step: 40, loss: 0.15821778774261475\n",
            "step: 50, loss: 0.15349358320236206\n",
            "step: 60, loss: 0.0814436599612236\n",
            "step: 70, loss: 0.16483674943447113\n",
            "step: 80, loss: 0.1837759166955948\n",
            "step: 90, loss: 0.06196345388889313\n",
            "step: 100, loss: 0.17996154725551605\n",
            "step: 110, loss: 0.11801635473966599\n",
            "step: 120, loss: 0.07300838083028793\n",
            "step: 130, loss: 0.235294371843338\n",
            "step: 140, loss: 0.10578346997499466\n",
            "step: 150, loss: 0.21834829449653625\n",
            "step: 160, loss: 0.11539751291275024\n",
            "step: 170, loss: 0.13497033715248108\n",
            "step: 180, loss: 0.15238328278064728\n",
            "step: 190, loss: 0.1744106560945511\n",
            "step: 200, loss: 0.2379525750875473\n",
            "step: 210, loss: 0.04973411187529564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.4861407249466951, f1=0.4708333333333334, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05742418020963669\n",
            "step: 10, loss: 0.0904601588845253\n",
            "step: 20, loss: 0.04822332039475441\n",
            "step: 30, loss: 0.03771226853132248\n",
            "step: 40, loss: 0.06893022358417511\n",
            "step: 50, loss: 0.1665831357240677\n",
            "step: 60, loss: 0.028278274461627007\n",
            "step: 70, loss: 0.04926125332713127\n",
            "step: 80, loss: 0.08993957191705704\n",
            "step: 90, loss: 0.0323953814804554\n",
            "step: 100, loss: 0.14056800305843353\n",
            "step: 110, loss: 0.02262728475034237\n",
            "step: 120, loss: 0.037716422230005264\n",
            "step: 130, loss: 0.03709957003593445\n",
            "step: 140, loss: 0.054537706077098846\n",
            "step: 150, loss: 0.03805225342512131\n",
            "step: 160, loss: 0.03163182735443115\n",
            "step: 170, loss: 0.1303476244211197\n",
            "step: 180, loss: 0.2674318552017212\n",
            "step: 190, loss: 0.06410899013280869\n",
            "step: 200, loss: 0.08993306010961533\n",
            "step: 210, loss: 0.12538883090019226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.456140350877193, f1=0.4657933042212518, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05813221633434296\n",
            "step: 10, loss: 0.014986395835876465\n",
            "step: 20, loss: 0.020977944135665894\n",
            "step: 30, loss: 0.07489190995693207\n",
            "step: 40, loss: 0.09706646203994751\n",
            "step: 50, loss: 0.0597965233027935\n",
            "step: 60, loss: 0.15155312418937683\n",
            "step: 70, loss: 0.003715973813086748\n",
            "step: 80, loss: 0.2203749269247055\n",
            "step: 90, loss: 0.201389878988266\n",
            "step: 100, loss: 0.03786781430244446\n",
            "step: 110, loss: 0.03243817016482353\n",
            "step: 120, loss: 0.03918377682566643\n",
            "step: 130, loss: 0.008373919874429703\n",
            "step: 140, loss: 0.1493033468723297\n",
            "step: 150, loss: 0.06867644190788269\n",
            "step: 160, loss: 0.19728641211986542\n",
            "step: 170, loss: 0.09200804680585861\n",
            "step: 180, loss: 0.029924873262643814\n",
            "step: 190, loss: 0.036646779626607895\n",
            "step: 200, loss: 0.019749047234654427\n",
            "step: 210, loss: 0.03712419420480728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4444444444444445, f1=0.463768115942029, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09640754014253616\n",
            "step: 10, loss: 0.007864260114729404\n",
            "step: 20, loss: 0.07015971839427948\n",
            "step: 30, loss: 0.020235704258084297\n",
            "step: 40, loss: 0.011621496640145779\n",
            "step: 50, loss: 0.13357461988925934\n",
            "step: 60, loss: 0.16390188038349152\n",
            "step: 70, loss: 0.05237138271331787\n",
            "step: 80, loss: 0.014513658359646797\n",
            "step: 90, loss: 0.10776166617870331\n",
            "step: 100, loss: 0.04401580989360809\n",
            "step: 110, loss: 0.0329226590692997\n",
            "step: 120, loss: 0.05745599791407585\n",
            "step: 130, loss: 0.00554531579837203\n",
            "step: 140, loss: 0.009953426197171211\n",
            "step: 150, loss: 0.03129784017801285\n",
            "step: 160, loss: 0.0706164687871933\n",
            "step: 170, loss: 0.007633023429661989\n",
            "step: 180, loss: 0.0018164686625823379\n",
            "step: 190, loss: 0.07399636507034302\n",
            "step: 200, loss: 0.03398749232292175\n",
            "step: 210, loss: 0.13271376490592957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4196277495769882, f1=0.4664429530201342, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.032599687576293945\n",
            "step: 10, loss: 0.009068922139704227\n",
            "step: 20, loss: 0.11694194376468658\n",
            "step: 30, loss: 0.00202498328872025\n",
            "step: 40, loss: 0.22008909285068512\n",
            "step: 50, loss: 0.02966858632862568\n",
            "step: 60, loss: 0.07434888184070587\n",
            "step: 70, loss: 0.018977778032422066\n",
            "step: 80, loss: 0.07624726742506027\n",
            "step: 90, loss: 0.1176474541425705\n",
            "step: 100, loss: 0.002490561455488205\n",
            "step: 110, loss: 0.0016458820318803191\n",
            "step: 120, loss: 0.03270075470209122\n",
            "step: 130, loss: 0.007593114860355854\n",
            "step: 140, loss: 0.07242237776517868\n",
            "step: 150, loss: 0.004248755984008312\n",
            "step: 160, loss: 0.08771200478076935\n",
            "step: 170, loss: 0.0037405493203550577\n",
            "step: 180, loss: 0.042301155626773834\n",
            "step: 190, loss: 0.023117559030652046\n",
            "step: 200, loss: 0.09406159073114395\n",
            "step: 210, loss: 0.10915740579366684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4051724137931035, f1=0.440251572327044, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10958531498908997\n",
            "step: 10, loss: 0.007302991580218077\n",
            "step: 20, loss: 0.12080637365579605\n",
            "step: 30, loss: 0.019734680652618408\n",
            "step: 40, loss: 0.015672490000724792\n",
            "step: 50, loss: 0.0017717719310894608\n",
            "step: 60, loss: 0.022497154772281647\n",
            "step: 70, loss: 0.0010308617493137717\n",
            "step: 80, loss: 0.0023329509422183037\n",
            "step: 90, loss: 0.15136368572711945\n",
            "step: 100, loss: 0.010130520910024643\n",
            "step: 110, loss: 0.005727812182158232\n",
            "step: 120, loss: 0.0030139987356960773\n",
            "step: 130, loss: 0.004736994858831167\n",
            "step: 140, loss: 0.04507861286401749\n",
            "step: 150, loss: 0.008736250922083855\n",
            "step: 160, loss: 0.0005062891286797822\n",
            "step: 170, loss: 0.002866193884983659\n",
            "step: 180, loss: 0.03321181237697601\n",
            "step: 190, loss: 0.03629881143569946\n",
            "step: 200, loss: 0.04581199213862419\n",
            "step: 210, loss: 0.0091177923604846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.39565217391304347, f1=0.4312896405919662, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035030031576752663\n",
            "step: 10, loss: 0.0013775225961580873\n",
            "step: 20, loss: 0.06852558255195618\n",
            "step: 30, loss: 0.008012042380869389\n",
            "step: 40, loss: 0.005351515021175146\n",
            "step: 50, loss: 0.028457464650273323\n",
            "step: 60, loss: 0.029855862259864807\n",
            "step: 70, loss: 0.010648488998413086\n",
            "step: 80, loss: 0.00038763193879276514\n",
            "step: 90, loss: 0.01760866492986679\n",
            "step: 100, loss: 0.00036028659087605774\n",
            "step: 110, loss: 0.0015128821833059192\n",
            "step: 120, loss: 0.02239670604467392\n",
            "step: 130, loss: 0.03578922152519226\n",
            "step: 140, loss: 0.00030999939190223813\n",
            "step: 150, loss: 0.07170045375823975\n",
            "step: 160, loss: 0.02475845441222191\n",
            "step: 170, loss: 0.002597522223368287\n",
            "step: 180, loss: 0.003173135919496417\n",
            "step: 190, loss: 0.051594484597444534\n",
            "step: 200, loss: 0.003044651821255684\n",
            "step: 210, loss: 0.14183150231838226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4146868250539957, f1=0.4273858921161826, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003037302754819393\n",
            "step: 10, loss: 0.03149773180484772\n",
            "step: 20, loss: 0.002392767695710063\n",
            "step: 30, loss: 0.006444964557886124\n",
            "step: 40, loss: 0.02964616008102894\n",
            "step: 50, loss: 0.002500551287084818\n",
            "step: 60, loss: 0.0077479928731918335\n",
            "step: 70, loss: 0.006069390568882227\n",
            "step: 80, loss: 0.08718675374984741\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.0005559442797675729\n",
            "step: 100, loss: 0.0007986897835507989\n",
            "step: 110, loss: 0.0014176114927977324\n",
            "step: 120, loss: 0.003062909236177802\n",
            "step: 130, loss: 0.02170775644481182\n",
            "step: 140, loss: 0.0025500573683530092\n",
            "step: 150, loss: 0.05591044947504997\n",
            "step: 160, loss: 0.005129865370690823\n",
            "step: 170, loss: 0.08491266518831253\n",
            "step: 180, loss: 0.014971202239394188\n",
            "step: 190, loss: 0.011914132162928581\n",
            "step: 200, loss: 0.00019417982548475266\n",
            "step: 210, loss: 0.00015084139886312187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.3963553530751709, f1=0.42035398230088494, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003051003150176257\n",
            "step: 10, loss: 0.00023666716879233718\n",
            "step: 20, loss: 0.030331384390592575\n",
            "step: 30, loss: 0.0026930964086204767\n",
            "step: 40, loss: 0.0018355417996644974\n",
            "step: 50, loss: 0.005720323882997036\n",
            "step: 60, loss: 0.0010507210390642285\n",
            "step: 70, loss: 0.014923356473445892\n",
            "step: 80, loss: 0.0035776584409177303\n",
            "step: 90, loss: 0.048487935215234756\n",
            "step: 100, loss: 0.0038705940824002028\n",
            "step: 110, loss: 0.00038271918310783803\n",
            "step: 120, loss: 0.007163667120039463\n",
            "step: 130, loss: 0.021083394065499306\n",
            "step: 140, loss: 0.001294023240916431\n",
            "step: 150, loss: 0.00028478316380642354\n",
            "step: 160, loss: 0.004532424733042717\n",
            "step: 170, loss: 0.0006357172969728708\n",
            "step: 180, loss: 0.0014851113082841039\n",
            "step: 190, loss: 0.024980653077363968\n",
            "step: 200, loss: 0.005054258741438389\n",
            "step: 210, loss: 0.04165303707122803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.41282565130260523, f1=0.45436893203883494, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018629670375958085\n",
            "step: 10, loss: 0.02149646356701851\n",
            "step: 20, loss: 0.0010000875918194652\n",
            "step: 30, loss: 0.002572194905951619\n",
            "step: 40, loss: 0.0005530782509595156\n",
            "step: 50, loss: 0.0007320474833250046\n",
            "step: 60, loss: 0.008802159689366817\n",
            "step: 70, loss: 0.05704515054821968\n",
            "step: 80, loss: 0.018053123727440834\n",
            "step: 90, loss: 0.004520464688539505\n",
            "step: 100, loss: 0.0025667890440672636\n",
            "step: 110, loss: 0.000768783560488373\n",
            "step: 120, loss: 0.00034222594695165753\n",
            "step: 130, loss: 0.000482038565678522\n",
            "step: 140, loss: 0.0008131967624649405\n",
            "step: 150, loss: 0.005545966327190399\n",
            "step: 160, loss: 0.007298201322555542\n",
            "step: 170, loss: 0.00191679154522717\n",
            "step: 180, loss: 0.0026509903836995363\n",
            "step: 190, loss: 0.08991855382919312\n",
            "step: 200, loss: 0.0006213897140696645\n",
            "step: 210, loss: 0.0006260970840230584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.4036281179138322, f1=0.4370860927152318, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005784186068922281\n",
            "step: 10, loss: 0.0005686279619112611\n",
            "step: 20, loss: 0.00021497112174984068\n",
            "step: 30, loss: 0.010891047306358814\n",
            "step: 40, loss: 0.005044637247920036\n",
            "step: 50, loss: 0.0006287121796049178\n",
            "step: 60, loss: 0.0004871422133874148\n",
            "step: 70, loss: 0.0018616531742736697\n",
            "step: 80, loss: 0.0038368008099496365\n",
            "step: 90, loss: 0.012557776644825935\n",
            "step: 100, loss: 0.0023448707070201635\n",
            "step: 110, loss: 0.0006142161437310278\n",
            "step: 120, loss: 0.0019989751745015383\n",
            "step: 130, loss: 0.0009262601379305124\n",
            "step: 140, loss: 0.0060825832188129425\n",
            "step: 150, loss: 0.03354710713028908\n",
            "step: 160, loss: 0.0010401932522654533\n",
            "step: 170, loss: 0.004052888602018356\n",
            "step: 180, loss: 0.00017033389303833246\n",
            "step: 190, loss: 0.0015600415645167232\n",
            "step: 200, loss: 0.0031501834746450186\n",
            "step: 210, loss: 0.00019078998593613505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.4053452115812918, f1=0.4292035398230089, best_f1=0.4708333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012004951946437359\n",
            "step: 10, loss: 0.0028956751339137554\n",
            "step: 20, loss: 0.0007842733175493777\n",
            "step: 30, loss: 0.002110114088281989\n",
            "step: 40, loss: 0.000365749147022143\n",
            "step: 50, loss: 0.00017067790031433105\n",
            "step: 60, loss: 0.0008544449810869992\n",
            "step: 70, loss: 0.0011217038845643401\n",
            "step: 80, loss: 0.002320552011951804\n",
            "step: 90, loss: 0.0005601731245405972\n",
            "step: 100, loss: 0.0004126471758354455\n",
            "step: 110, loss: 0.00010002106864703819\n",
            "step: 120, loss: 0.001671033096499741\n",
            "step: 130, loss: 0.0005591695080511272\n",
            "step: 140, loss: 0.00171552668325603\n",
            "step: 150, loss: 0.001859582494944334\n",
            "step: 160, loss: 0.00029184570303186774\n",
            "step: 170, loss: 0.0002890037721954286\n",
            "step: 180, loss: 0.01278870739042759\n",
            "step: 190, loss: 0.026795770972967148\n",
            "step: 200, loss: 0.005713484715670347\n",
            "step: 210, loss: 0.0016718414844945073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.402745995423341, f1=0.43595505617977526, best_f1=0.4708333333333334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:06, 329.11it/s]\n",
            "load_f1 = 0.47844827586206895\n",
            "real_f1 = 0.4861407249466951\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 261.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b92de55-f234-484d-815c-c88fd19bd166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8761487603187561\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.16827009618282318\n",
            "step: 20, loss: 0.1592557281255722\n",
            "step: 30, loss: 0.5007125735282898\n",
            "step: 40, loss: 0.25315526127815247\n",
            "step: 50, loss: 0.30820798873901367\n",
            "step: 60, loss: 0.36519694328308105\n",
            "step: 70, loss: 0.17030636966228485\n",
            "step: 80, loss: 0.5063663125038147\n",
            "step: 90, loss: 0.23614449799060822\n",
            "step: 100, loss: 0.22451668977737427\n",
            "step: 110, loss: 0.2368757128715515\n",
            "step: 120, loss: 0.41990941762924194\n",
            "step: 130, loss: 0.3385409712791443\n",
            "step: 140, loss: 0.3388080298900604\n",
            "step: 150, loss: 0.2692643702030182\n",
            "step: 160, loss: 0.23157714307308197\n",
            "step: 170, loss: 0.3980990946292877\n",
            "step: 180, loss: 0.30871450901031494\n",
            "step: 190, loss: 0.15105821192264557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4314606741573034, f1=0.41986455981941306, best_f1=0.41986455981941306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25304341316223145\n",
            "step: 10, loss: 0.04738935828208923\n",
            "step: 20, loss: 0.14744414389133453\n",
            "step: 30, loss: 0.17156334221363068\n",
            "step: 40, loss: 0.47651782631874084\n",
            "step: 50, loss: 0.2933870851993561\n",
            "step: 60, loss: 0.1367240846157074\n",
            "step: 70, loss: 0.23506928980350494\n",
            "step: 80, loss: 0.20613205432891846\n",
            "step: 90, loss: 0.10064970701932907\n",
            "step: 100, loss: 0.3008871376514435\n",
            "step: 110, loss: 0.19395211338996887\n",
            "step: 120, loss: 0.3134462237358093\n",
            "step: 130, loss: 0.19078773260116577\n",
            "step: 140, loss: 0.3469194173812866\n",
            "step: 150, loss: 0.04311797767877579\n",
            "step: 160, loss: 0.0805833712220192\n",
            "step: 170, loss: 0.21672265231609344\n",
            "step: 180, loss: 0.13882797956466675\n",
            "step: 190, loss: 0.2622933089733124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.706896551724138, f1=0.7228260869565217, best_f1=0.7228260869565217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21447128057479858\n",
            "step: 10, loss: 0.244327113032341\n",
            "step: 20, loss: 0.024613751098513603\n",
            "step: 30, loss: 0.1996963918209076\n",
            "step: 40, loss: 0.06178075075149536\n",
            "step: 50, loss: 0.055451489984989166\n",
            "step: 60, loss: 0.1260284036397934\n",
            "step: 70, loss: 0.20791411399841309\n",
            "step: 80, loss: 0.1738048791885376\n",
            "step: 90, loss: 0.19979935884475708\n",
            "step: 100, loss: 0.07958279550075531\n",
            "step: 110, loss: 0.27655118703842163\n",
            "step: 120, loss: 0.12106871604919434\n",
            "step: 130, loss: 0.02138308435678482\n",
            "step: 140, loss: 0.19427905976772308\n",
            "step: 150, loss: 0.2801416516304016\n",
            "step: 160, loss: 0.12116070091724396\n",
            "step: 170, loss: 0.05712412670254707\n",
            "step: 180, loss: 0.17120042443275452\n",
            "step: 190, loss: 0.21003364026546478\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6900269541778975, f1=0.6881720430107526, best_f1=0.7228260869565217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10311809927225113\n",
            "step: 10, loss: 0.05733533948659897\n",
            "step: 20, loss: 0.05356263741850853\n",
            "step: 30, loss: 0.046380504965782166\n",
            "step: 40, loss: 0.005314954556524754\n",
            "step: 50, loss: 0.04616566374897957\n",
            "step: 60, loss: 0.19066070020198822\n",
            "step: 70, loss: 0.04183505102992058\n",
            "step: 80, loss: 0.04199988767504692\n",
            "step: 90, loss: 0.038278013467788696\n",
            "step: 100, loss: 0.03942789137363434\n",
            "step: 110, loss: 0.055064745247364044\n",
            "step: 120, loss: 0.09546563029289246\n",
            "step: 130, loss: 0.39298707246780396\n",
            "step: 140, loss: 0.12463338673114777\n",
            "step: 150, loss: 0.017651595175266266\n",
            "step: 160, loss: 0.03338024392724037\n",
            "step: 170, loss: 0.020626427605748177\n",
            "step: 180, loss: 0.20611952245235443\n",
            "step: 190, loss: 0.03187886252999306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7401129943502824, f1=0.7520891364902507, best_f1=0.7520891364902507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02681226097047329\n",
            "step: 10, loss: 0.020974621176719666\n",
            "step: 20, loss: 0.0243227481842041\n",
            "step: 30, loss: 0.005657027941197157\n",
            "step: 40, loss: 0.015185238793492317\n",
            "step: 50, loss: 0.08472563326358795\n",
            "step: 60, loss: 0.05135254189372063\n",
            "step: 70, loss: 0.014097688719630241\n",
            "step: 80, loss: 0.004956577904522419\n",
            "step: 90, loss: 0.0013631567126139998\n",
            "step: 100, loss: 0.020815202966332436\n",
            "step: 110, loss: 0.007012709975242615\n",
            "step: 120, loss: 0.0034050203394144773\n",
            "step: 130, loss: 0.04882100969552994\n",
            "step: 140, loss: 0.007921307347714901\n",
            "step: 150, loss: 0.0123963113874197\n",
            "step: 160, loss: 0.0026515612844377756\n",
            "step: 170, loss: 0.009996980428695679\n",
            "step: 180, loss: 0.006493085995316505\n",
            "step: 190, loss: 0.13411729037761688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7666666666666667, f1=0.7465181058495822, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024876780807971954\n",
            "step: 10, loss: 0.0018207598477602005\n",
            "step: 20, loss: 0.002617559162899852\n",
            "step: 30, loss: 0.0025219458620995283\n",
            "step: 40, loss: 0.04395414516329765\n",
            "step: 50, loss: 0.01038337592035532\n",
            "step: 60, loss: 0.0019248644821345806\n",
            "step: 70, loss: 0.05368699133396149\n",
            "step: 80, loss: 0.16727857291698456\n",
            "step: 90, loss: 0.030846361070871353\n",
            "step: 100, loss: 0.058376286178827286\n",
            "step: 110, loss: 0.018717506900429726\n",
            "step: 120, loss: 0.01555632520467043\n",
            "step: 130, loss: 0.04343317076563835\n",
            "step: 140, loss: 0.01992988772690296\n",
            "step: 150, loss: 0.043862033635377884\n",
            "step: 160, loss: 0.016390802338719368\n",
            "step: 170, loss: 0.1462383270263672\n",
            "step: 180, loss: 0.09499972313642502\n",
            "step: 190, loss: 0.0346779003739357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7262872628726287, f1=0.7533875338753386, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017388706328347325\n",
            "step: 10, loss: 0.10212720185518265\n",
            "step: 20, loss: 0.0007435539155267179\n",
            "step: 30, loss: 0.09567087143659592\n",
            "step: 40, loss: 0.07744546979665756\n",
            "step: 50, loss: 0.03422486037015915\n",
            "step: 60, loss: 0.03879014030098915\n",
            "step: 70, loss: 0.0011421616654843092\n",
            "step: 80, loss: 0.009807142429053783\n",
            "step: 90, loss: 0.04443015530705452\n",
            "step: 100, loss: 0.002154221758246422\n",
            "step: 110, loss: 0.019095011055469513\n",
            "step: 120, loss: 0.005684332922101021\n",
            "step: 130, loss: 0.029797369614243507\n",
            "step: 140, loss: 0.0512615330517292\n",
            "step: 150, loss: 0.0030809720046818256\n",
            "step: 160, loss: 0.0011483338894322515\n",
            "step: 170, loss: 0.009227112866938114\n",
            "step: 180, loss: 0.0023341563064604998\n",
            "step: 190, loss: 0.008419702760875225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7341772151898734, f1=0.741687979539642, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005660993629135191\n",
            "step: 10, loss: 0.0013277410762384534\n",
            "step: 20, loss: 0.0005389744183048606\n",
            "step: 30, loss: 0.0032787343952804804\n",
            "step: 40, loss: 0.001134060206823051\n",
            "step: 50, loss: 0.0033166310749948025\n",
            "step: 60, loss: 0.0022665332071483135\n",
            "step: 70, loss: 0.0016868062084540725\n",
            "step: 80, loss: 0.004773894790560007\n",
            "step: 90, loss: 0.0007769943331368268\n",
            "step: 100, loss: 0.0028301943093538284\n",
            "step: 110, loss: 0.0006834767409600317\n",
            "step: 120, loss: 0.0006467464845627546\n",
            "step: 130, loss: 0.1663864552974701\n",
            "step: 140, loss: 0.001501482562161982\n",
            "step: 150, loss: 0.003353218315169215\n",
            "step: 160, loss: 0.0049040955491364\n",
            "step: 170, loss: 0.010556455701589584\n",
            "step: 180, loss: 0.002685681451112032\n",
            "step: 190, loss: 0.0027220153715461493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7365853658536586, f1=0.7146401985111663, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026014087721705437\n",
            "step: 10, loss: 0.004884480964392424\n",
            "step: 20, loss: 0.029052842408418655\n",
            "step: 30, loss: 0.0025337617844343185\n",
            "step: 40, loss: 0.0038584235589951277\n",
            "step: 50, loss: 0.004719422664493322\n",
            "step: 60, loss: 0.0003912674728780985\n",
            "step: 70, loss: 0.0013381425524130464\n",
            "step: 80, loss: 0.0009906056802719831\n",
            "step: 90, loss: 0.04980267956852913\n",
            "step: 100, loss: 0.002725412603467703\n",
            "step: 110, loss: 0.15595610439777374\n",
            "step: 120, loss: 0.07787436246871948\n",
            "step: 130, loss: 0.005393470171838999\n",
            "step: 140, loss: 0.0005075006047263741\n",
            "step: 150, loss: 0.06326925754547119\n",
            "step: 160, loss: 0.0015701957745477557\n",
            "step: 170, loss: 0.003272648900747299\n",
            "step: 180, loss: 0.0023901392705738544\n",
            "step: 190, loss: 0.0021480161231011152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7379134860050889, f1=0.7185929648241205, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003055390843655914\n",
            "step: 10, loss: 0.02103791944682598\n",
            "step: 20, loss: 0.0025192287284880877\n",
            "step: 30, loss: 0.003224650863558054\n",
            "step: 40, loss: 0.0044760447926819324\n",
            "step: 50, loss: 0.0003558742464520037\n",
            "step: 60, loss: 0.12085886299610138\n",
            "step: 70, loss: 0.0009713189792819321\n",
            "step: 80, loss: 0.0004306879418436438\n",
            "step: 90, loss: 0.000607224996201694\n",
            "step: 100, loss: 0.0049642049707472324\n",
            "step: 110, loss: 0.0020091868937015533\n",
            "step: 120, loss: 0.004663917701691389\n",
            "step: 130, loss: 0.001521357917226851\n",
            "step: 140, loss: 0.04392214119434357\n",
            "step: 150, loss: 0.00026236657868139446\n",
            "step: 160, loss: 0.0002346217370359227\n",
            "step: 170, loss: 0.00030735658947378397\n",
            "step: 180, loss: 0.0011574814561754465\n",
            "step: 190, loss: 0.022845566272735596\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7374005305039788, f1=0.7180851063829788, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001191004877910018\n",
            "step: 10, loss: 0.035642992705106735\n",
            "step: 20, loss: 0.0012098030420020223\n",
            "step: 30, loss: 0.007105839904397726\n",
            "step: 40, loss: 0.0017981813289225101\n",
            "step: 50, loss: 0.0008245132048614323\n",
            "step: 60, loss: 0.053585492074489594\n",
            "step: 70, loss: 0.012769915163516998\n",
            "step: 80, loss: 0.0001479166530771181\n",
            "step: 90, loss: 0.00786976981908083\n",
            "step: 100, loss: 0.003235525218769908\n",
            "step: 110, loss: 0.03563076630234718\n",
            "step: 120, loss: 0.00045039504766464233\n",
            "step: 130, loss: 0.00037298694951459765\n",
            "step: 140, loss: 0.00021453676163218915\n",
            "step: 150, loss: 0.0033998102881014347\n",
            "step: 160, loss: 0.0001982936664717272\n",
            "step: 170, loss: 0.0005462515400722623\n",
            "step: 180, loss: 0.025367528200149536\n",
            "step: 190, loss: 0.000774907588493079\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7422680412371134, f1=0.7480916030534353, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002339248778298497\n",
            "step: 10, loss: 0.0009859782876446843\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.0004727219056803733\n",
            "step: 30, loss: 0.0009032501839101315\n",
            "step: 40, loss: 0.0003196817706339061\n",
            "step: 50, loss: 0.013870255090296268\n",
            "step: 60, loss: 0.00016929904813878238\n",
            "step: 70, loss: 0.0002451046311762184\n",
            "step: 80, loss: 0.0006968540255911648\n",
            "step: 90, loss: 0.0003556654555723071\n",
            "step: 100, loss: 0.0003302493423689157\n",
            "step: 110, loss: 0.0036323636304587126\n",
            "step: 120, loss: 0.00022856007853988558\n",
            "step: 130, loss: 0.0001270551438210532\n",
            "step: 140, loss: 0.0004376911383587867\n",
            "step: 150, loss: 0.0009217443875968456\n",
            "step: 160, loss: 0.0007663911674171686\n",
            "step: 170, loss: 0.014804001897573471\n",
            "step: 180, loss: 0.001679292763583362\n",
            "step: 190, loss: 0.015818068757653236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7231920199501246, f1=0.7002518891687657, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00039677994209341705\n",
            "step: 10, loss: 0.00018699257634580135\n",
            "step: 20, loss: 0.0005322537035681307\n",
            "step: 30, loss: 0.00026210182113572955\n",
            "step: 40, loss: 0.00013204799324739724\n",
            "step: 50, loss: 0.0002107013133354485\n",
            "step: 60, loss: 0.018055498600006104\n",
            "step: 70, loss: 0.009160938672721386\n",
            "step: 80, loss: 0.00024024331651162356\n",
            "step: 90, loss: 0.003010894637554884\n",
            "step: 100, loss: 0.00020403796224854887\n",
            "step: 110, loss: 0.00019991249428130686\n",
            "step: 120, loss: 0.000559369451366365\n",
            "step: 130, loss: 0.00042651151306927204\n",
            "step: 140, loss: 0.005035865120589733\n",
            "step: 150, loss: 0.00011000613449141383\n",
            "step: 160, loss: 0.0007133673643693328\n",
            "step: 170, loss: 0.0001826234656618908\n",
            "step: 180, loss: 0.00041425428935326636\n",
            "step: 190, loss: 0.0003543277271091938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7291666666666667, f1=0.7150537634408601, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004072831361554563\n",
            "step: 10, loss: 0.0035179746337234974\n",
            "step: 20, loss: 0.0008117855759337544\n",
            "step: 30, loss: 0.0005686834920197725\n",
            "step: 40, loss: 0.0007530883885920048\n",
            "step: 50, loss: 0.015476394444704056\n",
            "step: 60, loss: 0.0001446992246201262\n",
            "step: 70, loss: 0.00013066575047560036\n",
            "step: 80, loss: 8.046428411034867e-05\n",
            "step: 90, loss: 0.0002372709714109078\n",
            "step: 100, loss: 0.00024151831166818738\n",
            "step: 110, loss: 0.00014337255561258644\n",
            "step: 120, loss: 0.00011191137309651822\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.0001232007925864309\n",
            "step: 140, loss: 0.0003091042162850499\n",
            "step: 150, loss: 0.00017258110165130347\n",
            "step: 160, loss: 0.0001316325506195426\n",
            "step: 170, loss: 0.0001854624570114538\n",
            "step: 180, loss: 0.00017939710232894868\n",
            "step: 190, loss: 0.0006391702336259186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7286432160804021, f1=0.7070707070707071, best_f1=0.7465181058495822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040546301170252264\n",
            "step: 10, loss: 9.235880133928731e-05\n",
            "step: 20, loss: 0.00023153467918746173\n",
            "step: 30, loss: 0.00014647688658442348\n",
            "step: 40, loss: 0.010306903161108494\n",
            "step: 50, loss: 0.0002502302813809365\n",
            "step: 60, loss: 0.0001278312411159277\n",
            "step: 70, loss: 0.001043430995196104\n",
            "step: 80, loss: 0.0011130711063742638\n",
            "step: 90, loss: 0.012854116968810558\n",
            "step: 100, loss: 0.00011085846199421212\n",
            "step: 110, loss: 0.0007992989849299192\n",
            "step: 120, loss: 0.05724530667066574\n",
            "step: 130, loss: 0.0001366051146760583\n",
            "step: 140, loss: 0.00033742416417226195\n",
            "step: 150, loss: 0.0019611159805208445\n",
            "step: 160, loss: 0.0003101261972915381\n",
            "step: 170, loss: 0.00011800304491771385\n",
            "step: 180, loss: 0.0004358137084636837\n",
            "step: 190, loss: 0.0016532279551029205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7295285359801489, f1=0.7061728395061728, best_f1=0.7465181058495822\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 228.22it/s]\n",
            "load_f1 = 0.7611111111111111\n",
            "real_f1 = 0.7632311977715878\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 260.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedc66b5-7114-4786-df67-9f651f4c6164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8608489632606506\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.2306361198425293\n",
            "step: 20, loss: 0.15786631405353546\n",
            "step: 30, loss: 0.22978875041007996\n",
            "step: 40, loss: 0.30703914165496826\n",
            "step: 50, loss: 0.37227678298950195\n",
            "step: 60, loss: 0.43672850728034973\n",
            "step: 70, loss: 0.30695319175720215\n",
            "step: 80, loss: 0.26054614782333374\n",
            "step: 90, loss: 0.4046425223350525\n",
            "step: 100, loss: 0.2357613742351532\n",
            "step: 110, loss: 0.1955546736717224\n",
            "step: 120, loss: 0.5353491902351379\n",
            "step: 130, loss: 0.42340344190597534\n",
            "step: 140, loss: 0.500351071357727\n",
            "step: 150, loss: 0.10675344616174698\n",
            "step: 160, loss: 0.33875319361686707\n",
            "step: 170, loss: 0.24890460073947906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3233695652173913, f1=0.2845070422535212, best_f1=0.2845070422535212\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4072779715061188\n",
            "step: 10, loss: 0.17043247818946838\n",
            "step: 20, loss: 0.3252335786819458\n",
            "step: 30, loss: 0.10441413521766663\n",
            "step: 40, loss: 0.2601054310798645\n",
            "step: 50, loss: 0.2427932769060135\n",
            "step: 60, loss: 0.18038034439086914\n",
            "step: 70, loss: 0.33970171213150024\n",
            "step: 80, loss: 0.13977079093456268\n",
            "step: 90, loss: 0.18667010962963104\n",
            "step: 100, loss: 0.1401086449623108\n",
            "step: 110, loss: 0.2710365056991577\n",
            "step: 120, loss: 0.19576366245746613\n",
            "step: 130, loss: 0.09868723154067993\n",
            "step: 140, loss: 0.18892794847488403\n",
            "step: 150, loss: 0.14060582220554352\n",
            "step: 160, loss: 0.13928410410881042\n",
            "step: 170, loss: 0.11403030157089233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6578366445916115, f1=0.6285714285714287, best_f1=0.6285714285714287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15102450549602509\n",
            "step: 10, loss: 0.06257514655590057\n",
            "step: 20, loss: 0.042643673717975616\n",
            "step: 30, loss: 0.15827889740467072\n",
            "step: 40, loss: 0.0784616619348526\n",
            "step: 50, loss: 0.13982756435871124\n",
            "step: 60, loss: 0.3209995925426483\n",
            "step: 70, loss: 0.09058491140604019\n",
            "step: 80, loss: 0.10672695934772491\n",
            "step: 90, loss: 0.06594245135784149\n",
            "step: 100, loss: 0.06691823899745941\n",
            "step: 110, loss: 0.05932944640517235\n",
            "step: 120, loss: 0.025978952646255493\n",
            "step: 130, loss: 0.1581786870956421\n",
            "step: 140, loss: 0.010054334066808224\n",
            "step: 150, loss: 0.10858942568302155\n",
            "step: 160, loss: 0.04883319139480591\n",
            "step: 170, loss: 0.0863766223192215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7378190255220417, f1=0.6859688195991092, best_f1=0.6859688195991092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06646993011236191\n",
            "step: 10, loss: 0.22583507001399994\n",
            "step: 20, loss: 0.054685723036527634\n",
            "step: 30, loss: 0.09091121703386307\n",
            "step: 40, loss: 0.11596576124429703\n",
            "step: 50, loss: 0.01485427562147379\n",
            "step: 60, loss: 0.10364589840173721\n",
            "step: 70, loss: 0.008422932587563992\n",
            "step: 80, loss: 0.10685736685991287\n",
            "step: 90, loss: 0.028982147574424744\n",
            "step: 100, loss: 0.01848428137600422\n",
            "step: 110, loss: 0.06877042353153229\n",
            "step: 120, loss: 0.08383633196353912\n",
            "step: 130, loss: 0.024290766566991806\n",
            "step: 140, loss: 0.010420718230307102\n",
            "step: 150, loss: 0.04859531670808792\n",
            "step: 160, loss: 0.062092412263154984\n",
            "step: 170, loss: 0.0447167232632637\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7428571428571429, f1=0.7135922330097088, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025563135743141174\n",
            "step: 10, loss: 0.03206585347652435\n",
            "step: 20, loss: 0.05359407514333725\n",
            "step: 30, loss: 0.047410257160663605\n",
            "step: 40, loss: 0.01844029314815998\n",
            "step: 50, loss: 0.026204640045762062\n",
            "step: 60, loss: 0.00432957848533988\n",
            "step: 70, loss: 0.06607375293970108\n",
            "step: 80, loss: 0.005234838929027319\n",
            "step: 90, loss: 0.01804163306951523\n",
            "step: 100, loss: 0.039255235344171524\n",
            "step: 110, loss: 0.010012791492044926\n",
            "step: 120, loss: 0.18006351590156555\n",
            "step: 130, loss: 0.024119095876812935\n",
            "step: 140, loss: 0.0848955437541008\n",
            "step: 150, loss: 0.026105405762791634\n",
            "step: 160, loss: 0.0852922573685646\n",
            "step: 170, loss: 0.0341547355055809\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6993006993006994, f1=0.6790697674418605, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01078171655535698\n",
            "step: 10, loss: 0.09926334768533707\n",
            "step: 20, loss: 0.007981089875102043\n",
            "step: 30, loss: 0.0346548855304718\n",
            "step: 40, loss: 0.005476702935993671\n",
            "step: 50, loss: 0.05075956508517265\n",
            "step: 60, loss: 0.0860941931605339\n",
            "step: 70, loss: 0.009708035737276077\n",
            "step: 80, loss: 0.1805901974439621\n",
            "step: 90, loss: 0.10662203282117844\n",
            "step: 100, loss: 0.12548862397670746\n",
            "step: 110, loss: 0.07562781870365143\n",
            "step: 120, loss: 0.048590365797281265\n",
            "step: 130, loss: 0.0635543242096901\n",
            "step: 140, loss: 0.0048276688903570175\n",
            "step: 150, loss: 0.23831816017627716\n",
            "step: 160, loss: 0.020463738590478897\n",
            "step: 170, loss: 0.011381966061890125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6738544474393531, f1=0.70026525198939, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13507190346717834\n",
            "step: 10, loss: 0.00215840432792902\n",
            "step: 20, loss: 0.04030141979455948\n",
            "step: 30, loss: 0.2606649398803711\n",
            "step: 40, loss: 0.008718535304069519\n",
            "step: 50, loss: 0.0028141443617641926\n",
            "step: 60, loss: 0.24139319360256195\n",
            "step: 70, loss: 0.005369317252188921\n",
            "step: 80, loss: 0.009897513315081596\n",
            "step: 90, loss: 0.003791780211031437\n",
            "step: 100, loss: 0.013137671165168285\n",
            "step: 110, loss: 0.003135469974949956\n",
            "step: 120, loss: 0.08432486653327942\n",
            "step: 130, loss: 0.08432458341121674\n",
            "step: 140, loss: 0.005789721850305796\n",
            "step: 150, loss: 0.012553243897855282\n",
            "step: 160, loss: 0.01445254497230053\n",
            "step: 170, loss: 0.1506538689136505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7030878859857482, f1=0.6710816777041942, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028349293395876884\n",
            "step: 10, loss: 0.0036438528914004564\n",
            "step: 20, loss: 0.011244961060583591\n",
            "step: 30, loss: 0.045726001262664795\n",
            "step: 40, loss: 0.002263593254610896\n",
            "step: 50, loss: 0.02279164083302021\n",
            "step: 60, loss: 0.0292336605489254\n",
            "step: 70, loss: 0.005173825193196535\n",
            "step: 80, loss: 0.010438934899866581\n",
            "step: 90, loss: 0.04534563794732094\n",
            "step: 100, loss: 0.0066126869060099125\n",
            "step: 110, loss: 0.03647811710834503\n",
            "step: 120, loss: 0.005496607162058353\n",
            "step: 130, loss: 0.004590762313455343\n",
            "step: 140, loss: 0.012667232193052769\n",
            "step: 150, loss: 0.06976262480020523\n",
            "step: 160, loss: 0.008359510451555252\n",
            "step: 170, loss: 0.02176726423203945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6873508353221957, f1=0.6914153132250581, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002186572179198265\n",
            "step: 10, loss: 0.005979318171739578\n",
            "step: 20, loss: 0.007830441929399967\n",
            "step: 30, loss: 0.04662897065281868\n",
            "step: 40, loss: 0.07027726620435715\n",
            "step: 50, loss: 0.006503643933683634\n",
            "step: 60, loss: 0.0050629605539143085\n",
            "step: 70, loss: 0.1234280914068222\n",
            "step: 80, loss: 0.01615181565284729\n",
            "step: 90, loss: 0.0024676467292010784\n",
            "step: 100, loss: 0.010249617509543896\n",
            "step: 110, loss: 0.037427760660648346\n",
            "step: 120, loss: 0.013643576763570309\n",
            "step: 130, loss: 0.0006800565170124173\n",
            "step: 140, loss: 0.0028958735056221485\n",
            "step: 150, loss: 0.11422757059335709\n",
            "step: 160, loss: 0.00943986140191555\n",
            "step: 170, loss: 0.014903188683092594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6783042394014963, f1=0.6926829268292684, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017283431952819228\n",
            "step: 10, loss: 0.0019483546493574977\n",
            "step: 20, loss: 0.001757920952513814\n",
            "step: 30, loss: 0.00042814016342163086\n",
            "step: 40, loss: 0.001691766083240509\n",
            "step: 50, loss: 0.0028404048644006252\n",
            "step: 60, loss: 0.0007930818828754127\n",
            "step: 70, loss: 0.09644650667905807\n",
            "step: 80, loss: 0.002373490948230028\n",
            "step: 90, loss: 0.03551719710230827\n",
            "step: 100, loss: 0.0013830717653036118\n",
            "step: 110, loss: 0.002063463907688856\n",
            "step: 120, loss: 0.003945994656533003\n",
            "step: 130, loss: 0.007879858836531639\n",
            "step: 140, loss: 0.014978701248764992\n",
            "step: 150, loss: 0.0028499187901616096\n",
            "step: 160, loss: 0.0005892091430723667\n",
            "step: 170, loss: 0.002732378663495183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6699029126213593, f1=0.6854460093896715, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005722495261579752\n",
            "step: 10, loss: 0.16957320272922516\n",
            "step: 20, loss: 0.03840116411447525\n",
            "step: 30, loss: 0.012776648625731468\n",
            "step: 40, loss: 0.019223805516958237\n",
            "step: 50, loss: 0.006290270481258631\n",
            "step: 60, loss: 0.0019134259782731533\n",
            "step: 70, loss: 0.007444364950060844\n",
            "step: 80, loss: 0.0014965580776333809\n",
            "step: 90, loss: 0.00046078822924755514\n",
            "step: 100, loss: 0.0004698699340224266\n",
            "step: 110, loss: 0.0010344600304961205\n",
            "step: 120, loss: 0.21744151413440704\n",
            "step: 130, loss: 0.0024906406179070473\n",
            "step: 140, loss: 0.0073813581839203835\n",
            "step: 150, loss: 0.0005764496163465083\n",
            "step: 160, loss: 0.0005767575930804014\n",
            "step: 170, loss: 0.001406301511451602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6733167082294264, f1=0.6921241050119331, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006138970493339002\n",
            "step: 10, loss: 0.0034161675721406937\n",
            "step: 20, loss: 0.0020284440834075212\n",
            "step: 30, loss: 0.0009283761028200388\n",
            "step: 40, loss: 0.0005930141196586192\n",
            "step: 50, loss: 0.0004564239934552461\n",
            "step: 60, loss: 0.000434606074122712\n",
            "step: 70, loss: 0.0889548510313034\n",
            "step: 80, loss: 0.07870832830667496\n",
            "step: 90, loss: 0.000439872412243858\n",
            "step: 100, loss: 0.007066185120493174\n",
            "step: 110, loss: 0.0010723511222749949\n",
            "step: 120, loss: 0.0011838986538350582\n",
            "step: 130, loss: 0.00435336260125041\n",
            "step: 140, loss: 0.0009870856301859021\n",
            "step: 150, loss: 0.0008648126968182623\n",
            "step: 160, loss: 0.03263968974351883\n",
            "step: 170, loss: 0.005404551979154348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6785714285714286, f1=0.7067307692307692, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045267364475876093\n",
            "step: 10, loss: 0.0012000491842627525\n",
            "step: 20, loss: 0.0016279099509119987\n",
            "step: 30, loss: 0.0013702022843062878\n",
            "step: 40, loss: 0.0003961588372476399\n",
            "step: 50, loss: 0.0008367110276594758\n",
            "step: 60, loss: 0.002320550847798586\n",
            "step: 70, loss: 0.006494148168712854\n",
            "step: 80, loss: 0.0006656351615674794\n",
            "step: 90, loss: 0.00107954244595021\n",
            "step: 100, loss: 0.00022233369236346334\n",
            "step: 110, loss: 0.0007358766160905361\n",
            "step: 120, loss: 0.0010015858570113778\n",
            "step: 130, loss: 0.000831477576866746\n",
            "step: 140, loss: 0.0010164843406528234\n",
            "step: 150, loss: 0.0003658805217128247\n",
            "step: 160, loss: 0.002920010592788458\n",
            "step: 170, loss: 0.0006144240614958107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6761229314420805, f1=0.6909090909090909, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00043147403630428016\n",
            "step: 10, loss: 0.0005305611994117498\n",
            "step: 20, loss: 0.000985272927209735\n",
            "step: 30, loss: 0.0007386384531855583\n",
            "step: 40, loss: 0.13315363228321075\n",
            "step: 50, loss: 0.000905898807104677\n",
            "step: 60, loss: 0.004285107366740704\n",
            "step: 70, loss: 0.0005897236987948418\n",
            "step: 80, loss: 0.0006450993241742253\n",
            "step: 90, loss: 0.00028545389068312943\n",
            "step: 100, loss: 0.0004042798245791346\n",
            "step: 110, loss: 0.0011178087443113327\n",
            "step: 120, loss: 0.0005121275316923857\n",
            "step: 130, loss: 0.0005365686374716461\n",
            "step: 140, loss: 0.0004726376791950315\n",
            "step: 150, loss: 0.001865085680037737\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.03976484388113022\n",
            "step: 170, loss: 0.0003629612911026925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6649616368286446, f1=0.698019801980198, best_f1=0.7135922330097088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02682551182806492\n",
            "step: 10, loss: 0.0023801294155418873\n",
            "step: 20, loss: 0.0045781852677464485\n",
            "step: 30, loss: 0.028307868167757988\n",
            "step: 40, loss: 0.0002527753240428865\n",
            "step: 50, loss: 0.0003415956743992865\n",
            "step: 60, loss: 0.0005003409460186958\n",
            "step: 70, loss: 0.0006146466475911438\n",
            "step: 80, loss: 0.0004298147396184504\n",
            "step: 90, loss: 0.000909948255866766\n",
            "step: 100, loss: 0.0019051338313147426\n",
            "step: 110, loss: 0.00027872720966115594\n",
            "step: 120, loss: 0.0011606727493926883\n",
            "step: 130, loss: 0.03661429509520531\n",
            "step: 140, loss: 0.0002954011724796146\n",
            "step: 150, loss: 0.13186024129390717\n",
            "step: 160, loss: 0.00025200832169502974\n",
            "step: 170, loss: 0.00024375786597374827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6701030927835051, f1=0.6960784313725491, best_f1=0.7135922330097088\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:05, 374.30it/s]\n",
            "load_f1 = 0.7482014388489209\n",
            "real_f1 = 0.7400468384074942\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 372.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72dd0cb-774b-4667-ab55-a11582a36fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.814489483833313\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.48781806230545044\n",
            "step: 20, loss: 0.6145778894424438\n",
            "step: 30, loss: 0.5015151500701904\n",
            "step: 40, loss: 0.4429256021976471\n",
            "step: 50, loss: 0.3443760871887207\n",
            "step: 60, loss: 0.17493656277656555\n",
            "step: 70, loss: 0.2202814519405365\n",
            "step: 80, loss: 0.2898426353931427\n",
            "step: 90, loss: 0.06689826399087906\n",
            "step: 100, loss: 0.11768453568220139\n",
            "step: 110, loss: 0.13516592979431152\n",
            "step: 120, loss: 0.04825238138437271\n",
            "step: 130, loss: 0.05126592889428139\n",
            "step: 140, loss: 0.16812963783740997\n",
            "step: 150, loss: 0.09894108772277832\n",
            "step: 160, loss: 0.15776775777339935\n",
            "step: 170, loss: 0.04915890097618103\n",
            "step: 180, loss: 0.0133759630843997\n",
            "step: 190, loss: 0.05878227949142456\n",
            "step: 200, loss: 0.07085469365119934\n",
            "step: 210, loss: 0.03249067813158035\n",
            "step: 220, loss: 0.029665615409612656\n",
            "step: 230, loss: 0.031024234369397163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9345172031076582, f1=0.9382716049382717, best_f1=0.9382716049382717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09312225878238678\n",
            "step: 10, loss: 0.04596473649144173\n",
            "step: 20, loss: 0.07011017948389053\n",
            "step: 30, loss: 0.009113159030675888\n",
            "step: 40, loss: 0.05269140005111694\n",
            "step: 50, loss: 0.11784442514181137\n",
            "step: 60, loss: 0.10828869789838791\n",
            "step: 70, loss: 0.18944597244262695\n",
            "step: 80, loss: 0.019028257578611374\n",
            "step: 90, loss: 0.00617460161447525\n",
            "step: 100, loss: 0.20960800349712372\n",
            "step: 110, loss: 0.09487877041101456\n",
            "step: 120, loss: 0.03710990399122238\n",
            "step: 130, loss: 0.21669110655784607\n",
            "step: 140, loss: 0.13401702046394348\n",
            "step: 150, loss: 0.057399407029151917\n",
            "step: 160, loss: 0.03680230677127838\n",
            "step: 170, loss: 0.023280341178178787\n",
            "step: 180, loss: 0.010061162523925304\n",
            "step: 190, loss: 0.09097343683242798\n",
            "step: 200, loss: 0.05020042508840561\n",
            "step: 210, loss: 0.12499090284109116\n",
            "step: 220, loss: 0.007273358292877674\n",
            "step: 230, loss: 0.0814865231513977\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9395973154362416, f1=0.9434389140271494, best_f1=0.9434389140271494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07387372106313705\n",
            "step: 10, loss: 0.04363185912370682\n",
            "step: 20, loss: 0.005947090219706297\n",
            "step: 30, loss: 0.04849690571427345\n",
            "step: 40, loss: 0.15038079023361206\n",
            "step: 50, loss: 0.042236726731061935\n",
            "step: 60, loss: 0.03578031063079834\n",
            "step: 70, loss: 0.002798483707010746\n",
            "step: 80, loss: 0.11811491847038269\n",
            "step: 90, loss: 0.16171254217624664\n",
            "step: 100, loss: 0.005251784343272448\n",
            "step: 110, loss: 0.05005211383104324\n",
            "step: 120, loss: 0.07108264416456223\n",
            "step: 130, loss: 0.0038537951186299324\n",
            "step: 140, loss: 0.001957807457074523\n",
            "step: 150, loss: 0.0531463585793972\n",
            "step: 160, loss: 0.05137402191758156\n",
            "step: 170, loss: 0.0599118135869503\n",
            "step: 180, loss: 0.0247525442391634\n",
            "step: 190, loss: 0.007204726338386536\n",
            "step: 200, loss: 0.00431451853364706\n",
            "step: 210, loss: 0.027381982654333115\n",
            "step: 220, loss: 0.005892922170460224\n",
            "step: 230, loss: 0.10644371807575226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.953229398663697, f1=0.9430167597765362, best_f1=0.9430167597765362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02168833091855049\n",
            "step: 10, loss: 0.01190017256885767\n",
            "step: 20, loss: 0.003302921773865819\n",
            "step: 30, loss: 0.0038575888611376286\n",
            "step: 40, loss: 0.012498910538852215\n",
            "step: 50, loss: 0.0032823635265231133\n",
            "step: 60, loss: 0.0014145669993013144\n",
            "step: 70, loss: 0.14791738986968994\n",
            "step: 80, loss: 0.24825721979141235\n",
            "step: 90, loss: 0.004677459131926298\n",
            "step: 100, loss: 0.005616261623799801\n",
            "step: 110, loss: 0.06428786367177963\n",
            "step: 120, loss: 0.01292477734386921\n",
            "step: 130, loss: 0.0021924260072410107\n",
            "step: 140, loss: 0.008263214491307735\n",
            "step: 150, loss: 0.001944119343534112\n",
            "step: 160, loss: 0.0012762050610035658\n",
            "step: 170, loss: 0.008128328248858452\n",
            "step: 180, loss: 0.12417804449796677\n",
            "step: 190, loss: 0.001468174858018756\n",
            "step: 200, loss: 0.005914758425205946\n",
            "step: 210, loss: 0.07733229547739029\n",
            "step: 220, loss: 0.0026664386969059706\n",
            "step: 230, loss: 0.07581069320440292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9508928571428572, f1=0.938132733408324, best_f1=0.9430167597765362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018843134865164757\n",
            "step: 10, loss: 0.014271107502281666\n",
            "step: 20, loss: 0.0049428013153374195\n",
            "step: 30, loss: 0.00028025006758980453\n",
            "step: 40, loss: 0.007346214726567268\n",
            "step: 50, loss: 0.000676060444675386\n",
            "step: 60, loss: 0.0030631425324827433\n",
            "step: 70, loss: 0.0004825841751880944\n",
            "step: 80, loss: 0.0014323705108836293\n",
            "step: 90, loss: 0.001632949337363243\n",
            "step: 100, loss: 0.008886642754077911\n",
            "step: 110, loss: 0.0021135490387678146\n",
            "step: 120, loss: 0.032504212111234665\n",
            "step: 130, loss: 0.048409052193164825\n",
            "step: 140, loss: 0.002185484394431114\n",
            "step: 150, loss: 0.006427498999983072\n",
            "step: 160, loss: 0.15850242972373962\n",
            "step: 170, loss: 0.017152005806565285\n",
            "step: 180, loss: 0.004396806936711073\n",
            "step: 190, loss: 0.0017112703062593937\n",
            "step: 200, loss: 0.0030371826142072678\n",
            "step: 210, loss: 0.0023610065691173077\n",
            "step: 220, loss: 0.0034641982056200504\n",
            "step: 230, loss: 0.0013139168731868267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9552572706935123, f1=0.9461883408071748, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036174370907247066\n",
            "step: 10, loss: 0.06631707400083542\n",
            "step: 20, loss: 0.011328352615237236\n",
            "step: 30, loss: 0.004946187138557434\n",
            "step: 40, loss: 0.017170144245028496\n",
            "step: 50, loss: 0.0038986122235655785\n",
            "step: 60, loss: 0.02239209972321987\n",
            "step: 70, loss: 0.00866083987057209\n",
            "step: 80, loss: 0.0033191777765750885\n",
            "step: 90, loss: 0.00149985880125314\n",
            "step: 100, loss: 0.003917364869266748\n",
            "step: 110, loss: 0.09822045266628265\n",
            "step: 120, loss: 0.0011497812811285257\n",
            "step: 130, loss: 0.0011015981435775757\n",
            "step: 140, loss: 0.0017664044862613082\n",
            "step: 150, loss: 0.0006781133124604821\n",
            "step: 160, loss: 0.00171833171043545\n",
            "step: 170, loss: 0.0010141978273168206\n",
            "step: 180, loss: 0.008912323042750359\n",
            "step: 190, loss: 0.0113186314702034\n",
            "step: 200, loss: 0.0037631921004503965\n",
            "step: 210, loss: 0.0049108886159956455\n",
            "step: 220, loss: 0.0007829729001969099\n",
            "step: 230, loss: 0.05152969807386398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9467849223946784, f1=0.936026936026936, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1842779815196991\n",
            "step: 10, loss: 0.0019002215703949332\n",
            "step: 20, loss: 0.0018070163205265999\n",
            "step: 30, loss: 0.0162779800593853\n",
            "step: 40, loss: 0.0022824800107628107\n",
            "step: 50, loss: 0.0007632087799720466\n",
            "step: 60, loss: 0.03466685116291046\n",
            "step: 70, loss: 0.06376931816339493\n",
            "step: 80, loss: 0.0026289261877536774\n",
            "step: 90, loss: 0.005540068726986647\n",
            "step: 100, loss: 0.0009424446034245193\n",
            "step: 110, loss: 0.0015039379941299558\n",
            "step: 120, loss: 0.012878300622105598\n",
            "step: 130, loss: 0.0021747665014117956\n",
            "step: 140, loss: 0.0008224168559536338\n",
            "step: 150, loss: 0.0016475036973133683\n",
            "step: 160, loss: 0.00039900446427054703\n",
            "step: 170, loss: 0.0006072286632843316\n",
            "step: 180, loss: 0.000999621581286192\n",
            "step: 190, loss: 0.0006927552167326212\n",
            "step: 200, loss: 0.005892452318221331\n",
            "step: 210, loss: 0.0005156842526048422\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 220, loss: 0.03137669712305069\n",
            "step: 230, loss: 0.04280814900994301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9364035087719298, f1=0.9377777777777777, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03427577391266823\n",
            "step: 10, loss: 0.059399355202913284\n",
            "step: 20, loss: 0.0022849508095532656\n",
            "step: 30, loss: 0.0017271110555157065\n",
            "step: 40, loss: 0.0014912192709743977\n",
            "step: 50, loss: 0.017460854724049568\n",
            "step: 60, loss: 0.0007821201579645276\n",
            "step: 70, loss: 0.0042071957141160965\n",
            "step: 80, loss: 0.00978758279234171\n",
            "step: 90, loss: 0.003249618923291564\n",
            "step: 100, loss: 0.0010350922821089625\n",
            "step: 110, loss: 0.010656801983714104\n",
            "step: 120, loss: 0.00019407464424148202\n",
            "step: 130, loss: 0.023637397214770317\n",
            "step: 140, loss: 0.00028941419441252947\n",
            "step: 150, loss: 0.0003937128640245646\n",
            "step: 160, loss: 0.0008830523584038019\n",
            "step: 170, loss: 0.00031563377706333995\n",
            "step: 180, loss: 0.004249304998666048\n",
            "step: 190, loss: 0.006661190651357174\n",
            "step: 200, loss: 0.01505704503506422\n",
            "step: 210, loss: 0.009896811097860336\n",
            "step: 220, loss: 0.0017104515573009849\n",
            "step: 230, loss: 0.19974835216999054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9480812641083521, f1=0.9350057012542761, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018136498692911118\n",
            "step: 10, loss: 0.0004314029647503048\n",
            "step: 20, loss: 0.004871074575930834\n",
            "step: 30, loss: 0.0007293711532838643\n",
            "step: 40, loss: 0.00023629871429875493\n",
            "step: 50, loss: 0.0001951568410731852\n",
            "step: 60, loss: 0.0004413228889461607\n",
            "step: 70, loss: 0.0006013137171976268\n",
            "step: 80, loss: 0.0209847129881382\n",
            "step: 90, loss: 0.0006841900758445263\n",
            "step: 100, loss: 0.004429434891790152\n",
            "step: 110, loss: 0.001515159267000854\n",
            "step: 120, loss: 0.11540920287370682\n",
            "step: 130, loss: 0.0007137209759093821\n",
            "step: 140, loss: 0.08536972850561142\n",
            "step: 150, loss: 0.0001819443132262677\n",
            "step: 160, loss: 0.0012449441710487008\n",
            "step: 170, loss: 0.0008118193945847452\n",
            "step: 180, loss: 0.00045070311171002686\n",
            "step: 190, loss: 0.0004027529212180525\n",
            "step: 200, loss: 0.00033341243397444487\n",
            "step: 210, loss: 0.0004629569302778691\n",
            "step: 220, loss: 0.00028603570535779\n",
            "step: 230, loss: 0.0012782547855749726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9443813847900113, f1=0.9260969976905312, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038023124216124415\n",
            "step: 10, loss: 0.00029298049048520625\n",
            "step: 20, loss: 0.0005608895444311202\n",
            "step: 30, loss: 0.00036277694744057953\n",
            "step: 40, loss: 0.00010263931471854448\n",
            "step: 50, loss: 0.0018161280313506722\n",
            "step: 60, loss: 0.05529871582984924\n",
            "step: 70, loss: 0.004636180587112904\n",
            "step: 80, loss: 0.00413444684818387\n",
            "step: 90, loss: 0.00023643262102268636\n",
            "step: 100, loss: 0.00019456727022770792\n",
            "step: 110, loss: 0.00017566238238941878\n",
            "step: 120, loss: 0.0035341728944331408\n",
            "step: 130, loss: 0.006881463807076216\n",
            "step: 140, loss: 0.024253062903881073\n",
            "step: 150, loss: 0.0002908961905632168\n",
            "step: 160, loss: 0.001187468646094203\n",
            "step: 170, loss: 0.00043741019908338785\n",
            "step: 180, loss: 0.0013614136260002851\n",
            "step: 190, loss: 0.000800392939709127\n",
            "step: 200, loss: 0.000234966617426835\n",
            "step: 210, loss: 0.0005636237328872085\n",
            "step: 220, loss: 0.0006354405195452273\n",
            "step: 230, loss: 0.0013088478008285165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.947250280583614, f1=0.9389140271493213, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004198603564873338\n",
            "step: 10, loss: 0.004118913318961859\n",
            "step: 20, loss: 0.0002315440506208688\n",
            "step: 30, loss: 0.0006447188789024949\n",
            "step: 40, loss: 0.0027423801366239786\n",
            "step: 50, loss: 0.001040876260958612\n",
            "step: 60, loss: 0.0003576611343305558\n",
            "step: 70, loss: 0.004111029207706451\n",
            "step: 80, loss: 0.00034814196988008916\n",
            "step: 90, loss: 0.0927751436829567\n",
            "step: 100, loss: 0.0021873670630156994\n",
            "step: 110, loss: 0.014122658409178257\n",
            "step: 120, loss: 0.0011070016771554947\n",
            "step: 130, loss: 0.0004433701396919787\n",
            "step: 140, loss: 0.0001281259028473869\n",
            "step: 150, loss: 0.0001215523443534039\n",
            "step: 160, loss: 0.00017043172556441277\n",
            "step: 170, loss: 0.0008681650506332517\n",
            "step: 180, loss: 0.00032899691723287106\n",
            "step: 190, loss: 0.0002501935523469001\n",
            "step: 200, loss: 0.00017400133947376162\n",
            "step: 210, loss: 0.0001272126828553155\n",
            "step: 220, loss: 0.0005011051543988287\n",
            "step: 230, loss: 0.0009086175705306232\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9440000000000001, f1=0.916083916083916, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003549036046024412\n",
            "step: 10, loss: 0.0018283799290657043\n",
            "step: 20, loss: 0.00018217445176560432\n",
            "step: 30, loss: 0.0002489920298103243\n",
            "step: 40, loss: 0.00010961965745082125\n",
            "step: 50, loss: 0.00019329015049152076\n",
            "step: 60, loss: 0.014678620733320713\n",
            "step: 70, loss: 0.00030910249915905297\n",
            "step: 80, loss: 9.587375825503841e-05\n",
            "step: 90, loss: 0.00014131487114354968\n",
            "step: 100, loss: 0.00031166159897111356\n",
            "step: 110, loss: 0.00029824141529388726\n",
            "step: 120, loss: 0.0003055720590054989\n",
            "step: 130, loss: 0.00017644606123212725\n",
            "step: 140, loss: 0.00015948963118717074\n",
            "step: 150, loss: 7.172362529672682e-05\n",
            "step: 160, loss: 9.150299592874944e-05\n",
            "step: 170, loss: 0.00017430954903829843\n",
            "step: 180, loss: 0.00012627728574443609\n",
            "step: 190, loss: 0.00012244636309333146\n",
            "step: 200, loss: 0.0005643479526042938\n",
            "step: 210, loss: 0.00027329556178301573\n",
            "step: 220, loss: 0.03390584513545036\n",
            "step: 230, loss: 0.00026270089438185096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9426048565121413, f1=0.9473684210526316, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.217434853781015e-05\n",
            "step: 10, loss: 9.328242595074698e-05\n",
            "step: 20, loss: 0.0001902538351714611\n",
            "step: 30, loss: 0.024083368480205536\n",
            "step: 40, loss: 0.0003345845325384289\n",
            "step: 50, loss: 0.00014077601372264326\n",
            "step: 60, loss: 0.00010255259985569865\n",
            "step: 70, loss: 0.00017633329844102263\n",
            "step: 80, loss: 0.004467830527573824\n",
            "step: 90, loss: 6.996157026151195e-05\n",
            "step: 100, loss: 0.00018755388737190515\n",
            "step: 110, loss: 0.00010629291500663385\n",
            "step: 120, loss: 0.00021175383881200105\n",
            "step: 130, loss: 0.00023204507306218147\n",
            "step: 140, loss: 0.002609068527817726\n",
            "step: 150, loss: 0.00817249808460474\n",
            "step: 160, loss: 0.00017737555026542395\n",
            "step: 170, loss: 0.0001362945040455088\n",
            "step: 180, loss: 0.00022174607147462666\n",
            "step: 190, loss: 0.0001198849204229191\n",
            "step: 200, loss: 0.00012177741882624105\n",
            "step: 210, loss: 0.00011519790132297203\n",
            "step: 220, loss: 8.043617708608508e-05\n",
            "step: 230, loss: 8.586268813814968e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9423728813559321, f1=0.9331797235023043, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010121092782355845\n",
            "step: 10, loss: 7.805632776580751e-05\n",
            "step: 20, loss: 0.000440483505371958\n",
            "step: 30, loss: 0.00043172293226234615\n",
            "step: 40, loss: 8.26369141577743e-05\n",
            "step: 50, loss: 0.000250777491601184\n",
            "step: 60, loss: 0.00021370782633312047\n",
            "step: 70, loss: 9.947707440005615e-05\n",
            "step: 80, loss: 0.00016048431280069053\n",
            "step: 90, loss: 8.929677278501913e-05\n",
            "step: 100, loss: 0.0004242270370014012\n",
            "step: 110, loss: 0.003277908079326153\n",
            "step: 120, loss: 0.013033553026616573\n",
            "step: 130, loss: 0.00015380473632831126\n",
            "step: 140, loss: 8.577782864449546e-05\n",
            "step: 150, loss: 0.00011335036106174812\n",
            "step: 160, loss: 6.962811312405393e-05\n",
            "step: 170, loss: 0.00012218556366860867\n",
            "step: 180, loss: 0.00026012645685113966\n",
            "step: 190, loss: 0.0007045022211968899\n",
            "step: 200, loss: 6.601051427423954e-05\n",
            "step: 210, loss: 0.0013154015177860856\n",
            "step: 220, loss: 0.00018242215446662158\n",
            "step: 230, loss: 4.8850375605979934e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9425028184892897, f1=0.9325714285714286, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001255348906852305\n",
            "step: 10, loss: 0.000165329227456823\n",
            "step: 20, loss: 0.0001674996456131339\n",
            "step: 30, loss: 0.0010954377939924598\n",
            "step: 40, loss: 0.00034872785909101367\n",
            "step: 50, loss: 0.0001577125076437369\n",
            "step: 60, loss: 4.591152537614107e-05\n",
            "step: 70, loss: 0.00011567794717848301\n",
            "step: 80, loss: 0.00015750553575344384\n",
            "step: 90, loss: 0.00015372515190392733\n",
            "step: 100, loss: 0.00015356627409346402\n",
            "step: 110, loss: 0.00011476752115413547\n",
            "step: 120, loss: 0.00024111766833811998\n",
            "step: 130, loss: 0.0009544765343889594\n",
            "step: 140, loss: 0.005434723105281591\n",
            "step: 150, loss: 0.00013531824515666813\n",
            "step: 160, loss: 0.0006853314116597176\n",
            "step: 170, loss: 6.568078242707998e-05\n",
            "step: 180, loss: 0.00014167727204039693\n",
            "step: 190, loss: 0.0037640095688402653\n",
            "step: 200, loss: 8.812181476969272e-05\n",
            "step: 210, loss: 0.0023857520427554846\n",
            "step: 220, loss: 0.027688302099704742\n",
            "step: 230, loss: 0.0004663541913032532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9427609427609427, f1=0.937428896473265, best_f1=0.9461883408071748\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:08, 290.12it/s]\n",
            "load_f1 = 0.9541899441340782\n",
            "real_f1 = 0.9541899441340782\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:12, 360.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab21e406-f0a3-4d60-90e0-f9409d2d26fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7996423840522766\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4531988799571991\n",
            "step: 20, loss: 0.5181674361228943\n",
            "step: 30, loss: 0.47831034660339355\n",
            "step: 40, loss: 0.4772384762763977\n",
            "step: 50, loss: 0.43809208273887634\n",
            "step: 60, loss: 0.480212926864624\n",
            "step: 70, loss: 0.19481585919857025\n",
            "step: 80, loss: 0.37870287895202637\n",
            "step: 90, loss: 0.39151132106781006\n",
            "step: 100, loss: 0.24875375628471375\n",
            "step: 110, loss: 0.179433211684227\n",
            "step: 120, loss: 0.12311061471700668\n",
            "step: 130, loss: 0.03741825371980667\n",
            "step: 140, loss: 0.29342958331108093\n",
            "step: 150, loss: 0.08082432299852371\n",
            "step: 160, loss: 0.16619153320789337\n",
            "step: 170, loss: 0.34405767917633057\n",
            "step: 180, loss: 0.18068313598632812\n",
            "step: 190, loss: 0.08044296503067017\n",
            "step: 200, loss: 0.15360690653324127\n",
            "step: 210, loss: 0.11788473278284073\n",
            "step: 220, loss: 0.07096456736326218\n",
            "step: 230, loss: 0.08059045672416687\n",
            "step: 240, loss: 0.09163957834243774\n",
            "step: 250, loss: 0.10735619068145752\n",
            "step: 260, loss: 0.03097505308687687\n",
            "step: 270, loss: 0.022827940061688423\n",
            "step: 280, loss: 0.2633565664291382\n",
            "step: 290, loss: 0.11020497232675552\n",
            "step: 300, loss: 0.1555689573287964\n",
            "step: 310, loss: 0.06299659609794617\n",
            "step: 320, loss: 0.12660396099090576\n",
            "step: 330, loss: 0.1233941838145256\n",
            "step: 340, loss: 0.2544216513633728\n",
            "step: 350, loss: 0.05846628174185753\n",
            "step: 360, loss: 0.09937279671430588\n",
            "step: 370, loss: 0.1422802358865738\n",
            "step: 380, loss: 0.32564279437065125\n",
            "step: 390, loss: 0.13509249687194824\n",
            "step: 400, loss: 0.03043108619749546\n",
            "step: 410, loss: 0.11129089444875717\n",
            "step: 420, loss: 0.0990593358874321\n",
            "step: 430, loss: 0.20507925748825073\n",
            "step: 440, loss: 0.13442477583885193\n",
            "step: 450, loss: 0.07429838925600052\n",
            "step: 460, loss: 0.02586640976369381\n",
            "step: 470, loss: 0.1376579999923706\n",
            "step: 480, loss: 0.23634764552116394\n",
            "step: 490, loss: 0.06236817315220833\n",
            "step: 500, loss: 0.033296599984169006\n",
            "step: 510, loss: 0.145770862698555\n",
            "step: 520, loss: 0.12511411309242249\n",
            "step: 530, loss: 0.037904445081949234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9021937842778793, f1=0.8985773290500229, best_f1=0.8985773290500229\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09967906028032303\n",
            "step: 10, loss: 0.22516654431819916\n",
            "step: 20, loss: 0.09377527981996536\n",
            "step: 30, loss: 0.07763013243675232\n",
            "step: 40, loss: 0.011509106494486332\n",
            "step: 50, loss: 0.08416309207677841\n",
            "step: 60, loss: 0.13866916298866272\n",
            "step: 70, loss: 0.16718167066574097\n",
            "step: 80, loss: 0.034490592777729034\n",
            "step: 90, loss: 0.12437502294778824\n",
            "step: 100, loss: 0.29771339893341064\n",
            "step: 110, loss: 0.11125250160694122\n",
            "step: 120, loss: 0.06341274827718735\n",
            "step: 130, loss: 0.08511684834957123\n",
            "step: 140, loss: 0.06989709287881851\n",
            "step: 150, loss: 0.06103016436100006\n",
            "step: 160, loss: 0.0423630066215992\n",
            "step: 170, loss: 0.21955378353595734\n",
            "step: 180, loss: 0.08263280987739563\n",
            "step: 190, loss: 0.014768457971513271\n",
            "step: 200, loss: 0.06735427677631378\n",
            "step: 210, loss: 0.027165982872247696\n",
            "step: 220, loss: 0.14469054341316223\n",
            "step: 230, loss: 0.12923437356948853\n",
            "step: 240, loss: 0.12241664528846741\n",
            "step: 250, loss: 0.0850035771727562\n",
            "step: 260, loss: 0.09146285057067871\n",
            "step: 270, loss: 0.19106930494308472\n",
            "step: 280, loss: 0.23906055092811584\n",
            "step: 290, loss: 0.07488849759101868\n",
            "step: 300, loss: 0.07577686011791229\n",
            "step: 310, loss: 0.11461343616247177\n",
            "step: 320, loss: 0.16370168328285217\n",
            "step: 330, loss: 0.06720801442861557\n",
            "step: 340, loss: 0.008895430713891983\n",
            "step: 350, loss: 0.17046263813972473\n",
            "step: 360, loss: 0.0758349671959877\n",
            "step: 370, loss: 0.0589132159948349\n",
            "step: 380, loss: 0.14950847625732422\n",
            "step: 390, loss: 0.037243977189064026\n",
            "step: 400, loss: 0.12677551805973053\n",
            "step: 410, loss: 0.00222926028072834\n",
            "step: 420, loss: 0.12465258687734604\n",
            "step: 430, loss: 0.03930887579917908\n",
            "step: 440, loss: 0.017550883814692497\n",
            "step: 450, loss: 0.1103738471865654\n",
            "step: 460, loss: 0.15866313874721527\n",
            "step: 470, loss: 0.024800147861242294\n",
            "step: 480, loss: 0.38800978660583496\n",
            "step: 490, loss: 0.040572915226221085\n",
            "step: 500, loss: 0.01645093783736229\n",
            "step: 510, loss: 0.06573927402496338\n",
            "step: 520, loss: 0.044113464653491974\n",
            "step: 530, loss: 0.06922230124473572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9154275092936803, f1=0.9039704524469068, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04170197993516922\n",
            "step: 10, loss: 0.05267787724733353\n",
            "step: 20, loss: 0.299456924200058\n",
            "step: 30, loss: 0.1531735360622406\n",
            "step: 40, loss: 0.057415809482336044\n",
            "step: 50, loss: 0.06906771659851074\n",
            "step: 60, loss: 0.0064699179492890835\n",
            "step: 70, loss: 0.031685199588537216\n",
            "step: 80, loss: 0.06531237065792084\n",
            "step: 90, loss: 0.3142153322696686\n",
            "step: 100, loss: 0.09282438457012177\n",
            "step: 110, loss: 0.01836032047867775\n",
            "step: 120, loss: 0.015150632709264755\n",
            "step: 130, loss: 0.02909386157989502\n",
            "step: 140, loss: 0.08229197561740875\n",
            "step: 150, loss: 0.005517967510968447\n",
            "step: 160, loss: 0.04853542521595955\n",
            "step: 170, loss: 0.007920455187559128\n",
            "step: 180, loss: 0.11575141549110413\n",
            "step: 190, loss: 0.012545754201710224\n",
            "step: 200, loss: 0.03304686397314072\n",
            "step: 210, loss: 0.1720399409532547\n",
            "step: 220, loss: 0.027444183826446533\n",
            "step: 230, loss: 0.011568532325327396\n",
            "step: 240, loss: 0.08452831953763962\n",
            "step: 250, loss: 0.03154844045639038\n",
            "step: 260, loss: 0.0029951445758342743\n",
            "step: 270, loss: 0.0377519465982914\n",
            "step: 280, loss: 0.02571670524775982\n",
            "step: 290, loss: 0.08456654101610184\n",
            "step: 300, loss: 0.0683378130197525\n",
            "step: 310, loss: 0.0967329815030098\n",
            "step: 320, loss: 0.1041552945971489\n",
            "step: 330, loss: 0.08043544739484787\n",
            "step: 340, loss: 0.006338839884847403\n",
            "step: 350, loss: 0.13880249857902527\n",
            "step: 360, loss: 0.012198081240057945\n",
            "step: 370, loss: 0.020007692277431488\n",
            "step: 380, loss: 0.006063760723918676\n",
            "step: 390, loss: 0.047482337802648544\n",
            "step: 400, loss: 0.028260059654712677\n",
            "step: 410, loss: 0.01569652184844017\n",
            "step: 420, loss: 0.08313288539648056\n",
            "step: 430, loss: 0.06184730306267738\n",
            "step: 440, loss: 0.042618878185749054\n",
            "step: 450, loss: 0.13794319331645966\n",
            "step: 460, loss: 0.21523648500442505\n",
            "step: 470, loss: 0.011034335941076279\n",
            "step: 480, loss: 0.015758804976940155\n",
            "step: 490, loss: 0.0063642049208283424\n",
            "step: 500, loss: 0.08859975636005402\n",
            "step: 510, loss: 0.080877386033535\n",
            "step: 520, loss: 0.0035205066669732332\n",
            "step: 530, loss: 0.02686295285820961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9049394221808016, f1=0.8969411764705882, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022679559886455536\n",
            "step: 10, loss: 0.10009033977985382\n",
            "step: 20, loss: 0.048448387533426285\n",
            "step: 30, loss: 0.049630388617515564\n",
            "step: 40, loss: 0.08614019304513931\n",
            "step: 50, loss: 0.005322567652910948\n",
            "step: 60, loss: 0.022777339443564415\n",
            "step: 70, loss: 0.00982017070055008\n",
            "step: 80, loss: 0.03871545195579529\n",
            "step: 90, loss: 0.08268319070339203\n",
            "step: 100, loss: 0.0029233302921056747\n",
            "step: 110, loss: 0.005875647068023682\n",
            "step: 120, loss: 0.05305924266576767\n",
            "step: 130, loss: 0.06675305962562561\n",
            "step: 140, loss: 0.018150314688682556\n",
            "step: 150, loss: 0.0018223256338387728\n",
            "step: 160, loss: 0.012516687624156475\n",
            "step: 170, loss: 0.002706179628148675\n",
            "step: 180, loss: 0.011110854335129261\n",
            "step: 190, loss: 0.008068357594311237\n",
            "step: 200, loss: 0.0016926862299442291\n",
            "step: 210, loss: 0.022151025012135506\n",
            "step: 220, loss: 0.009807681664824486\n",
            "step: 230, loss: 0.2869276702404022\n",
            "step: 240, loss: 0.007965579628944397\n",
            "step: 250, loss: 0.08467801660299301\n",
            "step: 260, loss: 0.01657911203801632\n",
            "step: 270, loss: 0.043173860758543015\n",
            "step: 280, loss: 0.024567391723394394\n",
            "step: 290, loss: 0.12247809022665024\n",
            "step: 300, loss: 0.013722556643188\n",
            "step: 310, loss: 0.010951833799481392\n",
            "step: 320, loss: 0.09152722358703613\n",
            "step: 330, loss: 0.06959055364131927\n",
            "step: 340, loss: 0.009050016291439533\n",
            "step: 350, loss: 0.016076255589723587\n",
            "step: 360, loss: 0.012883871793746948\n",
            "step: 370, loss: 0.004603770095854998\n",
            "step: 380, loss: 0.0036002707201987505\n",
            "step: 390, loss: 0.004201050382107496\n",
            "step: 400, loss: 0.028482254594564438\n",
            "step: 410, loss: 0.015498366206884384\n",
            "step: 420, loss: 0.05132339149713516\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 430, loss: 0.028038162738084793\n",
            "step: 440, loss: 0.11277024447917938\n",
            "step: 450, loss: 0.006867324002087116\n",
            "step: 460, loss: 0.008198808878660202\n",
            "step: 470, loss: 0.004507246892899275\n",
            "step: 480, loss: 0.05769260227680206\n",
            "step: 490, loss: 0.04425814747810364\n",
            "step: 500, loss: 0.018195437267422676\n",
            "step: 510, loss: 0.02143595553934574\n",
            "step: 520, loss: 0.12350798398256302\n",
            "step: 530, loss: 0.004333063960075378\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9046500704556129, f1=0.8944392082940621, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003550855442881584\n",
            "step: 10, loss: 0.05871516093611717\n",
            "step: 20, loss: 0.057234372943639755\n",
            "step: 30, loss: 0.008429497480392456\n",
            "step: 40, loss: 0.049983371049165726\n",
            "step: 50, loss: 0.021753134205937386\n",
            "step: 60, loss: 0.010459651239216328\n",
            "step: 70, loss: 0.001984945498406887\n",
            "step: 80, loss: 0.001101384754292667\n",
            "step: 90, loss: 0.009941254742443562\n",
            "step: 100, loss: 0.0024110875092446804\n",
            "step: 110, loss: 0.001979703549295664\n",
            "step: 120, loss: 0.0383453443646431\n",
            "step: 130, loss: 0.013727056793868542\n",
            "step: 140, loss: 0.01629684306681156\n",
            "step: 150, loss: 0.03771762177348137\n",
            "step: 160, loss: 0.16033999621868134\n",
            "step: 170, loss: 0.009970476850867271\n",
            "step: 180, loss: 0.08020877093076706\n",
            "step: 190, loss: 0.015425290912389755\n",
            "step: 200, loss: 0.0038672632072120905\n",
            "step: 210, loss: 0.0030196458101272583\n",
            "step: 220, loss: 0.0023867327254265547\n",
            "step: 230, loss: 0.010620261542499065\n",
            "step: 240, loss: 0.05411624163389206\n",
            "step: 250, loss: 0.002079189755022526\n",
            "step: 260, loss: 0.020764803513884544\n",
            "step: 270, loss: 0.007960702292621136\n",
            "step: 280, loss: 0.06865067780017853\n",
            "step: 290, loss: 0.0006597767933271825\n",
            "step: 300, loss: 0.015359065495431423\n",
            "step: 310, loss: 0.0007404147181659937\n",
            "step: 320, loss: 0.00537307932972908\n",
            "step: 330, loss: 0.11750238388776779\n",
            "step: 340, loss: 0.00276299100369215\n",
            "step: 350, loss: 0.001981459790840745\n",
            "step: 360, loss: 0.04987072944641113\n",
            "step: 370, loss: 0.005158618092536926\n",
            "step: 380, loss: 0.025466330349445343\n",
            "step: 390, loss: 0.015002626925706863\n",
            "step: 400, loss: 0.004623066168278456\n",
            "step: 410, loss: 0.002571123419329524\n",
            "step: 420, loss: 0.010251132771372795\n",
            "step: 430, loss: 0.019240915775299072\n",
            "step: 440, loss: 0.046277277171611786\n",
            "step: 450, loss: 0.014718924649059772\n",
            "step: 460, loss: 0.0015772258630022407\n",
            "step: 470, loss: 0.009952700696885586\n",
            "step: 480, loss: 0.014332267455756664\n",
            "step: 490, loss: 0.008072856813669205\n",
            "step: 500, loss: 0.006102705840021372\n",
            "step: 510, loss: 0.007605484686791897\n",
            "step: 520, loss: 0.0034924191422760487\n",
            "step: 530, loss: 0.006694120820611715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9125939849624061, f1=0.8983855650522317, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024289576336741447\n",
            "step: 10, loss: 0.00696193240582943\n",
            "step: 20, loss: 0.0017232733080163598\n",
            "step: 30, loss: 0.04646644368767738\n",
            "step: 40, loss: 0.052797671407461166\n",
            "step: 50, loss: 0.0013156816130504012\n",
            "step: 60, loss: 0.003655944252386689\n",
            "step: 70, loss: 0.0310970488935709\n",
            "step: 80, loss: 0.0003664682153612375\n",
            "step: 90, loss: 0.01587996818125248\n",
            "step: 100, loss: 0.10398095101118088\n",
            "step: 110, loss: 0.00576772540807724\n",
            "step: 120, loss: 0.00966945756226778\n",
            "step: 130, loss: 0.004439039155840874\n",
            "step: 140, loss: 0.02828352525830269\n",
            "step: 150, loss: 0.008091249503195286\n",
            "step: 160, loss: 0.04032314568758011\n",
            "step: 170, loss: 0.0007627442246302962\n",
            "step: 180, loss: 0.00037191619048826396\n",
            "step: 190, loss: 0.004821273032575846\n",
            "step: 200, loss: 0.0019694154616445303\n",
            "step: 210, loss: 0.0009998271707445383\n",
            "step: 220, loss: 0.0008615648839622736\n",
            "step: 230, loss: 0.000666275096591562\n",
            "step: 240, loss: 0.0251960139721632\n",
            "step: 250, loss: 0.002705363556742668\n",
            "step: 260, loss: 0.0005409325240179896\n",
            "step: 270, loss: 0.00043234415352344513\n",
            "step: 280, loss: 0.001438722014427185\n",
            "step: 290, loss: 0.001184886903502047\n",
            "step: 300, loss: 0.00045690464321523905\n",
            "step: 310, loss: 0.018925292417407036\n",
            "step: 320, loss: 0.00017548275354783982\n",
            "step: 330, loss: 0.0005540476995520294\n",
            "step: 340, loss: 0.07381068170070648\n",
            "step: 350, loss: 0.00432970467954874\n",
            "step: 360, loss: 0.003494270844385028\n",
            "step: 370, loss: 0.0018351242179051042\n",
            "step: 380, loss: 0.001133721205405891\n",
            "step: 390, loss: 0.008134541101753712\n",
            "step: 400, loss: 0.0001907299447339028\n",
            "step: 410, loss: 0.0004427308449521661\n",
            "step: 420, loss: 0.007844441570341587\n",
            "step: 430, loss: 0.0025843337643891573\n",
            "step: 440, loss: 0.0195468682795763\n",
            "step: 450, loss: 0.001417031278833747\n",
            "step: 460, loss: 0.20401601493358612\n",
            "step: 470, loss: 0.24887314438819885\n",
            "step: 480, loss: 0.0039696418680250645\n",
            "step: 490, loss: 0.0025980875361710787\n",
            "step: 500, loss: 0.004809664096683264\n",
            "step: 510, loss: 0.0029837065376341343\n",
            "step: 520, loss: 0.00852635595947504\n",
            "step: 530, loss: 0.03386378660798073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9063670411985018, f1=0.8990999526290857, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08862873166799545\n",
            "step: 10, loss: 0.008252574130892754\n",
            "step: 20, loss: 0.049535609781742096\n",
            "step: 30, loss: 0.0064278095960617065\n",
            "step: 40, loss: 0.002348031383007765\n",
            "step: 50, loss: 0.003014717483893037\n",
            "step: 60, loss: 0.002811809303238988\n",
            "step: 70, loss: 0.0012372517958283424\n",
            "step: 80, loss: 0.013505881652235985\n",
            "step: 90, loss: 0.00022432452533394098\n",
            "step: 100, loss: 0.0005119108245708048\n",
            "step: 110, loss: 0.0003486491914372891\n",
            "step: 120, loss: 0.0012875903630629182\n",
            "step: 130, loss: 0.0005173508543521166\n",
            "step: 140, loss: 0.000590306066442281\n",
            "step: 150, loss: 0.0008002883987501264\n",
            "step: 160, loss: 0.0005533329676836729\n",
            "step: 170, loss: 0.01966038905084133\n",
            "step: 180, loss: 0.0002035719808191061\n",
            "step: 190, loss: 0.001960823079571128\n",
            "step: 200, loss: 0.03135065361857414\n",
            "step: 210, loss: 0.034702736884355545\n",
            "step: 220, loss: 0.008197850547730923\n",
            "step: 230, loss: 0.001016082358546555\n",
            "step: 240, loss: 0.0019936009775847197\n",
            "step: 250, loss: 0.10510482639074326\n",
            "step: 260, loss: 0.0010555546032264829\n",
            "step: 270, loss: 0.00025598282809369266\n",
            "step: 280, loss: 0.01253444328904152\n",
            "step: 290, loss: 0.012491128407418728\n",
            "step: 300, loss: 0.0011837314814329147\n",
            "step: 310, loss: 0.00023096306540537626\n",
            "step: 320, loss: 0.010133779607713223\n",
            "step: 330, loss: 0.003028753213584423\n",
            "step: 340, loss: 0.008813809603452682\n",
            "step: 350, loss: 0.001046965247951448\n",
            "step: 360, loss: 0.002582848072052002\n",
            "step: 370, loss: 0.045369476079940796\n",
            "step: 380, loss: 0.01979156769812107\n",
            "step: 390, loss: 0.0107661047950387\n",
            "step: 400, loss: 0.0023952166084200144\n",
            "step: 410, loss: 0.002234665909782052\n",
            "step: 420, loss: 0.004018580541014671\n",
            "step: 430, loss: 0.00015614127914886922\n",
            "step: 440, loss: 0.00027398421661928296\n",
            "step: 450, loss: 0.00136516522616148\n",
            "step: 460, loss: 0.0014940116088837385\n",
            "step: 470, loss: 0.0524381548166275\n",
            "step: 480, loss: 0.00224848510697484\n",
            "step: 490, loss: 0.0009244962129741907\n",
            "step: 500, loss: 0.00015742868708912283\n",
            "step: 510, loss: 0.0017735975561663508\n",
            "step: 520, loss: 0.3349100947380066\n",
            "step: 530, loss: 0.0004130580055061728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9085768143261074, f1=0.9009009009009009, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005889153690077364\n",
            "step: 10, loss: 0.011127650737762451\n",
            "step: 20, loss: 0.004669674672186375\n",
            "step: 30, loss: 0.05397910624742508\n",
            "step: 40, loss: 0.009778554551303387\n",
            "step: 50, loss: 0.001115144114010036\n",
            "step: 60, loss: 0.0014765691012144089\n",
            "step: 70, loss: 0.0009861803846433759\n",
            "step: 80, loss: 0.00022310821805149317\n",
            "step: 90, loss: 0.004011086653918028\n",
            "step: 100, loss: 0.00036065600579604506\n",
            "step: 110, loss: 0.0011038730153813958\n",
            "step: 120, loss: 0.0019381206948310137\n",
            "step: 130, loss: 0.0003371835045982152\n",
            "step: 140, loss: 8.934747893363237e-05\n",
            "step: 150, loss: 0.0006329749012365937\n",
            "step: 160, loss: 0.0013044241350144148\n",
            "step: 170, loss: 0.003687160322442651\n",
            "step: 180, loss: 0.001660062000155449\n",
            "step: 190, loss: 0.005554789677262306\n",
            "step: 200, loss: 0.0019148490391671658\n",
            "step: 210, loss: 0.08468234539031982\n",
            "step: 220, loss: 0.000914761156309396\n",
            "step: 230, loss: 0.000696048024110496\n",
            "step: 240, loss: 0.0004881521745119244\n",
            "step: 250, loss: 0.0019634768832474947\n",
            "step: 260, loss: 0.0026773724239319563\n",
            "step: 270, loss: 0.0003385945165064186\n",
            "step: 280, loss: 0.01328683365136385\n",
            "step: 290, loss: 0.314879834651947\n",
            "step: 300, loss: 0.00012992304982617497\n",
            "step: 310, loss: 0.08548165112733841\n",
            "step: 320, loss: 0.0002222400944447145\n",
            "step: 330, loss: 0.0030393691267818213\n",
            "step: 340, loss: 0.003966981079429388\n",
            "step: 350, loss: 0.014482099562883377\n",
            "step: 360, loss: 0.0002835097548086196\n",
            "step: 370, loss: 0.00163958128541708\n",
            "step: 380, loss: 0.0037162508815526962\n",
            "step: 390, loss: 0.0011029393645003438\n",
            "step: 400, loss: 0.03247763216495514\n",
            "step: 410, loss: 0.0006472698296420276\n",
            "step: 420, loss: 0.000313727417960763\n",
            "step: 430, loss: 0.00016431727271992713\n",
            "step: 440, loss: 0.0011086463928222656\n",
            "step: 450, loss: 0.00032464347896166146\n",
            "step: 460, loss: 0.013047423213720322\n",
            "step: 470, loss: 0.0005118386470712721\n",
            "step: 480, loss: 0.0013788407668471336\n",
            "step: 490, loss: 0.000580630439799279\n",
            "step: 500, loss: 0.000280822190688923\n",
            "step: 510, loss: 0.00019966778927482665\n",
            "step: 520, loss: 0.0003803520812653005\n",
            "step: 530, loss: 0.0001615330984350294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9017773620205801, f1=0.898876404494382, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023553622304461896\n",
            "step: 10, loss: 0.0002724129008129239\n",
            "step: 20, loss: 0.000416945549659431\n",
            "step: 30, loss: 0.08493176102638245\n",
            "step: 40, loss: 0.11251581460237503\n",
            "step: 50, loss: 0.0002931604103650898\n",
            "step: 60, loss: 0.005573255941271782\n",
            "step: 70, loss: 0.0015247628325596452\n",
            "step: 80, loss: 0.00015883291780482978\n",
            "step: 90, loss: 0.01369550358504057\n",
            "step: 100, loss: 0.00147941242903471\n",
            "step: 110, loss: 0.00013731281796935946\n",
            "step: 120, loss: 0.00016761691949795932\n",
            "step: 130, loss: 0.004706798121333122\n",
            "step: 140, loss: 0.004767163656651974\n",
            "step: 150, loss: 8.174929826054722e-05\n",
            "step: 160, loss: 0.00044643902219831944\n",
            "step: 170, loss: 0.0007491235737688839\n",
            "step: 180, loss: 9.779255196917802e-05\n",
            "step: 190, loss: 0.0013708164915442467\n",
            "step: 200, loss: 0.00023802609939593822\n",
            "step: 210, loss: 0.00012008116027573124\n",
            "step: 220, loss: 0.002963021397590637\n",
            "step: 230, loss: 0.0005066500161774457\n",
            "step: 240, loss: 0.0007420388865284622\n",
            "step: 250, loss: 0.0008228387450799346\n",
            "step: 260, loss: 0.0013245587470009923\n",
            "step: 270, loss: 0.010014832019805908\n",
            "step: 280, loss: 0.0002398222277406603\n",
            "step: 290, loss: 0.00010879441106226295\n",
            "step: 300, loss: 0.0019286139868199825\n",
            "step: 310, loss: 0.0003485619090497494\n",
            "step: 320, loss: 0.0006553790881298482\n",
            "step: 330, loss: 0.0003354674845468253\n",
            "step: 340, loss: 0.00011880089004989713\n",
            "step: 350, loss: 0.00012716690253000706\n",
            "step: 360, loss: 0.00459745479747653\n",
            "step: 370, loss: 0.012061882764101028\n",
            "step: 380, loss: 0.0004044279339723289\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 390, loss: 0.00018631271086633205\n",
            "step: 400, loss: 0.003473997814580798\n",
            "step: 410, loss: 0.0010324701434001327\n",
            "step: 420, loss: 0.0009461604058742523\n",
            "step: 430, loss: 0.00012570508988574147\n",
            "step: 440, loss: 9.821002458920702e-05\n",
            "step: 450, loss: 9.979523747460917e-05\n",
            "step: 460, loss: 0.021887360140681267\n",
            "step: 470, loss: 0.00012560366303659976\n",
            "step: 480, loss: 0.001393738086335361\n",
            "step: 490, loss: 0.0006316562066785991\n",
            "step: 500, loss: 0.0006401462596841156\n",
            "step: 510, loss: 0.0002446482831146568\n",
            "step: 520, loss: 0.0001377338485326618\n",
            "step: 530, loss: 0.0005770967109128833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9087505849321479, f1=0.9049858889934148, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0039022478740662336\n",
            "step: 10, loss: 9.433190280105919e-05\n",
            "step: 20, loss: 0.0013365333434194326\n",
            "step: 30, loss: 0.00015147047815844417\n",
            "step: 40, loss: 0.0038029206916689873\n",
            "step: 50, loss: 9.884469181997702e-05\n",
            "step: 60, loss: 0.00027830631006509066\n",
            "step: 70, loss: 0.0004419922479428351\n",
            "step: 80, loss: 0.00029319257009774446\n",
            "step: 90, loss: 0.00020277457952033728\n",
            "step: 100, loss: 0.0017030220478773117\n",
            "step: 110, loss: 0.0032561644911766052\n",
            "step: 120, loss: 0.0004919233033433557\n",
            "step: 130, loss: 0.021349826827645302\n",
            "step: 140, loss: 0.00014087988529354334\n",
            "step: 150, loss: 0.000270505144726485\n",
            "step: 160, loss: 0.00026580423582345247\n",
            "step: 170, loss: 0.0002732225111685693\n",
            "step: 180, loss: 0.004822565242648125\n",
            "step: 190, loss: 0.0015448404010385275\n",
            "step: 200, loss: 0.0011830354342237115\n",
            "step: 210, loss: 0.00029367837123572826\n",
            "step: 220, loss: 0.00011360996722942218\n",
            "step: 230, loss: 0.0013801440363749862\n",
            "step: 240, loss: 0.00010795117123052478\n",
            "step: 250, loss: 0.00013317940465640277\n",
            "step: 260, loss: 9.302348917117342e-05\n",
            "step: 270, loss: 0.004189082887023687\n",
            "step: 280, loss: 8.530450577381998e-05\n",
            "step: 290, loss: 9.530016541248187e-05\n",
            "step: 300, loss: 0.0001709491480141878\n",
            "step: 310, loss: 0.002067041350528598\n",
            "step: 320, loss: 0.0006053319666534662\n",
            "step: 330, loss: 0.0001077880006050691\n",
            "step: 340, loss: 5.724294169340283e-05\n",
            "step: 350, loss: 5.977608452667482e-05\n",
            "step: 360, loss: 0.002294057048857212\n",
            "step: 370, loss: 0.004208848811686039\n",
            "step: 380, loss: 6.535309512401e-05\n",
            "step: 390, loss: 0.00010674908116925508\n",
            "step: 400, loss: 0.00021916769037488848\n",
            "step: 410, loss: 0.00031807785853743553\n",
            "step: 420, loss: 0.0009258968057110906\n",
            "step: 430, loss: 0.0003762409614864737\n",
            "step: 440, loss: 5.147767296875827e-05\n",
            "step: 450, loss: 9.680978109827265e-05\n",
            "step: 460, loss: 8.195445843739435e-05\n",
            "step: 470, loss: 9.100536408368498e-05\n",
            "step: 480, loss: 0.00011139416164951399\n",
            "step: 490, loss: 0.06431901454925537\n",
            "step: 500, loss: 0.00026265691849403083\n",
            "step: 510, loss: 0.00030311159207485616\n",
            "step: 520, loss: 0.0001371303224004805\n",
            "step: 530, loss: 4.3323543650330976e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9089210649229332, f1=0.9030131826741995, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001672731013968587\n",
            "step: 10, loss: 0.0005601735319942236\n",
            "step: 20, loss: 3.571302295313217e-05\n",
            "step: 30, loss: 0.0002476587542332709\n",
            "step: 40, loss: 0.0005160451983101666\n",
            "step: 50, loss: 0.00013860220497008413\n",
            "step: 60, loss: 5.1479997637216e-05\n",
            "step: 70, loss: 5.0079921493306756e-05\n",
            "step: 80, loss: 8.918515959521756e-05\n",
            "step: 90, loss: 8.350372809218243e-05\n",
            "step: 100, loss: 0.0002480149269104004\n",
            "step: 110, loss: 0.00010334251419408247\n",
            "step: 120, loss: 0.0001709375937934965\n",
            "step: 130, loss: 0.00047773082042112947\n",
            "step: 140, loss: 8.907634037313983e-05\n",
            "step: 150, loss: 0.0036916187964379787\n",
            "step: 160, loss: 0.00015062515740282834\n",
            "step: 170, loss: 0.00022500002523884177\n",
            "step: 180, loss: 4.8683465138310567e-05\n",
            "step: 190, loss: 0.0003805985616054386\n",
            "step: 200, loss: 7.291706424439326e-05\n",
            "step: 210, loss: 9.013474482344463e-05\n",
            "step: 220, loss: 8.419118967140093e-05\n",
            "step: 230, loss: 0.00010325410403311253\n",
            "step: 240, loss: 0.004771274980157614\n",
            "step: 250, loss: 0.0003887685015797615\n",
            "step: 260, loss: 8.98796715773642e-05\n",
            "step: 270, loss: 0.0023128390312194824\n",
            "step: 280, loss: 0.00020969372417312115\n",
            "step: 290, loss: 9.380467236042023e-05\n",
            "step: 300, loss: 0.017114030197262764\n",
            "step: 310, loss: 0.003645631717517972\n",
            "step: 320, loss: 0.05092315375804901\n",
            "step: 330, loss: 0.00027899249107576907\n",
            "step: 340, loss: 0.00013589876471087337\n",
            "step: 350, loss: 0.05383101850748062\n",
            "step: 360, loss: 0.004540913738310337\n",
            "step: 370, loss: 4.1385199438082054e-05\n",
            "step: 380, loss: 4.3806390749523416e-05\n",
            "step: 390, loss: 0.00020796930766664445\n",
            "step: 400, loss: 3.191713403793983e-05\n",
            "step: 410, loss: 2.914600190706551e-05\n",
            "step: 420, loss: 0.0028313971124589443\n",
            "step: 430, loss: 0.0007755258120596409\n",
            "step: 440, loss: 7.157215441111475e-05\n",
            "step: 450, loss: 3.751185431610793e-05\n",
            "step: 460, loss: 0.0004198942333459854\n",
            "step: 470, loss: 0.0007541536469943821\n",
            "step: 480, loss: 0.0003891197557095438\n",
            "step: 490, loss: 0.00038096753996796906\n",
            "step: 500, loss: 0.00021654368902090937\n",
            "step: 510, loss: 4.430636909091845e-05\n",
            "step: 520, loss: 4.46745652880054e-05\n",
            "step: 530, loss: 0.00042977489647455513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9083215796897038, f1=0.8959318826868495, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.944512279005721e-05\n",
            "step: 10, loss: 0.00018387407180853188\n",
            "step: 20, loss: 2.754412889771629e-05\n",
            "step: 30, loss: 5.0363400077912956e-05\n",
            "step: 40, loss: 0.00015681402874179184\n",
            "step: 50, loss: 5.36187544639688e-05\n",
            "step: 60, loss: 0.0008699352038092911\n",
            "step: 70, loss: 0.005944648291915655\n",
            "step: 80, loss: 0.00032148530590347946\n",
            "step: 90, loss: 0.012884755618870258\n",
            "step: 100, loss: 8.511572377756238e-05\n",
            "step: 110, loss: 3.261018719058484e-05\n",
            "step: 120, loss: 0.0011640549637377262\n",
            "step: 130, loss: 0.0002504217845853418\n",
            "step: 140, loss: 0.0001843659410951659\n",
            "step: 150, loss: 0.0004465110250748694\n",
            "step: 160, loss: 6.995621515670791e-05\n",
            "step: 170, loss: 0.019902531057596207\n",
            "step: 180, loss: 6.893208774272352e-05\n",
            "step: 190, loss: 4.152579276706092e-05\n",
            "step: 200, loss: 6.865258183097467e-05\n",
            "step: 210, loss: 0.00019546839757822454\n",
            "step: 220, loss: 8.925353176891804e-05\n",
            "step: 230, loss: 7.85619777161628e-05\n",
            "step: 240, loss: 4.208603786537424e-05\n",
            "step: 250, loss: 0.001235414994880557\n",
            "step: 260, loss: 0.004441940691322088\n",
            "step: 270, loss: 8.126660395646468e-05\n",
            "step: 280, loss: 0.0007078108610585332\n",
            "step: 290, loss: 0.005731785204261541\n",
            "step: 300, loss: 6.189584382809699e-05\n",
            "step: 310, loss: 6.0706650401698425e-05\n",
            "step: 320, loss: 5.5405296734534204e-05\n",
            "step: 330, loss: 4.1382259951205924e-05\n",
            "step: 340, loss: 6.989809480728582e-05\n",
            "step: 350, loss: 0.005777871701866388\n",
            "step: 360, loss: 0.00020107523596379906\n",
            "step: 370, loss: 0.0002880293468479067\n",
            "step: 380, loss: 0.00037341416464187205\n",
            "step: 390, loss: 0.0001986924180528149\n",
            "step: 400, loss: 4.314757097745314e-05\n",
            "step: 410, loss: 6.643805681960657e-05\n",
            "step: 420, loss: 0.00012297966168262064\n",
            "step: 430, loss: 6.6427506681066e-05\n",
            "step: 440, loss: 5.7708977692527696e-05\n",
            "step: 450, loss: 0.00011159385030623525\n",
            "step: 460, loss: 6.621429201913998e-05\n",
            "step: 470, loss: 0.00010685995948733762\n",
            "step: 480, loss: 5.720036642742343e-05\n",
            "step: 490, loss: 0.00037849892396479845\n",
            "step: 500, loss: 0.0010735387913882732\n",
            "step: 510, loss: 5.3708459745394066e-05\n",
            "step: 520, loss: 0.0002688208478502929\n",
            "step: 530, loss: 0.000652708753477782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9051643192488262, f1=0.89413988657845, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.695786578347906e-05\n",
            "step: 10, loss: 0.0033656007144600153\n",
            "step: 20, loss: 0.0015607330715283751\n",
            "step: 30, loss: 0.001548310974612832\n",
            "step: 40, loss: 0.00015376319061033428\n",
            "step: 50, loss: 8.842317765811458e-05\n",
            "step: 60, loss: 3.68084802175872e-05\n",
            "step: 70, loss: 0.0011569138150662184\n",
            "step: 80, loss: 0.00018122588517144322\n",
            "step: 90, loss: 0.00011651258682832122\n",
            "step: 100, loss: 3.7141242501093075e-05\n",
            "step: 110, loss: 0.00024959215079434216\n",
            "step: 120, loss: 0.0014166616601869464\n",
            "step: 130, loss: 7.06295613781549e-05\n",
            "step: 140, loss: 5.7254703278886154e-05\n",
            "step: 150, loss: 7.136238127714023e-05\n",
            "step: 160, loss: 5.0770682719303295e-05\n",
            "step: 170, loss: 6.292772013694048e-05\n",
            "step: 180, loss: 3.3921536669367924e-05\n",
            "step: 190, loss: 5.366226832848042e-05\n",
            "step: 200, loss: 0.001977220643311739\n",
            "step: 210, loss: 0.0019514269661158323\n",
            "step: 220, loss: 4.439812619239092e-05\n",
            "step: 230, loss: 2.4641974960104562e-05\n",
            "step: 240, loss: 0.00019227739539928734\n",
            "step: 250, loss: 0.00079876504605636\n",
            "step: 260, loss: 3.500135426293127e-05\n",
            "step: 270, loss: 4.4684511522063985e-05\n",
            "step: 280, loss: 0.0005210753879509866\n",
            "step: 290, loss: 0.00017952141934074461\n",
            "step: 300, loss: 0.0004881895729340613\n",
            "step: 310, loss: 2.073841096716933e-05\n",
            "step: 320, loss: 5.1107661420246586e-05\n",
            "step: 330, loss: 0.004188857972621918\n",
            "step: 340, loss: 4.8335317842429504e-05\n",
            "step: 350, loss: 0.0003189364215359092\n",
            "step: 360, loss: 4.317050479585305e-05\n",
            "step: 370, loss: 0.0007825332577340305\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 380, loss: 7.754963007755578e-05\n",
            "step: 390, loss: 7.185182766988873e-05\n",
            "step: 400, loss: 3.6799207009607926e-05\n",
            "step: 410, loss: 3.649117934401147e-05\n",
            "step: 420, loss: 0.0007405329961329699\n",
            "step: 430, loss: 3.8382462662411854e-05\n",
            "step: 440, loss: 0.00016950065037235618\n",
            "step: 450, loss: 7.819433085387573e-05\n",
            "step: 460, loss: 9.18054865906015e-05\n",
            "step: 470, loss: 0.004276062827557325\n",
            "step: 480, loss: 4.080418511875905e-05\n",
            "step: 490, loss: 4.0562081267125905e-05\n",
            "step: 500, loss: 8.8113134552259e-05\n",
            "step: 510, loss: 3.203668529749848e-05\n",
            "step: 520, loss: 0.00043465339695103467\n",
            "step: 530, loss: 4.0353981603402644e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9029815428300995, f1=0.9016627078384797, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015857847756706178\n",
            "step: 10, loss: 7.979221118148416e-05\n",
            "step: 20, loss: 2.4664590455358848e-05\n",
            "step: 30, loss: 0.0003265290579292923\n",
            "step: 40, loss: 3.6445642763283104e-05\n",
            "step: 50, loss: 0.0005233414121903479\n",
            "step: 60, loss: 3.0125445846351795e-05\n",
            "step: 70, loss: 0.000620041333604604\n",
            "step: 80, loss: 0.000243779577431269\n",
            "step: 90, loss: 4.6231827582232654e-05\n",
            "step: 100, loss: 4.764397090184502e-05\n",
            "step: 110, loss: 7.579204248031601e-05\n",
            "step: 120, loss: 4.315558908274397e-05\n",
            "step: 130, loss: 4.4565240386873484e-05\n",
            "step: 140, loss: 0.01811087876558304\n",
            "step: 150, loss: 0.0005344083183445036\n",
            "step: 160, loss: 0.0004301058070268482\n",
            "step: 170, loss: 5.43368405487854e-05\n",
            "step: 180, loss: 0.00019427179358899593\n",
            "step: 190, loss: 0.07796333730220795\n",
            "step: 200, loss: 0.0003624391392804682\n",
            "step: 210, loss: 0.0007065151003189385\n",
            "step: 220, loss: 0.00020902618416585028\n",
            "step: 230, loss: 0.00011789756536018103\n",
            "step: 240, loss: 6.836531247245148e-05\n",
            "step: 250, loss: 4.6979064791230485e-05\n",
            "step: 260, loss: 2.5595811166567728e-05\n",
            "step: 270, loss: 0.007623456884175539\n",
            "step: 280, loss: 3.9146612834883854e-05\n",
            "step: 290, loss: 0.00015247586998157203\n",
            "step: 300, loss: 3.312737317173742e-05\n",
            "step: 310, loss: 2.937653880508151e-05\n",
            "step: 320, loss: 0.002830727491527796\n",
            "step: 330, loss: 4.1444240196142346e-05\n",
            "step: 340, loss: 4.074570824741386e-05\n",
            "step: 350, loss: 8.425895066466182e-05\n",
            "step: 360, loss: 0.0002657863951753825\n",
            "step: 370, loss: 0.00025721167912706733\n",
            "step: 380, loss: 4.493632513913326e-05\n",
            "step: 390, loss: 0.0001988460135180503\n",
            "step: 400, loss: 7.04187696101144e-05\n",
            "step: 410, loss: 0.0002453791385050863\n",
            "step: 420, loss: 4.977460775990039e-05\n",
            "step: 430, loss: 0.0002045117726083845\n",
            "step: 440, loss: 1.79034577740822e-05\n",
            "step: 450, loss: 0.00021924084285274148\n",
            "step: 460, loss: 6.818126712460071e-05\n",
            "step: 470, loss: 0.0035196540411561728\n",
            "step: 480, loss: 3.1373212550533935e-05\n",
            "step: 490, loss: 4.775110573973507e-05\n",
            "step: 500, loss: 0.00032173143699765205\n",
            "step: 510, loss: 4.017445826320909e-05\n",
            "step: 520, loss: 5.6062683142954484e-05\n",
            "step: 530, loss: 0.00018331459432374686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9036144578313254, f1=0.9074848907484891, best_f1=0.9039704524469068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.426408122526482e-05\n",
            "step: 10, loss: 4.040641215397045e-05\n",
            "step: 20, loss: 5.7976634707301855e-05\n",
            "step: 30, loss: 3.942261901102029e-05\n",
            "step: 40, loss: 2.745462188613601e-05\n",
            "step: 50, loss: 2.1945070329820737e-05\n",
            "step: 60, loss: 5.662025068886578e-05\n",
            "step: 70, loss: 3.6192581319483e-05\n",
            "step: 80, loss: 8.914498903322965e-05\n",
            "step: 90, loss: 9.890960063785315e-05\n",
            "step: 100, loss: 2.7525422410690226e-05\n",
            "step: 110, loss: 0.0005083226133137941\n",
            "step: 120, loss: 0.0010827264050021768\n",
            "step: 130, loss: 0.00022481533233076334\n",
            "step: 140, loss: 4.166801954852417e-05\n",
            "step: 150, loss: 4.93105108034797e-05\n",
            "step: 160, loss: 3.1559146009385586e-05\n",
            "step: 170, loss: 0.00014290487160906196\n",
            "step: 180, loss: 2.113686059601605e-05\n",
            "step: 190, loss: 7.189540338004008e-05\n",
            "step: 200, loss: 2.9693379474338144e-05\n",
            "step: 210, loss: 7.973132596816868e-05\n",
            "step: 220, loss: 2.1293310055625625e-05\n",
            "step: 230, loss: 0.0001307603670284152\n",
            "step: 240, loss: 5.1759270718321204e-05\n",
            "step: 250, loss: 0.0019581655506044626\n",
            "step: 260, loss: 9.97993047349155e-05\n",
            "step: 270, loss: 0.00020203769963700324\n",
            "step: 280, loss: 5.9923699154751375e-05\n",
            "step: 290, loss: 0.00011090880434494466\n",
            "step: 300, loss: 0.0053869495168328285\n",
            "step: 310, loss: 7.061992801027372e-05\n",
            "step: 320, loss: 3.480057785054669e-05\n",
            "step: 330, loss: 0.003543006954714656\n",
            "step: 340, loss: 8.996052929433063e-05\n",
            "step: 350, loss: 0.036177389323711395\n",
            "step: 360, loss: 3.156342791044153e-05\n",
            "step: 370, loss: 2.5774452296900563e-05\n",
            "step: 380, loss: 0.0001486000546719879\n",
            "step: 390, loss: 4.925384928355925e-05\n",
            "step: 400, loss: 0.000313704862492159\n",
            "step: 410, loss: 3.8076905184425414e-05\n",
            "step: 420, loss: 7.300591823877767e-05\n",
            "step: 430, loss: 3.424124588491395e-05\n",
            "step: 440, loss: 9.998495079344139e-05\n",
            "step: 450, loss: 0.001186381559818983\n",
            "step: 460, loss: 5.891470209462568e-05\n",
            "step: 470, loss: 7.3326300480403e-05\n",
            "step: 480, loss: 2.516362837923225e-05\n",
            "step: 490, loss: 0.0037618866190314293\n",
            "step: 500, loss: 5.947229146840982e-05\n",
            "step: 510, loss: 0.01930922269821167\n",
            "step: 520, loss: 8.649734809296206e-05\n",
            "step: 530, loss: 3.173109871568158e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9053613053613055, f1=0.9097709209911174, best_f1=0.9039704524469068\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:15, 362.62it/s]\n",
            "load_f1 = 0.9141531322505799\n",
            "real_f1 = 0.9163568773234201\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 386.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "5vKDRsHPssdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec94e17c-4087-4f58-d8a9-724c49007211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=170b4f36d0dd08001db0f4d0eec58d661d8c85b02dc08264cb132953605d7a4f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o0zfgssc/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e5a725-fc88-4629-d738-716df20efb32"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8769121170043945\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.358974358974359, f1=0.3146067415730337, best_f1=0.3146067415730337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.35865864157676697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2711864406779661, f1=0.2816901408450704, best_f1=0.3146067415730337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3311608135700226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.2692307692307693, f1=0.2666666666666667, best_f1=0.3146067415730337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37242746353149414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.4210526315789474, f1=0.3013698630136986, best_f1=0.3013698630136986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29269978404045105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4126984126984127, f1=0.3373493975903615, best_f1=0.3013698630136986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2748320698738098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.45283018867924535, f1=0.32876712328767127, best_f1=0.32876712328767127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2668851315975189\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.42553191489361697, f1=0.38596491228070184, best_f1=0.32876712328767127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41150376200675964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.5, f1=0.4482758620689656, best_f1=0.4482758620689656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21257396042346954\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.43137254901960786, f1=0.42857142857142855, best_f1=0.4482758620689656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29201075434684753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.43902439024390244, f1=0.48, best_f1=0.4482758620689656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2737025320529938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5, f1=0.4897959183673469, best_f1=0.4482758620689656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25304538011550903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.4736842105263159, f1=0.45833333333333326, best_f1=0.4482758620689656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17561988532543182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.5116279069767441, f1=0.45833333333333326, best_f1=0.45833333333333326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1889357566833496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5116279069767441, f1=0.45833333333333326, best_f1=0.45833333333333326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27296966314315796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5116279069767441, f1=0.45833333333333326, best_f1=0.45833333333333326\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 126636.25it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.4761904761904762\n",
            "real_f1 = 0.4761904761904762\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 268.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2914d4-01f0-41ae-c2d0-1fc83563bc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7971441745758057\n",
            "step: 10, loss: 0.48138856887817383\n",
            "step: 20, loss: 0.6127474308013916\n",
            "step: 30, loss: 0.5085351467132568\n",
            "step: 40, loss: 0.42998477816581726\n",
            "step: 50, loss: 0.3414802849292755\n",
            "step: 60, loss: 0.24683596193790436\n",
            "step: 70, loss: 0.1364862322807312\n",
            "step: 80, loss: 0.12910018861293793\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.021884815767407417\n",
            "step: 100, loss: 0.16252119839191437\n",
            "step: 110, loss: 0.04280352219939232\n",
            "step: 120, loss: 0.0520084947347641\n",
            "step: 130, loss: 0.13457149267196655\n",
            "step: 140, loss: 0.18062494695186615\n",
            "step: 150, loss: 0.13240154087543488\n",
            "step: 160, loss: 0.22006531059741974\n",
            "step: 170, loss: 0.0069502610713243484\n",
            "step: 180, loss: 0.011754951439797878\n",
            "step: 190, loss: 0.11716275662183762\n",
            "step: 200, loss: 0.01257830299437046\n",
            "step: 210, loss: 0.014047220349311829\n",
            "step: 220, loss: 0.00679743429645896\n",
            "step: 230, loss: 0.005398761946707964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.970917225950783, f1=0.9570135746606334, best_f1=0.9570135746606334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06442093104124069\n",
            "step: 10, loss: 0.0019813545513898134\n",
            "step: 20, loss: 0.010282019153237343\n",
            "step: 30, loss: 0.007297302596271038\n",
            "step: 40, loss: 0.0272186491638422\n",
            "step: 50, loss: 0.010458247736096382\n",
            "step: 60, loss: 0.08646419644355774\n",
            "step: 70, loss: 0.059675183147192\n",
            "step: 80, loss: 0.0070898146368563175\n",
            "step: 90, loss: 0.004649091511964798\n",
            "step: 100, loss: 0.19660773873329163\n",
            "step: 110, loss: 0.08658973127603531\n",
            "step: 120, loss: 0.017047224566340446\n",
            "step: 130, loss: 0.07994914054870605\n",
            "step: 140, loss: 0.10261448472738266\n",
            "step: 150, loss: 0.006054380442947149\n",
            "step: 160, loss: 0.029169779270887375\n",
            "step: 170, loss: 0.07379808276891708\n",
            "step: 180, loss: 0.0026523214764893055\n",
            "step: 190, loss: 0.06895451247692108\n",
            "step: 200, loss: 0.006674130912870169\n",
            "step: 210, loss: 0.12359055131673813\n",
            "step: 220, loss: 0.0009558204910717905\n",
            "step: 230, loss: 0.09348660707473755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9743589743589743, f1=0.9598214285714285, best_f1=0.9598214285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02323567494750023\n",
            "step: 10, loss: 0.1287080943584442\n",
            "step: 20, loss: 0.053924594074487686\n",
            "step: 30, loss: 0.010514660738408566\n",
            "step: 40, loss: 0.018465286120772362\n",
            "step: 50, loss: 0.021387146785855293\n",
            "step: 60, loss: 0.0006444465252570808\n",
            "step: 70, loss: 0.007132370490580797\n",
            "step: 80, loss: 0.005730516742914915\n",
            "step: 90, loss: 0.015413918532431126\n",
            "step: 100, loss: 0.002817385597154498\n",
            "step: 110, loss: 0.058364201337099075\n",
            "step: 120, loss: 0.08242657780647278\n",
            "step: 130, loss: 0.0008438770310021937\n",
            "step: 140, loss: 0.020765507593750954\n",
            "step: 150, loss: 0.0020745513029396534\n",
            "step: 160, loss: 0.006124912295490503\n",
            "step: 170, loss: 0.0036801169626414776\n",
            "step: 180, loss: 0.004141069948673248\n",
            "step: 190, loss: 0.020739082247018814\n",
            "step: 200, loss: 0.0015794796636328101\n",
            "step: 210, loss: 0.04243094101548195\n",
            "step: 220, loss: 0.0046340785920619965\n",
            "step: 230, loss: 0.1036260798573494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9777777777777777, f1=0.9688195991091313, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005861867684870958\n",
            "step: 10, loss: 0.015471928752958775\n",
            "step: 20, loss: 0.00041279109427705407\n",
            "step: 30, loss: 0.00021875124366488308\n",
            "step: 40, loss: 0.0014712696429342031\n",
            "step: 50, loss: 0.003923339769244194\n",
            "step: 60, loss: 0.0004888083203695714\n",
            "step: 70, loss: 0.0015575832221657038\n",
            "step: 80, loss: 0.022987214848399162\n",
            "step: 90, loss: 0.22305722534656525\n",
            "step: 100, loss: 0.005293748341500759\n",
            "step: 110, loss: 0.008562845177948475\n",
            "step: 120, loss: 0.08368875831365585\n",
            "step: 130, loss: 0.0011394221801310778\n",
            "step: 140, loss: 0.00641384394839406\n",
            "step: 150, loss: 0.010087274014949799\n",
            "step: 160, loss: 0.0016500415513291955\n",
            "step: 170, loss: 0.02095113880932331\n",
            "step: 180, loss: 0.0736365094780922\n",
            "step: 190, loss: 0.007358016446232796\n",
            "step: 200, loss: 0.004841885529458523\n",
            "step: 210, loss: 0.004411677829921246\n",
            "step: 220, loss: 0.0005529565969482064\n",
            "step: 230, loss: 0.0008238345617428422\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9690265486725664, f1=0.966740576496674, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016328011406585574\n",
            "step: 10, loss: 0.035297926515340805\n",
            "step: 20, loss: 0.010730499401688576\n",
            "step: 30, loss: 0.002450797241181135\n",
            "step: 40, loss: 0.014457807876169682\n",
            "step: 50, loss: 0.00026490268646739423\n",
            "step: 60, loss: 0.00029603595612570643\n",
            "step: 70, loss: 0.0002152677916456014\n",
            "step: 80, loss: 0.0004515676700975746\n",
            "step: 90, loss: 0.013866066932678223\n",
            "step: 100, loss: 0.012165587395429611\n",
            "step: 110, loss: 0.006062759086489677\n",
            "step: 120, loss: 0.1570526510477066\n",
            "step: 130, loss: 0.0008855434716679156\n",
            "step: 140, loss: 0.00012871941726189107\n",
            "step: 150, loss: 0.00012370350304991007\n",
            "step: 160, loss: 0.0019985237158834934\n",
            "step: 170, loss: 0.024988600984215736\n",
            "step: 180, loss: 0.0015013021184131503\n",
            "step: 190, loss: 0.0003267016727477312\n",
            "step: 200, loss: 0.004675957374274731\n",
            "step: 210, loss: 0.0012589386897161603\n",
            "step: 220, loss: 0.003167507704347372\n",
            "step: 230, loss: 0.0031409948132932186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9663677130044843, f1=0.9627959413754228, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013640100369229913\n",
            "step: 10, loss: 0.007041348144412041\n",
            "step: 20, loss: 0.0106064323335886\n",
            "step: 30, loss: 0.00028705113800242543\n",
            "step: 40, loss: 0.0005486909649334848\n",
            "step: 50, loss: 0.03651507943868637\n",
            "step: 60, loss: 0.0008057118975557387\n",
            "step: 70, loss: 0.0004475804162211716\n",
            "step: 80, loss: 0.002028119983151555\n",
            "step: 90, loss: 0.0002219670859631151\n",
            "step: 100, loss: 0.00021825020667165518\n",
            "step: 110, loss: 0.03430471941828728\n",
            "step: 120, loss: 0.0027059230487793684\n",
            "step: 130, loss: 0.0082743801176548\n",
            "step: 140, loss: 0.006752931512892246\n",
            "step: 150, loss: 0.0010423630010336637\n",
            "step: 160, loss: 0.053576067090034485\n",
            "step: 170, loss: 0.0006005655159242451\n",
            "step: 180, loss: 0.0007219088729470968\n",
            "step: 190, loss: 0.0003270096785854548\n",
            "step: 200, loss: 0.004617572762072086\n",
            "step: 210, loss: 0.002853484358638525\n",
            "step: 220, loss: 0.00029770241235382855\n",
            "step: 230, loss: 0.09675063192844391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9707207207207207, f1=0.9694224235560589, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007204663008451462\n",
            "step: 10, loss: 0.0002442534314468503\n",
            "step: 20, loss: 0.0003460158477537334\n",
            "step: 30, loss: 0.00036878001992590725\n",
            "step: 40, loss: 0.0018304003169760108\n",
            "step: 50, loss: 0.00011984690354438499\n",
            "step: 60, loss: 0.00038463942473754287\n",
            "step: 70, loss: 0.23165592551231384\n",
            "step: 80, loss: 0.00016032863641157746\n",
            "step: 90, loss: 0.0009045269689522684\n",
            "step: 100, loss: 0.008761064149439335\n",
            "step: 110, loss: 0.00384707422927022\n",
            "step: 120, loss: 0.00033137318678200245\n",
            "step: 130, loss: 0.00025447746156714857\n",
            "step: 140, loss: 0.00017935257346834987\n",
            "step: 150, loss: 0.00016149523435160518\n",
            "step: 160, loss: 0.00035424393718130887\n",
            "step: 170, loss: 0.00014840118819847703\n",
            "step: 180, loss: 0.0004458134644664824\n",
            "step: 190, loss: 0.0003419704153202474\n",
            "step: 200, loss: 0.0007891985587775707\n",
            "step: 210, loss: 0.0004146145365666598\n",
            "step: 220, loss: 0.0027915108948946\n",
            "step: 230, loss: 0.008260932750999928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9673790776152981, f1=0.9662162162162162, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.033885687589645386\n",
            "step: 10, loss: 0.0025333198718726635\n",
            "step: 20, loss: 0.0005219669546931982\n",
            "step: 30, loss: 0.001118839718401432\n",
            "step: 40, loss: 0.00023021311790216714\n",
            "step: 50, loss: 0.00018492281378712505\n",
            "step: 60, loss: 0.00016985964612104\n",
            "step: 70, loss: 0.00014971022028476\n",
            "step: 80, loss: 0.00027035779203288257\n",
            "step: 90, loss: 0.00021234640735201538\n",
            "step: 100, loss: 0.000370849302271381\n",
            "step: 110, loss: 0.00014817748160567135\n",
            "step: 120, loss: 0.00010438740719109774\n",
            "step: 130, loss: 0.0002547878830228001\n",
            "step: 140, loss: 0.0021183881908655167\n",
            "step: 150, loss: 9.619628690415993e-05\n",
            "step: 160, loss: 0.00014563914737664163\n",
            "step: 170, loss: 0.00011046520376112312\n",
            "step: 180, loss: 0.0005017481162212789\n",
            "step: 190, loss: 9.456843690713868e-05\n",
            "step: 200, loss: 0.00015022441220935434\n",
            "step: 210, loss: 6.35032047284767e-05\n",
            "step: 220, loss: 0.00033345489646308124\n",
            "step: 230, loss: 0.009278159588575363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9749430523917996, f1=0.9694224235560589, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.339112587738782e-05\n",
            "step: 10, loss: 0.00011548707698239014\n",
            "step: 20, loss: 0.0011350598651915789\n",
            "step: 30, loss: 0.0002062375860987231\n",
            "step: 40, loss: 0.00036344697582535446\n",
            "step: 50, loss: 0.00014876325440127403\n",
            "step: 60, loss: 0.0001509926514700055\n",
            "step: 70, loss: 0.0005054495995864272\n",
            "step: 80, loss: 0.12397728860378265\n",
            "step: 90, loss: 0.005629525985568762\n",
            "step: 100, loss: 0.0011461629765108228\n",
            "step: 110, loss: 0.0014878390356898308\n",
            "step: 120, loss: 0.0020079368259757757\n",
            "step: 130, loss: 0.00015326014545280486\n",
            "step: 140, loss: 0.000332571828039363\n",
            "step: 150, loss: 0.00011800610081991181\n",
            "step: 160, loss: 0.0035427636466920376\n",
            "step: 170, loss: 0.00024603575002402067\n",
            "step: 180, loss: 0.0009110851096920669\n",
            "step: 190, loss: 0.00011809143325081095\n",
            "step: 200, loss: 0.00019450434774626046\n",
            "step: 210, loss: 0.0010620774701237679\n",
            "step: 220, loss: 0.07437395304441452\n",
            "step: 230, loss: 0.0053456025198102\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9729119638826186, f1=0.9683972911963882, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017196565750055015\n",
            "step: 10, loss: 0.00010959716746583581\n",
            "step: 20, loss: 8.882291876943782e-05\n",
            "step: 30, loss: 0.00016049780242610723\n",
            "step: 40, loss: 0.00014560170529875904\n",
            "step: 50, loss: 0.0005505103035829961\n",
            "step: 60, loss: 0.00014989773626439273\n",
            "step: 70, loss: 0.0007119251531548798\n",
            "step: 80, loss: 0.0003016425180248916\n",
            "step: 90, loss: 0.0005016293143853545\n",
            "step: 100, loss: 0.00012044620234519243\n",
            "step: 110, loss: 0.00024549508816562593\n",
            "step: 120, loss: 0.004999023862183094\n",
            "step: 130, loss: 0.00023731740657240152\n",
            "step: 140, loss: 0.016076337546110153\n",
            "step: 150, loss: 0.00014013804320711643\n",
            "step: 160, loss: 0.0002004855195991695\n",
            "step: 170, loss: 0.00011457655637059361\n",
            "step: 180, loss: 0.00012027277261950076\n",
            "step: 190, loss: 0.0004771137028001249\n",
            "step: 200, loss: 0.00012035483086947352\n",
            "step: 210, loss: 9.166279778582975e-05\n",
            "step: 220, loss: 0.0007384862983599305\n",
            "step: 230, loss: 9.654263703851029e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9696287964004499, f1=0.9695603156708005, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017303841013927013\n",
            "step: 10, loss: 6.145863881101832e-05\n",
            "step: 20, loss: 8.55574689921923e-05\n",
            "step: 30, loss: 0.00021205835219006985\n",
            "step: 40, loss: 0.002806599484756589\n",
            "step: 50, loss: 0.0003848798805847764\n",
            "step: 60, loss: 0.00016469552065245807\n",
            "step: 70, loss: 0.0013719910057261586\n",
            "step: 80, loss: 0.0010340956505388021\n",
            "step: 90, loss: 5.607675848295912e-05\n",
            "step: 100, loss: 0.0001118256404879503\n",
            "step: 110, loss: 5.5271986639127135e-05\n",
            "step: 120, loss: 8.048495510593057e-05\n",
            "step: 130, loss: 4.807556251762435e-05\n",
            "step: 140, loss: 8.370748400921002e-05\n",
            "step: 150, loss: 5.015380884287879e-05\n",
            "step: 160, loss: 7.587954314658418e-05\n",
            "step: 170, loss: 0.03381659835577011\n",
            "step: 180, loss: 0.00012175336451036856\n",
            "step: 190, loss: 0.0025360521394759417\n",
            "step: 200, loss: 7.27379010641016e-05\n",
            "step: 210, loss: 5.121623325976543e-05\n",
            "step: 220, loss: 7.520416693296283e-05\n",
            "step: 230, loss: 0.02369583770632744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9741282339707535, f1=0.9717514124293786, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040400284342467785\n",
            "step: 10, loss: 0.00033604662166908383\n",
            "step: 20, loss: 0.00013688605395145714\n",
            "step: 30, loss: 8.495297515764832e-05\n",
            "step: 40, loss: 0.00015863279986660928\n",
            "step: 50, loss: 0.000738665577955544\n",
            "step: 60, loss: 0.0005578828859142959\n",
            "step: 70, loss: 7.330378866754472e-05\n",
            "step: 80, loss: 8.161977166309953e-05\n",
            "step: 90, loss: 5.212449104874395e-05\n",
            "step: 100, loss: 4.240695125190541e-05\n",
            "step: 110, loss: 5.2452956879278645e-05\n",
            "step: 120, loss: 0.0001246215688297525\n",
            "step: 130, loss: 0.0003388676850590855\n",
            "step: 140, loss: 4.654880831367336e-05\n",
            "step: 150, loss: 4.2126546759391204e-05\n",
            "step: 160, loss: 0.00048609322402626276\n",
            "step: 170, loss: 6.45276450086385e-05\n",
            "step: 180, loss: 6.884251342853531e-05\n",
            "step: 190, loss: 5.0858285248978063e-05\n",
            "step: 200, loss: 0.0006844309973530471\n",
            "step: 210, loss: 0.013513928279280663\n",
            "step: 220, loss: 0.00010330423538107425\n",
            "step: 230, loss: 0.000714607653208077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9700332963374029, f1=0.9730941704035874, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012528487422969192\n",
            "step: 10, loss: 0.00036162871401757\n",
            "step: 20, loss: 0.00027251988649368286\n",
            "step: 30, loss: 0.0026424082461744547\n",
            "step: 40, loss: 0.0017556938109919429\n",
            "step: 50, loss: 6.750061584170908e-05\n",
            "step: 60, loss: 5.393187529989518e-05\n",
            "step: 70, loss: 6.024767571943812e-05\n",
            "step: 80, loss: 0.00017635576659813523\n",
            "step: 90, loss: 0.0001159084786195308\n",
            "step: 100, loss: 6.605970702366903e-05\n",
            "step: 110, loss: 6.068783477530815e-05\n",
            "step: 120, loss: 7.17723451089114e-05\n",
            "step: 130, loss: 0.00016338244313374162\n",
            "step: 140, loss: 8.398358477279544e-05\n",
            "step: 150, loss: 0.00044213375076651573\n",
            "step: 160, loss: 7.438616739818826e-05\n",
            "step: 170, loss: 5.332435466698371e-05\n",
            "step: 180, loss: 0.0001656525128055364\n",
            "step: 190, loss: 4.6470209781546146e-05\n",
            "step: 200, loss: 8.955226803664118e-05\n",
            "step: 210, loss: 0.0012237665941938758\n",
            "step: 220, loss: 6.279633817030117e-05\n",
            "step: 230, loss: 5.073791180620901e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9707207207207207, f1=0.9705882352941176, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.705561387818307e-05\n",
            "step: 10, loss: 9.492552635492757e-05\n",
            "step: 20, loss: 0.0011086120503023267\n",
            "step: 30, loss: 0.0002280273038195446\n",
            "step: 40, loss: 3.8844944356242195e-05\n",
            "step: 50, loss: 8.278049062937498e-05\n",
            "step: 60, loss: 4.9094636779045686e-05\n",
            "step: 70, loss: 5.0445036322344095e-05\n",
            "step: 80, loss: 8.654611883684993e-05\n",
            "step: 90, loss: 7.090003782650456e-05\n",
            "step: 100, loss: 0.001603966928087175\n",
            "step: 110, loss: 0.0003670515725389123\n",
            "step: 120, loss: 0.00020919143571518362\n",
            "step: 130, loss: 0.00010667595051927492\n",
            "step: 140, loss: 6.589750410057604e-05\n",
            "step: 150, loss: 0.00010157637007068843\n",
            "step: 160, loss: 3.6100005672778934e-05\n",
            "step: 170, loss: 0.00014798885968048126\n",
            "step: 180, loss: 0.00010449118417454883\n",
            "step: 190, loss: 0.00013727793702855706\n",
            "step: 200, loss: 4.432155401445925e-05\n",
            "step: 210, loss: 0.0001806648215278983\n",
            "step: 220, loss: 6.836403190391138e-05\n",
            "step: 230, loss: 2.817378845065832e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9719416386083053, f1=0.971815107102593, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.035756865050644e-05\n",
            "step: 10, loss: 0.00023126068117562681\n",
            "step: 20, loss: 0.00012023797899018973\n",
            "step: 30, loss: 8.500088733853772e-05\n",
            "step: 40, loss: 0.0001009302432066761\n",
            "step: 50, loss: 6.661035877186805e-05\n",
            "step: 60, loss: 0.000195499203982763\n",
            "step: 70, loss: 7.494421151932329e-05\n",
            "step: 80, loss: 5.154040627530776e-05\n",
            "step: 90, loss: 3.356339584570378e-05\n",
            "step: 100, loss: 0.00012063542089890689\n",
            "step: 110, loss: 6.794734508730471e-05\n",
            "step: 120, loss: 0.13564926385879517\n",
            "step: 130, loss: 0.00010011400445364416\n",
            "step: 140, loss: 0.0001263044832739979\n",
            "step: 150, loss: 0.00025977680343203247\n",
            "step: 160, loss: 4.120731318835169e-05\n",
            "step: 170, loss: 4.9993286665994674e-05\n",
            "step: 180, loss: 7.237595855258405e-05\n",
            "step: 190, loss: 0.0003825988678727299\n",
            "step: 200, loss: 8.513433567713946e-05\n",
            "step: 210, loss: 0.006949456874281168\n",
            "step: 220, loss: 5.416920976131223e-05\n",
            "step: 230, loss: 8.269414684036747e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9720044792833147, f1=0.972972972972973, best_f1=0.9688195991091313\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:10, 236.25it/s]\n",
            "load_f1 = 0.9765363128491621\n",
            "real_f1 = 0.9754464285714286\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:16, 260.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f363f2-61c6-4956-fb94-c019cd75cdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7924099564552307\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.46124163269996643\n",
            "step: 20, loss: 0.5231547355651855\n",
            "step: 30, loss: 0.4438588321208954\n",
            "step: 40, loss: 0.4883056581020355\n",
            "step: 50, loss: 0.4330879747867584\n",
            "step: 60, loss: 0.44488510489463806\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.19834551215171814\n",
            "step: 80, loss: 0.18686515092849731\n",
            "step: 90, loss: 0.1377580463886261\n",
            "step: 100, loss: 0.3265272378921509\n",
            "step: 110, loss: 0.19482457637786865\n",
            "step: 120, loss: 0.14257754385471344\n",
            "step: 130, loss: 0.05472497269511223\n",
            "step: 140, loss: 0.279985249042511\n",
            "step: 150, loss: 0.12542128562927246\n",
            "step: 160, loss: 0.338727742433548\n",
            "step: 170, loss: 0.4379519820213318\n",
            "step: 180, loss: 0.14841340482234955\n",
            "step: 190, loss: 0.1617499440908432\n",
            "step: 200, loss: 0.10757837444543839\n",
            "step: 210, loss: 0.10067395120859146\n",
            "step: 220, loss: 0.14214082062244415\n",
            "step: 230, loss: 0.10811727494001389\n",
            "step: 240, loss: 0.17133483290672302\n",
            "step: 250, loss: 0.07953298091888428\n",
            "step: 260, loss: 0.0413941890001297\n",
            "step: 270, loss: 0.11792970448732376\n",
            "step: 280, loss: 0.13387079536914825\n",
            "step: 290, loss: 0.11840201169252396\n",
            "step: 300, loss: 0.13221246004104614\n",
            "step: 310, loss: 0.16970475018024445\n",
            "step: 320, loss: 0.16635887324810028\n",
            "step: 330, loss: 0.19747692346572876\n",
            "step: 340, loss: 0.15303297340869904\n",
            "step: 350, loss: 0.14308585226535797\n",
            "step: 360, loss: 0.09377992898225784\n",
            "step: 370, loss: 0.16836795210838318\n",
            "step: 380, loss: 0.13831539452075958\n",
            "step: 390, loss: 0.11156544089317322\n",
            "step: 400, loss: 0.01677999086678028\n",
            "step: 410, loss: 0.11242063343524933\n",
            "step: 420, loss: 0.03436867892742157\n",
            "step: 430, loss: 0.11629840731620789\n",
            "step: 440, loss: 0.23745699226856232\n",
            "step: 450, loss: 0.018993055447936058\n",
            "step: 460, loss: 0.11275738477706909\n",
            "step: 470, loss: 0.2523824870586395\n",
            "step: 480, loss: 0.2635113000869751\n",
            "step: 490, loss: 0.08883830904960632\n",
            "step: 500, loss: 0.0277822595089674\n",
            "step: 510, loss: 0.10922891646623611\n",
            "step: 520, loss: 0.08884138613939285\n",
            "step: 530, loss: 0.2635475695133209\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8920056100981766, f1=0.8902382064455862, best_f1=0.8902382064455862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2386055290699005\n",
            "step: 10, loss: 0.11600880324840546\n",
            "step: 20, loss: 0.15492823719978333\n",
            "step: 30, loss: 0.13924865424633026\n",
            "step: 40, loss: 0.023804865777492523\n",
            "step: 50, loss: 0.10340335220098495\n",
            "step: 60, loss: 0.17240574955940247\n",
            "step: 70, loss: 0.34243106842041016\n",
            "step: 80, loss: 0.02026698924601078\n",
            "step: 90, loss: 0.09335444867610931\n",
            "step: 100, loss: 0.30028337240219116\n",
            "step: 110, loss: 0.07515314966440201\n",
            "step: 120, loss: 0.11114243417978287\n",
            "step: 130, loss: 0.08118550479412079\n",
            "step: 140, loss: 0.037513043731451035\n",
            "step: 150, loss: 0.03078765980899334\n",
            "step: 160, loss: 0.15260060131549835\n",
            "step: 170, loss: 0.04737955331802368\n",
            "step: 180, loss: 0.018871331587433815\n",
            "step: 190, loss: 0.0924186259508133\n",
            "step: 200, loss: 0.049342140555381775\n",
            "step: 210, loss: 0.1017257422208786\n",
            "step: 220, loss: 0.1053803488612175\n",
            "step: 230, loss: 0.09622996300458908\n",
            "step: 240, loss: 0.1262613832950592\n",
            "step: 250, loss: 0.04965320974588394\n",
            "step: 260, loss: 0.042372044175863266\n",
            "step: 270, loss: 0.07876594364643097\n",
            "step: 280, loss: 0.07916798442602158\n",
            "step: 290, loss: 0.041601214557886124\n",
            "step: 300, loss: 0.05776198208332062\n",
            "step: 310, loss: 0.03119998797774315\n",
            "step: 320, loss: 0.1108069196343422\n",
            "step: 330, loss: 0.022392740473151207\n",
            "step: 340, loss: 0.015024282969534397\n",
            "step: 350, loss: 0.059788625687360764\n",
            "step: 360, loss: 0.11621053516864777\n",
            "step: 370, loss: 0.04149946942925453\n",
            "step: 380, loss: 0.08435884118080139\n",
            "step: 390, loss: 0.06995288282632828\n",
            "step: 400, loss: 0.07652588933706284\n",
            "step: 410, loss: 0.009474245831370354\n",
            "step: 420, loss: 0.03189869597554207\n",
            "step: 430, loss: 0.05570197105407715\n",
            "step: 440, loss: 0.015284432098269463\n",
            "step: 450, loss: 0.17301814258098602\n",
            "step: 460, loss: 0.21228013932704926\n",
            "step: 470, loss: 0.12101995944976807\n",
            "step: 480, loss: 0.24344760179519653\n",
            "step: 490, loss: 0.12697802484035492\n",
            "step: 500, loss: 0.044768452644348145\n",
            "step: 510, loss: 0.1309777945280075\n",
            "step: 520, loss: 0.03172503411769867\n",
            "step: 530, loss: 0.19641993939876556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9070422535211268, f1=0.9104477611940298, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.039283789694309235\n",
            "step: 10, loss: 0.12061251699924469\n",
            "step: 20, loss: 0.04536302387714386\n",
            "step: 30, loss: 0.25900885462760925\n",
            "step: 40, loss: 0.01655598171055317\n",
            "step: 50, loss: 0.012724379077553749\n",
            "step: 60, loss: 0.021636370569467545\n",
            "step: 70, loss: 0.0074782841838896275\n",
            "step: 80, loss: 0.028658907860517502\n",
            "step: 90, loss: 0.14060568809509277\n",
            "step: 100, loss: 0.010174209251999855\n",
            "step: 110, loss: 0.03279908001422882\n",
            "step: 120, loss: 0.02558637410402298\n",
            "step: 130, loss: 0.00847102701663971\n",
            "step: 140, loss: 0.10102445632219315\n",
            "step: 150, loss: 0.025976212695240974\n",
            "step: 160, loss: 0.0038301709573715925\n",
            "step: 170, loss: 0.0491856187582016\n",
            "step: 180, loss: 0.11022146791219711\n",
            "step: 190, loss: 0.007169898133724928\n",
            "step: 200, loss: 0.21952049434185028\n",
            "step: 210, loss: 0.06911540031433105\n",
            "step: 220, loss: 0.1312745213508606\n",
            "step: 230, loss: 0.02332480624318123\n",
            "step: 240, loss: 0.012308722361922264\n",
            "step: 250, loss: 0.046046555042266846\n",
            "step: 260, loss: 0.006014632061123848\n",
            "step: 270, loss: 0.014142638072371483\n",
            "step: 280, loss: 0.10293428599834442\n",
            "step: 290, loss: 0.24774914979934692\n",
            "step: 300, loss: 0.03833485394716263\n",
            "step: 310, loss: 0.15181171894073486\n",
            "step: 320, loss: 0.013337945565581322\n",
            "step: 330, loss: 0.04627753421664238\n",
            "step: 340, loss: 0.0174228698015213\n",
            "step: 350, loss: 0.13698966801166534\n",
            "step: 360, loss: 0.011210797354578972\n",
            "step: 370, loss: 0.021506113931536674\n",
            "step: 380, loss: 0.07144390046596527\n",
            "step: 390, loss: 0.07190593332052231\n",
            "step: 400, loss: 0.04086451232433319\n",
            "step: 410, loss: 0.08002157509326935\n",
            "step: 420, loss: 0.07551326602697372\n",
            "step: 430, loss: 0.04114105552434921\n",
            "step: 440, loss: 0.05853309482336044\n",
            "step: 450, loss: 0.15678220987319946\n",
            "step: 460, loss: 0.13109996914863586\n",
            "step: 470, loss: 0.009140793234109879\n",
            "step: 480, loss: 0.05136948078870773\n",
            "step: 490, loss: 0.020129462704062462\n",
            "step: 500, loss: 0.1049535870552063\n",
            "step: 510, loss: 0.018447332084178925\n",
            "step: 520, loss: 0.008360697887837887\n",
            "step: 530, loss: 0.01095216628164053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9007285974499089, f1=0.9004092769440655, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006792099680751562\n",
            "step: 10, loss: 0.007050659041851759\n",
            "step: 20, loss: 0.09040488302707672\n",
            "step: 30, loss: 0.015588432550430298\n",
            "step: 40, loss: 0.0059374067932367325\n",
            "step: 50, loss: 0.046628884971141815\n",
            "step: 60, loss: 0.0025498028844594955\n",
            "step: 70, loss: 0.013851379975676537\n",
            "step: 80, loss: 0.017525173723697662\n",
            "step: 90, loss: 0.15853913128376007\n",
            "step: 100, loss: 0.1817716509103775\n",
            "step: 110, loss: 0.032825812697410583\n",
            "step: 120, loss: 0.007713764905929565\n",
            "step: 130, loss: 0.03247793763875961\n",
            "step: 140, loss: 0.022262969985604286\n",
            "step: 150, loss: 0.03382158279418945\n",
            "step: 160, loss: 0.00291619636118412\n",
            "step: 170, loss: 0.011065076105296612\n",
            "step: 180, loss: 0.018620261922478676\n",
            "step: 190, loss: 0.056306954473257065\n",
            "step: 200, loss: 0.0026280307210981846\n",
            "step: 210, loss: 0.04850895330309868\n",
            "step: 220, loss: 0.12409475445747375\n",
            "step: 230, loss: 0.4909496009349823\n",
            "step: 240, loss: 0.00778992148116231\n",
            "step: 250, loss: 0.018447065725922585\n",
            "step: 260, loss: 0.12538078427314758\n",
            "step: 270, loss: 0.14277228713035583\n",
            "step: 280, loss: 0.004448685795068741\n",
            "step: 290, loss: 0.03933870419859886\n",
            "step: 300, loss: 0.015632309019565582\n",
            "step: 310, loss: 0.0038525047712028027\n",
            "step: 320, loss: 0.08306685090065002\n",
            "step: 330, loss: 0.04955463856458664\n",
            "step: 340, loss: 0.05836660787463188\n",
            "step: 350, loss: 0.010832889936864376\n",
            "step: 360, loss: 0.03428703919053078\n",
            "step: 370, loss: 0.023519566282629967\n",
            "step: 380, loss: 0.008495645597577095\n",
            "step: 390, loss: 0.017530502751469612\n",
            "step: 400, loss: 0.00886064488440752\n",
            "step: 410, loss: 0.08011895418167114\n",
            "step: 420, loss: 0.07754646986722946\n",
            "step: 430, loss: 0.024317583069205284\n",
            "step: 440, loss: 0.1227385401725769\n",
            "step: 450, loss: 0.018870238214731216\n",
            "step: 460, loss: 0.0018563823541626334\n",
            "step: 470, loss: 0.014403345994651318\n",
            "step: 480, loss: 0.00887301191687584\n",
            "step: 490, loss: 0.09100735932588577\n",
            "step: 500, loss: 0.004918817430734634\n",
            "step: 510, loss: 0.14019858837127686\n",
            "step: 520, loss: 0.23201388120651245\n",
            "step: 530, loss: 0.020666461437940598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9021937842778793, f1=0.897716894977169, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006022321060299873\n",
            "step: 10, loss: 0.06432472169399261\n",
            "step: 20, loss: 0.047971852123737335\n",
            "step: 30, loss: 0.013478708453476429\n",
            "step: 40, loss: 0.04381547123193741\n",
            "step: 50, loss: 0.003696516389027238\n",
            "step: 60, loss: 0.0022524225059896708\n",
            "step: 70, loss: 0.1324538141489029\n",
            "step: 80, loss: 0.0017495541833341122\n",
            "step: 90, loss: 0.005056372378021479\n",
            "step: 100, loss: 0.0071142567321658134\n",
            "step: 110, loss: 0.007460595108568668\n",
            "step: 120, loss: 0.0208884384483099\n",
            "step: 130, loss: 0.007298387587070465\n",
            "step: 140, loss: 0.002524443669244647\n",
            "step: 150, loss: 0.009822946973145008\n",
            "step: 160, loss: 0.00971466489136219\n",
            "step: 170, loss: 0.0033675942104309797\n",
            "step: 180, loss: 0.0014111929340288043\n",
            "step: 190, loss: 0.017094366252422333\n",
            "step: 200, loss: 0.01030647661536932\n",
            "step: 210, loss: 0.007102060131728649\n",
            "step: 220, loss: 0.0005608184146694839\n",
            "step: 230, loss: 0.0008852812461555004\n",
            "step: 240, loss: 0.023265434429049492\n",
            "step: 250, loss: 0.0012780928518623114\n",
            "step: 260, loss: 0.03461616113781929\n",
            "step: 270, loss: 0.05295082554221153\n",
            "step: 280, loss: 0.07088273763656616\n",
            "step: 290, loss: 0.0028168438002467155\n",
            "step: 300, loss: 0.04198722541332245\n",
            "step: 310, loss: 0.0016504502855241299\n",
            "step: 320, loss: 0.004131937865167856\n",
            "step: 330, loss: 0.007252927869558334\n",
            "step: 340, loss: 0.018119996413588524\n",
            "step: 350, loss: 0.0027043959125876427\n",
            "step: 360, loss: 0.1498906910419464\n",
            "step: 370, loss: 0.0009155795560218394\n",
            "step: 380, loss: 0.011389797553420067\n",
            "step: 390, loss: 0.018422577530145645\n",
            "step: 400, loss: 0.02063353732228279\n",
            "step: 410, loss: 0.0013614011695608497\n",
            "step: 420, loss: 0.01434725895524025\n",
            "step: 430, loss: 0.01264370046555996\n",
            "step: 440, loss: 0.035095103085041046\n",
            "step: 450, loss: 0.012083783745765686\n",
            "step: 460, loss: 0.07523234933614731\n",
            "step: 470, loss: 0.02508188784122467\n",
            "step: 480, loss: 0.0019287520553916693\n",
            "step: 490, loss: 0.0074625094421207905\n",
            "step: 500, loss: 0.0016763993771746755\n",
            "step: 510, loss: 0.04651284217834473\n",
            "step: 520, loss: 0.011798799969255924\n",
            "step: 530, loss: 0.020899301394820213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9019980970504282, f1=0.8928229665071771, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0243715550750494\n",
            "step: 10, loss: 0.011245988309383392\n",
            "step: 20, loss: 0.004699685610830784\n",
            "step: 30, loss: 0.0013107939157634974\n",
            "step: 40, loss: 0.0007184430141933262\n",
            "step: 50, loss: 0.013813141733407974\n",
            "step: 60, loss: 0.03229542076587677\n",
            "step: 70, loss: 0.01012780237942934\n",
            "step: 80, loss: 0.0007894731243140996\n",
            "step: 90, loss: 0.0073320697993040085\n",
            "step: 100, loss: 0.08287346363067627\n",
            "step: 110, loss: 0.2106078565120697\n",
            "step: 120, loss: 0.011587828397750854\n",
            "step: 130, loss: 0.0012895665131509304\n",
            "step: 140, loss: 0.004900555592030287\n",
            "step: 150, loss: 0.001987220486626029\n",
            "step: 160, loss: 0.003406956559047103\n",
            "step: 170, loss: 0.0030640934128314257\n",
            "step: 180, loss: 0.00031467818189412355\n",
            "step: 190, loss: 0.00764817139133811\n",
            "step: 200, loss: 0.0002536106330808252\n",
            "step: 210, loss: 0.0003780378319788724\n",
            "step: 220, loss: 0.0006502858595922589\n",
            "step: 230, loss: 0.0029437316115945578\n",
            "step: 240, loss: 0.00601110327988863\n",
            "step: 250, loss: 0.0002402377431280911\n",
            "step: 260, loss: 0.04256713390350342\n",
            "step: 270, loss: 0.002197777619585395\n",
            "step: 280, loss: 0.10966670513153076\n",
            "step: 290, loss: 0.009332867339253426\n",
            "step: 300, loss: 0.0008973160875029862\n",
            "step: 310, loss: 0.001181241124868393\n",
            "step: 320, loss: 0.10431816428899765\n",
            "step: 330, loss: 0.00404629111289978\n",
            "step: 340, loss: 0.002001373562961817\n",
            "step: 350, loss: 0.08453928679227829\n",
            "step: 360, loss: 0.004227080848067999\n",
            "step: 370, loss: 0.008362467400729656\n",
            "step: 380, loss: 0.007225956302136183\n",
            "step: 390, loss: 0.16290897130966187\n",
            "step: 400, loss: 0.0003318320959806442\n",
            "step: 410, loss: 0.01989017240703106\n",
            "step: 420, loss: 0.030679097399115562\n",
            "step: 430, loss: 0.0006147891981527209\n",
            "step: 440, loss: 0.006477974820882082\n",
            "step: 450, loss: 0.022982297465205193\n",
            "step: 460, loss: 0.11566944420337677\n",
            "step: 470, loss: 0.007529266644269228\n",
            "step: 480, loss: 0.016544494777917862\n",
            "step: 490, loss: 0.0006045677582733333\n",
            "step: 500, loss: 0.004286336712539196\n",
            "step: 510, loss: 0.0241264458745718\n",
            "step: 520, loss: 0.006169862579554319\n",
            "step: 530, loss: 0.002293383702635765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9064272211720227, f1=0.9018867924528302, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02899659052491188\n",
            "step: 10, loss: 0.02447107620537281\n",
            "step: 20, loss: 0.007908526808023453\n",
            "step: 30, loss: 0.20208057761192322\n",
            "step: 40, loss: 0.05590961501002312\n",
            "step: 50, loss: 0.007333146408200264\n",
            "step: 60, loss: 0.0005152914091013372\n",
            "step: 70, loss: 0.0037516988813877106\n",
            "step: 80, loss: 0.0005301770870573819\n",
            "step: 90, loss: 0.0005313316360116005\n",
            "step: 100, loss: 0.0005374237080104649\n",
            "step: 110, loss: 0.03119746968150139\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.00028816916164942086\n",
            "step: 130, loss: 0.0008188208448700607\n",
            "step: 140, loss: 0.010508107021450996\n",
            "step: 150, loss: 0.00035504449624568224\n",
            "step: 160, loss: 0.0007633817149326205\n",
            "step: 170, loss: 0.0011460561072453856\n",
            "step: 180, loss: 0.0005727981915697455\n",
            "step: 190, loss: 0.0007862074999138713\n",
            "step: 200, loss: 0.00380427623167634\n",
            "step: 210, loss: 0.0032229407224804163\n",
            "step: 220, loss: 0.007276154588907957\n",
            "step: 230, loss: 0.01500729750841856\n",
            "step: 240, loss: 0.019522592425346375\n",
            "step: 250, loss: 0.01878364197909832\n",
            "step: 260, loss: 0.006786208134144545\n",
            "step: 270, loss: 0.0002125797764165327\n",
            "step: 280, loss: 0.004267312120646238\n",
            "step: 290, loss: 0.09476140886545181\n",
            "step: 300, loss: 0.0021209127735346556\n",
            "step: 310, loss: 0.00023385082022286952\n",
            "step: 320, loss: 0.029833802953362465\n",
            "step: 330, loss: 0.002774506341665983\n",
            "step: 340, loss: 0.010658608749508858\n",
            "step: 350, loss: 0.006102377083152533\n",
            "step: 360, loss: 0.004166932310909033\n",
            "step: 370, loss: 0.00023761013289913535\n",
            "step: 380, loss: 0.0023445766419172287\n",
            "step: 390, loss: 0.0018215967575088143\n",
            "step: 400, loss: 0.037627480924129486\n",
            "step: 410, loss: 0.0040895682759583\n",
            "step: 420, loss: 0.0005273489514365792\n",
            "step: 430, loss: 0.002296384423971176\n",
            "step: 440, loss: 0.0008560302667319775\n",
            "step: 450, loss: 0.0030368922743946314\n",
            "step: 460, loss: 0.00163132487796247\n",
            "step: 470, loss: 0.034151606261730194\n",
            "step: 480, loss: 0.006359501276165247\n",
            "step: 490, loss: 0.00024916810798458755\n",
            "step: 500, loss: 0.0002724384830798954\n",
            "step: 510, loss: 0.0006992605049163103\n",
            "step: 520, loss: 0.25812891125679016\n",
            "step: 530, loss: 0.008803033269941807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8995305164319248, f1=0.8838311996206734, best_f1=0.9104477611940298\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010449629044160247\n",
            "step: 10, loss: 0.0008594889659434557\n",
            "step: 20, loss: 0.0015271547017619014\n",
            "step: 30, loss: 0.0007915567839518189\n",
            "step: 40, loss: 0.02400072291493416\n",
            "step: 50, loss: 0.0008694994612596929\n",
            "step: 60, loss: 0.002102307043969631\n",
            "step: 70, loss: 0.005107145290821791\n",
            "step: 80, loss: 0.0013852114789187908\n",
            "step: 90, loss: 8.235334098571911e-05\n",
            "step: 100, loss: 0.014856306836009026\n",
            "step: 110, loss: 0.0004340430023148656\n",
            "step: 120, loss: 0.00042558545828796923\n",
            "step: 130, loss: 0.0019454753492027521\n",
            "step: 140, loss: 0.00010209379252046347\n",
            "step: 150, loss: 0.0028622914105653763\n",
            "step: 160, loss: 0.0053033907897770405\n",
            "step: 170, loss: 0.00026179637643508613\n",
            "step: 180, loss: 0.0005481241387315094\n",
            "step: 190, loss: 0.0054372940212488174\n",
            "step: 200, loss: 0.0002386776322964579\n",
            "step: 210, loss: 0.07276497036218643\n",
            "step: 220, loss: 0.0021834198851138353\n",
            "step: 230, loss: 0.003547589760273695\n",
            "step: 240, loss: 0.07326788455247879\n",
            "step: 250, loss: 0.008416907861828804\n",
            "step: 260, loss: 0.002834924729540944\n",
            "step: 270, loss: 0.00011732481652870774\n",
            "step: 280, loss: 0.028157051652669907\n",
            "step: 290, loss: 0.004547297954559326\n",
            "step: 300, loss: 0.00041533485637046397\n",
            "step: 310, loss: 0.00109567039180547\n",
            "step: 320, loss: 0.0004044041852466762\n",
            "step: 330, loss: 0.003341520670801401\n",
            "step: 340, loss: 0.002875254489481449\n",
            "step: 350, loss: 0.0008203869801945984\n",
            "step: 360, loss: 0.000877460406627506\n",
            "step: 370, loss: 0.0001723577152006328\n",
            "step: 380, loss: 0.00011180635920027271\n",
            "step: 390, loss: 0.0005369606078602374\n",
            "step: 400, loss: 0.011992468498647213\n",
            "step: 410, loss: 0.00012193244037916884\n",
            "step: 420, loss: 0.0004111335729248822\n",
            "step: 430, loss: 0.00017416418995708227\n",
            "step: 440, loss: 0.0025300111155956984\n",
            "step: 450, loss: 0.0013613927876576781\n",
            "step: 460, loss: 0.0009743022383190691\n",
            "step: 470, loss: 0.00026293261907994747\n",
            "step: 480, loss: 0.00021200325863901526\n",
            "step: 490, loss: 0.00012393620272632688\n",
            "step: 500, loss: 0.0002930932096205652\n",
            "step: 510, loss: 0.0003397312539163977\n",
            "step: 520, loss: 0.044243164360523224\n",
            "step: 530, loss: 0.0022585734259337187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9099264705882353, f1=0.9010082493125573, best_f1=0.9010082493125573\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012710157898254693\n",
            "step: 10, loss: 0.001811484806239605\n",
            "step: 20, loss: 0.00025207403814420104\n",
            "step: 30, loss: 0.04580257833003998\n",
            "step: 40, loss: 0.0017608747584745288\n",
            "step: 50, loss: 0.00016661181871313602\n",
            "step: 60, loss: 0.0013245880836620927\n",
            "step: 70, loss: 0.09608511626720428\n",
            "step: 80, loss: 5.836014315718785e-05\n",
            "step: 90, loss: 0.00696758134290576\n",
            "step: 100, loss: 0.00011815282050520182\n",
            "step: 110, loss: 0.0002632666437420994\n",
            "step: 120, loss: 6.68893480906263e-05\n",
            "step: 130, loss: 0.005571961402893066\n",
            "step: 140, loss: 0.00019173239707015455\n",
            "step: 150, loss: 0.00010960292274830863\n",
            "step: 160, loss: 9.629800479160622e-05\n",
            "step: 170, loss: 8.799450733931735e-05\n",
            "step: 180, loss: 0.009281539358198643\n",
            "step: 190, loss: 0.00016796053387224674\n",
            "step: 200, loss: 0.00016385194612666965\n",
            "step: 210, loss: 0.00019912865536753088\n",
            "step: 220, loss: 0.00015778363740537316\n",
            "step: 230, loss: 0.0053124018013477325\n",
            "step: 240, loss: 0.0004940003855153918\n",
            "step: 250, loss: 0.005892560351639986\n",
            "step: 260, loss: 0.00012653916201088578\n",
            "step: 270, loss: 0.0013661248376592994\n",
            "step: 280, loss: 7.181961700553074e-05\n",
            "step: 290, loss: 0.0001340630988124758\n",
            "step: 300, loss: 0.0001812126865843311\n",
            "step: 310, loss: 0.0005185390473343432\n",
            "step: 320, loss: 0.009207877330482006\n",
            "step: 330, loss: 7.320723671000451e-05\n",
            "step: 340, loss: 0.22737742960453033\n",
            "step: 350, loss: 0.00010987262066919357\n",
            "step: 360, loss: 0.06278065592050552\n",
            "step: 370, loss: 0.0013377299765124917\n",
            "step: 380, loss: 0.0004901491920463741\n",
            "step: 390, loss: 0.00016655886429362\n",
            "step: 400, loss: 0.0012883845483884215\n",
            "step: 410, loss: 0.030197497457265854\n",
            "step: 420, loss: 0.0005196749698370695\n",
            "step: 430, loss: 0.0007776746060699224\n",
            "step: 440, loss: 0.05967879667878151\n",
            "step: 450, loss: 7.489875133614987e-05\n",
            "step: 460, loss: 0.0002458388917148113\n",
            "step: 470, loss: 8.382445957977325e-05\n",
            "step: 480, loss: 0.00016517947369720787\n",
            "step: 490, loss: 7.674522930756211e-05\n",
            "step: 500, loss: 0.0001734695688355714\n",
            "step: 510, loss: 0.0010884073562920094\n",
            "step: 520, loss: 8.742558566154912e-05\n",
            "step: 530, loss: 0.00022627795988228172\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9158530915853093, f1=0.9069767441860467, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031125258654356003\n",
            "step: 10, loss: 5.099217014503665e-05\n",
            "step: 20, loss: 7.328562787733972e-05\n",
            "step: 30, loss: 5.222554682404734e-05\n",
            "step: 40, loss: 0.000323205633321777\n",
            "step: 50, loss: 0.00023811060236766934\n",
            "step: 60, loss: 0.00016147579299286008\n",
            "step: 70, loss: 0.00046902187750674784\n",
            "step: 80, loss: 0.001881958800368011\n",
            "step: 90, loss: 0.0004419989127200097\n",
            "step: 100, loss: 5.053061613580212e-05\n",
            "step: 110, loss: 0.0007081162184476852\n",
            "step: 120, loss: 4.041002466692589e-05\n",
            "step: 130, loss: 0.0001469740818720311\n",
            "step: 140, loss: 0.00010058742191176862\n",
            "step: 150, loss: 0.00033867047750391066\n",
            "step: 160, loss: 0.0002014257333939895\n",
            "step: 170, loss: 0.00023553776554763317\n",
            "step: 180, loss: 0.01798698864877224\n",
            "step: 190, loss: 0.0001193598291138187\n",
            "step: 200, loss: 0.02268211543560028\n",
            "step: 210, loss: 0.006876681931316853\n",
            "step: 220, loss: 0.0006023194873705506\n",
            "step: 230, loss: 0.000272151519311592\n",
            "step: 240, loss: 0.0003503556945361197\n",
            "step: 250, loss: 0.00041474390309304\n",
            "step: 260, loss: 0.0007429607794620097\n",
            "step: 270, loss: 0.001692013582214713\n",
            "step: 280, loss: 0.00010825843492057174\n",
            "step: 290, loss: 5.856111602042802e-05\n",
            "step: 300, loss: 0.0005933787324465811\n",
            "step: 310, loss: 0.025121593847870827\n",
            "step: 320, loss: 0.000229112891247496\n",
            "step: 330, loss: 0.012972884811460972\n",
            "step: 340, loss: 0.009888997301459312\n",
            "step: 350, loss: 0.0004994641640223563\n",
            "step: 360, loss: 9.330619650427252e-05\n",
            "step: 370, loss: 0.0020957186352461576\n",
            "step: 380, loss: 0.00029809516854584217\n",
            "step: 390, loss: 0.00020150220370851457\n",
            "step: 400, loss: 0.0003562269266694784\n",
            "step: 410, loss: 0.0010880044428631663\n",
            "step: 420, loss: 0.007383884862065315\n",
            "step: 430, loss: 0.00012508462532423437\n",
            "step: 440, loss: 7.621372060384601e-05\n",
            "step: 450, loss: 0.004394313786178827\n",
            "step: 460, loss: 0.00016555498586967587\n",
            "step: 470, loss: 0.00010418774036224931\n",
            "step: 480, loss: 5.058643000666052e-05\n",
            "step: 490, loss: 0.004938443657010794\n",
            "step: 500, loss: 0.0008587267948314548\n",
            "step: 510, loss: 0.0005562136648222804\n",
            "step: 520, loss: 0.0009345613652840257\n",
            "step: 530, loss: 0.0019049689872190356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.904476234425473, f1=0.8968070337806572, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001654300489462912\n",
            "step: 10, loss: 0.0020877299830317497\n",
            "step: 20, loss: 4.7750720113981515e-05\n",
            "step: 30, loss: 0.019613126292824745\n",
            "step: 40, loss: 0.0011956198140978813\n",
            "step: 50, loss: 0.09767451882362366\n",
            "step: 60, loss: 0.00029963150154799223\n",
            "step: 70, loss: 0.000275350728770718\n",
            "step: 80, loss: 0.000358305755071342\n",
            "step: 90, loss: 0.00025505467783659697\n",
            "step: 100, loss: 0.0005356220644898713\n",
            "step: 110, loss: 0.00024373221094720066\n",
            "step: 120, loss: 0.0003491388924885541\n",
            "step: 130, loss: 0.013229712843894958\n",
            "step: 140, loss: 4.205533332424238e-05\n",
            "step: 150, loss: 0.002645474160090089\n",
            "step: 160, loss: 0.0006867969059385359\n",
            "step: 170, loss: 0.00018871112843044102\n",
            "step: 180, loss: 0.0001726567861624062\n",
            "step: 190, loss: 0.0009360762778669596\n",
            "step: 200, loss: 0.000986530794762075\n",
            "step: 210, loss: 0.00021528905199375004\n",
            "step: 220, loss: 7.62420822866261e-05\n",
            "step: 230, loss: 0.0003438446728978306\n",
            "step: 240, loss: 5.6092889280989766e-05\n",
            "step: 250, loss: 0.001011133543215692\n",
            "step: 260, loss: 9.591527486918494e-05\n",
            "step: 270, loss: 0.002695602597668767\n",
            "step: 280, loss: 0.0002744044177234173\n",
            "step: 290, loss: 0.0010979297803714871\n",
            "step: 300, loss: 0.0022948586847633123\n",
            "step: 310, loss: 0.016274062916636467\n",
            "step: 320, loss: 0.0008773254230618477\n",
            "step: 330, loss: 0.00041663035517558455\n",
            "step: 340, loss: 0.00030127938953228295\n",
            "step: 350, loss: 0.0001914358581416309\n",
            "step: 360, loss: 0.0001377103035338223\n",
            "step: 370, loss: 4.1369425161974505e-05\n",
            "step: 380, loss: 0.00034361210418865085\n",
            "step: 390, loss: 0.0030731225851923227\n",
            "step: 400, loss: 4.901912689092569e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 410, loss: 0.08131949603557587\n",
            "step: 420, loss: 0.025283444672822952\n",
            "step: 430, loss: 0.0031803783494979143\n",
            "step: 440, loss: 6.969686364755034e-05\n",
            "step: 450, loss: 4.9273829063167796e-05\n",
            "step: 460, loss: 0.001445887959562242\n",
            "step: 470, loss: 0.0022307501640170813\n",
            "step: 480, loss: 5.057920134277083e-05\n",
            "step: 490, loss: 0.0004650955379474908\n",
            "step: 500, loss: 5.539812264032662e-05\n",
            "step: 510, loss: 2.630011113069486e-05\n",
            "step: 520, loss: 0.00023847923148423433\n",
            "step: 530, loss: 2.9905300834798254e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9074327405380758, f1=0.9032553874369554, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.091176379006356e-05\n",
            "step: 10, loss: 3.366453529451974e-05\n",
            "step: 20, loss: 0.0008462695986963809\n",
            "step: 30, loss: 0.00025200253003276885\n",
            "step: 40, loss: 0.0005262693157419562\n",
            "step: 50, loss: 3.0107115890132263e-05\n",
            "step: 60, loss: 3.827879118034616e-05\n",
            "step: 70, loss: 4.6601078793173656e-05\n",
            "step: 80, loss: 0.0005695407744497061\n",
            "step: 90, loss: 3.5482371458783746e-05\n",
            "step: 100, loss: 0.00011210536467842758\n",
            "step: 110, loss: 4.1090359445661306e-05\n",
            "step: 120, loss: 0.02698628418147564\n",
            "step: 130, loss: 0.00011947994789807126\n",
            "step: 140, loss: 5.200216037337668e-05\n",
            "step: 150, loss: 0.00038717302959412336\n",
            "step: 160, loss: 0.0014169042697176337\n",
            "step: 170, loss: 5.9110665461048484e-05\n",
            "step: 180, loss: 3.4148335544159636e-05\n",
            "step: 190, loss: 4.252750659361482e-05\n",
            "step: 200, loss: 5.074121509096585e-05\n",
            "step: 210, loss: 6.444691098295152e-05\n",
            "step: 220, loss: 3.3299438655376434e-05\n",
            "step: 230, loss: 0.0035138323437422514\n",
            "step: 240, loss: 9.688379941508174e-05\n",
            "step: 250, loss: 0.00010238060349365696\n",
            "step: 260, loss: 0.00028523773653432727\n",
            "step: 270, loss: 5.356501787900925e-05\n",
            "step: 280, loss: 0.013149772770702839\n",
            "step: 290, loss: 0.0011182422749698162\n",
            "step: 300, loss: 0.00015541893662884831\n",
            "step: 310, loss: 8.448466542176902e-05\n",
            "step: 320, loss: 0.0005379998474381864\n",
            "step: 330, loss: 3.159328844049014e-05\n",
            "step: 340, loss: 7.068787090247497e-05\n",
            "step: 350, loss: 0.0005230975802987814\n",
            "step: 360, loss: 0.0004228855250403285\n",
            "step: 370, loss: 9.188630065182224e-05\n",
            "step: 380, loss: 4.1881692595779896e-05\n",
            "step: 390, loss: 4.305052061681636e-05\n",
            "step: 400, loss: 0.001756215700879693\n",
            "step: 410, loss: 0.04588715359568596\n",
            "step: 420, loss: 0.012607629410922527\n",
            "step: 430, loss: 0.008855445310473442\n",
            "step: 440, loss: 0.0009375223889946938\n",
            "step: 450, loss: 0.0005675139254890382\n",
            "step: 460, loss: 0.0019415965070948005\n",
            "step: 470, loss: 0.0003656769695226103\n",
            "step: 480, loss: 4.466063910513185e-05\n",
            "step: 490, loss: 7.784023182466626e-05\n",
            "step: 500, loss: 0.006576853338629007\n",
            "step: 510, loss: 4.883242945652455e-05\n",
            "step: 520, loss: 0.005180027801543474\n",
            "step: 530, loss: 0.0014334887964650989\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9016166281755196, f1=0.8931050439611292, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.605150413932279e-05\n",
            "step: 10, loss: 3.612908039940521e-05\n",
            "step: 20, loss: 9.746100113261491e-05\n",
            "step: 30, loss: 0.0021923540625721216\n",
            "step: 40, loss: 4.9667327402858064e-05\n",
            "step: 50, loss: 3.0241146305343136e-05\n",
            "step: 60, loss: 3.896986527252011e-05\n",
            "step: 70, loss: 3.39209582307376e-05\n",
            "step: 80, loss: 0.0045065609738230705\n",
            "step: 90, loss: 0.00022525039094034582\n",
            "step: 100, loss: 0.0006386112654581666\n",
            "step: 110, loss: 0.012835056520998478\n",
            "step: 120, loss: 0.0019952021539211273\n",
            "step: 130, loss: 0.004622084088623524\n",
            "step: 140, loss: 0.0011830731527879834\n",
            "step: 150, loss: 2.9879747671657242e-05\n",
            "step: 160, loss: 4.200092735118233e-05\n",
            "step: 170, loss: 6.403511360986158e-05\n",
            "step: 180, loss: 0.0037274810019880533\n",
            "step: 190, loss: 3.713112892000936e-05\n",
            "step: 200, loss: 0.0012100718449801207\n",
            "step: 210, loss: 0.00025412882678210735\n",
            "step: 220, loss: 2.3502374460804276e-05\n",
            "step: 230, loss: 7.677119720028713e-05\n",
            "step: 240, loss: 3.311291220597923e-05\n",
            "step: 250, loss: 0.0009444250026717782\n",
            "step: 260, loss: 0.00012453034287318587\n",
            "step: 270, loss: 3.200252467649989e-05\n",
            "step: 280, loss: 1.991513818211388e-05\n",
            "step: 290, loss: 3.776461380766705e-05\n",
            "step: 300, loss: 3.578010364435613e-05\n",
            "step: 310, loss: 4.10159227612894e-05\n",
            "step: 320, loss: 3.083683259319514e-05\n",
            "step: 330, loss: 4.296115366742015e-05\n",
            "step: 340, loss: 2.9142203857190907e-05\n",
            "step: 350, loss: 3.627121259341948e-05\n",
            "step: 360, loss: 2.1561605535680428e-05\n",
            "step: 370, loss: 2.8977185138501227e-05\n",
            "step: 380, loss: 0.000666355132125318\n",
            "step: 390, loss: 4.466906102607027e-05\n",
            "step: 400, loss: 2.055214463325683e-05\n",
            "step: 410, loss: 0.00011210676893824711\n",
            "step: 420, loss: 2.179258081014268e-05\n",
            "step: 430, loss: 0.021266089752316475\n",
            "step: 440, loss: 0.021105865016579628\n",
            "step: 450, loss: 2.79838168353308e-05\n",
            "step: 460, loss: 4.19963471358642e-05\n",
            "step: 470, loss: 0.003944851458072662\n",
            "step: 480, loss: 0.00137917953543365\n",
            "step: 490, loss: 0.0007383065531030297\n",
            "step: 500, loss: 0.00013576331548392773\n",
            "step: 510, loss: 2.7685740860761143e-05\n",
            "step: 520, loss: 0.0046376436948776245\n",
            "step: 530, loss: 4.1287978092441335e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9041474654377881, f1=0.8934010152284264, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.867347135790624e-05\n",
            "step: 10, loss: 0.0002642383915372193\n",
            "step: 20, loss: 9.620797936804593e-05\n",
            "step: 30, loss: 0.0011449161684140563\n",
            "step: 40, loss: 5.538840559893288e-05\n",
            "step: 50, loss: 3.3723441447364166e-05\n",
            "step: 60, loss: 4.47359707322903e-05\n",
            "step: 70, loss: 2.1885778551222757e-05\n",
            "step: 80, loss: 5.0568985898280516e-05\n",
            "step: 90, loss: 0.002562990179285407\n",
            "step: 100, loss: 6.420002318918705e-05\n",
            "step: 110, loss: 8.522329881088808e-05\n",
            "step: 120, loss: 0.0001976750500034541\n",
            "step: 130, loss: 0.000411076529417187\n",
            "step: 140, loss: 0.013179624453186989\n",
            "step: 150, loss: 0.000386564526706934\n",
            "step: 160, loss: 0.0008030469762161374\n",
            "step: 170, loss: 3.6775105400010943e-05\n",
            "step: 180, loss: 3.814530646195635e-05\n",
            "step: 190, loss: 0.0084312055259943\n",
            "step: 200, loss: 5.866992796654813e-05\n",
            "step: 210, loss: 0.0024407743476331234\n",
            "step: 220, loss: 0.0012374832294881344\n",
            "step: 230, loss: 0.000518572167493403\n",
            "step: 240, loss: 3.49384754372295e-05\n",
            "step: 250, loss: 6.222239608177915e-05\n",
            "step: 260, loss: 2.603892789920792e-05\n",
            "step: 270, loss: 0.004786367062479258\n",
            "step: 280, loss: 0.002147115534171462\n",
            "step: 290, loss: 0.00011210680531803519\n",
            "step: 300, loss: 0.00026087561855092645\n",
            "step: 310, loss: 3.5295586712891236e-05\n",
            "step: 320, loss: 5.711897392757237e-05\n",
            "step: 330, loss: 0.00020519208919722587\n",
            "step: 340, loss: 0.0002542723377700895\n",
            "step: 350, loss: 0.00011697415902744979\n",
            "step: 360, loss: 0.00015846570022404194\n",
            "step: 370, loss: 0.0001857835886767134\n",
            "step: 380, loss: 4.407556480146013e-05\n",
            "step: 390, loss: 3.223751627956517e-05\n",
            "step: 400, loss: 3.0162802431732416e-05\n",
            "step: 410, loss: 4.318806895753369e-05\n",
            "step: 420, loss: 4.84569973195903e-05\n",
            "step: 430, loss: 2.798011337290518e-05\n",
            "step: 440, loss: 2.1099778678035364e-05\n",
            "step: 450, loss: 2.9745629944955e-05\n",
            "step: 460, loss: 2.7939129722653888e-05\n",
            "step: 470, loss: 5.590490036411211e-05\n",
            "step: 480, loss: 0.0011744922958314419\n",
            "step: 490, loss: 3.8958100049057975e-05\n",
            "step: 500, loss: 0.0001590478786965832\n",
            "step: 510, loss: 0.00019903035718016326\n",
            "step: 520, loss: 5.844482438988052e-05\n",
            "step: 530, loss: 0.0003778412065003067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9079189686924493, f1=0.8976014760147601, best_f1=0.9069767441860467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003732895420398563\n",
            "step: 10, loss: 7.040593482088298e-05\n",
            "step: 20, loss: 4.4415679440135136e-05\n",
            "step: 30, loss: 0.0001274504465982318\n",
            "step: 40, loss: 2.2113010345492512e-05\n",
            "step: 50, loss: 5.436444189399481e-05\n",
            "step: 60, loss: 2.8605647457879968e-05\n",
            "step: 70, loss: 5.6764623877825215e-05\n",
            "step: 80, loss: 3.646118057076819e-05\n",
            "step: 90, loss: 4.877339961240068e-05\n",
            "step: 100, loss: 2.850152122846339e-05\n",
            "step: 110, loss: 4.747490675072186e-05\n",
            "step: 120, loss: 0.0016623721458017826\n",
            "step: 130, loss: 0.00027713290182873607\n",
            "step: 140, loss: 2.2854314011055976e-05\n",
            "step: 150, loss: 5.511810741154477e-05\n",
            "step: 160, loss: 0.0001222019927809015\n",
            "step: 170, loss: 3.357484092703089e-05\n",
            "step: 180, loss: 0.00014840011135675013\n",
            "step: 190, loss: 5.69140502193477e-05\n",
            "step: 200, loss: 2.5290555640822276e-05\n",
            "step: 210, loss: 0.0025503505021333694\n",
            "step: 220, loss: 0.00034243383561261\n",
            "step: 230, loss: 0.0005987408221699297\n",
            "step: 240, loss: 0.0001609615283086896\n",
            "step: 250, loss: 0.0059795803390443325\n",
            "step: 260, loss: 3.868846033583395e-05\n",
            "step: 270, loss: 9.815231169341132e-05\n",
            "step: 280, loss: 4.259631532477215e-05\n",
            "step: 290, loss: 0.0033774932380765676\n",
            "step: 300, loss: 0.00011097986134700477\n",
            "step: 310, loss: 0.0010117068886756897\n",
            "step: 320, loss: 5.384182441048324e-05\n",
            "step: 330, loss: 0.001377042499370873\n",
            "step: 340, loss: 0.009545385837554932\n",
            "step: 350, loss: 0.0003916913701687008\n",
            "step: 360, loss: 7.666352030355483e-05\n",
            "step: 370, loss: 2.0302586563047953e-05\n",
            "step: 380, loss: 2.17217984754825e-05\n",
            "step: 390, loss: 2.2034775611246005e-05\n",
            "step: 400, loss: 2.314112862222828e-05\n",
            "step: 410, loss: 0.00023148178297560662\n",
            "step: 420, loss: 3.3984466426773e-05\n",
            "step: 430, loss: 3.065780038014054e-05\n",
            "step: 440, loss: 7.525619730586186e-05\n",
            "step: 450, loss: 4.537457425612956e-05\n",
            "step: 460, loss: 8.093791257124394e-05\n",
            "step: 470, loss: 3.870565342367627e-05\n",
            "step: 480, loss: 5.521561615751125e-05\n",
            "step: 490, loss: 3.567506428225897e-05\n",
            "step: 500, loss: 2.6612977308104746e-05\n",
            "step: 510, loss: 3.697119245771319e-05\n",
            "step: 520, loss: 2.2463173081632704e-05\n",
            "step: 530, loss: 4.254799205227755e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9100968188105117, f1=0.9000925069380203, best_f1=0.9069767441860467\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 245.70it/s]\n",
            "load_f1 = 0.9054494643688868\n",
            "real_f1 = 0.9017233348858873\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 254.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af65d6e0-97be-496c-a184-82893f038529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8338878154754639\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.06497710198163986\n",
            "step: 20, loss: 0.38273608684539795\n",
            "step: 30, loss: 0.37137657403945923\n",
            "step: 40, loss: 0.5108407735824585\n",
            "step: 50, loss: 0.3030611276626587\n",
            "step: 60, loss: 0.37115228176116943\n",
            "step: 70, loss: 0.2813487648963928\n",
            "step: 80, loss: 0.2790031135082245\n",
            "step: 90, loss: 0.4297909140586853\n",
            "step: 100, loss: 0.20846620202064514\n",
            "step: 110, loss: 0.3076065182685852\n",
            "step: 120, loss: 0.26456400752067566\n",
            "step: 130, loss: 0.21347135305404663\n",
            "step: 140, loss: 0.3191094398498535\n",
            "step: 150, loss: 0.3268556296825409\n",
            "step: 160, loss: 0.26260805130004883\n",
            "step: 170, loss: 0.18959330022335052\n",
            "step: 180, loss: 0.19118285179138184\n",
            "step: 190, loss: 0.2612443268299103\n",
            "step: 200, loss: 0.15704479813575745\n",
            "step: 210, loss: 0.47511595487594604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3824451410658307, f1=0.38880248833592534, best_f1=0.38880248833592534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09353360533714294\n",
            "step: 10, loss: 0.1322459727525711\n",
            "step: 20, loss: 0.30116644501686096\n",
            "step: 30, loss: 0.24740764498710632\n",
            "step: 40, loss: 0.12598305940628052\n",
            "step: 50, loss: 0.1700034886598587\n",
            "step: 60, loss: 0.11516140401363373\n",
            "step: 70, loss: 0.24956417083740234\n",
            "step: 80, loss: 0.28106510639190674\n",
            "step: 90, loss: 0.14641375839710236\n",
            "step: 100, loss: 0.15647655725479126\n",
            "step: 110, loss: 0.06928884238004684\n",
            "step: 120, loss: 0.24786721169948578\n",
            "step: 130, loss: 0.2297855019569397\n",
            "step: 140, loss: 0.28161945939064026\n",
            "step: 150, loss: 0.32324904203414917\n",
            "step: 160, loss: 0.17388340830802917\n",
            "step: 170, loss: 0.23023951053619385\n",
            "step: 180, loss: 0.44579562544822693\n",
            "step: 190, loss: 0.20656836032867432\n",
            "step: 200, loss: 0.22039370238780975\n",
            "step: 210, loss: 0.284611314535141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.49900990099009895, f1=0.49218750000000006, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29266121983528137\n",
            "step: 10, loss: 0.24167804419994354\n",
            "step: 20, loss: 0.32012367248535156\n",
            "step: 30, loss: 0.08076155930757523\n",
            "step: 40, loss: 0.1558769792318344\n",
            "step: 50, loss: 0.17722167074680328\n",
            "step: 60, loss: 0.2881155014038086\n",
            "step: 70, loss: 0.26495757699012756\n",
            "step: 80, loss: 0.15170399844646454\n",
            "step: 90, loss: 0.17096436023712158\n",
            "step: 100, loss: 0.14330120384693146\n",
            "step: 110, loss: 0.2530754804611206\n",
            "step: 120, loss: 0.2627081871032715\n",
            "step: 130, loss: 0.23430706560611725\n",
            "step: 140, loss: 0.2839588522911072\n",
            "step: 150, loss: 0.1559518426656723\n",
            "step: 160, loss: 0.09332317858934402\n",
            "step: 170, loss: 0.15985563397407532\n",
            "step: 180, loss: 0.08176379650831223\n",
            "step: 190, loss: 0.2081272155046463\n",
            "step: 200, loss: 0.19137969613075256\n",
            "step: 210, loss: 0.247709721326828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.46399999999999997, f1=0.48992248062015503, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19393259286880493\n",
            "step: 10, loss: 0.07184434682130814\n",
            "step: 20, loss: 0.14077900350093842\n",
            "step: 30, loss: 0.05794326588511467\n",
            "step: 40, loss: 0.0845976322889328\n",
            "step: 50, loss: 0.1028946116566658\n",
            "step: 60, loss: 0.08110840618610382\n",
            "step: 70, loss: 0.18115536868572235\n",
            "step: 80, loss: 0.11933949589729309\n",
            "step: 90, loss: 0.11839695274829865\n",
            "step: 100, loss: 0.2508111000061035\n",
            "step: 110, loss: 0.17835280299186707\n",
            "step: 120, loss: 0.07458696514368057\n",
            "step: 130, loss: 0.12932446599006653\n",
            "step: 140, loss: 0.20528781414031982\n",
            "step: 150, loss: 0.3058883845806122\n",
            "step: 160, loss: 0.12010950595140457\n",
            "step: 170, loss: 0.07084677368402481\n",
            "step: 180, loss: 0.16082356870174408\n",
            "step: 190, loss: 0.1550751030445099\n",
            "step: 200, loss: 0.30840063095092773\n",
            "step: 210, loss: 0.028456291183829308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.4892086330935252, f1=0.5045372050816697, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0319003164768219\n",
            "step: 10, loss: 0.024810025468468666\n",
            "step: 20, loss: 0.03419032692909241\n",
            "step: 30, loss: 0.052991751581430435\n",
            "step: 40, loss: 0.11496379226446152\n",
            "step: 50, loss: 0.10563930869102478\n",
            "step: 60, loss: 0.012383995577692986\n",
            "step: 70, loss: 0.05897487699985504\n",
            "step: 80, loss: 0.03536815196275711\n",
            "step: 90, loss: 0.032101113349199295\n",
            "step: 100, loss: 0.051147595047950745\n",
            "step: 110, loss: 0.011913386173546314\n",
            "step: 120, loss: 0.03212032467126846\n",
            "step: 130, loss: 0.09798366576433182\n",
            "step: 140, loss: 0.08156851679086685\n",
            "step: 150, loss: 0.06785580515861511\n",
            "step: 160, loss: 0.07603263854980469\n",
            "step: 170, loss: 0.16761212050914764\n",
            "step: 180, loss: 0.17829345166683197\n",
            "step: 190, loss: 0.035667866468429565\n",
            "step: 200, loss: 0.10190059244632721\n",
            "step: 210, loss: 0.10239781439304352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4794007490636704, f1=0.5057471264367815, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0782371535897255\n",
            "step: 10, loss: 0.010946298018097878\n",
            "step: 20, loss: 0.02598969079554081\n",
            "step: 30, loss: 0.09672939777374268\n",
            "step: 40, loss: 0.03216692432761192\n",
            "step: 50, loss: 0.08070214837789536\n",
            "step: 60, loss: 0.15275838971138\n",
            "step: 70, loss: 0.0017126903403550386\n",
            "step: 80, loss: 0.0703752413392067\n",
            "step: 90, loss: 0.08091215789318085\n",
            "step: 100, loss: 0.013807358220219612\n",
            "step: 110, loss: 0.018015142530202866\n",
            "step: 120, loss: 0.047368474304676056\n",
            "step: 130, loss: 0.01117868721485138\n",
            "step: 140, loss: 0.10730523616075516\n",
            "step: 150, loss: 0.12075506895780563\n",
            "step: 160, loss: 0.13373751938343048\n",
            "step: 170, loss: 0.08139638602733612\n",
            "step: 180, loss: 0.012852126732468605\n",
            "step: 190, loss: 0.07666759938001633\n",
            "step: 200, loss: 0.01204307284206152\n",
            "step: 210, loss: 0.04715175926685333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4387755102040817, f1=0.47750865051903113, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017000848427414894\n",
            "step: 10, loss: 0.027916939929127693\n",
            "step: 20, loss: 0.07492143660783768\n",
            "step: 30, loss: 0.005855211056768894\n",
            "step: 40, loss: 0.009085793979465961\n",
            "step: 50, loss: 0.040298763662576675\n",
            "step: 60, loss: 0.20512782037258148\n",
            "step: 70, loss: 0.007256664801388979\n",
            "step: 80, loss: 0.017021574079990387\n",
            "step: 90, loss: 0.0072993142530322075\n",
            "step: 100, loss: 0.04362857714295387\n",
            "step: 110, loss: 0.09218839555978775\n",
            "step: 120, loss: 0.007197810336947441\n",
            "step: 130, loss: 0.0029788960237056017\n",
            "step: 140, loss: 0.0008066437439993024\n",
            "step: 150, loss: 0.02458552084863186\n",
            "step: 160, loss: 0.04135756567120552\n",
            "step: 170, loss: 0.03155176341533661\n",
            "step: 180, loss: 0.004753591027110815\n",
            "step: 190, loss: 0.057393159717321396\n",
            "step: 200, loss: 0.04294731095433235\n",
            "step: 210, loss: 0.040976013988256454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.43963963963963965, f1=0.48888888888888893, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012109278701245785\n",
            "step: 10, loss: 0.011751762591302395\n",
            "step: 20, loss: 0.009209365583956242\n",
            "step: 30, loss: 0.0014716251753270626\n",
            "step: 40, loss: 0.030257536098361015\n",
            "step: 50, loss: 0.02277670055627823\n",
            "step: 60, loss: 0.23048779368400574\n",
            "step: 70, loss: 0.009943751618266106\n",
            "step: 80, loss: 0.04202204570174217\n",
            "step: 90, loss: 0.046513818204402924\n",
            "step: 100, loss: 0.012660510838031769\n",
            "step: 110, loss: 0.008888518437743187\n",
            "step: 120, loss: 0.006271565333008766\n",
            "step: 130, loss: 0.05151132494211197\n",
            "step: 140, loss: 0.007443216163665056\n",
            "step: 150, loss: 0.06714180111885071\n",
            "step: 160, loss: 0.11897037923336029\n",
            "step: 170, loss: 0.0009770413162186742\n",
            "step: 180, loss: 0.028074558824300766\n",
            "step: 190, loss: 0.02914891391992569\n",
            "step: 200, loss: 0.10696481913328171\n",
            "step: 210, loss: 0.07837813347578049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4572564612326043, f1=0.49896907216494846, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017119556665420532\n",
            "step: 10, loss: 0.003756521502509713\n",
            "step: 20, loss: 0.04297392815351486\n",
            "step: 30, loss: 0.030472377315163612\n",
            "step: 40, loss: 0.07798801362514496\n",
            "step: 50, loss: 0.007486653048545122\n",
            "step: 60, loss: 0.0010313253151252866\n",
            "step: 70, loss: 0.008587422780692577\n",
            "step: 80, loss: 0.0012304057599976659\n",
            "step: 90, loss: 0.10415980219841003\n",
            "step: 100, loss: 0.0032600234262645245\n",
            "step: 110, loss: 0.0007691835053265095\n",
            "step: 120, loss: 0.001676974119618535\n",
            "step: 130, loss: 0.003896548645570874\n",
            "step: 140, loss: 0.0077142538502812386\n",
            "step: 150, loss: 0.0617019459605217\n",
            "step: 160, loss: 0.00028995683533139527\n",
            "step: 170, loss: 0.0020947307348251343\n",
            "step: 180, loss: 0.015116715803742409\n",
            "step: 190, loss: 0.05891421437263489\n",
            "step: 200, loss: 0.01300676167011261\n",
            "step: 210, loss: 0.05510669946670532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.45783132530120485, f1=0.4696356275303644, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002767236903309822\n",
            "step: 10, loss: 0.004063886124640703\n",
            "step: 20, loss: 0.017100142315030098\n",
            "step: 30, loss: 0.003199910745024681\n",
            "step: 40, loss: 0.04819553345441818\n",
            "step: 50, loss: 0.006192728877067566\n",
            "step: 60, loss: 0.0005233351839706302\n",
            "step: 70, loss: 0.0016189905581995845\n",
            "step: 80, loss: 0.000508347584400326\n",
            "step: 90, loss: 0.006022964604198933\n",
            "step: 100, loss: 0.0008187335333786905\n",
            "step: 110, loss: 0.0028321873396635056\n",
            "step: 120, loss: 0.0008070819894783199\n",
            "step: 130, loss: 0.0008456273353658617\n",
            "step: 140, loss: 0.0002445665595587343\n",
            "step: 150, loss: 0.15087342262268066\n",
            "step: 160, loss: 0.017695432528853416\n",
            "step: 170, loss: 0.002236902480944991\n",
            "step: 180, loss: 0.0012027439661324024\n",
            "step: 190, loss: 0.019211288541555405\n",
            "step: 200, loss: 0.004555991850793362\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 210, loss: 0.004717202391475439\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4463894967177243, f1=0.43859649122807015, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02496843785047531\n",
            "step: 10, loss: 0.004682432860136032\n",
            "step: 20, loss: 0.06344481557607651\n",
            "step: 30, loss: 0.03850361332297325\n",
            "step: 40, loss: 0.005555060226470232\n",
            "step: 50, loss: 0.0006960831233300269\n",
            "step: 60, loss: 0.00028670954634435475\n",
            "step: 70, loss: 0.002888414543122053\n",
            "step: 80, loss: 0.09255292266607285\n",
            "step: 90, loss: 0.0010166139109060168\n",
            "step: 100, loss: 0.0009217164479196072\n",
            "step: 110, loss: 0.003042178228497505\n",
            "step: 120, loss: 0.002161058597266674\n",
            "step: 130, loss: 0.03051399067044258\n",
            "step: 140, loss: 0.048966072499752045\n",
            "step: 150, loss: 0.005869658198207617\n",
            "step: 160, loss: 0.0014246719656512141\n",
            "step: 170, loss: 0.1310221552848816\n",
            "step: 180, loss: 0.0035930778831243515\n",
            "step: 190, loss: 0.0046998606994748116\n",
            "step: 200, loss: 0.00021208437101449817\n",
            "step: 210, loss: 0.0005235841381363571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4375, f1=0.4403292181069959, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00028210721211507916\n",
            "step: 10, loss: 0.0012637312756851315\n",
            "step: 20, loss: 0.011408167891204357\n",
            "step: 30, loss: 0.02085551992058754\n",
            "step: 40, loss: 0.05751106142997742\n",
            "step: 50, loss: 0.0010013204300776124\n",
            "step: 60, loss: 0.009316311217844486\n",
            "step: 70, loss: 0.0012404231820255518\n",
            "step: 80, loss: 0.0006599728367291391\n",
            "step: 90, loss: 0.0004346598288975656\n",
            "step: 100, loss: 0.00018284816178493202\n",
            "step: 110, loss: 0.0011865299893543124\n",
            "step: 120, loss: 0.0036112440284341574\n",
            "step: 130, loss: 0.001374050509184599\n",
            "step: 140, loss: 0.0012604845687747002\n",
            "step: 150, loss: 0.00017267794464714825\n",
            "step: 160, loss: 0.0008969481568783522\n",
            "step: 170, loss: 0.00033243693178519607\n",
            "step: 180, loss: 0.005259924568235874\n",
            "step: 190, loss: 0.0006858069682493806\n",
            "step: 200, loss: 0.0011195304105058312\n",
            "step: 210, loss: 0.002078506164252758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.3981264637002342, f1=0.4232558139534883, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.839755562599748e-05\n",
            "step: 10, loss: 0.000406853825552389\n",
            "step: 20, loss: 0.00027645481168292463\n",
            "step: 30, loss: 0.0005868393345735967\n",
            "step: 40, loss: 0.0007998545770533383\n",
            "step: 50, loss: 0.003176879370585084\n",
            "step: 60, loss: 0.00015675624308642\n",
            "step: 70, loss: 0.048018135130405426\n",
            "step: 80, loss: 0.00021798143279738724\n",
            "step: 90, loss: 0.03358355909585953\n",
            "step: 100, loss: 0.052028853446245193\n",
            "step: 110, loss: 0.000436162983532995\n",
            "step: 120, loss: 0.0006057150894775987\n",
            "step: 130, loss: 0.000510345445945859\n",
            "step: 140, loss: 0.13916811347007751\n",
            "step: 150, loss: 0.09164286404848099\n",
            "step: 160, loss: 0.002156950067728758\n",
            "step: 170, loss: 0.0002371268637944013\n",
            "step: 180, loss: 0.0004598866798914969\n",
            "step: 190, loss: 0.00032123998971655965\n",
            "step: 200, loss: 0.00039617778384126723\n",
            "step: 210, loss: 0.0016223834827542305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.43933054393305443, f1=0.4458333333333333, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004745836369693279\n",
            "step: 10, loss: 0.0009157445165328681\n",
            "step: 20, loss: 0.002135121962055564\n",
            "step: 30, loss: 0.006142637692391872\n",
            "step: 40, loss: 0.0016895137960091233\n",
            "step: 50, loss: 0.0009162202477455139\n",
            "step: 60, loss: 0.004152150359004736\n",
            "step: 70, loss: 0.0011472722981125116\n",
            "step: 80, loss: 0.0019432558910921216\n",
            "step: 90, loss: 0.0008832734310999513\n",
            "step: 100, loss: 0.00024872971698641777\n",
            "step: 110, loss: 0.0005269584944471717\n",
            "step: 120, loss: 0.0006313372869044542\n",
            "step: 130, loss: 0.0005839674849994481\n",
            "step: 140, loss: 0.0014074043137952685\n",
            "step: 150, loss: 0.0006458775606006384\n",
            "step: 160, loss: 0.01908164471387863\n",
            "step: 170, loss: 0.01376801822334528\n",
            "step: 180, loss: 0.0003160179185215384\n",
            "step: 190, loss: 0.00011537795217009261\n",
            "step: 200, loss: 0.0002078519610222429\n",
            "step: 210, loss: 0.0004948569112457335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.438177874186551, f1=0.44347826086956516, best_f1=0.49218750000000006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005284194485284388\n",
            "step: 10, loss: 0.0004778845759574324\n",
            "step: 20, loss: 0.013378809206187725\n",
            "step: 30, loss: 0.0005869587766937912\n",
            "step: 40, loss: 0.00022351212101057172\n",
            "step: 50, loss: 0.00010028070391854271\n",
            "step: 60, loss: 0.0002727008832152933\n",
            "step: 70, loss: 0.0004395965952426195\n",
            "step: 80, loss: 0.00026717365835793316\n",
            "step: 90, loss: 0.00041518916259519756\n",
            "step: 100, loss: 0.00011150990758324042\n",
            "step: 110, loss: 0.00013876281445845962\n",
            "step: 120, loss: 0.0006130276597104967\n",
            "step: 130, loss: 0.016525575891137123\n",
            "step: 140, loss: 0.000304974993923679\n",
            "step: 150, loss: 0.004055396653711796\n",
            "step: 160, loss: 0.0036864825524389744\n",
            "step: 170, loss: 0.1351076364517212\n",
            "step: 180, loss: 0.024800432845950127\n",
            "step: 190, loss: 0.00043292134068906307\n",
            "step: 200, loss: 0.006173199974000454\n",
            "step: 210, loss: 0.0071951416321098804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.4298245614035088, f1=0.4410480349344978, best_f1=0.49218750000000006\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:06, 330.55it/s]\n",
            "load_f1 = 0.4724061810154525\n",
            "real_f1 = 0.4794816414686825\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 256.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec8994c-7129-4929-b5ab-9b2514f0a13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8501303791999817\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.16457441449165344\n",
            "step: 20, loss: 0.15011945366859436\n",
            "step: 30, loss: 0.5152413249015808\n",
            "step: 40, loss: 0.26419398188591003\n",
            "step: 50, loss: 0.3114648163318634\n",
            "step: 60, loss: 0.36679893732070923\n",
            "step: 70, loss: 0.17469274997711182\n",
            "step: 80, loss: 0.5372204780578613\n",
            "step: 90, loss: 0.25185471773147583\n",
            "step: 100, loss: 0.22086888551712036\n",
            "step: 110, loss: 0.23496046662330627\n",
            "step: 120, loss: 0.42458176612854004\n",
            "step: 130, loss: 0.34671908617019653\n",
            "step: 140, loss: 0.3412063717842102\n",
            "step: 150, loss: 0.2744591236114502\n",
            "step: 160, loss: 0.20913326740264893\n",
            "step: 170, loss: 0.40872883796691895\n",
            "step: 180, loss: 0.29683220386505127\n",
            "step: 190, loss: 0.16032689809799194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.46607669616519176, f1=0.44642857142857145, best_f1=0.44642857142857145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2929524779319763\n",
            "step: 10, loss: 0.03338158503174782\n",
            "step: 20, loss: 0.09596224874258041\n",
            "step: 30, loss: 0.19741125404834747\n",
            "step: 40, loss: 0.48275476694107056\n",
            "step: 50, loss: 0.2826204299926758\n",
            "step: 60, loss: 0.17881637811660767\n",
            "step: 70, loss: 0.2030232846736908\n",
            "step: 80, loss: 0.14457711577415466\n",
            "step: 90, loss: 0.2023307979106903\n",
            "step: 100, loss: 0.19528931379318237\n",
            "step: 110, loss: 0.2497977316379547\n",
            "step: 120, loss: 0.3456871211528778\n",
            "step: 130, loss: 0.18758489191532135\n",
            "step: 140, loss: 0.3711357116699219\n",
            "step: 150, loss: 0.08208344131708145\n",
            "step: 160, loss: 0.1594131737947464\n",
            "step: 170, loss: 0.40555474162101746\n",
            "step: 180, loss: 0.18158946931362152\n",
            "step: 190, loss: 0.3806755542755127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6887755102040816, f1=0.6851385390428211, best_f1=0.6851385390428211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18198277056217194\n",
            "step: 10, loss: 0.24562516808509827\n",
            "step: 20, loss: 0.03024105727672577\n",
            "step: 30, loss: 0.040213149040937424\n",
            "step: 40, loss: 0.10798300802707672\n",
            "step: 50, loss: 0.09596499055624008\n",
            "step: 60, loss: 0.06858957558870316\n",
            "step: 70, loss: 0.24957801401615143\n",
            "step: 80, loss: 0.19484186172485352\n",
            "step: 90, loss: 0.05160617455840111\n",
            "step: 100, loss: 0.17634990811347961\n",
            "step: 110, loss: 0.29813483357429504\n",
            "step: 120, loss: 0.05362170562148094\n",
            "step: 130, loss: 0.014036871492862701\n",
            "step: 140, loss: 0.2235841006040573\n",
            "step: 150, loss: 0.13746429979801178\n",
            "step: 160, loss: 0.15505777299404144\n",
            "step: 170, loss: 0.08898341655731201\n",
            "step: 180, loss: 0.12184399366378784\n",
            "step: 190, loss: 0.296196311712265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6756756756756755, f1=0.6869806094182824, best_f1=0.6851385390428211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08030256628990173\n",
            "step: 10, loss: 0.112972192466259\n",
            "step: 20, loss: 0.03520125150680542\n",
            "step: 30, loss: 0.06889086961746216\n",
            "step: 40, loss: 0.01866157352924347\n",
            "step: 50, loss: 0.11849835515022278\n",
            "step: 60, loss: 0.0540640652179718\n",
            "step: 70, loss: 0.00807055551558733\n",
            "step: 80, loss: 0.13333959877490997\n",
            "step: 90, loss: 0.09020942449569702\n",
            "step: 100, loss: 0.04741621017456055\n",
            "step: 110, loss: 0.17505383491516113\n",
            "step: 120, loss: 0.1322336196899414\n",
            "step: 130, loss: 0.14680026471614838\n",
            "step: 140, loss: 0.24969813227653503\n",
            "step: 150, loss: 0.04344376176595688\n",
            "step: 160, loss: 0.08656367659568787\n",
            "step: 170, loss: 0.049486372619867325\n",
            "step: 180, loss: 0.029279150068759918\n",
            "step: 190, loss: 0.030873417854309082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7394957983193278, f1=0.7570621468926554, best_f1=0.7570621468926554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02962687239050865\n",
            "step: 10, loss: 0.015201007016003132\n",
            "step: 20, loss: 0.057462360709905624\n",
            "step: 30, loss: 0.006080050021409988\n",
            "step: 40, loss: 0.040348730981349945\n",
            "step: 50, loss: 0.23095746338367462\n",
            "step: 60, loss: 0.01221542339771986\n",
            "step: 70, loss: 0.013690273277461529\n",
            "step: 80, loss: 0.009432127699255943\n",
            "step: 90, loss: 0.028031444177031517\n",
            "step: 100, loss: 0.01510606985539198\n",
            "step: 110, loss: 0.031073568388819695\n",
            "step: 120, loss: 0.002388094086199999\n",
            "step: 130, loss: 0.0499250628054142\n",
            "step: 140, loss: 0.004723282530903816\n",
            "step: 150, loss: 0.022283386439085007\n",
            "step: 160, loss: 0.027859121561050415\n",
            "step: 170, loss: 0.021117620170116425\n",
            "step: 180, loss: 0.014834470115602016\n",
            "step: 190, loss: 0.14805862307548523\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7287671232876712, f1=0.7401129943502824, best_f1=0.7570621468926554\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015247436240315437\n",
            "step: 10, loss: 0.002449555555358529\n",
            "step: 20, loss: 0.009459571912884712\n",
            "step: 30, loss: 0.07883941382169724\n",
            "step: 40, loss: 0.035573143512010574\n",
            "step: 50, loss: 0.002863406203687191\n",
            "step: 60, loss: 0.0010418544989079237\n",
            "step: 70, loss: 0.007604395039379597\n",
            "step: 80, loss: 0.0038354541175067425\n",
            "step: 90, loss: 0.008564746007323265\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.002556223887950182\n",
            "step: 110, loss: 0.1005745381116867\n",
            "step: 120, loss: 0.007923168130218983\n",
            "step: 130, loss: 0.012227599509060383\n",
            "step: 140, loss: 0.044581376016139984\n",
            "step: 150, loss: 0.04061779007315636\n",
            "step: 160, loss: 0.008737461641430855\n",
            "step: 170, loss: 0.003198132151737809\n",
            "step: 180, loss: 0.005369626916944981\n",
            "step: 190, loss: 0.008232822641730309\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7466666666666666, f1=0.7382920110192837, best_f1=0.7382920110192837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005575879476964474\n",
            "step: 10, loss: 0.06452945619821548\n",
            "step: 20, loss: 0.0019148363498970866\n",
            "step: 30, loss: 0.003408627351745963\n",
            "step: 40, loss: 0.0017343501094728708\n",
            "step: 50, loss: 0.019614117220044136\n",
            "step: 60, loss: 0.005055124405771494\n",
            "step: 70, loss: 0.0008362550870515406\n",
            "step: 80, loss: 0.10689535737037659\n",
            "step: 90, loss: 0.0013905455125495791\n",
            "step: 100, loss: 0.0048832399770617485\n",
            "step: 110, loss: 0.0037929590325802565\n",
            "step: 120, loss: 0.0008615341503173113\n",
            "step: 130, loss: 0.00310566951520741\n",
            "step: 140, loss: 0.018750814720988274\n",
            "step: 150, loss: 0.13435310125350952\n",
            "step: 160, loss: 0.14202678203582764\n",
            "step: 170, loss: 0.0303681381046772\n",
            "step: 180, loss: 0.05371781438589096\n",
            "step: 190, loss: 0.02919442392885685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7258485639686684, f1=0.7282913165266107, best_f1=0.7382920110192837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04410267993807793\n",
            "step: 10, loss: 0.0018289712024852633\n",
            "step: 20, loss: 0.0008801182266324759\n",
            "step: 30, loss: 0.0011837235651910305\n",
            "step: 40, loss: 0.004667059518396854\n",
            "step: 50, loss: 0.0011678938753902912\n",
            "step: 60, loss: 0.00047637338866479695\n",
            "step: 70, loss: 0.1003463864326477\n",
            "step: 80, loss: 0.005552257876843214\n",
            "step: 90, loss: 0.029183151200413704\n",
            "step: 100, loss: 0.0024894699454307556\n",
            "step: 110, loss: 0.0017817409243434668\n",
            "step: 120, loss: 0.002220110734924674\n",
            "step: 130, loss: 0.0026734662242233753\n",
            "step: 140, loss: 0.0027100478764623404\n",
            "step: 150, loss: 0.002146783284842968\n",
            "step: 160, loss: 0.0019079847261309624\n",
            "step: 170, loss: 0.0003312284534331411\n",
            "step: 180, loss: 0.0012188247637823224\n",
            "step: 190, loss: 0.08415098488330841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7631578947368421, f1=0.7446808510638299, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012074830010533333\n",
            "step: 10, loss: 0.0008768626139499247\n",
            "step: 20, loss: 0.008024855516850948\n",
            "step: 30, loss: 0.007743341848254204\n",
            "step: 40, loss: 0.010013674385845661\n",
            "step: 50, loss: 0.004992590751498938\n",
            "step: 60, loss: 0.001959201879799366\n",
            "step: 70, loss: 0.00086536246817559\n",
            "step: 80, loss: 0.011450227349996567\n",
            "step: 90, loss: 0.006135708652436733\n",
            "step: 100, loss: 0.0003346211451571435\n",
            "step: 110, loss: 0.01655171625316143\n",
            "step: 120, loss: 0.021694371476769447\n",
            "step: 130, loss: 0.00041832009446807206\n",
            "step: 140, loss: 0.0002577774866949767\n",
            "step: 150, loss: 0.0008210456580854952\n",
            "step: 160, loss: 0.0007759936270304024\n",
            "step: 170, loss: 0.0030000482220202684\n",
            "step: 180, loss: 0.0007967049605213106\n",
            "step: 190, loss: 0.0007106431294232607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7534626038781163, f1=0.7283236994219653, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005833299946971238\n",
            "step: 10, loss: 0.0013589084846898913\n",
            "step: 20, loss: 0.00023089365276973695\n",
            "step: 30, loss: 0.00041646271711215377\n",
            "step: 40, loss: 0.0005108599434606731\n",
            "step: 50, loss: 0.00039365599513985217\n",
            "step: 60, loss: 0.0011693225242197514\n",
            "step: 70, loss: 0.0007585844723507762\n",
            "step: 80, loss: 0.0002617575810290873\n",
            "step: 90, loss: 0.00046873881365172565\n",
            "step: 100, loss: 0.02462589740753174\n",
            "step: 110, loss: 0.0009187896503135562\n",
            "step: 120, loss: 0.002104537794366479\n",
            "step: 130, loss: 0.00049576599849388\n",
            "step: 140, loss: 0.00044140798854641616\n",
            "step: 150, loss: 0.00017295530415140092\n",
            "step: 160, loss: 0.002144442405551672\n",
            "step: 170, loss: 0.0004511508741416037\n",
            "step: 180, loss: 0.013126925565302372\n",
            "step: 190, loss: 0.005616732407361269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7582697201017812, f1=0.7139240506329113, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017736364679876715\n",
            "step: 10, loss: 0.17119188606739044\n",
            "step: 20, loss: 0.005182386375963688\n",
            "step: 30, loss: 0.22534029185771942\n",
            "step: 40, loss: 0.00032808739342726767\n",
            "step: 50, loss: 0.00027893518563359976\n",
            "step: 60, loss: 0.0019874584395438433\n",
            "step: 70, loss: 0.00021118843869771808\n",
            "step: 80, loss: 0.0002134142123395577\n",
            "step: 90, loss: 0.00045746820978820324\n",
            "step: 100, loss: 0.00014450427261181176\n",
            "step: 110, loss: 0.00012320144742261618\n",
            "step: 120, loss: 0.00020628572383429855\n",
            "step: 130, loss: 0.006764700636267662\n",
            "step: 140, loss: 0.0002100891142617911\n",
            "step: 150, loss: 0.00017483592091593891\n",
            "step: 160, loss: 0.0001237048563780263\n",
            "step: 170, loss: 0.0015585148939862847\n",
            "step: 180, loss: 0.08407895267009735\n",
            "step: 190, loss: 0.000168861573911272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7601078167115903, f1=0.7380281690140845, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000392377027310431\n",
            "step: 10, loss: 0.00032634378294460475\n",
            "step: 20, loss: 0.0002812123275361955\n",
            "step: 30, loss: 0.0003506412322167307\n",
            "step: 40, loss: 0.002463867422193289\n",
            "step: 50, loss: 0.0014942586421966553\n",
            "step: 60, loss: 0.00011763503425754607\n",
            "step: 70, loss: 0.016426289454102516\n",
            "step: 80, loss: 0.0014015280175954103\n",
            "step: 90, loss: 0.00025811957311816514\n",
            "step: 100, loss: 0.0002242983173346147\n",
            "step: 110, loss: 0.00017548890900798142\n",
            "step: 120, loss: 0.000393608002923429\n",
            "step: 130, loss: 0.018696531653404236\n",
            "step: 140, loss: 0.0004513626918196678\n",
            "step: 150, loss: 0.0007336662383750081\n",
            "step: 160, loss: 0.10172964632511139\n",
            "step: 170, loss: 0.0007064375677146018\n",
            "step: 180, loss: 0.004285057540982962\n",
            "step: 190, loss: 0.08618733286857605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7474226804123711, f1=0.6894736842105263, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021616432059090585\n",
            "step: 10, loss: 0.0002875989302992821\n",
            "step: 20, loss: 0.0029409106355160475\n",
            "step: 30, loss: 0.0006583961076103151\n",
            "step: 40, loss: 0.00019860915199387819\n",
            "step: 50, loss: 0.0011576834367588162\n",
            "step: 60, loss: 0.12132178992033005\n",
            "step: 70, loss: 0.0010691058123484254\n",
            "step: 80, loss: 0.0005015954957343638\n",
            "step: 90, loss: 0.008172261528670788\n",
            "step: 100, loss: 0.0003203694068361074\n",
            "step: 110, loss: 0.0008024502894841135\n",
            "step: 120, loss: 0.0003608637198340148\n",
            "step: 130, loss: 0.00041528386645950377\n",
            "step: 140, loss: 0.00021954749536234885\n",
            "step: 150, loss: 0.022059958428144455\n",
            "step: 160, loss: 0.00023070434690453112\n",
            "step: 170, loss: 9.903834870783612e-05\n",
            "step: 180, loss: 0.002306752372533083\n",
            "step: 190, loss: 0.002620298881083727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7596899224806202, f1=0.7336956521739131, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020216903067193925\n",
            "step: 10, loss: 0.18171446025371552\n",
            "step: 20, loss: 0.018644705414772034\n",
            "step: 30, loss: 0.00018798747623804957\n",
            "step: 40, loss: 0.0006468945648521185\n",
            "step: 50, loss: 0.00871792808175087\n",
            "step: 60, loss: 0.003233496565371752\n",
            "step: 70, loss: 0.0008173327660188079\n",
            "step: 80, loss: 0.00018529528460931033\n",
            "step: 90, loss: 0.0007344318437390029\n",
            "step: 100, loss: 0.00037060430622659624\n",
            "step: 110, loss: 0.0004582289548125118\n",
            "step: 120, loss: 0.0003574866568669677\n",
            "step: 130, loss: 0.0002725672966334969\n",
            "step: 140, loss: 0.0011874830815941095\n",
            "step: 150, loss: 0.0006131020491011441\n",
            "step: 160, loss: 0.0010833428241312504\n",
            "step: 170, loss: 0.0008733998402021825\n",
            "step: 180, loss: 0.00023690774105489254\n",
            "step: 190, loss: 0.00020079004752915353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7500000000000001, f1=0.7252124645892353, best_f1=0.7446808510638299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00846523605287075\n",
            "step: 10, loss: 0.004360022023320198\n",
            "step: 20, loss: 0.00019061268540099263\n",
            "step: 30, loss: 0.0009062004974111915\n",
            "step: 40, loss: 0.0007816830184310675\n",
            "step: 50, loss: 0.0004256390093360096\n",
            "step: 60, loss: 0.00023646077897865325\n",
            "step: 70, loss: 0.0006501839379779994\n",
            "step: 80, loss: 0.00045909604523330927\n",
            "step: 90, loss: 0.01877480000257492\n",
            "step: 100, loss: 0.001289099338464439\n",
            "step: 110, loss: 0.0006329793250188231\n",
            "step: 120, loss: 0.001638914691284299\n",
            "step: 130, loss: 0.00048436364158988\n",
            "step: 140, loss: 0.00135442812461406\n",
            "step: 150, loss: 0.015848658978939056\n",
            "step: 160, loss: 0.0021858224645256996\n",
            "step: 170, loss: 0.0006092911935411394\n",
            "step: 180, loss: 0.00023347255773842335\n",
            "step: 190, loss: 0.0011273128911852837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7520435967302452, f1=0.7252124645892353, best_f1=0.7446808510638299\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:09, 223.34it/s]\n",
            "load_f1 = 0.6980609418282547\n",
            "real_f1 = 0.6881720430107526\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 253.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4bb061-8c17-481a-c254-cebf35f99466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8447359204292297\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.2185119241476059\n",
            "step: 20, loss: 0.14797595143318176\n",
            "step: 30, loss: 0.254518061876297\n",
            "step: 40, loss: 0.3096427619457245\n",
            "step: 50, loss: 0.3834676146507263\n",
            "step: 60, loss: 0.4480210542678833\n",
            "step: 70, loss: 0.30818867683410645\n",
            "step: 80, loss: 0.25844940543174744\n",
            "step: 90, loss: 0.41776207089424133\n",
            "step: 100, loss: 0.2432185709476471\n",
            "step: 110, loss: 0.18405435979366302\n",
            "step: 120, loss: 0.5170094966888428\n",
            "step: 130, loss: 0.41420212388038635\n",
            "step: 140, loss: 0.4833517372608185\n",
            "step: 150, loss: 0.09051317721605301\n",
            "step: 160, loss: 0.34557321667671204\n",
            "step: 170, loss: 0.24815770983695984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2776349614395887, f1=0.26450742240215924, best_f1=0.26450742240215924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4187040328979492\n",
            "step: 10, loss: 0.2355216145515442\n",
            "step: 20, loss: 0.3476466238498688\n",
            "step: 30, loss: 0.27740219235420227\n",
            "step: 40, loss: 0.21709808707237244\n",
            "step: 50, loss: 0.266499787569046\n",
            "step: 60, loss: 0.0913069024682045\n",
            "step: 70, loss: 0.29338905215263367\n",
            "step: 80, loss: 0.1806657612323761\n",
            "step: 90, loss: 0.20741334557533264\n",
            "step: 100, loss: 0.16699238121509552\n",
            "step: 110, loss: 0.2921850383281708\n",
            "step: 120, loss: 0.10747256129980087\n",
            "step: 130, loss: 0.08211599290370941\n",
            "step: 140, loss: 0.2411159873008728\n",
            "step: 150, loss: 0.16573230922222137\n",
            "step: 160, loss: 0.1318647861480713\n",
            "step: 170, loss: 0.15133054554462433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6807511737089201, f1=0.6477541371158393, best_f1=0.6477541371158393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08227738738059998\n",
            "step: 10, loss: 0.11967489123344421\n",
            "step: 20, loss: 0.035037148743867874\n",
            "step: 30, loss: 0.19234313070774078\n",
            "step: 40, loss: 0.015792574733495712\n",
            "step: 50, loss: 0.18659977614879608\n",
            "step: 60, loss: 0.21205027401447296\n",
            "step: 70, loss: 0.0787908211350441\n",
            "step: 80, loss: 0.11277937144041061\n",
            "step: 90, loss: 0.13038720190525055\n",
            "step: 100, loss: 0.06557846814393997\n",
            "step: 110, loss: 0.030648859217762947\n",
            "step: 120, loss: 0.08743561059236526\n",
            "step: 130, loss: 0.1642531305551529\n",
            "step: 140, loss: 0.012613757513463497\n",
            "step: 150, loss: 0.04712966829538345\n",
            "step: 160, loss: 0.07696522772312164\n",
            "step: 170, loss: 0.14678062498569489\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7358916478555305, f1=0.688034188034188, best_f1=0.688034188034188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05724490061402321\n",
            "step: 10, loss: 0.11313051730394363\n",
            "step: 20, loss: 0.16729985177516937\n",
            "step: 30, loss: 0.1450107842683792\n",
            "step: 40, loss: 0.0802384614944458\n",
            "step: 50, loss: 0.030629783868789673\n",
            "step: 60, loss: 0.20966660976409912\n",
            "step: 70, loss: 0.010577344335615635\n",
            "step: 80, loss: 0.058273326605558395\n",
            "step: 90, loss: 0.06704433262348175\n",
            "step: 100, loss: 0.08968811482191086\n",
            "step: 110, loss: 0.057265400886535645\n",
            "step: 120, loss: 0.15664426982402802\n",
            "step: 130, loss: 0.07723170518875122\n",
            "step: 140, loss: 0.016189904883503914\n",
            "step: 150, loss: 0.06400885432958603\n",
            "step: 160, loss: 0.07886112481355667\n",
            "step: 170, loss: 0.025600584223866463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7487922705314008, f1=0.6713615023474179, best_f1=0.6713615023474179\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03159167617559433\n",
            "step: 10, loss: 0.07198892533779144\n",
            "step: 20, loss: 0.021870525553822517\n",
            "step: 30, loss: 0.09206955134868622\n",
            "step: 40, loss: 0.014060646295547485\n",
            "step: 50, loss: 0.0550447553396225\n",
            "step: 60, loss: 0.026114333420991898\n",
            "step: 70, loss: 0.05247151479125023\n",
            "step: 80, loss: 0.017189528793096542\n",
            "step: 90, loss: 0.012787071987986565\n",
            "step: 100, loss: 0.043214816600084305\n",
            "step: 110, loss: 0.1063096672296524\n",
            "step: 120, loss: 0.06630115956068039\n",
            "step: 130, loss: 0.020469743758440018\n",
            "step: 140, loss: 0.14778928458690643\n",
            "step: 150, loss: 0.0369558110833168\n",
            "step: 160, loss: 0.0588616281747818\n",
            "step: 170, loss: 0.06312073767185211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7518796992481204, f1=0.6960784313725491, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01402944978326559\n",
            "step: 10, loss: 0.17978280782699585\n",
            "step: 20, loss: 0.042615655809640884\n",
            "step: 30, loss: 0.02655099518597126\n",
            "step: 40, loss: 0.009567268192768097\n",
            "step: 50, loss: 0.05693545192480087\n",
            "step: 60, loss: 0.005065268836915493\n",
            "step: 70, loss: 0.0355156846344471\n",
            "step: 80, loss: 0.02570047415792942\n",
            "step: 90, loss: 0.013395008631050587\n",
            "step: 100, loss: 0.048367343842983246\n",
            "step: 110, loss: 0.16989922523498535\n",
            "step: 120, loss: 0.09450266510248184\n",
            "step: 130, loss: 0.1634443998336792\n",
            "step: 140, loss: 0.030086426064372063\n",
            "step: 150, loss: 0.27986854314804077\n",
            "step: 160, loss: 0.0314653106033802\n",
            "step: 170, loss: 0.007258689031004906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7335092348284961, f1=0.6917293233082706, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005253925919532776\n",
            "step: 10, loss: 0.0016549829160794616\n",
            "step: 20, loss: 0.009080762974917889\n",
            "step: 30, loss: 0.23545366525650024\n",
            "step: 40, loss: 0.017851104959845543\n",
            "step: 50, loss: 0.004385174252092838\n",
            "step: 60, loss: 0.2100452333688736\n",
            "step: 70, loss: 0.025138461962342262\n",
            "step: 80, loss: 0.0035359819885343313\n",
            "step: 90, loss: 0.04058254882693291\n",
            "step: 100, loss: 0.014467120170593262\n",
            "step: 110, loss: 0.001643993309698999\n",
            "step: 120, loss: 0.2358594834804535\n",
            "step: 130, loss: 0.03332772105932236\n",
            "step: 140, loss: 0.016141606494784355\n",
            "step: 150, loss: 0.016263503581285477\n",
            "step: 160, loss: 0.003666343865916133\n",
            "step: 170, loss: 0.2586556375026703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7146529562982005, f1=0.6748166259168704, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08883526921272278\n",
            "step: 10, loss: 0.0023299623280763626\n",
            "step: 20, loss: 0.014226030558347702\n",
            "step: 30, loss: 0.010798102244734764\n",
            "step: 40, loss: 0.002858178224414587\n",
            "step: 50, loss: 0.001798250712454319\n",
            "step: 60, loss: 0.009427161887288094\n",
            "step: 70, loss: 0.028828119859099388\n",
            "step: 80, loss: 0.0012873687082901597\n",
            "step: 90, loss: 0.012660348787903786\n",
            "step: 100, loss: 0.024538137018680573\n",
            "step: 110, loss: 0.06472413241863251\n",
            "step: 120, loss: 0.003783557331189513\n",
            "step: 130, loss: 0.04616234451532364\n",
            "step: 140, loss: 0.0031982585787773132\n",
            "step: 150, loss: 0.05931400880217552\n",
            "step: 160, loss: 0.10305511951446533\n",
            "step: 170, loss: 0.009534968994557858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7466666666666667, f1=0.7080103359173127, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005689740180969238\n",
            "step: 10, loss: 0.008867527358233929\n",
            "step: 20, loss: 0.0006725778221152723\n",
            "step: 30, loss: 0.009344722144305706\n",
            "step: 40, loss: 0.009246100671589375\n",
            "step: 50, loss: 0.056673843413591385\n",
            "step: 60, loss: 0.0008362063090316951\n",
            "step: 70, loss: 0.027615156024694443\n",
            "step: 80, loss: 0.00181251869071275\n",
            "step: 90, loss: 0.008497681468725204\n",
            "step: 100, loss: 0.009970703162252903\n",
            "step: 110, loss: 0.00024480969295836985\n",
            "step: 120, loss: 0.02205859310925007\n",
            "step: 130, loss: 0.0024690169375389814\n",
            "step: 140, loss: 0.004968215245753527\n",
            "step: 150, loss: 0.008532688021659851\n",
            "step: 160, loss: 0.057234469801187515\n",
            "step: 170, loss: 0.03261983394622803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7287234042553191, f1=0.695214105793451, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011591549264267087\n",
            "step: 10, loss: 0.02058831974864006\n",
            "step: 20, loss: 0.012613259255886078\n",
            "step: 30, loss: 0.0005742716602981091\n",
            "step: 40, loss: 0.0015140559989959002\n",
            "step: 50, loss: 0.0018933021929115057\n",
            "step: 60, loss: 0.001786844339221716\n",
            "step: 70, loss: 0.002769528655335307\n",
            "step: 80, loss: 0.001460416940972209\n",
            "step: 90, loss: 0.0850272923707962\n",
            "step: 100, loss: 0.06370753049850464\n",
            "step: 110, loss: 0.002375712152570486\n",
            "step: 120, loss: 0.04404887557029724\n",
            "step: 130, loss: 0.02945888228714466\n",
            "step: 140, loss: 0.01383874099701643\n",
            "step: 150, loss: 0.006711026653647423\n",
            "step: 160, loss: 0.000524491595569998\n",
            "step: 170, loss: 0.001337959198281169\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7307692307692308, f1=0.6824146981627297, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006746025756001472\n",
            "step: 10, loss: 0.042617328464984894\n",
            "step: 20, loss: 0.06779395788908005\n",
            "step: 30, loss: 0.0027401375118643045\n",
            "step: 40, loss: 0.018489159643650055\n",
            "step: 50, loss: 0.002013358986005187\n",
            "step: 60, loss: 0.034197140485048294\n",
            "step: 70, loss: 0.004388872999697924\n",
            "step: 80, loss: 0.0009289636509492993\n",
            "step: 90, loss: 0.002460867166519165\n",
            "step: 100, loss: 0.0003470202209427953\n",
            "step: 110, loss: 0.0005117470864206553\n",
            "step: 120, loss: 0.01779281534254551\n",
            "step: 130, loss: 0.013793845660984516\n",
            "step: 140, loss: 0.028088729828596115\n",
            "step: 150, loss: 0.004704730585217476\n",
            "step: 160, loss: 0.0015957854920998216\n",
            "step: 170, loss: 0.007745022885501385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7262872628726287, f1=0.690537084398977, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00042614215635694563\n",
            "step: 10, loss: 0.0016734987730160356\n",
            "step: 20, loss: 0.0025803218595683575\n",
            "step: 30, loss: 0.00033528235508129\n",
            "step: 40, loss: 0.0005769909475930035\n",
            "step: 50, loss: 0.001741513260640204\n",
            "step: 60, loss: 0.0003951621474698186\n",
            "step: 70, loss: 0.004217409063130617\n",
            "step: 80, loss: 0.009365440346300602\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.00022400241869036108\n",
            "step: 100, loss: 0.00044264638563618064\n",
            "step: 110, loss: 0.0006552557460963726\n",
            "step: 120, loss: 0.005560798104852438\n",
            "step: 130, loss: 0.007436810992658138\n",
            "step: 140, loss: 0.0007070120191201568\n",
            "step: 150, loss: 0.006435503251850605\n",
            "step: 160, loss: 0.000930282985791564\n",
            "step: 170, loss: 0.003183159278705716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.723514211886305, f1=0.6828087167070218, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003580857301130891\n",
            "step: 10, loss: 0.04988064244389534\n",
            "step: 20, loss: 0.01530646625906229\n",
            "step: 30, loss: 0.005301825236529112\n",
            "step: 40, loss: 0.01083078607916832\n",
            "step: 50, loss: 0.0009287673747166991\n",
            "step: 60, loss: 0.010526907630264759\n",
            "step: 70, loss: 0.0030237010214477777\n",
            "step: 80, loss: 0.0005208681104704738\n",
            "step: 90, loss: 0.0024014124646782875\n",
            "step: 100, loss: 0.006885797716677189\n",
            "step: 110, loss: 0.01801201142370701\n",
            "step: 120, loss: 0.015741486102342606\n",
            "step: 130, loss: 0.015688277781009674\n",
            "step: 140, loss: 0.0001876793976407498\n",
            "step: 150, loss: 0.013049378991127014\n",
            "step: 160, loss: 0.0009498127619735897\n",
            "step: 170, loss: 0.027273530140519142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7305699481865284, f1=0.6891566265060242, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008709542453289032\n",
            "step: 10, loss: 0.00020167624461464584\n",
            "step: 20, loss: 0.0040599144995212555\n",
            "step: 30, loss: 0.0010376506252214313\n",
            "step: 40, loss: 0.0009931594831869006\n",
            "step: 50, loss: 0.0028076861053705215\n",
            "step: 60, loss: 0.0007085507386364043\n",
            "step: 70, loss: 0.028666799888014793\n",
            "step: 80, loss: 0.0004000875633209944\n",
            "step: 90, loss: 0.00037169564166106284\n",
            "step: 100, loss: 0.0003493308904580772\n",
            "step: 110, loss: 0.0002658023440744728\n",
            "step: 120, loss: 0.00030900101410225034\n",
            "step: 130, loss: 0.0676349326968193\n",
            "step: 140, loss: 0.025249596685171127\n",
            "step: 150, loss: 0.010293868370354176\n",
            "step: 160, loss: 0.08328524231910706\n",
            "step: 170, loss: 0.00029857884510420263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7395833333333331, f1=0.6909975669099757, best_f1=0.6960784313725491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005628286395221949\n",
            "step: 10, loss: 0.0022449863608926535\n",
            "step: 20, loss: 0.010442489758133888\n",
            "step: 30, loss: 0.08892887085676193\n",
            "step: 40, loss: 0.00022726744646206498\n",
            "step: 50, loss: 0.0003986226220149547\n",
            "step: 60, loss: 0.00015881711442489177\n",
            "step: 70, loss: 0.0012359301326796412\n",
            "step: 80, loss: 0.007874148897826672\n",
            "step: 90, loss: 0.00803446676582098\n",
            "step: 100, loss: 0.0005448363372124732\n",
            "step: 110, loss: 0.08590254932641983\n",
            "step: 120, loss: 0.005987165030092001\n",
            "step: 130, loss: 0.0009821916464716196\n",
            "step: 140, loss: 0.09344137459993362\n",
            "step: 150, loss: 0.01716078445315361\n",
            "step: 160, loss: 0.0006809187470935285\n",
            "step: 170, loss: 0.0003141153429169208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7379134860050892, f1=0.6886792452830189, best_f1=0.6960784313725491\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:05, 322.28it/s]\n",
            "load_f1 = 0.7227722772277227\n",
            "real_f1 = 0.7073170731707317\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:17, 252.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb0a681-20ed-441b-fdb5-effdaa4040b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 442/442 [00:00<00:00, 507kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.50MB/s]\n",
            "Downloading: 100% 268M/268M [00:03<00:00, 67.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7977885007858276\n",
            "step: 10, loss: 0.47171780467033386\n",
            "step: 20, loss: 0.5826108455657959\n",
            "step: 30, loss: 0.4938201606273651\n",
            "step: 40, loss: 0.3953050971031189\n",
            "step: 50, loss: 0.35717830061912537\n",
            "step: 60, loss: 0.25754547119140625\n",
            "step: 70, loss: 0.16313371062278748\n",
            "step: 80, loss: 0.32194533944129944\n",
            "step: 90, loss: 0.08157812803983688\n",
            "step: 100, loss: 0.1351412683725357\n",
            "step: 110, loss: 0.1466190665960312\n",
            "step: 120, loss: 0.041793111711740494\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 130, loss: 0.0461166575551033\n",
            "step: 140, loss: 0.09841186553239822\n",
            "step: 150, loss: 0.1056588813662529\n",
            "step: 160, loss: 0.18748660385608673\n",
            "step: 170, loss: 0.025558898225426674\n",
            "step: 180, loss: 0.007389332167804241\n",
            "step: 190, loss: 0.3163536489009857\n",
            "step: 200, loss: 0.04778594523668289\n",
            "step: 210, loss: 0.05420665815472603\n",
            "step: 220, loss: 0.04983772709965706\n",
            "step: 230, loss: 0.019763000309467316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9371554575523705, f1=0.9289740698985345, best_f1=0.9289740698985345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11386702954769135\n",
            "step: 10, loss: 0.10373307019472122\n",
            "step: 20, loss: 0.02531144954264164\n",
            "step: 30, loss: 0.06679247319698334\n",
            "step: 40, loss: 0.055001188069581985\n",
            "step: 50, loss: 0.03589959070086479\n",
            "step: 60, loss: 0.021723132580518723\n",
            "step: 70, loss: 0.10690712183713913\n",
            "step: 80, loss: 0.0040284874849021435\n",
            "step: 90, loss: 0.01786421798169613\n",
            "step: 100, loss: 0.15191468596458435\n",
            "step: 110, loss: 0.06271378695964813\n",
            "step: 120, loss: 0.06387464702129364\n",
            "step: 130, loss: 0.13909219205379486\n",
            "step: 140, loss: 0.16073231399059296\n",
            "step: 150, loss: 0.0403754860162735\n",
            "step: 160, loss: 0.14100955426692963\n",
            "step: 170, loss: 0.052075427025556564\n",
            "step: 180, loss: 0.01155760232359171\n",
            "step: 190, loss: 0.12707985937595367\n",
            "step: 200, loss: 0.1183665543794632\n",
            "step: 210, loss: 0.11741166561841965\n",
            "step: 220, loss: 0.003219516947865486\n",
            "step: 230, loss: 0.08089778572320938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9384098544232924, f1=0.9465301478953355, best_f1=0.9465301478953355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08701024204492569\n",
            "step: 10, loss: 0.05237084999680519\n",
            "step: 20, loss: 0.03297644481062889\n",
            "step: 30, loss: 0.029846027493476868\n",
            "step: 40, loss: 0.16685399413108826\n",
            "step: 50, loss: 0.04648960381746292\n",
            "step: 60, loss: 0.09414080530405045\n",
            "step: 70, loss: 0.008882452733814716\n",
            "step: 80, loss: 0.01975242793560028\n",
            "step: 90, loss: 0.12438483536243439\n",
            "step: 100, loss: 0.012947270646691322\n",
            "step: 110, loss: 0.04492582380771637\n",
            "step: 120, loss: 0.013936996459960938\n",
            "step: 130, loss: 0.0037605317775160074\n",
            "step: 140, loss: 0.0017200721194967628\n",
            "step: 150, loss: 0.001639602705836296\n",
            "step: 160, loss: 0.004795771092176437\n",
            "step: 170, loss: 0.14416088163852692\n",
            "step: 180, loss: 0.009805064648389816\n",
            "step: 190, loss: 0.05256135016679764\n",
            "step: 200, loss: 0.0406491793692112\n",
            "step: 210, loss: 0.012876053340733051\n",
            "step: 220, loss: 0.009234538301825523\n",
            "step: 230, loss: 0.07237989455461502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9598214285714285, f1=0.9479638009049774, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030396828427910805\n",
            "step: 10, loss: 0.008617435581982136\n",
            "step: 20, loss: 0.06928694248199463\n",
            "step: 30, loss: 0.002149739535525441\n",
            "step: 40, loss: 0.017606167122721672\n",
            "step: 50, loss: 0.003987040836364031\n",
            "step: 60, loss: 0.006786162964999676\n",
            "step: 70, loss: 0.008155610412359238\n",
            "step: 80, loss: 0.10718601197004318\n",
            "step: 90, loss: 0.08000728487968445\n",
            "step: 100, loss: 0.0046770297922194\n",
            "step: 110, loss: 0.02410355769097805\n",
            "step: 120, loss: 0.013189934194087982\n",
            "step: 130, loss: 0.053980324417352676\n",
            "step: 140, loss: 0.018356595188379288\n",
            "step: 150, loss: 0.005098107270896435\n",
            "step: 160, loss: 0.0018948308425024152\n",
            "step: 170, loss: 0.006462803576141596\n",
            "step: 180, loss: 0.04665207862854004\n",
            "step: 190, loss: 0.002869817428290844\n",
            "step: 200, loss: 0.0027161238249391317\n",
            "step: 210, loss: 0.01634102687239647\n",
            "step: 220, loss: 0.004113192670047283\n",
            "step: 230, loss: 0.0024936676491051912\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9444444444444444, f1=0.9367369589345171, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04101866856217384\n",
            "step: 10, loss: 0.06464467942714691\n",
            "step: 20, loss: 0.022314608097076416\n",
            "step: 30, loss: 0.00075122358975932\n",
            "step: 40, loss: 0.008395036682486534\n",
            "step: 50, loss: 0.0018497987184673548\n",
            "step: 60, loss: 0.0035686874762177467\n",
            "step: 70, loss: 0.0006946175126358867\n",
            "step: 80, loss: 0.0013808173825964332\n",
            "step: 90, loss: 0.0007994745392352343\n",
            "step: 100, loss: 0.02149426005780697\n",
            "step: 110, loss: 0.0019644771236926317\n",
            "step: 120, loss: 0.04774923622608185\n",
            "step: 130, loss: 0.010034704580903053\n",
            "step: 140, loss: 0.0015676917973905802\n",
            "step: 150, loss: 0.003376706736162305\n",
            "step: 160, loss: 0.1659112274646759\n",
            "step: 170, loss: 0.008413535542786121\n",
            "step: 180, loss: 0.012881215661764145\n",
            "step: 190, loss: 0.0027898431289941072\n",
            "step: 200, loss: 0.0031790430657565594\n",
            "step: 210, loss: 0.0018737869104370475\n",
            "step: 220, loss: 0.001263050944544375\n",
            "step: 230, loss: 0.0010744382161647081\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9487459105779716, f1=0.9525909592061743, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028881325852125883\n",
            "step: 10, loss: 0.029838742688298225\n",
            "step: 20, loss: 0.005045570433139801\n",
            "step: 30, loss: 0.0015460718423128128\n",
            "step: 40, loss: 0.008475571870803833\n",
            "step: 50, loss: 0.006190781015902758\n",
            "step: 60, loss: 0.005852648988366127\n",
            "step: 70, loss: 0.0017503689741715789\n",
            "step: 80, loss: 0.00139673484954983\n",
            "step: 90, loss: 0.0006047565257176757\n",
            "step: 100, loss: 0.00649120332673192\n",
            "step: 110, loss: 0.08214198052883148\n",
            "step: 120, loss: 0.0023427328560501337\n",
            "step: 130, loss: 0.001928898273035884\n",
            "step: 140, loss: 0.007581348996609449\n",
            "step: 150, loss: 0.00091649399837479\n",
            "step: 160, loss: 0.049200110137462616\n",
            "step: 170, loss: 0.004178515635430813\n",
            "step: 180, loss: 0.006171730812638998\n",
            "step: 190, loss: 0.00979720987379551\n",
            "step: 200, loss: 0.026249542832374573\n",
            "step: 210, loss: 0.06296635419130325\n",
            "step: 220, loss: 0.0014455930795520544\n",
            "step: 230, loss: 0.009168514050543308\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.943646408839779, f1=0.9474860335195531, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19021227955818176\n",
            "step: 10, loss: 0.002048509195446968\n",
            "step: 20, loss: 0.001306361285969615\n",
            "step: 30, loss: 0.0026178371626883745\n",
            "step: 40, loss: 0.0026581797283142805\n",
            "step: 50, loss: 0.0006841907161287963\n",
            "step: 60, loss: 0.008219673298299313\n",
            "step: 70, loss: 0.0009313377668149769\n",
            "step: 80, loss: 0.0003656026383396238\n",
            "step: 90, loss: 0.0017064183484762907\n",
            "step: 100, loss: 0.0005026694270782173\n",
            "step: 110, loss: 0.0007357572321780026\n",
            "step: 120, loss: 0.0954316109418869\n",
            "step: 130, loss: 0.02964283712208271\n",
            "step: 140, loss: 0.0007044555386528373\n",
            "step: 150, loss: 0.0005595532711595297\n",
            "step: 160, loss: 0.000592812430113554\n",
            "step: 170, loss: 0.001978813437744975\n",
            "step: 180, loss: 0.0028487660456448793\n",
            "step: 190, loss: 0.0020977642852813005\n",
            "step: 200, loss: 0.006743014790117741\n",
            "step: 210, loss: 0.0010687625035643578\n",
            "step: 220, loss: 0.0009364539291709661\n",
            "step: 230, loss: 0.03908173739910126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9511111111111112, f1=0.9457013574660633, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00400164071470499\n",
            "step: 10, loss: 0.019374750554561615\n",
            "step: 20, loss: 0.0005522326100617647\n",
            "step: 30, loss: 0.03702152892947197\n",
            "step: 40, loss: 0.0009405020973645151\n",
            "step: 50, loss: 0.010271935723721981\n",
            "step: 60, loss: 0.00047609410830773413\n",
            "step: 70, loss: 0.005429540295153856\n",
            "step: 80, loss: 0.002202720148488879\n",
            "step: 90, loss: 0.03860677778720856\n",
            "step: 100, loss: 0.002140214666724205\n",
            "step: 110, loss: 0.00045867369044572115\n",
            "step: 120, loss: 0.00022198895749170333\n",
            "step: 130, loss: 0.0005526302848011255\n",
            "step: 140, loss: 0.00026993209030479193\n",
            "step: 150, loss: 0.00021469854982569814\n",
            "step: 160, loss: 0.0005251511465758085\n",
            "step: 170, loss: 0.0004968398716300726\n",
            "step: 180, loss: 0.004076933953911066\n",
            "step: 190, loss: 0.0014458841178566217\n",
            "step: 200, loss: 0.0007153634796850383\n",
            "step: 210, loss: 0.00025114466552622616\n",
            "step: 220, loss: 0.00017010040755849332\n",
            "step: 230, loss: 0.22308027744293213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9482948294829483, f1=0.9485458612975392, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04738464206457138\n",
            "step: 10, loss: 0.014497748576104641\n",
            "step: 20, loss: 0.012898939661681652\n",
            "step: 30, loss: 0.0008291436824947596\n",
            "step: 40, loss: 0.0005013038753531873\n",
            "step: 50, loss: 0.07927379757165909\n",
            "step: 60, loss: 0.006177426315844059\n",
            "step: 70, loss: 0.0018577426671981812\n",
            "step: 80, loss: 0.10376135259866714\n",
            "step: 90, loss: 0.002037618076428771\n",
            "step: 100, loss: 0.0003603090299293399\n",
            "step: 110, loss: 0.00040055817225947976\n",
            "step: 120, loss: 0.040650032460689545\n",
            "step: 130, loss: 0.0003346760349813849\n",
            "step: 140, loss: 0.0028034320566803217\n",
            "step: 150, loss: 0.00015051252557896078\n",
            "step: 160, loss: 0.0018238211050629616\n",
            "step: 170, loss: 0.00047887087566778064\n",
            "step: 180, loss: 0.019211186096072197\n",
            "step: 190, loss: 0.0022947771940380335\n",
            "step: 200, loss: 0.07304955273866653\n",
            "step: 210, loss: 0.0004690026107709855\n",
            "step: 220, loss: 0.00956203043460846\n",
            "step: 230, loss: 0.0007235648226924241\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9524861878453039, f1=0.9473684210526316, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001125274458900094\n",
            "step: 10, loss: 0.00016230846813414246\n",
            "step: 20, loss: 0.0005988606717437506\n",
            "step: 30, loss: 0.00023831722501199692\n",
            "step: 40, loss: 0.00019796271226368845\n",
            "step: 50, loss: 0.0012446132022887468\n",
            "step: 60, loss: 0.09826718270778656\n",
            "step: 70, loss: 0.0048406897112727165\n",
            "step: 80, loss: 0.00027769707958213985\n",
            "step: 90, loss: 0.0024116418790072203\n",
            "step: 100, loss: 0.0010352286044508219\n",
            "step: 110, loss: 0.00035468663554638624\n",
            "step: 120, loss: 0.0101505471393466\n",
            "step: 130, loss: 0.002519231056794524\n",
            "step: 140, loss: 0.0032332511618733406\n",
            "step: 150, loss: 0.00012443069135770202\n",
            "step: 160, loss: 0.00014134046796243638\n",
            "step: 170, loss: 0.0005444528069347143\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.13771778345108032\n",
            "step: 190, loss: 0.000654245144687593\n",
            "step: 200, loss: 0.00013092691369820386\n",
            "step: 210, loss: 0.00024212485004682094\n",
            "step: 220, loss: 0.0009639140916988254\n",
            "step: 230, loss: 0.0007067658007144928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9470198675496688, f1=0.9453734671125975, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001391887926729396\n",
            "step: 10, loss: 0.0005239841993898153\n",
            "step: 20, loss: 0.00017475760250817984\n",
            "step: 30, loss: 0.00024347545695491135\n",
            "step: 40, loss: 0.0003567553358152509\n",
            "step: 50, loss: 0.0003501740866340697\n",
            "step: 60, loss: 0.0012299712980166078\n",
            "step: 70, loss: 0.0003430080832913518\n",
            "step: 80, loss: 0.0002568041963968426\n",
            "step: 90, loss: 0.003922153729945421\n",
            "step: 100, loss: 0.00030365094426088035\n",
            "step: 110, loss: 0.0001438676263205707\n",
            "step: 120, loss: 0.0006474334513768554\n",
            "step: 130, loss: 0.00016004382632672787\n",
            "step: 140, loss: 0.00016014421998988837\n",
            "step: 150, loss: 0.0007599511882290244\n",
            "step: 160, loss: 0.001476808451116085\n",
            "step: 170, loss: 0.004366862587630749\n",
            "step: 180, loss: 0.004316161852329969\n",
            "step: 190, loss: 0.00027533958200365305\n",
            "step: 200, loss: 0.0001891755819087848\n",
            "step: 210, loss: 0.00016593930195085704\n",
            "step: 220, loss: 0.0007811755058355629\n",
            "step: 230, loss: 0.0018387967720627785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.94211123723042, f1=0.9402298850574713, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003084450727328658\n",
            "step: 10, loss: 0.0004783452895935625\n",
            "step: 20, loss: 0.00017138604016508907\n",
            "step: 30, loss: 0.0004146458231844008\n",
            "step: 40, loss: 0.00010302550799679011\n",
            "step: 50, loss: 8.391863229917362e-05\n",
            "step: 60, loss: 0.009438241831958294\n",
            "step: 70, loss: 0.00016417927690781653\n",
            "step: 80, loss: 0.0001222628925461322\n",
            "step: 90, loss: 0.00032979692332446575\n",
            "step: 100, loss: 7.80218470026739e-05\n",
            "step: 110, loss: 0.00013163154653739184\n",
            "step: 120, loss: 9.923594916472211e-05\n",
            "step: 130, loss: 0.00027608725940808654\n",
            "step: 140, loss: 0.0005769152194261551\n",
            "step: 150, loss: 9.719667286844924e-05\n",
            "step: 160, loss: 0.002140249591320753\n",
            "step: 170, loss: 0.00022713886573910713\n",
            "step: 180, loss: 0.00017172022489830852\n",
            "step: 190, loss: 0.00012515424168668687\n",
            "step: 200, loss: 0.0023237131536006927\n",
            "step: 210, loss: 7.323625322896987e-05\n",
            "step: 220, loss: 0.00036426677252165973\n",
            "step: 230, loss: 0.00024391907209064811\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9471264367816091, f1=0.9422632794457275, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.1531037974637e-05\n",
            "step: 10, loss: 7.89816549513489e-05\n",
            "step: 20, loss: 0.00021227338584139943\n",
            "step: 30, loss: 0.02020101249217987\n",
            "step: 40, loss: 0.00010554712935118005\n",
            "step: 50, loss: 0.00017312588170170784\n",
            "step: 60, loss: 0.00020536998636089265\n",
            "step: 70, loss: 0.00023272840189747512\n",
            "step: 80, loss: 0.0011238069273531437\n",
            "step: 90, loss: 5.4750486015109345e-05\n",
            "step: 100, loss: 0.0001515405165264383\n",
            "step: 110, loss: 9.636852337280288e-05\n",
            "step: 120, loss: 0.00011714937136275694\n",
            "step: 130, loss: 0.00011373918096069247\n",
            "step: 140, loss: 0.007043068762868643\n",
            "step: 150, loss: 0.0006405016174539924\n",
            "step: 160, loss: 8.043631532927975e-05\n",
            "step: 170, loss: 9.108307858696207e-05\n",
            "step: 180, loss: 0.00011744484800146893\n",
            "step: 190, loss: 0.0003757250669877976\n",
            "step: 200, loss: 0.0005506645538844168\n",
            "step: 210, loss: 0.00019003944180440158\n",
            "step: 220, loss: 0.00023221566516440362\n",
            "step: 230, loss: 6.594559090444818e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9502762430939226, f1=0.9508928571428572, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011694958084262908\n",
            "step: 10, loss: 4.6740031393710524e-05\n",
            "step: 20, loss: 0.00024412466154899448\n",
            "step: 30, loss: 0.00018298046779818833\n",
            "step: 40, loss: 9.870126086752862e-05\n",
            "step: 50, loss: 0.00014911578909959644\n",
            "step: 60, loss: 4.875754893873818e-05\n",
            "step: 70, loss: 8.551972132408991e-05\n",
            "step: 80, loss: 0.00016968310228548944\n",
            "step: 90, loss: 0.0004304655594751239\n",
            "step: 100, loss: 0.0002159986470360309\n",
            "step: 110, loss: 0.00015791891200933605\n",
            "step: 120, loss: 6.340788240777329e-05\n",
            "step: 130, loss: 0.000193584623048082\n",
            "step: 140, loss: 8.618066931376234e-05\n",
            "step: 150, loss: 9.678076457930729e-05\n",
            "step: 160, loss: 6.186541577335447e-05\n",
            "step: 170, loss: 0.00017048961308319122\n",
            "step: 180, loss: 0.0001536565978312865\n",
            "step: 190, loss: 0.00012340409739408642\n",
            "step: 200, loss: 6.29520436632447e-05\n",
            "step: 210, loss: 0.00278626405633986\n",
            "step: 220, loss: 0.00011117068061139435\n",
            "step: 230, loss: 6.257898348849267e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9501661129568106, f1=0.9519553072625698, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.773292261641473e-05\n",
            "step: 10, loss: 0.00017532311903778464\n",
            "step: 20, loss: 0.00013777667481917888\n",
            "step: 30, loss: 0.0004814527928829193\n",
            "step: 40, loss: 0.00012524322664830834\n",
            "step: 50, loss: 0.00010689692862797529\n",
            "step: 60, loss: 4.1930103179765865e-05\n",
            "step: 70, loss: 0.0001001342388917692\n",
            "step: 80, loss: 7.40452524041757e-05\n",
            "step: 90, loss: 4.614569843397476e-05\n",
            "step: 100, loss: 8.688222442287952e-05\n",
            "step: 110, loss: 0.00010922431101789698\n",
            "step: 120, loss: 0.00020937631779816002\n",
            "step: 130, loss: 0.00011841639206977561\n",
            "step: 140, loss: 0.005842987447977066\n",
            "step: 150, loss: 0.00011401013034628704\n",
            "step: 160, loss: 6.196399772306904e-05\n",
            "step: 170, loss: 5.445994975161739e-05\n",
            "step: 180, loss: 8.378813799936324e-05\n",
            "step: 190, loss: 0.0005321577191352844\n",
            "step: 200, loss: 6.712995673296973e-05\n",
            "step: 210, loss: 0.000438223680248484\n",
            "step: 220, loss: 0.0001456660102121532\n",
            "step: 230, loss: 8.161352889146656e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9501661129568106, f1=0.9519553072625698, best_f1=0.9479638009049774\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:07, 332.49it/s]\n",
            "load_f1 = 0.9470198675496688\n",
            "real_f1 = 0.947136563876652\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:10, 414.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99069e83-69ef-4af1-b923-1cc1aff87e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.797282338142395\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4381572902202606\n",
            "step: 20, loss: 0.5361637473106384\n",
            "step: 30, loss: 0.470694899559021\n",
            "step: 40, loss: 0.45946305990219116\n",
            "step: 50, loss: 0.44026052951812744\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.41191378235816956\n",
            "step: 70, loss: 0.17213290929794312\n",
            "step: 80, loss: 0.54940265417099\n",
            "step: 90, loss: 0.37176766991615295\n",
            "step: 100, loss: 0.20167188346385956\n",
            "step: 110, loss: 0.12563863396644592\n",
            "step: 120, loss: 0.164622962474823\n",
            "step: 130, loss: 0.055475566536188126\n",
            "step: 140, loss: 0.41605865955352783\n",
            "step: 150, loss: 0.06531757861375809\n",
            "step: 160, loss: 0.15003541111946106\n",
            "step: 170, loss: 0.3696750998497009\n",
            "step: 180, loss: 0.18651746213436127\n",
            "step: 190, loss: 0.06454052031040192\n",
            "step: 200, loss: 0.21349412202835083\n",
            "step: 210, loss: 0.13785040378570557\n",
            "step: 220, loss: 0.06063498929142952\n",
            "step: 230, loss: 0.08765969425439835\n",
            "step: 240, loss: 0.20303472876548767\n",
            "step: 250, loss: 0.09010292589664459\n",
            "step: 260, loss: 0.010502072982490063\n",
            "step: 270, loss: 0.016611667349934578\n",
            "step: 280, loss: 0.19776740670204163\n",
            "step: 290, loss: 0.07576479017734528\n",
            "step: 300, loss: 0.12024270743131638\n",
            "step: 310, loss: 0.0715879276394844\n",
            "step: 320, loss: 0.10092749446630478\n",
            "step: 330, loss: 0.24463598430156708\n",
            "step: 340, loss: 0.21330855786800385\n",
            "step: 350, loss: 0.10220780968666077\n",
            "step: 360, loss: 0.06618399918079376\n",
            "step: 370, loss: 0.16745974123477936\n",
            "step: 380, loss: 0.23667371273040771\n",
            "step: 390, loss: 0.02865820750594139\n",
            "step: 400, loss: 0.03206267207860947\n",
            "step: 410, loss: 0.09886012971401215\n",
            "step: 420, loss: 0.07681727409362793\n",
            "step: 430, loss: 0.22004497051239014\n",
            "step: 440, loss: 0.14609883725643158\n",
            "step: 450, loss: 0.08858579397201538\n",
            "step: 460, loss: 0.09028096497058868\n",
            "step: 470, loss: 0.1873289793729782\n",
            "step: 480, loss: 0.3006719946861267\n",
            "step: 490, loss: 0.11710362881422043\n",
            "step: 500, loss: 0.036209359765052795\n",
            "step: 510, loss: 0.07737740129232407\n",
            "step: 520, loss: 0.12792637944221497\n",
            "step: 530, loss: 0.07422596216201782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9011709601873535, f1=0.8905519176800748, best_f1=0.8905519176800748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15972748398780823\n",
            "step: 10, loss: 0.22207625210285187\n",
            "step: 20, loss: 0.13699108362197876\n",
            "step: 30, loss: 0.10502754151821136\n",
            "step: 40, loss: 0.009485970251262188\n",
            "step: 50, loss: 0.032837480306625366\n",
            "step: 60, loss: 0.17163223028182983\n",
            "step: 70, loss: 0.23936857283115387\n",
            "step: 80, loss: 0.061580169945955276\n",
            "step: 90, loss: 0.17062625288963318\n",
            "step: 100, loss: 0.21487727761268616\n",
            "step: 110, loss: 0.05369604751467705\n",
            "step: 120, loss: 0.053170155733823776\n",
            "step: 130, loss: 0.09085476398468018\n",
            "step: 140, loss: 0.03497575595974922\n",
            "step: 150, loss: 0.037166088819503784\n",
            "step: 160, loss: 0.06804423779249191\n",
            "step: 170, loss: 0.15605483949184418\n",
            "step: 180, loss: 0.04043520987033844\n",
            "step: 190, loss: 0.031029194593429565\n",
            "step: 200, loss: 0.025034798309206963\n",
            "step: 210, loss: 0.01024729572236538\n",
            "step: 220, loss: 0.1443796306848526\n",
            "step: 230, loss: 0.059119634330272675\n",
            "step: 240, loss: 0.15724530816078186\n",
            "step: 250, loss: 0.12526986002922058\n",
            "step: 260, loss: 0.05000067874789238\n",
            "step: 270, loss: 0.08666227012872696\n",
            "step: 280, loss: 0.1467241644859314\n",
            "step: 290, loss: 0.09078375995159149\n",
            "step: 300, loss: 0.020526554435491562\n",
            "step: 310, loss: 0.11931157112121582\n",
            "step: 320, loss: 0.14093336462974548\n",
            "step: 330, loss: 0.05796022340655327\n",
            "step: 340, loss: 0.006093531381338835\n",
            "step: 350, loss: 0.13035422563552856\n",
            "step: 360, loss: 0.13591714203357697\n",
            "step: 370, loss: 0.09150693565607071\n",
            "step: 380, loss: 0.1263812929391861\n",
            "step: 390, loss: 0.04842739924788475\n",
            "step: 400, loss: 0.11346912384033203\n",
            "step: 410, loss: 0.029394280165433884\n",
            "step: 420, loss: 0.13386259973049164\n",
            "step: 430, loss: 0.04000530019402504\n",
            "step: 440, loss: 0.011352281086146832\n",
            "step: 450, loss: 0.046582356095314026\n",
            "step: 460, loss: 0.22234812378883362\n",
            "step: 470, loss: 0.06957436352968216\n",
            "step: 480, loss: 0.3781655430793762\n",
            "step: 490, loss: 0.039861928671598434\n",
            "step: 500, loss: 0.03953102231025696\n",
            "step: 510, loss: 0.05724671110510826\n",
            "step: 520, loss: 0.050416942685842514\n",
            "step: 530, loss: 0.13437232375144958\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9072450392247347, f1=0.8967114404817044, best_f1=0.8967114404817044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031039293855428696\n",
            "step: 10, loss: 0.04072072356939316\n",
            "step: 20, loss: 0.15149356424808502\n",
            "step: 30, loss: 0.06729619204998016\n",
            "step: 40, loss: 0.005167340859770775\n",
            "step: 50, loss: 0.021029511466622353\n",
            "step: 60, loss: 0.003343126503750682\n",
            "step: 70, loss: 0.054645687341690063\n",
            "step: 80, loss: 0.11309605836868286\n",
            "step: 90, loss: 0.22833406925201416\n",
            "step: 100, loss: 0.04963088408112526\n",
            "step: 110, loss: 0.05199692025780678\n",
            "step: 120, loss: 0.029929358512163162\n",
            "step: 130, loss: 0.039914797991514206\n",
            "step: 140, loss: 0.05490243062376976\n",
            "step: 150, loss: 0.0019938619807362556\n",
            "step: 160, loss: 0.004732377361506224\n",
            "step: 170, loss: 0.018921274691820145\n",
            "step: 180, loss: 0.019439080730080605\n",
            "step: 190, loss: 0.020347265526652336\n",
            "step: 200, loss: 0.08001501113176346\n",
            "step: 210, loss: 0.131010040640831\n",
            "step: 220, loss: 0.014691818505525589\n",
            "step: 230, loss: 0.060467615723609924\n",
            "step: 240, loss: 0.20008960366249084\n",
            "step: 250, loss: 0.03356989473104477\n",
            "step: 260, loss: 0.008919358253479004\n",
            "step: 270, loss: 0.10965513437986374\n",
            "step: 280, loss: 0.07285510003566742\n",
            "step: 290, loss: 0.10549629479646683\n",
            "step: 300, loss: 0.10845974087715149\n",
            "step: 310, loss: 0.0315048024058342\n",
            "step: 320, loss: 0.1780410259962082\n",
            "step: 330, loss: 0.012999670580029488\n",
            "step: 340, loss: 0.0044219838455319405\n",
            "step: 350, loss: 0.15708863735198975\n",
            "step: 360, loss: 0.008732171729207039\n",
            "step: 370, loss: 0.1449793577194214\n",
            "step: 380, loss: 0.009962761774659157\n",
            "step: 390, loss: 0.04834398627281189\n",
            "step: 400, loss: 0.04711716249585152\n",
            "step: 410, loss: 0.11509226262569427\n",
            "step: 420, loss: 0.11855288594961166\n",
            "step: 430, loss: 0.026413410902023315\n",
            "step: 440, loss: 0.024699682369828224\n",
            "step: 450, loss: 0.18802586197853088\n",
            "step: 460, loss: 0.1604214459657669\n",
            "step: 470, loss: 0.025013813748955727\n",
            "step: 480, loss: 0.019723286852240562\n",
            "step: 490, loss: 0.006496070884168148\n",
            "step: 500, loss: 0.04881182312965393\n",
            "step: 510, loss: 0.11258653551340103\n",
            "step: 520, loss: 0.013586846180260181\n",
            "step: 530, loss: 0.082606740295887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9073033707865168, f1=0.9010367577756833, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011628438718616962\n",
            "step: 10, loss: 0.10638139396905899\n",
            "step: 20, loss: 0.044413335621356964\n",
            "step: 30, loss: 0.01792047545313835\n",
            "step: 40, loss: 0.022399887442588806\n",
            "step: 50, loss: 0.017524385824799538\n",
            "step: 60, loss: 0.011720895767211914\n",
            "step: 70, loss: 0.04287880286574364\n",
            "step: 80, loss: 0.01883440837264061\n",
            "step: 90, loss: 0.03518180921673775\n",
            "step: 100, loss: 0.011912989430129528\n",
            "step: 110, loss: 0.021122941747307777\n",
            "step: 120, loss: 0.12130428850650787\n",
            "step: 130, loss: 0.035035908222198486\n",
            "step: 140, loss: 0.07643639296293259\n",
            "step: 150, loss: 0.00882356334477663\n",
            "step: 160, loss: 0.012884639203548431\n",
            "step: 170, loss: 0.006457739509642124\n",
            "step: 180, loss: 0.028297068551182747\n",
            "step: 190, loss: 0.01861911080777645\n",
            "step: 200, loss: 0.00842305924743414\n",
            "step: 210, loss: 0.019965924322605133\n",
            "step: 220, loss: 0.011563226580619812\n",
            "step: 230, loss: 0.21347211301326752\n",
            "step: 240, loss: 0.022502748295664787\n",
            "step: 250, loss: 0.005232692696154118\n",
            "step: 260, loss: 0.12625281512737274\n",
            "step: 270, loss: 0.026927554979920387\n",
            "step: 280, loss: 0.019112899899482727\n",
            "step: 290, loss: 0.0012238946510478854\n",
            "step: 300, loss: 0.005246427375823259\n",
            "step: 310, loss: 0.005532054230570793\n",
            "step: 320, loss: 0.029842520132660866\n",
            "step: 330, loss: 0.051382407546043396\n",
            "step: 340, loss: 0.037573765963315964\n",
            "step: 350, loss: 0.006191709078848362\n",
            "step: 360, loss: 0.01434493251144886\n",
            "step: 370, loss: 0.06589564681053162\n",
            "step: 380, loss: 0.012893673963844776\n",
            "step: 390, loss: 0.058237772434949875\n",
            "step: 400, loss: 0.04903750121593475\n",
            "step: 410, loss: 0.013209155760705471\n",
            "step: 420, loss: 0.007900700904428959\n",
            "step: 430, loss: 0.01730825938284397\n",
            "step: 440, loss: 0.020664187148213387\n",
            "step: 450, loss: 0.0034342422150075436\n",
            "step: 460, loss: 0.001728206523694098\n",
            "step: 470, loss: 0.02082890085875988\n",
            "step: 480, loss: 0.034819457679986954\n",
            "step: 490, loss: 0.02694679982960224\n",
            "step: 500, loss: 0.008660035207867622\n",
            "step: 510, loss: 0.05405480042099953\n",
            "step: 520, loss: 0.09512132406234741\n",
            "step: 530, loss: 0.09151092171669006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.8991799324650265, f1=0.8911465892597968, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005539875943213701\n",
            "step: 10, loss: 0.022173862904310226\n",
            "step: 20, loss: 0.1690671145915985\n",
            "step: 30, loss: 0.0042058550752699375\n",
            "step: 40, loss: 0.04055263102054596\n",
            "step: 50, loss: 0.00914285983890295\n",
            "step: 60, loss: 0.18515107035636902\n",
            "step: 70, loss: 0.002050961134955287\n",
            "step: 80, loss: 0.00172512698918581\n",
            "step: 90, loss: 0.013793508522212505\n",
            "step: 100, loss: 0.006998343393206596\n",
            "step: 110, loss: 0.007260981015861034\n",
            "step: 120, loss: 0.06103251129388809\n",
            "step: 130, loss: 0.002628639107570052\n",
            "step: 140, loss: 0.0074408939108252525\n",
            "step: 150, loss: 0.04326586425304413\n",
            "step: 160, loss: 0.13466677069664001\n",
            "step: 170, loss: 0.11165191978216171\n",
            "step: 180, loss: 0.0026053208857774734\n",
            "step: 190, loss: 0.005209994036704302\n",
            "step: 200, loss: 0.0019380347803235054\n",
            "step: 210, loss: 0.0007005877560004592\n",
            "step: 220, loss: 0.0039573670364916325\n",
            "step: 230, loss: 0.0032215167302638292\n",
            "step: 240, loss: 0.003262768965214491\n",
            "step: 250, loss: 0.0009560944745317101\n",
            "step: 260, loss: 0.025056127458810806\n",
            "step: 270, loss: 0.019233018159866333\n",
            "step: 280, loss: 0.1498948633670807\n",
            "step: 290, loss: 0.022420862689614296\n",
            "step: 300, loss: 0.02787918969988823\n",
            "step: 310, loss: 0.0037839370779693127\n",
            "step: 320, loss: 0.02957027778029442\n",
            "step: 330, loss: 0.0024238829500973225\n",
            "step: 340, loss: 0.00335117825306952\n",
            "step: 350, loss: 0.0009954712586477399\n",
            "step: 360, loss: 0.003849540837109089\n",
            "step: 370, loss: 0.001968510914593935\n",
            "step: 380, loss: 0.04622243344783783\n",
            "step: 390, loss: 0.01678197830915451\n",
            "step: 400, loss: 0.10404867678880692\n",
            "step: 410, loss: 0.005201262421905994\n",
            "step: 420, loss: 0.003237547818571329\n",
            "step: 430, loss: 0.01445006113499403\n",
            "step: 440, loss: 0.020419608801603317\n",
            "step: 450, loss: 0.005057768430560827\n",
            "step: 460, loss: 0.0006478271679952741\n",
            "step: 470, loss: 0.0033413933124393225\n",
            "step: 480, loss: 0.00379933905787766\n",
            "step: 490, loss: 0.13944652676582336\n",
            "step: 500, loss: 0.013514005579054356\n",
            "step: 510, loss: 0.015572095289826393\n",
            "step: 520, loss: 0.018520109355449677\n",
            "step: 530, loss: 0.0054131061770021915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8994857410004675, f1=0.8992030004688233, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003760178340598941\n",
            "step: 10, loss: 0.07093912363052368\n",
            "step: 20, loss: 0.0055472212843596935\n",
            "step: 30, loss: 0.003724275855347514\n",
            "step: 40, loss: 0.0028137778863310814\n",
            "step: 50, loss: 0.033647798001766205\n",
            "step: 60, loss: 0.004030571319162846\n",
            "step: 70, loss: 0.016154294833540916\n",
            "step: 80, loss: 0.00020201240840833634\n",
            "step: 90, loss: 0.0016013724962249398\n",
            "step: 100, loss: 0.19362416863441467\n",
            "step: 110, loss: 0.004107970278710127\n",
            "step: 120, loss: 0.002116478979587555\n",
            "step: 130, loss: 0.0011726325610652566\n",
            "step: 140, loss: 0.00468513322994113\n",
            "step: 150, loss: 0.0005880589596927166\n",
            "step: 160, loss: 0.00017841279623098671\n",
            "step: 170, loss: 0.0001669116463745013\n",
            "step: 180, loss: 0.000192703097127378\n",
            "step: 190, loss: 0.010101656429469585\n",
            "step: 200, loss: 0.0011715731816366315\n",
            "step: 210, loss: 0.0031821439042687416\n",
            "step: 220, loss: 0.00035714704426936805\n",
            "step: 230, loss: 0.011005506850779057\n",
            "step: 240, loss: 0.011394361965358257\n",
            "step: 250, loss: 0.00026216544210910797\n",
            "step: 260, loss: 0.00046724374988116324\n",
            "step: 270, loss: 0.04396532103419304\n",
            "step: 280, loss: 0.03909163177013397\n",
            "step: 290, loss: 0.00042764184763655066\n",
            "step: 300, loss: 0.0012257697526365519\n",
            "step: 310, loss: 0.0005539812264032662\n",
            "step: 320, loss: 0.0011476275976747274\n",
            "step: 330, loss: 0.0196986123919487\n",
            "step: 340, loss: 0.008157526142895222\n",
            "step: 350, loss: 0.008681879378855228\n",
            "step: 360, loss: 0.00024985865456983447\n",
            "step: 370, loss: 0.08918130397796631\n",
            "step: 380, loss: 0.0019800690934062004\n",
            "step: 390, loss: 0.0028474219143390656\n",
            "step: 400, loss: 0.02593347802758217\n",
            "step: 410, loss: 0.000384768150979653\n",
            "step: 420, loss: 0.009618713520467281\n",
            "step: 430, loss: 0.002272305777296424\n",
            "step: 440, loss: 0.028569793328642845\n",
            "step: 450, loss: 0.06199350580573082\n",
            "step: 460, loss: 0.05499229580163956\n",
            "step: 470, loss: 0.13290993869304657\n",
            "step: 480, loss: 0.036652542650699615\n",
            "step: 490, loss: 0.008490282110869884\n",
            "step: 500, loss: 0.007875663228332996\n",
            "step: 510, loss: 0.0008308067917823792\n",
            "step: 520, loss: 0.0014489352470263839\n",
            "step: 530, loss: 0.0011585550382733345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8970380818053596, f1=0.8843794242567249, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1006716638803482\n",
            "step: 10, loss: 0.010061459615826607\n",
            "step: 20, loss: 0.007157766260206699\n",
            "step: 30, loss: 0.0009074903791770339\n",
            "step: 40, loss: 0.0005215282435528934\n",
            "step: 50, loss: 0.00022707045718561858\n",
            "step: 60, loss: 0.00027750778826884925\n",
            "step: 70, loss: 0.000662682403344661\n",
            "step: 80, loss: 0.00029789027757942677\n",
            "step: 90, loss: 0.00019847284420393407\n",
            "step: 100, loss: 0.000428570929216221\n",
            "step: 110, loss: 0.03943464905023575\n",
            "step: 120, loss: 0.00013005916844122112\n",
            "step: 130, loss: 7.107070268830284e-05\n",
            "step: 140, loss: 0.0004322124586906284\n",
            "step: 150, loss: 7.925921090645716e-05\n",
            "step: 160, loss: 0.034122709184885025\n",
            "step: 170, loss: 0.005427144002169371\n",
            "step: 180, loss: 0.00013066317478660494\n",
            "step: 190, loss: 0.00321800634264946\n",
            "step: 200, loss: 0.10968521982431412\n",
            "step: 210, loss: 0.005138372536748648\n",
            "step: 220, loss: 0.0005685415817424655\n",
            "step: 230, loss: 0.0039525991305708885\n",
            "step: 240, loss: 0.0009230350842699409\n",
            "step: 250, loss: 0.002790557686239481\n",
            "step: 260, loss: 0.004598698578774929\n",
            "step: 270, loss: 6.891605880809948e-05\n",
            "step: 280, loss: 0.08839989453554153\n",
            "step: 290, loss: 0.0014722573105245829\n",
            "step: 300, loss: 0.0002778730995487422\n",
            "step: 310, loss: 0.22262823581695557\n",
            "step: 320, loss: 0.011798267252743244\n",
            "step: 330, loss: 0.00023015090846456587\n",
            "step: 340, loss: 0.0012194806477054954\n",
            "step: 350, loss: 0.0003592855064198375\n",
            "step: 360, loss: 0.033826664090156555\n",
            "step: 370, loss: 0.03724155202507973\n",
            "step: 380, loss: 0.032383184880018234\n",
            "step: 390, loss: 0.0006102664628997445\n",
            "step: 400, loss: 0.0035002732183784246\n",
            "step: 410, loss: 0.0025341941509395838\n",
            "step: 420, loss: 0.03305184468626976\n",
            "step: 430, loss: 0.005902708508074284\n",
            "step: 440, loss: 8.804920798866078e-05\n",
            "step: 450, loss: 0.0020447031129151583\n",
            "step: 460, loss: 0.00047243168228305876\n",
            "step: 470, loss: 0.14246022701263428\n",
            "step: 480, loss: 0.03127482905983925\n",
            "step: 490, loss: 0.0010561299277469516\n",
            "step: 500, loss: 0.00023420368961524218\n",
            "step: 510, loss: 0.001955082407221198\n",
            "step: 520, loss: 0.004022487439215183\n",
            "step: 530, loss: 0.09760626405477524\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.902123730378578, f1=0.8929236499068901, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026262958999723196\n",
            "step: 10, loss: 0.012068478390574455\n",
            "step: 20, loss: 0.004422382917255163\n",
            "step: 30, loss: 0.006388548761606216\n",
            "step: 40, loss: 0.01508947554975748\n",
            "step: 50, loss: 0.00017649552319198847\n",
            "step: 60, loss: 0.002825257834047079\n",
            "step: 70, loss: 7.060931238811463e-05\n",
            "step: 80, loss: 0.0042481557466089725\n",
            "step: 90, loss: 0.0002301565691595897\n",
            "step: 100, loss: 0.0002271440316690132\n",
            "step: 110, loss: 0.001268636086024344\n",
            "step: 120, loss: 0.00025794963585212827\n",
            "step: 130, loss: 0.00015057479322422296\n",
            "step: 140, loss: 4.8646179493516684e-05\n",
            "step: 150, loss: 0.0011820174986496568\n",
            "step: 160, loss: 0.00016223214333876967\n",
            "step: 170, loss: 0.1665373295545578\n",
            "step: 180, loss: 0.018078595399856567\n",
            "step: 190, loss: 0.005411139689385891\n",
            "step: 200, loss: 0.00015102043107617646\n",
            "step: 210, loss: 0.16743136942386627\n",
            "step: 220, loss: 0.002215869491919875\n",
            "step: 230, loss: 0.00016640464309602976\n",
            "step: 240, loss: 0.003387731732800603\n",
            "step: 250, loss: 0.0006330079631879926\n",
            "step: 260, loss: 0.0006014241371303797\n",
            "step: 270, loss: 0.00026827698457054794\n",
            "step: 280, loss: 0.0008699047612026334\n",
            "step: 290, loss: 0.019417110830545425\n",
            "step: 300, loss: 0.0001480371574871242\n",
            "step: 310, loss: 0.053979966789484024\n",
            "step: 320, loss: 0.019578304141759872\n",
            "step: 330, loss: 0.028791315853595734\n",
            "step: 340, loss: 0.0007202147389762104\n",
            "step: 350, loss: 0.0006071575917303562\n",
            "step: 360, loss: 0.00047164823627099395\n",
            "step: 370, loss: 0.009273711591959\n",
            "step: 380, loss: 0.11564601212739944\n",
            "step: 390, loss: 0.0011114060180261731\n",
            "step: 400, loss: 0.005951728206127882\n",
            "step: 410, loss: 0.09336038678884506\n",
            "step: 420, loss: 0.0005846221465617418\n",
            "step: 430, loss: 0.0002512976061552763\n",
            "step: 440, loss: 0.006468301173299551\n",
            "step: 450, loss: 0.00019276388047728688\n",
            "step: 460, loss: 0.001595796667970717\n",
            "step: 470, loss: 0.000565899710636586\n",
            "step: 480, loss: 0.004885638598352671\n",
            "step: 490, loss: 0.00023944920394569635\n",
            "step: 500, loss: 0.001492420444265008\n",
            "step: 510, loss: 0.01848262920975685\n",
            "step: 520, loss: 0.0015927230706438422\n",
            "step: 530, loss: 0.00035365470102988183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8991954566966399, f1=0.8840996168582376, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002653761301189661\n",
            "step: 10, loss: 0.001569288084283471\n",
            "step: 20, loss: 0.0003667429555207491\n",
            "step: 30, loss: 0.012460277415812016\n",
            "step: 40, loss: 0.0003895804693456739\n",
            "step: 50, loss: 0.0015609057154506445\n",
            "step: 60, loss: 0.00013434034190140665\n",
            "step: 70, loss: 0.08426154404878616\n",
            "step: 80, loss: 0.0005193522665649652\n",
            "step: 90, loss: 0.0042283437214791775\n",
            "step: 100, loss: 0.0013826247304677963\n",
            "step: 110, loss: 0.0003680252411868423\n",
            "step: 120, loss: 0.0002496697998140007\n",
            "step: 130, loss: 8.947340393206105e-05\n",
            "step: 140, loss: 0.0009827063186094165\n",
            "step: 150, loss: 0.001690963632427156\n",
            "step: 160, loss: 0.00011251957766944543\n",
            "step: 170, loss: 0.0011489324970170856\n",
            "step: 180, loss: 5.307763422024436e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 190, loss: 0.14743882417678833\n",
            "step: 200, loss: 0.00043059090967290103\n",
            "step: 210, loss: 0.0002168121573049575\n",
            "step: 220, loss: 0.0020965179428458214\n",
            "step: 230, loss: 0.00021255257888697088\n",
            "step: 240, loss: 0.004001119174063206\n",
            "step: 250, loss: 0.0580708347260952\n",
            "step: 260, loss: 0.0009948932565748692\n",
            "step: 270, loss: 0.004432446323335171\n",
            "step: 280, loss: 0.0006297242944128811\n",
            "step: 290, loss: 0.00011728171375580132\n",
            "step: 300, loss: 0.0052511501125991344\n",
            "step: 310, loss: 0.00011987549805780873\n",
            "step: 320, loss: 0.00022356075351126492\n",
            "step: 330, loss: 0.0011144927702844143\n",
            "step: 340, loss: 7.06135033397004e-05\n",
            "step: 350, loss: 5.898510789847933e-05\n",
            "step: 360, loss: 0.008208216167986393\n",
            "step: 370, loss: 0.0691952034831047\n",
            "step: 380, loss: 0.00012753524060826749\n",
            "step: 390, loss: 0.00017599428247194737\n",
            "step: 400, loss: 0.042549166828393936\n",
            "step: 410, loss: 0.000544345413800329\n",
            "step: 420, loss: 0.0006795627996325493\n",
            "step: 430, loss: 0.00012832321226596832\n",
            "step: 440, loss: 0.0001987520226975903\n",
            "step: 450, loss: 0.0017523982096463442\n",
            "step: 460, loss: 0.02053011767566204\n",
            "step: 470, loss: 0.005663459189236164\n",
            "step: 480, loss: 0.00011090083717135713\n",
            "step: 490, loss: 0.0009433956583961844\n",
            "step: 500, loss: 0.0017456711502745748\n",
            "step: 510, loss: 0.00022256645024754107\n",
            "step: 520, loss: 6.069739174563438e-05\n",
            "step: 530, loss: 0.0006614120211452246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.89900426742532, f1=0.8883587786259542, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018567751976661384\n",
            "step: 10, loss: 2.8035996365360916e-05\n",
            "step: 20, loss: 0.0005400180816650391\n",
            "step: 30, loss: 5.288731335895136e-05\n",
            "step: 40, loss: 0.009004865773022175\n",
            "step: 50, loss: 0.0001903182128444314\n",
            "step: 60, loss: 5.389509169617668e-05\n",
            "step: 70, loss: 9.140982729149982e-05\n",
            "step: 80, loss: 0.007065861020237207\n",
            "step: 90, loss: 8.580060239182785e-05\n",
            "step: 100, loss: 0.00734102725982666\n",
            "step: 110, loss: 0.0020992346107959747\n",
            "step: 120, loss: 0.001569963525980711\n",
            "step: 130, loss: 0.002616242738440633\n",
            "step: 140, loss: 7.042875949991867e-05\n",
            "step: 150, loss: 0.00011666089994832873\n",
            "step: 160, loss: 0.00010938890045508742\n",
            "step: 170, loss: 0.0031670306343585253\n",
            "step: 180, loss: 0.15206220746040344\n",
            "step: 190, loss: 0.0004234028165228665\n",
            "step: 200, loss: 0.0030468706972897053\n",
            "step: 210, loss: 0.006380421109497547\n",
            "step: 220, loss: 0.00018839236872736365\n",
            "step: 230, loss: 0.0012321770191192627\n",
            "step: 240, loss: 0.00016350667283404619\n",
            "step: 250, loss: 0.0007382985786534846\n",
            "step: 260, loss: 0.026797188445925713\n",
            "step: 270, loss: 0.000872669043019414\n",
            "step: 280, loss: 0.00017228606157004833\n",
            "step: 290, loss: 0.0029061275999993086\n",
            "step: 300, loss: 0.006021385081112385\n",
            "step: 310, loss: 0.0003723542613442987\n",
            "step: 320, loss: 0.07731960713863373\n",
            "step: 330, loss: 0.0003460282750893384\n",
            "step: 340, loss: 0.00020218550343997777\n",
            "step: 350, loss: 0.0009674594621174037\n",
            "step: 360, loss: 0.00022483702923636883\n",
            "step: 370, loss: 0.0005131378420628607\n",
            "step: 380, loss: 0.00041432440048083663\n",
            "step: 390, loss: 0.0021063745953142643\n",
            "step: 400, loss: 0.0020649598445743322\n",
            "step: 410, loss: 0.00039692362770438194\n",
            "step: 420, loss: 5.248490924714133e-05\n",
            "step: 430, loss: 6.43537932774052e-05\n",
            "step: 440, loss: 3.861896402668208e-05\n",
            "step: 450, loss: 0.00019866494403686374\n",
            "step: 460, loss: 8.97282880032435e-05\n",
            "step: 470, loss: 0.00022195407655090094\n",
            "step: 480, loss: 8.157156116794795e-05\n",
            "step: 490, loss: 0.10406329482793808\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 500, loss: 0.00011815728794317693\n",
            "step: 510, loss: 0.0005042797420173883\n",
            "step: 520, loss: 0.00014088614261709154\n",
            "step: 530, loss: 0.00036622327752411366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9011095031355524, f1=0.8729609490855166, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012673699529841542\n",
            "step: 10, loss: 0.0033488881308585405\n",
            "step: 20, loss: 0.0011087703751400113\n",
            "step: 30, loss: 0.00036186579382047057\n",
            "step: 40, loss: 5.228550435276702e-05\n",
            "step: 50, loss: 0.00010642228153301403\n",
            "step: 60, loss: 3.529537207214162e-05\n",
            "step: 70, loss: 7.862656639190391e-05\n",
            "step: 80, loss: 5.494219658430666e-05\n",
            "step: 90, loss: 6.2868173699826e-05\n",
            "step: 100, loss: 3.228236164432019e-05\n",
            "step: 110, loss: 5.3674910304835066e-05\n",
            "step: 120, loss: 6.952081457711756e-05\n",
            "step: 130, loss: 5.225716085988097e-05\n",
            "step: 140, loss: 4.3683718104148284e-05\n",
            "step: 150, loss: 3.4115251764887944e-05\n",
            "step: 160, loss: 8.072747732512653e-05\n",
            "step: 170, loss: 0.00013810979726258665\n",
            "step: 180, loss: 2.310019772266969e-05\n",
            "step: 190, loss: 9.502654575044289e-05\n",
            "step: 200, loss: 0.0004389439127407968\n",
            "step: 210, loss: 3.5675624530995265e-05\n",
            "step: 220, loss: 0.000298016908345744\n",
            "step: 230, loss: 3.8104735722299665e-05\n",
            "step: 240, loss: 2.7778931325883605e-05\n",
            "step: 250, loss: 0.0005761768552474678\n",
            "step: 260, loss: 3.4498374589020386e-05\n",
            "step: 270, loss: 0.00043141868081875145\n",
            "step: 280, loss: 0.002853243611752987\n",
            "step: 290, loss: 0.00010713120718719438\n",
            "step: 300, loss: 0.00013891990238334984\n",
            "step: 310, loss: 0.015654245391488075\n",
            "step: 320, loss: 0.0006780445692129433\n",
            "step: 330, loss: 0.0019025546498596668\n",
            "step: 340, loss: 2.4381612092838623e-05\n",
            "step: 350, loss: 0.0005999594577588141\n",
            "step: 360, loss: 0.0003216999175492674\n",
            "step: 370, loss: 0.00023597570543643087\n",
            "step: 380, loss: 0.006831992417573929\n",
            "step: 390, loss: 0.001490720547735691\n",
            "step: 400, loss: 0.0014098346000537276\n",
            "step: 410, loss: 2.2567490304936655e-05\n",
            "step: 420, loss: 0.00463460385799408\n",
            "step: 430, loss: 3.185334207955748e-05\n",
            "step: 440, loss: 0.00013785959163215011\n",
            "step: 450, loss: 2.5305482267867774e-05\n",
            "step: 460, loss: 0.09656257182359695\n",
            "step: 470, loss: 0.0006012713420204818\n",
            "step: 480, loss: 0.0002561819856055081\n",
            "step: 490, loss: 0.002083658240735531\n",
            "step: 500, loss: 6.561349437106401e-05\n",
            "step: 510, loss: 5.8566234656609595e-05\n",
            "step: 520, loss: 3.0706371035194024e-05\n",
            "step: 530, loss: 3.1659830710850656e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8952830188679246, f1=0.8875, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009968921076506376\n",
            "step: 10, loss: 7.32696134946309e-05\n",
            "step: 20, loss: 2.8598418793990277e-05\n",
            "step: 30, loss: 3.994730286649428e-05\n",
            "step: 40, loss: 0.0015480684814974666\n",
            "step: 50, loss: 0.0009748090524226427\n",
            "step: 60, loss: 6.614008452743292e-05\n",
            "step: 70, loss: 0.00011210133379790932\n",
            "step: 80, loss: 0.00024656925234012306\n",
            "step: 90, loss: 0.00012889050412923098\n",
            "step: 100, loss: 0.00022410383098758757\n",
            "step: 110, loss: 2.4109647711156867e-05\n",
            "step: 120, loss: 0.0008389588329009712\n",
            "step: 130, loss: 3.503787957015447e-05\n",
            "step: 140, loss: 5.722701098420657e-05\n",
            "step: 150, loss: 0.0021576841827481985\n",
            "step: 160, loss: 3.697485590237193e-05\n",
            "step: 170, loss: 4.0798975533107296e-05\n",
            "step: 180, loss: 4.366682696854696e-05\n",
            "step: 190, loss: 2.096556272590533e-05\n",
            "step: 200, loss: 4.572779289446771e-05\n",
            "step: 210, loss: 8.830869774101302e-05\n",
            "step: 220, loss: 0.0012662699446082115\n",
            "step: 230, loss: 0.00017365740495733917\n",
            "step: 240, loss: 2.5595936676836573e-05\n",
            "step: 250, loss: 0.0002679743920452893\n",
            "step: 260, loss: 0.0005628978833556175\n",
            "step: 270, loss: 0.00012196910392958671\n",
            "step: 280, loss: 3.823516453849152e-05\n",
            "step: 290, loss: 0.0008354483870789409\n",
            "step: 300, loss: 0.010734152048826218\n",
            "step: 310, loss: 6.128466338850558e-05\n",
            "step: 320, loss: 0.00027894743834622204\n",
            "step: 330, loss: 6.183759978739545e-05\n",
            "step: 340, loss: 7.130236917873845e-05\n",
            "step: 350, loss: 0.0003576385206542909\n",
            "step: 360, loss: 0.005340684205293655\n",
            "step: 370, loss: 5.192149546928704e-05\n",
            "step: 380, loss: 0.00027348092407919466\n",
            "step: 390, loss: 4.0986622479977086e-05\n",
            "step: 400, loss: 3.759734318009578e-05\n",
            "step: 410, loss: 0.0003543886705301702\n",
            "step: 420, loss: 5.4805288527859375e-05\n",
            "step: 430, loss: 3.823880251729861e-05\n",
            "step: 440, loss: 9.34850177145563e-05\n",
            "step: 450, loss: 0.00033825053833425045\n",
            "step: 460, loss: 0.00024482241133227944\n",
            "step: 470, loss: 0.0005236067227087915\n",
            "step: 480, loss: 0.00015816812810953707\n",
            "step: 490, loss: 0.0030529522337019444\n",
            "step: 500, loss: 0.0001667961332714185\n",
            "step: 510, loss: 4.8109552153619006e-05\n",
            "step: 520, loss: 0.015967490151524544\n",
            "step: 530, loss: 6.780654075555503e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9003306565895135, f1=0.888888888888889, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.5347791203530505e-05\n",
            "step: 10, loss: 3.809457120951265e-05\n",
            "step: 20, loss: 0.0002583693712949753\n",
            "step: 30, loss: 0.0022858199663460255\n",
            "step: 40, loss: 6.154160655569285e-05\n",
            "step: 50, loss: 3.633486630860716e-05\n",
            "step: 60, loss: 7.445477967849001e-05\n",
            "step: 70, loss: 9.712432802189142e-05\n",
            "step: 80, loss: 3.590649794205092e-05\n",
            "step: 90, loss: 5.9558475186349824e-05\n",
            "step: 100, loss: 0.0001733774261083454\n",
            "step: 110, loss: 5.210713061387651e-05\n",
            "step: 120, loss: 5.0289661885472015e-05\n",
            "step: 130, loss: 6.066774221835658e-05\n",
            "step: 140, loss: 3.171236676280387e-05\n",
            "step: 150, loss: 4.1054398025153205e-05\n",
            "step: 160, loss: 0.00011460013047326356\n",
            "step: 170, loss: 0.00012346560833975673\n",
            "step: 180, loss: 0.00013212299381848425\n",
            "step: 190, loss: 8.871047612046823e-05\n",
            "step: 200, loss: 0.002077449345961213\n",
            "step: 210, loss: 0.001808093162253499\n",
            "step: 220, loss: 3.367129829712212e-05\n",
            "step: 230, loss: 0.0007155759958550334\n",
            "step: 240, loss: 3.298959927633405e-05\n",
            "step: 250, loss: 0.009622576646506786\n",
            "step: 260, loss: 0.00013461499474942684\n",
            "step: 270, loss: 4.568340227706358e-05\n",
            "step: 280, loss: 0.00012348112068139017\n",
            "step: 290, loss: 0.013424372300505638\n",
            "step: 300, loss: 6.690374721074477e-05\n",
            "step: 310, loss: 2.19527464651037e-05\n",
            "step: 320, loss: 2.1528076104004867e-05\n",
            "step: 330, loss: 4.147016443312168e-05\n",
            "step: 340, loss: 2.6866049665841274e-05\n",
            "step: 350, loss: 0.0003099808527622372\n",
            "step: 360, loss: 0.000140607007779181\n",
            "step: 370, loss: 0.0002892881166189909\n",
            "step: 380, loss: 0.0012452988885343075\n",
            "step: 390, loss: 2.2220949176698923e-05\n",
            "step: 400, loss: 2.1606307200272568e-05\n",
            "step: 410, loss: 4.4427939428715035e-05\n",
            "step: 420, loss: 9.250792209059e-05\n",
            "step: 430, loss: 3.348102472955361e-05\n",
            "step: 440, loss: 0.0037369902711361647\n",
            "step: 450, loss: 3.9214010030264035e-05\n",
            "step: 460, loss: 2.0086481526959687e-05\n",
            "step: 470, loss: 3.420401117182337e-05\n",
            "step: 480, loss: 3.431528602959588e-05\n",
            "step: 490, loss: 0.00014841670054011047\n",
            "step: 500, loss: 0.007499702274799347\n",
            "step: 510, loss: 2.5837951397988945e-05\n",
            "step: 520, loss: 0.00023273189435712993\n",
            "step: 530, loss: 3.032665881619323e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.902671755725191, f1=0.8875, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005899170413613319\n",
            "step: 10, loss: 4.769543738802895e-05\n",
            "step: 20, loss: 8.7280037405435e-05\n",
            "step: 30, loss: 8.046761649893597e-05\n",
            "step: 40, loss: 9.267444693250582e-05\n",
            "step: 50, loss: 4.5954202505527064e-05\n",
            "step: 60, loss: 4.707471816800535e-05\n",
            "step: 70, loss: 2.5022112822625786e-05\n",
            "step: 80, loss: 0.00013065009261481464\n",
            "step: 90, loss: 7.640883995918557e-05\n",
            "step: 100, loss: 8.789185812929645e-05\n",
            "step: 110, loss: 2.3785549274180084e-05\n",
            "step: 120, loss: 5.5874796089483425e-05\n",
            "step: 130, loss: 0.0005772595177404583\n",
            "step: 140, loss: 4.0268245356855914e-05\n",
            "step: 150, loss: 5.3760071750730276e-05\n",
            "step: 160, loss: 0.0003186299873050302\n",
            "step: 170, loss: 0.0006922595202922821\n",
            "step: 180, loss: 3.424768874538131e-05\n",
            "step: 190, loss: 0.00011263177293585613\n",
            "step: 200, loss: 3.1466042855754495e-05\n",
            "step: 210, loss: 7.90377234807238e-05\n",
            "step: 220, loss: 0.00017776561435312033\n",
            "step: 230, loss: 3.945163552998565e-05\n",
            "step: 240, loss: 8.515120862284675e-05\n",
            "step: 250, loss: 2.2530159185407683e-05\n",
            "step: 260, loss: 2.9452765375026502e-05\n",
            "step: 270, loss: 0.0017891849856823683\n",
            "step: 280, loss: 3.646792174549773e-05\n",
            "step: 290, loss: 0.00011237911530770361\n",
            "step: 300, loss: 0.001688729040324688\n",
            "step: 310, loss: 3.18156489811372e-05\n",
            "step: 320, loss: 0.00037942989729344845\n",
            "step: 330, loss: 5.759525447501801e-05\n",
            "step: 340, loss: 3.5631994251161814e-05\n",
            "step: 350, loss: 0.00010051445860881358\n",
            "step: 360, loss: 0.0005257803131826222\n",
            "step: 370, loss: 2.793866951833479e-05\n",
            "step: 380, loss: 2.8202153771417215e-05\n",
            "step: 390, loss: 2.710395710892044e-05\n",
            "step: 400, loss: 0.00014869397273287177\n",
            "step: 410, loss: 0.00015048269415274262\n",
            "step: 420, loss: 1.7948261302080937e-05\n",
            "step: 430, loss: 2.818100074364338e-05\n",
            "step: 440, loss: 2.2808866560808383e-05\n",
            "step: 450, loss: 2.4388953534071334e-05\n",
            "step: 460, loss: 5.115141539135948e-05\n",
            "step: 470, loss: 5.1164402975700796e-05\n",
            "step: 480, loss: 2.64822956523858e-05\n",
            "step: 490, loss: 2.5010822355397977e-05\n",
            "step: 500, loss: 4.0208127757068723e-05\n",
            "step: 510, loss: 3.2992818887578323e-05\n",
            "step: 520, loss: 3.893371831509285e-05\n",
            "step: 530, loss: 3.165613452438265e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9018867924528302, f1=0.8901569186875892, best_f1=0.9010367577756833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.1803658455610275e-05\n",
            "step: 10, loss: 3.4679869713727385e-05\n",
            "step: 20, loss: 3.764824941754341e-05\n",
            "step: 30, loss: 1.917010740726255e-05\n",
            "step: 40, loss: 0.002519866917282343\n",
            "step: 50, loss: 0.00019715684175025672\n",
            "step: 60, loss: 1.966164018085692e-05\n",
            "step: 70, loss: 3.1044677598401904e-05\n",
            "step: 80, loss: 9.341266559204087e-05\n",
            "step: 90, loss: 2.4373337510041893e-05\n",
            "step: 100, loss: 2.6773002900881693e-05\n",
            "step: 110, loss: 0.023883510380983353\n",
            "step: 120, loss: 0.000476348795928061\n",
            "step: 130, loss: 0.00010555365588515997\n",
            "step: 140, loss: 1.5340594472945668e-05\n",
            "step: 150, loss: 0.00012737087672576308\n",
            "step: 160, loss: 3.977943561039865e-05\n",
            "step: 170, loss: 4.336818165029399e-05\n",
            "step: 180, loss: 5.442549809231423e-05\n",
            "step: 190, loss: 4.56758207292296e-05\n",
            "step: 200, loss: 2.9200187782407738e-05\n",
            "step: 210, loss: 2.3595352104166523e-05\n",
            "step: 220, loss: 1.4614192878070753e-05\n",
            "step: 230, loss: 2.955742456833832e-05\n",
            "step: 240, loss: 5.51086341147311e-05\n",
            "step: 250, loss: 4.301282388041727e-05\n",
            "step: 260, loss: 8.454315684502944e-05\n",
            "step: 270, loss: 3.221826773369685e-05\n",
            "step: 280, loss: 0.0007717356784269214\n",
            "step: 290, loss: 2.8854577976744622e-05\n",
            "step: 300, loss: 0.020425403490662575\n",
            "step: 310, loss: 2.753615081019234e-05\n",
            "step: 320, loss: 3.390873462194577e-05\n",
            "step: 330, loss: 0.0029972316697239876\n",
            "step: 340, loss: 0.0008243973716162145\n",
            "step: 350, loss: 4.777988579007797e-05\n",
            "step: 360, loss: 3.0877767130732536e-05\n",
            "step: 370, loss: 1.7881193343782797e-05\n",
            "step: 380, loss: 2.0496001525316387e-05\n",
            "step: 390, loss: 2.000069071073085e-05\n",
            "step: 400, loss: 0.0009195629390887916\n",
            "step: 410, loss: 0.0002105372550431639\n",
            "step: 420, loss: 2.0924671844113618e-05\n",
            "step: 430, loss: 2.609071452752687e-05\n",
            "step: 440, loss: 0.0007093353196978569\n",
            "step: 450, loss: 0.00011124831507913768\n",
            "step: 460, loss: 2.627649882924743e-05\n",
            "step: 470, loss: 0.00012497570423875004\n",
            "step: 480, loss: 2.6145695301238447e-05\n",
            "step: 490, loss: 0.00010252720676362514\n",
            "step: 500, loss: 6.906674389028922e-05\n",
            "step: 510, loss: 0.0004617772647179663\n",
            "step: 520, loss: 3.179049235768616e-05\n",
            "step: 530, loss: 1.796684409782756e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9017175572519084, f1=0.8885658914728681, best_f1=0.9010367577756833\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:14, 395.38it/s]\n",
            "load_f1 = 0.9019788311090658\n",
            "real_f1 = 0.902450300508553\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 388.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9caff95-aeb4-48ab-ce59-ee8b57394714"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8681785464286804\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3111111111111111, f1=0.2857142857142857, best_f1=0.2857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37089288234710693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2857142857142857, f1=0.2745098039215686, best_f1=0.2857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3549487292766571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.27184466019417475, f1=0.2692307692307693, best_f1=0.2857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37196940183639526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.41935483870967744, f1=0.2891566265060241, best_f1=0.2891566265060241\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.275824636220932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.4705882352941177, f1=0.34374999999999994, best_f1=0.34374999999999994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2657514810562134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4444444444444444, f1=0.41379310344827586, best_f1=0.34374999999999994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2273828685283661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.4897959183673469, f1=0.4067796610169491, best_f1=0.4067796610169491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38199383020401\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.48888888888888893, f1=0.42857142857142855, best_f1=0.4067796610169491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22274532914161682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.4727272727272727, f1=0.4745762711864407, best_f1=0.4067796610169491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2553536295890808\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.5217391304347825, f1=0.42857142857142855, best_f1=0.42857142857142855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21962115168571472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.56, f1=0.47058823529411764, best_f1=0.47058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2702135443687439\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.5833333333333334, f1=0.4571428571428571, best_f1=0.4571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14939351379871368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.64, f1=0.47058823529411764, best_f1=0.47058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2216826230287552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.64, f1=0.4571428571428571, best_f1=0.47058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2302975356578827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.64, f1=0.4571428571428571, best_f1=0.47058823529411764\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 114105.13it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5294117647058824\n",
            "real_f1 = 0.5142857142857143\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:10, 424.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a17de25-1c33-4528-a276-17dad09d086a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8008906841278076\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49258244037628174\n",
            "step: 20, loss: 0.6089513301849365\n",
            "step: 30, loss: 0.5105851292610168\n",
            "step: 40, loss: 0.4511651396751404\n",
            "step: 50, loss: 0.3662388324737549\n",
            "step: 60, loss: 0.29804471135139465\n",
            "step: 70, loss: 0.14558997750282288\n",
            "step: 80, loss: 0.07524562627077103\n",
            "step: 90, loss: 0.009958893992006779\n",
            "step: 100, loss: 0.0638984888792038\n",
            "step: 110, loss: 0.010421739891171455\n",
            "step: 120, loss: 0.031301841139793396\n",
            "step: 130, loss: 0.17467671632766724\n",
            "step: 140, loss: 0.03312826529145241\n",
            "step: 150, loss: 0.04910871386528015\n",
            "step: 160, loss: 0.11703725159168243\n",
            "step: 170, loss: 0.021919064223766327\n",
            "step: 180, loss: 0.03741803392767906\n",
            "step: 190, loss: 0.3482377827167511\n",
            "step: 200, loss: 0.030877824872732162\n",
            "step: 210, loss: 0.026674211025238037\n",
            "step: 220, loss: 0.035566989332437515\n",
            "step: 230, loss: 0.037579141557216644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9622222222222223, f1=0.961625282167043, best_f1=0.961625282167043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030298730358481407\n",
            "step: 10, loss: 0.0015709699364379048\n",
            "step: 20, loss: 0.03306726738810539\n",
            "step: 30, loss: 0.004060381092131138\n",
            "step: 40, loss: 0.03387844190001488\n",
            "step: 50, loss: 0.0024172738194465637\n",
            "step: 60, loss: 0.12644165754318237\n",
            "step: 70, loss: 0.02073793113231659\n",
            "step: 80, loss: 0.009692663326859474\n",
            "step: 90, loss: 0.008979114703834057\n",
            "step: 100, loss: 0.03556143119931221\n",
            "step: 110, loss: 0.06333428621292114\n",
            "step: 120, loss: 0.07772713154554367\n",
            "step: 130, loss: 0.049290306866168976\n",
            "step: 140, loss: 0.015474338084459305\n",
            "step: 150, loss: 0.006832794286310673\n",
            "step: 160, loss: 0.01141358818858862\n",
            "step: 170, loss: 0.20973743498325348\n",
            "step: 180, loss: 0.009205206297338009\n",
            "step: 190, loss: 0.0255043376237154\n",
            "step: 200, loss: 0.004261136054992676\n",
            "step: 210, loss: 0.03996874764561653\n",
            "step: 220, loss: 0.0010629236930981278\n",
            "step: 230, loss: 0.1865631639957428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9723145071982282, f1=0.9688888888888889, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07753852009773254\n",
            "step: 10, loss: 0.053970541805028915\n",
            "step: 20, loss: 0.008245579898357391\n",
            "step: 30, loss: 0.013133893720805645\n",
            "step: 40, loss: 0.010669790208339691\n",
            "step: 50, loss: 0.04992109164595604\n",
            "step: 60, loss: 0.02977827377617359\n",
            "step: 70, loss: 0.00371891469694674\n",
            "step: 80, loss: 0.015700364485383034\n",
            "step: 90, loss: 0.025319620966911316\n",
            "step: 100, loss: 0.004651317838579416\n",
            "step: 110, loss: 0.16614404320716858\n",
            "step: 120, loss: 0.021322093904018402\n",
            "step: 130, loss: 0.003283932339400053\n",
            "step: 140, loss: 0.010268252342939377\n",
            "step: 150, loss: 0.0371820405125618\n",
            "step: 160, loss: 0.004760660231113434\n",
            "step: 170, loss: 0.016985571011900902\n",
            "step: 180, loss: 0.007153964601457119\n",
            "step: 190, loss: 0.020263442769646645\n",
            "step: 200, loss: 0.003419114276766777\n",
            "step: 210, loss: 0.04125913977622986\n",
            "step: 220, loss: 0.00678770849481225\n",
            "step: 230, loss: 0.06175587698817253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9689578713968958, f1=0.9689578713968958, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014000978553667665\n",
            "step: 10, loss: 0.0037144639063626528\n",
            "step: 20, loss: 0.0051808543503284454\n",
            "step: 30, loss: 0.0009898346615955234\n",
            "step: 40, loss: 0.004182893317192793\n",
            "step: 50, loss: 0.009851709008216858\n",
            "step: 60, loss: 0.005811662878841162\n",
            "step: 70, loss: 0.017459426075220108\n",
            "step: 80, loss: 0.04452252760529518\n",
            "step: 90, loss: 0.056147608906030655\n",
            "step: 100, loss: 0.02826179377734661\n",
            "step: 110, loss: 0.03340616822242737\n",
            "step: 120, loss: 0.010187164880335331\n",
            "step: 130, loss: 0.003221136052161455\n",
            "step: 140, loss: 0.0025603552348911762\n",
            "step: 150, loss: 0.0007357136928476393\n",
            "step: 160, loss: 0.0035150996409356594\n",
            "step: 170, loss: 0.006125818006694317\n",
            "step: 180, loss: 0.05456704646348953\n",
            "step: 190, loss: 0.006736688315868378\n",
            "step: 200, loss: 0.0023568077012896538\n",
            "step: 210, loss: 0.0008499989053234458\n",
            "step: 220, loss: 0.0014322769129648805\n",
            "step: 230, loss: 0.04239365831017494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9710467706013363, f1=0.9711751662971175, best_f1=0.9688888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002473984844982624\n",
            "step: 10, loss: 0.00519841443747282\n",
            "step: 20, loss: 0.03335786238312721\n",
            "step: 30, loss: 0.07401362806558609\n",
            "step: 40, loss: 0.013196263462305069\n",
            "step: 50, loss: 0.0006622842629440129\n",
            "step: 60, loss: 0.00016290748317260295\n",
            "step: 70, loss: 0.00037193234311416745\n",
            "step: 80, loss: 0.06261610239744186\n",
            "step: 90, loss: 0.023884519934654236\n",
            "step: 100, loss: 0.0100499102845788\n",
            "step: 110, loss: 0.00852509681135416\n",
            "step: 120, loss: 0.06917686760425568\n",
            "step: 130, loss: 0.0338449664413929\n",
            "step: 140, loss: 0.0005391016020439565\n",
            "step: 150, loss: 0.00042534081148914993\n",
            "step: 160, loss: 0.0167087409645319\n",
            "step: 170, loss: 0.12049739062786102\n",
            "step: 180, loss: 0.0042540510185062885\n",
            "step: 190, loss: 0.0005123170558363199\n",
            "step: 200, loss: 0.019625939428806305\n",
            "step: 210, loss: 0.003254529321566224\n",
            "step: 220, loss: 0.004446927923709154\n",
            "step: 230, loss: 0.0005432270118035376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9742441209406495, f1=0.9683972911963882, best_f1=0.9683972911963882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008944649016484618\n",
            "step: 10, loss: 0.001127816503867507\n",
            "step: 20, loss: 0.0005514916847459972\n",
            "step: 30, loss: 0.0004278359701856971\n",
            "step: 40, loss: 0.00772458640858531\n",
            "step: 50, loss: 0.0010999134974554181\n",
            "step: 60, loss: 0.0177697092294693\n",
            "step: 70, loss: 0.0018083018949255347\n",
            "step: 80, loss: 0.00491287000477314\n",
            "step: 90, loss: 0.00033778860233724117\n",
            "step: 100, loss: 0.00010324896720703691\n",
            "step: 110, loss: 0.07444257289171219\n",
            "step: 120, loss: 0.0006485146004706621\n",
            "step: 130, loss: 0.0004471586726140231\n",
            "step: 140, loss: 0.016990557312965393\n",
            "step: 150, loss: 0.0031235527712851763\n",
            "step: 160, loss: 0.01333669014275074\n",
            "step: 170, loss: 0.0005931280320510268\n",
            "step: 180, loss: 0.0013928498374298215\n",
            "step: 190, loss: 0.0064828782342374325\n",
            "step: 200, loss: 0.02022126317024231\n",
            "step: 210, loss: 0.0003319647803436965\n",
            "step: 220, loss: 0.0004084069805685431\n",
            "step: 230, loss: 0.00016558753850404173\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9797752808988766, f1=0.967670011148272, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.044319331645965576\n",
            "step: 10, loss: 0.00021371273032855242\n",
            "step: 20, loss: 0.0005053287604823709\n",
            "step: 30, loss: 0.000874430756084621\n",
            "step: 40, loss: 0.000155607151100412\n",
            "step: 50, loss: 9.087514627026394e-05\n",
            "step: 60, loss: 0.0046426826156675816\n",
            "step: 70, loss: 0.016280358657240868\n",
            "step: 80, loss: 0.009601768106222153\n",
            "step: 90, loss: 0.0010960021754726768\n",
            "step: 100, loss: 0.008327542804181576\n",
            "step: 110, loss: 0.012646033428609371\n",
            "step: 120, loss: 0.0002888813032768667\n",
            "step: 130, loss: 0.0005487108719535172\n",
            "step: 140, loss: 0.0005868448060937226\n",
            "step: 150, loss: 0.0016238511307165027\n",
            "step: 160, loss: 0.0002330341812921688\n",
            "step: 170, loss: 0.00016079883789643645\n",
            "step: 180, loss: 0.00924212671816349\n",
            "step: 190, loss: 0.021623268723487854\n",
            "step: 200, loss: 0.01174146868288517\n",
            "step: 210, loss: 0.00013605302956420928\n",
            "step: 220, loss: 0.000783480703830719\n",
            "step: 230, loss: 0.024310756474733353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9698996655518396, f1=0.9665924276169264, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0325370617210865\n",
            "step: 10, loss: 0.03208096697926521\n",
            "step: 20, loss: 0.00018091057427227497\n",
            "step: 30, loss: 0.00012456387048587203\n",
            "step: 40, loss: 0.0004758075810968876\n",
            "step: 50, loss: 0.0002413627371424809\n",
            "step: 60, loss: 7.421285408781841e-05\n",
            "step: 70, loss: 0.00019996306218672544\n",
            "step: 80, loss: 0.0015162023482844234\n",
            "step: 90, loss: 0.0008046245202422142\n",
            "step: 100, loss: 7.202091364888474e-05\n",
            "step: 110, loss: 0.006502984557300806\n",
            "step: 120, loss: 0.000335335178533569\n",
            "step: 130, loss: 0.01622764579951763\n",
            "step: 140, loss: 8.844503463478759e-05\n",
            "step: 150, loss: 0.0624002106487751\n",
            "step: 160, loss: 0.0008382551604881883\n",
            "step: 170, loss: 9.632504952605814e-05\n",
            "step: 180, loss: 0.013408687897026539\n",
            "step: 190, loss: 0.0016336950939148664\n",
            "step: 200, loss: 0.001495201955549419\n",
            "step: 210, loss: 7.986713171703741e-05\n",
            "step: 220, loss: 0.0022520185448229313\n",
            "step: 230, loss: 0.023109711706638336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9785794813979707, f1=0.9665924276169264, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026096946094185114\n",
            "step: 10, loss: 8.779771451372653e-05\n",
            "step: 20, loss: 0.0003411069919820875\n",
            "step: 30, loss: 0.0005756299360655248\n",
            "step: 40, loss: 0.003540599951520562\n",
            "step: 50, loss: 0.00013592773757409304\n",
            "step: 60, loss: 0.00011205569899175316\n",
            "step: 70, loss: 0.0011088326573371887\n",
            "step: 80, loss: 0.04752138629555702\n",
            "step: 90, loss: 0.007669596932828426\n",
            "step: 100, loss: 0.0001122577377827838\n",
            "step: 110, loss: 0.0007560760132037103\n",
            "step: 120, loss: 0.013500125147402287\n",
            "step: 130, loss: 0.00010399722668807954\n",
            "step: 140, loss: 0.003146954346448183\n",
            "step: 150, loss: 7.455705053871498e-05\n",
            "step: 160, loss: 0.004595446866005659\n",
            "step: 170, loss: 0.000818534754216671\n",
            "step: 180, loss: 0.010446294210851192\n",
            "step: 190, loss: 6.017863779561594e-05\n",
            "step: 200, loss: 0.0005618135910481215\n",
            "step: 210, loss: 0.0046147918328642845\n",
            "step: 220, loss: 0.025565307587385178\n",
            "step: 230, loss: 0.00546693429350853\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9753914988814317, f1=0.9656699889258028, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021256848704069853\n",
            "step: 10, loss: 8.5852800111752e-05\n",
            "step: 20, loss: 0.0478050522506237\n",
            "step: 30, loss: 0.001396358828060329\n",
            "step: 40, loss: 0.006119458936154842\n",
            "step: 50, loss: 0.015329397283494473\n",
            "step: 60, loss: 0.0004948206478729844\n",
            "step: 70, loss: 0.0001482867228332907\n",
            "step: 80, loss: 0.023825079202651978\n",
            "step: 90, loss: 0.004053385928273201\n",
            "step: 100, loss: 0.0002089149784296751\n",
            "step: 110, loss: 0.00029015069594606757\n",
            "step: 120, loss: 0.012830853462219238\n",
            "step: 130, loss: 0.002182260388508439\n",
            "step: 140, loss: 0.005266754422336817\n",
            "step: 150, loss: 0.0013516958570107818\n",
            "step: 160, loss: 0.00038342594052664936\n",
            "step: 170, loss: 0.001617770059965551\n",
            "step: 180, loss: 0.0006917246500961483\n",
            "step: 190, loss: 0.000144786637974903\n",
            "step: 200, loss: 0.00046497059520334005\n",
            "step: 210, loss: 0.0002981375146191567\n",
            "step: 220, loss: 0.05180314928293228\n",
            "step: 230, loss: 6.0506994486786425e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9741863075196409, f1=0.9698996655518396, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.691362068522722e-05\n",
            "step: 10, loss: 0.00032189980265684426\n",
            "step: 20, loss: 0.00015757459914311767\n",
            "step: 30, loss: 0.0027918301057070494\n",
            "step: 40, loss: 0.0029398060869425535\n",
            "step: 50, loss: 0.0007259403355419636\n",
            "step: 60, loss: 0.00438049016520381\n",
            "step: 70, loss: 0.12964454293251038\n",
            "step: 80, loss: 0.00018410268239676952\n",
            "step: 90, loss: 0.00011987437756033614\n",
            "step: 100, loss: 0.00020658536232076585\n",
            "step: 110, loss: 4.348342554294504e-05\n",
            "step: 120, loss: 0.0014444557018578053\n",
            "step: 130, loss: 0.0002599285217002034\n",
            "step: 140, loss: 6.975572614464909e-05\n",
            "step: 150, loss: 5.2483341278275475e-05\n",
            "step: 160, loss: 8.899635577108711e-05\n",
            "step: 170, loss: 0.004372553899884224\n",
            "step: 180, loss: 0.002142789773643017\n",
            "step: 190, loss: 0.010551055893301964\n",
            "step: 200, loss: 0.0011920193210244179\n",
            "step: 210, loss: 4.695697862189263e-05\n",
            "step: 220, loss: 0.00019824603805318475\n",
            "step: 230, loss: 0.02063286490738392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9753914988814317, f1=0.96875, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011547288158908486\n",
            "step: 10, loss: 0.0001322521857218817\n",
            "step: 20, loss: 5.4366046242648736e-05\n",
            "step: 30, loss: 0.0005177740240469575\n",
            "step: 40, loss: 5.953924483037554e-05\n",
            "step: 50, loss: 9.253650932805613e-05\n",
            "step: 60, loss: 0.001286118756979704\n",
            "step: 70, loss: 0.0018621193012222648\n",
            "step: 80, loss: 9.472314559388906e-05\n",
            "step: 90, loss: 3.0338071155711077e-05\n",
            "step: 100, loss: 0.00025342925800941885\n",
            "step: 110, loss: 0.0055036162957549095\n",
            "step: 120, loss: 0.0002806222182698548\n",
            "step: 130, loss: 0.00044442559010349214\n",
            "step: 140, loss: 5.6387565564364195e-05\n",
            "step: 150, loss: 0.00011190425720997155\n",
            "step: 160, loss: 0.02249736152589321\n",
            "step: 170, loss: 4.715430623036809e-05\n",
            "step: 180, loss: 3.877437120536342e-05\n",
            "step: 190, loss: 0.0013211960904300213\n",
            "step: 200, loss: 0.004711721558123827\n",
            "step: 210, loss: 2.946645145129878e-05\n",
            "step: 220, loss: 7.755469414405525e-05\n",
            "step: 230, loss: 3.710640157805756e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9740698985343857, f1=0.970917225950783, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020273257978260517\n",
            "step: 10, loss: 7.034639565972611e-05\n",
            "step: 20, loss: 4.300966247683391e-05\n",
            "step: 30, loss: 0.008791731670498848\n",
            "step: 40, loss: 6.442174344556406e-05\n",
            "step: 50, loss: 3.414526509004645e-05\n",
            "step: 60, loss: 0.0001264770544366911\n",
            "step: 70, loss: 3.501311221043579e-05\n",
            "step: 80, loss: 2.9187069230829366e-05\n",
            "step: 90, loss: 5.877812873222865e-05\n",
            "step: 100, loss: 0.00011764825467253104\n",
            "step: 110, loss: 7.411031401716173e-05\n",
            "step: 120, loss: 8.998569683171809e-05\n",
            "step: 130, loss: 0.00020616903202608228\n",
            "step: 140, loss: 0.000739079259801656\n",
            "step: 150, loss: 0.004006121773272753\n",
            "step: 160, loss: 3.9090733480406925e-05\n",
            "step: 170, loss: 4.576524588628672e-05\n",
            "step: 180, loss: 0.0002242780610686168\n",
            "step: 190, loss: 3.97396543121431e-05\n",
            "step: 200, loss: 0.00012532496475614607\n",
            "step: 210, loss: 0.000236681240494363\n",
            "step: 220, loss: 0.0002056544180959463\n",
            "step: 230, loss: 0.00035715196281671524\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9743589743589743, f1=0.9700996677740864, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.837024845182896e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.00030826020520180464\n",
            "step: 20, loss: 0.0005661172908730805\n",
            "step: 30, loss: 4.48876635346096e-05\n",
            "step: 40, loss: 2.609147850307636e-05\n",
            "step: 50, loss: 9.728903387440369e-05\n",
            "step: 60, loss: 6.583380309166387e-05\n",
            "step: 70, loss: 0.004341835156083107\n",
            "step: 80, loss: 2.8650552849285305e-05\n",
            "step: 90, loss: 8.699100726516917e-05\n",
            "step: 100, loss: 0.0019140300573781133\n",
            "step: 110, loss: 6.821937131462619e-05\n",
            "step: 120, loss: 0.001143977278843522\n",
            "step: 130, loss: 8.355638419743627e-05\n",
            "step: 140, loss: 0.00030842673731967807\n",
            "step: 150, loss: 3.907270365743898e-05\n",
            "step: 160, loss: 9.637501352699474e-05\n",
            "step: 170, loss: 4.94400410389062e-05\n",
            "step: 180, loss: 0.00012477970449253917\n",
            "step: 190, loss: 0.0002850544115062803\n",
            "step: 200, loss: 5.03504961670842e-05\n",
            "step: 210, loss: 0.0009960309835150838\n",
            "step: 220, loss: 7.248005567817017e-05\n",
            "step: 230, loss: 3.4638513170648366e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9732739420935412, f1=0.9711751662971175, best_f1=0.967670011148272\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.5279186704428867e-05\n",
            "step: 10, loss: 0.0001488558918936178\n",
            "step: 20, loss: 4.135813287575729e-05\n",
            "step: 30, loss: 3.1407307687914e-05\n",
            "step: 40, loss: 3.0226272428990342e-05\n",
            "step: 50, loss: 0.0006989146932028234\n",
            "step: 60, loss: 4.0603084926260635e-05\n",
            "step: 70, loss: 0.00015455445100087672\n",
            "step: 80, loss: 5.370835424400866e-05\n",
            "step: 90, loss: 3.9956721593625844e-05\n",
            "step: 100, loss: 4.5474916987586766e-05\n",
            "step: 110, loss: 3.184664456057362e-05\n",
            "step: 120, loss: 6.985862273722887e-05\n",
            "step: 130, loss: 0.0004181296972092241\n",
            "step: 140, loss: 0.00017001802916638553\n",
            "step: 150, loss: 0.03254682943224907\n",
            "step: 160, loss: 8.204958430724218e-05\n",
            "step: 170, loss: 2.6579447876429185e-05\n",
            "step: 180, loss: 4.859453474637121e-05\n",
            "step: 190, loss: 0.00013708621554542333\n",
            "step: 200, loss: 7.396860746666789e-05\n",
            "step: 210, loss: 0.010343484580516815\n",
            "step: 220, loss: 3.931035098503344e-05\n",
            "step: 230, loss: 0.00011814026947831735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9755011135857461, f1=0.9722530521642618, best_f1=0.967670011148272\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:07, 338.28it/s]\n",
            "load_f1 = 0.9797752808988766\n",
            "real_f1 = 0.9733333333333333\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:10, 417.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34222bc6-a261-4458-a0a6-fd13495ebf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7962208390235901\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.45240339636802673\n",
            "step: 20, loss: 0.5146725177764893\n",
            "step: 30, loss: 0.4641241133213043\n",
            "step: 40, loss: 0.4770752489566803\n",
            "step: 50, loss: 0.4327768385410309\n",
            "step: 60, loss: 0.49620193243026733\n",
            "step: 70, loss: 0.18784698843955994\n",
            "step: 80, loss: 0.12983889877796173\n",
            "step: 90, loss: 0.17973379790782928\n",
            "step: 100, loss: 0.266169011592865\n",
            "step: 110, loss: 0.06025358662009239\n",
            "step: 120, loss: 0.10096290707588196\n",
            "step: 130, loss: 0.06180498003959656\n",
            "step: 140, loss: 0.296170175075531\n",
            "step: 150, loss: 0.07345665991306305\n",
            "step: 160, loss: 0.23509329557418823\n",
            "step: 170, loss: 0.4540939927101135\n",
            "step: 180, loss: 0.12940283119678497\n",
            "step: 190, loss: 0.08643011003732681\n",
            "step: 200, loss: 0.10837559401988983\n",
            "step: 210, loss: 0.10077060014009476\n",
            "step: 220, loss: 0.1304081529378891\n",
            "step: 230, loss: 0.12837183475494385\n",
            "step: 240, loss: 0.10688367486000061\n",
            "step: 250, loss: 0.0881446897983551\n",
            "step: 260, loss: 0.06964649260044098\n",
            "step: 270, loss: 0.07021352648735046\n",
            "step: 280, loss: 0.14160357415676117\n",
            "step: 290, loss: 0.12100572884082794\n",
            "step: 300, loss: 0.17226368188858032\n",
            "step: 310, loss: 0.1686905473470688\n",
            "step: 320, loss: 0.1602027416229248\n",
            "step: 330, loss: 0.18323291838169098\n",
            "step: 340, loss: 0.2626543939113617\n",
            "step: 350, loss: 0.166444331407547\n",
            "step: 360, loss: 0.09450706839561462\n",
            "step: 370, loss: 0.14333681762218475\n",
            "step: 380, loss: 0.09005676954984665\n",
            "step: 390, loss: 0.1769273430109024\n",
            "step: 400, loss: 0.030528325587511063\n",
            "step: 410, loss: 0.11689458787441254\n",
            "step: 420, loss: 0.03779524937272072\n",
            "step: 430, loss: 0.08277522027492523\n",
            "step: 440, loss: 0.2795688509941101\n",
            "step: 450, loss: 0.04352149739861488\n",
            "step: 460, loss: 0.03861450403928757\n",
            "step: 470, loss: 0.2723231613636017\n",
            "step: 480, loss: 0.23819302022457123\n",
            "step: 490, loss: 0.18933254480361938\n",
            "step: 500, loss: 0.03552057966589928\n",
            "step: 510, loss: 0.07346659898757935\n",
            "step: 520, loss: 0.12829819321632385\n",
            "step: 530, loss: 0.2761383056640625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8999538958045182, f1=0.8901200369344413, best_f1=0.8901200369344413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14251986145973206\n",
            "step: 10, loss: 0.062065042555332184\n",
            "step: 20, loss: 0.13641692698001862\n",
            "step: 30, loss: 0.09132867306470871\n",
            "step: 40, loss: 0.029878711327910423\n",
            "step: 50, loss: 0.08046898245811462\n",
            "step: 60, loss: 0.13707831501960754\n",
            "step: 70, loss: 0.19255860149860382\n",
            "step: 80, loss: 0.011465254239737988\n",
            "step: 90, loss: 0.04843004420399666\n",
            "step: 100, loss: 0.3062833249568939\n",
            "step: 110, loss: 0.08812291920185089\n",
            "step: 120, loss: 0.07916849851608276\n",
            "step: 130, loss: 0.05000113323330879\n",
            "step: 140, loss: 0.02877311035990715\n",
            "step: 150, loss: 0.13320337235927582\n",
            "step: 160, loss: 0.23096147179603577\n",
            "step: 170, loss: 0.024058327078819275\n",
            "step: 180, loss: 0.029267892241477966\n",
            "step: 190, loss: 0.08212241530418396\n",
            "step: 200, loss: 0.1698990762233734\n",
            "step: 210, loss: 0.05398376286029816\n",
            "step: 220, loss: 0.16005678474903107\n",
            "step: 230, loss: 0.16152329742908478\n",
            "step: 240, loss: 0.11311998218297958\n",
            "step: 250, loss: 0.06328899413347244\n",
            "step: 260, loss: 0.03432203829288483\n",
            "step: 270, loss: 0.14704681932926178\n",
            "step: 280, loss: 0.18631580471992493\n",
            "step: 290, loss: 0.1001850962638855\n",
            "step: 300, loss: 0.07798206806182861\n",
            "step: 310, loss: 0.13322687149047852\n",
            "step: 320, loss: 0.13543754816055298\n",
            "step: 330, loss: 0.06500188261270523\n",
            "step: 340, loss: 0.015757372602820396\n",
            "step: 350, loss: 0.08654693514108658\n",
            "step: 360, loss: 0.15829506516456604\n",
            "step: 370, loss: 0.027066944167017937\n",
            "step: 380, loss: 0.10826614499092102\n",
            "step: 390, loss: 0.11051066219806671\n",
            "step: 400, loss: 0.08767487108707428\n",
            "step: 410, loss: 0.008579627610743046\n",
            "step: 420, loss: 0.05316021665930748\n",
            "step: 430, loss: 0.057026248425245285\n",
            "step: 440, loss: 0.017977671697735786\n",
            "step: 450, loss: 0.130628764629364\n",
            "step: 460, loss: 0.16051341593265533\n",
            "step: 470, loss: 0.043825797736644745\n",
            "step: 480, loss: 0.3169213533401489\n",
            "step: 490, loss: 0.1049647256731987\n",
            "step: 500, loss: 0.0449383519589901\n",
            "step: 510, loss: 0.1493045687675476\n",
            "step: 520, loss: 0.06762092560529709\n",
            "step: 530, loss: 0.17019535601139069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9057318806252961, f1=0.890057361376673, best_f1=0.890057361376673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04110715165734291\n",
            "step: 10, loss: 0.19656851887702942\n",
            "step: 20, loss: 0.11372288316488266\n",
            "step: 30, loss: 0.2728493809700012\n",
            "step: 40, loss: 0.03351106867194176\n",
            "step: 50, loss: 0.06234405189752579\n",
            "step: 60, loss: 0.012086108326911926\n",
            "step: 70, loss: 0.006936792284250259\n",
            "step: 80, loss: 0.22105184197425842\n",
            "step: 90, loss: 0.20003941655158997\n",
            "step: 100, loss: 0.0209360271692276\n",
            "step: 110, loss: 0.06888946145772934\n",
            "step: 120, loss: 0.09694348275661469\n",
            "step: 130, loss: 0.06270432472229004\n",
            "step: 140, loss: 0.02233877032995224\n",
            "step: 150, loss: 0.0785071924328804\n",
            "step: 160, loss: 0.0248276237398386\n",
            "step: 170, loss: 0.03848734498023987\n",
            "step: 180, loss: 0.028406841680407524\n",
            "step: 190, loss: 0.00473262881860137\n",
            "step: 200, loss: 0.05245091766119003\n",
            "step: 210, loss: 0.06774582713842392\n",
            "step: 220, loss: 0.015371538698673248\n",
            "step: 230, loss: 0.010242470540106297\n",
            "step: 240, loss: 0.008870039135217667\n",
            "step: 250, loss: 0.030476344749331474\n",
            "step: 260, loss: 0.023625267669558525\n",
            "step: 270, loss: 0.003828353714197874\n",
            "step: 280, loss: 0.07936891913414001\n",
            "step: 290, loss: 0.1755920648574829\n",
            "step: 300, loss: 0.0477004311978817\n",
            "step: 310, loss: 0.23543108999729156\n",
            "step: 320, loss: 0.06601742655038834\n",
            "step: 330, loss: 0.015656061470508575\n",
            "step: 340, loss: 0.00805297028273344\n",
            "step: 350, loss: 0.1305793970823288\n",
            "step: 360, loss: 0.008107698522508144\n",
            "step: 370, loss: 0.015508070588111877\n",
            "step: 380, loss: 0.06352129578590393\n",
            "step: 390, loss: 0.04187529906630516\n",
            "step: 400, loss: 0.02021603286266327\n",
            "step: 410, loss: 0.09517636895179749\n",
            "step: 420, loss: 0.06576332449913025\n",
            "step: 430, loss: 0.036433879286050797\n",
            "step: 440, loss: 0.11432290822267532\n",
            "step: 450, loss: 0.1734548658132553\n",
            "step: 460, loss: 0.14305202662944794\n",
            "step: 470, loss: 0.01315336674451828\n",
            "step: 480, loss: 0.015126921236515045\n",
            "step: 490, loss: 0.02473350614309311\n",
            "step: 500, loss: 0.06693438440561295\n",
            "step: 510, loss: 0.009074845351278782\n",
            "step: 520, loss: 0.010306923650205135\n",
            "step: 530, loss: 0.010773374699056149\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9063670411985018, f1=0.9027454630060493, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0073103224858641624\n",
            "step: 10, loss: 0.003320542396977544\n",
            "step: 20, loss: 0.1362529993057251\n",
            "step: 30, loss: 0.06551088392734528\n",
            "step: 40, loss: 0.056882183998823166\n",
            "step: 50, loss: 0.02853415720164776\n",
            "step: 60, loss: 0.004501865711063147\n",
            "step: 70, loss: 0.005438623484224081\n",
            "step: 80, loss: 0.013039987534284592\n",
            "step: 90, loss: 0.10791052877902985\n",
            "step: 100, loss: 0.052274417132139206\n",
            "step: 110, loss: 0.007742627523839474\n",
            "step: 120, loss: 0.03046443685889244\n",
            "step: 130, loss: 0.015626147389411926\n",
            "step: 140, loss: 0.017594151198863983\n",
            "step: 150, loss: 0.053912024945020676\n",
            "step: 160, loss: 0.006508626043796539\n",
            "step: 170, loss: 0.003856698516756296\n",
            "step: 180, loss: 0.03766312450170517\n",
            "step: 190, loss: 0.02809026651084423\n",
            "step: 200, loss: 0.012429637834429741\n",
            "step: 210, loss: 0.06681369990110397\n",
            "step: 220, loss: 0.024209247902035713\n",
            "step: 230, loss: 0.32829877734184265\n",
            "step: 240, loss: 0.015343692153692245\n",
            "step: 250, loss: 0.0037957525346428156\n",
            "step: 260, loss: 0.08274944871664047\n",
            "step: 270, loss: 0.18230803310871124\n",
            "step: 280, loss: 0.0021089077927172184\n",
            "step: 290, loss: 0.060283221304416656\n",
            "step: 300, loss: 0.003284615697339177\n",
            "step: 310, loss: 0.017286261543631554\n",
            "step: 320, loss: 0.07137191295623779\n",
            "step: 330, loss: 0.18019449710845947\n",
            "step: 340, loss: 0.15743137896060944\n",
            "step: 350, loss: 0.014208676293492317\n",
            "step: 360, loss: 0.06381186097860336\n",
            "step: 370, loss: 0.036725401878356934\n",
            "step: 380, loss: 0.029105307534337044\n",
            "step: 390, loss: 0.05147971957921982\n",
            "step: 400, loss: 0.07453811168670654\n",
            "step: 410, loss: 0.08119925111532211\n",
            "step: 420, loss: 0.04888863116502762\n",
            "step: 430, loss: 0.014295362867414951\n",
            "step: 440, loss: 0.0682268813252449\n",
            "step: 450, loss: 0.22595623135566711\n",
            "step: 460, loss: 0.003704611910507083\n",
            "step: 470, loss: 0.02559526264667511\n",
            "step: 480, loss: 0.020226871594786644\n",
            "step: 490, loss: 0.034705307334661484\n",
            "step: 500, loss: 0.018414093181490898\n",
            "step: 510, loss: 0.14736175537109375\n",
            "step: 520, loss: 0.11593315005302429\n",
            "step: 530, loss: 0.0038595865480601788\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9023474178403756, f1=0.8940737284181055, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02698279730975628\n",
            "step: 10, loss: 0.02216852642595768\n",
            "step: 20, loss: 0.023715874180197716\n",
            "step: 30, loss: 0.00909302569925785\n",
            "step: 40, loss: 0.11720822751522064\n",
            "step: 50, loss: 0.09556514769792557\n",
            "step: 60, loss: 0.0036565472837537527\n",
            "step: 70, loss: 0.06978994607925415\n",
            "step: 80, loss: 0.006877817213535309\n",
            "step: 90, loss: 0.002764668082818389\n",
            "step: 100, loss: 0.0015421593561768532\n",
            "step: 110, loss: 0.0028273207135498524\n",
            "step: 120, loss: 0.0018124980852007866\n",
            "step: 130, loss: 0.003443677444010973\n",
            "step: 140, loss: 0.006465388461947441\n",
            "step: 150, loss: 0.15316110849380493\n",
            "step: 160, loss: 0.00596837792545557\n",
            "step: 170, loss: 0.009327967651188374\n",
            "step: 180, loss: 0.0019358759745955467\n",
            "step: 190, loss: 0.007297426927834749\n",
            "step: 200, loss: 0.013115284033119678\n",
            "step: 210, loss: 0.051278356462717056\n",
            "step: 220, loss: 0.0018647693796083331\n",
            "step: 230, loss: 0.0012189610861241817\n",
            "step: 240, loss: 0.04734230414032936\n",
            "step: 250, loss: 0.0008721953490749002\n",
            "step: 260, loss: 0.03878113254904747\n",
            "step: 270, loss: 0.03758919611573219\n",
            "step: 280, loss: 0.02512168511748314\n",
            "step: 290, loss: 0.001664129551500082\n",
            "step: 300, loss: 0.16630595922470093\n",
            "step: 310, loss: 0.006214917171746492\n",
            "step: 320, loss: 0.00994543544948101\n",
            "step: 330, loss: 0.021349657326936722\n",
            "step: 340, loss: 0.017026619985699654\n",
            "step: 350, loss: 0.020230643451213837\n",
            "step: 360, loss: 0.06483243405818939\n",
            "step: 370, loss: 0.008596659637987614\n",
            "step: 380, loss: 0.06367364525794983\n",
            "step: 390, loss: 0.01079567614942789\n",
            "step: 400, loss: 0.012964032590389252\n",
            "step: 410, loss: 0.003305052639916539\n",
            "step: 420, loss: 0.005487170536071062\n",
            "step: 430, loss: 0.0015695802867412567\n",
            "step: 440, loss: 0.03652261570096016\n",
            "step: 450, loss: 0.067457415163517\n",
            "step: 460, loss: 0.020720968022942543\n",
            "step: 470, loss: 0.008232245221734047\n",
            "step: 480, loss: 0.0012066466733813286\n",
            "step: 490, loss: 0.0016438113525509834\n",
            "step: 500, loss: 0.01408644299954176\n",
            "step: 510, loss: 0.06012646108865738\n",
            "step: 520, loss: 0.006216266192495823\n",
            "step: 530, loss: 0.005175726022571325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9031665901789813, f1=0.9010584445467096, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004680450540035963\n",
            "step: 10, loss: 0.005016134586185217\n",
            "step: 20, loss: 0.00032884228858165443\n",
            "step: 30, loss: 0.000684837403241545\n",
            "step: 40, loss: 0.005414573475718498\n",
            "step: 50, loss: 0.012345099821686745\n",
            "step: 60, loss: 0.0005644093616865575\n",
            "step: 70, loss: 0.056608397513628006\n",
            "step: 80, loss: 0.03503487631678581\n",
            "step: 90, loss: 0.019852666184306145\n",
            "step: 100, loss: 0.01911591738462448\n",
            "step: 110, loss: 0.013340656645596027\n",
            "step: 120, loss: 0.013264870271086693\n",
            "step: 130, loss: 0.020321620628237724\n",
            "step: 140, loss: 0.00477188965305686\n",
            "step: 150, loss: 0.005033575929701328\n",
            "step: 160, loss: 0.04268990829586983\n",
            "step: 170, loss: 0.0006977989105507731\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 180, loss: 0.004151505883783102\n",
            "step: 190, loss: 0.0005564783350564539\n",
            "step: 200, loss: 0.001245860941708088\n",
            "step: 210, loss: 0.012972329743206501\n",
            "step: 220, loss: 0.0006872020312584937\n",
            "step: 230, loss: 0.006425303407013416\n",
            "step: 240, loss: 0.12209590524435043\n",
            "step: 250, loss: 0.013733088970184326\n",
            "step: 260, loss: 0.0030146820936352015\n",
            "step: 270, loss: 0.0029966377187520266\n",
            "step: 280, loss: 0.02782120183110237\n",
            "step: 290, loss: 0.006555716507136822\n",
            "step: 300, loss: 0.01898670196533203\n",
            "step: 310, loss: 0.03805257007479668\n",
            "step: 320, loss: 0.20741751790046692\n",
            "step: 330, loss: 0.00872295256704092\n",
            "step: 340, loss: 0.013545106165111065\n",
            "step: 350, loss: 0.12325219064950943\n",
            "step: 360, loss: 0.003114728257060051\n",
            "step: 370, loss: 0.011891528032720089\n",
            "step: 380, loss: 0.004123717080801725\n",
            "step: 390, loss: 0.1396486461162567\n",
            "step: 400, loss: 0.0011583009036257863\n",
            "step: 410, loss: 0.0012581013143062592\n",
            "step: 420, loss: 0.10716002434492111\n",
            "step: 430, loss: 0.04217642545700073\n",
            "step: 440, loss: 0.08671144396066666\n",
            "step: 450, loss: 0.014297859743237495\n",
            "step: 460, loss: 0.04002038389444351\n",
            "step: 470, loss: 0.12639039754867554\n",
            "step: 480, loss: 0.005964160431176424\n",
            "step: 490, loss: 0.014423368498682976\n",
            "step: 500, loss: 0.008271818980574608\n",
            "step: 510, loss: 0.0007389733218587935\n",
            "step: 520, loss: 0.03153461590409279\n",
            "step: 530, loss: 0.0017379054334014654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8995305164319248, f1=0.8900424728645587, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06159514933824539\n",
            "step: 10, loss: 0.016702065244317055\n",
            "step: 20, loss: 0.002920126775279641\n",
            "step: 30, loss: 0.04378432780504227\n",
            "step: 40, loss: 0.005719680339097977\n",
            "step: 50, loss: 0.008100222796201706\n",
            "step: 60, loss: 0.011897657066583633\n",
            "step: 70, loss: 0.02185901068150997\n",
            "step: 80, loss: 0.0157729871571064\n",
            "step: 90, loss: 0.004510801285505295\n",
            "step: 100, loss: 0.005952063482254744\n",
            "step: 110, loss: 0.0033148303627967834\n",
            "step: 120, loss: 0.021519910544157028\n",
            "step: 130, loss: 0.07720110565423965\n",
            "step: 140, loss: 0.09719906002283096\n",
            "step: 150, loss: 0.0033562686294317245\n",
            "step: 160, loss: 0.0002952925569843501\n",
            "step: 170, loss: 0.0008321120985783637\n",
            "step: 180, loss: 0.006510838866233826\n",
            "step: 190, loss: 0.0022852581460028887\n",
            "step: 200, loss: 0.002340524224564433\n",
            "step: 210, loss: 0.0036753315944224596\n",
            "step: 220, loss: 0.0012753314804285765\n",
            "step: 230, loss: 0.0019734669476747513\n",
            "step: 240, loss: 0.008456462062895298\n",
            "step: 250, loss: 0.005370312836021185\n",
            "step: 260, loss: 0.001996066654101014\n",
            "step: 270, loss: 0.0023446609266102314\n",
            "step: 280, loss: 0.006191902328282595\n",
            "step: 290, loss: 0.0021884306333959103\n",
            "step: 300, loss: 0.002486068056896329\n",
            "step: 310, loss: 0.0015791055047884583\n",
            "step: 320, loss: 0.014707997441291809\n",
            "step: 330, loss: 0.13361482322216034\n",
            "step: 340, loss: 0.023520637303590775\n",
            "step: 350, loss: 0.00040344733861275017\n",
            "step: 360, loss: 0.003710536053404212\n",
            "step: 370, loss: 0.08191792666912079\n",
            "step: 380, loss: 0.01976863667368889\n",
            "step: 390, loss: 0.0024652364663779736\n",
            "step: 400, loss: 0.05233769491314888\n",
            "step: 410, loss: 0.007110987789928913\n",
            "step: 420, loss: 0.0006584431394003332\n",
            "step: 430, loss: 0.0006233700551092625\n",
            "step: 440, loss: 0.0035204661544412374\n",
            "step: 450, loss: 0.0008708798559382558\n",
            "step: 460, loss: 0.03820662945508957\n",
            "step: 470, loss: 0.25860655307769775\n",
            "step: 480, loss: 0.0009085287456400692\n",
            "step: 490, loss: 0.00238681398332119\n",
            "step: 500, loss: 0.0003937946748919785\n",
            "step: 510, loss: 0.005973976105451584\n",
            "step: 520, loss: 0.18858025968074799\n",
            "step: 530, loss: 0.0037424226757138968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.896030245746692, f1=0.8959700093720713, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003045530989766121\n",
            "step: 10, loss: 0.08257834613323212\n",
            "step: 20, loss: 0.004759186413139105\n",
            "step: 30, loss: 0.001530241221189499\n",
            "step: 40, loss: 0.007739974185824394\n",
            "step: 50, loss: 0.000577569764573127\n",
            "step: 60, loss: 0.0003422510053496808\n",
            "step: 70, loss: 0.0013914515729993582\n",
            "step: 80, loss: 0.0003819323319476098\n",
            "step: 90, loss: 0.00015888364578131586\n",
            "step: 100, loss: 0.07398992031812668\n",
            "step: 110, loss: 0.0009453541715629399\n",
            "step: 120, loss: 0.011630724184215069\n",
            "step: 130, loss: 0.001715713762678206\n",
            "step: 140, loss: 0.00010081783693749458\n",
            "step: 150, loss: 0.0004342882602941245\n",
            "step: 160, loss: 0.0005539903650060296\n",
            "step: 170, loss: 0.00020794356532860547\n",
            "step: 180, loss: 0.0002515890228096396\n",
            "step: 190, loss: 0.0012573577696457505\n",
            "step: 200, loss: 0.0038430076092481613\n",
            "step: 210, loss: 0.0012138454476371408\n",
            "step: 220, loss: 0.0035013232845813036\n",
            "step: 230, loss: 0.00019136314222123474\n",
            "step: 240, loss: 0.11232120543718338\n",
            "step: 250, loss: 0.0010306148324161768\n",
            "step: 260, loss: 0.13777075707912445\n",
            "step: 270, loss: 0.000493975356221199\n",
            "step: 280, loss: 0.01423371396958828\n",
            "step: 290, loss: 0.0020494554191827774\n",
            "step: 300, loss: 0.003177644219249487\n",
            "step: 310, loss: 0.011287147179245949\n",
            "step: 320, loss: 0.0011347121326252818\n",
            "step: 330, loss: 0.047149986028671265\n",
            "step: 340, loss: 0.00024079506692942232\n",
            "step: 350, loss: 0.02425096370279789\n",
            "step: 360, loss: 0.01814117096364498\n",
            "step: 370, loss: 0.0049249534495174885\n",
            "step: 380, loss: 0.002232924336567521\n",
            "step: 390, loss: 0.0009366340236738324\n",
            "step: 400, loss: 0.007889585569500923\n",
            "step: 410, loss: 0.01421884074807167\n",
            "step: 420, loss: 0.00029136062948964536\n",
            "step: 430, loss: 0.00034823009627871215\n",
            "step: 440, loss: 0.0022088238038122654\n",
            "step: 450, loss: 0.005313721019774675\n",
            "step: 460, loss: 0.0032594462390989065\n",
            "step: 470, loss: 0.0009673749445937574\n",
            "step: 480, loss: 0.0009797756792977452\n",
            "step: 490, loss: 0.00027085954206995666\n",
            "step: 500, loss: 0.0017327879322692752\n",
            "step: 510, loss: 0.0034588815178722143\n",
            "step: 520, loss: 0.00036204938078299165\n",
            "step: 530, loss: 0.0003751624608412385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8993039443155452, f1=0.8966480446927374, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000962369900662452\n",
            "step: 10, loss: 0.003909677267074585\n",
            "step: 20, loss: 0.001201748731546104\n",
            "step: 30, loss: 0.000787049881182611\n",
            "step: 40, loss: 0.0024388409219682217\n",
            "step: 50, loss: 0.0008647070499137044\n",
            "step: 60, loss: 0.0009985575452446938\n",
            "step: 70, loss: 0.008809780701994896\n",
            "step: 80, loss: 0.00559203838929534\n",
            "step: 90, loss: 0.0022243044804781675\n",
            "step: 100, loss: 0.0014331587590277195\n",
            "step: 110, loss: 0.004981242585927248\n",
            "step: 120, loss: 0.1039477288722992\n",
            "step: 130, loss: 0.0019815790001302958\n",
            "step: 140, loss: 0.0008457927615381777\n",
            "step: 150, loss: 0.000847615476232022\n",
            "step: 160, loss: 0.00018157184240408242\n",
            "step: 170, loss: 0.00016663111455272883\n",
            "step: 180, loss: 0.0003769753675442189\n",
            "step: 190, loss: 0.015415113419294357\n",
            "step: 200, loss: 0.0017054729396477342\n",
            "step: 210, loss: 0.00022644751879852265\n",
            "step: 220, loss: 0.09579765051603317\n",
            "step: 230, loss: 0.004461487289518118\n",
            "step: 240, loss: 0.04223603382706642\n",
            "step: 250, loss: 9.439195855520666e-05\n",
            "step: 260, loss: 0.00032717431895434856\n",
            "step: 270, loss: 0.00361329922452569\n",
            "step: 280, loss: 0.013304183259606361\n",
            "step: 290, loss: 0.0011684661731123924\n",
            "step: 300, loss: 0.0013977650087326765\n",
            "step: 310, loss: 0.00013458965986501426\n",
            "step: 320, loss: 0.00023328552197199315\n",
            "step: 330, loss: 0.0027764562983065844\n",
            "step: 340, loss: 0.00021685179672203958\n",
            "step: 350, loss: 9.161206253338605e-05\n",
            "step: 360, loss: 0.0038089274894446135\n",
            "step: 370, loss: 0.0006516706780530512\n",
            "step: 380, loss: 0.002999226562678814\n",
            "step: 390, loss: 0.0044676694087684155\n",
            "step: 400, loss: 0.004484476521611214\n",
            "step: 410, loss: 0.004421387333422899\n",
            "step: 420, loss: 0.0020990725606679916\n",
            "step: 430, loss: 7.70676342654042e-05\n",
            "step: 440, loss: 0.0011764310766011477\n",
            "step: 450, loss: 0.01959751546382904\n",
            "step: 460, loss: 0.0004220456175971776\n",
            "step: 470, loss: 0.0002979912096634507\n",
            "step: 480, loss: 0.04287257418036461\n",
            "step: 490, loss: 0.00024268559354823083\n",
            "step: 500, loss: 0.000351451279129833\n",
            "step: 510, loss: 9.858674457063898e-05\n",
            "step: 520, loss: 0.00011446340067777783\n",
            "step: 530, loss: 0.06296223402023315\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8993536472760849, f1=0.900875979714154, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22814083099365234\n",
            "step: 10, loss: 9.934745321515948e-05\n",
            "step: 20, loss: 0.000296841433737427\n",
            "step: 30, loss: 0.00024823661078698933\n",
            "step: 40, loss: 0.0016113148303702474\n",
            "step: 50, loss: 0.0002508573525119573\n",
            "step: 60, loss: 0.000552161131054163\n",
            "step: 70, loss: 0.0008827560814097524\n",
            "step: 80, loss: 0.2449568808078766\n",
            "step: 90, loss: 0.00023125388543121517\n",
            "step: 100, loss: 0.0003345434961374849\n",
            "step: 110, loss: 0.0015703316312283278\n",
            "step: 120, loss: 0.0002788534911815077\n",
            "step: 130, loss: 9.925048652803525e-05\n",
            "step: 140, loss: 0.027002466842532158\n",
            "step: 150, loss: 9.300248348154128e-05\n",
            "step: 160, loss: 0.00032843052758835256\n",
            "step: 170, loss: 0.0003606760292313993\n",
            "step: 180, loss: 0.010442148894071579\n",
            "step: 190, loss: 0.005717063322663307\n",
            "step: 200, loss: 0.006540337111800909\n",
            "step: 210, loss: 0.0009406742756254971\n",
            "step: 220, loss: 0.06795637309551239\n",
            "step: 230, loss: 0.0001831122353905812\n",
            "step: 240, loss: 0.0007049958221614361\n",
            "step: 250, loss: 0.00013755467080045491\n",
            "step: 260, loss: 0.000264503265498206\n",
            "step: 270, loss: 0.00010082965309266001\n",
            "step: 280, loss: 8.345681999344379e-05\n",
            "step: 290, loss: 6.887525523779914e-05\n",
            "step: 300, loss: 9.9273442174308e-05\n",
            "step: 310, loss: 0.0031929723918437958\n",
            "step: 320, loss: 0.00026802613865584135\n",
            "step: 330, loss: 0.0006969129317440093\n",
            "step: 340, loss: 0.021139970049262047\n",
            "step: 350, loss: 0.00015362666454166174\n",
            "step: 360, loss: 0.00014584125892724842\n",
            "step: 370, loss: 0.0009510915260761976\n",
            "step: 380, loss: 0.004967842251062393\n",
            "step: 390, loss: 0.00023705873172730207\n",
            "step: 400, loss: 0.00016695550584699959\n",
            "step: 410, loss: 0.00022706396703142673\n",
            "step: 420, loss: 0.010191482491791248\n",
            "step: 430, loss: 0.0003153861325699836\n",
            "step: 440, loss: 5.364164462662302e-05\n",
            "step: 450, loss: 0.0033098275307565928\n",
            "step: 460, loss: 0.0008879936649464071\n",
            "step: 470, loss: 0.0004610349715221673\n",
            "step: 480, loss: 0.00023072260955814272\n",
            "step: 490, loss: 0.030857935547828674\n",
            "step: 500, loss: 0.001063888892531395\n",
            "step: 510, loss: 0.0007269468624144793\n",
            "step: 520, loss: 0.00189517333637923\n",
            "step: 530, loss: 0.002592496806755662\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9022346368715083, f1=0.8997214484679666, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002816690248437226\n",
            "step: 10, loss: 0.000609991024248302\n",
            "step: 20, loss: 0.00013929937267675996\n",
            "step: 30, loss: 0.0632651224732399\n",
            "step: 40, loss: 0.01081408467143774\n",
            "step: 50, loss: 0.0003473739488981664\n",
            "step: 60, loss: 0.0038319791201502085\n",
            "step: 70, loss: 0.00010288324119755998\n",
            "step: 80, loss: 0.01572365127503872\n",
            "step: 90, loss: 0.0008861118112690747\n",
            "step: 100, loss: 0.0007493494194932282\n",
            "step: 110, loss: 0.017988869920372963\n",
            "step: 120, loss: 0.04425783455371857\n",
            "step: 130, loss: 0.0009512359974905849\n",
            "step: 140, loss: 0.0005351672298274934\n",
            "step: 150, loss: 6.809618935221806e-05\n",
            "step: 160, loss: 0.0014821148943156004\n",
            "step: 170, loss: 0.0033869340550154448\n",
            "step: 180, loss: 0.0004974076291546226\n",
            "step: 190, loss: 0.004467319697141647\n",
            "step: 200, loss: 0.001739105093292892\n",
            "step: 210, loss: 0.0008785440004430711\n",
            "step: 220, loss: 0.00033569676452316344\n",
            "step: 230, loss: 0.0004337869177106768\n",
            "step: 240, loss: 0.00029975306824781\n",
            "step: 250, loss: 0.001016576192341745\n",
            "step: 260, loss: 0.0003205068060196936\n",
            "step: 270, loss: 0.0028547008987516165\n",
            "step: 280, loss: 0.00014249482774175704\n",
            "step: 290, loss: 0.0003490860399324447\n",
            "step: 300, loss: 0.06675835698843002\n",
            "step: 310, loss: 0.032106198370456696\n",
            "step: 320, loss: 0.005010400898754597\n",
            "step: 330, loss: 0.02156633511185646\n",
            "step: 340, loss: 0.0002661525213625282\n",
            "step: 350, loss: 0.00640815868973732\n",
            "step: 360, loss: 0.000878495629876852\n",
            "step: 370, loss: 0.00026784130022861063\n",
            "step: 380, loss: 0.00024920038413256407\n",
            "step: 390, loss: 0.0035464761313050985\n",
            "step: 400, loss: 7.960290531627834e-05\n",
            "step: 410, loss: 0.0003668582357931882\n",
            "step: 420, loss: 0.0017498470842838287\n",
            "step: 430, loss: 0.0001318879658356309\n",
            "step: 440, loss: 0.001962395152077079\n",
            "step: 450, loss: 0.00010190177999902517\n",
            "step: 460, loss: 0.0025064500514417887\n",
            "step: 470, loss: 0.0022439288441091776\n",
            "step: 480, loss: 0.012874843552708626\n",
            "step: 490, loss: 0.018947875127196312\n",
            "step: 500, loss: 7.619248935952783e-05\n",
            "step: 510, loss: 0.0008198341820389032\n",
            "step: 520, loss: 0.00018487274064682424\n",
            "step: 530, loss: 0.0031106751412153244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9031665901789813, f1=0.8956999085086915, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008558580884709954\n",
            "step: 10, loss: 8.335168240591884e-05\n",
            "step: 20, loss: 2.8319129341980442e-05\n",
            "step: 30, loss: 0.004737495444715023\n",
            "step: 40, loss: 0.0001271959044970572\n",
            "step: 50, loss: 0.11833216995000839\n",
            "step: 60, loss: 0.00037310938932932913\n",
            "step: 70, loss: 0.05426545813679695\n",
            "step: 80, loss: 0.00019941217033192515\n",
            "step: 90, loss: 0.0007597083458676934\n",
            "step: 100, loss: 0.00042929392657242715\n",
            "step: 110, loss: 0.004128274042159319\n",
            "step: 120, loss: 0.10215316712856293\n",
            "step: 130, loss: 0.0029541479889303446\n",
            "step: 140, loss: 9.960451279766858e-05\n",
            "step: 150, loss: 0.0025631021708250046\n",
            "step: 160, loss: 6.997343734838068e-05\n",
            "step: 170, loss: 0.0030730292201042175\n",
            "step: 180, loss: 6.0858583310619e-05\n",
            "step: 190, loss: 9.377855167258531e-05\n",
            "step: 200, loss: 0.0017337003955617547\n",
            "step: 210, loss: 0.005183802917599678\n",
            "step: 220, loss: 0.013745909556746483\n",
            "step: 230, loss: 0.009592709131538868\n",
            "step: 240, loss: 0.008591708727180958\n",
            "step: 250, loss: 0.004445330239832401\n",
            "step: 260, loss: 0.0008742101490497589\n",
            "step: 270, loss: 0.0005232832627370954\n",
            "step: 280, loss: 0.004159536212682724\n",
            "step: 290, loss: 0.0007537738420069218\n",
            "step: 300, loss: 0.0744992196559906\n",
            "step: 310, loss: 0.002382401144132018\n",
            "step: 320, loss: 0.041576024144887924\n",
            "step: 330, loss: 0.013269046321511269\n",
            "step: 340, loss: 0.002385195577517152\n",
            "step: 350, loss: 0.010348541662096977\n",
            "step: 360, loss: 0.0001840592740336433\n",
            "step: 370, loss: 0.0001110283046728\n",
            "step: 380, loss: 0.00016440583567600697\n",
            "step: 390, loss: 5.548700210056268e-05\n",
            "step: 400, loss: 0.0003175109450239688\n",
            "step: 410, loss: 0.00017780449707061052\n",
            "step: 420, loss: 0.005644018761813641\n",
            "step: 430, loss: 0.00035493462928570807\n",
            "step: 440, loss: 0.07341079413890839\n",
            "step: 450, loss: 0.0003999117761850357\n",
            "step: 460, loss: 0.003442251356318593\n",
            "step: 470, loss: 0.0016327629564329982\n",
            "step: 480, loss: 0.0023110040929168463\n",
            "step: 490, loss: 5.079768016003072e-05\n",
            "step: 500, loss: 0.0008290472324006259\n",
            "step: 510, loss: 9.764944115886465e-05\n",
            "step: 520, loss: 0.015944113954901695\n",
            "step: 530, loss: 0.0003509200469125062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8959107806691451, f1=0.8921795465062471, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019134802278131247\n",
            "step: 10, loss: 0.00010535571345826611\n",
            "step: 20, loss: 0.013235378079116344\n",
            "step: 30, loss: 0.012795322574675083\n",
            "step: 40, loss: 8.926322334446013e-05\n",
            "step: 50, loss: 0.0034928354434669018\n",
            "step: 60, loss: 0.00015719622024334967\n",
            "step: 70, loss: 5.967697507003322e-05\n",
            "step: 80, loss: 0.0005837525241076946\n",
            "step: 90, loss: 0.004209986422210932\n",
            "step: 100, loss: 7.034843292785808e-05\n",
            "step: 110, loss: 0.0002091568021569401\n",
            "step: 120, loss: 0.0005085683660581708\n",
            "step: 130, loss: 0.00015877251280471683\n",
            "step: 140, loss: 0.0001911168801598251\n",
            "step: 150, loss: 0.0009592222049832344\n",
            "step: 160, loss: 0.004294654820114374\n",
            "step: 170, loss: 0.0007899185875430703\n",
            "step: 180, loss: 0.0011278268648311496\n",
            "step: 190, loss: 0.0011807173723354936\n",
            "step: 200, loss: 0.000759391812607646\n",
            "step: 210, loss: 0.005586729384958744\n",
            "step: 220, loss: 0.00015211119898594916\n",
            "step: 230, loss: 8.734581933822483e-05\n",
            "step: 240, loss: 0.00013319849676918238\n",
            "step: 250, loss: 0.02174890600144863\n",
            "step: 260, loss: 0.0004517019260674715\n",
            "step: 270, loss: 0.00041162516572512686\n",
            "step: 280, loss: 0.0007169835735112429\n",
            "step: 290, loss: 0.0025090330746024847\n",
            "step: 300, loss: 0.002898824168369174\n",
            "step: 310, loss: 0.008598837070167065\n",
            "step: 320, loss: 0.00016698517720215023\n",
            "step: 330, loss: 0.00024867584579624236\n",
            "step: 340, loss: 0.0003830324567388743\n",
            "step: 350, loss: 0.0003825822495855391\n",
            "step: 360, loss: 0.006344336550682783\n",
            "step: 370, loss: 0.0006658190977759659\n",
            "step: 380, loss: 0.00011705694487318397\n",
            "step: 390, loss: 0.00057525176089257\n",
            "step: 400, loss: 0.0001865230151452124\n",
            "step: 410, loss: 0.00029313290724530816\n",
            "step: 420, loss: 0.00016064110968727618\n",
            "step: 430, loss: 0.0007153185433708131\n",
            "step: 440, loss: 0.003133901860564947\n",
            "step: 450, loss: 0.3841778635978699\n",
            "step: 460, loss: 0.00012029457138851285\n",
            "step: 470, loss: 0.0034258048981428146\n",
            "step: 480, loss: 0.00038582313572987914\n",
            "step: 490, loss: 0.0004142780671827495\n",
            "step: 500, loss: 0.00039281678618863225\n",
            "step: 510, loss: 7.98778492026031e-05\n",
            "step: 520, loss: 0.00029190070927143097\n",
            "step: 530, loss: 0.0003061641473323107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.89749430523918, f1=0.8964888280893752, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010381718166172504\n",
            "step: 10, loss: 0.005382867064327002\n",
            "step: 20, loss: 0.0006358257378451526\n",
            "step: 30, loss: 0.0007940993527881801\n",
            "step: 40, loss: 0.0016451855190098286\n",
            "step: 50, loss: 0.0046710846945643425\n",
            "step: 60, loss: 5.6487639085389674e-05\n",
            "step: 70, loss: 0.0013404550263658166\n",
            "step: 80, loss: 0.00011639976582955569\n",
            "step: 90, loss: 0.0001696411200100556\n",
            "step: 100, loss: 0.0028735478408634663\n",
            "step: 110, loss: 0.00016930473793763667\n",
            "step: 120, loss: 0.00014836870832368731\n",
            "step: 130, loss: 0.00029695749981328845\n",
            "step: 140, loss: 0.02551920898258686\n",
            "step: 150, loss: 0.00032053780159913003\n",
            "step: 160, loss: 0.00018889919738285244\n",
            "step: 170, loss: 0.001594259636476636\n",
            "step: 180, loss: 0.00010874262807192281\n",
            "step: 190, loss: 9.957711154129356e-05\n",
            "step: 200, loss: 0.0007791888201609254\n",
            "step: 210, loss: 0.0002469445753376931\n",
            "step: 220, loss: 8.848016295814887e-05\n",
            "step: 230, loss: 0.001371111604385078\n",
            "step: 240, loss: 0.00010087182454299182\n",
            "step: 250, loss: 0.0007037131581455469\n",
            "step: 260, loss: 4.499174247030169e-05\n",
            "step: 270, loss: 0.0037434531841427088\n",
            "step: 280, loss: 0.00028194356127642095\n",
            "step: 290, loss: 0.0013555530458688736\n",
            "step: 300, loss: 0.000976675539277494\n",
            "step: 310, loss: 0.0006641946965828538\n",
            "step: 320, loss: 4.353599069872871e-05\n",
            "step: 330, loss: 0.01984735205769539\n",
            "step: 340, loss: 0.00018766173161566257\n",
            "step: 350, loss: 0.0025836925487965345\n",
            "step: 360, loss: 0.004883033689111471\n",
            "step: 370, loss: 0.0028828687500208616\n",
            "step: 380, loss: 0.027705825865268707\n",
            "step: 390, loss: 0.013630061410367489\n",
            "step: 400, loss: 0.000116694689495489\n",
            "step: 410, loss: 0.00029955743229947984\n",
            "step: 420, loss: 0.0017881713574752212\n",
            "step: 430, loss: 0.0005890716565772891\n",
            "step: 440, loss: 0.0013277486432343721\n",
            "step: 450, loss: 0.00010998254583682865\n",
            "step: 460, loss: 0.035446155816316605\n",
            "step: 470, loss: 0.0006042335298843682\n",
            "step: 480, loss: 0.0001956464839167893\n",
            "step: 490, loss: 0.0006463929894380271\n",
            "step: 500, loss: 0.0023440271615982056\n",
            "step: 510, loss: 0.0002024737186729908\n",
            "step: 520, loss: 0.0001031463107210584\n",
            "step: 530, loss: 9.081820462597534e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8952205882352942, f1=0.897119341563786, best_f1=0.9027454630060493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015560330939479172\n",
            "step: 10, loss: 7.000496407272294e-05\n",
            "step: 20, loss: 0.0006924147019162774\n",
            "step: 30, loss: 0.0001886104728328064\n",
            "step: 40, loss: 0.0005382175440900028\n",
            "step: 50, loss: 0.0002611281524877995\n",
            "step: 60, loss: 0.00011797479237429798\n",
            "step: 70, loss: 0.0004875595332123339\n",
            "step: 80, loss: 0.00319140893407166\n",
            "step: 90, loss: 0.00024612920242361724\n",
            "step: 100, loss: 7.185762660810724e-05\n",
            "step: 110, loss: 0.3063594400882721\n",
            "step: 120, loss: 0.001268164487555623\n",
            "step: 130, loss: 0.0003203761298209429\n",
            "step: 140, loss: 4.0376799006480724e-05\n",
            "step: 150, loss: 0.004333821125328541\n",
            "step: 160, loss: 0.0006107601220719516\n",
            "step: 170, loss: 0.00033380003878846765\n",
            "step: 180, loss: 0.0001014614463201724\n",
            "step: 190, loss: 0.022832509130239487\n",
            "step: 200, loss: 0.00020683756156358868\n",
            "step: 210, loss: 0.0009635832393541932\n",
            "step: 220, loss: 0.0006635875906795263\n",
            "step: 230, loss: 0.00014557030226569623\n",
            "step: 240, loss: 0.00012939066800754517\n",
            "step: 250, loss: 0.011845709756016731\n",
            "step: 260, loss: 0.00011370598804205656\n",
            "step: 270, loss: 0.00023125008738134056\n",
            "step: 280, loss: 0.00023597344988957047\n",
            "step: 290, loss: 0.014116568490862846\n",
            "step: 300, loss: 0.0066685727797448635\n",
            "step: 310, loss: 0.0014746938832104206\n",
            "step: 320, loss: 0.00031953310826793313\n",
            "step: 330, loss: 0.009405773133039474\n",
            "step: 340, loss: 0.0054845078848302364\n",
            "step: 350, loss: 0.0004086285480298102\n",
            "step: 360, loss: 0.0002912074269261211\n",
            "step: 370, loss: 6.023043533787131e-05\n",
            "step: 380, loss: 3.439445936237462e-05\n",
            "step: 390, loss: 0.0002046507433988154\n",
            "step: 400, loss: 8.059047831920907e-05\n",
            "step: 410, loss: 0.005884159822016954\n",
            "step: 420, loss: 6.80003286106512e-05\n",
            "step: 430, loss: 3.0196413717931136e-05\n",
            "step: 440, loss: 0.0065803928300738335\n",
            "step: 450, loss: 0.012271305546164513\n",
            "step: 460, loss: 7.388190715573728e-05\n",
            "step: 470, loss: 0.00015665244427509606\n",
            "step: 480, loss: 0.006226339843124151\n",
            "step: 490, loss: 0.035045791417360306\n",
            "step: 500, loss: 7.806938810972497e-05\n",
            "step: 510, loss: 0.00014348413969855756\n",
            "step: 520, loss: 0.007989509031176567\n",
            "step: 530, loss: 0.0014587517362087965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8979963570127504, f1=0.8953594176524113, best_f1=0.9027454630060493\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:16, 349.26it/s]\n",
            "load_f1 = 0.9010270774976656\n",
            "real_f1 = 0.8997668997668998\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 388.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686cad51-4f57-49e7-eaf5-8c0b85bb4062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8441272377967834\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.0662633627653122\n",
            "step: 20, loss: 0.38605162501335144\n",
            "step: 30, loss: 0.3739221394062042\n",
            "step: 40, loss: 0.5057337284088135\n",
            "step: 50, loss: 0.31204909086227417\n",
            "step: 60, loss: 0.36970624327659607\n",
            "step: 70, loss: 0.27907049655914307\n",
            "step: 80, loss: 0.29751619696617126\n",
            "step: 90, loss: 0.4204917550086975\n",
            "step: 100, loss: 0.19495804607868195\n",
            "step: 110, loss: 0.3003688454627991\n",
            "step: 120, loss: 0.25707462430000305\n",
            "step: 130, loss: 0.24583424627780914\n",
            "step: 140, loss: 0.3085329830646515\n",
            "step: 150, loss: 0.32011765241622925\n",
            "step: 160, loss: 0.25933703780174255\n",
            "step: 170, loss: 0.22035370767116547\n",
            "step: 180, loss: 0.18370015919208527\n",
            "step: 190, loss: 0.26661545038223267\n",
            "step: 200, loss: 0.18901363015174866\n",
            "step: 210, loss: 0.4963773190975189\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3703703703703704, f1=0.36538461538461536, best_f1=0.36538461538461536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10539943724870682\n",
            "step: 10, loss: 0.07897390425205231\n",
            "step: 20, loss: 0.290034681558609\n",
            "step: 30, loss: 0.16430366039276123\n",
            "step: 40, loss: 0.05375111103057861\n",
            "step: 50, loss: 0.22843486070632935\n",
            "step: 60, loss: 0.14982455968856812\n",
            "step: 70, loss: 0.2739974558353424\n",
            "step: 80, loss: 0.28957879543304443\n",
            "step: 90, loss: 0.13701632618904114\n",
            "step: 100, loss: 0.17746712267398834\n",
            "step: 110, loss: 0.06645005941390991\n",
            "step: 120, loss: 0.20741496980190277\n",
            "step: 130, loss: 0.2555668354034424\n",
            "step: 140, loss: 0.28611981868743896\n",
            "step: 150, loss: 0.3140665292739868\n",
            "step: 160, loss: 0.1809387505054474\n",
            "step: 170, loss: 0.18532200157642365\n",
            "step: 180, loss: 0.3818863332271576\n",
            "step: 190, loss: 0.21687495708465576\n",
            "step: 200, loss: 0.2318144589662552\n",
            "step: 210, loss: 0.3657529950141907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4532374100719424, f1=0.5105633802816901, best_f1=0.5105633802816901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2741549015045166\n",
            "step: 10, loss: 0.2737467885017395\n",
            "step: 20, loss: 0.4599064588546753\n",
            "step: 30, loss: 0.07365485280752182\n",
            "step: 40, loss: 0.18326368927955627\n",
            "step: 50, loss: 0.19705244898796082\n",
            "step: 60, loss: 0.2920210361480713\n",
            "step: 70, loss: 0.30249860882759094\n",
            "step: 80, loss: 0.0876728817820549\n",
            "step: 90, loss: 0.13426613807678223\n",
            "step: 100, loss: 0.13766898214817047\n",
            "step: 110, loss: 0.2725963592529297\n",
            "step: 120, loss: 0.2903646230697632\n",
            "step: 130, loss: 0.22843682765960693\n",
            "step: 140, loss: 0.29960933327674866\n",
            "step: 150, loss: 0.2813377380371094\n",
            "step: 160, loss: 0.12223777174949646\n",
            "step: 170, loss: 0.24516069889068604\n",
            "step: 180, loss: 0.12101221829652786\n",
            "step: 190, loss: 0.18587608635425568\n",
            "step: 200, loss: 0.17985737323760986\n",
            "step: 210, loss: 0.2819949686527252\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.44025157232704404, f1=0.482084690553746, best_f1=0.5105633802816901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23139984905719757\n",
            "step: 10, loss: 0.07546710968017578\n",
            "step: 20, loss: 0.16790825128555298\n",
            "step: 30, loss: 0.09497273713350296\n",
            "step: 40, loss: 0.10462186485528946\n",
            "step: 50, loss: 0.12269996106624603\n",
            "step: 60, loss: 0.06910117715597153\n",
            "step: 70, loss: 0.261351078748703\n",
            "step: 80, loss: 0.15446820855140686\n",
            "step: 90, loss: 0.11213496327400208\n",
            "step: 100, loss: 0.16767434775829315\n",
            "step: 110, loss: 0.15442104637622833\n",
            "step: 120, loss: 0.11629240959882736\n",
            "step: 130, loss: 0.21402473747730255\n",
            "step: 140, loss: 0.2113310843706131\n",
            "step: 150, loss: 0.312632292509079\n",
            "step: 160, loss: 0.11461886763572693\n",
            "step: 170, loss: 0.11366105079650879\n",
            "step: 180, loss: 0.15245690941810608\n",
            "step: 190, loss: 0.24931207299232483\n",
            "step: 200, loss: 0.2365342080593109\n",
            "step: 210, loss: 0.052091967314481735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.45348837209302323, f1=0.5095057034220533, best_f1=0.5095057034220533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07729168981313705\n",
            "step: 10, loss: 0.05750212445855141\n",
            "step: 20, loss: 0.060332439839839935\n",
            "step: 30, loss: 0.15499965846538544\n",
            "step: 40, loss: 0.1827387809753418\n",
            "step: 50, loss: 0.10884533822536469\n",
            "step: 60, loss: 0.052966780960559845\n",
            "step: 70, loss: 0.16394633054733276\n",
            "step: 80, loss: 0.05183327943086624\n",
            "step: 90, loss: 0.15809275209903717\n",
            "step: 100, loss: 0.09709998965263367\n",
            "step: 110, loss: 0.015767985954880714\n",
            "step: 120, loss: 0.1648351401090622\n",
            "step: 130, loss: 0.08530182391405106\n",
            "step: 140, loss: 0.17170460522174835\n",
            "step: 150, loss: 0.07848021388053894\n",
            "step: 160, loss: 0.13045963644981384\n",
            "step: 170, loss: 0.18711598217487335\n",
            "step: 180, loss: 0.6647425293922424\n",
            "step: 190, loss: 0.10934831947088242\n",
            "step: 200, loss: 0.1342773735523224\n",
            "step: 210, loss: 0.18552149832248688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.45871559633027525, f1=0.5074074074074074, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041017431765794754\n",
            "step: 10, loss: 0.03364713862538338\n",
            "step: 20, loss: 0.0683208703994751\n",
            "step: 30, loss: 0.22756369411945343\n",
            "step: 40, loss: 0.04847296327352524\n",
            "step: 50, loss: 0.136225163936615\n",
            "step: 60, loss: 0.20988602936267853\n",
            "step: 70, loss: 0.013298937119543552\n",
            "step: 80, loss: 0.22682039439678192\n",
            "step: 90, loss: 0.17499634623527527\n",
            "step: 100, loss: 0.04699227586388588\n",
            "step: 110, loss: 0.06487131863832474\n",
            "step: 120, loss: 0.09065483510494232\n",
            "step: 130, loss: 0.01502637006342411\n",
            "step: 140, loss: 0.1022629663348198\n",
            "step: 150, loss: 0.10746806114912033\n",
            "step: 160, loss: 0.08899961411952972\n",
            "step: 170, loss: 0.09772831201553345\n",
            "step: 180, loss: 0.028799785301089287\n",
            "step: 190, loss: 0.10037198662757874\n",
            "step: 200, loss: 0.027637673541903496\n",
            "step: 210, loss: 0.03810734674334526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4387351778656126, f1=0.5100401606425703, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019923627376556396\n",
            "step: 10, loss: 0.04732421413064003\n",
            "step: 20, loss: 0.2020084410905838\n",
            "step: 30, loss: 0.01740965247154236\n",
            "step: 40, loss: 0.024738095700740814\n",
            "step: 50, loss: 0.12769943475723267\n",
            "step: 60, loss: 0.24184638261795044\n",
            "step: 70, loss: 0.10059928148984909\n",
            "step: 80, loss: 0.05930298939347267\n",
            "step: 90, loss: 0.05528659000992775\n",
            "step: 100, loss: 0.015258423052728176\n",
            "step: 110, loss: 0.07400799542665482\n",
            "step: 120, loss: 0.18839779496192932\n",
            "step: 130, loss: 0.030250703915953636\n",
            "step: 140, loss: 0.0028400812298059464\n",
            "step: 150, loss: 0.0373055599629879\n",
            "step: 160, loss: 0.0778304859995842\n",
            "step: 170, loss: 0.10752091556787491\n",
            "step: 180, loss: 0.02145901694893837\n",
            "step: 190, loss: 0.12794910371303558\n",
            "step: 200, loss: 0.07017089426517487\n",
            "step: 210, loss: 0.04554013907909393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4469820554649266, f1=0.4747967479674797, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07999546080827713\n",
            "step: 10, loss: 0.04684418812394142\n",
            "step: 20, loss: 0.16829334199428558\n",
            "step: 30, loss: 0.012941903434693813\n",
            "step: 40, loss: 0.05964460223913193\n",
            "step: 50, loss: 0.025577841326594353\n",
            "step: 60, loss: 0.48969805240631104\n",
            "step: 70, loss: 0.04412487521767616\n",
            "step: 80, loss: 0.05400398001074791\n",
            "step: 90, loss: 0.12031427770853043\n",
            "step: 100, loss: 0.015218543820083141\n",
            "step: 110, loss: 0.02179810218513012\n",
            "step: 120, loss: 0.052941326051950455\n",
            "step: 130, loss: 0.0694538801908493\n",
            "step: 140, loss: 0.14136651158332825\n",
            "step: 150, loss: 0.011539056897163391\n",
            "step: 160, loss: 0.07200875133275986\n",
            "step: 170, loss: 0.030682116746902466\n",
            "step: 180, loss: 0.03331300616264343\n",
            "step: 190, loss: 0.010649878531694412\n",
            "step: 200, loss: 0.16445180773735046\n",
            "step: 210, loss: 0.2081172913312912\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.43952299829642255, f1=0.48217317487266553, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043783508241176605\n",
            "step: 10, loss: 0.04286842420697212\n",
            "step: 20, loss: 0.06372788548469543\n",
            "step: 30, loss: 0.13519102334976196\n",
            "step: 40, loss: 0.10140614956617355\n",
            "step: 50, loss: 0.01702398620545864\n",
            "step: 60, loss: 0.07556377351284027\n",
            "step: 70, loss: 0.05814630910754204\n",
            "step: 80, loss: 0.004140733741223812\n",
            "step: 90, loss: 0.0687933936715126\n",
            "step: 100, loss: 0.02258625067770481\n",
            "step: 110, loss: 0.026528626680374146\n",
            "step: 120, loss: 0.17538891732692719\n",
            "step: 130, loss: 0.036193620413541794\n",
            "step: 140, loss: 0.07701165229082108\n",
            "step: 150, loss: 0.03785306215286255\n",
            "step: 160, loss: 0.002234733197838068\n",
            "step: 170, loss: 0.005625082645565271\n",
            "step: 180, loss: 0.020574646070599556\n",
            "step: 190, loss: 0.11696479469537735\n",
            "step: 200, loss: 0.07262391597032547\n",
            "step: 210, loss: 0.09246574342250824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.44086021505376344, f1=0.4695340501792115, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013369274325668812\n",
            "step: 10, loss: 0.0025828881189227104\n",
            "step: 20, loss: 0.013573967851698399\n",
            "step: 30, loss: 0.07394220679998398\n",
            "step: 40, loss: 0.07627624273300171\n",
            "step: 50, loss: 0.01997319422662258\n",
            "step: 60, loss: 0.015402175486087799\n",
            "step: 70, loss: 0.1652706116437912\n",
            "step: 80, loss: 0.002021591877564788\n",
            "step: 90, loss: 0.1627814620733261\n",
            "step: 100, loss: 0.002484612399712205\n",
            "step: 110, loss: 0.0435514934360981\n",
            "step: 120, loss: 0.0028909447137266397\n",
            "step: 130, loss: 0.03395015746355057\n",
            "step: 140, loss: 0.0029891813173890114\n",
            "step: 150, loss: 0.07915356755256653\n",
            "step: 160, loss: 0.15784451365470886\n",
            "step: 170, loss: 0.048895251005887985\n",
            "step: 180, loss: 0.03233252465724945\n",
            "step: 190, loss: 0.2088337391614914\n",
            "step: 200, loss: 0.018507840111851692\n",
            "step: 210, loss: 0.06757353991270065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4351145038167939, f1=0.462406015037594, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015441006980836391\n",
            "step: 10, loss: 0.10025814175605774\n",
            "step: 20, loss: 0.036935608834028244\n",
            "step: 30, loss: 0.02778504230082035\n",
            "step: 40, loss: 0.03222393989562988\n",
            "step: 50, loss: 0.0071099502965807915\n",
            "step: 60, loss: 0.006799345836043358\n",
            "step: 70, loss: 0.004366699606180191\n",
            "step: 80, loss: 0.09367915242910385\n",
            "step: 90, loss: 0.012889495119452477\n",
            "step: 100, loss: 0.05967722088098526\n",
            "step: 110, loss: 0.06242068111896515\n",
            "step: 120, loss: 0.0553935207426548\n",
            "step: 130, loss: 0.1410333663225174\n",
            "step: 140, loss: 0.08679372817277908\n",
            "step: 150, loss: 0.03911260887980461\n",
            "step: 160, loss: 0.010652498342096806\n",
            "step: 170, loss: 0.15101291239261627\n",
            "step: 180, loss: 0.028804628178477287\n",
            "step: 190, loss: 0.019888591021299362\n",
            "step: 200, loss: 0.004489711485803127\n",
            "step: 210, loss: 0.003124828450381756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4421906693711968, f1=0.4489795918367347, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0041617038659751415\n",
            "step: 10, loss: 0.023127729073166847\n",
            "step: 20, loss: 0.012957720085978508\n",
            "step: 30, loss: 0.004247849807143211\n",
            "step: 40, loss: 0.2579537630081177\n",
            "step: 50, loss: 0.01030450314283371\n",
            "step: 60, loss: 0.04520421475172043\n",
            "step: 70, loss: 0.002276407089084387\n",
            "step: 80, loss: 0.02648056112229824\n",
            "step: 90, loss: 0.0010403974447399378\n",
            "step: 100, loss: 0.0008415335323661566\n",
            "step: 110, loss: 0.010989510454237461\n",
            "step: 120, loss: 0.020957518368959427\n",
            "step: 130, loss: 0.0012963456101715565\n",
            "step: 140, loss: 0.16297322511672974\n",
            "step: 150, loss: 0.0006342598353512585\n",
            "step: 160, loss: 0.003544385777786374\n",
            "step: 170, loss: 0.01008114218711853\n",
            "step: 180, loss: 0.0021849279291927814\n",
            "step: 190, loss: 0.09030138701200485\n",
            "step: 200, loss: 0.02807706408202648\n",
            "step: 210, loss: 0.08903130888938904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.42857142857142855, f1=0.4512428298279158, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008711920818313956\n",
            "step: 10, loss: 0.010359122417867184\n",
            "step: 20, loss: 0.0016254018992185593\n",
            "step: 30, loss: 0.024338314309716225\n",
            "step: 40, loss: 0.01210832130163908\n",
            "step: 50, loss: 0.0007629207102581859\n",
            "step: 60, loss: 0.005639798939228058\n",
            "step: 70, loss: 0.013860241509974003\n",
            "step: 80, loss: 0.014788484200835228\n",
            "step: 90, loss: 0.038377512246370316\n",
            "step: 100, loss: 0.010672452859580517\n",
            "step: 110, loss: 0.18804532289505005\n",
            "step: 120, loss: 0.0015105202328413725\n",
            "step: 130, loss: 0.0015027615008875728\n",
            "step: 140, loss: 0.09815036505460739\n",
            "step: 150, loss: 0.00947783887386322\n",
            "step: 160, loss: 0.022610073909163475\n",
            "step: 170, loss: 0.04889387637376785\n",
            "step: 180, loss: 0.07689065486192703\n",
            "step: 190, loss: 0.019634084776043892\n",
            "step: 200, loss: 0.0054875509813427925\n",
            "step: 210, loss: 0.0015259976498782635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.41437632135306557, f1=0.4327731092436975, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005265103653073311\n",
            "step: 10, loss: 0.0035461110528558493\n",
            "step: 20, loss: 0.0029125092551112175\n",
            "step: 30, loss: 0.14113935828208923\n",
            "step: 40, loss: 0.012635786086320877\n",
            "step: 50, loss: 0.02304232493042946\n",
            "step: 60, loss: 0.06919068098068237\n",
            "step: 70, loss: 0.03139916807413101\n",
            "step: 80, loss: 0.023404141888022423\n",
            "step: 90, loss: 0.13104765117168427\n",
            "step: 100, loss: 0.011492365039885044\n",
            "step: 110, loss: 0.005472420249134302\n",
            "step: 120, loss: 0.017760783433914185\n",
            "step: 130, loss: 0.0033362500835210085\n",
            "step: 140, loss: 0.0027498442213982344\n",
            "step: 150, loss: 0.03704897686839104\n",
            "step: 160, loss: 0.0029470003210008144\n",
            "step: 170, loss: 0.008316176943480968\n",
            "step: 180, loss: 0.005720793269574642\n",
            "step: 190, loss: 0.010891432873904705\n",
            "step: 200, loss: 0.0015380759723484516\n",
            "step: 210, loss: 0.016351062804460526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.40377358490566034, f1=0.43703703703703706, best_f1=0.5074074074074074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02112194150686264\n",
            "step: 10, loss: 0.030900241807103157\n",
            "step: 20, loss: 0.0024201087653636932\n",
            "step: 30, loss: 0.004489411599934101\n",
            "step: 40, loss: 0.00594918429851532\n",
            "step: 50, loss: 0.0029710624366998672\n",
            "step: 60, loss: 0.0034407281782478094\n",
            "step: 70, loss: 0.006674975622445345\n",
            "step: 80, loss: 0.009175490587949753\n",
            "step: 90, loss: 0.0014441125094890594\n",
            "step: 100, loss: 0.012206586077809334\n",
            "step: 110, loss: 0.0011382485972717404\n",
            "step: 120, loss: 0.04412493109703064\n",
            "step: 130, loss: 0.1062123104929924\n",
            "step: 140, loss: 0.006573752965778112\n",
            "step: 150, loss: 0.009343939833343029\n",
            "step: 160, loss: 0.013952297158539295\n",
            "step: 170, loss: 0.014428957365453243\n",
            "step: 180, loss: 0.010949740186333656\n",
            "step: 190, loss: 0.07416656613349915\n",
            "step: 200, loss: 0.04339052736759186\n",
            "step: 210, loss: 0.11568061262369156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.41275797373358347, f1=0.43669724770642204, best_f1=0.5074074074074074\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 572.19it/s]\n",
            "load_f1 = 0.4612736660929432\n",
            "real_f1 = 0.4594127806563039\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 378.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb80d9e-8f23-4803-a093-d1cec6e400e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.8562628626823425\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.16457071900367737\n",
            "step: 20, loss: 0.14866065979003906\n",
            "step: 30, loss: 0.5040276646614075\n",
            "step: 40, loss: 0.2652975022792816\n",
            "step: 50, loss: 0.3099709451198578\n",
            "step: 60, loss: 0.36946019530296326\n",
            "step: 70, loss: 0.18170985579490662\n",
            "step: 80, loss: 0.546298623085022\n",
            "step: 90, loss: 0.2490377575159073\n",
            "step: 100, loss: 0.22367310523986816\n",
            "step: 110, loss: 0.23442304134368896\n",
            "step: 120, loss: 0.4181341528892517\n",
            "step: 130, loss: 0.3445049822330475\n",
            "step: 140, loss: 0.34079286456108093\n",
            "step: 150, loss: 0.271222859621048\n",
            "step: 160, loss: 0.2237856090068817\n",
            "step: 170, loss: 0.42328882217407227\n",
            "step: 180, loss: 0.294037789106369\n",
            "step: 190, loss: 0.14946913719177246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4716417910447761, f1=0.4531722054380664, best_f1=0.4531722054380664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32599350810050964\n",
            "step: 10, loss: 0.05706140398979187\n",
            "step: 20, loss: 0.0483696348965168\n",
            "step: 30, loss: 0.1285291463136673\n",
            "step: 40, loss: 0.49677473306655884\n",
            "step: 50, loss: 0.3053276240825653\n",
            "step: 60, loss: 0.13095182180404663\n",
            "step: 70, loss: 0.23371034860610962\n",
            "step: 80, loss: 0.13908453285694122\n",
            "step: 90, loss: 0.10531173646450043\n",
            "step: 100, loss: 0.12394724041223526\n",
            "step: 110, loss: 0.2026384174823761\n",
            "step: 120, loss: 0.4150000810623169\n",
            "step: 130, loss: 0.15099549293518066\n",
            "step: 140, loss: 0.25506794452667236\n",
            "step: 150, loss: 0.06408865004777908\n",
            "step: 160, loss: 0.018491940572857857\n",
            "step: 170, loss: 0.2596144378185272\n",
            "step: 180, loss: 0.2015933394432068\n",
            "step: 190, loss: 0.3044692575931549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7017543859649122, f1=0.6968838526912181, best_f1=0.6968838526912181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17333191633224487\n",
            "step: 10, loss: 0.1763083040714264\n",
            "step: 20, loss: 0.02443438209593296\n",
            "step: 30, loss: 0.03988929092884064\n",
            "step: 40, loss: 0.04093324393033981\n",
            "step: 50, loss: 0.11826827377080917\n",
            "step: 60, loss: 0.07888365536928177\n",
            "step: 70, loss: 0.2420177310705185\n",
            "step: 80, loss: 0.2311423420906067\n",
            "step: 90, loss: 0.06050461158156395\n",
            "step: 100, loss: 0.16890867054462433\n",
            "step: 110, loss: 0.45211392641067505\n",
            "step: 120, loss: 0.01422809436917305\n",
            "step: 130, loss: 0.08513092249631882\n",
            "step: 140, loss: 0.07579810917377472\n",
            "step: 150, loss: 0.15160530805587769\n",
            "step: 160, loss: 0.20544281601905823\n",
            "step: 170, loss: 0.04030827060341835\n",
            "step: 180, loss: 0.13794070482254028\n",
            "step: 190, loss: 0.12363626807928085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7357512953367874, f1=0.7382198952879582, best_f1=0.7382198952879582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06042323634028435\n",
            "step: 10, loss: 0.10830776393413544\n",
            "step: 20, loss: 0.03939659893512726\n",
            "step: 30, loss: 0.10106305778026581\n",
            "step: 40, loss: 0.013145948760211468\n",
            "step: 50, loss: 0.09846565872430801\n",
            "step: 60, loss: 0.19319449365139008\n",
            "step: 70, loss: 0.029324166476726532\n",
            "step: 80, loss: 0.20521000027656555\n",
            "step: 90, loss: 0.07416851818561554\n",
            "step: 100, loss: 0.0656505823135376\n",
            "step: 110, loss: 0.04443582892417908\n",
            "step: 120, loss: 0.11376077681779861\n",
            "step: 130, loss: 0.2595915198326111\n",
            "step: 140, loss: 0.15336202085018158\n",
            "step: 150, loss: 0.01688913255929947\n",
            "step: 160, loss: 0.02333679050207138\n",
            "step: 170, loss: 0.0071931700222194195\n",
            "step: 180, loss: 0.16989882290363312\n",
            "step: 190, loss: 0.053911611437797546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7807486631016043, f1=0.75, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04253234714269638\n",
            "step: 10, loss: 0.054366473108530045\n",
            "step: 20, loss: 0.03601953387260437\n",
            "step: 30, loss: 0.05966058373451233\n",
            "step: 40, loss: 0.06342075020074844\n",
            "step: 50, loss: 0.05907874554395676\n",
            "step: 60, loss: 0.029868675395846367\n",
            "step: 70, loss: 0.03554488718509674\n",
            "step: 80, loss: 0.06270436942577362\n",
            "step: 90, loss: 0.008074912242591381\n",
            "step: 100, loss: 0.054932694882154465\n",
            "step: 110, loss: 0.06673789769411087\n",
            "step: 120, loss: 0.026772111654281616\n",
            "step: 130, loss: 0.025122372433543205\n",
            "step: 140, loss: 0.004449798259884119\n",
            "step: 150, loss: 0.03915473818778992\n",
            "step: 160, loss: 0.08101249486207962\n",
            "step: 170, loss: 0.054045163094997406\n",
            "step: 180, loss: 0.09775663167238235\n",
            "step: 190, loss: 0.1923867017030716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7405405405405405, f1=0.7365591397849462, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01865379698574543\n",
            "step: 10, loss: 0.0036874788347631693\n",
            "step: 20, loss: 0.02162850648164749\n",
            "step: 30, loss: 0.004214625805616379\n",
            "step: 40, loss: 0.013069391250610352\n",
            "step: 50, loss: 0.004729916341602802\n",
            "step: 60, loss: 0.004658858757466078\n",
            "step: 70, loss: 0.18472634255886078\n",
            "step: 80, loss: 0.059995897114276886\n",
            "step: 90, loss: 0.09493895620107651\n",
            "step: 100, loss: 0.012729750946164131\n",
            "step: 110, loss: 0.07292407751083374\n",
            "step: 120, loss: 0.057513874024152756\n",
            "step: 130, loss: 0.01566443033516407\n",
            "step: 140, loss: 0.012511517852544785\n",
            "step: 150, loss: 0.0881476029753685\n",
            "step: 160, loss: 0.03452416509389877\n",
            "step: 170, loss: 0.09766115993261337\n",
            "step: 180, loss: 0.006861855275928974\n",
            "step: 190, loss: 0.003892763750627637\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7520891364902507, f1=0.7493112947658401, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006578109227120876\n",
            "step: 10, loss: 0.052267204970121384\n",
            "step: 20, loss: 0.0010538690257817507\n",
            "step: 30, loss: 0.03197978436946869\n",
            "step: 40, loss: 0.03788352012634277\n",
            "step: 50, loss: 0.02734304592013359\n",
            "step: 60, loss: 0.009417633526027203\n",
            "step: 70, loss: 0.003349879989400506\n",
            "step: 80, loss: 0.13091015815734863\n",
            "step: 90, loss: 0.0030895129311829805\n",
            "step: 100, loss: 0.005505729466676712\n",
            "step: 110, loss: 0.019435012713074684\n",
            "step: 120, loss: 0.02476588450372219\n",
            "step: 130, loss: 0.0032069445587694645\n",
            "step: 140, loss: 0.018204577267169952\n",
            "step: 150, loss: 0.005548818502575159\n",
            "step: 160, loss: 0.002483718330040574\n",
            "step: 170, loss: 0.0028426055796444416\n",
            "step: 180, loss: 0.006610897369682789\n",
            "step: 190, loss: 0.04067108780145645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7559055118110236, f1=0.7195767195767196, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014058771776035428\n",
            "step: 10, loss: 0.010480690747499466\n",
            "step: 20, loss: 0.004777503665536642\n",
            "step: 30, loss: 0.003002490848302841\n",
            "step: 40, loss: 0.15334944427013397\n",
            "step: 50, loss: 0.010723496787250042\n",
            "step: 60, loss: 0.037877634167671204\n",
            "step: 70, loss: 0.0033447009045630693\n",
            "step: 80, loss: 0.004825165960937738\n",
            "step: 90, loss: 0.0018207021057605743\n",
            "step: 100, loss: 0.20287667214870453\n",
            "step: 110, loss: 0.007783415727317333\n",
            "step: 120, loss: 0.08014723658561707\n",
            "step: 130, loss: 0.0031975216697901487\n",
            "step: 140, loss: 0.010950236581265926\n",
            "step: 150, loss: 0.06490515917539597\n",
            "step: 160, loss: 0.01912875846028328\n",
            "step: 170, loss: 0.01363363116979599\n",
            "step: 180, loss: 0.023227276280522346\n",
            "step: 190, loss: 0.0431981086730957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7455012853470436, f1=0.743142144638404, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2172415405511856\n",
            "step: 10, loss: 0.0034309346228837967\n",
            "step: 20, loss: 0.14433152973651886\n",
            "step: 30, loss: 0.06333014369010925\n",
            "step: 40, loss: 0.00883129145950079\n",
            "step: 50, loss: 0.0029986875597387552\n",
            "step: 60, loss: 0.004717483185231686\n",
            "step: 70, loss: 0.0004816264845430851\n",
            "step: 80, loss: 0.011903837323188782\n",
            "step: 90, loss: 0.020100325345993042\n",
            "step: 100, loss: 0.0014285410288721323\n",
            "step: 110, loss: 0.002583723049610853\n",
            "step: 120, loss: 0.0674452930688858\n",
            "step: 130, loss: 0.007769250776618719\n",
            "step: 140, loss: 0.0020829858258366585\n",
            "step: 150, loss: 0.005577942822128534\n",
            "step: 160, loss: 0.07913389801979065\n",
            "step: 170, loss: 0.06691926717758179\n",
            "step: 180, loss: 0.07511626183986664\n",
            "step: 190, loss: 0.00816453155130148\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7493403693931397, f1=0.7168831168831169, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014377835905179381\n",
            "step: 10, loss: 0.001997358165681362\n",
            "step: 20, loss: 0.007418425288051367\n",
            "step: 30, loss: 0.008117283694446087\n",
            "step: 40, loss: 0.0012784932041540742\n",
            "step: 50, loss: 0.0066302623599767685\n",
            "step: 60, loss: 0.1507616937160492\n",
            "step: 70, loss: 0.007806878536939621\n",
            "step: 80, loss: 0.0016922628274187446\n",
            "step: 90, loss: 0.004298159386962652\n",
            "step: 100, loss: 0.017009085044264793\n",
            "step: 110, loss: 0.024779127910733223\n",
            "step: 120, loss: 0.00242599262855947\n",
            "step: 130, loss: 0.0012166649103164673\n",
            "step: 140, loss: 0.005118592642247677\n",
            "step: 150, loss: 0.0015548779629170895\n",
            "step: 160, loss: 0.0008071210468187928\n",
            "step: 170, loss: 0.001264885999262333\n",
            "step: 180, loss: 0.009494726546108723\n",
            "step: 190, loss: 0.00920331571251154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7345844504021448, f1=0.7282321899736147, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026649204082787037\n",
            "step: 10, loss: 0.011290152557194233\n",
            "step: 20, loss: 0.0020829602144658566\n",
            "step: 30, loss: 0.0022144028916954994\n",
            "step: 40, loss: 0.0028773811645805836\n",
            "step: 50, loss: 0.001332474290393293\n",
            "step: 60, loss: 0.0043749697506427765\n",
            "step: 70, loss: 0.0008162546437233686\n",
            "step: 80, loss: 0.0046321433037519455\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.0013402156764641404\n",
            "step: 100, loss: 0.0011141253635287285\n",
            "step: 110, loss: 0.0014098434476181865\n",
            "step: 120, loss: 0.001461922307498753\n",
            "step: 130, loss: 0.0021245493553578854\n",
            "step: 140, loss: 0.0006588895339518785\n",
            "step: 150, loss: 0.006615896709263325\n",
            "step: 160, loss: 0.001122821937315166\n",
            "step: 170, loss: 0.058531925082206726\n",
            "step: 180, loss: 0.04201061651110649\n",
            "step: 190, loss: 0.005394786596298218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7567567567567567, f1=0.7049180327868853, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016755149699747562\n",
            "step: 10, loss: 0.005513709504157305\n",
            "step: 20, loss: 0.003835184033960104\n",
            "step: 30, loss: 0.07912891358137131\n",
            "step: 40, loss: 0.008966632187366486\n",
            "step: 50, loss: 0.0014443728141486645\n",
            "step: 60, loss: 0.00046374715748243034\n",
            "step: 70, loss: 0.0021270031575113535\n",
            "step: 80, loss: 0.0016072371508926153\n",
            "step: 90, loss: 0.20068852603435516\n",
            "step: 100, loss: 0.003975106868892908\n",
            "step: 110, loss: 0.008296751417219639\n",
            "step: 120, loss: 0.001254492555744946\n",
            "step: 130, loss: 0.005653807893395424\n",
            "step: 140, loss: 0.02282690815627575\n",
            "step: 150, loss: 0.0023059339728206396\n",
            "step: 160, loss: 0.017518723383545876\n",
            "step: 170, loss: 0.0056769223883748055\n",
            "step: 180, loss: 0.0014648601645603776\n",
            "step: 190, loss: 0.1536587029695511\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7317073170731707, f1=0.7154471544715448, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008345250971615314\n",
            "step: 10, loss: 0.0032533679623156786\n",
            "step: 20, loss: 0.00221968162804842\n",
            "step: 30, loss: 0.004009414929896593\n",
            "step: 40, loss: 0.0010042381472885609\n",
            "step: 50, loss: 0.011640372686088085\n",
            "step: 60, loss: 0.002024967223405838\n",
            "step: 70, loss: 0.0005866261781193316\n",
            "step: 80, loss: 0.0038511166349053383\n",
            "step: 90, loss: 0.041880447417497635\n",
            "step: 100, loss: 0.0052299718372523785\n",
            "step: 110, loss: 0.0014124992303550243\n",
            "step: 120, loss: 0.0021639158949255943\n",
            "step: 130, loss: 0.052662163972854614\n",
            "step: 140, loss: 0.004107967484742403\n",
            "step: 150, loss: 0.018198223784565926\n",
            "step: 160, loss: 0.007957279682159424\n",
            "step: 170, loss: 0.001885897945612669\n",
            "step: 180, loss: 0.00280465604737401\n",
            "step: 190, loss: 0.02137838862836361\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7317073170731707, f1=0.7162534435261707, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12202328443527222\n",
            "step: 10, loss: 0.002188280690461397\n",
            "step: 20, loss: 0.0027806032449007034\n",
            "step: 30, loss: 0.007026742212474346\n",
            "step: 40, loss: 0.0009155264706350863\n",
            "step: 50, loss: 0.0011608425993472338\n",
            "step: 60, loss: 0.0035965361166745424\n",
            "step: 70, loss: 0.00914947409182787\n",
            "step: 80, loss: 0.001285324222408235\n",
            "step: 90, loss: 0.007822778075933456\n",
            "step: 100, loss: 0.014757073484361172\n",
            "step: 110, loss: 0.0005374421016313136\n",
            "step: 120, loss: 0.0014354687882587314\n",
            "step: 130, loss: 0.0461464524269104\n",
            "step: 140, loss: 0.0027879599947482347\n",
            "step: 150, loss: 0.0008925607544369996\n",
            "step: 160, loss: 0.005988023243844509\n",
            "step: 170, loss: 0.21258771419525146\n",
            "step: 180, loss: 0.0016485777450725436\n",
            "step: 190, loss: 0.0007395753054879606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7272727272727273, f1=0.7138964577656676, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011328815016895533\n",
            "step: 10, loss: 0.0023356920573860407\n",
            "step: 20, loss: 0.0005165491602383554\n",
            "step: 30, loss: 0.0012348806485533714\n",
            "step: 40, loss: 0.0008135930984281003\n",
            "step: 50, loss: 0.001165401772595942\n",
            "step: 60, loss: 0.0006674696342088282\n",
            "step: 70, loss: 0.0018377716187387705\n",
            "step: 80, loss: 0.0405329130589962\n",
            "step: 90, loss: 0.0545785091817379\n",
            "step: 100, loss: 0.0005807693814858794\n",
            "step: 110, loss: 0.006647940259426832\n",
            "step: 120, loss: 0.0015899526188150048\n",
            "step: 130, loss: 0.004809225909411907\n",
            "step: 140, loss: 0.005679886322468519\n",
            "step: 150, loss: 0.001350156613625586\n",
            "step: 160, loss: 0.008555784821510315\n",
            "step: 170, loss: 0.00040892360266298056\n",
            "step: 180, loss: 0.001959078246727586\n",
            "step: 190, loss: 0.0009524824563413858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7308781869688386, f1=0.6961651917404129, best_f1=0.75\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:06, 316.13it/s]\n",
            "load_f1 = 0.7382920110192837\n",
            "real_f1 = 0.745308310991957\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:12, 362.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9868df3a-ce5b-4c36-ccdf-07809e30bd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.846549928188324\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.21384912729263306\n",
            "step: 20, loss: 0.14727512001991272\n",
            "step: 30, loss: 0.24446696043014526\n",
            "step: 40, loss: 0.3136391043663025\n",
            "step: 50, loss: 0.37785282731056213\n",
            "step: 60, loss: 0.43352562189102173\n",
            "step: 70, loss: 0.308724582195282\n",
            "step: 80, loss: 0.2513115108013153\n",
            "step: 90, loss: 0.4104374349117279\n",
            "step: 100, loss: 0.2350817173719406\n",
            "step: 110, loss: 0.18789924681186676\n",
            "step: 120, loss: 0.5299362540245056\n",
            "step: 130, loss: 0.4188780188560486\n",
            "step: 140, loss: 0.47661417722702026\n",
            "step: 150, loss: 0.09693492949008942\n",
            "step: 160, loss: 0.3466000258922577\n",
            "step: 170, loss: 0.24337105453014374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2907268170426065, f1=0.24646983311938386, best_f1=0.24646983311938386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.42361608147621155\n",
            "step: 10, loss: 0.24290873110294342\n",
            "step: 20, loss: 0.3396557569503784\n",
            "step: 30, loss: 0.2972502112388611\n",
            "step: 40, loss: 0.21070623397827148\n",
            "step: 50, loss: 0.2896919250488281\n",
            "step: 60, loss: 0.18321597576141357\n",
            "step: 70, loss: 0.23228619992733002\n",
            "step: 80, loss: 0.09910836815834045\n",
            "step: 90, loss: 0.2618633210659027\n",
            "step: 100, loss: 0.11349281668663025\n",
            "step: 110, loss: 0.30151864886283875\n",
            "step: 120, loss: 0.09044674783945084\n",
            "step: 130, loss: 0.05945061892271042\n",
            "step: 140, loss: 0.2074000984430313\n",
            "step: 150, loss: 0.15411125123500824\n",
            "step: 160, loss: 0.18712730705738068\n",
            "step: 170, loss: 0.1089579164981842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6477024070021883, f1=0.6233183856502242, best_f1=0.6233183856502242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.157114639878273\n",
            "step: 10, loss: 0.12349268794059753\n",
            "step: 20, loss: 0.04277724772691727\n",
            "step: 30, loss: 0.38413605093955994\n",
            "step: 40, loss: 0.011858168989419937\n",
            "step: 50, loss: 0.18130715191364288\n",
            "step: 60, loss: 0.2608896493911743\n",
            "step: 70, loss: 0.2945590913295746\n",
            "step: 80, loss: 0.16900953650474548\n",
            "step: 90, loss: 0.23851680755615234\n",
            "step: 100, loss: 0.03689416125416756\n",
            "step: 110, loss: 0.13638626039028168\n",
            "step: 120, loss: 0.028339054435491562\n",
            "step: 130, loss: 0.17442795634269714\n",
            "step: 140, loss: 0.015921350568532944\n",
            "step: 150, loss: 0.12858940660953522\n",
            "step: 160, loss: 0.07268083095550537\n",
            "step: 170, loss: 0.22855377197265625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7405405405405405, f1=0.6984126984126984, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08215069025754929\n",
            "step: 10, loss: 0.08592269569635391\n",
            "step: 20, loss: 0.02895314246416092\n",
            "step: 30, loss: 0.17120403051376343\n",
            "step: 40, loss: 0.099124975502491\n",
            "step: 50, loss: 0.05228649824857712\n",
            "step: 60, loss: 0.16615648567676544\n",
            "step: 70, loss: 0.010889961384236813\n",
            "step: 80, loss: 0.027010781690478325\n",
            "step: 90, loss: 0.06087670102715492\n",
            "step: 100, loss: 0.06346023082733154\n",
            "step: 110, loss: 0.14496682584285736\n",
            "step: 120, loss: 0.17454279959201813\n",
            "step: 130, loss: 0.06603571772575378\n",
            "step: 140, loss: 0.030099380761384964\n",
            "step: 150, loss: 0.05142296105623245\n",
            "step: 160, loss: 0.09828471392393112\n",
            "step: 170, loss: 0.04875842481851578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7240506329113925, f1=0.7002518891687658, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07941460609436035\n",
            "step: 10, loss: 0.1082068607211113\n",
            "step: 20, loss: 0.03650841489434242\n",
            "step: 30, loss: 0.16683518886566162\n",
            "step: 40, loss: 0.021395744755864143\n",
            "step: 50, loss: 0.025644950568675995\n",
            "step: 60, loss: 0.004081200808286667\n",
            "step: 70, loss: 0.08487547934055328\n",
            "step: 80, loss: 0.014791733585298061\n",
            "step: 90, loss: 0.0921589657664299\n",
            "step: 100, loss: 0.03885551169514656\n",
            "step: 110, loss: 0.1373131424188614\n",
            "step: 120, loss: 0.13631895184516907\n",
            "step: 130, loss: 0.029522234573960304\n",
            "step: 140, loss: 0.03829227760434151\n",
            "step: 150, loss: 0.048949360847473145\n",
            "step: 160, loss: 0.05331040546298027\n",
            "step: 170, loss: 0.048667244613170624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.7286821705426356, f1=0.685, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02714608982205391\n",
            "step: 10, loss: 0.09025851637125015\n",
            "step: 20, loss: 0.035152241587638855\n",
            "step: 30, loss: 0.053470585495233536\n",
            "step: 40, loss: 0.013123015873134136\n",
            "step: 50, loss: 0.04003998264670372\n",
            "step: 60, loss: 0.022680656984448433\n",
            "step: 70, loss: 0.004066824913024902\n",
            "step: 80, loss: 0.20483875274658203\n",
            "step: 90, loss: 0.04535358399152756\n",
            "step: 100, loss: 0.20315150916576385\n",
            "step: 110, loss: 0.11110392212867737\n",
            "step: 120, loss: 0.1692485213279724\n",
            "step: 130, loss: 0.3056694567203522\n",
            "step: 140, loss: 0.009307385422289371\n",
            "step: 150, loss: 0.3852970600128174\n",
            "step: 160, loss: 0.04916002228856087\n",
            "step: 170, loss: 0.015759317204356194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7268041237113402, f1=0.6813725490196079, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02127944491803646\n",
            "step: 10, loss: 0.00475477147847414\n",
            "step: 20, loss: 0.0198493842035532\n",
            "step: 30, loss: 0.10608416050672531\n",
            "step: 40, loss: 0.0148554015904665\n",
            "step: 50, loss: 0.029195230454206467\n",
            "step: 60, loss: 0.25656774640083313\n",
            "step: 70, loss: 0.00846855714917183\n",
            "step: 80, loss: 0.029064955189824104\n",
            "step: 90, loss: 0.033885419368743896\n",
            "step: 100, loss: 0.005566906183958054\n",
            "step: 110, loss: 0.011969661340117455\n",
            "step: 120, loss: 0.21391771733760834\n",
            "step: 130, loss: 0.1469971239566803\n",
            "step: 140, loss: 0.010301053524017334\n",
            "step: 150, loss: 0.04446905478835106\n",
            "step: 160, loss: 0.013664853759109974\n",
            "step: 170, loss: 0.1785700023174286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7180851063829787, f1=0.6898395721925135, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.059110838919878006\n",
            "step: 10, loss: 0.014437716454267502\n",
            "step: 20, loss: 0.05101816728711128\n",
            "step: 30, loss: 0.06109825149178505\n",
            "step: 40, loss: 0.021749090403318405\n",
            "step: 50, loss: 0.11845327913761139\n",
            "step: 60, loss: 0.023875916376709938\n",
            "step: 70, loss: 0.057708170264959335\n",
            "step: 80, loss: 0.004061523359268904\n",
            "step: 90, loss: 0.1057099997997284\n",
            "step: 100, loss: 0.013519179075956345\n",
            "step: 110, loss: 0.0466693714261055\n",
            "step: 120, loss: 0.01641143672168255\n",
            "step: 130, loss: 0.0051255906000733376\n",
            "step: 140, loss: 0.005804760381579399\n",
            "step: 150, loss: 0.15102745592594147\n",
            "step: 160, loss: 0.02776653878390789\n",
            "step: 170, loss: 0.01543737854808569\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7212276214833758, f1=0.6962962962962963, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00475778104737401\n",
            "step: 10, loss: 0.10249608755111694\n",
            "step: 20, loss: 0.13305748999118805\n",
            "step: 30, loss: 0.1017623022198677\n",
            "step: 40, loss: 0.14053623378276825\n",
            "step: 50, loss: 0.08125433325767517\n",
            "step: 60, loss: 0.008577299304306507\n",
            "step: 70, loss: 0.012360382825136185\n",
            "step: 80, loss: 0.057217735797166824\n",
            "step: 90, loss: 0.023509949445724487\n",
            "step: 100, loss: 0.049436263740062714\n",
            "step: 110, loss: 0.06677766889333725\n",
            "step: 120, loss: 0.01824929006397724\n",
            "step: 130, loss: 0.0018922097515314817\n",
            "step: 140, loss: 0.005495480261743069\n",
            "step: 150, loss: 0.037767067551612854\n",
            "step: 160, loss: 0.02379184402525425\n",
            "step: 170, loss: 0.03176990896463394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7262872628726287, f1=0.6986666666666667, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024874428287148476\n",
            "step: 10, loss: 0.011784031987190247\n",
            "step: 20, loss: 0.004495501518249512\n",
            "step: 30, loss: 0.03061024472117424\n",
            "step: 40, loss: 0.003593273228034377\n",
            "step: 50, loss: 0.006767392158508301\n",
            "step: 60, loss: 0.025543304160237312\n",
            "step: 70, loss: 0.004275710321962833\n",
            "step: 80, loss: 0.008730929344892502\n",
            "step: 90, loss: 0.011370300315320492\n",
            "step: 100, loss: 0.057431790977716446\n",
            "step: 110, loss: 0.0007117881905287504\n",
            "step: 120, loss: 0.026705851778388023\n",
            "step: 130, loss: 0.1020040437579155\n",
            "step: 140, loss: 0.029118737205863\n",
            "step: 150, loss: 0.0016576729249209166\n",
            "step: 160, loss: 0.06936217099428177\n",
            "step: 170, loss: 0.017659470438957214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7394957983193278, f1=0.6956521739130433, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008569757919758558\n",
            "step: 10, loss: 0.026814335957169533\n",
            "step: 20, loss: 0.01574811153113842\n",
            "step: 30, loss: 0.005117741413414478\n",
            "step: 40, loss: 0.21816898882389069\n",
            "step: 50, loss: 0.0036729092244058847\n",
            "step: 60, loss: 0.00884929671883583\n",
            "step: 70, loss: 0.007854265160858631\n",
            "step: 80, loss: 0.06539975851774216\n",
            "step: 90, loss: 0.01966344192624092\n",
            "step: 100, loss: 0.0016566303092986345\n",
            "step: 110, loss: 0.02991321310400963\n",
            "step: 120, loss: 0.08633332699537277\n",
            "step: 130, loss: 0.008159532211720943\n",
            "step: 140, loss: 0.07921252399682999\n",
            "step: 150, loss: 0.0035327638033777475\n",
            "step: 160, loss: 0.08930165320634842\n",
            "step: 170, loss: 0.05046509951353073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7150259067357513, f1=0.6564885496183206, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010847226716578007\n",
            "step: 10, loss: 0.011227929033339024\n",
            "step: 20, loss: 0.019241902977228165\n",
            "step: 30, loss: 0.004051906988024712\n",
            "step: 40, loss: 0.004927445203065872\n",
            "step: 50, loss: 0.016715966165065765\n",
            "step: 60, loss: 0.026588989421725273\n",
            "step: 70, loss: 0.008946651592850685\n",
            "step: 80, loss: 0.12215344607830048\n",
            "step: 90, loss: 0.0018732595490291715\n",
            "step: 100, loss: 0.07627738267183304\n",
            "step: 110, loss: 0.02507144771516323\n",
            "step: 120, loss: 0.02543114870786667\n",
            "step: 130, loss: 0.03081163763999939\n",
            "step: 140, loss: 0.005801243707537651\n",
            "step: 150, loss: 0.01694730669260025\n",
            "step: 160, loss: 0.004458484705537558\n",
            "step: 170, loss: 0.025764059275388718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7139240506329114, f1=0.6813725490196079, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003546877996996045\n",
            "step: 10, loss: 0.06300707161426544\n",
            "step: 20, loss: 0.0014050058089196682\n",
            "step: 30, loss: 0.0013903815997764468\n",
            "step: 40, loss: 0.0031025169882923365\n",
            "step: 50, loss: 0.0017493950435891747\n",
            "step: 60, loss: 0.0006721844547428191\n",
            "step: 70, loss: 0.004898956045508385\n",
            "step: 80, loss: 0.0014340280322358012\n",
            "step: 90, loss: 0.0008613343816250563\n",
            "step: 100, loss: 0.0009778960375115275\n",
            "step: 110, loss: 0.023336950689554214\n",
            "step: 120, loss: 0.007077925838530064\n",
            "step: 130, loss: 0.006416539195924997\n",
            "step: 140, loss: 0.0017410935834050179\n",
            "step: 150, loss: 0.0031908140517771244\n",
            "step: 160, loss: 0.0012026039185002446\n",
            "step: 170, loss: 0.0041062175296247005\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7166666666666667, f1=0.6927374301675977, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001968675060197711\n",
            "step: 10, loss: 0.002344615524634719\n",
            "step: 20, loss: 0.011279827915132046\n",
            "step: 30, loss: 0.033747054636478424\n",
            "step: 40, loss: 0.00090132059995085\n",
            "step: 50, loss: 0.21145674586296082\n",
            "step: 60, loss: 0.002648168709129095\n",
            "step: 70, loss: 0.0021362826228141785\n",
            "step: 80, loss: 0.005708268843591213\n",
            "step: 90, loss: 0.04799745976924896\n",
            "step: 100, loss: 0.006653282791376114\n",
            "step: 110, loss: 0.0036332898307591677\n",
            "step: 120, loss: 0.004287000745534897\n",
            "step: 130, loss: 0.0027089843060821295\n",
            "step: 140, loss: 0.00543819647282362\n",
            "step: 150, loss: 0.03182568401098251\n",
            "step: 160, loss: 0.05098462849855423\n",
            "step: 170, loss: 0.0008320931810885668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7247956403269755, f1=0.6991869918699187, best_f1=0.6984126984126984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005451960489153862\n",
            "step: 10, loss: 0.002433191053569317\n",
            "step: 20, loss: 0.020416809245944023\n",
            "step: 30, loss: 0.04189841076731682\n",
            "step: 40, loss: 0.0005028037703596056\n",
            "step: 50, loss: 0.002937138546258211\n",
            "step: 60, loss: 0.0007449996192008257\n",
            "step: 70, loss: 0.005136698484420776\n",
            "step: 80, loss: 0.0012278524227440357\n",
            "step: 90, loss: 0.0036347422283142805\n",
            "step: 100, loss: 0.004637017846107483\n",
            "step: 110, loss: 0.0056334906257689\n",
            "step: 120, loss: 0.020624201744794846\n",
            "step: 130, loss: 0.0012358510866761208\n",
            "step: 140, loss: 0.0013828924857079983\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.04521197825670242\n",
            "step: 160, loss: 0.0090520940721035\n",
            "step: 170, loss: 0.0017555560916662216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7237569060773481, f1=0.7036011080332409, best_f1=0.6984126984126984\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:04, 438.05it/s]\n",
            "load_f1 = 0.7086614173228347\n",
            "real_f1 = 0.7015706806282723\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 399.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6aa75a-1f00-4ec1-8f37-056420778750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7981746196746826\n",
            "step: 10, loss: 0.47928956151008606\n",
            "step: 20, loss: 0.5966882705688477\n",
            "step: 30, loss: 0.4985109567642212\n",
            "step: 40, loss: 0.3850580155849457\n",
            "step: 50, loss: 0.3307805359363556\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.16282522678375244\n",
            "step: 70, loss: 0.23142722249031067\n",
            "step: 80, loss: 0.24285255372524261\n",
            "step: 90, loss: 0.13599742949008942\n",
            "step: 100, loss: 0.12443788349628448\n",
            "step: 110, loss: 0.11283382773399353\n",
            "step: 120, loss: 0.05452249199151993\n",
            "step: 130, loss: 0.028280459344387054\n",
            "step: 140, loss: 0.09828140586614609\n",
            "step: 150, loss: 0.11178809404373169\n",
            "step: 160, loss: 0.15617595613002777\n",
            "step: 170, loss: 0.013633045367896557\n",
            "step: 180, loss: 0.014914633706212044\n",
            "step: 190, loss: 0.2601086497306824\n",
            "step: 200, loss: 0.13426411151885986\n",
            "step: 210, loss: 0.04637556150555611\n",
            "step: 220, loss: 0.015230854973196983\n",
            "step: 230, loss: 0.04160868003964424\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9317180616740088, f1=0.9318435754189943, best_f1=0.9318435754189943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1187271773815155\n",
            "step: 10, loss: 0.11351843923330307\n",
            "step: 20, loss: 0.0360351987183094\n",
            "step: 30, loss: 0.008694672025740147\n",
            "step: 40, loss: 0.08736088871955872\n",
            "step: 50, loss: 0.04958823695778847\n",
            "step: 60, loss: 0.04659897834062576\n",
            "step: 70, loss: 0.024463798850774765\n",
            "step: 80, loss: 0.01974346674978733\n",
            "step: 90, loss: 0.016679903492331505\n",
            "step: 100, loss: 0.18471936881542206\n",
            "step: 110, loss: 0.037987180054187775\n",
            "step: 120, loss: 0.06907433271408081\n",
            "step: 130, loss: 0.08151856064796448\n",
            "step: 140, loss: 0.15185606479644775\n",
            "step: 150, loss: 0.11089866608381271\n",
            "step: 160, loss: 0.06191747635602951\n",
            "step: 170, loss: 0.043284520506858826\n",
            "step: 180, loss: 0.05666545405983925\n",
            "step: 190, loss: 0.1227840781211853\n",
            "step: 200, loss: 0.04269307479262352\n",
            "step: 210, loss: 0.09872410446405411\n",
            "step: 220, loss: 0.004611428827047348\n",
            "step: 230, loss: 0.017230499535799026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9481808158765159, f1=0.9358830146231721, best_f1=0.9358830146231721\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06462721526622772\n",
            "step: 10, loss: 0.03604961931705475\n",
            "step: 20, loss: 0.005403400398790836\n",
            "step: 30, loss: 0.053473781794309616\n",
            "step: 40, loss: 0.05887873098254204\n",
            "step: 50, loss: 0.03519086539745331\n",
            "step: 60, loss: 0.12665115296840668\n",
            "step: 70, loss: 0.031079763546586037\n",
            "step: 80, loss: 0.034283678978681564\n",
            "step: 90, loss: 0.04823361337184906\n",
            "step: 100, loss: 0.0052452655509114265\n",
            "step: 110, loss: 0.1020960584282875\n",
            "step: 120, loss: 0.01358024962246418\n",
            "step: 130, loss: 0.001587537582963705\n",
            "step: 140, loss: 0.0026156583335250616\n",
            "step: 150, loss: 0.01029429491609335\n",
            "step: 160, loss: 0.009446787647902966\n",
            "step: 170, loss: 0.13460826873779297\n",
            "step: 180, loss: 0.007128271274268627\n",
            "step: 190, loss: 0.013460456393659115\n",
            "step: 200, loss: 0.0076395366340875626\n",
            "step: 210, loss: 0.009566099382936954\n",
            "step: 220, loss: 0.018931657075881958\n",
            "step: 230, loss: 0.13945169746875763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9491150442477876, f1=0.9461883408071748, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007177019026130438\n",
            "step: 10, loss: 0.00611441396176815\n",
            "step: 20, loss: 0.05844248831272125\n",
            "step: 30, loss: 0.019985785707831383\n",
            "step: 40, loss: 0.026715505868196487\n",
            "step: 50, loss: 0.007457279600203037\n",
            "step: 60, loss: 0.001395341125316918\n",
            "step: 70, loss: 0.014799901284277439\n",
            "step: 80, loss: 0.12532952427864075\n",
            "step: 90, loss: 0.005480160471051931\n",
            "step: 100, loss: 0.0028589756693691015\n",
            "step: 110, loss: 0.0257476344704628\n",
            "step: 120, loss: 0.005595740862190723\n",
            "step: 130, loss: 0.0021560548339039087\n",
            "step: 140, loss: 0.006380511447787285\n",
            "step: 150, loss: 0.004977156408131123\n",
            "step: 160, loss: 0.008680887520313263\n",
            "step: 170, loss: 0.1059633120894432\n",
            "step: 180, loss: 0.0047697811387479305\n",
            "step: 190, loss: 0.031264662742614746\n",
            "step: 200, loss: 0.004618074744939804\n",
            "step: 210, loss: 0.07072529941797256\n",
            "step: 220, loss: 0.016803890466690063\n",
            "step: 230, loss: 0.00422531645745039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9387755102040816, f1=0.9345579793340988, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009079794399440289\n",
            "step: 10, loss: 0.007084952667355537\n",
            "step: 20, loss: 0.003943896386772394\n",
            "step: 30, loss: 0.0006764406571164727\n",
            "step: 40, loss: 0.004474794492125511\n",
            "step: 50, loss: 0.004131501540541649\n",
            "step: 60, loss: 0.07746041566133499\n",
            "step: 70, loss: 0.0038793806452304125\n",
            "step: 80, loss: 0.002897412283346057\n",
            "step: 90, loss: 0.0076660169288516045\n",
            "step: 100, loss: 0.04943598434329033\n",
            "step: 110, loss: 0.004660635720938444\n",
            "step: 120, loss: 0.02186436578631401\n",
            "step: 130, loss: 0.09700558334589005\n",
            "step: 140, loss: 0.003687991062179208\n",
            "step: 150, loss: 0.004335220903158188\n",
            "step: 160, loss: 0.03458915278315544\n",
            "step: 170, loss: 0.17893041670322418\n",
            "step: 180, loss: 0.002665470587089658\n",
            "step: 190, loss: 0.003291034372523427\n",
            "step: 200, loss: 0.018258055672049522\n",
            "step: 210, loss: 0.0027353139594197273\n",
            "step: 220, loss: 0.0011099124094471335\n",
            "step: 230, loss: 0.04938459396362305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9486607142857143, f1=0.9415730337078653, best_f1=0.9461883408071748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037016270216554403\n",
            "step: 10, loss: 0.02308814972639084\n",
            "step: 20, loss: 0.001052799983881414\n",
            "step: 30, loss: 0.003159956308081746\n",
            "step: 40, loss: 0.04059892147779465\n",
            "step: 50, loss: 0.09616944193840027\n",
            "step: 60, loss: 0.010274147614836693\n",
            "step: 70, loss: 0.0011672945693135262\n",
            "step: 80, loss: 0.0023021858651190996\n",
            "step: 90, loss: 0.0014660876477137208\n",
            "step: 100, loss: 0.0037294665817171335\n",
            "step: 110, loss: 0.2289830595254898\n",
            "step: 120, loss: 0.008185871876776218\n",
            "step: 130, loss: 0.0038335449062287807\n",
            "step: 140, loss: 0.00817815400660038\n",
            "step: 150, loss: 0.004081493243575096\n",
            "step: 160, loss: 0.0014121417189016938\n",
            "step: 170, loss: 0.002939034253358841\n",
            "step: 180, loss: 0.002280141692608595\n",
            "step: 190, loss: 0.030609775334596634\n",
            "step: 200, loss: 0.01247985940426588\n",
            "step: 210, loss: 0.001266294508241117\n",
            "step: 220, loss: 0.0005463407724164426\n",
            "step: 230, loss: 0.0020001695957034826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9512195121951219, f1=0.9498327759197325, best_f1=0.9498327759197325\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18865913152694702\n",
            "step: 10, loss: 0.0009666946716606617\n",
            "step: 20, loss: 0.00300309294834733\n",
            "step: 30, loss: 0.01454308070242405\n",
            "step: 40, loss: 0.002978892996907234\n",
            "step: 50, loss: 0.0007001270423643291\n",
            "step: 60, loss: 0.04247775301337242\n",
            "step: 70, loss: 0.008524863049387932\n",
            "step: 80, loss: 0.011748308315873146\n",
            "step: 90, loss: 0.005750618875026703\n",
            "step: 100, loss: 0.0006076113204471767\n",
            "step: 110, loss: 0.007204617839306593\n",
            "step: 120, loss: 0.013148884288966656\n",
            "step: 130, loss: 0.019146496430039406\n",
            "step: 140, loss: 0.0015004774322733283\n",
            "step: 150, loss: 0.003621876472607255\n",
            "step: 160, loss: 0.04694371670484543\n",
            "step: 170, loss: 0.00034149823477491736\n",
            "step: 180, loss: 0.0007529232534579933\n",
            "step: 190, loss: 0.006398649420589209\n",
            "step: 200, loss: 0.0061067561618983746\n",
            "step: 210, loss: 0.0006421646103262901\n",
            "step: 220, loss: 0.003520811442285776\n",
            "step: 230, loss: 0.08416302502155304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9518477043673013, f1=0.9435665914221218, best_f1=0.9435665914221218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011834357865154743\n",
            "step: 10, loss: 0.34571516513824463\n",
            "step: 20, loss: 0.0007321450975723565\n",
            "step: 30, loss: 0.0010351238306611776\n",
            "step: 40, loss: 0.0009783478453755379\n",
            "step: 50, loss: 0.0018574680434539914\n",
            "step: 60, loss: 0.00046295669744722545\n",
            "step: 70, loss: 0.003315561218187213\n",
            "step: 80, loss: 0.004633861593902111\n",
            "step: 90, loss: 0.003979984670877457\n",
            "step: 100, loss: 0.0005346803227439523\n",
            "step: 110, loss: 0.0020469047594815493\n",
            "step: 120, loss: 0.006996171083301306\n",
            "step: 130, loss: 0.004177777096629143\n",
            "step: 140, loss: 0.0007914719171822071\n",
            "step: 150, loss: 0.0012727963039651513\n",
            "step: 160, loss: 0.003525302279740572\n",
            "step: 170, loss: 0.0015382288256660104\n",
            "step: 180, loss: 0.011366945691406727\n",
            "step: 190, loss: 0.0009278489160351455\n",
            "step: 200, loss: 0.014774810522794724\n",
            "step: 210, loss: 0.004021852742880583\n",
            "step: 220, loss: 0.0004040678031742573\n",
            "step: 230, loss: 0.20406535267829895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9518477043673013, f1=0.9410430839002268, best_f1=0.9435665914221218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002408822299912572\n",
            "step: 10, loss: 0.0017490088939666748\n",
            "step: 20, loss: 0.004784193821251392\n",
            "step: 30, loss: 0.0015192677965387702\n",
            "step: 40, loss: 0.05053116753697395\n",
            "step: 50, loss: 0.0005720055196434259\n",
            "step: 60, loss: 0.008777856826782227\n",
            "step: 70, loss: 0.09685872495174408\n",
            "step: 80, loss: 0.07569149881601334\n",
            "step: 90, loss: 0.0016874163411557674\n",
            "step: 100, loss: 0.000855709717143327\n",
            "step: 110, loss: 0.0003974508144892752\n",
            "step: 120, loss: 0.05079597234725952\n",
            "step: 130, loss: 0.0016560337971895933\n",
            "step: 140, loss: 0.04256248474121094\n",
            "step: 150, loss: 0.001489349640905857\n",
            "step: 160, loss: 0.0006784720462746918\n",
            "step: 170, loss: 0.00019556567713152617\n",
            "step: 180, loss: 0.0017376340692862868\n",
            "step: 190, loss: 0.004045199602842331\n",
            "step: 200, loss: 0.00021754980843979865\n",
            "step: 210, loss: 0.0003640512877609581\n",
            "step: 220, loss: 0.018482958897948265\n",
            "step: 230, loss: 0.002803141251206398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9565217391304347, f1=0.9451287793952967, best_f1=0.9451287793952967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002505892189219594\n",
            "step: 10, loss: 0.0001385996729368344\n",
            "step: 20, loss: 0.0002799782669171691\n",
            "step: 30, loss: 0.0004420682380441576\n",
            "step: 40, loss: 0.002517684595659375\n",
            "step: 50, loss: 0.031951893121004105\n",
            "step: 60, loss: 0.0416036881506443\n",
            "step: 70, loss: 0.00406602630391717\n",
            "step: 80, loss: 0.0011823491659015417\n",
            "step: 90, loss: 0.00018366152653470635\n",
            "step: 100, loss: 0.00041908628190867603\n",
            "step: 110, loss: 0.00017749816470313817\n",
            "step: 120, loss: 0.01474534161388874\n",
            "step: 130, loss: 0.009812125004827976\n",
            "step: 140, loss: 0.05412349849939346\n",
            "step: 150, loss: 0.0004374094132799655\n",
            "step: 160, loss: 0.0008944948785938323\n",
            "step: 170, loss: 0.0010266833705827594\n",
            "step: 180, loss: 0.0070130350068211555\n",
            "step: 190, loss: 0.0007549207657575607\n",
            "step: 200, loss: 0.003374811029061675\n",
            "step: 210, loss: 0.0004711260844487697\n",
            "step: 220, loss: 0.002592662116512656\n",
            "step: 230, loss: 0.021909721195697784\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9530201342281879, f1=0.9503386004514672, best_f1=0.9451287793952967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006287691066972911\n",
            "step: 10, loss: 0.0007832198170945048\n",
            "step: 20, loss: 0.0002618947473820299\n",
            "step: 30, loss: 0.00017239000590052456\n",
            "step: 40, loss: 0.0028181227389723063\n",
            "step: 50, loss: 0.0006304644048213959\n",
            "step: 60, loss: 0.014190525747835636\n",
            "step: 70, loss: 0.0018445922760292888\n",
            "step: 80, loss: 0.004529297351837158\n",
            "step: 90, loss: 0.0063543482683598995\n",
            "step: 100, loss: 0.000562359462492168\n",
            "step: 110, loss: 0.0002390672598266974\n",
            "step: 120, loss: 0.0016845723148435354\n",
            "step: 130, loss: 0.0002416400529909879\n",
            "step: 140, loss: 0.00018677336629480124\n",
            "step: 150, loss: 0.0002602917666081339\n",
            "step: 160, loss: 0.0008026125724427402\n",
            "step: 170, loss: 0.00018674526654649526\n",
            "step: 180, loss: 0.0011836475459858775\n",
            "step: 190, loss: 0.004612443502992392\n",
            "step: 200, loss: 0.0005909692845307291\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 210, loss: 0.0001641264243517071\n",
            "step: 220, loss: 0.0003336627851240337\n",
            "step: 230, loss: 0.009352847933769226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9501661129568106, f1=0.9518477043673013, best_f1=0.9451287793952967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003191999567206949\n",
            "step: 10, loss: 0.0002891384647227824\n",
            "step: 20, loss: 0.00024258316261693835\n",
            "step: 30, loss: 0.0421210341155529\n",
            "step: 40, loss: 0.0046315984800457954\n",
            "step: 50, loss: 0.0001861527271103114\n",
            "step: 60, loss: 0.009885805658996105\n",
            "step: 70, loss: 0.0002621510357130319\n",
            "step: 80, loss: 0.00015110449749045074\n",
            "step: 90, loss: 0.0004283634480088949\n",
            "step: 100, loss: 0.0002666455402504653\n",
            "step: 110, loss: 0.0009617353207431734\n",
            "step: 120, loss: 0.00028355655376799405\n",
            "step: 130, loss: 0.0018560473108664155\n",
            "step: 140, loss: 0.0001273863308597356\n",
            "step: 150, loss: 7.586924766656011e-05\n",
            "step: 160, loss: 0.00012133269046898931\n",
            "step: 170, loss: 0.000478225527331233\n",
            "step: 180, loss: 8.12937505543232e-05\n",
            "step: 190, loss: 0.0007234029471874237\n",
            "step: 200, loss: 0.0006055555422790349\n",
            "step: 210, loss: 0.02047288790345192\n",
            "step: 220, loss: 0.007192941382527351\n",
            "step: 230, loss: 0.00016017093730624765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9588431590656283, f1=0.9506726457399103, best_f1=0.9506726457399103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012215584283694625\n",
            "step: 10, loss: 0.00012022460578009486\n",
            "step: 20, loss: 0.0002973934751935303\n",
            "step: 30, loss: 0.02156616933643818\n",
            "step: 40, loss: 0.000215393170947209\n",
            "step: 50, loss: 0.00030645637889392674\n",
            "step: 60, loss: 0.0001501150254625827\n",
            "step: 70, loss: 0.00013762890012003481\n",
            "step: 80, loss: 0.00013376298011280596\n",
            "step: 90, loss: 5.9815207350766286e-05\n",
            "step: 100, loss: 0.0011212952667847276\n",
            "step: 110, loss: 0.00023841422807890922\n",
            "step: 120, loss: 0.0002497639216016978\n",
            "step: 130, loss: 0.0003961913753300905\n",
            "step: 140, loss: 0.0023063335102051497\n",
            "step: 150, loss: 0.020171958953142166\n",
            "step: 160, loss: 0.00014472361363004893\n",
            "step: 170, loss: 0.00019942129438277334\n",
            "step: 180, loss: 0.0002558663545642048\n",
            "step: 190, loss: 0.01691138744354248\n",
            "step: 200, loss: 0.0004286569310352206\n",
            "step: 210, loss: 9.979152673622593e-05\n",
            "step: 220, loss: 0.000207486460567452\n",
            "step: 230, loss: 0.000875392637681216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9601769911504424, f1=0.9534368070953436, best_f1=0.9534368070953436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012840832641813904\n",
            "step: 10, loss: 6.0484991990961134e-05\n",
            "step: 20, loss: 0.004971032030880451\n",
            "step: 30, loss: 0.0002781510120257735\n",
            "step: 40, loss: 0.0008068294846452773\n",
            "step: 50, loss: 0.0001574660127516836\n",
            "step: 60, loss: 0.0001665893942117691\n",
            "step: 70, loss: 0.0001103335089283064\n",
            "step: 80, loss: 0.00024053252127487212\n",
            "step: 90, loss: 0.0002702136989682913\n",
            "step: 100, loss: 0.0003977168817073107\n",
            "step: 110, loss: 0.003651308361440897\n",
            "step: 120, loss: 0.004561041947454214\n",
            "step: 130, loss: 0.0001618391543161124\n",
            "step: 140, loss: 0.0001383497437927872\n",
            "step: 150, loss: 0.00015769233868923038\n",
            "step: 160, loss: 5.868625157745555e-05\n",
            "step: 170, loss: 0.00011352447472745553\n",
            "step: 180, loss: 0.00014190253568813205\n",
            "step: 190, loss: 0.00014624741743318737\n",
            "step: 200, loss: 0.00010199299140367657\n",
            "step: 210, loss: 0.0002676156582310796\n",
            "step: 220, loss: 0.0009285097476094961\n",
            "step: 230, loss: 5.563479135162197e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9573033707865168, f1=0.9513023782559457, best_f1=0.9534368070953436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.672803283436224e-05\n",
            "step: 10, loss: 0.00016045267693698406\n",
            "step: 20, loss: 0.0005548906046897173\n",
            "step: 30, loss: 0.00629072729498148\n",
            "step: 40, loss: 0.00010228485916741192\n",
            "step: 50, loss: 0.00022146159608382732\n",
            "step: 60, loss: 8.230601815739647e-05\n",
            "step: 70, loss: 0.00014556283713318408\n",
            "step: 80, loss: 0.0004923587548546493\n",
            "step: 90, loss: 0.00010691978968679905\n",
            "step: 100, loss: 0.00015930742665659636\n",
            "step: 110, loss: 0.00010777737043099478\n",
            "step: 120, loss: 0.0001188338064821437\n",
            "step: 130, loss: 0.00022872546105645597\n",
            "step: 140, loss: 0.00994477141648531\n",
            "step: 150, loss: 0.00011982439900748432\n",
            "step: 160, loss: 0.0009991723345592618\n",
            "step: 170, loss: 5.6245102314278483e-05\n",
            "step: 180, loss: 0.00011659820302156731\n",
            "step: 190, loss: 0.00019628381414804608\n",
            "step: 200, loss: 8.487709419569e-05\n",
            "step: 210, loss: 0.0009776350343599916\n",
            "step: 220, loss: 0.12423345446586609\n",
            "step: 230, loss: 0.00010679682600311935\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9600886917960089, f1=0.9533333333333333, best_f1=0.9534368070953436\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:08, 299.76it/s]\n",
            "load_f1 = 0.9569060773480662\n",
            "real_f1 = 0.9547960308710033\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:12, 358.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48c3831-7688-4933-d580-0f107ce87690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7982829809188843\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4464222490787506\n",
            "step: 20, loss: 0.5130362510681152\n",
            "step: 30, loss: 0.46509647369384766\n",
            "step: 40, loss: 0.45697081089019775\n",
            "step: 50, loss: 0.4262579679489136\n",
            "step: 60, loss: 0.34881386160850525\n",
            "step: 70, loss: 0.24456660449504852\n",
            "step: 80, loss: 0.42046743631362915\n",
            "step: 90, loss: 0.3932913541793823\n",
            "step: 100, loss: 0.20574498176574707\n",
            "step: 110, loss: 0.13382203876972198\n",
            "step: 120, loss: 0.16483166813850403\n",
            "step: 130, loss: 0.051462382078170776\n",
            "step: 140, loss: 0.44651874899864197\n",
            "step: 150, loss: 0.03997713699936867\n",
            "step: 160, loss: 0.16308239102363586\n",
            "step: 170, loss: 0.3571757376194\n",
            "step: 180, loss: 0.14475825428962708\n",
            "step: 190, loss: 0.05860590189695358\n",
            "step: 200, loss: 0.21275562047958374\n",
            "step: 210, loss: 0.10961677134037018\n",
            "step: 220, loss: 0.03240974247455597\n",
            "step: 230, loss: 0.09209427982568741\n",
            "step: 240, loss: 0.07035419344902039\n",
            "step: 250, loss: 0.10267607122659683\n",
            "step: 260, loss: 0.04524501785635948\n",
            "step: 270, loss: 0.024730414152145386\n",
            "step: 280, loss: 0.26218175888061523\n",
            "step: 290, loss: 0.22570261359214783\n",
            "step: 300, loss: 0.08830635249614716\n",
            "step: 310, loss: 0.05208001285791397\n",
            "step: 320, loss: 0.25876444578170776\n",
            "step: 330, loss: 0.19156429171562195\n",
            "step: 340, loss: 0.2791988253593445\n",
            "step: 350, loss: 0.0571381114423275\n",
            "step: 360, loss: 0.09291105717420578\n",
            "step: 370, loss: 0.11560910940170288\n",
            "step: 380, loss: 0.2357829511165619\n",
            "step: 390, loss: 0.17992079257965088\n",
            "step: 400, loss: 0.04467613250017166\n",
            "step: 410, loss: 0.07972530275583267\n",
            "step: 420, loss: 0.03446609154343605\n",
            "step: 430, loss: 0.19771605730056763\n",
            "step: 440, loss: 0.13597305119037628\n",
            "step: 450, loss: 0.09677958488464355\n",
            "step: 460, loss: 0.03825301304459572\n",
            "step: 470, loss: 0.18752069771289825\n",
            "step: 480, loss: 0.20512022078037262\n",
            "step: 490, loss: 0.13242879509925842\n",
            "step: 500, loss: 0.05141246318817139\n",
            "step: 510, loss: 0.07536085695028305\n",
            "step: 520, loss: 0.10368998348712921\n",
            "step: 530, loss: 0.06921650469303131\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9031963470319634, f1=0.8968978102189782, best_f1=0.8968978102189782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14758658409118652\n",
            "step: 10, loss: 0.1849534511566162\n",
            "step: 20, loss: 0.12472571432590485\n",
            "step: 30, loss: 0.04602784290909767\n",
            "step: 40, loss: 0.0070354831404984\n",
            "step: 50, loss: 0.03534610569477081\n",
            "step: 60, loss: 0.14841832220554352\n",
            "step: 70, loss: 0.11638058722019196\n",
            "step: 80, loss: 0.04458790645003319\n",
            "step: 90, loss: 0.12686200439929962\n",
            "step: 100, loss: 0.29792603850364685\n",
            "step: 110, loss: 0.03999379277229309\n",
            "step: 120, loss: 0.05961020663380623\n",
            "step: 130, loss: 0.08935576677322388\n",
            "step: 140, loss: 0.034836847335100174\n",
            "step: 150, loss: 0.09516876935958862\n",
            "step: 160, loss: 0.12636230885982513\n",
            "step: 170, loss: 0.2119758576154709\n",
            "step: 180, loss: 0.02471499890089035\n",
            "step: 190, loss: 0.06445015966892242\n",
            "step: 200, loss: 0.07098326832056046\n",
            "step: 210, loss: 0.07859379053115845\n",
            "step: 220, loss: 0.2956644296646118\n",
            "step: 230, loss: 0.06763769686222076\n",
            "step: 240, loss: 0.19837713241577148\n",
            "step: 250, loss: 0.1472877711057663\n",
            "step: 260, loss: 0.10624399036169052\n",
            "step: 270, loss: 0.15051932632923126\n",
            "step: 280, loss: 0.15498360991477966\n",
            "step: 290, loss: 0.09267909824848175\n",
            "step: 300, loss: 0.022046077996492386\n",
            "step: 310, loss: 0.3676856458187103\n",
            "step: 320, loss: 0.17332488298416138\n",
            "step: 330, loss: 0.10459278523921967\n",
            "step: 340, loss: 0.012971488758921623\n",
            "step: 350, loss: 0.22586101293563843\n",
            "step: 360, loss: 0.2055545151233673\n",
            "step: 370, loss: 0.044420674443244934\n",
            "step: 380, loss: 0.17351606488227844\n",
            "step: 390, loss: 0.0404730886220932\n",
            "step: 400, loss: 0.09949889034032822\n",
            "step: 410, loss: 0.005594071000814438\n",
            "step: 420, loss: 0.10594378411769867\n",
            "step: 430, loss: 0.044273022562265396\n",
            "step: 440, loss: 0.04025763273239136\n",
            "step: 450, loss: 0.05129757523536682\n",
            "step: 460, loss: 0.11632007360458374\n",
            "step: 470, loss: 0.08016475290060043\n",
            "step: 480, loss: 0.32457271218299866\n",
            "step: 490, loss: 0.04924792796373367\n",
            "step: 500, loss: 0.04077231511473656\n",
            "step: 510, loss: 0.027998214587569237\n",
            "step: 520, loss: 0.06820857524871826\n",
            "step: 530, loss: 0.10850340873003006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9106223678053346, f1=0.9018691588785047, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03825894743204117\n",
            "step: 10, loss: 0.04234534874558449\n",
            "step: 20, loss: 0.1378730833530426\n",
            "step: 30, loss: 0.11847293376922607\n",
            "step: 40, loss: 0.00622442364692688\n",
            "step: 50, loss: 0.12245883792638779\n",
            "step: 60, loss: 0.02345062978565693\n",
            "step: 70, loss: 0.010302943177521229\n",
            "step: 80, loss: 0.16449153423309326\n",
            "step: 90, loss: 0.43125614523887634\n",
            "step: 100, loss: 0.01929451711475849\n",
            "step: 110, loss: 0.04321666806936264\n",
            "step: 120, loss: 0.016466228291392326\n",
            "step: 130, loss: 0.0198192335665226\n",
            "step: 140, loss: 0.02723868191242218\n",
            "step: 150, loss: 0.06322763115167618\n",
            "step: 160, loss: 0.005680506583303213\n",
            "step: 170, loss: 0.05938377231359482\n",
            "step: 180, loss: 0.055588606745004654\n",
            "step: 190, loss: 0.0187629833817482\n",
            "step: 200, loss: 0.012609181925654411\n",
            "step: 210, loss: 0.09358619898557663\n",
            "step: 220, loss: 0.012968008406460285\n",
            "step: 230, loss: 0.04546431452035904\n",
            "step: 240, loss: 0.14553318917751312\n",
            "step: 250, loss: 0.04552246257662773\n",
            "step: 260, loss: 0.01668866164982319\n",
            "step: 270, loss: 0.04843959957361221\n",
            "step: 280, loss: 0.046303991228342056\n",
            "step: 290, loss: 0.11192066222429276\n",
            "step: 300, loss: 0.033322498202323914\n",
            "step: 310, loss: 0.06694579869508743\n",
            "step: 320, loss: 0.23736423254013062\n",
            "step: 330, loss: 0.03760208189487457\n",
            "step: 340, loss: 0.008031913079321384\n",
            "step: 350, loss: 0.07777071744203568\n",
            "step: 360, loss: 0.029809672385454178\n",
            "step: 370, loss: 0.07910262793302536\n",
            "step: 380, loss: 0.0045060585252940655\n",
            "step: 390, loss: 0.05904122814536095\n",
            "step: 400, loss: 0.04982192441821098\n",
            "step: 410, loss: 0.08997942507266998\n",
            "step: 420, loss: 0.07264994829893112\n",
            "step: 430, loss: 0.024079425260424614\n",
            "step: 440, loss: 0.07470434904098511\n",
            "step: 450, loss: 0.3387143611907959\n",
            "step: 460, loss: 0.16267913579940796\n",
            "step: 470, loss: 0.09984640032052994\n",
            "step: 480, loss: 0.017579995095729828\n",
            "step: 490, loss: 0.012687447480857372\n",
            "step: 500, loss: 0.11381826549768448\n",
            "step: 510, loss: 0.02992122247815132\n",
            "step: 520, loss: 0.027329402044415474\n",
            "step: 530, loss: 0.09111739695072174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9034676663542643, f1=0.8987816307403936, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00949192140251398\n",
            "step: 10, loss: 0.02455357275903225\n",
            "step: 20, loss: 0.018866166472434998\n",
            "step: 30, loss: 0.061631541699171066\n",
            "step: 40, loss: 0.03715266287326813\n",
            "step: 50, loss: 0.0925774872303009\n",
            "step: 60, loss: 0.04223945736885071\n",
            "step: 70, loss: 0.046036358922719955\n",
            "step: 80, loss: 0.019735081121325493\n",
            "step: 90, loss: 0.07762885093688965\n",
            "step: 100, loss: 0.026903139427304268\n",
            "step: 110, loss: 0.010609393939375877\n",
            "step: 120, loss: 0.049230583012104034\n",
            "step: 130, loss: 0.05898257717490196\n",
            "step: 140, loss: 0.008153392001986504\n",
            "step: 150, loss: 0.010185516439378262\n",
            "step: 160, loss: 0.015793202444911003\n",
            "step: 170, loss: 0.005714637227356434\n",
            "step: 180, loss: 0.019362427294254303\n",
            "step: 190, loss: 0.05633723363280296\n",
            "step: 200, loss: 0.004069034475833178\n",
            "step: 210, loss: 0.04003511741757393\n",
            "step: 220, loss: 0.011169001460075378\n",
            "step: 230, loss: 0.2651325762271881\n",
            "step: 240, loss: 0.006894652731716633\n",
            "step: 250, loss: 0.01763981394469738\n",
            "step: 260, loss: 0.05506746843457222\n",
            "step: 270, loss: 0.06506313383579254\n",
            "step: 280, loss: 0.007697486784309149\n",
            "step: 290, loss: 0.11614476889371872\n",
            "step: 300, loss: 0.019400259479880333\n",
            "step: 310, loss: 0.03575952351093292\n",
            "step: 320, loss: 0.04865473508834839\n",
            "step: 330, loss: 0.09919914603233337\n",
            "step: 340, loss: 0.15374065935611725\n",
            "step: 350, loss: 0.11978582292795181\n",
            "step: 360, loss: 0.04501991346478462\n",
            "step: 370, loss: 0.03568125516176224\n",
            "step: 380, loss: 0.012294307351112366\n",
            "step: 390, loss: 0.03295261040329933\n",
            "step: 400, loss: 0.03676719218492508\n",
            "step: 410, loss: 0.09193430095911026\n",
            "step: 420, loss: 0.07059762626886368\n",
            "step: 430, loss: 0.01758289337158203\n",
            "step: 440, loss: 0.037335190922021866\n",
            "step: 450, loss: 0.15598802268505096\n",
            "step: 460, loss: 0.013091343455016613\n",
            "step: 470, loss: 0.02564297243952751\n",
            "step: 480, loss: 0.07587657868862152\n",
            "step: 490, loss: 0.08643491566181183\n",
            "step: 500, loss: 0.030564382672309875\n",
            "step: 510, loss: 0.09564092010259628\n",
            "step: 520, loss: 0.1388179361820221\n",
            "step: 530, loss: 0.011920561082661152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9047399907961343, f1=0.8979779411764706, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005694983061403036\n",
            "step: 10, loss: 0.05972948670387268\n",
            "step: 20, loss: 0.1154671311378479\n",
            "step: 30, loss: 0.004450671374797821\n",
            "step: 40, loss: 0.06322582066059113\n",
            "step: 50, loss: 0.0037441032472997904\n",
            "step: 60, loss: 0.016159363090991974\n",
            "step: 70, loss: 0.030527932569384575\n",
            "step: 80, loss: 0.015611753799021244\n",
            "step: 90, loss: 0.010125990957021713\n",
            "step: 100, loss: 0.01511367503553629\n",
            "step: 110, loss: 0.004830013494938612\n",
            "step: 120, loss: 0.010847913101315498\n",
            "step: 130, loss: 0.01847393997013569\n",
            "step: 140, loss: 0.01718250662088394\n",
            "step: 150, loss: 0.1071280911564827\n",
            "step: 160, loss: 0.19249218702316284\n",
            "step: 170, loss: 0.02645530365407467\n",
            "step: 180, loss: 0.10257900506258011\n",
            "step: 190, loss: 0.004902875050902367\n",
            "step: 200, loss: 0.001651435042731464\n",
            "step: 210, loss: 0.0028760910499840975\n",
            "step: 220, loss: 0.00237189675681293\n",
            "step: 230, loss: 0.0013801357708871365\n",
            "step: 240, loss: 0.03192963823676109\n",
            "step: 250, loss: 0.0010654230136424303\n",
            "step: 260, loss: 0.044270068407058716\n",
            "step: 270, loss: 0.04338373243808746\n",
            "step: 280, loss: 0.13503782451152802\n",
            "step: 290, loss: 0.010246182791888714\n",
            "step: 300, loss: 0.09486734867095947\n",
            "step: 310, loss: 0.0017548976466059685\n",
            "step: 320, loss: 0.025495151057839394\n",
            "step: 330, loss: 0.006351694464683533\n",
            "step: 340, loss: 0.0012410851195454597\n",
            "step: 350, loss: 0.005915845278650522\n",
            "step: 360, loss: 0.00808770302683115\n",
            "step: 370, loss: 0.014679256826639175\n",
            "step: 380, loss: 0.011986649595201015\n",
            "step: 390, loss: 0.012160150334239006\n",
            "step: 400, loss: 0.01005606446415186\n",
            "step: 410, loss: 0.0025126696564257145\n",
            "step: 420, loss: 0.0009977903682738543\n",
            "step: 430, loss: 0.02173933945596218\n",
            "step: 440, loss: 0.008066880516707897\n",
            "step: 450, loss: 0.011741871014237404\n",
            "step: 460, loss: 0.0066484143026173115\n",
            "step: 470, loss: 0.0016458386089652777\n",
            "step: 480, loss: 0.031590305268764496\n",
            "step: 490, loss: 0.011751066893339157\n",
            "step: 500, loss: 0.09254276752471924\n",
            "step: 510, loss: 0.08580372482538223\n",
            "step: 520, loss: 0.020045340061187744\n",
            "step: 530, loss: 0.019534600898623466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.897998093422307, f1=0.8941063727839004, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02261875569820404\n",
            "step: 10, loss: 0.003615695284679532\n",
            "step: 20, loss: 0.001492846291512251\n",
            "step: 30, loss: 0.10798071324825287\n",
            "step: 40, loss: 0.005292493384331465\n",
            "step: 50, loss: 0.006549288984388113\n",
            "step: 60, loss: 0.003564016195014119\n",
            "step: 70, loss: 0.030891794711351395\n",
            "step: 80, loss: 0.006126376800239086\n",
            "step: 90, loss: 0.009247872978448868\n",
            "step: 100, loss: 0.17160281538963318\n",
            "step: 110, loss: 0.007791436742991209\n",
            "step: 120, loss: 0.03728912025690079\n",
            "step: 130, loss: 0.001251782989129424\n",
            "step: 140, loss: 0.0035225017927587032\n",
            "step: 150, loss: 0.01502310298383236\n",
            "step: 160, loss: 0.0020381142385303974\n",
            "step: 170, loss: 0.0045013814233243465\n",
            "step: 180, loss: 0.002975250594317913\n",
            "step: 190, loss: 0.04182012006640434\n",
            "step: 200, loss: 0.0017221340676769614\n",
            "step: 210, loss: 0.017037060111761093\n",
            "step: 220, loss: 0.004888026509433985\n",
            "step: 230, loss: 0.001384291215799749\n",
            "step: 240, loss: 0.014991635456681252\n",
            "step: 250, loss: 0.05478812754154205\n",
            "step: 260, loss: 0.01636170782148838\n",
            "step: 270, loss: 0.005938827060163021\n",
            "step: 280, loss: 0.05668129399418831\n",
            "step: 290, loss: 0.0032382067292928696\n",
            "step: 300, loss: 0.013971775770187378\n",
            "step: 310, loss: 0.0036689762491732836\n",
            "step: 320, loss: 0.003498539561405778\n",
            "step: 330, loss: 0.01680118404328823\n",
            "step: 340, loss: 0.005184834823012352\n",
            "step: 350, loss: 0.08557348698377609\n",
            "step: 360, loss: 0.00240496126934886\n",
            "step: 370, loss: 0.028351282700896263\n",
            "step: 380, loss: 0.006289043463766575\n",
            "step: 390, loss: 0.007012534886598587\n",
            "step: 400, loss: 0.03177235648036003\n",
            "step: 410, loss: 0.002664527390152216\n",
            "step: 420, loss: 0.01372231263667345\n",
            "step: 430, loss: 0.0009247343405149877\n",
            "step: 440, loss: 0.014624989591538906\n",
            "step: 450, loss: 0.012954012490808964\n",
            "step: 460, loss: 0.11166241019964218\n",
            "step: 470, loss: 0.19331343472003937\n",
            "step: 480, loss: 0.0013070735149085522\n",
            "step: 490, loss: 0.00035277154529467225\n",
            "step: 500, loss: 0.003148062853142619\n",
            "step: 510, loss: 0.0002703421632759273\n",
            "step: 520, loss: 0.0293479785323143\n",
            "step: 530, loss: 0.026543177664279938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.900976290097629, f1=0.8984557791296208, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1025884598493576\n",
            "step: 10, loss: 0.021899014711380005\n",
            "step: 20, loss: 0.09114135056734085\n",
            "step: 30, loss: 0.010739671997725964\n",
            "step: 40, loss: 0.0014450668822973967\n",
            "step: 50, loss: 0.008575382642447948\n",
            "step: 60, loss: 0.13863256573677063\n",
            "step: 70, loss: 0.0039099715650081635\n",
            "step: 80, loss: 0.0021618136670440435\n",
            "step: 90, loss: 0.0023560875561088324\n",
            "step: 100, loss: 0.004888542927801609\n",
            "step: 110, loss: 0.0005981494323350489\n",
            "step: 120, loss: 0.003330579260364175\n",
            "step: 130, loss: 0.007229745388031006\n",
            "step: 140, loss: 0.0012210758868604898\n",
            "step: 150, loss: 0.010626249015331268\n",
            "step: 160, loss: 0.03184524178504944\n",
            "step: 170, loss: 0.0005580020952038467\n",
            "step: 180, loss: 0.00016923858493100852\n",
            "step: 190, loss: 0.00041830644477158785\n",
            "step: 200, loss: 0.10008571296930313\n",
            "step: 210, loss: 0.005642113275825977\n",
            "step: 220, loss: 0.0012870035134255886\n",
            "step: 230, loss: 0.002792528597638011\n",
            "step: 240, loss: 0.007669567130506039\n",
            "step: 250, loss: 0.06794651597738266\n",
            "step: 260, loss: 0.0018096998101100326\n",
            "step: 270, loss: 0.0008586003095842898\n",
            "step: 280, loss: 0.06439966708421707\n",
            "step: 290, loss: 0.012451743707060814\n",
            "step: 300, loss: 0.0020789944101125\n",
            "step: 310, loss: 0.0011098942486569285\n",
            "step: 320, loss: 0.014999127946794033\n",
            "step: 330, loss: 0.0016055674059316516\n",
            "step: 340, loss: 0.0036702502984553576\n",
            "step: 350, loss: 0.000509137287735939\n",
            "step: 360, loss: 0.0017484967829659581\n",
            "step: 370, loss: 0.052420541644096375\n",
            "step: 380, loss: 0.02371697686612606\n",
            "step: 390, loss: 0.0887996107339859\n",
            "step: 400, loss: 0.0008282305207103491\n",
            "step: 410, loss: 0.0006793743814341724\n",
            "step: 420, loss: 0.07761706411838531\n",
            "step: 430, loss: 0.005867110565304756\n",
            "step: 440, loss: 0.0023358475882560015\n",
            "step: 450, loss: 0.15369990468025208\n",
            "step: 460, loss: 0.0047406842932105064\n",
            "step: 470, loss: 0.07186049222946167\n",
            "step: 480, loss: 0.02204899862408638\n",
            "step: 490, loss: 0.0013421833282336593\n",
            "step: 500, loss: 0.0023861927911639214\n",
            "step: 510, loss: 0.0626869797706604\n",
            "step: 520, loss: 0.011427217163145542\n",
            "step: 530, loss: 0.006198153365403414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.902867715078631, f1=0.895053166897827, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021147553343325853\n",
            "step: 10, loss: 0.012118292972445488\n",
            "step: 20, loss: 0.002598653081804514\n",
            "step: 30, loss: 0.046417322009801865\n",
            "step: 40, loss: 0.03596482798457146\n",
            "step: 50, loss: 0.007105269469320774\n",
            "step: 60, loss: 0.0005788626149296761\n",
            "step: 70, loss: 0.0027497904375195503\n",
            "step: 80, loss: 0.0003799244877882302\n",
            "step: 90, loss: 0.0014975713565945625\n",
            "step: 100, loss: 0.0016718741971999407\n",
            "step: 110, loss: 0.014063311740756035\n",
            "step: 120, loss: 0.00292645744048059\n",
            "step: 130, loss: 0.0023795857559889555\n",
            "step: 140, loss: 0.00838882103562355\n",
            "step: 150, loss: 0.008219695650041103\n",
            "step: 160, loss: 0.004183778073638678\n",
            "step: 170, loss: 0.0004660072736442089\n",
            "step: 180, loss: 0.0006778939277864993\n",
            "step: 190, loss: 0.02022307552397251\n",
            "step: 200, loss: 0.12221097946166992\n",
            "step: 210, loss: 0.07664517313241959\n",
            "step: 220, loss: 0.05119811370968819\n",
            "step: 230, loss: 0.014756621792912483\n",
            "step: 240, loss: 0.002670933958142996\n",
            "step: 250, loss: 0.0024444579612463713\n",
            "step: 260, loss: 0.008775202557444572\n",
            "step: 270, loss: 0.030993878841400146\n",
            "step: 280, loss: 0.0031554296147078276\n",
            "step: 290, loss: 0.00820474699139595\n",
            "step: 300, loss: 0.0010663140565156937\n",
            "step: 310, loss: 0.1537461280822754\n",
            "step: 320, loss: 0.0014392659068107605\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 330, loss: 0.011940453201532364\n",
            "step: 340, loss: 0.0026539878454059362\n",
            "step: 350, loss: 0.0021168726962059736\n",
            "step: 360, loss: 0.009972946718335152\n",
            "step: 370, loss: 0.0002928411995526403\n",
            "step: 380, loss: 0.013438199646770954\n",
            "step: 390, loss: 0.0003107581287622452\n",
            "step: 400, loss: 0.037685543298721313\n",
            "step: 410, loss: 0.042224619537591934\n",
            "step: 420, loss: 0.010965961031615734\n",
            "step: 430, loss: 0.00027797420625574887\n",
            "step: 440, loss: 0.000565855298191309\n",
            "step: 450, loss: 0.00019999158394057304\n",
            "step: 460, loss: 0.005684616044163704\n",
            "step: 470, loss: 0.00031648701406084\n",
            "step: 480, loss: 0.0012036680709570646\n",
            "step: 490, loss: 0.0028630474116653204\n",
            "step: 500, loss: 0.00553123839199543\n",
            "step: 510, loss: 0.008456171490252018\n",
            "step: 520, loss: 0.0012767510488629341\n",
            "step: 530, loss: 0.0011223091278225183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9045368620037807, f1=0.8946360153256706, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00564716849476099\n",
            "step: 10, loss: 0.0056289322674274445\n",
            "step: 20, loss: 0.005199304781854153\n",
            "step: 30, loss: 0.00021358694357331842\n",
            "step: 40, loss: 0.007544674910604954\n",
            "step: 50, loss: 0.00011369844287401065\n",
            "step: 60, loss: 0.0751718133687973\n",
            "step: 70, loss: 0.04117821529507637\n",
            "step: 80, loss: 0.0007010928820818663\n",
            "step: 90, loss: 0.003998066298663616\n",
            "step: 100, loss: 0.0004571712634060532\n",
            "step: 110, loss: 0.0011872985633090138\n",
            "step: 120, loss: 0.014522627927362919\n",
            "step: 130, loss: 0.001164962537586689\n",
            "step: 140, loss: 0.03517058119177818\n",
            "step: 150, loss: 0.0008350281277671456\n",
            "step: 160, loss: 0.0003857213305309415\n",
            "step: 170, loss: 0.0016956122126430273\n",
            "step: 180, loss: 0.0004570067976601422\n",
            "step: 190, loss: 0.018826473504304886\n",
            "step: 200, loss: 0.01827535405755043\n",
            "step: 210, loss: 0.024299079552292824\n",
            "step: 220, loss: 0.001977416453883052\n",
            "step: 230, loss: 0.00035494109033606946\n",
            "step: 240, loss: 0.0007385443896055222\n",
            "step: 250, loss: 0.025908075273036957\n",
            "step: 260, loss: 0.0005093482905067503\n",
            "step: 270, loss: 0.0009971554391086102\n",
            "step: 280, loss: 0.0007454385049641132\n",
            "step: 290, loss: 0.004448758438229561\n",
            "step: 300, loss: 0.006682144477963448\n",
            "step: 310, loss: 0.0029678253922611475\n",
            "step: 320, loss: 0.00018512620590627193\n",
            "step: 330, loss: 0.0016137483762577176\n",
            "step: 340, loss: 0.00033842556877061725\n",
            "step: 350, loss: 0.0004195731598883867\n",
            "step: 360, loss: 0.12560687959194183\n",
            "step: 370, loss: 0.003002808429300785\n",
            "step: 380, loss: 0.000516363128554076\n",
            "step: 390, loss: 0.00045841190149076283\n",
            "step: 400, loss: 0.013568051159381866\n",
            "step: 410, loss: 0.0009336645598523319\n",
            "step: 420, loss: 0.002737500239163637\n",
            "step: 430, loss: 8.7067557615228e-05\n",
            "step: 440, loss: 0.00015615567099303007\n",
            "step: 450, loss: 0.00018836840172298253\n",
            "step: 460, loss: 0.01116976048797369\n",
            "step: 470, loss: 0.0034195471089333296\n",
            "step: 480, loss: 0.00022866670042276382\n",
            "step: 490, loss: 0.017325570806860924\n",
            "step: 500, loss: 0.005075111053884029\n",
            "step: 510, loss: 0.000851477961987257\n",
            "step: 520, loss: 0.0004450225969776511\n",
            "step: 530, loss: 0.011307810433208942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.909689557855127, f1=0.8974600188146755, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014510875334963202\n",
            "step: 10, loss: 5.348390550352633e-05\n",
            "step: 20, loss: 0.00033563983743079007\n",
            "step: 30, loss: 0.000579712213948369\n",
            "step: 40, loss: 0.002069519367069006\n",
            "step: 50, loss: 0.0008557431283406913\n",
            "step: 60, loss: 0.00030664229416288435\n",
            "step: 70, loss: 0.001862548291683197\n",
            "step: 80, loss: 0.0013343878090381622\n",
            "step: 90, loss: 0.00013970861618872732\n",
            "step: 100, loss: 0.00041801054612733424\n",
            "step: 110, loss: 0.0013092479202896357\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 120, loss: 0.060049451887607574\n",
            "step: 130, loss: 0.0004665509914048016\n",
            "step: 140, loss: 0.00018372925114817917\n",
            "step: 150, loss: 0.00011535705561982468\n",
            "step: 160, loss: 0.0001132856632466428\n",
            "step: 170, loss: 8.702820196049288e-05\n",
            "step: 180, loss: 0.0041213869117200375\n",
            "step: 190, loss: 0.00425015389919281\n",
            "step: 200, loss: 0.0010279413545504212\n",
            "step: 210, loss: 0.0009273221949115396\n",
            "step: 220, loss: 0.040195707231760025\n",
            "step: 230, loss: 0.00041240412974730134\n",
            "step: 240, loss: 0.00026329830870963633\n",
            "step: 250, loss: 0.0007406338700093329\n",
            "step: 260, loss: 0.029413679614663124\n",
            "step: 270, loss: 0.018715035170316696\n",
            "step: 280, loss: 0.0023499876260757446\n",
            "step: 290, loss: 0.0015848601469770074\n",
            "step: 300, loss: 9.948539809556678e-05\n",
            "step: 310, loss: 0.0004565985582303256\n",
            "step: 320, loss: 0.0008190007647499442\n",
            "step: 330, loss: 0.0005142292939126492\n",
            "step: 340, loss: 0.0002180187002522871\n",
            "step: 350, loss: 0.0013291981304064393\n",
            "step: 360, loss: 0.010360551066696644\n",
            "step: 370, loss: 0.030326316133141518\n",
            "step: 380, loss: 0.002050215844064951\n",
            "step: 390, loss: 0.0003189879935234785\n",
            "step: 400, loss: 0.001730454503558576\n",
            "step: 410, loss: 0.00016337656415998936\n",
            "step: 420, loss: 0.0027209350373595953\n",
            "step: 430, loss: 0.00013890657282900065\n",
            "step: 440, loss: 0.00028123368974775076\n",
            "step: 450, loss: 0.005348957143723965\n",
            "step: 460, loss: 0.006199202965945005\n",
            "step: 470, loss: 0.0011373708257451653\n",
            "step: 480, loss: 0.03535556420683861\n",
            "step: 490, loss: 0.06211197003722191\n",
            "step: 500, loss: 0.047885745763778687\n",
            "step: 510, loss: 0.0009636637405492365\n",
            "step: 520, loss: 0.007405717391520739\n",
            "step: 530, loss: 0.00030635553412139416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9063809967396367, f1=0.8998127340823969, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004500838927924633\n",
            "step: 10, loss: 0.02629302628338337\n",
            "step: 20, loss: 0.00012884955503977835\n",
            "step: 30, loss: 0.002587533090263605\n",
            "step: 40, loss: 0.001340128481388092\n",
            "step: 50, loss: 0.0003754629287868738\n",
            "step: 60, loss: 0.0007827695226296782\n",
            "step: 70, loss: 0.00032670609652996063\n",
            "step: 80, loss: 0.0004821764014195651\n",
            "step: 90, loss: 0.023742714896798134\n",
            "step: 100, loss: 0.0025149991270154715\n",
            "step: 110, loss: 0.012129341252148151\n",
            "step: 120, loss: 0.001681641093455255\n",
            "step: 130, loss: 0.0011537373065948486\n",
            "step: 140, loss: 0.00023889285512268543\n",
            "step: 150, loss: 0.0008498994866386056\n",
            "step: 160, loss: 0.013743972405791283\n",
            "step: 170, loss: 0.10277321189641953\n",
            "step: 180, loss: 0.001778391539119184\n",
            "step: 190, loss: 0.0035955794155597687\n",
            "step: 200, loss: 0.0014492339687421918\n",
            "step: 210, loss: 0.00022756373800802976\n",
            "step: 220, loss: 0.009299754165112972\n",
            "step: 230, loss: 0.00369303859770298\n",
            "step: 240, loss: 0.01687893457710743\n",
            "step: 250, loss: 0.011287764646112919\n",
            "step: 260, loss: 0.0008093130891211331\n",
            "step: 270, loss: 0.0025043704081326723\n",
            "step: 280, loss: 0.039190974086523056\n",
            "step: 290, loss: 0.0794457346200943\n",
            "step: 300, loss: 0.015268977731466293\n",
            "step: 310, loss: 0.03180598095059395\n",
            "step: 320, loss: 0.00975189171731472\n",
            "step: 330, loss: 0.00029940297827124596\n",
            "step: 340, loss: 0.0008060156833380461\n",
            "step: 350, loss: 0.00048817283823154867\n",
            "step: 360, loss: 0.00031645153649151325\n",
            "step: 370, loss: 9.615744784241542e-05\n",
            "step: 380, loss: 0.00019404084014240652\n",
            "step: 390, loss: 0.010689321905374527\n",
            "step: 400, loss: 5.9560901718214154e-05\n",
            "step: 410, loss: 6.33394083706662e-05\n",
            "step: 420, loss: 0.0016930322162806988\n",
            "step: 430, loss: 0.10574033111333847\n",
            "step: 440, loss: 0.0007854807772673666\n",
            "step: 450, loss: 0.0023246014025062323\n",
            "step: 460, loss: 0.002701259683817625\n",
            "step: 470, loss: 0.0015444901073351502\n",
            "step: 480, loss: 0.0018672990845516324\n",
            "step: 490, loss: 0.009808644652366638\n",
            "step: 500, loss: 0.00014631239173468202\n",
            "step: 510, loss: 0.001237741787917912\n",
            "step: 520, loss: 0.0007262483122758567\n",
            "step: 530, loss: 0.000774629705119878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9078885214926783, f1=0.8921897460469573, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017689166590571404\n",
            "step: 10, loss: 0.002758654300123453\n",
            "step: 20, loss: 0.00016214372590184212\n",
            "step: 30, loss: 0.0021541034802794456\n",
            "step: 40, loss: 0.007923010736703873\n",
            "step: 50, loss: 0.001392142497934401\n",
            "step: 60, loss: 0.0013603456318378448\n",
            "step: 70, loss: 0.051240477710962296\n",
            "step: 80, loss: 0.0017095537623390555\n",
            "step: 90, loss: 0.00031162326922640204\n",
            "step: 100, loss: 0.0003878317656926811\n",
            "step: 110, loss: 0.03193813934922218\n",
            "step: 120, loss: 0.0048200832679867744\n",
            "step: 130, loss: 0.0001300815201830119\n",
            "step: 140, loss: 0.0019140883814543486\n",
            "step: 150, loss: 0.00178417784627527\n",
            "step: 160, loss: 0.002395977731794119\n",
            "step: 170, loss: 0.0005725394585169852\n",
            "step: 180, loss: 0.00011707811063388363\n",
            "step: 190, loss: 4.6523498895112425e-05\n",
            "step: 200, loss: 0.00017144490266218781\n",
            "step: 210, loss: 0.00048115718527697027\n",
            "step: 220, loss: 0.0010320550063624978\n",
            "step: 230, loss: 0.0013901203637942672\n",
            "step: 240, loss: 0.006509731058031321\n",
            "step: 250, loss: 9.208107076119632e-05\n",
            "step: 260, loss: 0.007472457364201546\n",
            "step: 270, loss: 0.0021734428592026234\n",
            "step: 280, loss: 0.0005063028074800968\n",
            "step: 290, loss: 0.002734552603214979\n",
            "step: 300, loss: 0.0009465275215916336\n",
            "step: 310, loss: 0.0002366201370023191\n",
            "step: 320, loss: 0.03438413888216019\n",
            "step: 330, loss: 4.719717981060967e-05\n",
            "step: 340, loss: 0.0026942668482661247\n",
            "step: 350, loss: 0.008574465289711952\n",
            "step: 360, loss: 0.06544957309961319\n",
            "step: 370, loss: 0.003381578717380762\n",
            "step: 380, loss: 0.0018557123839855194\n",
            "step: 390, loss: 6.689751899102703e-05\n",
            "step: 400, loss: 0.0006841438007541001\n",
            "step: 410, loss: 0.0007656239322386682\n",
            "step: 420, loss: 0.003603107761591673\n",
            "step: 430, loss: 0.0009408665937371552\n",
            "step: 440, loss: 0.0011350024724379182\n",
            "step: 450, loss: 0.00410850066691637\n",
            "step: 460, loss: 0.0005143980379216373\n",
            "step: 470, loss: 0.00028481963090598583\n",
            "step: 480, loss: 0.0047204080037772655\n",
            "step: 490, loss: 0.0012178727192804217\n",
            "step: 500, loss: 0.04734024405479431\n",
            "step: 510, loss: 0.003943316638469696\n",
            "step: 520, loss: 0.02699017897248268\n",
            "step: 530, loss: 0.00045606750063598156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9052044609665428, f1=0.9004207573632538, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003362427232787013\n",
            "step: 10, loss: 0.0009891543304547668\n",
            "step: 20, loss: 0.006844718009233475\n",
            "step: 30, loss: 0.0028039757162332535\n",
            "step: 40, loss: 0.0021331619936972857\n",
            "step: 50, loss: 0.00027809679158963263\n",
            "step: 60, loss: 0.0004339178849477321\n",
            "step: 70, loss: 0.00012681067164521664\n",
            "step: 80, loss: 0.003235679119825363\n",
            "step: 90, loss: 0.00026608232292346656\n",
            "step: 100, loss: 0.00014002232637722045\n",
            "step: 110, loss: 0.00316513329744339\n",
            "step: 120, loss: 0.0005847300635650754\n",
            "step: 130, loss: 0.00042620583553798497\n",
            "step: 140, loss: 0.00018643939984031022\n",
            "step: 150, loss: 0.00016130581207107753\n",
            "step: 160, loss: 0.00019262875139247626\n",
            "step: 170, loss: 0.00011534024815773591\n",
            "step: 180, loss: 0.00010961997759295627\n",
            "step: 190, loss: 0.00036281815846450627\n",
            "step: 200, loss: 0.003266807645559311\n",
            "step: 210, loss: 0.010818293318152428\n",
            "step: 220, loss: 0.00035568352905102074\n",
            "step: 230, loss: 0.002471545711159706\n",
            "step: 240, loss: 4.568430449580774e-05\n",
            "step: 250, loss: 0.00783555582165718\n",
            "step: 260, loss: 0.00010502606892259791\n",
            "step: 270, loss: 0.0002526793396100402\n",
            "step: 280, loss: 0.0005216982681304216\n",
            "step: 290, loss: 0.00010251572530250996\n",
            "step: 300, loss: 0.08309199661016464\n",
            "step: 310, loss: 6.260585359996185e-05\n",
            "step: 320, loss: 6.656361802015454e-05\n",
            "step: 330, loss: 0.008517318405210972\n",
            "step: 340, loss: 6.0823924286523834e-05\n",
            "step: 350, loss: 0.02015695348381996\n",
            "step: 360, loss: 4.7685334720881656e-05\n",
            "step: 370, loss: 0.00022537459153681993\n",
            "step: 380, loss: 0.0001646478776820004\n",
            "step: 390, loss: 0.0027441338170319796\n",
            "step: 400, loss: 8.002568210940808e-05\n",
            "step: 410, loss: 0.0017210105434060097\n",
            "step: 420, loss: 0.0001638496178202331\n",
            "step: 430, loss: 0.00021311210002750158\n",
            "step: 440, loss: 0.0007066582329571247\n",
            "step: 450, loss: 0.035697903484106064\n",
            "step: 460, loss: 3.017406379512977e-05\n",
            "step: 470, loss: 0.00015888974303379655\n",
            "step: 480, loss: 0.00015581378829665482\n",
            "step: 490, loss: 0.00012976604921277612\n",
            "step: 500, loss: 0.0010981657542288303\n",
            "step: 510, loss: 0.0003643592062871903\n",
            "step: 520, loss: 0.0031372078228741884\n",
            "step: 530, loss: 4.464137236936949e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9081439393939394, f1=0.8956646021915198, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011103971861302853\n",
            "step: 10, loss: 0.0002533833321649581\n",
            "step: 20, loss: 0.002520930953323841\n",
            "step: 30, loss: 0.0001367545046377927\n",
            "step: 40, loss: 0.0016813272377476096\n",
            "step: 50, loss: 0.007435955572873354\n",
            "step: 60, loss: 5.764148590969853e-05\n",
            "step: 70, loss: 0.00010172993643209338\n",
            "step: 80, loss: 0.0008153197122737765\n",
            "step: 90, loss: 3.526260843500495e-05\n",
            "step: 100, loss: 0.019410571083426476\n",
            "step: 110, loss: 0.0009099774179048836\n",
            "step: 120, loss: 0.0005080591654404998\n",
            "step: 130, loss: 0.013277019374072552\n",
            "step: 140, loss: 0.001084362855181098\n",
            "step: 150, loss: 0.0016927020624279976\n",
            "step: 160, loss: 0.0022211463656276464\n",
            "step: 170, loss: 0.029058555141091347\n",
            "step: 180, loss: 0.00017117369861807674\n",
            "step: 190, loss: 0.0003252696478739381\n",
            "step: 200, loss: 0.0005044098361395299\n",
            "step: 210, loss: 0.0011394941247999668\n",
            "step: 220, loss: 0.0005045440630055964\n",
            "step: 230, loss: 0.00018764031119644642\n",
            "step: 240, loss: 0.029508911073207855\n",
            "step: 250, loss: 0.00013873455463908613\n",
            "step: 260, loss: 4.0086426452035084e-05\n",
            "step: 270, loss: 0.00338375149294734\n",
            "step: 280, loss: 0.0005839442601427436\n",
            "step: 290, loss: 0.00014334962179418653\n",
            "step: 300, loss: 0.0118851438164711\n",
            "step: 310, loss: 7.801549509167671e-05\n",
            "step: 320, loss: 0.0013877426972612739\n",
            "step: 330, loss: 0.0004280692955944687\n",
            "step: 340, loss: 8.011395402718335e-05\n",
            "step: 350, loss: 0.08361838757991791\n",
            "step: 360, loss: 0.004957842640578747\n",
            "step: 370, loss: 0.004078226629644632\n",
            "step: 380, loss: 0.000567456241697073\n",
            "step: 390, loss: 0.0003723127010744065\n",
            "step: 400, loss: 8.384545799344778e-05\n",
            "step: 410, loss: 0.00029917582287453115\n",
            "step: 420, loss: 0.00044559402158483863\n",
            "step: 430, loss: 0.03826957568526268\n",
            "step: 440, loss: 8.488256571581587e-05\n",
            "step: 450, loss: 0.00042476505041122437\n",
            "step: 460, loss: 0.0003505288332235068\n",
            "step: 470, loss: 0.0002665844513103366\n",
            "step: 480, loss: 8.673420234117657e-05\n",
            "step: 490, loss: 0.0002494853688403964\n",
            "step: 500, loss: 0.0002464163117110729\n",
            "step: 510, loss: 0.00022857241856399924\n",
            "step: 520, loss: 0.00030458849505521357\n",
            "step: 530, loss: 0.0001447522226953879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.907906976744186, f1=0.9010782934833569, best_f1=0.9018691588785047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003322795091662556\n",
            "step: 10, loss: 6.990740803303197e-05\n",
            "step: 20, loss: 0.0010017832973971963\n",
            "step: 30, loss: 0.05226628854870796\n",
            "step: 40, loss: 5.313063593348488e-05\n",
            "step: 50, loss: 0.00012790581968147308\n",
            "step: 60, loss: 4.87485813209787e-05\n",
            "step: 70, loss: 0.00013733140076510608\n",
            "step: 80, loss: 0.00048187977517955005\n",
            "step: 90, loss: 5.172291639610194e-05\n",
            "step: 100, loss: 0.00012843689182773232\n",
            "step: 110, loss: 0.01055108942091465\n",
            "step: 120, loss: 0.0026062140241265297\n",
            "step: 130, loss: 0.00624282518401742\n",
            "step: 140, loss: 5.6472516007488593e-05\n",
            "step: 150, loss: 0.00023452599998563528\n",
            "step: 160, loss: 0.04265584796667099\n",
            "step: 170, loss: 0.0023937434889376163\n",
            "step: 180, loss: 0.00018533741240389645\n",
            "step: 190, loss: 0.00028078255127184093\n",
            "step: 200, loss: 0.00041029395652003586\n",
            "step: 210, loss: 0.038416944444179535\n",
            "step: 220, loss: 0.00023327778035309166\n",
            "step: 230, loss: 0.0006458965362980962\n",
            "step: 240, loss: 0.030704353004693985\n",
            "step: 250, loss: 0.00011783543595811352\n",
            "step: 260, loss: 8.200830052373931e-05\n",
            "step: 270, loss: 0.0020487839356064796\n",
            "step: 280, loss: 0.0004050748539157212\n",
            "step: 290, loss: 6.690154259558767e-05\n",
            "step: 300, loss: 0.0032447385601699352\n",
            "step: 310, loss: 0.0008522473508492112\n",
            "step: 320, loss: 0.0001961445959750563\n",
            "step: 330, loss: 0.0023437633644789457\n",
            "step: 340, loss: 0.0011092734057456255\n",
            "step: 350, loss: 0.00025926329544745386\n",
            "step: 360, loss: 0.0004146920400671661\n",
            "step: 370, loss: 0.0001325151533819735\n",
            "step: 380, loss: 0.00013743346789851785\n",
            "step: 390, loss: 0.0005480347317643464\n",
            "step: 400, loss: 0.0005917568923905492\n",
            "step: 410, loss: 0.0003089495876338333\n",
            "step: 420, loss: 0.0007494863239116967\n",
            "step: 430, loss: 0.000527364551089704\n",
            "step: 440, loss: 0.00036570316297002137\n",
            "step: 450, loss: 0.0017207941273227334\n",
            "step: 460, loss: 0.00020908673468511552\n",
            "step: 470, loss: 0.0003068136575166136\n",
            "step: 480, loss: 0.00033877143869176507\n",
            "step: 490, loss: 0.006712194532155991\n",
            "step: 500, loss: 0.0010226760059595108\n",
            "step: 510, loss: 8.474996866425499e-05\n",
            "step: 520, loss: 0.00023826381948310882\n",
            "step: 530, loss: 0.0001066458280547522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9064814814814813, f1=0.8997188378631676, best_f1=0.9018691588785047\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:14, 383.22it/s]\n",
            "load_f1 = 0.909090909090909\n",
            "real_f1 = 0.907906976744186\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:11, 381.82it/s]\n"
          ]
        }
      ]
    }
  ]
}